<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>4&nbsp; 2.1 Goals for this chapter – Modern Statistics for Modern Biology</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./03-chap.html" rel="next">
<link href="./01-chap.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-22854ec117201859c8a7ba6f538122c9.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="msmb.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./02-chap.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">2.1 Goals for this chapter</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Modern Statistics for Modern Biology</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Home</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The challenge: heterogeneity</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">1.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-chap.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">2.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">3.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">4.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">5.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">6.1 Goals for this Chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">7.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">8.1 Goals of this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">9.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">10.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">11.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">12.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">13.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">14-chap.html</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">15-chap.html</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">16-chap.html</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-difference-between-statistical-and-probabilistic-models" id="toc-the-difference-between-statistical-and-probabilistic-models" class="nav-link active" data-scroll-target="#the-difference-between-statistical-and-probabilistic-models"><span class="header-section-number">4.1</span> 2.2 The difference between statistical and probabilistic models</a></li>
  <li><a href="#a-simple-example-of-statistical-modeling" id="toc-a-simple-example-of-statistical-modeling" class="nav-link" data-scroll-target="#a-simple-example-of-statistical-modeling"><span class="header-section-number">4.2</span> 2.3 A simple example of statistical modeling</a>
  <ul class="collapse">
  <li><a href="#classical-statistics-for-classical-data" id="toc-classical-statistics-for-classical-data" class="nav-link" data-scroll-target="#classical-statistics-for-classical-data"><span class="header-section-number">4.2.1</span> 2.3.1 Classical statistics for classical data</a></li>
  </ul></li>
  <li><a href="#binomial-distributions-and-maximum-likelihood" id="toc-binomial-distributions-and-maximum-likelihood" class="nav-link" data-scroll-target="#binomial-distributions-and-maximum-likelihood"><span class="header-section-number">4.3</span> 2.4 Binomial distributions and maximum likelihood</a>
  <ul class="collapse">
  <li><a href="#an-example" id="toc-an-example" class="nav-link" data-scroll-target="#an-example"><span class="header-section-number">4.3.1</span> 2.4.1 An example</a></li>
  <li><a href="#likelihood-for-the-binomial-distribution" id="toc-likelihood-for-the-binomial-distribution" class="nav-link" data-scroll-target="#likelihood-for-the-binomial-distribution"><span class="header-section-number">4.3.2</span> 2.4.2 Likelihood for the binomial distribution</a></li>
  </ul></li>
  <li><a href="#more-boxesmultinomial-data" id="toc-more-boxesmultinomial-data" class="nav-link" data-scroll-target="#more-boxesmultinomial-data"><span class="header-section-number">4.4</span> 2.5 More boxes:multinomial data</a>
  <ul class="collapse">
  <li><a href="#dna-count-modeling-base-pairs" id="toc-dna-count-modeling-base-pairs" class="nav-link" data-scroll-target="#dna-count-modeling-base-pairs"><span class="header-section-number">4.4.1</span> 2.5.1 DNA count modeling: base pairs</a></li>
  <li><a href="#nucleotide-bias" id="toc-nucleotide-bias" class="nav-link" data-scroll-target="#nucleotide-bias"><span class="header-section-number">4.4.2</span> 2.5.2 Nucleotide bias</a></li>
  </ul></li>
  <li><a href="#the-2-distribution" id="toc-the-2-distribution" class="nav-link" data-scroll-target="#the-2-distribution"><span class="header-section-number">4.5</span> 2.6 The \(^2\) distribution</a>
  <ul class="collapse">
  <li><a href="#intermezzo-quantiles-and-the-quantile-quantile-plot" id="toc-intermezzo-quantiles-and-the-quantile-quantile-plot" class="nav-link" data-scroll-target="#intermezzo-quantiles-and-the-quantile-quantile-plot"><span class="header-section-number">4.5.1</span> 2.6.1 Intermezzo: quantiles and the quantile-quantile plot</a></li>
  </ul></li>
  <li><a href="#chargaffs-rule" id="toc-chargaffs-rule" class="nav-link" data-scroll-target="#chargaffs-rule"><span class="header-section-number">4.6</span> 2.7 Chargaff’s Rule</a>
  <ul class="collapse">
  <li><a href="#two-categorical-variables" id="toc-two-categorical-variables" class="nav-link" data-scroll-target="#two-categorical-variables"><span class="header-section-number">4.6.1</span> 2.7.1 Two categorical variables</a></li>
  <li><a href="#a-special-multinomial-hardy-weinberg-equilibrium" id="toc-a-special-multinomial-hardy-weinberg-equilibrium" class="nav-link" data-scroll-target="#a-special-multinomial-hardy-weinberg-equilibrium"><span class="header-section-number">4.6.2</span> 2.7.2 A special multinomial: Hardy-Weinberg equilibrium</a></li>
  <li><a href="#concatenating-several-multinomials-sequence-motifs-and-logos" id="toc-concatenating-several-multinomials-sequence-motifs-and-logos" class="nav-link" data-scroll-target="#concatenating-several-multinomials-sequence-motifs-and-logos"><span class="header-section-number">4.6.3</span> 2.7.3 Concatenating several multinomials: sequence motifs and logos</a></li>
  </ul></li>
  <li><a href="#modeling-sequential-dependencies-markov-chains" id="toc-modeling-sequential-dependencies-markov-chains" class="nav-link" data-scroll-target="#modeling-sequential-dependencies-markov-chains"><span class="header-section-number">4.7</span> 2.8 Modeling sequential dependencies: Markov chains</a></li>
  <li><a href="#bayesian-thinking" id="toc-bayesian-thinking" class="nav-link" data-scroll-target="#bayesian-thinking"><span class="header-section-number">4.8</span> 2.9 Bayesian Thinking</a>
  <ul class="collapse">
  <li><a href="#example-haplotype-frequencies" id="toc-example-haplotype-frequencies" class="nav-link" data-scroll-target="#example-haplotype-frequencies"><span class="header-section-number">4.8.1</span> 2.9.1 Example: haplotype frequencies</a></li>
  <li><a href="#simulation-study-of-the-bayesian-paradigm-for-the-binomial" id="toc-simulation-study-of-the-bayesian-paradigm-for-the-binomial" class="nav-link" data-scroll-target="#simulation-study-of-the-bayesian-paradigm-for-the-binomial"><span class="header-section-number">4.8.2</span> 2.9.2 Simulation study of the Bayesian paradigm for the binomial</a></li>
  <li><a href="#the-distribution-of-y" id="toc-the-distribution-of-y" class="nav-link" data-scroll-target="#the-distribution-of-y"><span class="header-section-number">4.8.3</span> 2.9.3 The distribution of \(Y\)</a></li>
  <li><a href="#histogram-of-all-the-ps-such-that-y40-the-posterior" id="toc-histogram-of-all-the-ps-such-that-y40-the-posterior" class="nav-link" data-scroll-target="#histogram-of-all-the-ps-such-that-y40-the-posterior"><span class="header-section-number">4.8.4</span> 2.9.4 Histogram of all the \(p\)s such that \(Y=40\): the posterior</a></li>
  <li><a href="#the-posterior-distribution-is-also-a-beta" id="toc-the-posterior-distribution-is-also-a-beta" class="nav-link" data-scroll-target="#the-posterior-distribution-is-also-a-beta"><span class="header-section-number">4.8.5</span> 2.9.5 The posterior distribution is also a Beta</a></li>
  <li><a href="#suppose-we-had-a-second-series-of-data" id="toc-suppose-we-had-a-second-series-of-data" class="nav-link" data-scroll-target="#suppose-we-had-a-second-series-of-data"><span class="header-section-number">4.8.6</span> 2.9.6 Suppose we had a second series of data</a></li>
  <li><a href="#confidence-statements-for-the-proportion-parameter" id="toc-confidence-statements-for-the-proportion-parameter" class="nav-link" data-scroll-target="#confidence-statements-for-the-proportion-parameter"><span class="header-section-number">4.8.7</span> 2.9.7 Confidence Statements for the proportion parameter</a></li>
  </ul></li>
  <li><a href="#example-occurrence-of-a-nucleotide-pattern-in-a-genome" id="toc-example-occurrence-of-a-nucleotide-pattern-in-a-genome" class="nav-link" data-scroll-target="#example-occurrence-of-a-nucleotide-pattern-in-a-genome"><span class="header-section-number">4.9</span> 2.10 Example: occurrence of a nucleotide pattern in a genome</a>
  <ul class="collapse">
  <li><a href="#modeling-in-the-case-of-dependencies" id="toc-modeling-in-the-case-of-dependencies" class="nav-link" data-scroll-target="#modeling-in-the-case-of-dependencies"><span class="header-section-number">4.9.1</span> 2.10.1 Modeling in the case of dependencies</a></li>
  </ul></li>
  <li><a href="#summary-of-this-chapter" id="toc-summary-of-this-chapter" class="nav-link" data-scroll-target="#summary-of-this-chapter"><span class="header-section-number">4.10</span> 2.11 Summary of this chapter</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">4.11</span> 2.12 Further reading</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">4.12</span> 2.13 Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">2.1 Goals for this chapter</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><a href="imgs/StatDiagram.png"><img src="imgs/StatDiagram.png" class="img-fluid"></a></p>
<p>In the previous chapter, the knowledge of both the generative model and the values of the parameters provided us with probabilities we could use for decision making – for instance, whether we had really found an epitope. In many real situations, neither the generative model nor the parameters are known, and we will need to estimate them using the data we have collected. Statistical modeling works from the data <em>upwards</em> to a model that <em>might</em> plausibly explain the data1. This upward-reasoning step is called statistical <strong>inference</strong>. This chapter will show us some of the distributions and estimation mechanisms that serve as building blocks for inference. Although the examples in this chapter are all parametric (i.e., the statistical models only have a small number of unknown parameters), the principles we discuss will generalize.</p>
<p>1 Even if we have found a model that perfectly explains all our current data, it could always be that reality is more complex. A new set of data lets us conclude that another model is needed, and may include the current model as a special case or approximation.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="In a statistical setting, we start with the data X and use them to estimate the parameters. These estimates are denoted by Greek letters with what we call hats on them, as in \widehat{\theta}."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>In a statistical setting, we start with the data X and use them to estimate the parameters. These estimates are denoted by Greek letters with what we call hats on them, as in \widehat{\theta}.</figcaption>
</figure>
</div>
<p>In a statistical setting, we start with the data \(X\) and use them to <em>estimate</em> the parameters. These estimates are denoted by Greek letters with what we call hats on them, as in \(\).</p>
<p>In this chapter we will:</p>
<ul>
<li><p>See that there is a difference between two subjects that are often confused: “Probability” and “Statistics”.</p></li>
<li><p>Fit data to probability distributions using histograms and other visualization tricks.</p></li>
<li><p>Have a first encounter with an estimating procedure known as <strong>maximum likelihood</strong> through a simulation experiment.</p></li>
<li><p>Make inferences from data for which we have prior information. For this we will use the Bayesian paradigm which will involve new distributions with specially tailored properties. We will use simulations and see how Bayesian estimation differs from simple application of maximum likelihood.</p></li>
<li><p>Use statistical models and estimation to evaluate dependencies in binomial and multinomial distributions.</p></li>
<li><p>Analyse some historically interesting genomic data assembled into tables.</p></li>
<li><p>Make Markov chain models for <strong>dependent</strong> data.</p></li>
<li><p>Do a few concrete applications counting motifs in whole genomes and manipulate special Bioconductor classes dedicated to genomic data.</p></li>
</ul>
<p><a href="imgs/Parameters.png"><img src="imgs/Parameters.png" class="img-fluid"></a></p>
<p><strong>Examples of parameters</strong> : the single parameter \(\) defines a Poisson distribution. The letter \(\) is often used for the mean of the normal. More generally, we use the Greek letter \(\) to designate a generic tuple of parameters necessary to specify a probability model. For instance, in the case of the binomial distribution, \(=(n,p)\) comprises two numbers, a positive integer and a real number between 0 and 1.</p>
<section id="parameters-are-the-key." class="level4" data-number="4.0.0.1">
<h4 data-number="4.0.0.1" class="anchored" data-anchor-id="parameters-are-the-key."><span class="header-section-number">4.0.0.1</span> Parameters are the key.</h4>
<p>We saw in <a href="01-chap.html">Chapter 1</a> that the knowledge of all the parameter values in the epitope example enabled us to use our probability model and test a null hypothesis based on the data we had at hand. We will see different approaches to statistical modeling through some real examples and computer simulations, but let’s start by making a distinction between two situations depending on how much information is available.</p>
</section>
<section id="the-difference-between-statistical-and-probabilistic-models" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="the-difference-between-statistical-and-probabilistic-models"><span class="header-section-number">4.1</span> 2.2 The difference between statistical and probabilistic models</h2>
<p>A probabilistic analysis is possible when we know a good generative model for the randomness in the data, <em>and</em> we know its parameters’ actual values.</p>
<p><a href="imgs/ProbaDiagram.png" title="Figure 2.1: The probabilistic model we obtained in sec-generative. The data are represented as x in green. If we know the true value of \theta, then we can compute the probability of observing x for all possible instances of x, in particular, for an x that we observed."><img src="imgs/ProbaDiagram.png" class="img-fluid"></a></p>
<p>Figure 2.1: The probabilistic model we obtained in <a href="01-chap.html">Chapter 1</a>. The data are represented as \(x\) in green. If we know the true value of \(\), then we can compute the probability of observing \(x\) for all possible instances of \(x\), in particular, for an \(x\) that we observed.</p>
<p>In the epitope example, knowing that false positives occur as Bernoulli(0.01) per position, the number of patient samples assayed and the length of the protein, meant that there were <em>no unknown parameters</em>.</p>
<p>In such a case, we can use mathematical <strong>deduction</strong> to compute the probability of an event as schematized in Figure 2.1. In the epitope examples, we used the Poisson probability as our <strong>null model</strong> with the given parameter \(\). We were able to conclude through mathematical deduction that the chances of seeing a maximum value of 7 or larger was around \(10^{-4}\) and thus that in fact the observed data were highly unlikely under that model (or “null hypothesis”).</p>
<p>Now suppose that we know the number of patients and the length of the proteins (these are given by the experimental design) but not the distribution itself and the false positive rate. Once we observe data, we need to go <em>up</em> from the data to estimate both a probability model \(F\) (Poisson, normal, binomial) and eventually the missing parameter(s) for that model. This is the type of statistical <strong>inference</strong> we will explain in this chapter.</p>
</section>
<section id="a-simple-example-of-statistical-modeling" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="a-simple-example-of-statistical-modeling"><span class="header-section-number">4.2</span> 2.3 A simple example of statistical modeling</h2>
<section id="start-with-the-data" class="level4" data-number="4.2.0.1">
<h4 data-number="4.2.0.1" class="anchored" data-anchor-id="start-with-the-data"><span class="header-section-number">4.2.0.1</span> Start with the data</h4>
<p>There are two parts to the modeling procedure. First we need a reasonable probability <em>distribution</em> to model the data generation process. As we saw in <a href="01-chap.html">Chapter 1</a>, discrete count data may be modeled by simple probability distributions such as binomial, multinomial or Poisson distributions. The normal distribution, or bell shaped curve, is often a good model for continuous measurements. Distributions can also be more complicated mixtures of these elementary ones (more on this in <a href="04-chap.html">Chapter 4</a>).</p>
<p>Let’s revisit the epitope example from the previous chapter, starting without the tricky outlier.</p>
<pre><code>load("../data/e100.RData")
e99 = e100[-which.max(e100)]__</code></pre>
</section>
<section id="goodness-of-fit-visual-evaluation" class="level4" data-number="4.2.0.2">
<h4 data-number="4.2.0.2" class="anchored" data-anchor-id="goodness-of-fit-visual-evaluation"><span class="header-section-number">4.2.0.2</span> Goodness-of-fit : visual evaluation</h4>
<p>Our first step is to find a fit from candidate distributions; this requires consulting graphical and quantitative goodness-of-fit plots. For discrete data, we can plot a barplot of frequencies (for continuous data, we would look at the histogram) as in Figure 2.2.</p>
<pre><code>barplot(table(e99), space = 0.8, col = "chartreuse4")__</code></pre>
<p><a href="02-chap_files/figure- html/fig-twopoisson-1.png" title="Figure 2.2: The observed distribution of the epitope data without the outlier."><img src="02-chap_files/figure-html/fig-twopoisson-1.png" class="img-fluid"></a></p>
<p>Figure 2.2: The observed distribution of the epitope data without the outlier.</p>
<p>However, it is hard to decide which theoretical distribution fits the data best without using a comparison. One visual <strong>goodness-of-fit</strong> diagram is known as the <strong>rootogram</strong> (<a href="16-chap.html#ref-Tukey:1988">Cleveland 1988</a>); it hangs the bars with the observed counts from the theoretical red points. If the counts correspond exactly to their theoretical values, the bottom of the boxes will align exactly with the horizontal axis.</p>
<pre><code>library("vcd")
gf1 = goodfit( e99, "poisson")
rootogram(gf1, xlab = "", rect_gp = gpar(fill = "chartreuse4"))__</code></pre>
<p><a href="02-chap_files/figure- html/fig-stat-rooto-1.png" title="Figure 2.3: Rootogram showing the square root of the theoretical values as red dots and the square root of the observed frequencies as drop down rectangles. (We’ll see a bit below how the goodfit function decided which \lambda to use.)"><img src="02-chap_files/figure-html/fig-stat-rooto-1.png" class="img-fluid"></a></p>
<p>Figure 2.3: Rootogram showing the square root of the theoretical values as red dots and the square root of the observed frequencies as drop down rectangles. (We’ll see a bit below how the <code>goodfit</code> function decided which \(\) to use.)</p>
<p>__</p>
<p>Question 2.1</p>
<p>To calibrate what such a plot looks like with a known Poisson variable, use <code>rpois</code> with \(\) = 0.05 to generate 100 Poisson distributed numbers and draw their rootogram.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>simp = rpois(100, lambda = 0.05)
gf2 = goodfit(simp, "poisson")
rootogram(gf2, xlab = "")__</code></pre>
<p>We see that the rootogram for <code>e99</code> seems to fit the Poisson model reasonably well. But remember, to make this happen we removed the outlier. The Poisson is completely determined by one parameter, often called the Poisson mean \(\). In most cases where we can guess the data follows a Poisson distribution, we will need to estimate the Poisson parameter from the data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="The parameter is called the Poisson mean because it is the mean of the theoretical distribution and, as it turns out, is estimated by the sample mean. This overloading of the word is confusing to everyone."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>The parameter is called the Poisson mean because it is the mean of the theoretical distribution and, as it turns out, is estimated by the sample mean. This overloading of the word is confusing to everyone.</figcaption>
</figure>
</div>
<p>The parameter is called the Poisson mean because it is the mean of the theoretical distribution <em>and</em> , as it turns out, is estimated by the sample mean. This overloading of the word is confusing to everyone.</p>
<p>The most common way of estimating \(\) is to choose the value \(\) that makes the observed data the most likely. This is called the <strong>maximum likelihood estimator</strong> (<a href="16-chap.html#ref-Rice:2007">Rice 2006, chap. 8</a>, Section 5), often abbreviated <strong>MLE</strong>. We will illustrate this rather paradoxical idea in the next section.</p>
<p>Although we above took out the extreme observation before taking a guess at the probability distribution, we are going to return to the data with it for the rest of our analysis. In practice we would not know whether there is an outlier, and which data point(s) it is / they are. The effect of leaving it in is to make our estimate of the mean higher. In turns this would make it more likely that we’d observe a value of 7 under the null model, resulting in a larger p-value. So, if the resulting p-value is small even with the outlier included, we are assured that our analysis is up to something real. We call such a tactic being <strong>conservative</strong> : we err on the side of caution, of not detecting something.</p>
</section>
<section id="estimating-the-parameter-of-the-poisson-distribution" class="level4" data-number="4.2.0.3">
<h4 data-number="4.2.0.3" class="anchored" data-anchor-id="estimating-the-parameter-of-the-poisson-distribution"><span class="header-section-number">4.2.0.3</span> Estimating the parameter of the Poisson distribution</h4>
<p>What value for the Poisson mean makes the data the most probable? In a first step, we tally the outcomes.</p>
<pre><code>table(e100)__


e100
 0  1  2  7 
58 34  7  1 </code></pre>
<p>Then we are going to try out different values for the Poisson mean and see which one gives the best fit to our data. If the mean \(\) of the Poisson distribution were 3, the counts would look something like this:</p>
<pre><code>table(rpois(100, 3))__


 0  1  2  3  4  5  6  7 
 4 12 23 24 14 16  4  3 </code></pre>
<p>which has many more 2’s and 3’s than we see in our data. So we see that \(\) is unlikely to have produced our data, as the counts do not match up so well.</p>
<p>__</p>
<p>Question 2.2</p>
<p>Repeat this simulation with different values of \(\). Can you find one that gives counts close to the observed ones just by trial and error?</p>
<p>So we could try out many possible values and proceed by brute force. However, we’ll do something more elegant and use a little mathematics to see which value maximizes the probability of observing our data. Let’s calculate the probability of seeing the data if the value of the Poisson parameter is \(m\). Since we suppose the data derive from independent draws, this probability is simply the product of individual probabilities:</p>
<p>\[<span class="math display">\[\begin{equation*} \begin{aligned} P(58 \times 0, 34 \times 1, 7 \times 2,
\text{one }7 \;|\; \text{data are Poisson}(m)) = P(0)^{58}\times
P(1)^{34}\times P(2)^{7}\times P(7)^{1}.\end{aligned} \end{equation*}\]</span>\]</p>
<p>For \(m=3\) we can compute this2.</p>
<p>2 Note how we here use R’s vectorization: the call to <code>dpois</code> returns four values, corresponding to the four different numbers. We then take these to the powers of 58, 34, 7 and 1, respectively, using the <code>^</code> operator, resulting again in four values. Finally, we collapse them into one number, the product, with the <code>prod</code> function.</p>
<pre><code>prod(dpois(c(0, 1, 2, 7), lambda = 3) ^ (c(58, 34, 7, 1)))__


[1] 1.392143e-110</code></pre>
<p>__</p>
<p>Question 2.3</p>
<p>Compute the probability as above for \(m=0,1,2\). Does \(m\) have to be integer? Try computing the probability for \(m=0.4\) for example.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>prod(dpois(c(0, 1, 2, 7), lambda = 0.4) ^ (c(58, 34, 7, 1)))__


[1] 8.5483e-46</code></pre>
<p>This probability is the <strong>likelihood function</strong> of \(\), given the data, and we write it</p>
<p>Here \(L\) stands for likelihood and \(f(k)=e^{-} ,^k,/,k!\), the Poisson probability we saw earlier.</p>
<p>\[ L(,,x=(k_1,k_2,k_3,…))=_{i=1}^{100}f(k_i) \]</p>
<p>Instead of working with multiplications of a hundred small numbers, it is convenient3 to take the logarithm. Since the logarithm is strictly increasing, if there is a point where the logarithm achieves its maximum within an interval it will also be the maximum for the probability.</p>
<p>3 That’s usually true both for pencil and paper and for computer calculations.</p>
<p>4 Here we again use R’s vector syntax that allows us to write the computation without an explicit loop over the data points. Compared to the code above, here we call <code>dpois</code> on each of the 100 data points, rather than tabulating <code>data</code> with the <code>table</code> function before calling <code>dpois</code> only on the distinct values. This is a simple example for alternative solutions whose results are equivalent, but may differ in how easy it is to read the code or how long it takes to execute.</p>
<p>Let’s start with a computational illustration. We compute the likelihood for many different values of the Poisson parameter. To do this we need to write a small function that computes the probability of the data for different values4.</p>
<pre><code>loglikelihood  =  function(lambda, data = e100) {
  sum(log(dpois(data, lambda)))
}__</code></pre>
<p>Now we can compute the likelihood for a whole series of <code>lambda</code> values from 0.05 to 0.95 (Figure 2.4).</p>
<pre><code>lambdas = seq(0.05, 0.95, length = 100)
loglik = vapply(lambdas, loglikelihood, numeric(1))
plot(lambdas, loglik, type = "l", col = "red", ylab = "", lwd = 2,
     xlab = expression(lambda))
m0 = mean(e100)
abline(v = m0, col = "blue", lwd = 2)
abline(h = loglikelihood(m0), col = "purple", lwd = 2)
m0 __


[1] 0.55</code></pre>
<p><a href="02-chap_files/figure- html/fig-poislikel-1-1.png" title="Figure 2.4: The red curve is the log-likelihood function. The vertical line shows the value of m (the mean) and the horizontal line the log-likelihood of m. It looks like m maximizes the likelihood."><img src="02-chap_files/figure-html/fig-poislikel-1-1.png" class="img-fluid"></a></p>
<p>Figure 2.4: The red curve is the log-likelihood function. The vertical line shows the value of <code>m</code> (the mean) and the horizontal line the log-likelihood of <code>m</code>. It looks like <code>m</code> maximizes the likelihood.</p>
<p>__</p>
<p>Question 2.4</p>
<p>What does the <code>vapply</code> function do in the above code? Hint: check its manual page.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p><code>vapply</code> takes its first argument, the vector <code>lambdas</code> in this case, and iteratively applies the function <code>loglikelihood</code> (its second argument) to each of the vector elements. As a result, it returns a vector of the results. The function also needs a third argument, <code>numeric(1)</code> in this case, that specifies what type of value each individual call to <code>loglikelihood</code> is supposed to return: a single number. (In general, it could happen that the function sometimes returns something else, say, a character string, or two numbers; in that case it would not be possible to assemble the overall results into a coherent vector, and <code>vapply</code> would complain.)</p>
<p>In fact there is a shortcut: the function <code>goodfit</code>.</p>
<pre><code>gf  =  goodfit(e100, "poisson")
names(gf)__


[1] "observed" "count"    "fitted"   "type"     "method"   "df"       "par"     


gf$par __


$lambda
[1] 0.55</code></pre>
<p>The output of <code>goodfit</code> is a composite object called a list. One of its components is called <code>par</code> and contains the value(s) of the fitted parameter(s) for the distribution studied. In this case it’s only one number, the estimate of \(\).</p>
<p>__</p>
<p>Question 2.5</p>
<p>What are the other components of the output from the <code>goodfit</code> function?</p>
<p>__</p>
<p>Task</p>
<p>Compare the value of <code>m</code> to the value that we used previously for \(\), 0.5. Redo the modeling that we did in <a href="01-chap.html">Chapter 1</a> with <code>m</code> instead of 0.5.</p>
</section>
<section id="classical-statistics-for-classical-data" class="level3" data-number="4.2.1">
<h3 data-number="4.2.1" class="anchored" data-anchor-id="classical-statistics-for-classical-data"><span class="header-section-number">4.2.1</span> 2.3.1 Classical statistics for classical data</h3>
<p>Here is a formal proof of our computational finding that the sample mean maximizes the (log-)likelihood.</p>
<p>\[ <span class="math display">\[\begin{align} \log L(\lambda, x) &amp;= \sum_{i=1}^{100} - \lambda +
k_i\log\lambda - \log(k_i!) \\\ &amp;= -100\lambda +
\log\lambda\left(\sum_{i=1}^{100}k_i\right) + \text{const.} \end{align}\]</span> \]</p>
<p>We use the catch-all “const.” for terms that do not depend on \(\) (although they do depend on \(x\), i.e., on the \(k_i\)). To find the \(\) that maximizes this, we compute the derivative in \(\) and set it to zero.</p>
<p>\[ <span class="math display">\[\begin{align} \frac{d}{d\lambda}\log L &amp;= -100 + \frac{1}{\lambda}
\sum_{i=1}^{100}k_i \stackrel{?}{=}0 \\\ \lambda &amp;= \frac{1}{100}
\sum_{i=1}^{100}k_i = \bar{k} \end{align}\]</span> \]</p>
<p>You have just seen the first steps of a <em>statistical approach</em> , starting `from the ground up’ (from the data) to infer the model parameter(s): this is statistical <em>estimation</em> of a parameter from data. Another important component will be choosing which family of distributions we use to model our data; that part is done by evaluating the <em>goodness of fit</em>. We will encounter this later.</p>
<p>In the classical <em>statistical testing</em> framework, we consider one single model, that we call the <em>null model</em> , for the data. The null model formulates an “uninteresting” baseline, such as that all observations come from the same random distribution regardless of which group or treatment they are from. We then test whether there is something more interesting going on by computing the probability that the data are compatible with that model. Often, this is the best we can do, since we do not know in sufficient detail what the “interesting”, non-null or alternative model should be. In other situations, we have two competing models that we can compare, as we will see later.</p>
<p>__</p>
<p>Question 2.6</p>
<p>What is the value of modeling with a known distribution? For instance, why is it interesting to know a variable has a Poisson distribution ?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Models are concise but expressive representations of the data generating process. For the Poisson for instance, knowing one number allows us to know everything about the distribution, including, as we saw earlier, the probabilities of extreme or rare events.</p>
<p>Another useful direction is <strong>regression</strong>. We may be interested in knowing how our count-based response variable (e.g., the result of counting sequencing reads) depends on a continuous covariate, say, temperature or nutrient concentration. You may already have encountered linear regression, where our model is that the response variable \(y\) depends on the covariate \(x\) via the equation \(y = ax+b + e\), with parameters \(a\) and \(b\) (that we need to estimate), and with residuals \(e\) whose probability model is a normal distribution (whose variance we usually also need to estimate). For count data the same type of regression model is possible, although the probability distribution for the residuals then needs to be non-normal. In that case we use the <strong>generalized linear models</strong> framework. We will see examples when studying RNA-Seq in <a href="08-chap.html">Chapter 8</a> and another type of next generation sequencing data, 16S rRNA data, in <a href="09-chap.html">Chapter 9</a>.</p>
<p>Knowing that our probability model involves a Poisson, binomial, multinomial distribution or another parametric family will enable us to have quick answers to questions about the parameters of the model and compute quantities such as p-values and confidence intervals.</p>
</section>
</section>
<section id="binomial-distributions-and-maximum-likelihood" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="binomial-distributions-and-maximum-likelihood"><span class="header-section-number">4.3</span> 2.4 Binomial distributions and maximum likelihood</h2>
<p>In a binomial distribution there are two parameters: the number of trials \(n\), which is typically known, and the probability \(p\) of seeing a 1 in a trial. This probability is often unknown.</p>
<section id="an-example" class="level3" data-number="4.3.1">
<h3 data-number="4.3.1" class="anchored" data-anchor-id="an-example"><span class="header-section-number">4.3.1</span> 2.4.1 An example</h3>
<p>Suppose we take a sample of \(n=120\) males and test them for red-green colorblindness. We can code the data as 0 if the subject is not colorblind and 1 if he is. We summarize the data by the table:</p>
<pre><code>table(cb)__


cb
  0   1 
110  10 </code></pre>
<p>__</p>
<p>Question 2.7</p>
<p>Which value of \(p\) is the most likely given these data?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>\(=\).</p>
<pre><code>mean(cb)__


[1] 0.08333333</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="However, be careful: sometimes, maximum likelihood estimates are harder to guess and to compute, as well as being much less intuitive (see Exercise imp-models-mlmax)."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>However, be careful: sometimes, maximum likelihood estimates are harder to guess and to compute, as well as being much less intuitive (see Exercise&nbsp;imp- models-mlmax).</figcaption>
</figure>
</div>
<p>However, be careful: sometimes, maximum likelihood estimates are harder to guess and to compute, as well as being much less intuitive (see Exercise 2.2).</p>
<p>In this special case, your intuition may give you the estimate \(=\), which turns out to be the maximum likelihood estimate. We put a hat over the letter to remind us that this is not (necessarily) the underlying true value, but an estimate we make from the data.</p>
<p>As before in the case of the Poisson, if we compute the likelihood for many possible \(p\), we can plot it and see where its maximum falls (Figure 2.5).</p>
<pre><code>probs  =  seq(0, 0.3, by = 0.005)
likelihood = dbinom(sum(cb), prob = probs, size = length(cb))
plot(probs, likelihood, pch = 16, xlab = "probability of success",
       ylab = "likelihood", cex=0.6)
probs[which.max(likelihood)]__


[1] 0.085</code></pre>
<p><a href="02-chap_files/figure- html/fig-likely1-1-1.png" title="Figure 2.5: Plot of the likelihood as a function of the probabilities. The likelihood is a function on [0, 1]. Here we have zoomed into the range of [0, 0.3], as the likelihood is practically zero for larger values of p."><img src="02-chap_files/figure-html/fig-likely1-1-1.png" class="img-fluid"></a></p>
<p>Figure 2.5: Plot of the likelihood as a function of the probabilities. The likelihood is a function on \([0, 1]\). Here we have zoomed into the range of \([0, 0.3]\), as the likelihood is practically zero for larger values of \(p\).</p>
<p>Note: 0.085 is not exactly the value we expected \(()\), and that is because the set of values that we tried (in <code>probs</code>) did not include the exact value of \(\), so we obtained the next best one. We could use numeric optimisation methods to overcome that.</p>
</section>
<section id="likelihood-for-the-binomial-distribution" class="level3" data-number="4.3.2">
<h3 data-number="4.3.2" class="anchored" data-anchor-id="likelihood-for-the-binomial-distribution"><span class="header-section-number">4.3.2</span> 2.4.2 Likelihood for the binomial distribution</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="One can come up with different criteria than maximum likelihood, which lead to other estimators. They all carry hats. We’ll see other examples in sec-mixtures."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>One can come up with different criteria than maximum likelihood, which lead to other estimators. They all carry hats. We’ll see other examples in sec- mixtures.</figcaption>
</figure>
</div>
<p>One can come up with different criteria than maximum likelihood, which lead to other estimators. They all carry hats. We’ll see other examples in <a href="04-chap.html">Chapter 4</a>.</p>
<p>The likelihood and the probability are the same mathematical function, only interpreted in different ways – in one case, the function tells us how probable it is to see a particular set of values of the data, given the parameters; in the other case, we consider the data as given, and ask for the parameter value(s) that likely generated these data. Suppose \(n=300\), and we observe \(y=40\) successes. Then, for the binomial distribution:</p>
<p>\[ f(p,|,n,y) = f(y,|,n,p)={n y} , p^y , (1-p)^{(n-y)}. \]</p>
<p>Again, it is more convenient to work with the logarithm of the likelihood,</p>
<p>\[ f(p |y) = + y(p) + (n-y)(1-p). \]</p>
<p>Here’s a function we can use to calculate it5,</p>
<p>5 In practice, we would try to avoid explicitly computing <code>choose(n, y)</code>, since it can be a very large number that tests the limits of our computer’s floating point arithmetic (for <code>n=300</code> and <code>y=40</code>, it is around 9.8e+49). One could approximate the term using Stirling’s formula, or indeed ignore it, as it is only an additive offset independent of \(p\) that does not impact the maximization.</p>
<pre><code>loglikelihood = function(p, n = 300, y = 40) {
  log(choose(n, y)) + y * log(p) + (n - y) * log(1 - p)
}__</code></pre>
<p>which we plot for the range of \(p\) from 0 to 1 (Figure 2.6).</p>
<pre><code>p_seq = seq(0, 1, by = 0.001)
plot(p_seq, loglikelihood(p_seq), xlab = "p", ylab = "log f(p|y)", type = "l")__</code></pre>
<p><a href="02-chap_files/figure-html/fig-loglikelihood-1-1.png &quot;Figure 2.6: Plot of the log likelihood function for n=300 and y=40.&quot;"><img src="02-chap_files/figure-html/fig- loglikelihood-1-1.png" class="img-fluid"></a></p>
<p>Figure 2.6: Plot of the log likelihood function for \(n=300\) and \(y=40\).</p>
<p>The maximum lies at 40/300 = 0.1333… , consistent with intuition, but we see that other values of \(p\) are almost equally likely, as the function is quite flat around the maximum. We will see in a later section how Bayesian methods enable us to work with a range of values for \(p\) instead of just picking a single maximum.</p>
</section>
</section>
<section id="more-boxesmultinomial-data" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="more-boxesmultinomial-data"><span class="header-section-number">4.4</span> 2.5 More boxes:multinomial data</h2>
<section id="dna-count-modeling-base-pairs" class="level3" data-number="4.4.1">
<h3 data-number="4.4.1" class="anchored" data-anchor-id="dna-count-modeling-base-pairs"><span class="header-section-number">4.4.1</span> 2.5.1 DNA count modeling: base pairs</h3>
<p>There are four basic molecules of DNA: A - adenine, C - cytosine, G - guanine, T - thymine. The nucleotides are classified into 2 groups: purines (A and G) and pyrimidines (C and T). The binomial would work as a model for the purine/pyrimidine groupings but not if we want to use A, C, G, T; for that we need the multinomial model from <a href="01-chap.html#sec-generative- multinomial">Section 1.4</a>. Let’s look at noticeable patterns that occur in these frequencies.</p>
</section>
<section id="nucleotide-bias" class="level3" data-number="4.4.2">
<h3 data-number="4.4.2" class="anchored" data-anchor-id="nucleotide-bias"><span class="header-section-number">4.4.2</span> 2.5.2 Nucleotide bias</h3>
<p>This section combines estimation and testing by simulation in a real example. Data from one strand of DNA for the genes of <em>Staphylococcus aureus</em> bacterium are available in a <em>fasta</em> file <code>staphsequence.ffn.txt</code>, which we can read with a function from the Bioconductor package <strong><a href="https://bioconductor.org/packages/Biostrings/">Biostrings</a></strong>.</p>
<pre><code>library("Biostrings")
staph = readDNAStringSet("../data/staphsequence.ffn.txt", "fasta")__</code></pre>
<p>Let’s look at the first gene:</p>
<pre><code>staph[1]__


DNAStringSet object of length 1:
    width seq                                               names               
[1]  1362 ATGTCGGAAAAAGAAATTTGGGA...AAAAAGAAATAAGAAATGTATAA lcl|NC_002952.2_c...


letterFrequency(staph[[1]], letters = "ACGT", OR = 0)__


  A   C   G   T 
522 219 229 392 </code></pre>
<p>__</p>
<p>Question 2.8</p>
<p>Why did we use double square brackets in the second line?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>The double square brackets <code>[[i]]</code> extract the sequence of the <code>i</code>-th gene as a <em>DNAString</em> , as opposed to the pair of single brackets <code>[i]</code>, which return a <em>DNAStringSet</em> with just a single <em>DNAString</em> in it. If you look at the length of <code>staph[1]</code>, it is 1, whereas <code>staph[[1]]</code> has length 1362.</p>
<p>__</p>
<p>Question 2.9</p>
<p>Following a similar procedure as in <a href="01-chap.html#imp- generative-genomefrequency">Exercise 1.8</a>, test whether the nucleotides are equally distributed across the four nucleotides for this first gene.</p>
<p>Due to their different physical properties, evolutionary selection can act on the nucleotide frequencies. So we can ask whether, say, the first ten genes from these data come from the same multinomial. We do not have a prior reference, we just want to decide whether the nucleotides occur in the same proportions in the first 10 genes. If not, this would provide us with evidence for varying selective pressure on these ten genes.</p>
<pre><code>letterFrq = vapply(staph, letterFrequency, FUN.VALUE = numeric(4),
         letters = "ACGT", OR = 0)
colnames(letterFrq) = paste0("gene", seq(along = staph))
tab10 = letterFrq[, 1:10]
computeProportions = function(x) { x/sum(x) }
prop10 = apply(tab10, 2, computeProportions)
round(prop10, digits = 2)__


  gene1 gene2 gene3 gene4 gene5 gene6 gene7 gene8 gene9 gene10
A  0.38  0.36  0.35  0.37  0.35  0.33  0.33  0.34  0.38   0.27
C  0.16  0.16  0.13  0.15  0.15  0.15  0.16  0.16  0.14   0.16
G  0.17  0.17  0.23  0.19  0.22  0.22  0.20  0.21  0.20   0.20
T  0.29  0.31  0.30  0.29  0.27  0.30  0.30  0.29  0.28   0.36


p0 = rowMeans(prop10)
p0 __


        A         C         G         T 
0.3470531 0.1518313 0.2011442 0.2999714 </code></pre>
<p>So let’s suppose <code>p0</code> is the vector of multinomial probabilities for all the ten genes and use a Monte Carlo simulation to test whether the departures between the observed letter frequencies and expected values under this supposition are within a plausible range.</p>
<p>We compute the expected counts by taking the <code>outer</code> product of the vector of probabilities p0 with the sums of nucleotide counts from each of the 10 columns, <code>cs</code>.</p>
<pre><code>cs = colSums(tab10)
cs __


 gene1  gene2  gene3  gene4  gene5  gene6  gene7  gene8  gene9 gene10 
  1362   1134    246   1113   1932   2661    831   1515   1287    696 


expectedtab10 = outer(p0, cs, FUN = "*")
round(expectedtab10)__


  gene1 gene2 gene3 gene4 gene5 gene6 gene7 gene8 gene9 gene10
A   473   394    85   386   671   924   288   526   447    242
C   207   172    37   169   293   404   126   230   195    106
G   274   228    49   224   389   535   167   305   259    140
T   409   340    74   334   580   798   249   454   386    209</code></pre>
<p>We can now create a random table with the correct column sums using the <code>rmultinom</code> function. This table is generated according to the null hypothesis that the true proportions are given by <code>p0</code>.</p>
<pre><code>randomtab10 = sapply(cs, function(s) { rmultinom(1, s, p0) } )
all(colSums(randomtab10) == cs)__


[1] TRUE</code></pre>
<p>Now we repeat this B = 1000 times. For each table we compute our test statistic from <a href="01-chap.html#sec-generative-SimulatingForPower">Section 1.4.1</a> in <a href="01-chap.html">Chapter 1</a> (the function <code>stat</code>) and store the results in the vector <code>simulstat</code>. Together, these values constitute our null distribution, as they were generated under the null hypothesis that <code>p0</code> is the vector of multinomial proportions for each of the 10 genes.</p>
<pre><code>stat = function(obsvd, exptd) {
   sum((obsvd - exptd)^2 / exptd)
}
B = 1000
simulstat = replicate(B, {
  randomtab10 = sapply(cs, function(s) { rmultinom(1, s, p0) })
  stat(randomtab10, expectedtab10)
})
S1 = stat(tab10, expectedtab10)
sum(simulstat &gt;= S1)__


[1] 0


hist(simulstat, col = "lavender", breaks = seq(0, 75, length.out=50))
abline(v = S1, col = "red")
abline(v = quantile(simulstat, probs = c(0.95, 0.99)),
       col = c("darkgreen", "blue"), lty = 2)__</code></pre>
<p><a href="02-chap_files/figure- html/fig-quant12-1-1.png" title="Figure 2.7: Histogram of simulstat. The value of S1 is marked by the vertical red line, those of the 0.95 and 0.99 quantiles (see next section) by the dotted lines."><img src="02-chap_files/figure-html/fig-quant12-1-1.png" class="img-fluid"></a></p>
<p>Figure 2.7: Histogram of <code>simulstat</code>. The value of <code>S1</code> is marked by the vertical red line, those of the 0.95 and 0.99 quantiles (see next section) by the dotted lines.</p>
<p>The histogram is shown in Figure 2.7. We see that the probability of seeing a value as large as <code>S1</code>=70.1 is very small under the <em>null model</em>. It happened 0 times in our 1000 simulations that a value as big as <code>S1</code> occurred. Thus the ten genes do not seem to come from the same multinomial model.</p>
</section>
</section>
<section id="the-2-distribution" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="the-2-distribution"><span class="header-section-number">4.5</span> 2.6 The \(^2\) distribution</h2>
<p>In fact, we could have used statistical theory to come to the same conclusion without running these simulations. The theoretical distribution of the <code>simulstat</code> statistic is called the \(^2\) (chi-squared) distribution6 with parameter 30 (\(=10(4-1)\)). We can use this for computing the probability of having a value as large as <code>S1</code> \(=\) 70.1. As we just saw above, small probabilities are difficult to compute by Monte Carlo: the granularity of the computation is \(1/B\), so we cannot estimate any probabilities smaller than that, and in fact the uncertainty of the estimate is larger. So if any theory is applicable, that tends to be useful. We can check how well theory and simulation match up in our case using another visual goodness-of-fit tool: the <strong>quantile-quantile</strong> (<strong>QQ</strong>) plot. When comparing two distributions, whether from two different samples or from one sample versus a theoretical model, just looking at histograms is not informative enough. We use a method based on the quantiles of each of the distributions.</p>
<p>6 Strictly speaking, the distribution of <code>simulstat</code> is approximately described by a \(^2\) distribution; the approximation is particularly good if the counts in the table are large.</p>
<section id="intermezzo-quantiles-and-the-quantile-quantile-plot" class="level3" data-number="4.5.1">
<h3 data-number="4.5.1" class="anchored" data-anchor-id="intermezzo-quantiles-and-the-quantile-quantile-plot"><span class="header-section-number">4.5.1</span> 2.6.1 Intermezzo: quantiles and the quantile-quantile plot</h3>
<p>In the previous chapter, we ordered the 100 sample values \(x_{(1)},x_{(2)},…,x_{(100)}\). Say we want the 22nd percentile. We can take any value between the 22nd and the 23rd value, i.e., any value that fulfills \(x_{(22)} c_{0.22} &lt; x_{(23)}\) is acceptable as a 0.22 <strong>quantile</strong> (\(c_{0.22}\)). In other words, \(c_{0.22}\) is defined by</p>
<p>\[ = 0.22. \]</p>
<p>In <a href="03-chap.html#sec-graphics-ecdf">Section 3.6.7</a>, we’ll introduce the <strong>empirical cumulative distribution</strong> function (<strong>ECDF</strong>) \(\), and we’ll see that our definition of \(c_{0.22}\) can also be written as \(<em>n(c</em>{0.22}) = 0.22\). In Figure 2.7, our histogram of the distribution of <code>simulstat</code>, the quantiles \(c_{0.95}\) and \(c_{0.99}\) are also shown.</p>
<p>__</p>
<p>Question 2.10</p>
<ol type="1">
<li><p>Compare the <code>simulstat</code> values and 1000 randomly generated \(^2_{30}\) random numbers by displaying them in histograms with 50 bins each.</p></li>
<li><p>Compute the quantiles of the <code>simulstat</code> values and compare them to those of the \(_{30}^2\) distribution. Hint:</p></li>
</ol>
<pre><code>qs = ppoints(100)
quantile(simulstat, qs)
quantile(qchisq(qs, df = 30), qs)__</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="A name collision occurs here. Statisticians call the summary statistic we just computed as simulstat (sum of squares of weighted differences), the chi-squared or \chi^2 statistic. The theoretical distribution \chi^2_\nu is a distribution in its own right, with a parameter \nu called the degrees of freedom. When reading about the chi-squared or \chi^2, you will need to pay attention to the context to see which meaning is appropriate."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>A name collision occurs here. Statisticians call the summary statistic we just computed as simulstat (sum of squares of weighted differences), the chi-squared or \chi^2 statistic. The theoretical distribution \chi^2_\nu is a distribution in its own right, with a parameter \nu called the degrees of freedom. When reading about the chi-squared or \chi^2, you will need to pay attention to the context to see which meaning is appropriate.</figcaption>
</figure>
</div>
<p>A name collision occurs here. Statisticians call the summary statistic we just computed as <code>simulstat</code> (sum of squares of weighted differences), the <strong>chi- squared</strong> or \(^2\) <em>statistic</em>. The theoretical <em>distribution</em> \(^2_\) is a distribution in its own right, with a parameter \(\) called the degrees of freedom. When reading about the chi-squared or \(^2\), you will need to pay attention to the context to see which meaning is appropriate.</p>
<p>__</p>
<p>Question 2.11</p>
<p>Do you know another name for the 0.5 quantile?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>The median.</p>
<p>__</p>
<p>Question 2.12</p>
<p>In the above definition, we were a little vague on how the quantile is defined in general, i.e., not just for 0.22. How is the quantile computed for any number between 0 and 1, including ones that are not multiples of \(1/n\)?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Check the manual page of the <code>quantile</code> function and its argument named <code>type</code>.</p>
<p>Now that we have an idea what quantiles are, we can do the quantile-quantile plot. We plot the quantiles of the <code>simulstat</code> values, which we simulated under the null hypothesis, against the theoretical null distribution \(^2_{30}\) (Figure 2.8):</p>
<pre><code>qqplot(qchisq(ppoints(B), df = 30), simulstat, main = "",
  xlab = expression(chi[nu==30]^2), asp = 1, cex = 0.5, pch = 16)
abline(a = 0, b = 1, col = "red")__</code></pre>
<p><a href="02-chap_files/figure- html/fig-qqplot3-1-1.png" title="Figure 2.8: Our simulated statistic’s distribution compared to \chi_{30}^2 using a quantile-quantile (QQ) plot, which shows the theoretical quantiles for the \chi^2_{30} distribution on the horizontal axis and the sampled ones on the vertical axis."><img src="02-chap_files/figure-html/fig-qqplot3-1-1.png" class="img-fluid"></a></p>
<p>Figure 2.8: Our simulated statistic’s distribution compared to \(<em>{30}^2\) using a quantile-quantile (QQ) plot, which shows the theoretical <strong>quantiles</strong> for the \(^2</em>{30}\) distribution on the horizontal axis and the sampled ones on the vertical axis.</p>
<p>Having convinced ourselves that <code>simulstat</code> is well described by a \(^2_{30}\) distribution, we can use that to compute our p-value, i.e., the probability that under the null hypothesis (counts are distributed as multinomial with probabilities \(p_{} = 0.35\), \(p_{} = 0.15\), \(p_{} = 0.2\), \(p_{} = 0.3\)) we observe a value as high as <code>S1</code>=70.1:</p>
<pre><code>1 - pchisq(S1, df = 30)__


[1] 4.74342e-05</code></pre>
<p>With such a small p-value, the null hypothesis seems improbable. Note how this computation did not require the 1000 simulations and was faster.</p>
</section>
</section>
<section id="chargaffs-rule" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="chargaffs-rule"><span class="header-section-number">4.6</span> 2.7 Chargaff’s Rule</h2>
<p>The most important pattern in the nucleotide frequencies was discovered by Chargaff (<a href="16-chap.html#ref-Chargaff">Elson and Chargaff 1952</a>).</p>
<p><a href="imgs/ChargaffColdSpring_web.jpg"><img src="imgs/ChargaffColdSpring_web.jpg" class="img-fluid"></a></p>
<p>Long before DNA sequencing was available, using the weight of the molecules, he asked whether the nucleotides occurred at equal frequencies. He called this the tetranucleotide hypothesis. We would translate that into asking whether \(p_{} = p_{} = p_{} = p_{}\).</p>
<p>Unfortunately, Chargaff only published the <em>percentages</em> of the mass present in different organisms for each of the nucleotides, not the measurements themselves.</p>
<pre><code>load("../data/ChargaffTable.RData")
ChargaffTable __


                  A    T    C    G
Human-Thymus   30.9 29.4 19.9 19.8
Mycobac.Tuber  15.1 14.6 34.9 35.4
Chicken-Eryth. 28.8 29.2 20.5 21.5
Sheep-liver    29.3 29.3 20.5 20.7
Sea Urchin     32.8 32.1 17.7 17.3
Wheat          27.3 27.1 22.7 22.8
Yeast          31.3 32.9 18.7 17.1
E.coli         24.7 23.6 26.0 25.7</code></pre>
<p><a href="02-chap_files/figure- html/fig-ChargaffBars-1.png" title="Figure 2.9: Barplots for the different rows in ChargaffTable. Can you spot the pattern?"><img src="02-chap_files/figure-html/fig-ChargaffBars-1.png" class="img-fluid"></a></p>
<p>Figure 2.9: Barplots for the different rows in <code>ChargaffTable</code>. Can you spot the pattern?</p>
<p>__</p>
<p>Question 2.13</p>
<ul>
<li><p>Do these data seem to come from equally likely multinomial categories?</p></li>
<li><p>Can you suggest an alternative pattern? puted from simulations u</p></li>
<li><p>Can you do a quantitative analysis of the pattern, perhaps inspired by the simulations above?</p></li>
</ul>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Chargaff <em>saw</em> the answer to this question and postulated a pattern called <em>base pairing</em> , which ensured a perfect match of the amount of adenine (A) in the DNA of an organism to the amount of thymine (T). Similarly, whatever the amount of guanine (G), the amount of cytosine (C) would be the same. This is now called Chargaff’s rule. On the other hand, the amount of C/G in an organism could be quite different from that of A/T, with no obvious pattern across organisms. Based on Chargaff’s rule, we might define a statistic</p>
<p>\[ (p_{} - p_{})^2 + (p_{} - p_{})^2, \]</p>
<p>summed over all rows of the table. We are going to look at a comparison between the data and what would occur if the nucleotides were ‘exchangeable’, in the sense that the probabilities observed in each row were in no particular order, so that there were no special relationship between the proportions of As and Ts, or between those of Cs and Gs.</p>
<pre><code>statChf = function(x){
  sum((x[, "C"] - x[, "G"])^2 + (x[, "A"] - x[, "T"])^2)
}
chfstat = statChf(ChargaffTable)
permstat = replicate(100000, {
     permuted = t(apply(ChargaffTable, 1, sample))
     colnames(permuted) = colnames(ChargaffTable)
     statChf(permuted)
})
pChf = mean(permstat &lt;= chfstat)
pChf __


[1] 0.00014


hist(permstat, breaks = 100, main = "", col = "lavender")
abline(v = chfstat, lwd = 2, col = "red")__</code></pre>
<p><a href="02-chap_files/figure- html/fig-permstatChf-1-1.png" title="Figure 2.10: Histogram of our statistic statChf computed from simulations using per-row permutations of the columns. The value it yields for the observed data is shown by the red line."><img src="02-chap_files/figure-html/fig-permstatChf-1-1.png" class="img-fluid"></a></p>
<p>Figure 2.10: Histogram of our statistic <code>statChf</code> computed from simulations using per-row permutations of the columns. The value it yields for the observed data is shown by the red line.</p>
<p>The histogram in Figure 2.10 shows that it is quite rare to have a value as small as the observed 11.1, where the red line is drawn. The probability of observing a value as small or smaller is <code>pChf</code>=1.4^{-4}. Thus the data strongly support Chargaff’s insight.</p>
<p>__</p>
<p>Question 2.14</p>
<p>When computing <code>pChf</code>, we only looked at the values in the null distribution smaller than the observed value. Why did we do this in a one-sided way here?</p>
<section id="two-categorical-variables" class="level3" data-number="4.6.1">
<h3 data-number="4.6.1" class="anchored" data-anchor-id="two-categorical-variables"><span class="header-section-number">4.6.1</span> 2.7.1 Two categorical variables</h3>
<p>Up to now, we have visited cases where the data are taken from a sample that can be classified into different boxes: the binomial for Yes/No binary boxes and the multinomial distribution for categorical variables such as A, C, G, T or different genotypes such aa, aA, AA. However it might be that we measure two (or more) categorical variables on a set of subjects, for instance eye color and hair color. We can then cross-tabulate the counts for every combination of eye and hair color. We obtain a table of counts called a <strong>contingency table</strong>. This concept is very useful for many biological data types.</p>
<pre><code>HairEyeColor[,, "Female"]__


       Eye
Hair    Brown Blue Hazel Green
  Black    36    9     5     2
  Brown    66   34    29    14
  Red      16    7     7     7
  Blond     4   64     5     8</code></pre>
<p>__</p>
<p>Question 2.15</p>
<p>Explore the <code>HairEyeColor</code> object in R. What data type, shape and dimensions does it have?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>It is a numeric array with three dimensions:</p>
<pre><code>str(HairEyeColor)__


 'table' num [1:4, 1:4, 1:2] 32 53 10 3 11 50 10 30 10 25 ...
 - attr(*, "dimnames")=List of 3
  ..$ Hair: chr [1:4] "Black" "Brown" "Red" "Blond"
  ..$ Eye : chr [1:4] "Brown" "Blue" "Hazel" "Green"
  ..$ Sex : chr [1:2] "Male" "Female"


## ?HairEyeColor __</code></pre>
<section id="color-blindness-and-sex" class="level4" data-number="4.6.1.1">
<h4 data-number="4.6.1.1" class="anchored" data-anchor-id="color-blindness-and-sex"><span class="header-section-number">4.6.1.1</span> Color blindness and sex</h4>
<p>Deuteranopia is a form of red-green color blindness due to the fact that medium wavelength sensitive cones (green) are missing. A deuteranope can only distinguish 2 to 3 different hues, whereas somebody with normal vision sees 7 different hues. A survey for this type of color blindness in human subjects produced a two-way table crossing color blindness and sex.</p>
<pre><code>load("../data/Deuteranopia.RData")
Deuteranopia __


          Men Women
Deute      19     2
NonDeute 1981  1998</code></pre>
<p>How do we test whether there is a relationship between sex and the occurrence of color blindness? We postulate the null model with two independent binomials: one for sex and one for color blindness. Under this model we can estimate all the cells’ multinomial probabilities, and we can compare the observed counts to the expected ones. This is done through the <code>chisq.test</code> function in R.</p>
<pre><code>chisq.test(Deuteranopia)__


    Pearson's Chi-squared test with Yates' continuity correction

data:  Deuteranopia
X-squared = 12.255, df = 1, p-value = 0.0004641</code></pre>
<p>The small p value tells us that we should expect to see such a table with only a very small probability under the null model – i.e., if the fractions of deuteranopic color blind among women and men were the same.</p>
<p>We’ll see another test for this type of data called Fisher’s exact test (also known as the hypergeometric test) in <a href="10-chap.html#sec-graphs- GSEA">Section 10.3.2</a>. This test is widely used for testing the over-representations of certain types of genes in a list of significantly expressed ones.</p>
</section>
</section>
<section id="a-special-multinomial-hardy-weinberg-equilibrium" class="level3" data-number="4.6.2">
<h3 data-number="4.6.2" class="anchored" data-anchor-id="a-special-multinomial-hardy-weinberg-equilibrium"><span class="header-section-number">4.6.2</span> 2.7.2 A special multinomial: Hardy-Weinberg equilibrium</h3>
<p>Here we highlight the use of a multinomial with three possible levels created by combining two alleles M and N. Suppose that the overall frequency of allele M in the population is \(p\), so that of N is \(q = 1-p\). The Hardy- Weinberg model looks at the relationship between \(p\) and \(q\) if there is independence of the frequency of both alleles in a genotype, the so-called <strong>Hardy-Weinberg equilibrium</strong> (HWE). This would be the case if there is random mating in a large population with equal distribution of the alleles among sexes. The probabilities of the three genotypes are then as follows:</p>
<p>\[ p_{}=p<sup>2,p_{}=q</sup>2,p_{}=2pq \]</p>
<p>We only observe the frequencies \((n_{},,n_{},,n_{})\) for the genotypes MM, MN, NN and the total number \(S=n_{}+ n_{}+n_{}\). We can write the likelihood, i.e., the probability of the observed data when the probabilities of the categories are given by Equation 2.5, using the multinomial formula</p>
<p>\[ P(n_{},,n_{},,n_{};|;p) = {S n_{},n_{},n_{}} (p<sup>2)</sup>{n_{}} ,, (2pq)^{n_{}} ,, (q<sup>2)</sup>{n_{}}, \]</p>
<p>and the log-likelihood under HWE</p>
<p>\[ L(p)=n_{}(p^2)+n_{} (2pq)+n_{}(q^2). \]</p>
<p>The value of \(p\) that maximizes the log-likelihood is</p>
<p>\[ p = . \]</p>
<p>See (<a href="16-chap.html#ref-Rice:2007">Rice 2006, chap.&nbsp;8</a>, Section 5) for the proof. Given the data \((n_{},,n_{},,n_{})\), the log-likelihood \(L\) is a function of only one parameter, \(p\). Figure 2.11 shows this log-likelihood function for different values of \(p\) for the 216th row of the Mourant data7, computed in the following code.</p>
<p>7 This is genotype frequency data of blood group alleles from Mourant, Kopec, and Domaniewska-Sobczak (<a href="16-chap.html#ref-Mourant1976">1976</a>) available through the R package <strong><a href="https://cran.r-project.org/web/packages/HardyWeinberg/">HardyWeinberg</a></strong>.</p>
<pre><code>library("HardyWeinberg")
data("Mourant")
Mourant[214:216,]__


    Population    Country Total  MM  MN  NN
214    Oceania Micronesia   962 228 436 298
215    Oceania Micronesia   678  36 229 413
216    Oceania     Tahiti   580 188 296  96


nMM = Mourant$MM[216]
nMN = Mourant$MN[216]
nNN = Mourant$NN[216]
loglik = function(p, q = 1 - p) {
  2 * nMM * log(p) + nMN * log(2*p*q) + 2 * nNN * log(q)
}
xv = seq(0.01, 0.99, by = 0.01)
yv = loglik(xv)
plot(x = xv, y = yv, type = "l", lwd = 2,
     xlab = "p", ylab = "log-likelihood")
imax = which.max(yv)
abline(v = xv[imax], h = yv[imax], lwd = 1.5, col = "blue")
abline(h = yv[imax], lwd = 1.5, col = "purple")__</code></pre>
<p><a href="02-chap_files/figure-html/fig-HardyWeinberg-1-1.png &quot;Figure 2.11: Plot of the log-likelihood for the Tahiti data.&quot;"><img src="02-chap_files/figure-html/fig- HardyWeinberg-1-1.png" class="img-fluid"></a></p>
<p>Figure 2.11: Plot of the log-likelihood for the Tahiti data.</p>
<p>The maximum likelihood estimate for the probabilities in the multinomial is also obtained by using the observed frequencies as in the binomial case, however the estimates have to account for the relationships between the three probabilities. We can compute \(<em>{}\), \(</em>{}\) and \(_{}\) using the <code>af</code> function from the <strong><a href="https://cran.r-project.org/web/packages/HardyWeinberg/">HardyWeinberg</a></strong> package.</p>
<pre><code>phat  =  af(c(nMM, nMN, nNN))
phat __


        A 
0.5793103 


pMM   =  phat^2
qhat  =  1 - phat __</code></pre>
<p>The expected values under Hardy-Weinberg equilibrium are then</p>
<pre><code>pHW = c(MM = phat^2, MN = 2*phat*qhat, NN = qhat^2)
sum(c(nMM, nMN, nNN)) * pHW __


    MM.A     MN.A     NN.A 
194.6483 282.7034 102.6483 </code></pre>
<p>which we can compare to the observed values above. We can see that they are quite close to the observed values. We could further test whether the observed values allow us to reject the Hardy-Weinberg model, either by doing a simulation or a \(^2\) test as above. A visual evaluation of the goodness-of-fit of Hardy-Weinberg was designed by de Finetti (<a href="16-chap.html#ref-definetti26">Finetti 1926</a>; <a href="16-chap.html#ref-Cannings1968">Cannings and Edwards 1968</a>). It places every sample at a point whose coordinates are given by the proportions of each of the different alleles.</p>
<section id="visual-comparison-to-the-hardy-weinberg-equilibrium" class="level4" data-number="4.6.2.1">
<h4 data-number="4.6.2.1" class="anchored" data-anchor-id="visual-comparison-to-the-hardy-weinberg-equilibrium"><span class="header-section-number">4.6.2.1</span> Visual comparison to the Hardy-Weinberg equilibrium</h4>
<p>We use the <code>HWTernaryPlot</code> function to display the data and compare it to Hardy-Weinberg equilibrium graphically.</p>
<pre><code>pops = c(1, 69, 128, 148, 192)
genotypeFrequencies = as.matrix(Mourant[, c("MM", "MN", "NN")])
HWTernaryPlot(genotypeFrequencies[pops, ],
        markerlab = Mourant$Country[pops],
        alpha = 0.0001, curvecols = c("red", rep("purple", 4)),
        mcex = 0.75, vertex.cex = 1)__</code></pre>
<p><a href="02-chap_files/figure- html/fig-HWtern-1.png" title="Figure 2.12: This de Finetti plot shows the points as barycenters of the three genotypes using the frequencies as weights on each of the corners of the triangle. The Hardy-Weinberg model is the red curve, the acceptance region is between the two purple lines. We see that the US is the furthest from being in HW equilibrium."><img src="02-chap_files/figure-html/fig-HWtern-1.png" class="img-fluid"></a></p>
<p>Figure 2.12: This <strong>de Finetti plot</strong> shows the points as barycenters of the three genotypes using the frequencies as weights on each of the corners of the triangle. The Hardy-Weinberg model is the red curve, the acceptance region is between the two purple lines. We see that the US is the furthest from being in HW equilibrium.</p>
<p>__</p>
<p>Question 2.16</p>
<p>Make the ternary plot as in the code above, then add the other data points to it, what do you notice? You could back up your discussion using the <code>HWChisq</code> function.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>HWTernaryPlot(genotypeFrequencies[-pops, ], 
              newframe = FALSE, alpha = 0.0001, cex = 0.5)__</code></pre>
<p>__</p>
<p>Question 2.17</p>
<p>Divide all total frequencies by 50, keeping the same proportions for each of the genotypes, and recreate the ternary plot.</p>
<ul>
<li><p>What happens to the points ?</p></li>
<li><p>What happens to the confidence regions and why?</p></li>
</ul>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>newgf = round(genotypeFrequencies / 50)
HWTernaryPlot(newgf[pops, ],
              markerlab = Mourant$Country[pops],
              curvecols = c("red", rep("purple", 4)),
              alpha = 0.0001, mcex = 0.75, vertex.cex = 1)__</code></pre>
</section>
</section>
<section id="concatenating-several-multinomials-sequence-motifs-and-logos" class="level3" data-number="4.6.3">
<h3 data-number="4.6.3" class="anchored" data-anchor-id="concatenating-several-multinomials-sequence-motifs-and-logos"><span class="header-section-number">4.6.3</span> 2.7.3 Concatenating several multinomials: sequence motifs and logos</h3>
<p>The <a href="http://www.sciencegateway.org/resources/kozak.htm">Kozak Motif</a> is a sequence that occurs close to the start codon <strong>ATG</strong> of a coding region. The start codon itself always has a fixed spelling but in positions 5 to the left of it, there is a nucleotide pattern in which the letters are quite far from being equally likely.</p>
<p>We summarize this by giving the <strong>position weight matrix</strong> (PWM) or <strong>position-specific scoring matrix</strong> (PSSM), which provides the multinomial probabilities at every position. This is encoded graphically by the <strong>sequence logo</strong> (Figure 2.13).</p>
<pre><code>library("seqLogo")
load("../data/kozak.RData")
kozak __


  [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
A 0.33 0.25  0.4 0.15 0.20    1    0    0 0.05
C 0.12 0.25  0.1 0.40 0.40    0    0    0 0.05
G 0.33 0.25  0.4 0.20 0.25    0    0    1 0.90
T 0.22 0.25  0.1 0.25 0.15    0    1    0 0.00


pwm = makePWM(kozak)
seqLogo(pwm, ic.scale = FALSE)__</code></pre>
<p><a href="02-chap_files/figure- html/fig-seqlogo-1-1.png" title="Figure 2.13: Here is a diagram called a sequence logo for the position dependent multinomial used to model the Kozak motif. It codifies the amount of variation in each of the positions on a log scale. The large letters represent positions where there is no uncertainty about which nucleotide occurs."><img src="02-chap_files/figure-html/fig-seqlogo-1-1.png" class="img-fluid"></a></p>
<p>Figure 2.13: Here is a diagram called a sequence logo for the position dependent multinomial used to model the Kozak motif. It codifies the amount of variation in each of the positions on a log scale. The large letters represent positions where there is no uncertainty about which nucleotide occurs.</p>
<p>Over the last sections, we’ve seen how the different “boxes” in the multinomial distributions we have encountered very rarely have equal probabilities. In other words, the parameters \(p_1, p_2, …\) are often different, depending on what is being modeled. Examples of multinomials with unequal frequencies include the twenty different amino acids, blood types and hair color.</p>
<p>If there are multiple categorical variables, we have seen that they are rarely independent (sex and colorblindness, hair and eye color, …). We will see later in <a href="09-chap.html">Chapter 9</a> that we can explore the patterns in these dependencies by using multivariate decompositions of the contingency tables. Here, we’ll look at an important special case of dependencies between categorical variables: those that occur along a sequence (or “chain”) of categorical variables, e.g., over time or along a biopolymer.</p>
</section>
</section>
<section id="modeling-sequential-dependencies-markov-chains" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="modeling-sequential-dependencies-markov-chains"><span class="header-section-number">4.7</span> 2.8 Modeling sequential dependencies: Markov chains</h2>
<p>If we want to predict tomorrow’s weather, a reasonably good guess is that it will most likely be the same as today’s weather, in addition we may state the probabilities for various kinds of possible changes^[The same reasoning can also be applied in reverse: we could “predict” yesterday’s weather from today’s.. This method for weather forecasting is an example for the Markov assumption: the prediction for tomorrow only depends on the state of things today, but not on yesterday or three weeks ago (all information we could potentially use is already contained in today’s weather). The weather example also highlights that such an assumption need not necessarily be exactly true, but it should be a good enough assumption. It is fairly straightforward to extend this assumption to dependencies on the previous \(k\) days, where \(k\) is a finite and hopefully not too large number. The essence of the Markov assumption is that the process has a finite “memory”, so that predictions only need to look back for a finite amount of time.</p>
<p>Instead of temporal sequences, we can also apply this to biological sequences. In DNA, we may see specific succession of patterns so that pairs of nucleotides, called digrams, say, [CG, CA, CC] and [CT] are not equally frequent. For instance, in parts of the genome we see more frequent instances of [CA] than we would expect under independence:</p>
<p>\[ P() P() , P(). \]</p>
<p>We model this dependency in the sequence as a <strong>Markov chain</strong> :</p>
<p>\[ P() = P() = P() = P(…) = P() , P(), \]</p>
<p>where N stands for any nucleotide, and \(P()\) stands for “the probability of \(\), given that the preceding base is a \(\)”. Figure 2.14 shows a schematic representation of such transitions on a graph.</p>
<p><a href="02-chap_files/figure-html/fig-statsfourstateMC-1.png &quot;Figure 2.14: Visualisation of a 4-state Markov chain. The probability of each possible digram (e.,g., CA) is given by the weight of the edge between the corresponding nodes. So for instance, the probability of CA is given by the edge C$ o$ A. We’ll see in sec-images how to use R packages to draw these type of network graphs.&quot;"><img src="02-chap_files/figure-html/fig- statsfourstateMC-1.png" class="img-fluid"></a></p>
<p>Figure 2.14: Visualisation of a 4-state Markov chain. The probability of each possible digram (e.,g., CA) is given by the weight of the edge between the corresponding nodes. So for instance, the probability of CA is given by the edge C$ o$ A. We’ll see in <a href="11-chap.html">Chapter 11</a> how to use R packages to draw these type of network graphs.</p>
</section>
<section id="bayesian-thinking" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="bayesian-thinking"><span class="header-section-number">4.8</span> 2.9 Bayesian Thinking</h2>
<p><a href="imgs/turtlesalltheway.png" title="Figure 2.15: Turtles all the way down. Bayesian modeling of the uncertainty of the parameter of a distribution is done by using a random variable whose distribution may depend on parameters whose uncertainty can be modeled as a random variable; these are called hierarchical models."><img src="imgs/turtlesalltheway.png" class="img-fluid"></a></p>
<p>Figure 2.15: Turtles all the way down. Bayesian modeling of the uncertainty of the parameter of a distribution is done by using a random variable whose distribution may depend on parameters whose uncertainty can be modeled as a random variable; these are called hierarchical models.</p>
<p>Up to now we have followed a classical approach, where the parameters of our models and the distributions they use, i.e., the probabilities of the possible different outcomes, represent long term frequencies. The parameters are—at least conceptually—definite, knowable and fixed. We may not know them, so we estimate them from the data at hand. However, such an approach does not take into account any information that we might already have, and that might inform us on the parameters or make certain parameter values or their combinations more likely than others—even <em>before</em> we see any of the current set of data. For that we need a different approach, in which we use probabilistic models (i.e., distributions) to express our prior knowledge8 about the parameters, and use the current data to <em>update</em> such knowledge, for instance by shifting those distributions or making them more narrow. Such an approach is provided by the Bayesian paradigm (Figure 2.15).</p>
<p>8 Some like to say “our belief(s)”.</p>
<p>The Bayesian paradigm is a practical approach where <em>prior</em> and <em>posterior</em> distributions to model our knowledge <em>before</em> and <em>after</em> collecting some data and making an observation. It can be iterated ad infinitum: the posterior after one round of data generation can be used as the prior for the next round. Thus, it is also particularly useful for integrating or combining information from different sources.</p>
<p>The same idea can also be applied to hypothesis testing, where we want to use data to decide whether we believe that a certain statement—which we might call the hypothesis \(H\)—is true. Here, our “parameter” is the probability that \(H\) is true, and we can formalize our prior knowledge in the form of a <strong>prior</strong> probability, written \(P(H)\)9. After we see the data, we have the <strong>posterior</strong> probability. We write it as \(P(H,|,D)\), the probability of \(H\) given that we saw \(D\). This may be higher or lower than \(P(H)\), depending on what the data \(D\) were.</p>
<p>9 For a so-called frequentist, such a probability does not exist. Their viewpoint is that, although the truth is unknown, in reality the hypothesis is either true or false; there is no meaning in calling it, say, “70% true”.</p>
<section id="example-haplotype-frequencies" class="level3" data-number="4.8.1">
<h3 data-number="4.8.1" class="anchored" data-anchor-id="example-haplotype-frequencies"><span class="header-section-number">4.8.1</span> 2.9.1 Example: haplotype frequencies</h3>
<p>To keep the mathematical formalism to a minimum, we start with an example from forensics, using combined signatures (haplotypes) from the Y chromosome.</p>
<p>A <em>haplotype</em> is a collection of alleles (DNA sequence variants) that are spatially adjacent on a chromosome, are usually inherited together (recombination tends not to disconnect them), and thus are genetically linked. In this case we are looking at linked variants on the Y chromosome.</p>
<p>First we’ll look at the motivation behind haplotype frequency analyses, then we’ll revisit the idea of likelihood. After this, we’ll explain how we can think of unknown parameters as being random numbers themselves, modeling their uncertainty with a prior distribution. Then we will see how to incorporate new data observed into the probability distributions and compute posterior confidence statements about the parameters.</p>
<p><a href="imgs/STRDefinition.png" title="Figure 2.16: A short tandem repeat (STR) in DNA occurs when a pattern of two or more nucleotides is repeated, and the repeated sequences are directly adjacent to each other. An STR is also known as a microsatellite. The pattern can range in length from 2 to 13 nucleotides, and the number of repeats is highly variable across individuals. STR numbers can be used as genetic signatures."><img src="imgs/STRDefinition.png" class="img-fluid"></a></p>
<p>Figure 2.16: A short tandem repeat (STR) in DNA occurs when a pattern of two or more nucleotides is repeated, and the repeated sequences are directly adjacent to each other. An STR is also known as a microsatellite. The pattern can range in length from 2 to 13 nucleotides, and the number of repeats is highly variable across individuals. STR numbers can be used as genetic signatures.</p>
<p><a href="imgs/YSTRPositions.jpg" title="Figure 2.17: Location of short tandem repeats (STR) on the human Y chromosome. Source: https://strbase.nist.gov/ystrpos1.htm"><img src="imgs/YSTRPositions.jpg" class="img-fluid"></a></p>
<p>Figure 2.17: Location of short tandem repeats (STR) on the human Y chromosome. Source: <a href="https://strbase.nist.gov/ystrpos1.htm" class="uri">https://strbase.nist.gov/ystrpos1.htm</a></p>
<p><a href="imgs/USY-STR.png" title="Figure 2.18: Y STR haplotype lookup from a database used by the FBI."><img src="imgs/USY-STR.png" class="img-fluid"></a></p>
<p>Figure 2.18: Y STR haplotype lookup from a database used by the FBI.</p>
<p>We’re interested in the frequencies of particular Y-haplotypes that consist of a set of different short tandem repeats (STR). The combination of STR numbers at the specific locations used for DNA forensics are labeled by the number of repeats at the specific positions. Here is a short excerpt of such an STR haplotype table:</p>
<pre><code>haplo6 = read.table("../data/haplotype6.txt", header = TRUE)
haplo6 __


  Individual DYS19 DXYS156Y DYS389m DYS389n DYS389p
1         H1    14       12       4      12       3
2         H3    15       13       4      13       3
3         H4    15       11       5      11       3
4         H5    17       13       4      11       3
5         H7    13       12       5      12       3
6         H8    16       11       5      12       3</code></pre>
<p>The table says that the haplotype H1 has 14 repeats at position <code>DYS19</code>, 12 repeats at position <code>DXYS156Y</code>, etc. Suppose we want to find the underlying proportion \(p\) of a particular haplotype in a population of interest, by haplotyping \(n=300\) men; and suppose we found H1 in \(y=40\) of them. We are going to use the binomial distribution \(B(n,p)\) to model this, with \(p\) unknown.</p>
<p>The haplotypes created through the use of these Y-STR profiles are shared between men in the same patriarchal lineages. Thus, it is possible that two different men share the same profile.</p>
</section>
<section id="simulation-study-of-the-bayesian-paradigm-for-the-binomial" class="level3" data-number="4.8.2">
<h3 data-number="4.8.2" class="anchored" data-anchor-id="simulation-study-of-the-bayesian-paradigm-for-the-binomial"><span class="header-section-number">4.8.2</span> 2.9.2 Simulation study of the Bayesian paradigm for the binomial</h3>
<p>distribution</p>
<p>Instead of assuming that our parameter \(p\) has one single value (e.g., the maximum likelihood estimate 40/300), the Bayesian approach allows us to see it as a draw from a statistical distribution. The distribution expresses our belief about the possible values of the parameter \(p\). In principle, we can use any distribution that we like whose possible values are permissible for \(p\). As here we are looking at a parameter that expresses a proportion or a probability, and which takes its values between 0 and 1, it is convenient to use the <em>Beta distribution</em>. Its density formula is written</p>
<p>\[ f_{,}(x) = (,)=. \]</p>
<p>We can see in Figure 2.19 how this function depends on two parameters \(\) and \(\), which makes it a very flexible family of distributions (it can “fit” a lot different situations). And it has a nice mathematical property: if we start with a prior belief on \(p\) that is Beta-shaped, observe a dataset of \(n\) binomial trials, then update our belief, the posterior distribution on \(p\) will also have a Beta distribution, albeit with updated parameters. This is a mathematical fact. We will not prove it here, however we demonstrate it by simulation.</p>
<p><a href="02-chap_files/figure- html/fig-histobeta2-1.png" title="Figure 2.19: Beta distributions with \alpha=10,20,50 and \beta=30,60,150. We can use these as a prior for probability of success in a binomial experiment. These three distributions have the same mean (\frac{\alpha}{\alpha +\beta}), but different concentrations around the mean."><img src="02-chap_files/figure-html/fig-histobeta2-1.png" class="img-fluid"></a></p>
<p>Figure 2.19: Beta distributions with \(,20,50\) and \(,60,150\). We can use these as a <strong>prior</strong> for probability of success in a binomial experiment. These three distributions have the same mean (\(\)), but different concentrations around the mean.</p>
</section>
<section id="the-distribution-of-y" class="level3" data-number="4.8.3">
<h3 data-number="4.8.3" class="anchored" data-anchor-id="the-distribution-of-y"><span class="header-section-number">4.8.3</span> 2.9.3 The distribution of \(Y\)</h3>
<p>For a given choice of \(p\), we know what the distribution of \(Y\) is, by virtue of Equation 2.3. But what is the distribution of \(Y\) if \(p\) itself also varies according to some distribution? We call this the <strong>marginal distribution</strong> of \(Y\). Let’s simulate that. First we generate a random sample <code>rp</code> of 100000 \(p\)s. For each of them, we then generate a random sample of \(Y\), shown in Figure 2.20. In the code below, for the sake of demonstration we use the parameters 50 and 350 for the prior. Such a prior is already quite informative (“peaked”) and may, e.g., reflect beliefs we have based on previous studies. In Question 2.20 you have the opportunity to try out a “softer” (less informative) prior. We again use <code>vapply</code> to apply a function, the unnamed (anonymous) function of <code>x</code>, across all elements of <code>rp</code> to obtain as a result another vector <code>y</code> of the same length.</p>
<pre><code>rp = rbeta(100000, 50, 350)
y = vapply(rp, 
           function(x) rbinom(1, prob = x, size = 300), 
           integer(1))
hist(y, breaks = 50, col = "orange", main = "", xlab = "")__</code></pre>
<p><a href="02-chap_files/figure-html/fig-histmarginal-1-1.png &quot;Figure 2.20: Marginal Distribution of Y.&quot;"><img src="02-chap_files/figure-html/fig- histmarginal-1-1.png" class="img-fluid"></a></p>
<p>Figure 2.20: Marginal Distribution of \(Y\).</p>
<p>__</p>
<p>Question 2.18</p>
<p>Verify that we could have gotten the same result as in the above code chunk by using R’s vectorisation capabilities and writing <code>rbinom(length(rp), rp, size = 300)</code>.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>set.seed(0xbebe)
y1 = vapply(rp, 
            function(x) rbinom(1, prob = x, size = 300), 
            integer(1))
set.seed(0xbebe)
y2 = rbinom(length(rp), rp, size = 300)
stopifnot(identical(y1, y2))__</code></pre>
</section>
<section id="histogram-of-all-the-ps-such-that-y40-the-posterior" class="level3" data-number="4.8.4">
<h3 data-number="4.8.4" class="anchored" data-anchor-id="histogram-of-all-the-ps-such-that-y40-the-posterior"><span class="header-section-number">4.8.4</span> 2.9.4 Histogram of all the \(p\)s such that \(Y=40\): the posterior</h3>
<p>distribution</p>
<p>So now let’s compute the posterior distribution of \(p\) by conditioning on those outcomes where \(Y\) was 40. We compare it to the theoretical posterior, <code>densPostTheory</code>, of which more below. The results are shown in Figure 2.21.</p>
<pre><code>pPostEmp = rp[ y == 40 ]
hist(pPostEmp, breaks = 40, col = "chartreuse4", main = "",
  probability = TRUE, xlab = "posterior p")

p_seq = seq(0, 1, by = 0.001)
densPostTheory = dbeta(p_seq, 50 + 40, 350 + 260)
lines(p_seq, densPostTheory, type = "l", lwd = 3)__</code></pre>
<p><a href="02-chap_files/figure-html/fig- densityposterior-1-1.png" title="Figure 2.21: Only choosing the values of the distribution with Y=40 gives the posterior distribution of p. The histogram (green) shows the simulated values for the posterior distribution, the line the density of a Beta distribution with the theoretical parameters."><img src="02-chap_files/figure-html/fig- densityposterior-1-1.png" class="img-fluid"></a></p>
<p>Figure 2.21: Only choosing the values of the distribution with \(Y=40\) gives the posterior distribution of \(p\). The histogram (green) shows the simulated values for the posterior distribution, the line the density of a Beta distribution with the theoretical parameters.</p>
<p>We can also check the means of both distributions computed above and see that they are close to 4 significant digits.</p>
<pre><code>mean(pPostEmp)__


[1] 0.128726


dp = p_seq[2] - p_seq[1]
sum(p_seq * densPostTheory * dp)__


[1] 0.1285714</code></pre>
<p>To approximate the mean of the theoretical density <code>densPostTheory</code>, we have above literally computed the integral</p>
<p>\[ _0^1 p , f(p) , dp \]</p>
<p>using numerical integration, i.e., the <code>sum</code> over the integral. This is not always convenient (or feasible), in particular if our model involves not just a single, scalar parameter \(p\), but has many parameters, so that we are dealing with a high-dimensional parameter vector and a high-dimensional integral. If the integral cannot be computed analytically, we can use <strong>Monte Carlo integration</strong>. You already saw a very simple instance of Monte Carlo integration in the code above, where we sampled the posterior with <code>pPostEmp</code> and performed integration to compute the posterior mean by calling R’s <code>mean</code> function. In this case, an alternative Monte Carlo algorithm is to generate posterior samples using the <code>rbeta</code> function directly with the right parameters.</p>
<pre><code>pPostMC = rbeta(n = 100000, 90, 610)
mean(pPostMC)__


[1] 0.1285718</code></pre>
<p>We can check the concordance between the Monte Carlo samples <code>pPostMC</code> and <code>pPostEmp</code>, generated in slightly different ways, using a <strong>quantile-quantile plot</strong> (<strong>QQ-plot</strong> , Figure 2.22).</p>
<pre><code>qqplot(pPostMC, pPostEmp, type = "l", asp = 1)
abline(a = 0, b = 1, col = "blue")__</code></pre>
<p><a href="02-chap_files/figure- html/fig-qqplotbeta-1-1.png" title="Figure 2.22: Quantile-quantile (QQ) plot of our Monte Carlo sample pPostMC from the theoretical distribution and our simulation sample pPostEmp. We could also similarly compare either of these two distributions to the theoretical distribution function pbeta(., 90, 610). If the curve lies on the line y=x, this indicates a good agreement. There are some random differences at the tails."><img src="02-chap_files/figure-html/fig-qqplotbeta-1-1.png" class="img-fluid"></a></p>
<p>Figure 2.22: Quantile-quantile (QQ) plot of our Monte Carlo sample <code>pPostMC</code> from the theoretical distribution and our simulation sample <code>pPostEmp</code>. We could also similarly compare either of these two distributions to the theoretical distribution function <code>pbeta(., 90, 610)</code>. If the curve lies on the line \(y=x\), this indicates a good agreement. There are some random differences at the tails.</p>
<p>__</p>
<p>Question 2.19</p>
<p>What is the difference between the simulation that results in <code>pPostEmp</code> and the Monte Carlo simulation that leads to <code>pPostMC</code>?</p>
</section>
<section id="the-posterior-distribution-is-also-a-beta" class="level3" data-number="4.8.5">
<h3 data-number="4.8.5" class="anchored" data-anchor-id="the-posterior-distribution-is-also-a-beta"><span class="header-section-number">4.8.5</span> 2.9.5 The posterior distribution is also a Beta</h3>
<p>Now we have seen that the posterior distribution is also a Beta. In our case its parameters \(\) and \(\) were obtained by summing the prior parameters \(\), \(\) with the observed successes \(y=40\) and the observed failures \(n-y=260\), thus obtaining the posterior</p>
<p>\[ (90,, 610)=(+y,+(n-y)). \]</p>
<p>We can use it to give the best10 estimate we can for \(p\) with its uncertainty given by the posterior distribution.</p>
<p>10 We could take the value that maximizes the posterior distribution as our best estimate, this is called the <strong>MAP</strong> estimate, and in this case it would be \(=\).</p>
</section>
<section id="suppose-we-had-a-second-series-of-data" class="level3" data-number="4.8.6">
<h3 data-number="4.8.6" class="anchored" data-anchor-id="suppose-we-had-a-second-series-of-data"><span class="header-section-number">4.8.6</span> 2.9.6 Suppose we had a second series of data</h3>
<p>After seeing our previous data, we now have a new prior, \((90, 610)\). Suppose we collect a new set of data with \(n=150\) observations and \(y=25\) successes, thus 125 failures. Now what would we take to be our best guess at \(p\)?</p>
<p>Using the same reasoning as before, the new posterior will be \((90+25=115,, 610+125=735)\). The mean of this distribution is \(=\), thus one estimate of \(p\) would be 0.135. The <strong>maximum a posteriori</strong> (MAP) estimate would be the mode of \((115, 735)\), ie \(\). Let’s check this numerically.</p>
<pre><code>densPost2 = dbeta(p_seq, 115, 735)
mcPost2   = rbeta(1e6, 115, 735)
sum(p_seq * densPost2 * dp)   # mean, by numeric integration __


[1] 0.1352941


mean(mcPost2)                 # mean by MC __


[1] 0.1352655


p_seq[which.max(densPost2)]   # MAP estimate __


[1] 0.134</code></pre>
<p>__</p>
<p>Question 2.20</p>
<p>Redo all the computations replacing our original prior with a softer prior (less peaked), meaning that we use less prior information. For instance, try Beta(1,1), which is the uniform distribution. How much does this change the final result?</p>
<p>As a general rule, the prior rarely changes the posterior distribution substantially except if it is very peaked. This would be the case if, at the outset, we were already rather sure of what to expect. Another case when the prior has an influence is if there is very little data.</p>
<p>The best situation to be in is to have enough data to swamp the prior so that its choice doesn’t have much impact on the final result.</p>
</section>
<section id="confidence-statements-for-the-proportion-parameter" class="level3" data-number="4.8.7">
<h3 data-number="4.8.7" class="anchored" data-anchor-id="confidence-statements-for-the-proportion-parameter"><span class="header-section-number">4.8.7</span> 2.9.7 Confidence Statements for the proportion parameter</h3>
<p>Now it is time to conclude about what the proportion \(p\) actually is, given the data. One summary is a posterior credibility interval, which is a Bayesian analog of the confidence interval. We can take the 2.5 and 97.5-th percentiles of the posterior distribution: \(P(q_{2.5%} p q_{97.5%})=0.95\).</p>
<pre><code>quantile(mcPost2, c(0.025, 0.975))__


     2.5%     97.5% 
0.1131080 0.1590221 </code></pre>
<p><a href="imgs/DESeq2-Prediction-Interval.png &quot;Figure 2.23: An example from @LoveDESeq2 shows plots of the likelihoods (solid lines, scaled to integrate to 1) and the posteriors (dashed lines) for the green and purple genes and of the prior (solid black line): due to the higher dispersion of the purple gene, its likelihood is wider and less peaked (indicating less information), and the prior has more influence on its posterior than for the green gene. The stronger curvature of the green posterior at its maximum translates to a smaller reported standard error for the MAP logarithmic fold change (LFC) estimate (horizontal error bar).&quot;"><img src="imgs/DESeq2-Prediction-Interval.png" class="img-fluid"></a></p>
<p>Figure 2.23: An example from Love, Huber, and Anders (<a href="16-chap.html#ref- LoveDESeq2">2014</a>) shows plots of the likelihoods (solid lines, scaled to integrate to 1) and the posteriors (dashed lines) for the green and purple genes and of the prior (solid black line): due to the higher dispersion of the purple gene, its likelihood is wider and less peaked (indicating less information), and the prior has more influence on its posterior than for the green gene. The stronger curvature of the green posterior at its maximum translates to a smaller reported standard error for the MAP logarithmic fold change (LFC) estimate (horizontal error bar).</p>
</section>
</section>
<section id="example-occurrence-of-a-nucleotide-pattern-in-a-genome" class="level2" data-number="4.9">
<h2 data-number="4.9" class="anchored" data-anchor-id="example-occurrence-of-a-nucleotide-pattern-in-a-genome"><span class="header-section-number">4.9</span> 2.10 Example: occurrence of a nucleotide pattern in a genome</h2>
<p>The examples we have seen up to now have concentrated on distributions of discrete counts and categorical data. Let’s look at an example of distributions of distances, which are quasi-continuous. This case study of the distributions of the distances between instances of a specific motif in genome sequences will also allow us to explore specific genomic sequence manipulations in Bioconductor.</p>
<p>The <strong><a href="https://bioconductor.org/packages/Biostrings/">Biostrings</a></strong> package provides tools for working with sequence data. The essential data structures, or <em>classes</em> as they are known in R, are <em>DNAString</em> and <em>DNAStringSet</em>. These enable us to work with one or multiple DNA sequences efficiently .</p>
<p>The <strong><a href="https://bioconductor.org/packages/Biostrings/">Biostrings</a></strong> package also contains additional classes for representing amino acid sequences, and more general, biology-inspired sequences.</p>
<pre><code>library("Biostrings")__</code></pre>
<p>__</p>
<p>Question 2.21</p>
<p>Explore some of the useful data and functions provided in the Biostrings package by exploring the tutorial vignette.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>The first line prints genetic code information, the second one returns IUPAC nucleotide ambiguity codes. The third line lists all the vignettes available in the <strong><a href="https://bioconductor.org/packages/Biostrings/">Biostrings</a></strong> package, the fourth display one particular vignette.</p>
<pre><code>GENETIC_CODE
IUPAC_CODE_MAP
vignette(package = "Biostrings")
vignette("BiostringsQuickOverview", package = "Biostrings")__</code></pre>
<p>This last command will open a list in your browser window from which you can access the documentation11. The <strong><a href="https://bioconductor.org/packages/BSgenome/">BSgenome</a></strong> package provides access to many genomes, and you can access the names of the data packages that contain the whole genome sequences by typing</p>
<p>11 Vignettes are manuals for the packages complete with examples and case studies.</p>
<pre><code>library("BSgenome")
ag = available.genomes()
length(ag)__


[1] 113


ag[1:2]__


[1] "BSgenome.Alyrata.JGI.v1"              
[2] "BSgenome.Amellifera.BeeBase.assembly4"</code></pre>
<p>We are going to explore the occurrence of the <code>AGGAGGT</code> motif12 in the genome of E.coli. We use the genome sequence of one particular strain, <strong>Escherichia coli</strong> str. K12 substr.DH10B13, whose NCBI accession number is NC_010473.</p>
<p>12 This is the <a href="https://en.wikipedia.org/wiki/Shine-%20Dalgarno_sequence">Shine-Dalgarno</a> motif which helps initiate protein synthesis in bacteria.</p>
<p>13 It is known as the laboratory workhorse, often used in experiments.</p>
<pre><code>library("BSgenome.Ecoli.NCBI.20080805")
Ecoli
shineDalgarno = "AGGAGGT"
ecoli = Ecoli$NC_010473 __</code></pre>
<p>We can count the pattern’s occurrence in windows of width 50000 using the <code>countPattern</code> function.</p>
<pre><code>window = 50000
starts = seq(1, length(ecoli) - window, by = window)
ends   = starts + window - 1
numMatches = vapply(seq_along(starts), function(i) {
  countPattern(shineDalgarno, ecoli[starts[i]:ends[i]],
               max.mismatch = 0)
  }, numeric(1))
table(numMatches)__


numMatches
 0  1  2  3  4 
48 32  8  3  2 </code></pre>
<p>__</p>
<p>Question 2.22</p>
<p>What distribution might this table fit ?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>The Poisson is a good candidate, as a quantitative and graphical evaluation (see Figure 2.24) for these data shows.</p>
<pre><code>library("vcd")
gf = goodfit(numMatches, "poisson")
summary(gf)__


     Goodness-of-fit test for poisson distribution

                      X^2 df  P(&gt; X^2)
Likelihood Ratio 4.134932  3 0.2472577


distplot(numMatches, type = "poisson")__</code></pre>
<p><a href="02-chap_files/figure- html/fig-poissonness-1.png" title="Figure 2.24: Evaluation of a Poisson model for motif counts along the sequence Ecoli$NC_010473."><img src="02-chap_files/figure-html/fig-poissonness-1.png" class="img-fluid"></a></p>
<p>Figure 2.24: Evaluation of a Poisson model for motif counts along the sequence <code>Ecoli$NC_010473</code>.</p>
<p>We can inspect the matches using the <code>matchPattern</code> function.</p>
<pre><code>sdMatches = matchPattern(shineDalgarno, ecoli, max.mismatch = 0)__</code></pre>
<p>You can type <code>sdMatches</code> in the R command line to obtain a summary of this object. It contains the locations of all 65 pattern matches, represented as a set of so-called <em>views</em> on the original sequence. Now what are the distances between them?</p>
<pre><code>betweenmotifs = gaps(sdMatches)__</code></pre>
<p>So these are in fact the 66 complementary regions. Now let’s find a model for the distribution of the gap sizes between motifs. If the motifs occur at random locations, we expect the gap lengths to follow an exponential distribution14. The code below (whose output is shown in Figure 2.25) assesses this assumption. If the exponential distribution is a good fit, the points should lie roughly on a straight line. The exponential distribution has one parameter, the rate, and the line with slope corresponding to an estimate from the data is also shown.</p>
<p>14 How could we guess that the exponential is the right fit here? Whenever we have independent, random Bernoulli occurrences along a sequence, the gap lengths are exponential. You may be familiar with radioactive decay, where the waiting times between emissions are also exponentially distributed. It is a good idea if you are not familiar with this distribution to look up more details in the <a href="http://en.wikipedia.org/wiki/Exponential_distribution">Wikipedia</a>.</p>
<pre><code>library("Renext")
expplot(width(betweenmotifs), rate = 1/mean(width(betweenmotifs)),
        labels = "fit")__</code></pre>
<p><a href="02-chap_files/figure- html/fig-expplotdata-1-1.png" title="Figure 2.25: Evaluation of fit to the exponential distribution (black line) of the gaps between the motifs."><img src="02-chap_files/figure-html/fig-expplotdata-1-1.png" class="img-fluid"></a></p>
<p>Figure 2.25: Evaluation of fit to the exponential distribution (black line) of the gaps between the motifs.</p>
<p>__</p>
<p>Question 2.23</p>
<p>There appears to be a slight deviation from the fitted line in Figure 2.25 at the right tail of the distribution, i.e., for the largest values. What could be the reason?</p>
<section id="modeling-in-the-case-of-dependencies" class="level3" data-number="4.9.1">
<h3 data-number="4.9.1" class="anchored" data-anchor-id="modeling-in-the-case-of-dependencies"><span class="header-section-number">4.9.1</span> 2.10.1 Modeling in the case of dependencies</h3>
<p>As we saw in Section 2.8, nucleotide sequences are often dependent: the probability of seing a certain nucleotide at a given position tends to depend on the surrounding sequence. Here we are going to put into practice dependency modeling using a <strong>Markov chain</strong>. We are going to look at regions of chromosome 8 of the human genome and try to discover differences between regions called CpG15 islands and the rest.</p>
<p>15 CpG stands for 5’-C-phosphate-G-3’; this means that a C is connected to a G through a phosphate along the strand (this is unrelated to C-G base-pairing of Section 2.7). The cytosines in the CpG dinucleotide can be methylated, changing the levels of gene expression. This type of gene regulation is part of <strong>epigenetics</strong>. Some more information is on Wikipedia: <a href="https://en.wikipedia.org/wiki/CpG_site">CpG site</a> and <a href="https://en.wikipedia.org/wiki/Epigenetics">epigenetics</a>.</p>
<p>We use data (generated by Irizarry, Wu, and Feinberg (<a href="16-chap.html#ref- Irizarry2009">2009</a>)) that tell us where the start and end points of the islands are in the genome and look at the frequencies of nucleotides and of the digrams ‘CG’, ‘CT’, ‘CA’, ‘CC’. So we can ask whether there are dependencies between the nucleotide occurrences and if so, how to model them.</p>
<pre><code>library("BSgenome.Hsapiens.UCSC.hg19")
chr8  =  Hsapiens$chr8
CpGtab = read.table("../data/model-based-cpg-islands-hg19.txt",
                    header = TRUE)
nrow(CpGtab)__


[1] 65699


head(CpGtab)__


    chr  start    end length CpGcount GCcontent pctGC obsExp
1 chr10  93098  93818    721       32       403 0.559  0.572
2 chr10  94002  94165    164       12        97 0.591  0.841
3 chr10  94527  95302    776       65       538 0.693  0.702
4 chr10 119652 120193    542       53       369 0.681  0.866
5 chr10 122133 122621    489       51       339 0.693  0.880
6 chr10 180265 180720    456       32       256 0.561  0.893


irCpG = with(dplyr::filter(CpGtab, chr == "chr8"),
         IRanges(start = start, end = end))__</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="We use the :: operator to call the filter function specifically from the dplyr package—and not from any other packages that may happen to be loaded and defining functions of the same name. This precaution is particularly advisable in the case of the filter function, since this name is used by quite a few other packages. You can think of the normal (without ::) way of calling R functions like calling people by their first (given) names; whereas the fully qualified version with :: corresponds to calling someone by their full name. At least within the reach of the CRAN and Bioconductor repositories, such fully qualified names are guaranteed to be unique."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>We use the :: operator to call the filter function specifically from the dplyr package—and not from any other packages that may happen to be loaded and defining functions of the same name. This precaution is particularly advisable in the case of the filter function, since this name is used by quite a few other packages. You can think of the normal (without ::) way of calling R functions like calling people by their first (given) names; whereas the fully qualified version with :: corresponds to calling someone by their full name. At least within the reach of the CRAN and Bioconductor repositories, such fully qualified names are guaranteed to be unique.</figcaption>
</figure>
</div>
<p>We use the <code>::</code> operator to call the <code>filter</code> function specifically from the <code>dplyr</code> package—and not from any other packages that may happen to be loaded and defining functions of the same name. This precaution is particularly advisable in the case of the <code>filter</code> function, since this name is used by quite a few other packages. You can think of the normal (without <code>::</code>) way of calling R functions like calling people by their first (given) names; whereas the fully qualified version with <code>::</code> corresponds to calling someone by their full name. At least within the reach of the CRAN and Bioconductor repositories, such fully qualified names are guaranteed to be unique.</p>
<p>In the line above, we subset (<code>filter</code>) the data frame <code>CpGtab</code> to only chromosome 8, and then we create an <em>IRanges</em> object whose start and end positions are defined by the equally named columns of the data frame. In the <code>IRanges</code> function call (which constructs the object from its arguments), the first <code>start</code> is the argument name of the function, the second <code>start</code> refers to the column in the data frame obtained as an output from <code>filter</code>; and similarly for <code>end</code>. <em>IRanges</em> is a general container for mathematical intervals. We create the biological context16 with the next line.</p>
<p>16 The “I” in <em>IRanges</em> stands for “interval”; the “G” in <em>GRanges</em> for “genomic”.</p>
<pre><code>grCpG = GRanges(ranges = irCpG, seqnames = "chr8", strand = "+")
genome(grCpG) = "hg19"__</code></pre>
<p>Now let’s visualize; see the output in Figure 2.26.</p>
<pre><code>library("Gviz")
ideo = IdeogramTrack(genome = "hg19", chromosome = "chr8")
plotTracks(
  list(GenomeAxisTrack(),
    AnnotationTrack(grCpG, name = "CpG"), ideo),
    from = 2200000, to = 5800000,
    shape = "box", fill = "#006400", stacking = "dense")__</code></pre>
<p><a href="02-chap_files/figure-html/fig-freqandbayes-ideo-1.png &quot;Figure 2.26: Gviz plot of CpG locations in a selected region of chromosome 8.&quot;"><img src="02-chap_files/figure-html/fig-freqandbayes- ideo-1.png" class="img-fluid"></a></p>
<p>Figure 2.26: <strong><a href="https://bioconductor.org/packages/Gviz/">Gviz</a></strong> plot of CpG locations in a selected region of chromosome 8.</p>
<p>We now define so-called views on the chromosome sequence that correspond to the CpG islands, <code>irCpG</code>, and to the regions in between (<code>gaps(irCpG)</code>). The resulting objects <code>CGIview</code> and <code>NonCGIview</code> only contain the coordinates, not the sequences themselves (these stay in the big object <code>Hsapiens$chr8</code>), so they are fairly lightweight in terms of storage.</p>
<pre><code>CGIview    = Views(unmasked(Hsapiens$chr8), irCpG)
NonCGIview = Views(unmasked(Hsapiens$chr8), gaps(irCpG))__</code></pre>
<p>We compute transition counts in CpG islands and non-islands using the data.</p>
<pre><code>seqCGI      = as(CGIview, "DNAStringSet")
seqNonCGI   = as(NonCGIview, "DNAStringSet")
dinucCpG    = sapply(seqCGI, dinucleotideFrequency)
dinucNonCpG = sapply(seqNonCGI, dinucleotideFrequency)
dinucNonCpG[, 1]__


 AA  AC  AG  AT  CA  CC  CG  CT  GA  GC  GG  GT  TA  TC  TG  TT 
389 351 400 436 498 560 112 603 359 336 403 336 330 527 519 485 


NonICounts = rowSums(dinucNonCpG)
IslCounts  = rowSums(dinucCpG)__</code></pre>
<p>For a four state Markov chain as we have, we define the transition matrix as a matrix where the rows are the <code>from</code> state and the columns are the <code>to</code> state.</p>
<pre><code>TI  = matrix( IslCounts, ncol = 4, byrow = TRUE)
TnI = matrix(NonICounts, ncol = 4, byrow = TRUE)
dimnames(TI) = dimnames(TnI) =
  list(c("A", "C", "G", "T"), c("A", "C", "G", "T"))__</code></pre>
<p>We use the counts of numbers of transitions of each type to compute frequencies and put them into two matrices.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="The transition probabilities are probabilities so the rows need to sum to 1."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>The transition probabilities are probabilities so the rows need to sum to 1.</figcaption>
</figure>
</div>
<p>The transition probabilities are probabilities so the rows need to sum to 1.</p>
<pre><code>MI = TI /rowSums(TI)
MI __


           A         C         G         T
A 0.20457773 0.2652333 0.3897678 0.1404212
C 0.20128250 0.3442381 0.2371595 0.2173200
G 0.18657245 0.3145299 0.3450223 0.1538754
T 0.09802105 0.3352314 0.3598984 0.2068492


MN = TnI / rowSums(TnI)
MN __


          A         C          G         T
A 0.3351380 0.1680007 0.23080886 0.2660524
C 0.3641054 0.2464366 0.04177094 0.3476871
G 0.2976696 0.2029017 0.24655406 0.2528746
T 0.2265813 0.1972407 0.24117528 0.3350027</code></pre>
<p>__</p>
<p>Question 2.24</p>
<p>Are the transitions different in the different rows? This would mean that, for instance, \(P(,|,) P(,|,)\).</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>The transitions are different. For instance, the transitions from C to A and T to A for in the islands (MI) transition matrix seem very different (0.201 versus 0.098).</p>
<p>__</p>
<p>Question 2.25</p>
<p>Are the relative frequencies of the different nucleotides different in CpG islands compared to elsewhere?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>freqIsl = alphabetFrequency(seqCGI, baseOnly = TRUE, collapse = TRUE)[1:4]
freqIsl / sum(freqIsl)__


        A         C         G         T 
0.1781693 0.3201109 0.3206298 0.1810901 


freqNon = alphabetFrequency(seqNonCGI, baseOnly = TRUE, collapse = TRUE)[1:4]
freqNon / sum(freqNon)__


        A         C         G         T 
0.3008292 0.1993832 0.1993737 0.3004139 </code></pre>
<p>This shows an inverse pattern: in the CpG islands, C and G have frequencies around 0.32, whereas in the non-CpG islands, we have A and T that have frequencies around 0.30.</p>
<p>__</p>
<p>Question 2.26</p>
<p>How can we use these differences to decide whether a given sequence comes from a CpG island?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Use a \(^2\) statistic to compare the frequencies between the observed and <code>freqIsl</code> and <code>freqNon</code> frequencies. For shorter sequences, this may not be sensitive enough, and a more sensitive approach is given below.</p>
<p>Given a sequence for which we do not know whether it is in a CpG island or not, we can ask what is the probability it belongs to a CpG island compared to somewhere else. We compute a score based on what is called the odds ratio. Let’s do an example: suppose our sequence \(x\) is <code>ACGTTATACTACG</code>, and we want to decide whether it comes from a CpG island or not.</p>
<p>If we model the sequence as a first order Markov chain we can write, supposing that the sequence comes from a CpG island:</p>
<p>\[ <span class="math display">\[\begin{align} P_{\text{i}}(x = \mathtt{ACGTTATACTACG}) = \;
&amp;P_{\text{i}}(\mathtt{A}) \, P_{\text{i}}(\mathtt{AC})\,
P_{\text{i}}(\mathtt{CG})\, P_{\text{i}}(\mathtt{GT})\,
P_{\text{i}}(\mathtt{TT}) \times \\\ &amp;P_{\text{i}}(\mathtt{TA})\,
P_{\text{i}}(\mathtt{AT})\, P_{\text{i}}(\mathtt{TA})\,
P_{\text{i}}(\mathtt{AC})\, P_{\text{i}}(\mathtt{CG}). \end{align}\]</span> \]</p>
<p>We are going to compare this probability to the probability for non-islands. As we saw above, these probabilities tend to be quite different. We will take their ratio and see if it is larger or smaller than 1. These probabilities are going to be products of many small terms and become very small. We can work around this by taking logarithms.</p>
<p>\[ <span class="math display">\[\begin{align} \log&amp;\frac{P(x\,|\, \text{island})}{P(x\,|\,\text{non-
island})}=\\\ \log&amp;\left( \frac{P_{\text{i}}(\mathtt{A})\,
P_{\text{i}}(\mathtt{A}\rightarrow \mathtt{C})\,
P_{\text{i}}(\mathtt{C}\rightarrow \mathtt{G})\,
P_{\text{i}}(\mathtt{G}\rightarrow \mathtt{T})\,
P_{\text{i}}(\mathtt{T}\rightarrow \mathtt{T})\,
P_{\text{i}}(\mathtt{T}\rightarrow \mathtt{A})} {P_{\text{n}}(\mathtt{A})\,
P_{\text{n}}(\mathtt{A}\rightarrow \mathtt{C})\,
P_{\text{n}}(\mathtt{C}\rightarrow \mathtt{G})\,
P_{\text{n}}(\mathtt{G}\rightarrow \mathtt{T})\, P_{\text{n}}(
\mathtt{T}\rightarrow \mathtt{T})\, P_{\text{n}}( \mathtt{T}\rightarrow
\mathtt{A})} \right. \times\\\ &amp;\left.
\frac{P_{\text{i}}(\mathtt{A}\rightarrow \mathtt{T})\,
P_{\text{i}}(\mathtt{T}\rightarrow \mathtt{A})\,
P_{\text{i}}(\mathtt{A}\rightarrow \mathtt{C})\,
P_{\text{i}}(\mathtt{C}\rightarrow \mathtt{G})}
{P_{\text{n}}(\mathtt{A}\rightarrow \mathtt{T})\,
P_{\text{n}}(\mathtt{T}\rightarrow \mathtt{A})\,
P_{\text{n}}(\mathtt{A}\rightarrow \mathtt{C})\,
P_{\text{n}}(\mathtt{C}\rightarrow \mathtt{G})} \right) \end{align}\]</span> \]</p>
<p>This is the <strong>log-likelihood ratio</strong> score. To speed up the calculation, we compute the log-ratios \((P_{}()/P_{}()),…, (P_{}( )/P_{}())\) once and for all and then sum up the relevant ones to obtain our score.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/book_icon.png" title="Worked out examples and many useful details can be found in @DEKM."><img src="imgs/book_icon.png" class="img-fluid figure-img"></a></p>
<figcaption>Worked out examples and many useful details can be found in Durbin et al. (1998).</figcaption>
</figure>
</div>
<p>Worked out examples and many useful details can be found in Durbin et al. (<a href="16-chap.html#ref-DEKM">1998</a>).</p>
<pre><code>alpha = log((freqIsl/sum(freqIsl)) / (freqNon/sum(freqNon)))
beta  = log(MI / MN)__


x = "ACGTTATACTACG"
scorefun = function(x) {
  s = unlist(strsplit(x, ""))
  score = alpha[s[1]]
  if (length(s) &gt;= 2)
    for (j in 2:length(s))
      score = score + beta[s[j-1], s[j]]
  score
}
scorefun(x)__


         A 
-0.2824623 </code></pre>
<p>In the code below, we pick sequences of length <code>len = 100</code> out of the 2855 sequences in the <code>seqCGI</code> object, and then out of the 2854 sequences in the <code>seqNonCGI</code> object (each of them is a <em>DNAStringSet</em>). In the first three lines of the <code>generateRandomScores</code> function, we drop sequences that contain any letters other than A, C, T, G; such as “.” (a character used for undefined nucleotides). Among the remaining sequences, we sample with probabilities proportional to their length minus <code>len</code> and then pick subsequences of length <code>len</code> out of them. The start points of the subsequences are sampled uniformly, with the constraint that the subsequences have to fit in.</p>
<pre><code>generateRandomScores = function(s, len = 100, B = 1000) {
  alphFreq = alphabetFrequency(s)
  isGoodSeq = rowSums(alphFreq[, 5:ncol(alphFreq)]) == 0
  s = s[isGoodSeq]
  slen = sapply(s, length)
  prob = pmax(slen - len, 0)
  prob = prob / sum(prob)
  idx  = sample(length(s), B, replace = TRUE, prob = prob)
  ssmp = s[idx]
  start = sapply(ssmp, function(x) sample(length(x) - len, 1))
  scores = sapply(seq_len(B), function(i)
    scorefun(as.character(ssmp[[i]][start[i]+(1:len)]))
  )
  scores / len
}
scoresCGI    = generateRandomScores(seqCGI)
scoresNonCGI = generateRandomScores(seqNonCGI)__


rgs = range(c(scoresCGI, scoresNonCGI))
br = seq(rgs[1], rgs[2], length.out = 50)
h1 = hist(scoresCGI,    breaks = br, plot = FALSE)
h2 = hist(scoresNonCGI, breaks = br, plot = FALSE)
plot(h1, col = rgb(0, 0, 1, 1/4), xlim = c(-0.5, 0.5), ylim=c(0,120))
plot(h2, col = rgb(1, 0, 0, 1/4), add = TRUE)__</code></pre>
<p><a href="02-chap_files/figure-html/fig-ScoreMixture-1-1.png &quot;Figure 2.27: Island and non-island scores as generated by the function generateRandomScores. This is the first instance of a mixture we encounter. We will revisit them in sec-mixtures.&quot;"><img src="02-chap_files/figure-html/fig- ScoreMixture-1-1.png" class="img-fluid"></a></p>
<p>Figure 2.27: Island and non-island scores as generated by the function <code>generateRandomScores</code>. This is the first instance of a <strong>mixture</strong> we encounter. We will revisit them in <a href="04-chap.html">Chapter 4</a>.</p>
<p>We can consider these our <em>training data</em> : from data for which we know the types, we can see whether our score is useful for discriminating – see Figure 2.27.</p>
</section>
</section>
<section id="summary-of-this-chapter" class="level2" data-number="4.10">
<h2 data-number="4.10" class="anchored" data-anchor-id="summary-of-this-chapter"><span class="header-section-number">4.10</span> 2.11 Summary of this chapter</h2>
<p>In this chapter we experienced the basic yoga of statistics: how to go from the data back to the possible generating distributions and how to estimate the parameters that define these distributions.</p>
<p><strong>Statistical models</strong> We showed some specific statistical models for experiments with categorical outcomes (binomial and multinomial).</p>
<p><strong>Goodness of fit</strong> We used different visualizations and showed how to run simulation experiments to test whether our data could be fit by a fair four box multinomial model. We encountered the chi-square statistic and saw how to compare simulation and theory using a qq-plot.</p>
<p><strong>Estimation</strong> We explained maximum likelihood and Bayesian estimation procedures. These approaches were illustrated on examples involving nucleotide pattern discovery and haplotype estimations.</p>
<p><strong>Prior and posterior distributions</strong> When assessing data of a type that has been been previously studied, such as haplotypes, it can be beneficial to compute the posterior distribution of the data. This enables us to incorporate uncertainty in the decision making, by way of a simple computation. The choice of the prior has little effect on the result as long as there is sufficient data.</p>
<p><strong>CpG islands and Markov chains</strong> We saw how dependencies along DNA sequences can be modeled by Markov chain transitions. We used this to build scores based on likelihood ratios that enable us to see whether long DNA sequences come from CpG islands or not. When we made the histogram of scores, we saw in Figure 2.27 a noticeable feature: it seemed to be made of two pieces. This <strong>bimodality</strong> was our first encounter with mixtures, they are the subject of <a href="04-chap.html">Chapter 4</a>.</p>
<p>This is the first instance of building a model on some training data: sequences which we knew were in CpG islands, that we could use later to classify new data. We will develop a much more complete way of doing this in <a href="12-chap.html">Chapter 12</a>.</p>
</section>
<section id="further-reading" class="level2" data-number="4.11">
<h2 data-number="4.11" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">4.11</span> 2.12 Further reading</h2>
<p>One of the best introductory statistics books available is Freedman, Pisani, and Purves (<a href="16-chap.html#ref-Freedman:1997">1997</a>). It uses box models to explain the important concepts. If you have never taken a statistics class, or you feel you need a refresher, we highly recommend it. Many introductory statistics classes do not cover statistics for discrete data in any depth. The subject is an important part of what we need for biological applications. A book-long introduction to these types of analyses can be found in (<a href="16-chap.html#ref-Agresti:2007">Agresti 2007</a>).</p>
<p>Here we gave examples of simple unstructured multinomials. However, sometimes the categories (or boxes) of a multinomial have specific structure. For instance, the 64 possible codons code for 20 amino acids and the stop codons (61+3). So we can see the amino acids themselves as a multinomial with 20 degrees of freedom. Within each amino acid there are multinomials with differing numbers of categories (Proline has four: <code>CCA, CCG, CCC, CCT</code>, see Exercise 2.3). Some multivariate methods have been specifically designed to decompose the variability between codon usage within the differently abundant amino-acids (<a href="16-chap.html#ref-Grantham1981">Grantham et al.&nbsp;1981</a>; <a href="16-chap.html#ref-Perriere2002">Perrière and Thioulouse 2002</a>), and this enables discovery of latent gene transfer and translational selection. We will cover the specific methods used in those papers when we delve into the multivariate exploration of categorical data in <a href="09-chap.html">Chapter 9</a>.</p>
<p>There are many examples of successful uses of the Bayesian paradigm to quantify uncertainties. In recent years the computation of the posterior distribution has been revolutionized by special types of Monte Carlo that use either a Markov chain or random walk or Hamiltonian dynamics. These methods provide approximations that converge to the correct posterior distribution after quite a few iterations. For examples and much more see (<a href="16-chap.html#ref-Casella2009">Robert and Casella 2009</a>; <a href="16-chap.html#ref-Marin2007">Marin and Robert 2007</a>; <a href="16-chap.html#ref- McElreath2015">McElreath 2015</a>).</p>
</section>
<section id="exercises" class="level2" data-number="4.12">
<h2 data-number="4.12" class="anchored" data-anchor-id="exercises"><span class="header-section-number">4.12</span> 2.13 Exercises</h2>
<p>__</p>
<p>Exercise 2.1</p>
<p>Generate 1,000 random 0/1 variables that model mutations occurring along a 1,000 long gene sequence. These occur independently at a rate of \(10^{-4}\) each. Then sum the 1,000 positions to count how many mutations in sequences of length 1,000.</p>
<p>Find the correct distribution for these mutation sums using a goodness of fit test and make a plot to visualize the quality of the fit.</p>
<p>__</p>
<p>Exercise 2.2</p>
<p>Make a function that generates \(n\) random uniform numbers between \(0\) and \(7\) and returns their maximum. Execute the function for \(n=25\). Repeat this procedure \(B=100\) times. Plot the distribution of these maxima.<br>
What is the maximum likelihood estimate of the maximum of a sample of size 25 (call it \(\))?<br>
Can you find a theoretical justification and the true maximum \(\)?</p>
<p>__</p>
<p>Exercise 2.3</p>
<p>A sequence of three nucleotides (a <strong>codon</strong>) taken in a coding region of a gene can be transcribed into one of 20 possible amino acids. There are \(4^3=64\) possible codon sequences, but only 20 amino acids. We say the <strong>genetic code</strong> is redundant: there are several ways to <em>spell</em> each amino acid.</p>
<p>The multiplicity (the number of codons that code for the same amino acid) varies from 2 to 6. The different codon-spellings of each amino acid do not occur with equal probabilities. Let’s look at the data for the standard laboratory strain of tuberculosis (H37Rv):</p>
<pre><code>mtb = read.table("../data/M_tuberculosis.txt", header = TRUE)
head(mtb, n = 4)__


  AmAcid Codon Number PerThous
1    Gly   GGG  25874    19.25
2    Gly   GGA  13306     9.90
3    Gly   GGT  25320    18.84
4    Gly   GGC  68310    50.82</code></pre>
<p>The codons for the amino acid proline are of the form \(CC*\), and they occur with the following frequencies in Mycobacterium turberculosis:</p>
<pre><code>pro  =  mtb[ mtb$AmAcid == "Pro", "Number"]
pro/sum(pro)__


[1] 0.54302025 0.10532985 0.05859765 0.29305225</code></pre>
<ol type="1">
<li><p>Explore the data <code>mtb</code> using <code>table</code> to tabulate the <code>AmAcid</code> and <code>Codon</code> variables.</p></li>
<li><p>How was the <code>PerThous</code> variable created?</p></li>
<li><p>Write an R function that you can apply to the table to find which of the amino acids shows the strongest <strong>codon bias</strong> , i.e., the strongest departure from uniform distribution among its possible spellings.</p></li>
</ol>
<p>\(*\) stands for any of the 4 letters, using the computer notation for a regular expression.</p>
<p>__</p>
<p>Exercise 2.4</p>
<p>Display GC content in a running window along the sequence of <em>Staphylococcus Aureus</em>. Read in a <em>fasta</em> file sequence from a file.</p>
<pre><code>staph = readDNAStringSet("../data/staphsequence.ffn.txt", "fasta")__</code></pre>
<ol type="1">
<li><p>Look at the complete <code>staph</code> object and then display the first three sequences in the set.</p></li>
<li><p>Find the GC content along the sequence in sliding windows of width 100.</p></li>
<li><p>Display the result of b).</p></li>
<li><p>How could we visualize the overall trends of these proportions along the sequence?</p></li>
</ol>
<p>__</p>
<p>Solution</p>
<p>__</p>
<ol type="1">
<li>The data is displayed using:</li>
</ol>
<pre><code>staph[1:3, ]__


DNAStringSet object of length 3:
    width seq                                               names               
[1]  1362 ATGTCGGAAAAAGAAATTTGGGA...AAAAAGAAATAAGAAATGTATAA lcl|NC_002952.2_c...
[2]  1134 ATGATGGAATTCACTATTAAAAG...TTTTACCAATCAGAACTTACTAA lcl|NC_002952.2_c...
[3]   246 GTGATTATTTTGGTTCAAGAAGT...TCATTCATCAAGGTGAACAATGA lcl|NC_002952.2_c...


staph __


DNAStringSet object of length 2650:
       width seq                                            names               
   [1]  1362 ATGTCGGAAAAAGAAATTTGGG...AAAGAAATAAGAAATGTATAA lcl|NC_002952.2_c...
   [2]  1134 ATGATGGAATTCACTATTAAAA...TTACCAATCAGAACTTACTAA lcl|NC_002952.2_c...
   [3]   246 GTGATTATTTTGGTTCAAGAAG...ATTCATCAAGGTGAACAATGA lcl|NC_002952.2_c...
   [4]  1113 ATGAAGTTAAATACACTCCAAT...CAAGGTGAAATTATAAAGTAA lcl|NC_002952.2_c...
   [5]  1932 GTGACTGCATTGTCAGATGTAA...TATGCAAACTTAGACTTCTAA lcl|NC_002952.2_c...
   ...   ... ...
[2646]   720 ATGACTGTAGAATGGTTAGCAG...ACTCCTTTACTTGAAAAATAA lcl|NC_002952.2_c...
[2647]  1878 GTGGTTCAAGAATATGATGTAA...CTCCAAAGGGTGAGTGACTAA lcl|NC_002952.2_c...
[2648]  1380 ATGGATTTAGATACAATTACGA...CAATTCTGCTTAGGTAAATAG lcl|NC_002952.2_c...
[2649]   348 TTGGAAAAAGCTTACCGAATTA...TTTAATAAAAAGATTAAGTAA lcl|NC_002952.2_c...
[2650]   138 ATGGTAAAACGTACTTATCAAC...CGTAAAGTTTTATCTGCATAA lcl|NC_002952.2_c...</code></pre>
<ol start="2" type="1">
<li>We can compute the frequencies using the function <code>letterFrequency</code>.</li>
</ol>
<pre><code>letterFrequency(staph[[1]], letters = "ACGT", OR = 0)__


  A   C   G   T 
522 219 229 392 


GCstaph = data.frame(
  ID = names(staph),
  GC = rowSums(alphabetFrequency(staph)[, 2:3] / width(staph)) * 100
)__</code></pre>
<ol start="3" type="1">
<li>Plotting can be done as follows, here exemplarily for sequence 364 (Figure 2.28):</li>
</ol>
<pre><code>window = 100
gc = rowSums( letterFrequencyInSlidingView(staph[[364]], window,
      c("G","C")))/window
plot(x = seq(along = gc), y = gc, type = "l")__</code></pre>
<p><a href="02-chap_files/figure- html/fig-SlidingGC-1-1.png" title="Figure 2.28: GC content along sequence 364 of the Staphylococcus Aureus genome."><img src="02-chap_files/figure-html/fig-SlidingGC-1-1.png" class="img-fluid"></a></p>
<p>Figure 2.28: GC content along sequence 364 of the <em>Staphylococcus Aureus</em> genome.</p>
<ol start="4" type="1">
<li>We can look at the overall trends by smoothing the data using the function <code>lowess</code> along a window.</li>
</ol>
<pre><code>plot(x = seq(along = gc), y = gc, type = "l")
lines(lowess(x = seq(along = gc), y = gc, f = 0.2), col = 2)__</code></pre>
<p><a href="02-chap_files/figure-html/fig- SmoothSlidingGC-1-1.png" title="Figure 2.29: Similar to Figure fig-SlidingGC-1, with smoothing."><img src="02-chap_files/figure-html/fig- SmoothSlidingGC-1-1.png" class="img-fluid"></a></p>
<p>Figure 2.29: Similar to Figure 2.28, with smoothing.</p>
<p>We will see later an appropriate way of deciding whether the window has an abnormally high GC content by using the idea that as we move along the sequences, we are always in one of several possible <strong>states</strong>. However, we don’t directly observe the state, just the sequence. Such models are called <strong>hidden (state) Markov models</strong> , or HMM for short (see <a href="http://en.wikipedia.org/wiki/Hidden_Markov_model">Wikipedia</a>). The <em>Markov</em> in the name of these models is for how they model dependencies between neighboring positions, the <em>hidden</em> part indicates that the state is not directly observed, that is, hidden.</p>
<p>__</p>
<p>Exercise 2.5</p>
<p>Redo a figure similar to Figure 2.19, but include two other distributions: the uniform (which is Beta(1,1)) and Beta(\(,\)). What do you notice?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>dfbetas = data.frame(
  p = rep(p_seq, 5),
  dbeta = c(dbeta(p_seq, 0.5, 0.5), 
            dbeta(p_seq,   1,   1), 
            dbeta(p_seq,  10,  30),
            dbeta(p_seq,  20,  60), 
            dbeta(p_seq,  50, 150)),
  pars = rep(c("Beta(0.5,0.5)", "U(0,1)=Beta(1,1)", 
               "Beta(10,30)", "Beta(20,60)", 
               "Beta(50,150)"), each = length(p_seq)))
ggplot(dfbetas) +
  geom_line(aes(x = p, y = dbeta, colour = pars)) +
  theme(legend.title = element_blank()) +
  geom_vline(aes(xintercept = 0.25), colour = "#990000", linetype = "dashed")__</code></pre>
<p><a href="02-chap_files/figure- html/fig-histobeta4-1-1.png" title="Figure 2.30: Beta densities for different parameter choices."><img src="02-chap_files/figure-html/fig-histobeta4-1-1.png" class="img-fluid"></a></p>
<p>Figure 2.30: Beta densities for different parameter choices.</p>
<p>Whereas the Beta distributions with parameters larger than one are unimodal, the Beta(0.5,0.5) distribution is bimodal and the Beta(1,1) is flat and has no mode.</p>
<p>__</p>
<p>Exercise 2.6</p>
<p>Choose your own prior for the parameters of the Beta distribution. You can do this by sketching it here: <a href="https://jhubiostatistics.shinyapps.io/drawyourprior" class="uri">https://jhubiostatistics.shinyapps.io/drawyourprior</a>. Once you have set up a prior, re-analyse the data from Section 2.9.1, where we saw \(Y = 40\) successes out of \(n=300\) trials. Compare your posterior distribution to the one we obtained in that section using a QQ-plot.</p>
<p>Agresti, Alan. 2007. <em>An Introduction to Categorical Data Analysis</em>. John Wiley.</p>
<p>Cannings, Chris, and Anthony WF Edwards. 1968. “Natural Selection and the de Finetti Diagram.” <em>Annals of Human Genetics</em> 31 (4): 421–28.</p>
<p>Cleveland, William S. 1988. <em>The Collected Works of John w. Tukey: Graphics 1965-1985</em>. Vol. 5. CRC Press.</p>
<p>Durbin, Richard, Sean Eddy, Anders Krogh, and Graeme Mitchison. 1998. <em>Biological Sequence Analysis</em>. Cambridge University Press.</p>
<p>Elson, D, and E Chargaff. 1952. “On the Desoxyribonucleic Acid Content of Sea Urchin Gametes.” <em>Experientia</em> 8 (4): 143–45.</p>
<p>Finetti, Bruno de. 1926. “Considerazioni Matematiche Sull’ereditarieta Mendeliana.” <em>Metron</em> 6: 3–41.</p>
<p>Freedman, David, Robert Pisani, and Roger Purves. 1997. <em>Statistics</em>. New York, NY: WW Norton.</p>
<p>Grantham, Richard, Christian Gautier, Manolo Gouy, M Jacobzone, and R Mercier. 1981. “Codon Catalog Usage Is a Genome Strategy Modulated for Gene Expressivity.” <em>Nucleic Acids Research</em> 9 (1): 213–13.</p>
<p>Irizarry, Rafael A, Hao Wu, and Andrew P Feinberg. 2009. “A Species- Generalized Probabilistic Model-Based Definition of CpG Islands.” <em>Mammalian Genome</em> 20 (9-10): 674–80.</p>
<p>Love, Michael I, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation of Fold Change and Dispersion for RNA-seq Data with DESeq2.” <em>Gnome Biology</em> 15 (12): 1–21.</p>
<p>Marin, Jean-Michel, and Christian Robert. 2007. <em>Bayesian Core: A Practical Approach to Computational Bayesian Statistics</em>. Springer Science &amp; Business Media.</p>
<p>McElreath, Richard. 2015. <em>Statistical Rethinking: A Bayesian Course with Examples in R and Stan</em>. Chapman; Hall/CRC.</p>
<p>Mourant, AE, Ada Kopec, and K Domaniewska-Sobczak. 1976. “The Distribution of the Human Blood Groups 2nd Edition.” Oxford University Press London.</p>
<p>Perrière, Guy, and Jean Thioulouse. 2002. “Use and Misuse of Correspondence Analysis in Codon Usage Studies.” <em>Nucleic Acids Research</em> 30 (20): 4548–55.</p>
<p>Rice, John. 2006. <em>Mathematical Statistics and Data Analysis</em>. Cengage Learning.</p>
<p>Robert, Christian, and George Casella. 2009. <em>Introducing Monte Carlo Methods with R</em>. Springer Science &amp; Business Media.</p>
<p>Page built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./01-chap.html" class="pagination-link" aria-label="1.1 Goals for this chapter">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">1.1 Goals for this chapter</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./03-chap.html" class="pagination-link" aria-label="3.1 Goals for this chapter">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">3.1 Goals for this chapter</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>