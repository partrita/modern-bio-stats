<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>7&nbsp; 5.1 이 장의 목표 – Modern Statistics for Modern Biology</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./06-chap.html" rel="next">
<link href="./04-chap.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-7bf12d62aa84b4fa538b342f1416a45b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="msmb.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./05-chap.html"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">5.1 이 장의 목표</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Modern Statistics for Modern Biology</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">홈</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">주제: 이질성</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">1.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">2.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">3.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">4.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-chap.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">5.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">6.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">7.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">8.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">9.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">10.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">11.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">12.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">13.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">14-chap.html</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">15-chap.html</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">16-chap.html</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#데이터란-무엇이며-왜-군집화하는가" id="toc-데이터란-무엇이며-왜-군집화하는가" class="nav-link active" data-scroll-target="#데이터란-무엇이며-왜-군집화하는가"><span class="header-section-number">7.1</span> 5.2 데이터란 무엇이며 왜 군집화하는가?</a>
  <ul class="collapse">
  <li><a href="#군집화는-때때로-발견으로-이어질-수-있습니다." id="toc-군집화는-때때로-발견으로-이어질-수-있습니다." class="nav-link" data-scroll-target="#군집화는-때때로-발견으로-이어질-수-있습니다."><span class="header-section-number">7.1.1</span> 5.2.1 군집화는 때때로 발견으로 이어질 수 있습니다.</a></li>
  </ul></li>
  <li><a href="#how-do-we-measure-similarity" id="toc-how-do-we-measure-similarity" class="nav-link" data-scroll-target="#how-do-we-measure-similarity"><span class="header-section-number">7.2</span> 5.3 How do we measure similarity?</a>
  <ul class="collapse">
  <li><a href="#computations-related-to-distances-in-r" id="toc-computations-related-to-distances-in-r" class="nav-link" data-scroll-target="#computations-related-to-distances-in-r"><span class="header-section-number">7.2.1</span> 5.3.1 Computations related to distances in R</a></li>
  </ul></li>
  <li><a href="#nonparametric-mixture-detection" id="toc-nonparametric-mixture-detection" class="nav-link" data-scroll-target="#nonparametric-mixture-detection"><span class="header-section-number">7.3</span> 5.4 Nonparametric mixture detection</a>
  <ul class="collapse">
  <li><a href="#k-methods-k-means-k-medoids-and-pam" id="toc-k-methods-k-means-k-medoids-and-pam" class="nav-link" data-scroll-target="#k-methods-k-means-k-medoids-and-pam"><span class="header-section-number">7.3.1</span> 5.4.1 \(k\)-methods: \(k\)-means, \(k\)-medoids and PAM</a></li>
  <li><a href="#tight-clusters-with-resampling" id="toc-tight-clusters-with-resampling" class="nav-link" data-scroll-target="#tight-clusters-with-resampling"><span class="header-section-number">7.3.2</span> 5.4.2 Tight clusters with resampling</a></li>
  </ul></li>
  <li><a href="#군집화-예시-유세포-분석-및-질량-분석" id="toc-군집화-예시-유세포-분석-및-질량-분석" class="nav-link" data-scroll-target="#군집화-예시-유세포-분석-및-질량-분석"><span class="header-section-number">7.4</span> 5.5 군집화 예시: 유세포 분석 및 질량 분석</a>
  <ul class="collapse">
  <li><a href="#유세포-분석-및-질량-분석" id="toc-유세포-분석-및-질량-분석" class="nav-link" data-scroll-target="#유세포-분석-및-질량-분석"><span class="header-section-number">7.4.1</span> 5.5.1 유세포 분석 및 질량 분석</a></li>
  <li><a href="#데이터-전처리" id="toc-데이터-전처리" class="nav-link" data-scroll-target="#데이터-전처리"><span class="header-section-number">7.4.2</span> 5.5.2 데이터 전처리</a></li>
  <li><a href="#밀도-기반-군집화density-based-clustering" id="toc-밀도-기반-군집화density-based-clustering" class="nav-link" data-scroll-target="#밀도-기반-군집화density-based-clustering"><span class="header-section-number">7.4.3</span> 5.5.3 밀도 기반 군집화(Density-based clustering)</a></li>
  </ul></li>
  <li><a href="#계층적-군집화hierarchical-clustering" id="toc-계층적-군집화hierarchical-clustering" class="nav-link" data-scroll-target="#계층적-군집화hierarchical-clustering"><span class="header-section-number">7.5</span> 5.6 계층적 군집화(Hierarchical clustering)</a>
  <ul class="collapse">
  <li><a href="#집계된-클러스터-간의-비유사성을-어떻게-계산하는가" id="toc-집계된-클러스터-간의-비유사성을-어떻게-계산하는가" class="nav-link" data-scroll-target="#집계된-클러스터-간의-비유사성을-어떻게-계산하는가"><span class="header-section-number">7.5.1</span> 5.6.1 집계된 클러스터 간의 (비)유사성을 어떻게 계산하는가?</a></li>
  </ul></li>
  <li><a href="#클러스터-수-검증-및-선택" id="toc-클러스터-수-검증-및-선택" class="nav-link" data-scroll-target="#클러스터-수-검증-및-선택"><span class="header-section-number">7.6</span> 5.7 클러스터 수 검증 및 선택</a>
  <ul class="collapse">
  <li><a href="#갭-통계량gap-statistic-사용하기" id="toc-갭-통계량gap-statistic-사용하기" class="nav-link" data-scroll-target="#갭-통계량gap-statistic-사용하기"><span class="header-section-number">7.6.1</span> 5.7.1 갭 통계량(gap statistic) 사용하기</a></li>
  <li><a href="#붓스트랩을-이용한-클러스터-검증" id="toc-붓스트랩을-이용한-클러스터-검증" class="nav-link" data-scroll-target="#붓스트랩을-이용한-클러스터-검증"><span class="header-section-number">7.6.2</span> 5.7.2 붓스트랩을 이용한 클러스터 검증</a></li>
  </ul></li>
  <li><a href="#노이즈-제거-수단으로서의-군집화" id="toc-노이즈-제거-수단으로서의-군집화" class="nav-link" data-scroll-target="#노이즈-제거-수단으로서의-군집화"><span class="header-section-number">7.7</span> 5.8 노이즈 제거 수단으로서의 군집화</a>
  <ul class="collapse">
  <li><a href="#서로-다른-베이스라인-빈도를-가진-노이즈-섞인-관측치" id="toc-서로-다른-베이스라인-빈도를-가진-노이즈-섞인-관측치" class="nav-link" data-scroll-target="#서로-다른-베이스라인-빈도를-가진-노이즈-섞인-관측치"><span class="header-section-number">7.7.1</span> 5.8.1 서로 다른 베이스라인 빈도를 가진 노이즈 섞인 관측치</a></li>
  <li><a href="#denoising-16s-rrna-sequences" id="toc-denoising-16s-rrna-sequences" class="nav-link" data-scroll-target="#denoising-16s-rrna-sequences"><span class="header-section-number">7.7.2</span> 5.8.2 Denoising 16S rRNA sequences</a></li>
  <li><a href="#infer-sequence-variants" id="toc-infer-sequence-variants" class="nav-link" data-scroll-target="#infer-sequence-variants"><span class="header-section-number">7.7.3</span> 5.8.3 Infer sequence variants</a></li>
  </ul></li>
  <li><a href="#summary-of-this-chapter" id="toc-summary-of-this-chapter" class="nav-link" data-scroll-target="#summary-of-this-chapter"><span class="header-section-number">7.8</span> 5.9 Summary of this chapter</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">7.9</span> 5.10 Further reading</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">7.10</span> 5.11 Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">5.1 이 장의 목표</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><img src="imgs/starlings_copyrightfree.jpg" class="img-fluid"></p>
<p>세포, 질병, 유기체의 범주를 찾고 그 이름을 짓는 것은 자연과학의 핵심 활동입니다. <a href="04-chap.html">4장</a>에서 우리는 일부 데이터를 명확한 모수적 생성 모델을 가진 서로 다른 그룹이나 모집단의 혼합물로 모델링할 수 있음을 보았습니다. 우리는 그러한 예제들에서 EM 알고리즘을 사용하여 성분들을 어떻게 분리할 수 있는지 보았습니다. 이제 우리는 <strong>클러스터(clusters)</strong>가 반드시 예쁜 타원형1 모양을 가질 필요는 없는 경우로 그룹들을 풀어내는 아이디어를 확장해 보려 합니다.</p>
<p>1 다변량 정규 분포를 이용한 혼합 모델링은 타원형 클러스터 경계를 함축합니다.</p>
<p>군집화(Clustering)는 데이터(연속형 또는 준연속형)를 가져와서 새로운 범주형 <em>그룹</em> 변수를 추가하며, 이는 때때로 중간 상태를 무시하는 대가를 치르더라도 의사 결정을 단순화할 수 있습니다. 예를 들어, 공복 혈당, 당화혈색소, 섭취 2시간 후 혈장 포도당 수치와 관련된 복잡한 고차원 진단 수치들을 단순히 환자를 당뇨병 “그룹”에 할당함으로써 의료 결정을 단순화합니다.</p>
<p>이 장에서 우리는 저차원 및 고차원 <strong>비모수적(nonparametric)</strong> 환경 모두에서 의미 있는 클러스터나 그룹을 찾는 방법을 공부할 것입니다. 그러나 주의할 점이 있습니다: 군집화 알고리즘은 클러스터를 찾도록 설계되었으므로, 클러스터가 없는 곳에서도 클러스터를 찾아낼 것입니다2. 따라서 클러스터의 존재를 뒷받침하는 사전 지식이 없는 경우, 클러스터 _검증(validation)_은 우리 프로세스의 필수적인 구성 요소입니다.</p>
<p>2 이는 인간을 연상시킵니다: 우리는 무작위성 속에서도 패턴을 보기를 좋아합니다.</p>
<p>이 장에서 우리는 다음을 수행할 것입니다:</p>
<ul>
<li><p>유익하게 군집화될 수 있는 다양한 유형의 데이터를 공부합니다.</p></li>
<li><p>클러스터를 정의하는 데 도움이 되는 (비)유사성 척도와 <strong>거리(distances)</strong>를 살펴봅니다.</p></li>
<li><p>데이터를 더 <em>조밀한</em> 세트로 분할하여 숨겨진 또는 잠재된 군집을 찾아냅니다.</p></li>
<li><p>수십만 개의 세포 각각에 대한 바이오마커가 주어졌을 때 군집화를 사용해 봅니다. 예를 들어 면역 세포가 어떻게 자연스럽게 조밀한 하위 집단으로 그룹화될 수 있는지 보게 될 것입니다.</p></li>
<li><p>실제 단일 세포 데이터에 <strong>\(k\) -평균(\(k\)-means)</strong>, <strong>\(k\) -메도이드(\(k\)-medoids)</strong>와 같은 비모수적 알고리즘을 실행합니다.</p></li>
<li><p>관측치와 그룹을 세트의 계층 구조로 결합하는 군집화에 대한 재귀적 접근 방식을 실험합니다. 이러한 방법은 <strong>계층적 군집화(hierarchical clustering)</strong>로 알려져 있습니다.</p></li>
<li><p>재표본 추출 기반의 붓스트랩 접근 방식을 통해 클러스터를 검증하는 방법을 공부하며, 이를 단일 세포 데이터 세트에서 시연할 것입니다.</p></li>
</ul>
<p><a href="imgs/SnowMapSmallest_web.png" title="그림 5.1: 존 스노우의 콜레라 사례 지도: 각 집의 작은 막대 그래프는 진단된 사례들의 군집을 나타냅니다."><img src="imgs/SnowMapSmallest_web.png" class="img-fluid"></a></p>
<p>그림 5.1: 존 스노우의 콜레라 사례 지도: 각 집의 작은 막대 그래프는 진단된 사례들의 군집을 나타냅니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/book_icon.png" title="David Freedman은 이 발견으로 이어진 모든 단계에 대해 훌륭하고 상세한 설명을 남겼습니다 [@freedman1991]."><img src="imgs/book_icon.png" class="img-fluid figure-img"></a></p>
<figcaption>David Freedman은 이 발견으로 이어진 모든 단계에 대해 훌륭하고 상세한 설명을 남겼습니다 (Freedman 1991).</figcaption>
</figure>
</div>
<p>David Freedman은 이 발견으로 이어진 모든 단계에 대해 훌륭하고 상세한 설명을 남겼습니다 (<a href="16-chap.html#ref-freedman1991">Freedman 1991</a>).</p>
<section id="데이터란-무엇이며-왜-군집화하는가" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="데이터란-무엇이며-왜-군집화하는가"><span class="header-section-number">7.1</span> 5.2 데이터란 무엇이며 왜 군집화하는가?</h2>
<section id="군집화는-때때로-발견으로-이어질-수-있습니다." class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="군집화는-때때로-발견으로-이어질-수-있습니다."><span class="header-section-number">7.1.1</span> 5.2.1 군집화는 때때로 발견으로 이어질 수 있습니다.</h3>
<p>존 스노우(John Snow)는 콜레라 사례 지도를 만들고 사례들의 _클러스터_를 식별했습니다. 그런 다음 그는 펌프의 위치에 대한 추가 정보를 수집했습니다. 밀집된 사례 클러스터들이 브로드 스트리트(Broadstreet) 펌프와 가깝다는 사실은 물이 범인일 가능성을 지목했습니다. 그는 콜레라 발생의 원인을 추론할 수 있게 해주는 별도의 정보원들을 수집했습니다.</p>
<p>이제 그림 5.2에 표시된 또 다른 런던 지도를 살펴봅시다. 빨간색 점들은 제2차 세계대전 중 폭격을 받은 위치를 나타냅니다. 전쟁 중에 분석 팀들은 많은 이론을 내놓았습니다. 그들은 폭격 패턴(유틸리티 공장, 병기창과의 근접성, \(…\))에 대한 합리적인 설명을 찾으려 노력했습니다. 사실, 전쟁 후에 폭격은 특정 목표물을 타격하려는 시도 없이 무작위로 분포되었다는 것이 밝혀졌습니다.</p>
<p><a href="imgs/RedBombsLondon_web.png" title="그림 5.2: 1940년 9월 7일 런던에 떨어진 폭탄의 위치 지도입니다. 영국 국립 보존 기록관 웹사이트 http://bombsight.org에서 묘사한 것입니다."><img src="imgs/RedBombsLondon_web.png" class="img-fluid"></a></p>
<p>그림 5.2: 1940년 9월 7일 런던에 떨어진 폭탄의 위치 지도. 영국 국립 보존 기록관 웹사이트 <a href="http://bombsight.org" class="uri">http://bombsight.org</a>에서 가져온 것입니다.</p>
<p>군집화는 복잡한 다변량 데이터를 이해하는 데 유용한 기법이며, 이는 <strong>비지도(unsupervised)</strong>3 학습입니다. 탐색적 기법은 데이터를 해석하는 데 중요할 수 있는 그룹화된 모습을 보여줍니다.</p>
<p>3 모든 변수가 동일한 상태를 가지며, 설명 변수의 정보를 바탕으로 한 변수(감독 반응)의 값을 예측하거나 학습하려고 하지 않기 때문에 이렇게 불립니다.</p>
<p>예를 들어, 군집화를 통해 연구자들은 암 생물학에 대한 이해를 높일 수 있었습니다. 해부학적 위치와 조직 병리학적 소견으로는 동일해 보였던 종양들이 유전자 발현 데이터와 같은 분자적 특성에 따라 여러 클러스터로 나뉘었습니다 (<a href="16-chap.html#ref-Hallett2012">Hallett et al.&nbsp;2012</a>). 결국 이러한 군집화는 새롭고 더 적절한 질병 유형의 정의로 이어질 수 있습니다. 적절성은 예를 들어 서로 다른 환자 예후와 연관되어 있다는 사실로 입증됩니다. 이 장에서 우리가 하고자 하는 것은 그림 5.3과 같은 그림들이 어떻게 구성되는지, 그리고 어떻게 해석해야 하는지를 이해하는 것입니다.</p>
<p><a href="imgs/BreastCancerSubType_Biomed.png &quot;그림 5.3: 유방암 샘플들(The Cancer Genome Atlas (TCGA) 및 Molecular Taxonomy of Breast Cancer International Consortium (METABRIC)에서 가져옴)은 그들의 miRNA 발현을 사용하여 그룹으로 나뉠 수 있습니다 [@Aure2017]. 저자들은 하단 플롯에서 서로 다른 그룹의 생존 기간이 다르다는 것을 보여줍니다. 따라서 이러한 클러스터들은 생물학적 및 임상적으로 의미가 있었습니다. 이러한 분석의 약속은 그룹들을 사용하여 더 구체적이고 최적화된 치료를 제공할 수 있다는 것입니다.&quot;"><img src="imgs/BreastCancerSubType_Biomed.png" class="img-fluid"></a></p>
<p>그림 5.3: 유방암 샘플들(The Cancer Genome Atlas (TCGA) 및 Molecular Taxonomy of Breast Cancer International Consortium (METABRIC)에서 가져옴)은 그들의 miRNA 발현을 사용하여 그룹으로 나뉠 수 있습니다 (<a href="16-chap.html#ref-Aure2017">Aure et al.&nbsp;2017</a>). 저자들은 하단 플롯에서 서로 다른 그룹의 생존 기간이 다르다는 것을 보여줍니다. 따라서 이러한 클러스터들은 생물학적 및 임상적으로 의미가 있었습니다. 이러한 분석의 약속은 그룹들을 사용하여 더 구체적이고 최적화된 치료를 제공할 수 있다는 것입니다.</p>
<p><a href="04-chap.html">4장</a>에서 우리는 이미 그룹을 찾아내기 위한 한 가지 기법인 EM 알고리즘을 공부했습니다. 이 장에서 우리가 탐구하는 기법들은 더 일반적이며 더 복잡한 데이터에 적용될 수 있습니다. 이들 중 상당수는 관측치 쌍 사이의 거리에 기초하며(이는 전체 대 전체일 수도 있고, 때로는 전체 대 일부일 수도 있음), 정규 분포, 감마-포아송 등과 같은 특정 분포군을 포함하는 데이터의 생성 메커니즘에 대해 명시적인 가정을 하지 않습니다. 문헌과 과학 소프트웨어 분야에는 군집화 알고리즘이 넘쳐나며, 이는 위협적으로 느껴질 수 있습니다. 사실 이는 데이터 유형의 다양성과 각 분야에서 추구하는 목표의 다양성과 연결되어 있습니다.</p>
<p>__</p>
<p>태스크</p>
<p><a href="http://www.bioconductor.org/packages/release/BiocViews.html">BiocViews Clustering</a> 또는 <a href="https://cran.r-project.org/web/views/Cluster.html">CRAN의 Cluster view</a>를 찾아보고 군집화 도구를 제공하는 패키지 수를 세어 보세요.</p>
<p><a href="imgs/ClusteringA.png" title="그림 5.4: 우리는 군집화 알고리즘에서 이루어지는 선택들을 취해진 단계에 따라 분해합니다: 관측치-특성 직사각형 테이블 X에서 시작하여, 관측치 간 거리 척도를 선택하고 거리 행렬을 계산합니다(여기서는 삼각형으로 도식화됨). 거리는 클러스터를 구성하는 데 사용됩니다. 왼쪽에는 계층적 군집 트리를 구축하는 응집형(agglomerative) 방법을 도식화했습니다. 오른쪽에는 데이터를 하위 집합으로 분리하는 분할형(partitioning) 방법을 도식화했습니다. 두 유형의 방법 모두 클러스터 수 k의 선택을 필요로 합니다. k-평균과 같은 분할 접근법의 경우 이 선택은 처음에 이루어져야 합니다. 계층적 군집화의 경우 분석이 끝날 때까지 이를 미룰 수 있습니다."><img src="imgs/ClusteringA.png" class="img-fluid"></a></p>
<p>그림 5.4: 우리는 군집화 알고리즘에서 이루어지는 선택들을 취해진 단계에 따라 분해합니다: 관측치-특성 직사각형 테이블 \(X\)에서 시작하여, 관측치 간 거리 척도를 선택하고 거리 행렬을 계산합니다(여기서는 삼각형으로 도식화됨). 거리는 클러스터를 구성하는 데 사용됩니다. 왼쪽에는 계층적 군집 트리를 구축하는 응집형(agglomerative) 방법을 도식화했습니다. 오른쪽에는 데이터를 하위 집합으로 분리하는 분할형(partitioning) 방법을 도식화했습니다. 두 유형의 방법 모두 클러스터 수 \(k\)의 선택을 필요로 합니다. \(k\)-평균과 같은 분할 접근법의 경우 이 선택은 처음에 이루어져야 합니다. 계층적 군집화의 경우 분석이 끝날 때까지 이를 미룰 수 있습니다.</p>
</section>
</section>
<section id="how-do-we-measure-similarity" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="how-do-we-measure-similarity"><span class="header-section-number">7.2</span> 5.3 How do we measure similarity?</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="Of a feather: how the distances are measured and similarities between observations defined has a strong impact on the clustering result."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>Of a feather: how the distances are measured and similarities between observations defined has a strong impact on the clustering result.</figcaption>
</figure>
</div>
<p><strong>Of a feather</strong> : how the distances are measured and similarities between observations defined has a strong impact on the clustering result.</p>
<p>Our first step is to decide what we mean by <em>similar</em>. There are multiple ways of comparing birds: for instance, a distance using size and weight will give a different clustering than one using diet or habitat. Once we have chosen the relevant features, we have to decide how we combine differences between the multiple features into a single number. Here is a selection of choices, some of them are illustrated in Figure 5.5.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/FourDistances_a.png" class="img-fluid figure-img"></p>
<figcaption>Figure&nbsp;5.5&nbsp;(a):</figcaption>
</figure>
</div>
<ol type="a">
<li></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/FourDistances_b.png" class="img-fluid figure-img"></p>
<figcaption>Figure&nbsp;5.5&nbsp;(b):</figcaption>
</figure>
</div>
<ol start="2" type="a">
<li></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/FourDistances_c.png" class="img-fluid figure-img"></p>
<figcaption>Figure&nbsp;5.5&nbsp;(c):</figcaption>
</figure>
</div>
<ol start="3" type="a">
<li></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="imgs/FourDistances_d.png" class="img-fluid figure-img"></p>
<figcaption>Figure&nbsp;5.5&nbsp;(d):</figcaption>
</figure>
</div>
<ol start="4" type="a">
<li></li>
</ol>
<p>Figure 5.5: Equal-distance contour plots according to four different distances: points on any one curve are all the same distance from the center point.</p>
<p><strong>Euclidean</strong> The Euclidean distance between two points \(A=(a_1,…,a_p)\) and \(B= (b_1,…,b_p)\) in a \(p\)-dimensional space (for the \(p\) features) is the square root of the sum of squares of the differences in all \(p\) coordinate directions:</p>
<p>\[ d(A,B)=. \]</p>
<p><strong>Manhattan</strong> The Manhattan, City Block, Taxicab or \(L_1\) distance takes the sum of the absolute differences in all coordinates.</p>
<p>\[ d(A,B)=|a_1-b_1|+|a_2-b_2|+… +|a_p-b_p|. \]</p>
<p><strong>Maximum</strong> The maximum of the absolute differences between coordinates is also called the \(L_\) distance:</p>
<p>\[ d_(A,B)= _{i}|a_i-b_i|. \]</p>
<p><strong>Weighted Euclidean distance</strong> is a generalization of the ordinary Euclidean distance, by giving different directions in feature space different weights. We have already encountered one example of a weighted Euclidean distance in <a href="02-chap.html">Chapter 2</a>, the \(^2\) distance. It is used to compare rows in contingency tables, and the weight of each feature is the inverse of the expected value. The <em>Mahalanobis</em> distance is another weighted Euclidean distance that takes into account the fact that different features may have a different dynamic range, and that some features may be positively or negatively correlated with each other. The weights in this case are derived from the covariance matrix of the features. See also Question 5.1.</p>
<p><strong>Minkowski</strong> Allowing the exponent to be \(m\) instead of \(2\), as in the Euclidean distance, gives the Minkowski distance</p>
<p>\[ d(A,B) = ( (a_1-b_1)<sup>m+(a_2-b_2)</sup>m+… +(a_p-b_p)^m )^. \]</p>
<p><strong>Edit, Hamming</strong> This distance is the simplest way to compare character sequences. It simply counts the number of differences between two character strings. This could be applied to nucleotide or amino acid sequences – although in that case, the different character substitutions are usually associated with different contributions to the distance (to account for physical or evolutionary similarity), and deletions and insertions may also be allowed.</p>
<p><strong>Binary</strong> When the two vectors have binary bits as coordinates, we can think of the non-zero elements as ‘on’ and the zero elements as ‘off’. The binary distance is the proportion of features having only one bit on amongst those features that have at least one bit on.</p>
<p><strong>Jaccard Distance</strong> Occurrence of traits or features in ecological or mutation data can be translated into presence and absence and encoded as 1’s and 0’s. In such situations, co-occurence is often more informative than co- absence. For instance, when comparing mutation patterns in HIV, the co- existence in two different strains of a mutation tends to be a more important observation than its co-absence. For this reason, biologists use the <strong>Jaccard index</strong>. Let’s call our two observation vectors \(S\) and \(T\), \(f_{11}\) the number of times a feature co-occurs in \(S\) and \(T\), \(f_{10}\) (and \(f_{01}\)) the number of times a feature occurs in \(S\) but not in \(T\) (and vice versa), and \(f_{00}\) the number of times a feature is co-absent. The Jaccard index is</p>
<p>\[ J(S,T) = , \]</p>
<p>(i.e., it ignores \(f_{00}\)), and the <strong>Jaccard dissimilarity</strong> is</p>
<p>\[ d_J(S,T) = 1-J(S,T) = . \]</p>
<p><strong>Correlation based distance</strong></p>
<p>\[ d(A,B)=. \]</p>
<p><a href="05-chap_files/figure- html/fig-Mahalanobis-1.png" title="Figure 5.6: An example for the use of Mahalanobis distances to measure the distance of a new data point (red) from two cluster centers."><img src="05-chap_files/figure-html/fig-Mahalanobis-1.png" class="img-fluid"></a></p>
<p>Figure 5.6: An example for the use of Mahalanobis distances to measure the distance of a new data point (red) from two cluster centers.</p>
<p>__</p>
<p>Question 5.1</p>
<p>Which of the two cluster centers in Figure 5.6 is the red point closest to?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>A naïve answer would use the Euclidean metric and decide that the point is closer to the left cluster. However, as we see that the features have different ranges and correlations, and that these even differ between the two clusters, it makes sense to use cluster-specific Mahalanobis distances. The figure shows contour lines for both clusters. These were obtained from a density estimate; the Mahalanobis distance approximates these contours with ellipses. The distance between the red point and each of the cluster centers corresponds to the number of contour lines crossed. We see that as the group on the right is more spread out, the red point is in fact closer to it.</p>
<p><a href="imgs/DistanceTriangle.png" title="Figure 5.7: The lower triangle of distances can be computed by any of a hundred different functions in various R packages (vegdist in vegan, daisy in cluster, genetic_distance in gstudio, dist.dna in ape, Dist in amap, distance in ecodist, dist.multiPhylo in distory, shortestPath in gdistance, % dudi.dist and dist.genet in ade4)."><img src="imgs/DistanceTriangle.png" class="img-fluid"></a></p>
<p>Figure 5.7: The lower triangle of distances can be computed by any of a hundred different functions in various R packages (<code>vegdist</code> in <strong><a href="https://cran.r-project.org/web/packages/vegan/">vegan</a></strong> , <code>daisy</code> in <strong><a href="https://cran.r-project.org/web/packages/cluster/">cluster</a></strong> , <code>genetic_distance</code> in <strong><a href="https://cran.r-project.org/web/packages/gstudio/">gstudio</a></strong> , <code>dist.dna</code> in <strong><a href="https://cran.r-project.org/web/packages/ape/">ape</a></strong> , <code>Dist</code> in <strong><a href="https://cran.r-project.org/web/packages/amap/">amap</a></strong> , <code>distance</code> in <strong><a href="https://cran.r-project.org/web/packages/ecodist/">ecodist</a></strong> , <code>dist.multiPhylo</code> in <strong><a href="https://cran.r-project.org/web/packages/distory/">distory</a></strong> , <code>shortestPath</code> in <strong><a href="https://cran.r-project.org/web/packages/gdistance/">gdistance</a></strong> , % <code>dudi.dist</code> and <code>dist.genet</code> in <strong><a href="https://cran.r-project.org/web/packages/ade4/">ade4</a></strong>).</p>
<section id="computations-related-to-distances-in-r" class="level3" data-number="7.2.1">
<h3 data-number="7.2.1" class="anchored" data-anchor-id="computations-related-to-distances-in-r"><span class="header-section-number">7.2.1</span> 5.3.1 Computations related to distances in R</h3>
<p>The <code>dist</code> function in R is designed to use less space than the full \(n^2\) positions a complete \(n n\) distance matrix between \(n\) objects would require. The function computes one of six choices of distance (<code>euclidean</code>, <code>maximum</code>, <code>manhattan</code>, <code>canberra</code>, <code>binary</code>, <code>minkowski</code>) and outputs a vector of values sufficient to reconstruct the complete distance matrix. The function returns a special object of class <code>dist</code> that encodes the relevant vector of size \(n(n-1)/2\). Here is the output for a \(3\) by \(3\) matrix:</p>
<pre><code>mx  = c(0, 0, 0, 1, 1, 1)
my  = c(1, 0, 1, 1, 0, 1)
mz  = c(1, 1, 1, 0, 1, 1)
mat = rbind(mx, my, mz)
dist(mat)__


         mx       my
my 1.732051         
mz 2.000000 1.732051


dist(mat, method = "binary")__


          mx        my
my 0.6000000          
mz 0.6666667 0.5000000</code></pre>
<p>In order to access a particular distance (for example the distance between observations 1 and 2), one has to turn the <code>dist</code> class object back into a matrix.</p>
<pre><code>load("../data/Morder.RData")
sqrt(sum((Morder[1, ] - Morder[2, ])^2))__


[1] 5.593667


as.matrix(dist(Morder))[2, 1]__


[1] 5.593667</code></pre>
<p>Let’s look at how we would compute the Jaccard distance we defined above between HIV strains.</p>
<pre><code>mut = read.csv("../data/HIVmutations.csv")
mut[1:3, 10:16]__


  p32I p33F p34Q p35G p43T p46I p46L
1    0    1    0    0    0    0    0
2    0    1    0    0    0    1    0
3    0    1    0    0    0    0    0</code></pre>
<p>__</p>
<p>Question 5.2</p>
<p>Compare the Jaccard distance (available as the function <code>vegdist</code> in the R package <strong><a href="https://cran.r-project.org/web/packages/vegan/">vegan</a></strong>) between mutations in the HIV data <code>mut</code> to the correlation based distance.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>library("vegan")
mutJ = vegdist(mut, "jaccard")
mutC = sqrt(2 * (1 - cor(t(mut))))
mutJ __


      1     2     3     4
2 0.800                  
3 0.750 0.889            
4 0.900 0.778 0.846      
5 1.000 0.800 0.889 0.900


as.dist(mutC)__


     1    2    3    4
2 1.19               
3 1.10 1.30          
4 1.32 1.13 1.30     
5 1.45 1.19 1.30 1.32</code></pre>
<p><a href="imgs/birds_and_dinosaurs.png" title="Figure 5.8: An example of computing the cophenetic distance (xkcd)."><img src="imgs/birds_and_dinosaurs.png" class="img-fluid"></a></p>
<p>Figure 5.8: An example of computing the cophenetic distance (xkcd).</p>
<p>It can also be interesting to compare complex objects that are not traditional vectors or real numbers using dissimilarities or distances. Gower’s distance for data of mixed modalities (both categorical factors and continuous variables) can be computed with the <code>daisy</code> function. In fact, distances can be defined between any pairs of objects, not just points in \({ R}^p\) or character sequences. For instance, the <code>shortest.paths</code> function from the <strong><a href="https://cran.r-project.org/web/packages/igraph/">igraph</a></strong> package that we will see in <a href="10-chap.html">Chapter 10</a> computes the distance between vertices on a graph and the function <code>cophenetic</code> computes the distance between leaves of a tree as illustrated in Figure 5.8. We can compute the distance between trees using <code>dist.multiPhylo</code> in the <strong><a href="https://cran.r-project.org/web/packages/distory/">distory</a></strong> package.</p>
<p>The Jaccard index between graphs can be computed by looking at two graphs built on the same nodes and counting the number of co-occurring edges. This is implemented in the function <code>similarity</code> in the <strong><a href="https://cran.r-project.org/web/packages/igraph/">igraph</a></strong> package. Distances and dissimilarities are also used to compare images, sounds, maps and documents. A distance can usefully encompass domain knowledge and, if carefully chosen, can lead to the solution of many hard problems involving heterogeneous data. Asking yourself what is the <em>relevant</em> notion of “closeness” or similarity for your data can provide useful ways of representing them, as we will explore in <a href="09-chap.html">Chapter 9</a>.</p>
</section>
</section>
<section id="nonparametric-mixture-detection" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="nonparametric-mixture-detection"><span class="header-section-number">7.3</span> 5.4 Nonparametric mixture detection</h2>
<section id="k-methods-k-means-k-medoids-and-pam" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="k-methods-k-means-k-medoids-and-pam"><span class="header-section-number">7.3.1</span> 5.4.1 \(k\)-methods: \(k\)-means, \(k\)-medoids and PAM</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="The centers of the groups are sometimes called medoids, thus the name PAM (partitioning around medoids)."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>The centers of the groups are sometimes called medoids, thus the name PAM (partitioning around medoids).</figcaption>
</figure>
</div>
<p>The centers of the groups are sometimes called medoids, thus the name PAM (partitioning around medoids).</p>
<p>Partitioning or iterative relocation methods work well in high-dimensional settings, where we cannot4 easily use probability densities, the EM algorithm and parametric mixture modeling in the way we did in <a href="04-chap.html">Chapter 4</a>. Besides the distance measure, the main choice to be made is the number of clusters \(k\). The PAM (partitioning around medoids, Kaufman and Rousseeuw (<a href="16-chap.html#ref-Kaufman2009">2009</a>)) method is as follows:</p>
<p>4 This is due to the so-called curse of dimensionality. We will discuss this in more detail in <a href="12-chap.html">Chapter 12</a>.</p>
<ol type="1">
<li><p>Starts from a matrix of \(p\) features measured on a set of \(n\) observations.</p></li>
<li><p>Randomly pick \(k\) distinct <em>cluster centers</em> out of the \(n\) observations (“seeds”).</p></li>
<li><p>Assign each of the remaining observation to the group to whose center it is the closest.</p></li>
<li><p>For each group, choose a new center from the observations in the group, such that the sum of the distances of group members to the center is minimal; this is called the <em>medoid</em>.</p></li>
<li><p>Repeat Steps 3 and 4 until the groups stabilize.</p></li>
</ol>
<p>Each time the algorithm is run, different initial seeds will be picked in Step 2, and in general, this can lead to different final results. A popular implementation is the <code>pam</code> function in the package <strong><a href="https://cran.r-project.org/web/packages/cluster/">cluster</a></strong>.</p>
<p>A slight variation of the method replaces the medoids by the arithmetic means (centers of gravity) of the clusters and is called \(k\)-means. While in PAM, the centers are observations, this is not, in general, the case with \(k\)-means. The function <code>kmeans</code> comes with every installation of R in the <strong><a href="https://cran.r-project.org/web/packages/stats/">stats</a></strong> package; an example run is shown in Figure 5.9.</p>
<p>[<img src="05-chap_files/figure-html/fig-clust- kmeansastep1-1.png" class="img-fluid">](05-chap_files/figure-html/fig-clust-kmeansastep1-1.png “Figure&nbsp;5.9&nbsp;(a):”)</p>
<ol type="a">
<li></li>
</ol>
<p>[<img src="05-chap_files/figure-html/fig-clust- kmeansastep1-2.png" class="img-fluid">](05-chap_files/figure-html/fig-clust-kmeansastep1-2.png “Figure&nbsp;5.9&nbsp;(b):”)</p>
<ol start="2" type="a">
<li></li>
</ol>
<p>[<img src="05-chap_files/figure-html/fig-clust- kmeansastep1-3.png" class="img-fluid">](05-chap_files/figure-html/fig-clust-kmeansastep1-3.png “Figure&nbsp;5.9&nbsp;(c):”)</p>
<ol start="3" type="a">
<li></li>
</ol>
<p>Figure 5.9: An example run of the \(k\)-means algorithm. The initial, randomly chosen centers (black circles) and groups (colors) are shown in (a). The group memberships are assigned based on their distance to centers. At each iteration (b) and (c), the group centers are redefined, and the points reassigned to the cluster centers.</p>
<p>These so-called \(k\)-methods are the most common off-the-shelf methods for clustering; they work particularly well when the clusters are of comparable size and convex (blob-shaped). On the other hand, if the true clusters are very different in size, the larger ones will tend to be broken up; the same is true for groups that have pronounced non-spherical or non-elliptic shapes.</p>
<p>__</p>
<p>Question 5.3</p>
<p>The \(k\)-means algorithm alternates between computing the average point and assigning the points to clusters. How does this alternating, iterative method differ from an EM-algorithm?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>In the EM algorithm, each point participates in the computation of the mean of all the groups through a probabilistic weight assigned to it. In the \(k\)-means method, the points are either attributed to a cluster or not, so each point participates only, and entirely, in the computation of the center of one cluster.</p>
</section>
<section id="tight-clusters-with-resampling" class="level3" data-number="7.3.2">
<h3 data-number="7.3.2" class="anchored" data-anchor-id="tight-clusters-with-resampling"><span class="header-section-number">7.3.2</span> 5.4.2 Tight clusters with resampling</h3>
<p>There are clever schemes that repeat the process many times using different initial centers or resampled datasets. Repeating a clustering procedure multiple times on the same data, but with different starting points creates <em>strong forms</em> according to Diday and Brito (<a href="16-chap.html#ref- Diday1989">1989</a>). Repeated subsampling of the dataset and applying a clustering method will result in groups of observations that are “almost always” grouped together; these are called <em>tight clusters</em> (<a href="16-chap.html#ref-Tseng:2005">Tseng and Wong 2005</a>). The study of strong forms or tight clusters facilitates the choice of the number of clusters. A recent package developed to combine and compare the output from many different clusterings is <strong><a href="https://bioconductor.org/packages/clusterExperiment/">clusterExperiment</a></strong>. Here we give an example from its vignette. Single-cell RNA-Seq experiments provide counts of reads, representing gene transcripts, from individual cells. The single cell resolution enables scientists, among other things, to follow cell lineage dynamics. Clustering has proved very useful for analysing such data.</p>
<p>__</p>
<p>Question 5.4</p>
<p>Follow the vignette of the package <strong><a href="https://bioconductor.org/packages/clusterExperiment/">clusterExperiment</a></strong>. Call the ensemble clustering function <code>clusterMany</code>, using <code>pam</code> for the individual clustering efforts. Set the choice of genes to include at either the 60, 100 or 150 most variable genes. Plot the clustering results for \(k\) varying between 4 and 9. What do you notice?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>The following code produces Figure 5.10.</p>
<pre><code>library("clusterExperiment")
fluidigm = scRNAseq::ReprocessedFluidigmData()
se = fluidigm[, fluidigm$Coverage_Type == "High"]
assays(se) = list(normalized_counts = as.matrix(limma::normalizeQuantiles(assay(se))))
ce = clusterMany(se, clusterFunction = "pam", ks = c(5, 7, 9), run = TRUE,
                 isCount = TRUE, reduceMethod = "var", nFilterDims = c(60, 100, 150))__


9 parameter combinations, 0 use sequential method, 0 use subsampling method
Running Clustering on Parameter Combinations...
done.


clusterLabels(ce) = sub("FilterDims", "", clusterLabels(ce))
plotClusters(ce, whichClusters = "workflow", axisLine = -1)__</code></pre>
<p><a href="05-chap_files/figure- html/fig-quiltclust-1-1.png" title="Figure 5.10: Comparison of clustering results (rows), for different numbers of included genes and for varying numbers of clusters, k. Each column of the heatmap corresponds to a cell, and the colors represent the cluster assignments."><img src="05-chap_files/figure-html/fig-quiltclust-1-1.png" class="img-fluid"></a></p>
<p>Figure 5.10: Comparison of clustering results (rows), for different numbers of included genes and for varying numbers of clusters, \(k\). Each column of the heatmap corresponds to a cell, and the colors represent the cluster assignments.</p>
</section>
</section>
<section id="군집화-예시-유세포-분석-및-질량-분석" class="level2" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="군집화-예시-유세포-분석-및-질량-분석"><span class="header-section-number">7.4</span> 5.5 군집화 예시: 유세포 분석 및 질량 분석</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/book_icon.png" title="유세포 분석을 위한 생물정보학적 방법들에 대한 리뷰는 [@oneill2013flow]와 잘 관리된 위키백과 문서에서 찾을 수 있습니다."><img src="imgs/book_icon.png" class="img-fluid figure-img"></a></p>
<figcaption>유세포 분석을 위한 생물정보학적 방법들에 대한 리뷰는 (O’Neill et al.&nbsp;2013)와 잘 관리된 위키백과 문서에서 찾을 수 있습니다.</figcaption>
</figure>
</div>
<p>유세포 분석을 위한 생물정보학적 방법들에 대한 리뷰는 (<a href="16-chap.html#ref-oneill2013flow">O’Neill et al.&nbsp;2013</a>)와 <a href="https://en.wikipedia.org/wiki/Flow_cytometry_bioinformatics">잘 관리된 위키백과 문서</a>에서 찾을 수 있습니다.</p>
<p>단일 세포에 대한 측정값을 연구하면 세포 유형과 역학을 분석할 수 있는 초점과 해상도가 모두 향상됩니다. 유세포 분석(flow cytometry)은 약 10가지의 서로 다른 세포 마커를 동시에 측정할 수 있게 해줍니다. 질량 분석(mass cytometry)은 측정 컬렉션을 세포당 최대 80개의 단백질로 확장합니다. 이 기술의 특히 유망한 응용 분야는 면역 세포 역학 연구입니다.</p>
<section id="유세포-분석-및-질량-분석" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="유세포-분석-및-질량-분석"><span class="header-section-number">7.4.1</span> 5.5.1 유세포 분석 및 질량 분석</h3>
<p>발달의 서로 다른 단계에서 면역 세포는 표면에 고유한 단백질 조합을 발현합니다. 이러한 단백질 마커는 <strong>CD</strong> (<strong>clusters of differentiation</strong>, 분화 클러스터)라고 불리며 유세포 분석(형광 사용, Hulett et al.&nbsp;(<a href="16-chap.html#ref-flowsort">1969</a>) 참조) 또는 질량 분석(중원소 리포터의 단일 세포 원자 질량 분석법 사용, Bendall et al.&nbsp;(<a href="16-chap.html#ref-BendallCell">2012</a>) 참조)에 의해 수집됩니다. 흔히 사용되는 CD의 예로 CD4가 있는데, 이 단백질은 “CD4+”라고 불리는 보조 T 세포(helper T cells)에 의해 발현됩니다. 그러나 일부 세포는 CD4를 발현하지만(따라서 CD4+임), 실제로는 보조 T 세포가 아니라는 점에 유의하세요. 먼저 세포 분석(cytometry) 데이터를 위한 유용한 Bioconductor 패키지인 <strong><a href="https://bioconductor.org/packages/flowCore/">flowCore</a></strong>와 <strong><a href="https://bioconductor.org/packages/flowViz/">flowViz</a></strong>를 불러오고, 다음과 같이 예시 데이터 객체 <code>fcsB</code>를 읽어들입니다:</p>
<pre><code>library("flowCore")
library("flowViz")
fcsB = read.FCS("../data/Bendall_2011.fcs", truncate_max_range = FALSE)
slotNames(fcsB)__


[1] "exprs"       "parameters"  "description"</code></pre>
<p>그림 5.11은 <code>fcsB</code> 데이터에서 사용 가능한 두 변수의 산점도를 보여줍니다. (이러한 플롯을 만드는 방법은 아래에서 살펴보겠습니다.) 이 두 차원에서 명확한 이봉성과 군집화를 볼 수 있습니다.</p>
<p>__</p>
<p>질문 5.5</p>
<ol type="1">
<li><p><code>fcsB</code> 객체의 구조를 살펴보세요(힌트: <code>colnames</code> 함수). 얼마나 많은 변수가 측정되었나요?</p></li>
<li><p>처음 몇 행을 보기 위해 데이터를 하위 집합화해 보세요(힌트: <code>Biobase::exprs(fcsB)</code> 사용). 얼마나 많은 세포가 측정되었나요?</p></li>
</ol>
</section>
<section id="데이터-전처리" class="level3" data-number="7.4.2">
<h3 data-number="7.4.2" class="anchored" data-anchor-id="데이터-전처리"><span class="header-section-number">7.4.2</span> 5.5.2 데이터 전처리</h3>
<p>먼저 동위원소(isotopes)와 마커(항체) 사이의 매핑을 보고하는 테이블 데이터를 불러온 다음, <code>fcsB</code>의 열 이름에 있는 동위원소 이름을 마커 이름으로 바꿉니다. 이렇게 하면 후속 분석 및 플로팅 코드가 더 읽기 쉬워집니다:</p>
<pre><code>markersB = readr::read_csv("../data/Bendall_2011_markers.csv")
mt = match(markersB$isotope, colnames(fcsB))
stopifnot(!any(is.na(mt)))
colnames(fcsB)[mt] = markersB$marker __</code></pre>
<p>이제 그림 5.11을 생성할 준비가 되었습니다.</p>
<pre><code>flowPlot(fcsB, plotParameters = colnames(fcsB)[2:3], logy = TRUE)__</code></pre>
<p><a href="05-chap_files/figure-html/fig-ObviousClusters-1.png &quot;Figure 5.11: Cell measurements that show clear clustering in two dimensions.&quot;"><img src="05-chap_files/figure-html/fig- ObviousClusters-1.png" class="img-fluid"></a></p>
<p>Figure 5.11: Cell measurements that show clear clustering in two dimensions.</p>
<p>Plotting the data in two dimensions as in Figure 5.11 already shows that the cells can be grouped into subpopulations. Sometimes just one of the markers can be used to define populations on their own; in that case simple <strong>rectangular gating</strong> is used to separate the populations; for instance, CD4+ cells can be gated by taking the subpopulation with high values for the CD4 marker. Cell clustering can be improved by carefully choosing transformations of the data. The left part of Figure 5.12 shows a simple one dimensional histogram before transformation; on the right of Figure 5.12 we see the distribution after transformation. It reveals a bimodality and the existence of two cell populations.</p>
<p><strong>Data Transformation: hyperbolic arcsin (asinh)</strong>. It is standard to transform both flow and mass cytometry data using one of several special functions. We take the example of the inverse hyperbolic sine (asinh):</p>
<p>\[ (x) = . \]</p>
<p>From this we can see that for large values of \(x\), \((x)\)는 로그 함수처럼 행동하며 실제로는 \((x)+(2)\)와 거의 같습니다. (x)가 작을 때 이 함수는 (x)에 대해 거의 선형적입니다.</p>
<p>__</p>
<p>태스크</p>
<p>변환의 두 가지 주요 영역인 작은 값과 큰 값을 확인하기 위해 다음 코드를 실행해 보세요.</p>
<pre><code>v1 = seq(0, 1, length.out = 100)
plot(log(v1), asinh(v1), type = 'l')__


 plot(v1, asinh(v1), type = 'l')__


v3 = seq(30, 3000, length = 100)
plot(log(v3), asinh(v3), type= 'l')__</code></pre>
<p>이것은 <a href="04-chap.html">4장</a>과 <a href="08-chap.html">8장</a>에서도 언급된 분산 안정화 변환의 또 다른 예입니다. 그림 5.12는 <strong><a href="https://bioconductor.org/packages/flowCore/">flowCore</a></strong> 패키지의 <code>arcsinhTransform</code> 함수를 사용하는 다음 코드로 생성되었습니다.</p>
<pre><code>asinhtrsf = arcsinhTransform(a = 0.1, b = 1)
fcsBT = transform(fcsB, transformList(colnames(fcsB)[-c(1, 2, 41)], asinhtrsf))
densityplot(~`CD3all`, fcsB)
densityplot(~`CD3all`, fcsBT)__</code></pre>
<p><a href="05-chap_files/figure-html/fig- plotTransformations-1.png" title="그림 5.12 (a):"><img src="05-chap_files/figure-html/fig- plotTransformations-1.png" class="img-fluid"></a></p>
<ol type="a">
<li></li>
</ol>
<p><a href="05-chap_files/figure-html/fig- plotTransformations-2.png" title="그림 5.12 (b):"><img src="05-chap_files/figure-html/fig- plotTransformations-2.png" class="img-fluid"></a></p>
<ol start="2" type="a">
<li></li>
</ol>
<p>그림 5.12: 패널 (a)는 CD3all 변수의 히스토그램을 보여줍니다: 세포들이 0 근처에 군집해 있고 몇 개의 큰 값들이 있습니다. (b)에서는 asinh 변환 후 세포들이 군집을 이루어 두 그룹 또는 유형으로 나뉘는 것을 볼 수 있습니다.</p>
<p>__</p>
<p>질문 5.6</p>
<p>다음 코드는 \(k\)-평균을 사용하여 데이터를 2개의 그룹으로 나누기 위해 몇 개의 차원을 사용하나요?</p>
<pre><code>kf = kmeansFilter("CD3all" = c("Pop1","Pop2"), filterId="myKmFilter")
fres = flowCore::filter(fcsBT, kf)
summary(fres)__


Pop1: 33434 of 91392 events (36.58%)
Pop2: 57958 of 91392 events (63.42%)


fcsBT1 = flowCore::split(fcsBT, fres, population = "Pop1")
fcsBT2 = flowCore::split(fcsBT, fres, population = "Pop2")__</code></pre>
<p>다음 코드로 생성된 그림 5.13은 CD3와 CD56 마커에 의해 확장된 두 차원으로 데이터를 단순 투영한 것을 보여줍니다:</p>
<pre><code>library("flowPeaks")
fp = flowPeaks(Biobase::exprs(fcsBT)[, c("CD3all", "CD56")])
plot(fp)__</code></pre>
<p><a href="05-chap_files/figure- html/fig-flowCD3CD56-1-1.png" title="그림 5.13: 변환 후 이 세포들은 kmeans를 사용하여 군집화되었습니다."><img src="05-chap_files/figure-html/fig-flowCD3CD56-1-1.png" class="img-fluid"></a></p>
<p>그림 5.13: 변환 후 이 세포들은 <code>kmeans</code>를 사용하여 군집화되었습니다.</p>
<p>어느 영역에 밀집된 점들을 플롯할 때는 겹쳐그리기(overplotting)를 피해야 합니다. <a href="03-chap.html">3장</a>에서 선호되는 기법들 중 일부를 보았습니다. 여기서는 등고선(contours)과 음영(shading)을 사용합니다. 다음과 같이 수행합니다:</p>
<pre><code>flowPlot(fcsBT, plotParameters = c("CD3all", "CD56"), logy = FALSE)
contour(fcsBT[, c(40, 19)], add = TRUE)__</code></pre>
<p><a href="05-chap_files/figure-html/fig- groupcontourCD3CD56-1.png" title="그림 5.14: 그림 5.13과 유사하지만 등고선을 사용했습니다."><img src="05-chap_files/figure-html/fig- groupcontourCD3CD56-1.png" class="img-fluid"></a></p>
<p>그림 5.14: 그림 5.13과 유사하지만 등고선을 사용했습니다.</p>
<p>이 코드는 그림 5.13보다 더 많은 정보를 담고 있는 그림 5.14를 생성합니다.</p>
<p>__</p>
<p>태스크</p>
<p>Bioconductor 패키지 <strong><a href="https://bioconductor.org/packages/ggcyto/">ggcyto</a></strong>는 <code>ggplot</code>을 사용하여 각 환자의 데이터를 서로 다른 패싯(facet)에 그릴 수 있게 해줍니다. 다음과 같은 방식으로 이 접근법을 사용한 출력과 위에서 수행한 작업을 비교해 보세요:</p>
<pre><code>library("ggcyto")
library("labeling")

p1 = ggcyto(fcsB, aes(x = CD4)) + geom_histogram(bins = 60)
p2 = ggcyto(fcsB, aes(x = CD8)) + geom_histogram(bins = 60)
p3 = ggcyto(fcsB, aes(x = CD4, y = CD8)) + geom_density2d(colour = "black")

fcsBT = transform(fcsB, transformList(colnames(fcsB)[-c(1, 2, 41)], 
                                      arcsinhTransform(a = 0, b = 1)))
                                      
p1t = ggcyto(fcsBT, aes(x = CD4))            + geom_histogram(bins = 90)
p2t = ggcyto(fcsBT, aes(x = CD4,y = CD8))    + geom_density2d(colour = "black")
p3t = ggcyto(fcsBT, aes(x = CD45RA,y = CD20))+ geom_density2d(colour = "black")__</code></pre>
</section>
<section id="밀도-기반-군집화density-based-clustering" class="level3" data-number="7.4.3">
<h3 data-number="7.4.3" class="anchored" data-anchor-id="밀도-기반-군집화density-based-clustering"><span class="header-section-number">7.4.3</span> 5.5.3 밀도 기반 군집화(Density-based clustering)</h3>
<p>마커 수가 적고 세포 수가 많은 유세포 분석과 같은 데이터 세트는 밀도 기반 군집화에 적합합니다. 이 방법은 희소한 영역에 의해 분리된 고밀도 영역을 찾습니다. 이 방법은 클러스터가 반드시 볼록할 필요가 없는 경우에도 대처할 수 있다는 장점이 있습니다. 이러한 방법의 한 구현체로 dbscan이 있습니다. 다음 코드를 실행하여 예시를 살펴보겠습니다.</p>
<pre><code>library("dbscan")
mc5 = Biobase::exprs(fcsBT)[, c(15,16,19,40,33)]
res5 = dbscan::dbscan(mc5, eps = 0.65, minPts = 30)
mc5df = data.frame(mc5, cluster = as.factor(res5$cluster))
table(mc5df$cluster)__


    0     1     2     3     4     5     6     7     8 
76053  4031  5450  5310   257   160    63    25    43 


ggplot(mc5df, aes(x=CD4,    y=CD8,  col=cluster))+geom_density2d()
ggplot(mc5df, aes(x=CD3all, y=CD20, col=cluster))+geom_density2d()__</code></pre>
<p><a href="05-chap_files/figure- html/fig-dbscanfcs5-1.png" title="그림 5.15 (a):"><img src="05-chap_files/figure-html/fig-dbscanfcs5-1.png" class="img-fluid"></a></p>
<ol type="a">
<li></li>
</ol>
<p><a href="05-chap_files/figure- html/fig-dbscanfcs5-2.png" title="그림 5.15 (b):"><img src="05-chap_files/figure-html/fig-dbscanfcs5-2.png" class="img-fluid"></a></p>
<ol start="2" type="a">
<li></li>
</ol>
<p>그림 5.15: 이 두 플롯은 5개의 마커를 사용하여 <code>dbscan</code>으로 군집화한 결과를 보여줍니다. 여기서는 데이터를 CD4-CD8 및 C3all-CD20 평면에 투영한 것만 보여줍니다.</p>
<p>출력 결과는 그림 5.15에 나와 있습니다. 2D 투영에서의 클러스터 중첩을 통해 군집화의 다차원적 특성을 이해할 수 있습니다.</p>
<p>__</p>
<p>질문 5.7</p>
<p>입력 데이터에서 CD 마커 변수 하나를 추가하여 차원을 6으로 늘려보세요.<br>
그런 다음 <code>eps</code>를 변화시키면서, 적어도 두 개가 100개 이상의 점을 가진 4개의 클러스터를 찾아보세요.<br>
7개의 CD 마커 변수로 이 작업을 반복해 보세요. 무엇을 알 수 있나요?</p>
<p>__</p>
<p>해결책</p>
<p>__</p>
<p>다음 6개 마커를 사용한 예시입니다.</p>
<pre><code>mc6 = Biobase::exprs(fcsBT)[, c(15, 16, 19, 33, 25, 40)]
res = dbscan::dbscan(mc6, eps = 0.65, minPts = 20)
mc6df = data.frame(mc6, cluster = as.factor(res$cluster))
table(mc6df$cluster)__


    0     1     2     3     4     5     6 
91068    34    61    20    67   121    21 </code></pre>
<p>우리는 eps=0.75일 때 eps=0.65일 때보다 충분히 큰 클러스터를 찾기가 더 쉽다는 것을 알 수 있으며, eps=0.55일 때는 불가능합니다. 차원수를 7로 늘리면 eps를 훨씬 더 크게 만들어야 합니다.</p>
<pre><code>mc7 = Biobase::exprs(fcsBT)[, c(11, 15, 16, 19, 25, 33, 40)]
res = dbscan::dbscan(mc7, eps = 0.95, minPts = 20)
mc7df = data.frame(mc7, cluster = as.factor(res$cluster))
table(mc7df$cluster)__


    0     1     2     3     4     5     6     7     8     9    10 
90249    21   102   445   158   119    19   224    17    20    18 </code></pre>
<p>이는 소위 <strong>차원의 저주(curse of dimensionality)</strong>가 실제로 작동하는 것을 보여주며, 이에 대해서는 <a href="12-chap.html">12장</a>에서 더 자세히 다룹니다.</p>
<section id="밀도-기반-군집화dbscan는-어떻게-작동하나요" class="level4" data-number="7.4.3.1">
<h4 data-number="7.4.3.1" class="anchored" data-anchor-id="밀도-기반-군집화dbscan는-어떻게-작동하나요"><span class="header-section-number">7.4.3.1</span> 밀도 기반 군집화(dbscan)는 어떻게 작동하나요?</h4>
<p>dbscan 방법은 <em>밀도 연결성(density-connectedness)</em> 기준에 따라 고밀도 영역의 점들을 군집화합니다. 이 방법은 점들이 연결되어 있는지 확인하기 위해 반지름 \(\)인 작은 이웃 구(neighborhood spheres)를 살펴봅니다.</p>
<p>dbscan의 기본 구성 요소는 밀도 도달 가능성(density-reachability) 개념입니다: 점 \(q\)가 점 \(p\)로부터 주어진 임계값 \(\)보다 멀리 있지 않고, \(p\)가 충분히 많은 점들에 둘러싸여 있어 \(p\)(및 \(q\))를 밀집 영역의 일부로 간주할 수 있다면, 점 \(q\)는 점 \(p\)로부터 직접 <strong>밀도 도달 가능(density-reachable)</strong>합니다. \(p_1 = p\)이고 \(p_n = q\)인 일련의 점 \(p_1, …, p_n\)이 있어서 각 \(p_{i+1}\)이 \(p_i\)로부터 직접 밀도 도달 가능하다면, \(q\)는 \(p\)로부터 _밀도 도달 가능_하다고 합니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="방법이 이웃에서 높은 점 밀도를 찾는 것이 중요합니다. 클러스터 사이의 공백(void) 또는 &quot;누락된 점&quot;에 의해 클러스터를 정의하려는 다른 방법들이 존재합니다. 그러나 이러한 방법들은 차원의 저주에 취약하며, 가짜 &quot;공백&quot;을 만들어낼 수 있습니다."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>방법이 이웃에서 높은 점 밀도를 찾는 것이 중요합니다. 클러스터 사이의 공백(void) 또는 “누락된 점”에 의해 클러스터를 정의하려는 다른 방법들이 존재합니다. 그러나 이러한 방법들은 차원의 저주에 취약하며, 가짜 “공백”을 만들어낼 수 있습니다.</figcaption>
</figure>
</div>
<p>방법이 이웃에서 높은 점 밀도를 찾는 것이 중요합니다. 클러스터 사이의 공백(void) 또는 “누락된 점”에 의해 클러스터를 정의하려는 다른 방법들이 존재합니다. 그러나 이러한 방법들은 차원의 저주에 취약하며, 가짜 “공백”을 만들어낼 수 있습니다.</p>
<p>그러면 _클러스터_는 다음 속성들을 만족하는 점들의 하위 집합입니다:</p>
<ol type="1">
<li><p>클러스터 내의 모든 점은 서로 밀도 연결되어 있습니다.</p></li>
<li><p>어떤 점이 클러스터의 임의의 점에 밀도 연결되어 있다면, 그 점 역시 클러스터의 일부입니다.</p></li>
<li><p>점들의 그룹이 클러스터로 간주되려면 적어도 <code>MinPts</code>개의 점을 가져야 합니다.</p></li>
</ol>
</section>
</section>
</section>
<section id="계층적-군집화hierarchical-clustering" class="level2" data-number="7.5">
<h2 data-number="7.5" class="anchored" data-anchor-id="계층적-군집화hierarchical-clustering"><span class="header-section-number">7.5</span> 5.6 계층적 군집화(Hierarchical clustering)</h2>
<p><a href="imgs/LinnaeusClass-01.png" title="그림 5.16: 특성 유사성에 따라 유기체들을 군집화한 린네의 분류 체계의 일부입니다."><img src="imgs/LinnaeusClass-01.png" class="img-fluid"></a></p>
<p>그림 5.16: 특성 유사성에 따라 유기체들을 군집화한 린네의 분류 체계의 일부.</p>
<p>계층적 군집화는 유사한 관측치와 하위 클래스를 반복적으로 조립하는 상향식(bottom-up) 접근 방식입니다. 그림 5.16은 린네가 특정 특성에 따라 유기체들의 중첩된 클러스터를 어떻게 만들었는지 보여줍니다. 이러한 계층적 조직은 많은 분야에서 유용하게 사용되어 왔으며, _자연의 사다리(ladder of nature)_를 상정한 아리스토텔레스까지 거슬러 올라갑니다.</p>
<p><strong>덴드로그램 순서(Dendrogram ordering)</strong>. 그림 5.17의 예에서 볼 수 있듯이, 레이블의 순서는 형제 쌍(sibling pairs) 내에서는 중요하지 않습니다. 수평 거리는 대개 무의미한 반면, 수직 거리는 어떤 정보를 인코딩합니다. 이러한 속성들은 계통발생학적(monophyletic)이지 않지만(즉, 동일한 서브트리나 클레이드(clade)에 속하지 않음) 플롯에서는 이웃으로 나타나는 대상(예를 들어 오른쪽 나무의 B와 D)에 대해 해석을 내릴 때 기억해야 할 중요한 사항입니다.</p>
<p><a href="imgs/SameTree-01.png" title="그림 5.17: 동일한 계층적 군집 트리의 세 가지 표현 방식."><img src="imgs/SameTree-01.png" class="img-fluid"></a></p>
<p>그림 5.17: <strong>동일한</strong> 계층적 군집 트리의 세 가지 표현 방식.</p>
<p><strong>하향식 계층 구조(Top-down hierarchies)</strong>. 대안적인 하향식 접근 방식은 모든 객체를 가져와서 선택된 기준에 따라 순차적으로 분할합니다. 이러한 소위 <strong>재귀적 분할(recursive partitioning)</strong> 방법은 종종 의사 결정 나무(decision trees)를 만드는 데 사용됩니다. 이들은 예측(예를 들어 의료 진단이 주어졌을 때의 생존 기간)에 유용할 수 있습니다: 우리는 그러한 사례들에서 분할을 통해 불균질한 모집단을 더 균질한 하위 그룹으로 나누기를 희망합니다. 이 장에서 우리는 상향식 접근 방식에 집중합니다. <a href="12-chap.html">12장</a>에서 비지도 학습과 분류에 대해 이야기할 때 분할 방식으로 다시 돌아올 것입니다.</p>
<section id="집계된-클러스터-간의-비유사성을-어떻게-계산하는가" class="level3" data-number="7.5.1">
<h3 data-number="7.5.1" class="anchored" data-anchor-id="집계된-클러스터-간의-비유사성을-어떻게-계산하는가"><span class="header-section-number">7.5.1</span> 5.6.1 집계된 클러스터 간의 (비)유사성을 어떻게 계산하는가?</h3>
<p><a href="imgs/ClusterStepChoiceSingle1b.png &quot;Figure 5.18: In the single linkage method, the distance between groups C_1 and C_2 is defined as the distance between the closest two points from the groups.&quot;"><img src="imgs/ClusterStepChoiceSingle1b.png" class="img-fluid"></a></p>
<p>Figure 5.18: In the single linkage method, the distance between groups \(C_1\) and \(C_2\) is defined as the distance between the closest two points from the groups.</p>
<p>A hierarchical clustering algorithm, which works by aggregation, is easy enough to get started, by grouping the most similar observations together. But we will need more than just the distances between all pairs of individual objects. Once an aggregation is made, one is required to say how the distance between the newly formed cluster and all other points (or existing clusters) is computed. There are different choices, all based on the object-object distances, and each choice results in a different type of hierarchical clustering.</p>
<p>The <strong>minimal jump</strong> method, also called <strong>single linkage</strong> or nearest neighbor method computes the distance between clusters as the smallest distance between any two points in the two clusters (as shown in Figure 5.18):</p>
<p>\[ d_{12} = <em>{i C_1, j C_2 } d</em>{ij}. \]</p>
<p>This method tends to create clusters that look like contiguous strings of points. The cluster tree often looks like a comb.</p>
<p><a href="imgs/ClusterStepChoiceComplete1b.png &quot;Figure 5.19: In the complete linkage method, the distance between groups C_1 and C_2 is defined as the maximum distance between pairs of points from the two groups.&quot;"><img src="imgs/ClusterStepChoiceComplete1b.png" class="img-fluid"></a></p>
<p>Figure 5.19: In the complete linkage method, the distance between groups \(C_1\) and \(C_2\) is defined as the maximum distance between pairs of points from the two groups.</p>
<p>The <strong>maximum jump</strong> (or <strong>complete linkage</strong>) method defines the distance between clusters as the largest distance between any two objects in the two clusters, as represented in Figure 5.19:</p>
<p>\[ d_{12} = <em>{i C_1, j C_2 } d</em>{ij}. \]</p>
<p>The <strong>average linkage</strong> method is half way between the two above (here, \(|C_k|\) denotes the number of elements of cluster \(k\)):</p>
<p>\[ d_{12} = <em>{i C_1, j C_2 } d</em>{ij} \]</p>
<p><a href="imgs/BetweenWithinb.png" title="Figure 5.20: The Ward method maximizes the between group sum of squares (red edges), while minimizing the sums of squares within groups (black edges)."><img src="imgs/BetweenWithinb.png" class="img-fluid"></a></p>
<p>Figure 5.20: The Ward method maximizes the between group sum of squares (red edges), while minimizing the sums of squares within groups (black edges).</p>
<p><strong>Ward’s method</strong> takes an analysis of variance approach, where the goal is to minimize the variance within clusters. This method is very efficient, however, it tends to create clusters of smaller sizes.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Advantages and disadvantages of various choices of defining distances between aggregates (<a href="16-chap.html#ref-distory-paper">Chakerian and Holmes 2012</a>). Method</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Single linkage</td>
<td>number of clusters</td>
<td>comblike trees</td>
</tr>
<tr class="even">
<td>Complete linkage</td>
<td>compact classes</td>
<td>one observation can alter groups</td>
</tr>
<tr class="odd">
<td>Average linkage</td>
<td>similar size and variance</td>
<td>not robust</td>
</tr>
<tr class="even">
<td>Centroid</td>
<td>robust to outliers</td>
<td>smaller number of clusters</td>
</tr>
<tr class="odd">
<td>Ward</td>
<td>minimising an inertia</td>
<td>classes small if high variability</td>
</tr>
</tbody>
</table>
<p><a href="imgs/CalderHand.png" title="Figure 5.21: Hierarchical clustering output has similar properties to a mobile: the branches can rotate freely from their suspension points."><img src="imgs/CalderHand.png" class="img-fluid"></a></p>
<p>Figure 5.21: Hierarchical clustering output has similar properties to a mobile: the branches can rotate freely from their suspension points.</p>
<p>These are the choices we have to make building hierarchical clustering trees. An advantage of hierarchical clustering compared to the partitioning methods 좋은 점은 그룹화의 강도를 그래픽으로 진단할 수 있다는 것입니다: 트리의 내부 에지(inner edges)의 길이입니다.</p>
<p>클러스터들의 크기가 거의 같다는 사전 지식이 있다면, 그룹 내 분산을 최소화하는 평균 연결법(average linkage)이나 와드 방법(Ward’s method)을 사용하는 것이 가장 좋은 전술입니다.</p>
<p>__</p>
<p>질문 5.8</p>
<p><strong>세포군에 대한 계층적 군집화</strong> <code>Morder</code> 데이터는 10명의 환자로부터 얻은 3가지 유형(naïve, effector, memory)의 T 세포에 대한 156개 유전자의 발현 측정값입니다(<a href="16-chap.html#ref-holmes2005memory">Holmes et al.&nbsp;2005</a>). <strong><a href="https://cran.r-project.org/web/packages/pheatmap/">pheatmap</a></strong> 패키지를 사용하여, 이 데이터의 유클리드 거리와 맨해튼 거리에 대해 덴드로그램이나 재정렬 없이 두 개의 단순한 히트맵을 만드세요.</p>
<p>__</p>
<p>질문 5.9</p>
<p>이제 이 두 거리를 사용한 계층적 군집 트리에서의 순서 차이를 살펴보세요. 어떤 차이점이 눈에 띄나요?</p>
<p><img src="imgs/single14heatmap.png" class="img-fluid">: “)</p>
<ol type="a">
<li></li>
</ol>
<p><img src="imgs/average14heatmap.png" class="img-fluid">: “)</p>
<ol start="2" type="a">
<li></li>
</ol>
<p>[<img src="imgs/complete14heatmap.png" class="img-fluid">](imgs/complete14heatmap.png “그림 5.22 (c):”)</p>
<ol start="3" type="a">
<li></li>
</ol>
<p>그림 5.22: 서로 다른 응집(agglomeration) 선택으로 만들어진 세 개의 계층적 군집 플롯. (a)의 단일 연결법(single linkage)에 대한 빗 모양 구조에 주목하세요. 평균 연결법 (b)와 완전 연결법 (c) 트리는 내부 분기의 길이에 의해서만 다릅니다.</p>
<p><a href="imgs/apeclust14.png" title="그림 5.23: 이 트리는 여러 가지 다른 방식으로 그려질 수 있습니다. 여기에 나타난 잎들의 순서는 (8,11,9,10,7,5,6,1,4,2,3)입니다."><img src="imgs/apeclust14.png" class="img-fluid"></a></p>
<p>그림 5.23: 이 트리는 여러 가지 다른 방식으로 그려질 수 있습니다. 여기에 나타난 잎들의 순서는 \((8,11,9,10,7,5,6,1,4,2,3)\)입니다.</p>
<p>__</p>
<p>질문 5.10</p>
<p>계층적 군집 트리는 그림 5.21의 콜더 모빌(Calder mobile)과 같아서 많은 내부 피벗 점들을 중심으로 회전할 수 있으며, 주어진 트리와 일치하는 많은 팁(tips) 순서를 제공합니다. 그림 5.23의 트리를 보세요. 이 트리와 일관성을 유지하면서 팁 레이블을 정렬할 수 있는 방법은 몇 가지가 있을까요?</p>
<p>행 및/또는 열이 계층적 군집 트리에 기반하여 정렬된 히트맵을 흔히 볼 수 있습니다. 때때로 이것은 일부 클러스터를 매우 강력해 보이게 만듭니다 – 트리가 실제로 암시하는 것보다 더 강력하게 말이죠. 히트맵에서 행과 열을 정렬하는 대안적인 방법들이 있는데, 예를 들어 서열화 방법(ordination methods)5을 사용하여 순서를 찾는 <strong><a href="https://cran.r-project.org/web/packages/NeatMap/">NeatMap</a></strong> 패키지가 있습니다.</p>
<p>5 이들은 <a href="09-chap.html">9장</a>에서 설명될 것입니다.</p>
</section>
</section>
<section id="클러스터-수-검증-및-선택" class="level2" data-number="7.6">
<h2 data-number="7.6" class="anchored" data-anchor-id="클러스터-수-검증-및-선택"><span class="header-section-number">7.6</span> 5.7 클러스터 수 검증 및 선택</h2>
<p>우리가 설명한 군집화 방법들은 다양한 제약 조건 하에서 데이터의 좋은 그룹화를 제공하도록 맞춤화되어 있습니다. 그러나 군집화 방법은 클러스터가 없더라도 항상 그룹을 제공한다는 점을 명심하세요. 만약 데이터에 실제 클러스터가 없다면, 계층적 군집 트리는 상대적으로 짧은 내부 분기를 보여줄 수 있지만, 이를 정량화하기는 어렵습니다. 일반적으로 보다 객관적인 기준으로 클러스터 선택을 검증하는 것이 중요합니다.</p>
<p>군집화 결과의 품질을 평가하는 한 가지 기준은 그룹 내 거리를 작게 유지하면서 그룹 간 차이를 어느 정도까지 최대화하느냐 하는 것입니다(그림 5.20에서 빨간색 선의 길이를 최대화하고 검은색 선의 길이를 최소화하는 것). 우리는 이를 그룹 내 제곱 거리 합(within-groups sum of squared distances, WSS)으로 공식화합니다:</p>
<p>\[ <em>k=</em>{}^k <em>{x_i C</em>} d^2(x_i, {x}_{}) \]</p>
<p>여기서 \(k\)는 클러스터의 수, \(C_\)은 \(\)번째 클러스터에 있는 객체들의 집합, 그리고 \({x}_\)은 \(\)번째 클러스터의 질량 중심(평균점)입니다. 우리는 동일한 클러스터 알고리즘에 대해 서로 다른 \(k\) 값에 걸쳐 이 수치를 비교하는 데 관심이 있으므로 식 5.4에서 WSS의 \(k\)에 대한 의존성을 명시합니다. 하지만 WSS 그 자체로는 충분한 기준이 되지 못합니다: WSS의 최솟값은 단순히 각 점을 개별 클러스터로 만듦으로써 얻어질 수 있기 때문입니다. WSS는 유용한 구성 요소이지만, 이 숫자만 보는 것보다 더 정교한 아이디어가 필요합니다.</p>
<p>한 가지 아이디어는 \(k\)의 함수로서 \(_k\)를 살펴보는 것입니다. 이는 항상 감소 함수이겠지만, 급격히 감소하다가 완만해지는 뚜렷한 영역이 있다면, 우리는 이를 _엘보우(elbow)_라고 부르며 이를 클러스터 수의 잠재적인 최적 지점으로 간주할 수 있습니다.</p>
<p>__</p>
<p>질문 5.11</p>
<p>**\(_k\)에 대한 대안적 표현**. R을 사용하여 클러스터 내의 모든 점 쌍 사이의 거리 합을 계산하고 이를 \(_k\)와 비교해 보세요. \(_k\)가 다음과 같이 쓰여질 수 있음을 알 수 있나요?</p>
<p>\[ <em>k=</em>{}^k <em>{x_i C</em>} <em>{x_j C</em>} d^2(x_i,x_j), \]</p>
<p>여기서 \(n_\)은 \(\)번째 클러스터의 크기입니다.</p>
<p>질문 5.11은 클러스터 내 제곱합 \(_k\)가 클러스터 내의 모든 점과 중심 사이의 거리뿐만 아니라, 클러스터 내의 모든 점 쌍 사이의 평균 거리도 측정한다는 것을 보여줍니다.</p>
<p>데이터에 적합한 클러스터 수를 결정하는 데 도움이 되는 다양한 지수와 통계량의 거동을 살펴볼 때, 정답을 실제로 알고 있는 경우를 살펴보는 것이 유용할 수 있습니다.</p>
<p>시작하기 위해, 네 개의 그룹에서 나오는 데이터를 시뮬레이션합니다. 우리는 파이프(<code>%&gt;%</code>) 연산자와 <strong><a href="https://cran.r-project.org/web/packages/dplyr/">dplyr</a></strong>의 <code>bind_rows</code> 함수를 사용하여 각 클러스터에 해당하는 네 개의 _tibble_을 하나의 큰 _tibble_로 연결합니다.6</p>
<p>6 파이프 연산자는 왼쪽에 있는 값을 오른쪽 함수로 전달합니다. 이는 코드에서 데이터의 흐름을 더 쉽게 따라갈 수 있게 해줍니다: <code>f(x) %&gt;% g(y)</code>는 <code>g(f(x), y)</code>와 동일합니다.</p>
<pre><code>library("dplyr")
simdat = lapply(c(0, 8), function(mx) {
  lapply(c(0,8), function(my) {
    tibble(x = rnorm(100, mean = mx, sd = 2),
           y = rnorm(100, mean = my, sd = 2),
           class = paste(mx, my, sep = ":"))
   }) %&gt;% bind_rows
}) %&gt;% bind_rows
simdat __


# A tibble: 400 × 3
        x      y class
    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;
 1 -2.42  -4.59  0:0  
 2  1.89  -1.56  0:0  
 3  0.558  2.17  0:0  
 4  2.51  -0.873 0:0  
 5 -2.52  -0.766 0:0  
 6  3.62   0.953 0:0  
 7  0.774  2.43  0:0  
 8 -1.71  -2.63  0:0  
 9  2.01   1.28  0:0  
10  2.03  -1.25  0:0  
# ℹ 390 more rows


simdatxy = simdat[, c("x", "y")] # class 레이블 제외 __


ggplot(simdat, aes(x = x, y = y, col = class)) + geom_point() +
  coord_fixed()__</code></pre>
<p><a href="05-chap_files/figure- html/fig-simdat-1-1.png" title="그림 5.24: 클래스 레이블로 색상이 입혀진 simdat 데이터. 여기서는 우리가 데이터를 생성했으므로 레이블을 알고 있습니다 – 보통은 이를 알지 못합니다."><img src="05-chap_files/figure-html/fig-simdat-1-1.png" class="img-fluid"></a></p>
<p>그림 5.24: 클래스 레이블로 색상이 입혀진 <code>simdat</code> 데이터. 여기서는 우리가 데이터를 생성했으므로 레이블을 알고 있습니다 – 보통은 이를 알지 못합니다.</p>
<p>우리는 \(k\)-평균 방법으로 얻은 클러스터들에 대해 그룹 내 제곱합을 계산합니다:</p>
<pre><code>wss = tibble(k = 1:8, value = NA_real_)
wss$value[1] = sum(scale(simdatxy, scale = FALSE)^2)
for (i in 2:nrow(wss)) {
  km  = kmeans(simdatxy, centers = wss$k[i])
  wss$value[i] = sum(km$withinss)
}
ggplot(wss, aes(x = k, y = value)) + geom_col()__</code></pre>
<p><a href="05-chap_files/figure-html/fig- WSS-1.png" title="그림 5.25: k의 함수로서 WSS 통계량의 막대 그래프는 마지막 실질적인 도약이 k=4 직전에 있음을 보여줍니다. 이는 이 데이터에 대한 최선의 선택이 k=4임을 나타냅니다."><img src="05-chap_files/figure-html/fig-WSS-1.png" class="img-fluid"></a></p>
<p>그림 5.25: \(k\)의 함수로서 WSS 통계량의 막대 그래프는 마지막 실질적인 도약이 \(k=4\) 직전에 있음을 보여줍니다. 이는 이 데이터에 대한 최선의 선택이 \(k=4\)임을 나타냅니다.</p>
<p>__</p>
<p>질문 5.12</p>
<ol type="1">
<li><p>위의 코드를 여러 번 실행하고 서로 다른 실행에서의 <code>wss</code> 값을 비교해 보세요. 왜 이들은 서로 다를까요?</p></li>
<li><p><code>simdat</code>와 동일한 범위와 차원을 가진, 정규 분포 대신 균등 분포(uniform distributions)로부터 오는 데이터 세트를 만드세요. 이 데이터에 대해 WSS 값을 계산해 보세요. 무엇을 결론지을 수 있나요?</p></li>
</ol>
<p>__</p>
<p>질문 5.13</p>
<p>이른바 <strong>칼린스키-하라바츠(Calinski-Harabasz)</strong> 지수는 WSS와 BSS(between group sums of squares, 그룹 간 제곱합)를 사용합니다. 이는 분산 분석에서 사용되는 \(F\) 통계량 — 인자에 의해 설명되는 평균 제곱합 대 평균 잔차 제곱합의 비율 — 에서 영감을 받았습니다:</p>
<p>\[ (k)= <em>k = </em>{}^k n_({x}_{}-{x})^2, \]</p>
<p>여기서 \({x}\)는 전체 질량 중심(평균점)입니다. <code>simdat</code> 데이터에 대한 칼린스키-하라바츠 지수를 플롯해 보세요.</p>
<p>__</p>
<p>해결책</p>
<p>__</p>
<p>그림 5.26을 생성하는 코드는 다음과 같습니다.</p>
<pre><code>library("fpc")
library("cluster")
CH = tibble(
  k = 2:8,
  value = sapply(k, function(i) {
    p = pam(simdatxy, i)
    calinhara(simdatxy, p$cluster)
  })
)
ggplot(CH, aes(x = k, y = value)) + geom_line() + geom_point() +
  ylab("CH index")__</code></pre>
<p><a href="05-chap_files/figure- html/fig-CHIndex-1-1.png" title="그림 5.26: simdat 데이터에 대해 계산된, 서로 다른 k 선택에 대한 그룹 간 분산과 그룹 내 분산의 비율인 칼린스키-하라바츠 지수."><img src="05-chap_files/figure-html/fig-CHIndex-1-1.png" class="img-fluid"></a></p>
<p>그림 5.26: <code>simdat</code> 데이터에 대해 계산된, 서로 다른 \(k\) 선택에 대한 그룹 간 분산과 그룹 내 분산의 비율인 칼린스키-하라바츠 지수.</p>
<section id="갭-통계량gap-statistic-사용하기" class="level3" data-number="7.6.1">
<h3 data-number="7.6.1" class="anchored" data-anchor-id="갭-통계량gap-statistic-사용하기"><span class="header-section-number">7.6.1</span> 5.7.1 갭 통계량(gap statistic) 사용하기</h3>
<p>그룹 내 제곱합의 로그(\((_k)\))를 취하고 이를 구조가 덜한 시뮬레이션 데이터의 평균과 비교하는 것은 \(k\)를 선택하는 좋은 방법이 될 수 있습니다. 이것이 Tibshirani, Walther, Hastie (<a href="16-chap.html#ref-gap2001">2001</a>)에 의해 도입된 <strong>갭 통계량(gap statistic)</strong>의 기본 아이디어입니다. 우리는 클러스터 수인 여러 \(k\) 값에 대해 \((_k)\)를 계산하고, 이를 다양한 가능한 ‘군집되지 않은’ 분포를 가진 유사한 차원의 참조 데이터에서 얻은 것과 비교합니다. 위에서 했던 것처럼 균등하게 분포된 데이터를 사용하거나, 원래 데이터와 동일한 공분산 구조를 가진 시뮬레이션 데이터를 사용할 수 있습니다.</p>
<p><img src="imgs/roulette.png" class="img-fluid"></p>
<p>이 알고리즘은 관측된 데이터에 대한 갭 통계량을 유사한 구조를 가진 시뮬레이션 데이터의 평균과 비교하는 몬테카를로 방법입니다.</p>
<p><strong>갭 통계량 계산 알고리즘 (<a href="16-chap.html#ref-gap2001">Tibshirani, Walther, and Hastie 2001</a>):</strong></p>
<ol type="1">
<li><p>데이터를 \(k\)개의 클러스터로 군집화하고 다양한 \(k\) 선택에 대해 \(_k\)를 계산합니다.</p></li>
<li><p>균질한 분포로부터 몬테카를로 샘플링을 사용하여 \(B\)개의 그럴듯한 참조 데이터 세트를 생성하고, 이 새로운 시뮬레이션 데이터에 대해 위의 1단계를 다시 수행합니다. 그 결과 시뮬레이션 데이터에 대한 \(B\)개의 새로운 그룹 내 제곱합 \(W_{kb}^*\)(\(b=1,…,B\))을 얻습니다.</p></li>
<li><p>\((k)\)-통계량을 계산합니다:</p></li>
</ol>
<p>\[ (k) = _k - _k <em>k =</em>{b=1}^B W^*_{kb} \]</p>
<p>군집화가 잘 되었다면(즉, WSS가 더 작다면) 첫 번째 항이 두 번째 항보다 클 것으로 예상됩니다. 따라서 갭 통계량은 대부분 양수일 것이며 우리는 그 최댓값을 찾습니다.</p>
<ol start="4" type="1">
<li>표준 편차</li>
</ol>
<p>\[ <em>k^2 = </em>{b=1}<sup>B((W</sup>*_{kb})-_k)^2 \]</p>
<p>를 사용하여 최적의 \(k\)를 선택하는 데 도움을 줄 수 있습니다. 여러 선택지가 있는데, 예를 들어 다음과 같은 최소의 \(k\)를 선택하는 것입니다.</p>
<p>\[ (k) (k+1) - s’<em>{k+1} s’</em>{k+1}=_{k+1}. \]</p>
<p>The packages <strong><a href="https://cran.r-project.org/web/packages/cluster/">cluster</a></strong> and <strong><a href="https://cran.r-project.org/web/packages/clusterCrit/">clusterCrit</a></strong> provide implementations.</p>
<p>__</p>
<p>Question 5.14</p>
<p>Make a function that plots the gap statistic as in Figure 5.27. Show the output for the <code>simdat</code> example dataset clustered with the <code>pam</code> function.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>library("cluster")
library("ggplot2")
pamfun = function(x, k)
  list(cluster = pam(x, k, cluster.only = TRUE))

gss = clusGap(simdatxy, FUN = pamfun, K.max = 8, B = 50,
              verbose = FALSE)
plot_gap = function(x) {
  gstab = data.frame(x$Tab, k = seq_len(nrow(x$Tab)))
  ggplot(gstab, aes(k, gap)) + geom_line() +
    geom_errorbar(aes(ymax = gap + SE.sim,
                      ymin = gap - SE.sim), width=0.1) +
    geom_point(size = 3, col=  "red")
}
plot_gap(gss)__</code></pre>
<p><a href="05-chap_files/figure- html/fig-GapStat-1-1.png" title="Figure 5.27: The gap statistic, see Question wrn- clustering-gapstat."><img src="05-chap_files/figure-html/fig-GapStat-1-1.png" class="img-fluid"></a></p>
<p>Figure 5.27: The gap statistic, see Question 5.14.</p>
<p>Let’s now use the method on a real example. We load the <strong><a href="https://bioconductor.org/packages/Hiiragi/">Hiiragi</a></strong> data that we already explored in <a href="03-chap.html">Chapter 3</a> and will see how the cells cluster.</p>
<pre><code>library("Hiiragi2013")__


In chunk 'Hiiragi': Warning: replacing previous import 'boot::logit' by 'gtools::logit' whenloading 'Hiiragi2013'


In chunk 'Hiiragi': Warning: replacing previous import 'boot::inv.logit' by 'gtools::inv.logit'when loading 'Hiiragi2013'


data("x")__</code></pre>
<p>We start by choosing the 50 most variable genes (features)7.</p>
<p>7 The intention behind this step is to reduce the influence of technical (or batch) effects. Although individually small, when accumulated over all the 45101 features in <code>x</code>, many of which match genes that are weakly or not expressed, without this feature selection step, such effects are prone to suppress the biological signal.</p>
<pre><code>selFeats = order(rowVars(Biobase::exprs(x)), decreasing = TRUE)[1:50]
embmat = t(Biobase::exprs(x)[selFeats, ])
embgap = clusGap(embmat, FUN = pamfun, K.max = 24, verbose = FALSE)
k1 = maxSE(embgap$Tab[, "gap"], embgap$Tab[, "SE.sim"])
k2 = maxSE(embgap$Tab[, "gap"], embgap$Tab[, "SE.sim"],
           method = "Tibs2001SEmax")
c(k1, k2)__


[1] 9 7</code></pre>
<p>The default choice for the number of clusters, <code>k1</code>, is the first value of \(k\) for which the gap is not larger than the first local maximum minus a standard error \(s\) (see the manual page of the <code>clusGap</code> function). This gives a number of clusters \(k = 9\), whereas the choice recommended by Tibshirani, Walther, and Hastie (<a href="16-chap.html#ref-gap2001">2001</a>) is the smallest \(k\) such that \((k) (k+1) - s_{k+1}’\), this gives \(k = 7\). Let’s plot the gap statistic (Figure 5.28).</p>
<pre><code>plot(embgap, main = "")
cl = pamfun(embmat, k = k1)$cluster
table(pData(x)[names(cl), "sampleGroup"], cl)__


                 cl
                   1  2  3  4  5  6  7  8  9
  E3.25           23 11  1  1  0  0  0  0  0
  E3.25 (FGF4-KO)  0  0  1 16  0  0  0  0  0
  E3.5 (EPI)       2  1  0  0  0  8  0  0  0
  E3.5 (FGF4-KO)   0  0  8  0  0  0  0  0  0
  E3.5 (PE)        0  0  0  0  9  2  0  0  0
  E4.5 (EPI)       0  0  0  0  0  0  0  4  0
  E4.5 (FGF4-KO)   0  0  0  0  0  0  0  0 10
  E4.5 (PE)        0  0  0  0  0  0  4  0  0</code></pre>
<p><a href="05-chap_files/figure- html/fig-gapHiiragi-1-1.png" title="그림 5.28: Hiiragi2013 데이터에 대한 갭 통계량."><img src="05-chap_files/figure-html/fig-gapHiiragi-1-1.png" class="img-fluid"></a></p>
<p>그림 5.28: <strong><a href="https://bioconductor.org/packages/Hiiragi2013/">Hiiragi2013</a></strong> 데이터에 대한 갭 통계량.</p>
<p>위에서 우리는 <code>pamfun</code>으로부터 얻은 군집화 결과와 데이터의 어노테이션에 포함된 샘플 레이블을 비교한 것을 볼 수 있습니다.</p>
<p>__</p>
<p>질문 5.15</p>
<p>가장 가변적인 상위 50개 유전자만 사용하는 대신 <code>x</code>의 모든 특성을 사용하면 결과가 어떻게 달라질까요?</p>
</section>
<section id="붓스트랩을-이용한-클러스터-검증" class="level3" data-number="7.6.2">
<h3 data-number="7.6.2" class="anchored" data-anchor-id="붓스트랩을-이용한-클러스터-검증"><span class="header-section-number">7.6.2</span> 5.7.2 붓스트랩을 이용한 클러스터 검증</h3>
<p>[<img src="imgs/BootstrapClusterNew.png" class="img-fluid">](imgs/BootstrapClusterNew.png “그림 5.29 (a):”)</p>
<ol type="a">
<li></li>
</ol>
<p>[<img src="imgs/BootstrapCluster2New.png" class="img-fluid">](imgs/BootstrapCluster2New.png “그림 5.29 (b):”)</p>
<ol start="2" type="a">
<li></li>
</ol>
<p>그림 5.29: 동일한 분포 \(F\)로부터 얻은 서로 다른 표본들은 서로 다른 군집화 결과로 이어집니다. (a)에서 우리는 실제 표집 가변성을 봅니다. 붓스트랩은 (b)에서 보듯이 경험적 분포 함수 \(_n\)을 사용하여 하위 표본(subsamples)을 추출함으로써 이러한 표집 가변성을 시뮬레이션합니다.</p>
<p>우리는 <a href="04-chap.html">4장</a>에서 붓스트랩 원리를 보았습니다: 이상적으로는 기저의 데이터 생성 프로세스로부터 많은 새로운 표본(데이터 세트)을 얻어 각각에 군집화 방법을 적용한 다음, 군집화를 비교하기 위해 위에서 사용했던 것과 같은 지수를 사용하여 클러스터가 얼마나 안정적인지 또는 얼마나 변하는지 보고 싶을 것입니다. 물론 우리에게는 이러한 추가 표본이 없습니다. 따라서 우리는 단순히 데이터의 서로 다른 무작위 하위 표본을 취하여 매번 얻는 서로 다른 군집화 결과를 비교함으로써 새로운 데이터 세트를 만들 것입니다. Tibshirani, Walther, Hastie (<a href="16-chap.html#ref-gap2001">2001</a>)는 갭 통계량을 사용하여 클러스터 수를 추론할 때 붓스트랩 재표본 추출을 사용할 것을 권장합니다.</p>
<p>우리는 계속해서 <strong><a href="https://bioconductor.org/packages/Hiiragi2013/">Hiiragi2013</a></strong> 데이터를 사용하겠습니다. 여기서는 생쥐 배아의 배아기 3.5일(E3.5) 배반포의 내세포집단(ICM)이 다분화능 에피블라스트(EPI)와 원시 내배엽(PE)에 해당하는 두 개의 클러스터로 “자연스럽게” 나뉘는 반면, 배아기 3.25일(E3.25)의 데이터는 아직 이러한 대칭성 붕괴(symmetry breaking)를 보여주지 않는다는 가설에 대한 조사를 따라갑니다.</p>
<p>우리는 군집화 과정에서 실제 그룹 레이블을 사용하지 않고, 결과의 최종 해석에서만 이를 사용할 것입니다. 우리는 (E3.5)와 (E3.25)라는 두 가지 서로 다른 데이터 세트에 각각 붓스트랩을 적용할 것입니다. 붓스트랩의 각 단계는 데이터의 무작위 하위 집합에 대한 군집화 결과를 생성할 것이며, 우리는 이를 클러스터 앙상블의 합의(consensus)를 통해 비교해야 할 것입니다. <strong><a href="https://cran.r-project.org/web/packages/clue/">clue</a></strong> 패키지(<a href="16-chap.html#ref-Hornik2005">Hornik 2005</a>)에 이를 위한 유용한 프레임워크가 있습니다. Ohnishi 등 (<a href="16-chap.html#ref-Ohnishi2014">2014</a>)의 부록에서 가져온 <code>clusterResampling</code> 함수가 이 접근 방식을 구현합니다:</p>
<pre><code>clusterResampling = function(x, ngenes = 50, k = 2, B = 250,
                             prob = 0.67) {
  mat = Biobase::exprs(x)
  ce = cl_ensemble(list = lapply(seq_len(B), function(b) {
    selSamps = sample(ncol(mat), size = round(prob * ncol(mat)),
                      replace = FALSE)
    submat = mat[, selSamps, drop = FALSE]
    sel = order(rowVars(submat), decreasing = TRUE)[seq_len(ngenes)]
    submat = submat[sel,, drop = FALSE]
    pamres = pam(t(submat), k = k)
    pred = cl_predict(pamres, t(mat[sel, ]), "memberships")
    as.cl_partition(pred)
  }))
  cons = cl_consensus(ce)
  ag = sapply(ce, cl_agreement, y = cons)
  list(agreements = ag, consensus = cons)
}__</code></pre>
<p><code>clusterResampling</code> 함수는 다음 단계들을 수행합니다:</p>
<ol type="1">
<li><p>복원 추출 없이 샘플의 67%를 선택하여 데이터(모든 E3.25 샘플 또는 모든 E3.5 샘플)의 무작위 하위 집합을 추출합니다.</p></li>
<li><p>(하위 집합에서) 전체 분산 기준 상위 <code>ngenes</code>개의 특성을 선택합니다.</p></li>
<li><p>\(k\)-평균 군집화를 적용하고, <strong><a href="https://cran.r-project.org/web/packages/clue/">clue</a></strong> 패키지의 <code>cl_predict</code> 메서드를 사용하여 하위 집합에 포함되지 않았던 샘플들이 어느 클러스터 중심과 가장 가까운지에 따라 그들의 클러스터 멤버십을 예측합니다.</p></li>
<li><p>1-3단계를 <code>B</code>번 반복합니다.</p></li>
<li><p>합의 군집화(consensus clustering, <code>cl_consensus</code>)를 적용합니다.</p></li>
<li><p><code>B</code>개의 군집화 결과 각각에 대해 <code>cl_agreement</code> 함수를 통해 합의 결과와의 일치도를 측정합니다. 여기서 일치도가 높으면 1에 가까운 값을, 낮으면 더 작은 값을 갖습니다. 일치도가 전반적으로 높다면 \(k\)개 클래스로의 군집화는 안정적이고 재현 가능한 것으로 간주될 수 있습니다. 반대로 낮다면 샘플들을 \(k\)개 클러스터로 나누는 안정적인 분할이 뚜렷하지 않음을 의미합니다.</p></li>
</ol>
<p>합의 군집화를 위한 클러스터 간 거리 척도로 멤버십의 <em>유클리드</em> 비유사성이 사용됩니다. 즉, \(\)와 \(\)의 모든 열 순열 사이의 최소 제곱 차이합의 제곱근입니다(여기서 \(\)와 \(\)는 클러스터 멤버십 행렬임). 일치도 측정값으로는 \(1 - d/m\) 수치가 사용되는데, 여기서 \(d\)는 유클리드 비유사성이고 \(m\)은 최대 유클리드 비유사성의 상한선입니다.</p>
<pre><code>iswt = (x$genotype == "WT")
cr1 = clusterResampling(x[, x$Embryonic.day == "E3.25" &amp; iswt])
cr2 = clusterResampling(x[, x$Embryonic.day == "E3.5"  &amp; iswt])__</code></pre>
<p>결과는 그림 5.30에 나와 있습니다. 이들은 E3.5 데이터가 두 개의 클러스터로 나뉜다는 가설을 확인해 줍니다.</p>
<pre><code>ag1 = tibble(agreements = cr1$agreements, day = "E3.25")
ag2 = tibble(agreements = cr2$agreements, day = "E3.5")
p1 &lt;- ggplot(bind_rows(ag1, ag2), aes(x = day, y = agreements)) +
  geom_boxplot() +
  ggbeeswarm::geom_beeswarm(cex = 1.5, col = "#0000ff40")
mem1 = tibble(y = sort(cl_membership(cr1$consensus)[, 1]),
              x = seq(along = y), day = "E3.25")
mem2 = tibble(y = sort(cl_membership(cr2$consensus)[, 1]),
              x = seq(along = y), day = "E3.5")
p2 &lt;- ggplot(bind_rows(mem1, mem2), aes(x = x, y = y, col = day)) +
  geom_point() + facet_grid(~ day, scales = "free_x")
gridExtra::grid.arrange(p1, p2, widths = c(2.4,4.0))__</code></pre>
<p><a href="05-chap_files/figure- html/fig-figClue1-1.png" title="그림 5.30: E3.25 및 E3.5 샘플을 이용한 클러스터 안정성 분석. 왼쪽: B개의 군집화 결과와 합의 결과 사이의 일치도에 대한 beeswarm 플롯; 1은 완벽한 일치를 나타내며, 낮은 값은 낮은 일치도를 나타냅니다. 오른쪽: 합의 군집화의 멤버십 확률. E3.25의 경우 확률이 분산되어 있어 개별 군집화 결과들이 자주 일치하지 않음을 나타내는 반면, E3.5의 경우 분포가 이봉형(bimodal)이며 모호한 샘플은 하나뿐입니다."><img src="05-chap_files/figure-html/fig-figClue1-1.png" class="img-fluid"></a></p>
<p>그림 5.30: E3.25 및 E3.5 샘플을 이용한 클러스터 안정성 분석. 왼쪽: <code>B</code>개의 군집화 결과와 합의 결과 사이의 일치도에 대한 beeswarm 플롯; \(1\)은 완벽한 일치를 나타내며, 낮은 값은 낮은 일치도를 나타냅니다. 오른쪽: 합의 군집화의 멤버십 확률. E3.25의 경우 확률이 분산되어 있어 개별 군집화 결과들이 자주 일치하지 않음을 나타내는 반면, E3.5의 경우 분포가 이봉형(bimodal)이며 모호한 샘플은 하나뿐입니다.</p>
<section id="계산-및-메모리-문제" class="level4" data-number="7.6.2.1">
<h4 data-number="7.6.2.1" class="anchored" data-anchor-id="계산-및-메모리-문제"><span class="header-section-number">7.6.2.1</span> 계산 및 메모리 문제</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="계산 복잡도. 어떤 알고리즘이 O(n^k)라고 불리는 것은, n이 커짐에 따라 자원 소비(CPU 시간 또는 메모리)가 n^k에 비례하여 증가함을 의미합니다. 다른(때로는 상당한) 기본 비용이나 n의 더 낮은 거듭제곱에 비례하여 증가하는 비용이 있을 수 있지만, n이 무한대로 감에 따라 이들은 항상 지배적인 항에 비해 무시할 수 있는 수준이 됩니다."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>계산 복잡도. 어떤 알고리즘이 O(n^k)라고 불리는 것은, n이 커짐에 따라 자원 소비(CPU 시간 또는 메모리)가 n^k에 비례하여 증가함을 의미합니다. 다른(때로는 상당한) 기본 비용이나 n의 더 낮은 거듭제곱에 비례하여 증가하는 비용이 있을 수 있지만, n이 무한대로 감에 따라 이들은 항상 지배적인 항에 비해 무시할 수 있는 수준이 됩니다.</figcaption>
</figure>
</div>
<p><strong>계산 복잡도(Computational complexity)</strong>. 어떤 알고리즘이 \(O(n^k)\)라고 불리는 것은, \(n\)이 커짐에 따라 자원 소비(CPU 시간 또는 메모리)가 \(n^k\)에 비례하여 증가함을 의미합니다. 다른(때로는 상당한) 기본 비용이나 \(n\)의 더 낮은 거듭제곱에 비례하여 증가하는 비용이 있을 수 있지만, \(n\)임에 따라 이들은 항상 지배적인 항에 비해 무시할 수 있는 수준이 됩니다.</p>
<p>\(n\)개 객체의 전체 대 전체 거리 계산이 (시간과 메모리 측면에서) \(O(n^2)\) 작업임을 기억하는 것이 중요합니다. 전통적인 계층적 군집화 접근 방식(<strong><a href="https://cran.r-project.org/web/packages/stats/">stats</a></strong> 패키지의 <code>hclust</code> 등)은 시간 측면에서 심지어 \(O(n^3)\)입니다. \(n\)이 크다면 이는 비실용적일 수 있습니다8. 우리는 전체 대 전체 거리 행렬의 완전한 계산을 피할 수 있습니다. 예를 들어, \(k\)-평균은 각 객체와 클러스터 중심 사이의 거리만 추적하면 되므로 \(O(n)\)의 계산만 필요하다는 장점이 있으며, 클러스터 중심의 수는 \(n\)이 증가하더라도 일정하게 유지됩니다.</p>
<p>8 예를 들어 백만 개 객체에 대한 거리 행렬을 8바이트 부동 소수점으로 저장하면 약 4테라바이트를 차지하며, <code>hclust</code>와 같은 알고리즘은 각 반복이 1나노초만 걸린다는 낙관적인 가정 하에서도 30년 동안 실행될 것입니다.</p>
<p><strong><a href="https://cran.r-project.org/web/packages/fastclust/">fastclust</a></strong> (<a href="16-chap.html#ref-Mullner:2013">Müllner 2013</a>) 및 <strong><a href="https://cran.r-project.org/web/packages/dbscan/">dbscan</a></strong>과 같은 빠른 구현체들은 많은 수의 관측치를 처리하기 위해 신중하게 최적화되었습니다.</p>
</section>
</section>
</section>
<section id="노이즈-제거-수단으로서의-군집화" class="level2" data-number="7.7">
<h2 data-number="7.7" class="anchored" data-anchor-id="노이즈-제거-수단으로서의-군집화"><span class="header-section-number">7.7</span> 5.8 노이즈 제거 수단으로서의 군집화</h2>
<p>어떤 기저의 실제 값(예를 들어 게놈의 DNA 서열로 표현되는 종)을 반영하지만 기술적 노이즈에 의해 변질된 측정값 세트를 생각해 봅시다. 군집화는 이러한 노이즈를 제거하는 데 사용될 수 있습니다.</p>
<section id="서로-다른-베이스라인-빈도를-가진-노이즈-섞인-관측치" class="level3" data-number="7.7.1">
<h3 data-number="7.7.1" class="anchored" data-anchor-id="서로-다른-베이스라인-빈도를-가진-노이즈-섞인-관측치"><span class="header-section-number">7.7.1</span> 5.8.1 서로 다른 베이스라인 빈도를 가진 노이즈 섞인 관측치</h3>
<p>동일한 오차 분산으로 만들어진 관측치들의 이변량 분포(bivariate distribution)가 있다고 가정해 봅시다. 그러나 샘플링은 베이스라인 빈도가 매우 다른 두 그룹으로부터 이루어집니다. 더 나아가, 오차는 연속적이고 독립적인 이변량 정규 분포를 따른다고 가정합시다. 다음 코드에서 생성된 것처럼 우리는 <code>seq1</code>을 \(10^{3}\)개, <code>seq2</code>를 \(10^{5}\)개 가지고 있습니다:</p>
<pre><code>library("mixtools")
seq1 = rmvnorm(n = 1e3, mu = -c(1, 1), sigma = 0.5 * diag(c(1, 1)))
seq2 = rmvnorm(n = 1e5, mu =  c(1, 1), sigma = 0.5 * diag(c(1, 1)))
twogr = data.frame(
  rbind(seq1, seq2),
  seq = factor(c(rep(1, nrow(seq1)),
                 rep(2, nrow(seq2))))
)
colnames(twogr)[1:2] = c("x", "y")
library("ggplot2")
ggplot(twogr, aes(x = x, y = y, colour = seq,fill = seq)) +
  geom_hex(alpha = 0.5, bins = 50) + coord_fixed()__</code></pre>
<p><a href="05-chap_files/figure- html/fig-seqradius-1.png" title="그림 5.31: 두 그룹 모두 동일한 분산의 노이즈 분포를 가지고 있음에도 불구하고, 그룹의 겉보기 반지름은 매우 다릅니다. seq2의 10^{5}개 인스턴스는 10^{3}개뿐인 seq1보다 오차가 발생할 기회가 훨씬 더 많습니다. 따라서 우리는 데이터를 군집화할 때 빈도가 중요하다는 것을 알 수 있습니다."><img src="05-chap_files/figure-html/fig-seqradius-1.png" class="img-fluid"></a></p>
<p>그림 5.31: 두 그룹 모두 동일한 분산의 노이즈 분포를 가지고 있음에도 불구하고, 그룹의 겉보기 반지름은 매우 다릅니다. <code>seq2</code>의 \(10^{5}\)개 인스턴스는 \(10^{3}\)개뿐인 <code>seq1</code>보다 오차가 발생할 기회가 훨씬 더 많습니다. 따라서 우리는 데이터를 군집화할 때 빈도가 중요하다는 것을 알 수 있습니다.</p>
<p>관측된 값들은 그림 5.31과 같이 보일 것입니다.</p>
<p>__</p>
<p>질문 5.16</p>
<p>데이터 <code>seq1</code>과 <code>seq2</code>를 가져와서 그룹 중심으로부터의 거리에 따라 두 그룹으로 군집화해 보세요. 결과가 두 서열 유형 각각의 빈도에 의존해야 한다고 생각하시나요?</p>
<p>__</p>
<p>해결책</p>
<p>__</p>
<p>분류학적 군집화(taxonomic clustering), 즉 OTU(operational taxonomic unit) 군집화(<a href="16-chap.html#ref-caporaso2010qiime">Caporaso et al.&nbsp;2010</a>; <a href="16-chap.html#ref-mothur">P. D. Schloss et al.&nbsp;2009</a>) 방법들에서 종종 사용되는 이러한 접근 방식은 최적이 아닙니다.</p>
<p>오직 유사성에만 기반한 방법들은 <em>대표성(representativeness)</em> 휴리스틱에 내재된 편향으로 인해 고통받습니다. 군집화 및 분류학적 할당에서 대표성과 거리 기반 휴리스틱만을 사용하려는 우리의 자연스러운 경향이 어떻게 편향된 결과로 이어질 수 있는지 설명하는 데 도움이 되는 인지 심리학의 세계로 잠시 외도해 봅시다.</p>
<p>1970년대에 Tversky와 Kahneman (<a href="16-chap.html#ref-tversky1975judgment">1975</a>)은 우리가 일반적으로 가장 유사한 _대표자(representatives)_를 살펴봄으로써 그룹을 할당한다고 지적했습니다. 군집화와 그룹 할당에서 이는 새로운 서열을 그 중심까지의 거리에 따라 그룹에 할당하는 것을 의미할 것입니다. 사실 이는 서로 다른 그룹의 보급률(prevalence) 차이를 무시하고 동일한 반지름을 가진 공을 취하는 것과 같습니다. 이러한 심리학적 오류는 많은 다양한 휴리스틱과 편향을 다루는 중요한 Science 논문(<a href="16-chap.html#ref-tversky1974heuristics">Tversky and Kahneman 1974</a>)에서 처음 논의되었습니다.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/book_icon.png" title="우리의 자연스러운 휴리스틱과 우리가 확률 계산을 할 때 그것들이 우리를 오도할 수 있는 방식에 대한 단행본 규모의 논의는 @kahneman2011을 참조하십시오(특히 14장과 15장을 추천합니다)."><img src="imgs/book_icon.png" class="img-fluid figure-img"></a></p>
<figcaption>우리의 자연스러운 휴리스틱과 우리가 확률 계산을 할 때 그것들이 우리를 오도할 수 있는 방식에 대한 단행본 규모의 논의는 Kahneman (2011)을 참조하십시오(특히 14장과 15장을 추천합니다).</figcaption>
</figure>
</div>
<p>우리의 자연스러운 휴리스틱과 우리가 확률 계산을 할 때 그것들이 우리를 오도할 수 있는 방식에 대한 단행본 규모의 논의는 Kahneman (<a href="16-chap.html#ref-kahneman2011">2011</a>)을 참조하십시오. (특히 14장과 15장을 추천합니다.) when we make probability calculations (we recommend especially Chapters 14 and 15).</p>
<p>__</p>
<p>Task</p>
<p>Simulate <code>n=2000</code> binary variables of length <code>len=200</code> that indicate the quality of <code>n</code> sequencing reads of length <code>len</code>. For simplicity, let us assume that sequencing errors occur independently and uniformly with probability <code>perr=0.001</code>. That is, we only care whether a base was called correctly (<code>TRUE</code>) or not (<code>FALSE</code>).</p>
<pre><code>n    = 2000
len  = 200
perr = 0.001
seqs = matrix(runif(n * len) &gt;= perr, nrow = n, ncol = len)__</code></pre>
<p>Now, compute all pairwise distances between reads.</p>
<pre><code>dists = as.matrix(dist(seqs, method = "manhattan"))__</code></pre>
<p>For various values of number of reads <code>k</code> (from 2 to <code>n</code>), the maximum distance within this set of reads is computed by the code below and shown in Figure 5.32.</p>
<pre><code>library("tibble")
dfseqs = tibble(
  k = 10 ^ seq(log10(2), log10(n), length.out = 20),
  diameter = vapply(k, function(i) {
    s = sample(n, i)
    max(dists[s, s])
    }, numeric(1)))
ggplot(dfseqs, aes(x = k, y = diameter)) + geom_point()+geom_smooth()__</code></pre>
<p><a href="05-chap_files/figure- html/fig-diameter-1.png" title="Figure 5.32: The diameter of a set of sequences as a function of the number of sequences."><img src="05-chap_files/figure-html/fig-diameter-1.png" class="img-fluid"></a></p>
<p>Figure 5.32: The diameter of a set of sequences as a function of the number of sequences.</p>
<p>We will now improve the 16SrRNA-read clustering using a denoising mechanism that incorporates error probabilities.</p>
</section>
<section id="denoising-16s-rrna-sequences" class="level3" data-number="7.7.2">
<h3 data-number="7.7.2" class="anchored" data-anchor-id="denoising-16s-rrna-sequences"><span class="header-section-number">7.7.2</span> 5.8.2 Denoising 16S rRNA sequences</h3>
<p><strong>What are the data?</strong> In the bacterial 16SrRNA gene there are so-called <strong>variable regions</strong> that are taxa-specific. These provide fingerprints that enables <em>taxon</em> 9 identification. The raw data are FASTQ-files with quality scored sequences of PCR-amplified DNA regions10. We use an iterative alternating approach11 to build a probabilistic noise model from the data. We call this a <em>de novo</em> method, because we use clustering, and we use the cluster centers as our denoised sequence variants (a.k.a. Amplicon Sequence Variants, ASVs, see (<a href="16-chap.html#ref-Callahan:2017">Benjamin J. Callahan, McMurdie, and Holmes 2017</a>)). After finding all the denoised variants, we create contingency tables of their counts across the different samples. We will show in <a href="10-chap.html">Chapter 10</a> how these tables can be used to infer properties of the underlying bacterial communities using networks and graphs.</p>
<p>9 Calling different groups of bacteria <em>taxa</em> rather than <em>species</em> highlights the approximate nature of the concept, as the notion of species is more fluid in bacteria than, say, in animals.</p>
<p>10 The <a href="https://en.wikipedia.org/wiki/FASTQ_format">FASTQ format is described here</a>.</p>
<p>11 Similar to the EM algorithm we saw in <a href="04-chap.html">Chapter 4</a>.</p>
<p>In order to improve data quality, one often has to start with the raw data and model all the sources of variation carefully. One can think of this as an example of <em>cooking from scratch</em> (see the gruesome details in Ben J. Callahan et al.&nbsp;(<a href="16-chap.html#ref-Callahan2016Bioc">2016</a>) and Exercise 5.5).</p>
<p>__</p>
<p>Question 5.17</p>
<p>Suppose that we have two sequences of length 200 (<code>seq1</code> and <code>seq2</code>) present in our sample at very different abundances. We are told that the technological sequencing errors occur as independent Bernoulli(0.0005) random events for each nucleotide.<br>
What is the distribution of the number of errors per sequence?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Probability theory tells us that the sum of 200 independent Poisson(0.0005) will be Poisson(0.1).</p>
<p>We can also verify this by Monte Carlo simulation:</p>
<pre><code>simseq10K = replicate(1e5, sum(rpois(200, 0.0005)))
mean(simseq10K)__


[1] 0.10143


vcd::distplot(simseq10K, "poisson")__</code></pre>
<p><a href="05-chap_files/figure- html/fig-seqradiusex-1.png" title="Figure 5.33: distplot for the simseq10K data."><img src="05-chap_files/figure-html/fig-seqradiusex-1.png" class="img-fluid"></a></p>
<p>Figure 5.33: <code>distplot</code> for the <code>simseq10K</code> data.</p>
<p>Figure 5.33 shows us how close the distribution is to being Poisson distributed.</p>
</section>
<section id="infer-sequence-variants" class="level3" data-number="7.7.3">
<h3 data-number="7.7.3" class="anchored" data-anchor-id="infer-sequence-variants"><span class="header-section-number">7.7.3</span> 5.8.3 Infer sequence variants</h3>
<p>The DADA method (Divisive Amplicon Denoising Algorithm, Rosen et al. (<a href="16-chap.html#ref-Rosen:2012">2012</a>)) uses a parameterized model of substitution errors that distinguishes sequencing errors from real biological variation. The model computes the probabilities of base substitutions, such as seeing an \({}\) instead of a \({}\). It assumes that these probabilities are independent of the position along the sequence. Because error rates vary substantially between sequencing runs and PCR protocols, the model parameters are estimated from the data themselves using an EM-type approach. A read is classified as noisy or exact given the current parameters, and the noise model parameters are updated accordingly12.</p>
<p>12 In the case of a large data set, the noise model estimation step does not have to be done on the complete set. See <a href="https://benjjneb.github.io/dada2/bigdata.html" class="uri">https://benjjneb.github.io/dada2/bigdata.html</a> for tricks and tools when dealing with large data sets.</p>
<p>13 F stands for forward strand and R for reverse strand.</p>
<p>The dereplicated sequences13 are read in and then divisive denoising and estimation is run with the <code>dada</code> function as in the following code:</p>
<pre><code>derepFs = readRDS(file="../data/derepFs.rds")
derepRs = readRDS(file="../data/derepRs.rds")
library("dada2")
ddF = dada(derepFs, err = NULL, selfConsist = TRUE)
ddR = dada(derepRs, err = NULL, selfConsist = TRUE)__</code></pre>
<p>In order to verify that the error transition rates have been reasonably well estimated, we inspect the fit between the observed error rates (black points) and the fitted error rates (black lines) (Figure 5.34).</p>
<pre><code>plotErrors(ddF)__


In chunk 'fig-rerrorprofile1': Warning: Removed 82 rows containing missing values or values outside the scale range(`geom_line()`).</code></pre>
<p><a href="05-chap_files/figure-html/fig-rerrorprofile1-1.png &quot;Figure 5.34: Forward transition error rates as provided by plotErrors(ddF). This shows the frequencies of each type of nucleotide transition as a function of quality.&quot;"><img src="05-chap_files/figure-html/fig- rerrorprofile1-1.png" class="img-fluid"></a></p>
<p>Figure 5.34: Forward transition error rates as provided by <code>plotErrors(ddF)</code>. This shows the frequencies of each type of nucleotide transition as a function of quality.</p>
<p>Once the errors have been estimated, the algorithm is rerun on the data to find the sequence variants:</p>
<pre><code>dadaFs = dada(derepFs, err=ddF[[1]]$err_out, pool = TRUE)
dadaRs = dada(derepRs, err=ddR[[1]]$err_out, pool = TRUE)__</code></pre>
<p><strong>Note:</strong> The sequence inference function can run in two different modes: Independent inference by sample (<code>pool = FALSE</code>), and pooled inference from the sequencing reads combined from all samples. Independent inference has two advantages: as a functions of the number of samples, computation time is linear and memory requirements are constant. Pooled inference is more computationally taxing, however it can improve the detection of rare variants that occur just once or twice in an individual sample but more often across all samples. As this dataset is not particularly large, we performed pooled inference.</p>
<p>Sequence inference removes nearly all substitution and <strong>indel</strong> 14 errors from the data. We merge the inferred forward and reverse sequences, while removing paired sequences that do not perfectly overlap as a final control against residual errors.</p>
<p>14 The term <em>indel</em> stands for insertion-deletion; when comparing two sequences that differ by a small stretch of characters, it is a matter of viewpoint whether this is an insertion or a deletion, thus the name.</p>
<pre><code>mergers = mergePairs(dadaFs, derepFs, dadaRs, derepRs)__</code></pre>
<p>We produce a contingency table of counts of ASVs. This is a higher-resolution analogue of the “OTU15 table”, i.e., a samples by features table whose cells contain the number of times each sequence variant was observed in each sample.</p>
<p>15 operational taxonomic units</p>
<pre><code>seqtab.all = makeSequenceTable(mergers[!grepl("Mock",names(mergers))])__</code></pre>
<p>__</p>
<p>Question 5.18</p>
<p>Explore the components of the objects <code>dadaRs</code> and <code>mergers</code>.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p><code>dadaRs</code> is a list of length 20. Its elements are objects class <em>dada</em> that contain the denoised reads. We will see in <a href="10-chap.html">Chapter 10</a> how to align the sequences, assign their taxonomies and combine them with the sample information for downstream analyses.</p>
<pre><code>[1] 20


[1] 20


[1] "list"


 [1] "F3D0"   "F3D1"   "F3D141" "F3D142" "F3D143" "F3D144" "F3D145" "F3D146"
 [9] "F3D147" "F3D148" "F3D149" "F3D150" "F3D2"   "F3D3"   "F3D5"   "F3D6"  
[17] "F3D7"   "F3D8"   "F3D9"   "Mock"  


[1] "list"


[1] 20</code></pre>
<p>Chimera are sequences that are artificially created during PCR amplification by the melding of two (in rare cases, more) of the original sequences. To complete our denoising workflow, we remove them with a call to the function <code>removeBimeraDenovo</code>, leaving us with a clean contingency table we will use later on.</p>
<pre><code>seqtab = removeBimeraDenovo(seqtab.all)__</code></pre>
<p>__</p>
<p>Question 5.19</p>
<p>Why do you think the chimera are quite easy to recognize?<br>
What proportion of the reads were chimeric in the <code>seqtab.all</code> data?<br>
What proportion of unique sequence variants are chimeric?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Here we observed some sequence variants as chimeric, but these only represent 7% of all reads.</p>
</section>
</section>
<section id="summary-of-this-chapter" class="level2" data-number="7.8">
<h2 data-number="7.8" class="anchored" data-anchor-id="summary-of-this-chapter"><span class="header-section-number">7.8</span> 5.9 Summary of this chapter</h2>
<p><strong>Of a feather: how to compare observations</strong> We saw at the start of the chapter how finding the <strong>right distance</strong> is an essential first step in a clustering analysis; this is a case where the <em>garbage in, garbage out</em> motto is in full force. Always choose a distance that is scientifically meaningful and compare output from as many distances as possible; sometimes the same data require different distances when different scientific objectives are pursued.</p>
<p><strong>Two ways of clustering</strong> We saw there are two approaches to clustering:</p>
<ul>
<li><p>iterative partitioning approaches such as \(k\)-means and \(k\)-medoids (PAM) that alternated between estimating the cluster centers and assigning points to them;</p></li>
<li><p>hierarchical clustering approaches that first agglomerate points, and subsequently the growing clusters, into nested sequences of sets that can be represented by hierarchical clustering <em>trees</em>.</p></li>
</ul>
<p><strong>Biological examples</strong> Clustering is important tool for finding latent classes in single cell measurements, especially in immunology and single cell data analyses. We saw how density-based clustering is useful for lower dimensional data where sparsity is not an issue.</p>
<p><strong>Validating</strong> Clustering algorithms <em>always</em> deliver clusters, so we need to assess their quality and the number of clusters to choose carefully. Such validation steps are performed using visualization tools and repeating the clustering on many resamples of the data. We saw how statistics such as WSS/BSS or \(()\) can be calibrated using simulations on data where we understand the group structure and can provide useful benchmarks for choosing the number of clusters on new data. Of course, the use of biologically relevant information to inform and confirm the meaning of clusters is always the best validation approach.</p>
<p>There is arguably no ground truth to compare a clustering result against, in general. The old adage of “all models are wrong, some are useful” also applies here. A good clustering is one that turns out to be useful.</p>
<p><strong>Distances and probabilities</strong> Finally: distances are not everything. We showed how important it was to take into account baseline frequencies and local densities when clustering. This is essential in a cases such as clustering to denoise 16S rRNA sequence reads where the true class or taxa group occur at very different frequencies.</p>
</section>
<section id="further-reading" class="level2" data-number="7.9">
<h2 data-number="7.9" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">7.9</span> 5.10 Further reading</h2>
<p>For a complete book on <em>Finding groups in data</em> , see Kaufman and Rousseeuw (<a href="16-chap.html#ref-Kaufman2009">2009</a>). The vignette of the <strong><a href="https://bioconductor.org/packages/clusterExperiment/">clusterExperiment</a></strong> package contains a complete workflow for generating clusters using many different techniques, including preliminary dimension reduction (PCA) that we will cover in <a href="07-chap.html">Chapter 7</a>. There is no consensus on methods for deciding how many clusters are needed to describe data in the absence of contiguous biological information. However, making hierarchical clusters of the <em>strong forms</em> is a method that has the advantage of allowing the user to decide how far down to cut the hierarchical tree and be careful not to cut in places where these inner branches are short. See the vignette of <strong><a href="https://bioconductor.org/packages/clusterExperiment/">clusterExperiment</a></strong> for an application to single cell RNA experimental data.</p>
<p>In analyzing the Hiiragi data, we used cluster probabilities, a concept already mentioned in <a href="04-chap.html">Chapter 4</a>, where the EM algorithm used them as weights to compute expected value statistics. The notion of probabilistic clustering is well-developed in the Bayesian nonparametric mixture framework, which enriches the mixture models we covered in <a href="04-chap.html">Chapter 4</a> to more general settings. See Dundar et al. (<a href="16-chap.html#ref-Dundar2014">2014</a>) for a real example using this framework for flow cytometry. In the denoising and assignment of high-throughput sequencing reads to specific strains of bacteria or viruses, clustering is essential. In the presence of noise, clustering into groups of <em>true</em> strains of very unequal sizes can be challenging. Using the data to create a noise model enables both denoising and cluster assignment concurrently. Denoising algorithms such as those by Rosen et al.&nbsp;(<a href="16-chap.html#ref-Rosen:2012">2012</a>) or Benjamin J. Callahan et al.&nbsp;(<a href="16-chap.html#ref-dada2">2016</a>) use an iterative workflow inspired by the EM method (<a href="16-chap.html#ref-mclachlan2007algorithm">McLachlan and Krishnan 2007</a>).</p>
</section>
<section id="exercises" class="level2" data-number="7.10">
<h2 data-number="7.10" class="anchored" data-anchor-id="exercises"><span class="header-section-number">7.10</span> 5.11 Exercises</h2>
<p>__</p>
<p>Exercise 5.1</p>
<p>We can define the average dissimilarity of a point \(x_i\) to a cluster \(C_k\) as the average of the distances from \(x_i\) to all points in \(C_k\). Let \(A(i)\) be the average dissimilarity of all points in the cluster that \(x_i\) belongs to. Let \(B(i)\) be the lowest average dissimilarity of \(x_i\) to any other cluster of which \(x_i\) is not a member. The cluster with this lowest average dissimilarity is said to be the <strong>neighboring cluster</strong> of \(x_i\), because it is the next best fit cluster for point \(x_i\). The <strong>silhouette index</strong> is</p>
<p>\[ S(i)=. \]</p>
<p>Compute the silhouette index for the <code>simdat</code> data we simulated in Section 5.7.</p>
<pre><code>library("cluster")
pam4 = pam(simdatxy, 4)
sil = silhouette(pam4, 4)
plot(sil, col=c("red","green","blue","purple"), main="Silhouette")__</code></pre>
<p>Change the number of clusters \(k\) and assess which \(k\) gives the best silhouette index.</p>
<p>Now, repeat this for groups that have uniform (unclustered) data distributions over a whole range of values.</p>
<p>__</p>
<p>Exercise 5.2</p>
<p>Make a “character” representation of the distance between the 20 locations in the <code>dune</code> data from the <strong><a href="https://cran.r-project.org/web/packages/vegan/">vegan</a></strong> package using the function <code>symnum</code>.</p>
<p>Make a heatmap plot of these distances.</p>
<p>__</p>
<p>Exercise 5.3</p>
<p>Load the <code>spirals</code> data from the <strong><a href="https://cran.r-project.org/web/packages/kernlab/">kernlab</a></strong> package. Plot the results of using \(k\)-means on the data. This should give you something similar to Figure 5.35.</p>
<p><a href="05-chap_files/figure- html/fig-kmeanspital1-1.png" title="Figure 5.35 (a):"><img src="05-chap_files/figure-html/fig-kmeanspital1-1.png" class="img-fluid"></a></p>
<ol type="a">
<li></li>
</ol>
<p><a href="05-chap_files/figure- html/fig-kmeanspital1-2.png" title="Figure 5.35 (b):"><img src="05-chap_files/figure-html/fig-kmeanspital1-2.png" class="img-fluid"></a></p>
<ol start="2" type="a">
<li></li>
</ol>
<p>Figure 5.35: An example of non-convex clusters. In (a), we show the result of \(k\)-means clustering with \(k=2\). In (b), we have the output from <code>dbscan</code>. The colors represent the three clusters found by the algorithm for the settings .</p>
<p>You’ll notice that the clustering in Figure 5.35 seems unsatisfactory. Show how a different method, such as <code>specc</code> or <code>dbscan</code>, could cluster <code>spirals</code> data in a more useful manner.</p>
<p>Repeat the <code>dbscan</code> clustering with different parameters. How robust is the number of groups?</p>
<p>__</p>
<p>Exercise 5.4</p>
<p>Looking at graphical representations in simple two-dimensional maps can often reveal important clumping patterns. We saw an example for this with the map that enabled Snow to discover the source of the London cholera outbreak. Such clusterings can often indicate important information about hidden variables acting on the observations. Look at a map for breast cancer incidence in the US at:<br>
<a href="http://www.huffingtonpost.com/bill-davenhall/post_1663_b_817254.html" class="uri">http://www.huffingtonpost.com/bill-davenhall/post_1663_b_817254.html</a> (<a href="16-chap.html#ref-mandal2009">Mandal et al.&nbsp;2009</a>); the areas of high incidence seem spatially clustered. Can you guess the reason(s) for this clustering and high incidence rates on the West and East coasts and around Chicago?</p>
<p>__</p>
<p>Exercise 5.5</p>
<p><strong>Amplicon bioinformatics: from raw reads to dereplicated sequences</strong>. As a supplementary exercise, we provide the intermediate steps necessary to a full data preprocessing workflow for denoising 16S rRNA sequences. We start by setting the directories and loading the downloaded data:</p>
<pre><code>base_dir = "../data"
miseq_path = file.path(base_dir, "MiSeq_SOP")
filt_path = file.path(miseq_path, "filtered")
fnFs = sort(list.files(miseq_path, pattern="_R1_001.fastq"))
fnRs = sort(list.files(miseq_path, pattern="_R2_001.fastq"))
sampleNames = sapply(strsplit(fnFs, "_"), `[`, 1)
if (!file_test("-d", filt_path)) dir.create(filt_path)
filtFs = file.path(filt_path, paste0(sampleNames, "_F_filt.fastq.gz"))
filtRs = file.path(filt_path, paste0(sampleNames, "_R_filt.fastq.gz"))
fnFs = file.path(miseq_path, fnFs)
fnRs = file.path(miseq_path, fnRs)
print(length(fnFs))__


[1] 20</code></pre>
<p>The data are highly-overlapping Illumina Miseq \(2\) amplicon sequences from the V4 region of the 16S rRNA gene (<a href="16-chap.html#ref-Kozich2013">Kozich et al. 2013</a>). There were originally 360 fecal samples collected longitudinally from 12 mice over the first year of life. These were collected by P. D. Schloss et al.&nbsp;(<a href="16-chap.html#ref- schloss2012stabilization">2012</a>) to investigate the development and stabilization of the murine microbiome. We have selected 20 samples to illustrate how to preprocess the data.</p>
<p>We will need to filter out low-quality reads and trim them to a consistent length. While generally recommended filtering and trimming parameters serve as a starting point, no two datasets are identical and therefore it is always worth inspecting the quality of the data before proceeding. We show the sequence quality plots for the two first samples in Figure 5.36. They are generated by:</p>
<pre><code>plotQualityProfile(fnFs[1:2]) + ggtitle("Forward")
plotQualityProfile(fnRs[1:2]) + ggtitle("Reverse")__</code></pre>
<p><a href="05-chap_files/figure- html/fig-profile-1-1.png" title="Figure 5.36 (a):"><img src="05-chap_files/figure-html/fig-profile-1-1.png" class="img-fluid"></a></p>
<ol type="a">
<li></li>
</ol>
<p><a href="05-chap_files/figure- html/fig-profile-1-2.png" title="Figure 5.36 (b):"><img src="05-chap_files/figure-html/fig-profile-1-2.png" class="img-fluid"></a></p>
<ol start="2" type="a">
<li></li>
</ol>
<p>Figure 5.36: Quality scores. The lines show positional summary statistics: green is the mean, orange is the median, and the dashed orange lines are the 25th and 75th quantiles.</p>
<p>Note that we also see the background distribution of quality scores at each position in Figure 5.36 as a grey-scale heat map. The dark colors correspond to higher frequency.</p>
<p>__</p>
<p>Exercise 5.6</p>
<p>Generate similar plots for four randomly selected sets of forward and reverse reads. Compare forward and reverse read qualities; what do you notice?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>ii = sample(length(fnFs), 4)
plotQualityProfile(fnFs[ii]) + ggtitle("Forward")__


 plotQualityProfile(fnRs[ii]) + ggtitle("Reverse")__</code></pre>
<p>__</p>
<p>Exercise 5.7</p>
<p>Here, the forward reads maintain high quality throughout, while the quality of the reverse reads drops significantly at about position 160. Therefore, we truncate the forward reads at position 240, and trimm the first 10 nucleotides as these positions are of lower quality. The reverse reads are trimmed at position 160. Combine these trimming parameters with standard filtering parameters remember to enforce a maximum of 2 expected errors per-read. (Hint: Trim and filter on paired reads jointly, i.e., both reads must pass the filter for the pair to pass. The input arguments should be chosen following the <strong><a href="https://bioconductor.org/packages/dada2/">dada2</a></strong> vignette carefully. We recommend filtering out all reads with any ambiguous nucleotides.)</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Most Illumina sequencing data show a trend of decreasing quality towards the end of the reads.</p>
<pre><code>out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),
        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,  trimLeft=10,
        compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
head(out)__


                              reads.in reads.out
F3D0_S188_L001_R1_001.fastq       7793      7139
F3D1_S189_L001_R1_001.fastq       5869      5314
F3D141_S207_L001_R1_001.fastq     5958      5478
F3D142_S208_L001_R1_001.fastq     3183      2926
F3D143_S209_L001_R1_001.fastq     3178      2955
F3D144_S210_L001_R1_001.fastq     4827      4323</code></pre>
<p>The <code>maxN</code> parameter omits all reads with more than <code>maxN = 0</code> ambiguous nucleotides and <code>maxEE</code> at 2 excludes reads with more than 2 expected errors.</p>
<p>The sequence data was imported into R from demultiplexed <em>fastq</em> files (i.e. one <em>fastq</em> for each sample) and simultaneously dereplicated to remove redundancy. Name the resulting objects by their sample provenance; they will have <em>derep</em> as their class.</p>
<pre><code>derepFs = derepFastq(filtFs, verbose = FALSE)
derepRs = derepFastq(filtRs, verbose = FALSE)
names(derepFs) = sampleNames
names(derepRs) = sampleNames __</code></pre>
<p>__</p>
<p>Exercise 5.8</p>
<p>Use R to create a map like the one shown in Figure 5.2. Hint: go to the <a href="http://bombsight.org">website of the British National Archives</a> and download street addresses of hits, use an address resolution service to convert these into geographic coordinates, and display these as points on a map of London.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>See the Gist <a href="https://gist.github.com/wolfganghuber/523622aff6a156d26e77a87ccbe7bd0d" class="uri">https://gist.github.com/wolfganghuber/523622aff6a156d26e77a87ccbe7bd0d</a> by Andrzej Oles.</p>
<p>Aure, Miriam Ragle, Valeria Vitelli, Sandra Jernström, Surendra Kumar, Marit Krohn, Eldri U Due, Tonje Husby Haukaas, et al.&nbsp;2017. “Integrative Clustering Reveals a Novel Split in the Luminal A Subtype of Breast Cancer with Impact on Outcome.” <em>Breast Cancer Research</em> 19 (1): 44.</p>
<p>Bendall, Sean C, Garry P Nolan, Mario Roederer, and Pratip K Chattopadhyay. 2012. “A Deep Profiler’s Guide to Cytometry.” <em>Trends in Immunology</em> 33 (7): 323–32.</p>
<p>Callahan, Ben J, Kris Sankaran, Julia A Fukuyama, Paul J McMurdie, and Susan P Holmes. 2016. “Bioconductor Workflow for Microbiome Data Analysis: From Raw Reads to Community Analyses.” <em>F1000Research</em> 5.</p>
<p>Callahan, Benjamin J, Paul J McMurdie, and Susan P Holmes. 2017. “Exact Sequence Variants Should Replace Operational Taxonomic Units in Marker Gene Data Analysis.” <em>ISME Journal</em> , 1–5.</p>
<p>Callahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy J Johnson, and Susan P Holmes. 2016. “DADA2: High Resolution Sample Inference from Amplicon Data.” <em>Nature Methods</em> , 1–4.</p>
<p>Caporaso, J. G., J. Kuczynski, J. Stombaugh, K. Bittinger, F. D. Bushman, E. K. Costello, N. Fierer, et al.&nbsp;2010. “QIIME Allows Analysis of High-Throughput Community Sequencing Data.” <em>Nature Methods</em> 7 (5): 335–36.</p>
<p>Chakerian, John, and Susan Holmes. 2012. “Computational Tools for Evaluating Phylogenetic and Hierarchical Clustering Trees.” <em>Journal of Computational and Graphical Statistics</em> 21 (3): 581–99.</p>
<p>Diday, Edwin, and M Paula Brito. 1989. “Symbolic Cluster Analysis.” In <em>Conceptual and Numerical Analysis of Data</em> , 45–84. Springer.</p>
<p>Dundar, Murat, Ferit Akova, Halid Z. Yerebakan, and Bartek Rajwa. 2014. “A Non-Parametric Bayesian Model for Joint Cell Clustering and Cluster Matching: Identification of Anomalous Sample Phenotypes with Random Effects.” <em>BMC Bioinformatics</em> 15 (1): 1–15. <a href="https://doi.org/10.1186/1471-2105-15-314" class="uri">https://doi.org/10.1186/1471-2105-15-314</a>.</p>
<p>Freedman, David A. 1991. “Statistical Models and Shoe Leather.” <em>Sociological Methodology</em> 21 (2): 291–313.</p>
<p>Hallett, Robin M, Anna Dvorkin-Gheva, Anita Bane, and John A Hassell. 2012. “A Gene Signature for Predicting Outcome in Patients with Basal-Like Breast Cancer.” <em>Scientific Reports</em> 2.</p>
<p>Holmes, Susan, Michael He, Tong Xu, and Peter P Lee. 2005. “Memory t Cells Have Gene Expression Patterns Intermediate Between Naive and Effector.” <em>PNAS</em> 102 (15): 5519–23.</p>
<p>Hornik, Kurt. 2005. “A CLUE for CLUster Ensembles.” <em>Journal of Statistical Software</em> 14 (12).</p>
<p>Hulett, Henry R, William A Bonner, Janet Barrett, and Leonard A Herzenberg. 1969. “Cell Sorting: Automated Separation of Mammalian Cells as a Function of Intracellular Fluorescence.” <em>Science</em> 166 (3906): 747–49.</p>
<p>Kahneman, Daniel. 2011. <em>Thinking, Fast and Slow</em>. Macmillan.</p>
<p>Kaufman, Leonard, and Peter J Rousseeuw. 2009. <em>Finding Groups in Data: An Introduction to Cluster Analysis</em>. Vol. 344. John Wiley &amp; Sons.</p>
<p>Kozich, James J, Sarah L Westcott, Nielson T Baxter, Sarah K Highlander, and Patrick D Schloss. 2013. “Development of a Dual-Index Sequencing Strategy and Curation Pipeline for Analyzing Amplicon Sequence Data on the MiSeq Illumina Sequencing Platform.” <em>Applied and Environmental Microbiology</em> 79 (17): 5112–20.</p>
<p>Mandal, Rakesh, Sophie St-Hilaire, John G Kie, and DeWayne Derryberry. 2009. “Spatial Trends of Breast and Prostate Cancers in the United States Between 2000 and 2005.” <em>International Journal of Health Geographics</em> 8 (1): 53.</p>
<p>McLachlan, Geoffrey, and Thriyambakam Krishnan. 2007. <em>The EM Algorithm and Extensions</em>. Vol. 382. John Wiley &amp; Sons.</p>
<p>Müllner, Daniel. 2013. “Fastcluster: Fast Hierarchical, Agglomerative Clustering Routines for r and Python.” <em>Journal of Statistical Software</em> 53 (9): 1–18.</p>
<p>O’Neill, Kieran, Nima Aghaeepour, Josef Špidlen, and Ryan Brinkman. 2013. “Flow Cytometry Bioinformatics.” <em>PLoS Computational Biology</em> 9 (12): e1003365.</p>
<p>Ohnishi, Y., W. Huber, A. Tsumura, M. Kang, P. Xenopoulos, K. Kurimoto, A. K. Oles, et al.&nbsp;2014. “Cell-to-Cell Expression Variability Followed by Signal Reinforcement Progressively Segregates Early Mouse Lineages.” <em>Nature Cell Biology</em> 16 (1): 27–37.</p>
<p>Rosen, Michael J, Benjamin J Callahan, Daniel S Fisher, and Susan P Holmes. 2012. “Denoising PCR-Amplified Metagenome Data.” <em>BMC Bioinformatics</em> 13 (1): 283.</p>
<p>Schloss, P D, S L Westcott, T Ryabin, J R Hall, M Hartmann, E B Hollister, R A Lesniewski, et al.&nbsp;2009. “Introducing mothur: Open-Source, Platform- Independent, Community-Supported Software for Describing and Comparing Microbial Communities.” <em>Applied and Environmental Microbiology</em> 75 (23): 7537–41.</p>
<p>Schloss, P. D., A. M. Schuber, J. P. Zackular, K. D. Iverson, Young V. B., and Petrosino J. F. 2012. “Stabilization of the Murine Gut Microbiome Following Weaning.” <em>Gut Microbes</em> 3 (4): 383–93.</p>
<p>Tibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. “Estimating the Number of Clusters in a Data Set via the Gap Statistic.” <em>JRSSB</em> 63 (2): 411–23.</p>
<p>Tseng, George C, and Wing H Wong. 2005. “Tight Clustering: A Resampling-Based Approach for Identifying Stable and Tight Patterns in Data.” <em>Biometrics</em> 61 (1): 10–16.</p>
<p>Tversky, Amos, and Daniel Kahneman. 1974. “Heuristics and Biases: Judgement Under Uncertainty.” <em>Science</em> 185: 1124–30.</p>
<p>———. 1975. “Judgment Under Uncertainty: Heuristics and Biases.” In <em>Utility, Probability, and Human Decision Making</em> , 141–62. Springer.</p>
<p>Page built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./04-chap.html" class="pagination-link" aria-label="4.1 이 장의 목표">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">4.1 이 장의 목표</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./06-chap.html" class="pagination-link" aria-label="6.1 이 장의 목표">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">6.1 이 장의 목표</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>