[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modern Statistics for Modern Biology",
    "section": "",
    "text": "1 홈\n이 책의 챕터 간 이동은 상단의 드롭다운 메뉴(“Chapters”)를 이용해 주세요.\n책 인용 방법: Modern Statistics for Modern Biology, by Susan Holmes and Wolfgang Huber, Cambridge University Press (2019). ISBN: 9781108705295.\n라이센스: CC BY-NC-SA\n소개\n이산 데이터를 위한 생성 모델\n통계적 모델링\n데이터 시각화\n혼합 모델\n클러스터링\n가설 검정\n다변량 분석\n고처리량 카운트 데이터 & 일반화 선형 모델\n이질적 데이터를 위한 다변량 방법\n네트워크와 트리\n이미지 데이터\n지도 학습\n고처리량 실험 설계 및 분석\n통계적 일치성\n감사의 말\n참고 문헌",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>홈</span>"
    ]
  },
  {
    "objectID": "index.html#독자들에게",
    "href": "index.html#독자들에게",
    "title": "Modern Statistics for Modern Biology",
    "section": "1.1 독자들에게",
    "text": "1.1 독자들에게\n이 책의 인쇄본(아래 참조)은 2019년에 출판되었지만, 온라인 버전은 계속 업데이트하고 있습니다. 오류, 실수, 오타를 발견하시면 알려주세요. 최선을 다해 수정하겠습니다.\n코드 예제와 관련하여: R과 CRAN 및 Bioconductor의 패키지들은 매우 역동적인 환경입니다. 인쇄본은 2018년 R 3.5와 Bioconductor 3.7을 기준으로 마무리되었습니다. 우리는 R이나 패키지의 변경 사항에 맞춰 코드를 지속적으로 업데이트하고 있습니다. 여기에서 보시는 내용은 2025-06-13 기준 R 버전 4.5.1과 2025-09-01 기준 최신 패키지 버전을 사용하여 빌드되었습니다. 자동화된 탐지로 발견하기 어려운 의도치 않은 변경 사항(예: 플롯 출력이나 특정 계산 결과)을 간과했을 수 있습니다. 우리는 이러한 위험이 독자들이 최신 컴퓨팅 환경에서 작업할 수 있도록 하는 가치 있는 대가라고 생각하지만, 혼란을 드릴 수 있는 점에 대해 사과드립니다.\n발견하신 점이 있다면 wolfgang.huber [at] embl.org 로 이메일을 보내주세요.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>홈</span>"
    ]
  },
  {
    "objectID": "index.html#코드와-데이터",
    "href": "index.html#코드와-데이터",
    "title": "Modern Statistics for Modern Biology",
    "section": "1.2 코드와 데이터",
    "text": "1.2 코드와 데이터\n\n1.2.1 패키지 설치\n이 책의 모든 코드 예제를 실행하는 데 필요한 모든 패키지는 다음 명령어로 설치할 수 있습니다:\nsource(\"https://www.huber.embl.de/msmb/install_packages.R\")\n\n\n1.2.2 데이터\n\n데이터 파일 (zip 폴더)\nlast modified: 2022-12-15 16:05:03\nmd5 hash: 4aefffbcd826d9645b9e0e5b12274f07 -\n\n\n\n1.2.3 코드\n\nR 코드 (zip 폴더)\n\n\n\n1.2.4 전체 책 전자 사본 다운로드\n\n책 HTML 트리 (zip)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>홈</span>"
    ]
  },
  {
    "objectID": "index.html#실물-책",
    "href": "index.html#실물-책",
    "title": "Modern Statistics for Modern Biology",
    "section": "1.3 실물 책",
    "text": "1.3 실물 책\nModern Statistics for Modern Biology의 하드카피를 원하신다면 Cambridge University Press 에서 구매하실 수 있습니다.\n\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>홈</span>"
    ]
  },
  {
    "objectID": "00-chap.html",
    "href": "00-chap.html",
    "title": "2  서문 (Preface)",
    "section": "",
    "text": "2.1 이 책에는 무엇이 들어있나요?\n이 책 제목에 있는 _현대_라는 두 가지 사례는 생물학적 데이터 분석에 있어서 최근의 두 가지 주요 혁명을 반영합니다.\n이 책의 목적은 생물학 연구에 종사하는 과학자들이 실험과 기타 이용 가능한 데이터를 최대한 활용하는 데 필요한 많은 중요한 아이디어와 방법을 빠르게 배울 수 있도록 하는 것입니다. 이 책은 실무적인 접근 방식을 취합니다. 내러티브는 질문 클래스나 특정 데이터 유형에 따라 결정됩니다. 방법과 이론은 알아야 할 필요에 따라 소개됩니다. 우리는 첫 번째 원리로부터 체계적으로 추론하려고 노력하지 않습니다. 이 책은 종종 독자들을 수영장에 빠뜨리고 너무 많은 세부 사항이 누락되었음에도 불구하고 수영할 수 있기를 바랍니다.\n이 책은 확률, 선형 대수학, 미적분학, 컴퓨터 과학, 데이터베이스, 다변량 통계 등 기본 이론에 대한 체계적인 교육을 결코 대체하지 않습니다. 이러한 훈련에는 여러 학기의 교과 과정이 필요합니다. 아마도 이 책은 이러한 분야 중 하나에 더 깊이 관여하고 싶은 욕구를 자극할 것입니다.\n모든 생물학적 시스템이나 유기체는 수만 개의 구성 요소로 구성되며, 이는 서로 다른 상태에 있을 수 있고 다양한 방식으로 상호 작용할 수 있습니다. 현대 생물학은 다중 공변량 및 상호작용을 통해 시간적, 공간적 맥락에서 포괄적인(이는 고차원적) 데이터를 획득함으로써 그러한 시스템을 이해하는 것을 목표로 합니다. 이러한 복잡성을 처리하는 것이 우리의 주요 과제가 될 것입니다. 여기에는 항상 불완전한 도구를 사용하여 얻을 수 있는 데이터의 실제적 복잡성과 이질성뿐만 아니라 실제 생물학적 복잡성도 포함됩니다.\n생물학적 데이터는 핵산 및 단백질 서열, 직사각형 개수 표, 다중 표, 연속 변수, 배치 요인, 표현형 이미지, 공간 좌표 등 모든 종류의 형태로 제공됩니다. 실험실 실험에서 측정된 데이터 외에도 임상 데이터, 환경 관찰, 공간 및 시간 측정, 네트워크 및 계보 트리, 자유 텍스트 또는 제어된 어휘로 생물학적 데이터베이스에 이전에 축적된 지식의 더미가 있습니다. (…)\n이 책의 컴퓨팅 플랫폼으로 R과 Bioconductor를 선택한 동기는 이러한 이질성 때문입니다. 이에 대한 자세한 내용은 아래에서 설명합니다.\n그림 1: R.A.가 권장하는 가설 테스트 패러다임 Fisher는 데이터를 수집하기 전에 귀무 가설의 공식화와 실험 설계부터 시작합니다. 모델 피팅에 대해서도 비슷한 개략적인 방식으로 생각할 수 있습니다. _가설 H0_을 _파라메트릭 모델_과 _매개변수 맞춤_으로 바꾸면 됩니다.\n그림 1은 통계 데이터 분석의 순차적 보기를 간략하게 설명합니다. 1930년대 Fisher(1935)와 Neyman 및 Pearson(1936)의 유의성과 가설 테스트에 대한 획기적인 작업에 영감을 받아 수학적 형식주의에 잘 적응하며, 특히 가설(null 또는 대안) 또는 분포 가정을 설정해야 하며 분석적 근사치를 검색할 수 있습니다.\n그림 2: J.W. Tukey는 데이터로 분석을 시작할 것을 권장하며 다음과 같이 썼습니다. “어떤 기술 장비도 예상할 수 있는 것과 관계없이 볼 수 있는 것을 원하지 않을 것입니다.” (홈즈 - Junca 1985).\n실제 과학적 발견은 그림 1의 캐리커처 방식으로는 거의 작동하지 않습니다. Tukey(1977)는 두 가지 별도의 접근 방식을 강조했습니다. 그는 첫 번째를 탐색적 데이터 분석(EDA)이라고 불렀습니다. EDA는 데이터 자체를 사용하여 통계 분석 수행 방법을 결정합니다. EDA는 데이터 시각화를 기반으로 구축되었으며 확증 데이터 분석(CDA)으로 보완됩니다. 즉, 이상적으로는 강력하고 복잡한 가정에 의존하지 않는 가설 중심 추론 방법입니다. Tukey는 다양한 해상도와 관점에서 데이터를 볼 수 있도록 그림 2에 도식화된 반복적 접근 방식을 권장했습니다. 이를 통해 데이터와 근본적인 자연 현상에 대한 이해를 연속적으로 개선할 수 있습니다.\n1990년대 후반 생물학은 큰 (p), 작은 (n) 문제를 제기했습니다. (p=20000) 유전자에 대한 (n=200) 환자 유래 조직 샘플에 대한 유전자 발현 데이터세트를 고려하세요. (20000) 유전자 또는 특징으로부터 임상 변수(예: 질병 유형 또는 결과)를 “예측” 하는 회귀 또는 분류 모델을 구축하려는 경우 잠재적인 모델 매개변수 수가 측정 수보다 훨씬 더 클 수 있기 때문에 즉시 문제에 직면하게 됩니다. 문제는 매개변수의 비식별성 또는 _과적합_으로 인해 발생합니다. 적어도 이는 일반적인 다변량 선형 모델과 같은 공통 모델의 경우입니다. 통계학자들은 정규화 기술(Hastie, Tibshirani, and Friedman 2008)을 사용하여 희소성을 요구함으로써, 즉 많은 잠재적인 매개변수가 0이거나 적어도 그에 가깝도록 요구함으로써 상황을 해결할 수 있다는 것을 깨달았습니다.\n희소성 원리의 일반화는 경험적 베이즈라는 이름으로 진행되는 고차원 통계에서 가장 강력한 최신 아이디어 중 하나를 호출하여 달성됩니다. 즉, 모든 매개변수를 처음부터 학습하려고 시도하지 않고 매개변수 그룹이 유사하거나 심지어 동일할 것이라는 사실을 사용합니다. 현대 추정 및 가설 테스트에 필수적인 대규모 추론 주제에 대한 몇 가지 중요한 책 (Efron 2010)이 있습니다. 2010년대에 컴퓨터 과학자들은 매개변수 식별 가능성이나 고유성에 대해 너무 걱정하지 않고 때로는 놀라운 예측 품질을 제공할 수 있는 “심층 신경망”을 엔지니어링하는 방법도 발견했습니다.\n몬테카를로 근사법을 사용하는 시기를 식별하기 위해 룰렛 아이콘을 사용하겠습니다. 이 방법의 이름은 카지노 게임의 무작위성과 유사하게 무작위성을 사용하기 때문에 사용됩니다. 아이러니하게도 많은 카지노 게임의 경우 승리 확률은 분석적으로 알려지지 않았으며 카지노에서는 자체 경험적 데이터를 사용하여 승리 확률을 평가합니다.\n_시뮬레이션_은 우리가 필요로 하는 많은 결과가 표준 분석 접근 방식의 범위를 벗어나기 때문에 이 책에서 필수적인 역할을 합니다. 즉, 시뮬레이션은 “종이와 연필 수학”으로 다룰 수 있는 방법만 고려할 수 있는 방법과 가정이나 근사를 단순화하는 것이 적절할지 걱정하는 것에서 우리를 해방시킵니다.\n이 책에서 우리는 이러한 개발의 광범위한 스펙트럼과 현재 생물학 연구에 대한 적용을 다루려고 노력합니다. 우리는 RNA-seq, 유세포 분석, 분류군 관련도, 소속 데이터 및 단일 세포 측정을 포함하여 현대 생물학자가 처리해야 하는 다양한 유형의 데이터를 다룹니다. 우리는 통계에 대한 사전 공식 교육이 없다고 가정합니다. 그러나 R에 대한 어느 정도의 친숙함과 수학적, 분석적 사고에 참여하려는 의지가 필요합니다.\n악마 아이콘 아래에 메모와 추가 정보를 넣을 것입니다. 이는 세부 사항을 관리하는 악마입니다.\n다음에서 설명하는 것처럼 이 책의 각 장은 서로 연결되어 있지만 합리적으로 자체 포함되어 있으므로 별도로 공부할 수도 있습니다. 각 장은 동기와 목표 섹션으로 시작됩니다. 본문에 있는 질문은 귀하가 따라하고 있는지 확인하는 데 도움이 됩니다. 이 텍스트에는 전체 R 코드 예제가 포함되어 있습니다. HTML에서 R 코드를 스크랩하거나 책에서 수동으로 복사할 필요가 없습니다. 이 웹사이트에 있는 R 파일(확장자 .R)을 사용하세요. 각 장은 주요 요점 요약과 일련의 연습으로 마무리됩니다.\n_생성 모델_은 우리의 기본 구성 요소입니다. 복잡한 데이터에 대한 결론을 도출하려면 이런 저런 상황에서 생성된 데이터에 대한 간단한 모델을 갖는 것이 유용한 경향이 있습니다. 우리는 1 이산 데이터에 대한 생성 모델에서 소개하는 확률 이론과 생성 모델을 사용하여 이를 수행합니다. 우리는 면역학 및 DNA 분석의 예를 사용하여 생물학적 데이터(이항, 다항 및 포아송 확률 변수)에 대한 유용한 생성 모델을 설명합니다.\n특정 모델에서 데이터가 어떻게 보이는지 알게 되면 거꾸로 작업을 시작할 수 있습니다. 특정 데이터가 주어지면 어떤 모델이 이를 가장 잘 설명할 수 있을까요? 이러한 _상향식 접근 방식_은 통계적 사고의 핵심이며, 2 통계 모델링에서 설명합니다.\nTukey의 계획(그림 2)에서 _graphics_의 주요 역할을 살펴보았으므로 3 데이터 시각화에서 데이터를 시각화하는 방법을 알아보겠습니다. 그래픽 문법과 ggplot2를 사용하겠습니다.\n실제 생물학적 데이터는 1 이산 데이터에 대한 생성 모델에서 다룰 수 있는 것보다 더 복잡한 분포 특성을 갖는 경우가 많습니다. 우리는 4개의 혼합물 모델에서 탐색하는 _mixtures_를 사용할 것입니다. 이를 통해 우리는 이질적인 생물학적 데이터에 대한 현실적인 모델을 구축하고 적절한 분산 안정화 변환을 선택하기 위한 견고한 기반을 제공할 수 있습니다.\n생물학의 대규모 매트릭스와 같은 데이터 세트는 자연스럽게 _클러스터링_에 적합합니다. 일단 매트릭스 행(특징) 사이의 거리 측정값을 정의하면 발현 패턴의 유사성에 따라 유전자를 클러스터링하고 그룹화할 수 있으며 마찬가지로 열(환자 샘플)에 대해서도 마찬가지입니다. 클러스터링에 대해서는 5 클러스터링에서 다루겠습니다. 클러스터링은 거리에만 의존하기 때문에 행렬 형태가 아닌 데이터에도 객체와 객체 사이에 정의된 거리가 있는 한 이를 적용할 수 있습니다.\nEDA의 길을 따라 7 다변량 분석에서 단순 행렬에 대한 가장 기본적인 비지도 분석 방법인 _주성분 분석_을 다룹니다. 9 이종 데이터에 대한 다변량 방법에서는 여러 데이터 유형을 결합하는 보다 이질적인 데이터를 살펴보겠습니다. 여기서는 단일 셀 데이터의 개수를 계산하는 비선형 비지도 방법을 살펴보겠습니다. 또한 7 다변량 분석에서 다룬 다변량 접근 방식의 일반화를 범주형 변수의 조합과 동일한 관찰 단위에 기록된 여러 분석에 사용하는 방법도 다룰 것입니다.\n그림 1에 설명된 기본 가설 테스트 워크플로는 6 테스트에 설명되어 있습니다. 우리는 이를 (np)-데이터세트에 대한 가장 일반적인 쿼리 중 하나에 적용할 수 있는 기회를 이용합니다. 즉, 어떤 유전자(특징)가 샘플의 특정 속성(예: 질병 유형 또는 결과)과 _관련_되어 있습니까? 그러나 기존의 유의성 임계값은 많은 거짓 연관성으로 이어질 수 있습니다. ()의 거짓 긍정 비율을 사용하면 (p=20000) 기능 중 어느 것도 진정한 연관성을 갖지 않으면 (p) 거짓 긍정이 예상됩니다. 따라서 우리는 또한 여러 테스트를 처리해야 합니다.\n통계에서 가장 유용한 아이디어 중 하나는 분산 분해, 즉 분산 분석(ANOVA)입니다. 8개의 높은 처리량 데이터 및 일반화 선형 모델에서 선형 모델과 일반화 선형 모델의 프레임워크에서 이에 대해 살펴보겠습니다. 우리는 RNA-Seq 실험에서 예시 데이터를 그릴 것이기 때문에, 이는 우리에게 그러한 개수 데이터에 대한 모델과 _강건성_의 개념을 논의할 기회도 제공합니다.\n생물학의 어떤 것도 진화의 관점을 제외하고는 의미가 없으며, 진화 관계는 계통발생수에 유용하게 코드화되어 있습니다. 10 가지 네트워크 및 트리에서 네트워크와 트리를 살펴보겠습니다.\n1 테오도시우스 도브잔스키, https://en.wikipedia.org/wiki/Nothing_in_Biology_Makes_Sense_Except_in_the_Light_of_Evolution\n생물학에서 풍부한 데이터 소스는 이미지이며, 11 이미지 데이터에서는 이미지 및 공간 통계에서 특징 추출을 탐색하여 모든 종류의 이질적인 데이터 유형에 대해 EDA를 수행하려는 의지를 강화합니다.\n12 지도 학습에서는 지도 학습을 살펴봅니다. 즉, 각 객체에 대한 다변량 특징 세트가 주어지면 객체의 다양한 클래스를 구별하는 알고리즘을 훈련합니다. 저차원 특징 벡터와 선형 방법으로 간단하게 시작한 다음 고차원 설정에서 분류 문제 중 일부를 살펴보겠습니다. 여기서는 (적어도 개념적으로) 정답 분류의 훈련 세트를 알고리즘에서 동시에 동시에 학습할 수 있는 ‘고전적인’ 지도 학습에 중점을 둡니다. 우리는 (아직…) 하나 이상의 에이전트가 연속적으로 작업을 수행하고 이에 대한 피드백을 받아 소위 환경과 상호 작용하여 학습하는 보다 유연한 프레임워크인 강화 학습을 다루지 않습니다. 따라서 해당 프레임워크에는 time 및 state 개념이 있습니다.\n실험 설계 및 데이터 분석의 모범 사례에 대한 고려 사항을 포함하여 13 고처리량 실험 설계 및 분석을 마무리합니다. 이를 위해 우리는 이전 장에서 배운 내용을 사용하고 반영할 것입니다.\n그림 3: 데이터 분석은 1단계 프로세스가 아닙니다. 각 단계에는 데이터의 복잡성 중 일부를 시각화하고 분해하는 작업이 포함됩니다. Tukey의 반복적 데이터 구조는 (Total=V_1+V_2+V_3)으로 개념화될 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>서문 (Preface)</span>"
    ]
  },
  {
    "objectID": "00-chap.html#이-책에는-무엇이-들어있나요",
    "href": "00-chap.html#이-책에는-무엇이-들어있나요",
    "title": "2  서문 (Preface)",
    "section": "",
    "text": "악마 아이콘 아래에 메모와 추가 정보를 넣을 것입니다. 이것은 세부 사항을 돌보는 악마입니다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>서문 (Preface)</span>"
    ]
  },
  {
    "objectID": "00-chap.html#현대-생물학자를-위한-계산-도구",
    "href": "00-chap.html#현대-생물학자를-위한-계산-도구",
    "title": "2  서문 (Preface)",
    "section": "2.2 현대 생물학자를 위한 계산 도구",
    "text": "2.2 현대 생물학자를 위한 계산 도구\n계속해서 살펴보겠지만, 분석 접근 방식, 도구 및 선택 사항은 다양합니다. 우리 작업은 재현 가능한 스크립트 형식으로 주의 깊게 기록을 유지해야만 검증될 수 있습니다. R과 Bioconductor는 이러한 플랫폼을 제공합니다.\n우리는 다양한 유형의 데이터, 질문 및 통계 방법을 직접 다루고 있지만 모든 계산을 한 지붕 아래에 유지하여 일관된 계산 접근 방식을 유지합니다. 즉, Bioconductor 프로젝트의 생물학적 데이터 인프라 및 특수 방법 패키지로 강화된 R 프로그래밍 언어 및 통계 환경입니다. 독자는 책을 사용하기 전에 R에 어느 정도 익숙해지는 것부터 시작해야 합니다. 좋은 책과 온라인 자료가 많이 있습니다. 그 중 하나는 Grolemund와 Wickham(2017)이 온라인으로 http://r4ds.had.co.nz에서 작성한 것입니다.\nR 코드는 이 책의 주요 구성 요소입니다. 이 것이 우리가 텍스트 설명을 명시적으로 만드는 방법입니다. 본질적으로 책에 나오는 모든 데이터 시각화는 표시된 코드로 생성되며, 독자는 이러한 모든 수치와 표시된 다른 결과를 복제할 수 있어야 합니다.\n모든 코드가 HTML 페이지에 표시되는 것은 아닙니다. 특히 코드가 길고 교육 목표에 직접적으로 도움이 되지 않는 경우에는 더욱 그렇습니다. 그러나 모든 코드는 이 웹페이지에서 제공하는 .R 파일에서 사용할 수 있습니다.\nR에 어느 정도 익숙하더라도 책의 모든 코드 줄을 즉시 이해하지 못하더라도 걱정하지 마세요. 코드를 명시적으로 유지하고 잠재적으로 어려운 부분에 대한 팁과 힌트를 제공하려고 노력했지만\n\n이전에 본 적이 없고 신비한 일을 하는 함수가 호출되었습니다.\n이해하지 못하는 복잡한 R 표현식이 있습니다(아마도 dplyr 패키지의 apply 함수 또는 데이터 조작과 관련됨).\n\n당황하지 말 것. 신비한 기능에 대해서는 매뉴얼 페이지를 살펴보십시오. RStudio를 열고 개체 탐색기를 사용하여 표현식에 들어가는 변수와 나오는 변수를 살펴보세요. 표현식을 분할하여 중간 값을 확인합니다.\n1 이산 데이터를 위한 생성 모델 및 2 통계 모델링 장에서는 가벼운 플롯팅 및 데이터 조작을 위해 base R 기능을 사용합니다. 점점 더 정교한 연산이 필요해짐에 따라 3 데이터 시각화에서 그래픽을 만드는 ggplot2 방식을 소개합니다. 제한된 명령 세트만 사용하여 정교한 플롯을 생성할 수 있게 해주는 그래픽 개념의 강력한 문법 외에도 이는 dplyr 방식의 데이터 조작을 사용함을 의미합니다.\n\n2.2.0.1 왜 R과 바이오컨덕터인가?\n우리가 R(Ihaka and Gentleman 1996) 및 Bioconductor(Huber et al. 2015) 플랫폼에 대한 모든 분석을 제시하기로 선택한 데에는 여러 가지 이유가 있습니다.\n책에 나오는 코드를 따라하려면 R 및 Rstudio를 다운로드하세요.\n최첨단 솔루션. (10,000)개 이상의 패키지를 사용할 수 있으므로 최신 개발을 포함하여 거의 모든 통계 방법을 사용할 수 있습니다. 또한 컴퓨터 과학, 수학, 기계 학습, 데이터 관리, 시각화 및 인터넷 기술의 다양한 방법을 구현하거나 이에 대한 인터페이스가 있습니다. 이를 통해 수천 명에 달하는 전문가의 작업을 손쉽게 확인할 수 있습니다.\n오픈 소스 및 커뮤니티 소유. R과 Bioconductor는 대규모 개발자 커뮤니티에 의해 공동으로 구축되었습니다. 수많은 사용자가 지속적으로 시도하고 테스트합니다.\n데이터 입력 및 랭글링. 바이오컨덕터 패키지는 현대 생물학에서 사용되는 측정 장비에서 생성된 다양한 데이터 유형 및 형식의 판독과 필요한 기술별 “전처리” 루틴을 지원합니다. 커뮤니티는 장비 시장의 급속한 발전에 맞춰 이러한 최신 정보를 적극적으로 유지하고 있습니다.\n시뮬레이션. 알려진 모든 통계 분포에 대한 난수 생성기와 선형 대수, 최적화 등을 위한 강력한 수치 루틴이 있습니다.\n시각화 및 프리젠테이션. R은 매력적인 출판 품질의 그래픽을 만들 수 있습니다. 이에 대한 내용을 3개의 데이터 시각화에 전념하고 책 전반에 걸쳐 데이터 시각화를 광범위하게 실천합니다.\n사용하기 쉬운 대화형 개발 환경. RStudio는 사용하기 쉽고 재미있으며 R 프로그래밍의 모든 측면에 도움이 됩니다. 그림 2에 도식화된 데이터 분석에 대한 반복적 접근 방식을 따르는 데 필수적인 부분입니다.\n재현성. 실험실 작업의 표준 모범 사례인 실험실 노트와 동일하게 R 마크다운 또는 4절판 형식으로 작성된 계산 일기의 사용을 옹호합니다. 우리는 quarto 시스템을 사용하여 이러한 파일을 읽기 쉽고 공유 가능한 HTML 또는 PDF 문서로 변환합니다. 이는 본격적인 과학 기사나 보충 자료가 될 수도 있습니다. 버전 제어 시스템과 함께 이 접근 방식은 변경 사항을 추적하는 데도 도움이 됩니다.\n협업 환경. Quarto를 사용하면 최소한의 작업으로 코드, 텍스트, 그림 및 표가 포함된 웹사이트를 만들 수 있습니다.\n풍부한 데이터 구조. Bioconductor 프로젝트는 복잡한 생물학적 데이터 세트를 나타내는 특수 데이터 컨테이너를 정의했습니다. 이는 데이터를 일관되고 안전하며 사용하기 쉽게 유지하는 데 도움이 됩니다.\n상호 운용성 및 분산 개발. 특히 Bioconductor에는 광범위한 기능을 포괄하지만 공통 데이터 컨테이너로 인해 여전히 상호 운용되는 다양한 작성자의 패키지가 포함되어 있습니다.\n문서화. 많은 R 패키에는 기능 매뉴얼 페이지와 삽화에 훌륭한 문서가 포함되어 있습니다. 비네트는 일반적으로 패키지의 기능에 대한 높은 수준의 설명을 제공하는 반면, 매뉴얼 페이지는 각 기능의 입력, 출력 및 내부 작동에 대한 자세한 정보를 제공하므로 일반적으로 패키지의 가장 좋은 시작점입니다. R 및 Bioconductor 작업의 다양한 측면에 대한 온라인 자습서, 포럼 및 메일링 목록이 있습니다.\n고급 언어. R은 해석된 고급 언어입니다. LISP에 뿌리를 두고 있으며 함수형 프로그래밍 기능은 코드가 데이터이고 계산될 수 있다는 것을 의미하며, 이는 효율적인 프로그래밍을 가능하게 하고 재미있습니다. 이러한 기능은 강력한 도메인별 언어 구축을 용이하게 합니다2. R은 고정된 언어가 아닙니다. 역사 전반에 걸쳐 R은 적극적으로 발전해 왔으며 지속적으로 개선되고 있습니다.\n2 예에는 R의 수식 인터페이스, ggplot2의 그래픽 문법, dplyr의 데이터 조작 기능 및 R 마크다운이 포함됩니다.\n에프론, 브래들리. 2010. 대규모 추론: 추정, 테스트 및 예측을 위한 경험적 베이즈 방법. 케임브리지 대학 출판부.\n피셔, 로널드 아일머. 1935. 실험 설계. 올리버 앤 보이드.\nGrolemund, Garrett 및 Hadley Wickham. 2017. 데이터사이언스를 위한 R. 오라일리.\n헤이스티, 트레버, 로버트 팁시라니, 제롬 프리드먼. 2008. 통계학습의 요소. 2^{} ed. 뛰는 것.\n홈즈 - 준카, 수잔. 1985. “데이터 분석 결과의 관련성을 평가하기 위한 컴퓨터 도구.” 프랑스 몽펠리에 2대학 박사학위 논문.\nHuber, Wolfgang, Vincent J Carey, Robert Gentleman, Simon Anders, Marc Carlson, Benilton S Carvalho, Hector Corrada Bravo 등 2015. “Bioconductor를 사용하여 처리량이 높은 게놈 분석 조정.” Nature Methods 12 (2): 115–21.\n이하카, 로스, 로버트 젠틀맨. 1996. “R: 데이터 분석 및 그래픽을 위한 언어.” 계산 및 그래픽 통계 저널 5 (3): 299–314.\n네이먼, 저지, 에곤 S 피어슨. 1936. 충분한 통계와 통계적 가설에 대한 균일하게 가장 강력한 검정. 캘리포니아 대학 출판부.\nTukey, John W. 1977. “탐색적 데이터 분석.” 매사추세츠: 애디슨-웨슬리.\nR 버전 4.5.1(2025-06-13)을 사용하여 2025-09-01 01:33에 작성된 페이지",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>서문 (Preface)</span>"
    ]
  },
  {
    "objectID": "01-chap.html",
    "href": "01-chap.html",
    "title": "3  이산 데이터를 위한 생성 모델",
    "section": "",
    "text": "3.1 1.2 실제 예제\n분자 생물학에서 많은 상황은 사건의 수를 세는 것과 관련이 있습니다: 얼마나 많은 코돈이 특정 철자를 사용하는지, 얼마나 많은 DNA 리드가 참조 유전체와 일치하는지, DNA 서열에서 얼마나 많은 CpG 다이뉴클레오타이드(dinucleotides)가 관찰되는지 등입니다. 이러한 카운트는 질량이나 강도와 같이 연속적인 척도로 측정되는 양과 대조적으로 우리에게 이산(discrete) 변수를 제공합니다.\n연구 중인 메커니즘이 따르는 규칙을 알고 있다면, 비록 그 결과가 무작위일지라도 계산과 표준 확률 법칙을 통해 우리가 관심 있는 모든 사건의 확률을 생성할 수 있습니다. 이는 연역과 확률 조작 방법에 대한 우리의 지식에 기반한 하향식(top-down) 접근 방식입니다. 2장에서는 이를 데이터 기반(상향식, bottom-up) 통계 모델링과 결합하는 방법을 살펴볼 것입니다.\n이 장에서 우리는 다음을 수행할 것입니다:\n데이터 생성 과정에 대한 확률 모델이 있는 예제로 뛰어들어 봅시다. 우리 모델은 HIV(인간 면역결핍 바이러스)의 게놈을 따른 돌연변이가 복제 주기당 뉴클레오타이드당 \\(5 ^{-4}\\)의 비율로 발생한다고 말합니다. 돌연변이율은 각 뉴클레오타이드 위치에서 동일하며, 한 위치에서의 돌연변이는 다른 위치에서 일어나는 일과 독립적으로 발생합니다1. HIV의 게놈 크기는 약 \\(10^4=10,000\\) 뉴클레오타이드이므로, 한 주기 후에 총 돌연변이 수는 비율이 \\(5 ^{-4} ^4 = 5\\)인 푸아송(Poisson) 분포2를 따를 것입니다. 이것이 우리에게 무엇을 말해줄까요?\n1 실제로는, 그리고 엄밀히 말하자면, 충분히 자세히 살펴보면 완전하고 절대적인 독립성이 현실에서 유지되는 경우는 드뭅니다. 따라서 모델러들이 그러한 주장을 할 때 대개 의미하는 바는, 발생 가능한 상관관계나 의존성이 너무 약하고 드물어서 이를 무시하는 것이 충분히 좋은 근사치라는 것입니다.\n2 이러한 유형의 확률 분포에 대해서는 나중에 더 자세히 다룰 것입니다.\n이 확률 모델은 한 번의 복제 주기 동안의 돌연변이 수가 5에 가까울 것이며, 이 추정치의 가변성(표준 오차)은 \\(\\)가 될 것이라고 예측합니다. 이제 우리는 전형적인 HIV 균주에서 볼 것으로 예상되는 돌연변이 수와 그 가변성 모두에 대한 기준 참조 값을 갖게 되었습니다.\n사실 우리는 훨씬 더 자세한 정보를 이끌어낼 수 있습니다. 만약 우리가 푸아송(5) 모델하에서 3번의 돌연변이가 얼마나 자주 발생할 수 있는지 알고 싶다면, 람다(\\(\\))라고 불리는 푸아송 분포의 율 매개변수(rate parameter) 값을 5로 설정하고 R 함수를 사용하여 \\(x=3\\)인 사건을 볼 확률을 생성할 수 있습니다.\n\\(\\)나 \\(\\)와 같은 그리스 문자는 종종 우리가 사용하는 확률 분포를 특징짓는 중요한 매개변수를 나타냅니다.\n이는 정확히 세 번의 사건을 볼 확률이 약 0.14, 즉 7번 중 1번 꼴임을 말해줍니다.\n0부터 12까지 모든 값의 확률을 생성하고 싶다면 루프(loop)를 작성할 필요가 없습니다. R의 시퀀스 연산자인 콜론(“:”)을 사용하여 첫 번째 인수를 이 13개 값의 벡터(vector)로 간단히 설정할 수 있습니다. 확률들을 플로팅하여 확인할 수 있습니다(그림 1.1). 이 그림과 마찬가지로, 이 책의 여백에 있는 대부분의 그림은 본문에 표시된 코드에 의해 생성됩니다.\nR의 출력 형식이 어떻게 지정되는지 주목하세요: 첫 번째 줄은 벡터의 첫 번째 항목으로 시작하므로 [1]이고, 두 번째 줄은 9번째 항목으로 시작하므로 [9]입니다. 이는 긴 벡터에서 요소들의 위치를 파악하는 데 도움이 됩니다. _벡터(vector)_라는 용어는 동일한 유형(이 경우 숫자)의 순서가 지정된 요소 리스트를 일컫는 R 용어입니다.\n그림 1.1: 푸아송(5) 분포에 의해 모델링된 0, 1, 2, …, 12개의 돌연변이가 나타날 확률. 플롯은 4개나 5개의 돌연변이를 자주 보게 되겠지만 12개만큼 많이 보는 경우는 드물 것임을 보여줍니다. 분포는 더 큰 숫자(\\(13, …\\))로 계속 이어지지만 확률은 점차 작아질 것이며, 여기서는 시각화하지 않았습니다.\n수학 이론은 \\(x\\)라는 값을 볼 푸아송 확률이 \\(e^{-} ^x / x!\\) 공식에 의해 주어짐을 알려줍니다. 이 책에서 우리는 때때로 이론을 논의하겠지만, 그림 1.1과 같은 구체적인 수치 예제와 시각화를 보여주는 것을 선호할 것입니다.\n푸아송 분포는 돌연변이와 같은 희귀 사건에 대한 좋은 모델입니다. 이산 사건(discrete events)에 대한 다른 유용한 확률 모델로는 베르누이(Bernoulli), 이항(binomial), 다항(multinomial) 분포가 있습니다. 이 장에서 이러한 모델들을 탐구할 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>이산 데이터를 위한 생성 모델</span>"
    ]
  },
  {
    "objectID": "01-chap.html#실제-예제",
    "href": "01-chap.html#실제-예제",
    "title": "3  이산 데이터를 위한 생성 모델",
    "section": "",
    "text": "dpois(x = 3, lambda = 5)\n\n\n[1] 0.1403739\n\n\n\n\n\nR의 출력 형식이 어떻게 지정되는지 주목하세요: 첫 번째 줄은 벡터의 첫 번째 항목으로 시작하므로 [1]이고, 두 번째 줄은 9번째 항목으로 시작하므로 [9]입니다. 이는 긴 벡터에서 요소들의 위치를 파악하는 데 도움이 됩니다. 벡터(vector)라는 용어는 동일한 유형(이 경우 숫자)의 순서가 지정된 요소 리스트를 일컫는 R 용어입니다.\n\n\n\n0:12\n\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12\n\n\ndpois(x = 0:12, lambda = 5)\n\n\n [1] 0.0067 0.0337 0.0842 0.1404 0.1755 0.1755 0.1462 0.1044 0.0653 0.0363\n[11] 0.0181 0.0082 0.0034\n\n\nbarplot(dpois(0:12, 5), names.arg = 0:12, col = \"red\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>이산 데이터를 위한 생성 모델</span>"
    ]
  },
  {
    "objectID": "01-chap.html#이산-확률-모델-사용하기",
    "href": "01-chap.html#이산-확률-모델-사용하기",
    "title": "3  이산 데이터를 위한 생성 모델",
    "section": "3.2 1.3 이산 확률 모델 사용하기",
    "text": "3.2 1.3 이산 확률 모델 사용하기\n\n\n\n범주형 변수가 서로 다른 대안적인 값들을 갖는 것으로 생각하세요. 이들은 유전자 좌위에서의 서로 다른 대안인 대립유전자(alleles)와 유사한 수준(levels)들입니다.\n\n\n범주형 변수가 서로 다른 대안적인 값들을 갖는 것으로 생각하세요. 이들은 유전자 좌위에서의 서로 다른 대안인 _대립유전자(alleles)_와 유사한 수준(levels)들입니다.\n점 돌연변이는 발생하거나 발생하지 않을 수 있습니다; 이는 이진(binary) 사건입니다. 두 가지 가능한 결과(예, 아니오)를 범주형 변수의 수준(levels)이라고 부릅니다.\n모든 사건이 이진인 것은 아닙니다. 예를 들어, 이배체 유기체의 유전자형은 세 가지 수준(AA, Aa, aa)을 가질 수 있습니다.\n때때로 범주형 변수의 수준 수가 매우 많을 수 있습니다; 생물학적 샘플에 있는 서로 다른 박테리아 유형의 수(수백 또는 수천 개)나 3개의 뉴클레오타이드로 형성된 코돈의 수(64개 수준)가 그 예입니다.\n샘플에서 범주형 변수를 측정할 때, 우리는 종종 카운트 벡터에서 서로 다른 수준의 빈도를 집계하고 싶어 합니다. R은 범주형 변수를 위한 특수한 인코딩을 가지고 있으며 이를 요인(factors) 3이라고 부릅니다. 여기서는 19명의 피험자에 대한 서로 다른 혈액 유전자형을 벡터에 담아 표로 만듭니다.\n3 R은 요인 변수가 다른 “허용되지 않은” 값들을 받아들이지 않도록 보장하며, 이는 계산을 안전하게 유지하는 데 유용합니다.\n\n\n\nc()는 가장 기본적인 함수 중 하나입니다. 동일한 유형의 요소들을 하나의 벡터로 결합합니다. 여기에 표시된 코드에서 genotype의 요소들은 문자열입니다.\n\n\nc()는 가장 기본적인 함수 중 하나입니다. 동일한 유형의 요소들을 하나의 벡터로 결합합니다. 여기에 표시된 코드에서 genotype의 요소들은 문자열입니다.\ngenotype = c(\"AA\",\"AO\",\"BB\",\"AO\",\"OO\",\"AO\",\"AA\",\"BO\",\"BO\",\n             \"AO\",\"BB\",\"AO\",\"BO\",\"AB\",\"OO\",\"AB\",\"BB\",\"AO\",\"AO\")\ntable(genotype)\n\n\ngenotype\nAA AB AO BB BO OO \n 2  2  7  3  3  2 \n_요인(factor)_을 생성할 때, R은 자동으로 수준을 감지합니다. levels 함수를 사용하여 수준에 접근할 수 있습니다.\n\n\n\ntable 함수의 출력에서 입력이 요인이었다는 사실이 명확히 드러나지는 않습니다. 하지만 만약 데이터에 나타나지 않은 다른 수준이 있었다면, 테이블은 해당 수준에 대해 0의 카운트를 포함했을 것입니다.\n\n\ntable 함수의 출력에서 입력이 요인이었다는 사실이 명확히 드러나지는 않습니다. 하지만 만약 데이터에 나타나지 않은 다른 수준이 있었다면, 테이블은 해당 수준에 대해 0의 카운트를 포함했을 것입니다.\ngenotypeF = factor(genotype)\nlevels(genotypeF)\n\n\n[1] \"AA\" \"AB\" \"AO\" \"BB\" \"BO\" \"OO\"\n\n\ntable(genotypeF)\n\n\ngenotypeF\nAA AB AO BB BO OO \n 2  2  7  3  3  2 \n질문 1.1\n데이터에 아직 존재하지 않는 일부 수준을 가진 _요인_을 만들고 싶다면 어떻게 해야 할까요?\n해결책\nfactor 함수의 매뉴얼 페이지를 참조하세요.\n데이터가 관찰되는 순서가 중요하지 않다면, 그 확률 변수를 교환 가능(exchangeable)하다고 부릅니다. 이 경우 요인에서 가용한 모든 정보는 요인 수준의 카운트로 요약됩니다. 그러면 빈도 벡터가 데이터의 모든 관련 정보를 캡처하는 데 충분(sufficient)하다고 말하며, 이는 데이터를 압축하는 효과적인 방법을 제공합니다.\n\n3.2.1 1.3.1 베르누이 시행\n\n그림 1.2: 확률이 서로 다른 두 가지 가능한 사건. 우리는 이를 확률 매개변수 \\(p=2/3\\)인 베르누이 분포로 모델링합니다.\n동전을 던지는 것에는 두 가지 가능한 결과가 있습니다. 베르누이 시행(Bernoulli trial)이라 불리는 이 간단한 실험은 이른바 베르누이 확률 변수를 사용하여 모델링됩니다. 이 구성 요소를 이해하면 놀라울 정도로 멀리 갈 수 있습니다. 이를 사용하여 더 복잡한 모델들을 구축할 수 있습니다.\n이러한 확률 변수들 중 일부가 어떻게 생겼는지 보기 위해 몇 가지 실험을 시도해 봅시다. 우리는 각 분포 유형에 대해 결과를 생성하도록 맞춰진 특수한 R 함수들을 사용합니다. 이들은 모두 문자 r로 시작하며 그 뒤에 모델 사양이 옵니다. 여기서는 이항(binomial)의 약어인 binom을 사용한 rbinom입니다.\n15번의 공정한 동전 던지기 시퀀스를 시뮬레이션하고 싶다고 가정해 봅시다. 성공 확률이 0.5(공정한 동전)인 15번의 베르누이 시행 결과를 얻으려면 다음과 같이 씁니다.\nrbinom(15, prob = 0.5, size = 1)\n\n\n [1] 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0\n우리는 특정 매개변수(parameters) 4 세트와 함께 rbinom 함수를 사용합니다: 첫 번째 매개변수는 우리가 관찰하고 싶은 시행 횟수이며, 여기서는 15를 선택했습니다. prob로 성공 확률을 지정합니다. size=1을 통해 각 개별 시행이 단 한 번의 동전 던지기로만 구성됨을 선언합니다.\n4 R 함수의 경우, 매개변수는 인수(argument)라고도 불립니다.\n질문 1.2\n이 함수 호출을 여러 번 반복해 보세요. 왜 답이 매번 똑같지 않을까요?\n성공과 실패의 확률이 합해서 1이 되는 한, 베르누이 시행에서 두 확률이 서로 다를 수 있습니다5. 그림 1.2에 표시된 것처럼 오른쪽 상자에 떨어질 확률이 \\(\\)이고 왼쪽 상자에 떨어질 확률이 \\(\\)인 조건에서 공을 던지는 12번의 시행을 시뮬레이션하려면 다음과 같이 씁니다.\n5 우리는 이러한 사건들을 여사건(complementary)이라고 부릅니다.\nrbinom(12, prob = 2/3, size = 1)\n\n\n [1] 1 1 1 0 0 0 1 0 1 0 1 0\n1은 성공을 나타내며 공이 오른쪽 상자에 떨어졌음을 의미하고, 0은 공이 왼쪽 상자에 떨어졌음을 의미합니다.\n\n\n3.2.2 1.3.2 이항 성공 횟수\n만약 우리가 오직 몇 개의 공이 오른쪽 상자에 들어갔는지만 관심이 있다면, 던지는 순서는 중요하지 않으며6 출력 벡터의 셀들을 합산함으로써 이 숫자를 얻을 수 있습니다. 따라서 위에서 본 이진 벡터 대신 단일 숫자만을 보고하면 됩니다. R에서는 size 매개변수를 12로 설정하고 rbinom 함수를 한 번 호출함으로써 이를 수행할 수 있습니다.\n두 가지 결과가 있고 크기가 1 이상이면 이항 시행(binomial trial)이 됩니다. 만약 크기가 1이라면, 이는 베르누이 시행의 특수한 사례입니다.\n6 교환성(exchangeability) 속성.\nrbinom(1, prob = 2/3, size = 12)\n\n\n[1] 9\n이 출력은 12개의 공 중에서 몇 개가 오른쪽 상자(확률 2/3인 결과)에 떨어졌는지 알려줍니다. 앞면 또는 뒷면, 성공 또는 실패, CpG 또는 non-CpG, M 또는 F, Y = 피리미딘 또는 R = 퓨린, 질병 또는 건강, 참 또는 거짓과 같이 두 가지 가능한 결과만 있는 경우 무작위 2-박스 모델을 사용합니다. “성공” 확률 \\(p\\)만 알면 되는데, “실패”(여사건)는 \\(1-p\\)의 확률로 발생하기 때문입니다. 그러한 여러 번의 시행 결과를 볼 때, 만약 그것들이 교환 가능하다면7 성공 횟수만을 기록합니다. 따라서 SSSSSFSSSSFFFSF는 (성공 횟수=10, 실패 횟수=5), 즉 \\(x=10, n=15\\)로 요약됩니다.\n7 시행들이 서로 독립적이라면 교환 가능한 상황 중 하나입니다.\n성공 확률이 0.3인 15번의 베르누이 시행에서의 성공 횟수를 이항(binomial) 확률 변수, 또는 \\(B(15, 0.3)\\) 분포를 따르는 확률 변수라고 부릅니다. 샘플을 생성하기 위해 시행 횟수를 15로 설정한 rbinom 함수 호출을 사용합니다.\n\n\n\nset.seed는 여기서 무엇을 하나요?\n\n\n여기서 set.seed는 무엇을 하나요?\nset.seed(235569515)\nrbinom(1, prob = 0.3, size = 15)\n\n\n[1] 5\n질문 1.3\n이 함수 호출을 10번 반복해 보세요. 가장 흔하게 나타나는 결과는 무엇인 것 같나요?\n해결책\n가장 빈번한 값은 4입니다. 사실, \\(X\\)가 \\(B(15, 0.3)\\)을 따를 때 4가 나타날 것으로 기대되는 이론적 비율은 \\(X=4\\)일 확률값입니다.\n전체 확률 질량 분포(probability mass distribution)는 다음을 입력하여 확인할 수 있습니다.\n출력되는 소수점 자릿수를 2자리로 유지하기 위해 round 함수를 사용합니다.\nprobabilities = dbinom(0:15, prob = 0.3, size = 15)\nround(probabilities, 2)\n\n\n [1] 0.00 0.03 0.09 0.17 0.22 0.21 0.15 0.08 0.03 0.01 0.00 0.00 0.00 0.00 0.00\n[16] 0.00\n그림 1.3에 표시된 이 분포의 막대 그래프를 생성할 수 있습니다.\nbarplot(probabilities, names.arg = 0:15, col = \"red\")\n\n그림 1.3: \\(B(15, 0.3)\\)의 이론적 분포. 가장 높은 막대는 \\(x=4\\)에 있습니다. 우리는 이론적 값을 전체적으로 빨간색으로 표현하기로 선택했습니다.\n시행 횟수는 R에서 size 매개변수로 입력한 숫자이며 종종 \\(n\\)으로 쓰이고, 성공 확률은 \\(p\\)입니다. 수학 이론은 매개변수가 \\((n, p)\\)인 이항 분포로 분포된 \\(X\\)(\\(X B(n, p)\\)로 씀)에 대해, \\(X=k\\)번의 성공을 볼 확률은 다음과 같음을 알려줍니다.\n\n\n\nn! / (n-k)!k! 대신 특수 표기법 (n k)를 단축키로 사용할 수 있습니다.\n\n\n\\(\\) 대신 특수 표기법 \\({n k}\\)를 단축키로 사용할 수 있습니다.\n[\n\\[\\begin{aligned} P(X=k) &=\\frac{n\\times (n-1)... (n-k+1)}{k\\times(k-1)... 1}\\; p^k\\, (1-p)^{n-k}\\\\ &=\\frac{n!}{(n-k)!k!}\\;p^k\\, (1-p)^{n-k}\\\\ &={ n \\choose k}\\; p^k\\, (1-p)^{n-k}. \\end{aligned}\\]\n]\n질문 1.4\n\\(k=3, p=2/3, n=4\\)일 때 위 공식의 결과값은 얼마인가요?\n\n\n3.2.3 1.3.3 푸아송 분포\n\n그림 1.4: 푸아송 분포의 이름을 따온 시메옹 푸아송(이것이 R 코드를 제외하고는 항상 대문자로 시작하는 이유입니다).\n성공 확률 \\(p\\)가 작고 시행 횟수 \\(n\\)이 클 때, 이항 분포 \\(B(n, p)\\)는 더 단순한 분포인 율 매개변수 \\(=np\\)를 가진 푸아송(Poisson) 분포에 의해 충실하게 근사될 수 있습니다. 우리는 이미 HIV 예제(그림 1.1)에서 이 사실과 분포를 사용했습니다.\n질문 1.5\n뉴클레오타이드당 확률이 \\(p = 5 ^{-4}\\)일 때, \\(n = 10^4\\) 뉴클레오타이드 게놈에서 0:12개의 돌연변이를 관찰할 확률 질량 분포는 무엇인가요? 이항 분포 \\(B(n, p)\\)로 모델링할 때와 푸아송 분포 \\(Poisson(=np)\\)로 모델링할 때 결과가 유사한가요?\n해결책\n이항 분포와 달리 푸아송 분포는 더 이상 두 개의 개별 매개변수 \\(n\\)과 \\(p\\)에 의존하지 않고, 오직 그들의 곱인 \\(np\\)에만 의존한다는 점에 유의하세요. 이항 분포의 경우와 마찬가지로, 푸아송 확률을 계산하기 위한 수학 공식도 존재합니다.\n[ P(X=k)= . ]\n예를 들어, \\(\\)로 두고 \\(P(X=3)\\)을 계산해 봅시다.\n5^3 * exp(-5) / factorial(3)\n\n\n[1] 0.1403739\n이는 위에서 dpois를 사용하여 계산한 것과 비교할 수 있습니다.\n태스크\n돌연변이율이 \\(5^{-4}\\)인 10,000개 위치에서의 돌연변이 과정을 시뮬레이션하고 돌연변이 수를 셉니다. 이를 여러 번 반복하고 barplot 함수를 사용하여 분포를 플롯하세요 (그림 1.5 참조).\nrbinom(1, prob = 5e-4, size = 10000)\n\n\n[1] 6\n\n\nsimulations = rbinom(n = 300000, prob = 5e-4, size = 10000)\nbarplot(table(simulations), col = \"lavender\")\n\n그림 1.5: 300,000번의 시뮬레이션에 대한 B(10000, \\(5 ^{-4}\\))의 시뮬레이션된 분포.\n이제 사례 연구에서 확률 계산을 사용할 준비가 되었습니다.\n\n\n3.2.4 1.3.4 에피토프 검출을 위한 생성 모델\n특정 의약 화합물을 테스트할 때, 알레르기 반응을 유도하는 단백질을 찾아내는 것이 중요합니다. 그러한 반응을 일으키는 분자 부위를 에피토프(epitopes)라고 부릅니다. 에피토프의 기술적 정의는 다음과 같습니다.\n\n항체가 결합하는 거대 분자 항원의 특정 부분. T 세포에 의해 인식되는 단백질 항원의 경우, 에피토프 또는 결정기(determinant)는 T 세포 수용체(TCR)에 의한 인식을 위해 주조직 적합성 복합체(MHC) 분자에 결합하는 펩타이드 부분 또는 부위이다.\n\n면역학이 생소하신 분들을 위해 설명하자면: 항체(antibody)(그림 1.6에 도식화됨)는 체내의 이물질인 항원(antigen)에 대응하여 특정 백혈구에서 만들어지는 단백질의 일종입니다.\n\n그림 1.6: 여러 면역글로불린 도메인을 색상으로 보여주는 항체 다이어그램.\n항체는 항원에 (어느 정도의 특이성을 가지고) 결합합니다. 결합의 목적은 항원을 파괴하는 것을 돕는 것입니다. 항체는 항원의 성격에 따라 여러 방식으로 작동할 수 있습니다. 일부 항체는 항원을 직접 파괴합니다. 다른 것들은 항원을 파괴하기 위해 백혈구를 모집하는 것을 돕습니다. 항원 결정기라고도 알려진 에피토프는 면역계, 특히 항체, B 세포 또는 T 세포에 의해 인식되는 항원의 일부입니다.\n\n3.2.4.1 알려진 매개변수를 가진 ELISA 오차 모델\nELISA8 어세이는 단백질을 따른 서로 다른 위치에서 특정 에피토프를 검출하는 데 사용됩니다. 우리가 사용하는 ELISA 어레이에 대해 다음 사실들이 성립한다고 가정해 봅시다.\n8 Enzyme-Linked ImmunoSorbent Assay (Wikipedia ELISA 링크).\n\n위치당 기본 노이즈 수준, 즉 위양성률(false positive rate)은 1%입니다. 이는 에피토프가 없는데도 히트(hit)라고 선언할 –즉, 에피토프가 있다고 생각할– 확률입니다. 우리는 이를 \\(P(|)\\)9으로 씁니다.\n단백질은 서로 독립적이라고 가정되는 100개의 서로 다른 위치에서 테스트됩니다.\n우리는 50명의 환자 샘플 컬렉션을 조사할 것입니다.\n\n9 \\(X|Y\\)와 같은 표현식에서 수직 막대는 “\\(Y\\)인 조건하에 \\(X\\)가 발생함”을 의미합니다.\n\n\n3.2.4.2 한 환자의 데이터\n한 환자의 어세이 데이터는 다음과 같습니다.\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n여기서 1은 히트(따라서 알레르기 반응의 가능성)를 나타내고, 0은 해당 위치에서 반응이 없음을 나타냅니다.\n태스크\n50개의 독립적인 베르누이 변수(\\(p=0.01\\))의 합이 –충분히 좋은 근사치로– 푸아송(0.5) 확률 변수와 같음을 시뮬레이션을 통해 확인하세요.\n\n\n3.2.4.3 50번의 어세이 결과\n우리는 100개 위치 각각에서 집계된 50명 환자 모두의 데이터를 연구할 것입니다. 만약 알레르기 반응이 없다면, 위양성률은 한 환자에 대해 각 개별 위치가 1/100의 확률로 1이 됨을 의미합니다. 따라서 50명의 환자를 집계한 후, 임의의 주어진 위치에서의 50개 관측된 \\((0, 1)\\) 변수의 합은 매개변수가 0.5인 푸아송 분포를 가질 것으로 기대합니다. 전형적인 결과는 그림 1.7과 같을 것입니다. 이제 그림 1.8에 표시된 것과 같은 실제 데이터가 데이터 파일 e100.RData로부터 R 객체 e100으로 로드되었다고 가정해 봅시다.\n\n그림 1.7: 배경(즉, 위양성 히트)에 대한 우리 생성 모델로부터 얻은 전형적인 데이터 플롯: 단백질을 따른 100개 위치에서, 각 위치의 카운트는 푸아송(0.5) 확률 변수로부터 추출되었습니다.\nload(\"../data/e100.RData\")\nbarplot(e100, ylim = c(0, 7), width = 0.7, xlim = c(-0.5, 100.5),\n  names.arg = seq(along = e100), col = \"darkolivegreen\")\n\n그림 1.8: 100개 위치에서의 50명 환자에 대한 ELISA 어레이 결과 출력.\n그림 1.8의 스파이크(spike)가 인상적입니다. 에피토프가 존재하지 않는다면, 7만큼 큰 값이 나타날 확률은 얼마일까요?\n하나의 푸아송(0.5) 확률 변수를 고려할 때 7만큼 큰 숫자(또는 그 이상)를 볼 확률을 찾는다면, 그 답은 다음과 같이 닫힌 형태(closed form)로 계산될 수 있습니다.\n[ P(X)= _{k=7}^P(X=k). ]\n이는 물론 \\(1-P(X)\\)과 같습니다. 확률 \\(P(X)\\)은 6에서의 이른바 누적 분포(cumulative distribution) 함수이며, R에는 이를 계산하기 위한 ppois 함수가 있는데, 다음 두 가지 방법 중 하나로 사용할 수 있습니다:10\n10 1에서 뺄 필요가 없다는 편리함 외에도, 이 계산들 중 두 번째 방식은 확률이 작을 때 더 정확한 경향이 있습니다. 이는 부동 소수점 연산의 한계와 관련이 있습니다.\n1 - ppois(6, 0.5)\n\n\n[1] 1.00238e-06\n\n\nppois(6, 0.5, lower.tail = FALSE)\n\n\n[1] 1.00238e-06\n태스크\nppois의 매뉴얼 페이지에서 lower.tail 인수의 의미를 확인하세요.\n우리는 이 숫자를 그리스 문자 엡실론(\\(\\)11)으로 표시합니다. 우리는 에피토프 반응이 없다고 가정할 때 7만큼 큰 카운트를 볼 확률이 다음과 같음을 보였습니다.\n11 수학자들은 종종 작은 숫자(및 어린이들)를 \\(\\)이라고 부릅니다.\n[ =P(X)=1-P(X)^{-6}. ]\n\n\n3.2.4.4 푸아송 분포에 대한 극단값 분석\n잠깐만요! 위의 계산은 이 경우에 대한 올바른 계산이 아닙니다.\n질문 1.6\n에피토프가 없을 때 이 데이터를 관찰할 확률을 계산하고 싶다면, 우리의 추론에서 어떤 결함을 찾을 수 있나요?\n해결책\n우리는 100개 위치 모두를 살펴보고 가장 큰 값을 찾아 7임을 확인했습니다. 이러한 선택 때문에, 7만큼 큰 값이 나타날 확률은 단 하나의 위치만을 보았을 때보다 더 높습니다.\n따라서 푸아송(0.5)이 7만큼 클 확률이 얼마인지 묻는 대신, 우리는 100번의 푸아송(0.5) 시행의 최댓값이 7만큼 클 확률은 얼마인지 자문해야 합니다. 여기서 우리는 극단값(extreme value) 분석을 사용합니다12. 우리는 데이터 값 \\(x_1, x_2, …, x_{100}\\)을 정렬하고 이름을 \\(x_{(1)}, x_{(2)}, x_{(3)}, …, x_{(100)}\\)으로 변경하여, \\(x_{(1)}\\)이 100개 위치 중에서 가장 작은 카운트를 나타내고 \\(x_{(100)}\\)이 가장 큰 카운트를 나타내도록 합니다. \\(x_{(1)}, …, x_{(100)}\\)을 통칭하여 이 100개 값 표본의 순위 통계량(rank statistic)이라고 부릅니다.\n12 무작위 분포의 매우 크거나 작은 값(예: 최댓값 또는 최솟값)의 거동에 관심이 있다는 뜻입니다.\n13 \\(\\)를 사용한 표기법은 합계를 위한 \\(\\)과 유사하게 일련의 항들의 곱을 쓰는 간결한 방법일 뿐입니다.\n최댓값이 7만큼 큰 사건은 100개의 카운트가 모두 6보다 작거나 같은 사건의 여사건입니다. 두 여사건의 확률 합은 1입니다. 위치들이 독립적이라고 가정하므로, 이제 계산을 수행할 수 있습니다13.\n[\n\\[\\begin{aligned} P(x_{(100)}\\geq7) &=1-P(x_{(100)}\\leq6)\\\\ &=1-P(x_{(1)}\\leq6)\\times P(x_{(2)}\\leq6)\\times \\cdots \\times P(x_{(100)}\\leq6)\\\\ &=1-P(x_1\\leq6)\\times P(x_2\\leq6)\\times \\cdots \\times P(x_{100}\\leq6)\\\\ &=1-\\prod_{i=1}^{100} P(x_i\\leq6). \\end{aligned}\\]\n]\n이 100개의 사건 각각이 독립적이라고 가정하므로, 위에서의 결과를 사용할 수 있습니다.\n[ _{i=1}^{100} P(x_i )= (P(x_i ))^{100}= (1-)^{100}. ]\n\n\n3.2.4.5 실제로 수치 계산하기\n우리는 단지 R이 이 수치의 값인 \\((1-)^{100}\\)을 계산하게 할 수도 있습니다. 그러한 계산이 근사를 통해 어떻게 단축될 수 있는지 관심 있는 분들을 위해 몇 가지 세부 사항을 제공합니다. 처음 읽으실 때는 건너뛰셔도 됩니다.\n우리는 위에서 \\(^{-6}\\)이 1보다 훨씬 작다는 것을 기억합니다. \\((1-)^{100}\\)의 값을 대략적으로 계산하기 위해, 우리는 이항 정리를 사용하고 \\(\\)의 “고차” 항들을 모두 버릴 수 있습니다. 즉, \\(^2, ^3, …\\) 항들은 나머지(“주요”) 항에 비해 무시할 수 있을 정도로 작기 때문입니다.\n[ (1-)^n = _{k=0}^n {nk} , 1^{n-k} , (-)^k = 1-n+{n} ^2 - {n} ^3 + … -n - 10^{-4} ]\n또 다른 동등한 경로는 \\(e^{-} -\\) 근사를 사용하는 것인데, 이는 \\((1-)-\\)과 같습니다. 따라서\n[ (1-)^{100} = e{((1-){100})} = e^{ 100 (1-)} e^{-100 } e{-10{-4}} - 10^{-4}. ]\n따라서 에피토프가 존재하지 않을 때 100개 위치에서 히트 수가 7개 이상일 정확한 확률은 이전에 잘못 계산했던 확률의 약 100배입니다.\n계산된 두 확률 \\(10^{-6}\\)과 \\(10^{-4}\\) 모두 표준 유의성 임계값(예: 0.05, 0.01 또는 0.001)보다 작습니다. 에피토프가 없다는 귀무 가설을 기각하는 결정은 동일했을 것입니다. 하지만 일부 법의학 법정 사례에서처럼 p-값을 유효 숫자 8자리까지 방어해야 한다면14 그것은 다른 문제입니다. 검정의 다중성을 고려한 수정된 p-값이 보고되어야 하는 값이며, 이 중요한 문제로 6장에서 다시 돌아올 것입니다.\n14 이는 O.J. 심슨 사건의 법의학적 증거 조사에서 발생했습니다.\n\n\n3.2.4.6 시뮬레이션을 통한 확률 계산\n방금 살펴본 사례에서는 이론적 확률 계산이 매우 간단했고 명시적인 계산을 통해 결과를 알아낼 수 있었습니다. 실제로는 상황이 더 복잡해지는 경향이 있으므로, 우리가 관심 있는 사건의 확률을 찾아주는 생성 모델 기반의 컴퓨터 시뮬레이션인 몬테카를로(Monte Carlo) 방법을 사용하여 확률을 계산하는 것이 더 낫습니다. 아래에서는 100개의 푸아송 분포 수에서 최댓값을 뽑는 과정을 100,000번 반복하여 생성합니다.\nmaxes = replicate(100000, {\n  max(rpois(100, 0.5))\n})\ntable(maxes)\n\n\nmaxes\n    1     2     3     4     5     6     7     9 \n    7 23028 60840 14364  1604   141    15     1 \n100,000번의 시행 중 16번에서 최댓값이 7 이상이었습니다. 이는 \\(P(X_{})\\)15에 대해 다음과 같은 근사치를 제공합니다.\n15 R에서 maxes &gt;= 7 표현식은 maxes와 길이는 같지만 TRUE와 FALSE 값을 가진 논리 벡터로 평가됩니다. 여기에 mean 함수를 적용하면 해당 벡터는 0과 1로 변환되며, 계산 결과는 1의 비율, 즉 TRUE의 비율과 같아집니다.\nmean( maxes &gt;= 7 )\n\n\n[1] 0.00016\n이는 우리의 이론적 계산과 어느 정도 일치합니다. 우리는 이미 몬테카를로 시뮬레이션의 잠재적인 한계 중 하나를 보았습니다: 시뮬레이션 결과의 “해상도(granularity)”는 시뮬레이션 횟수(100,000)의 역수에 의해 결정되므로 약 (10^{-5}) 수준이 됩니다. 추정된 확률은 이 해상도보다 더 정밀할 수 없으며, 실제로 우리 추정치의 정밀도는 그 수 배 수준일 것입니다. 지금까지 우리가 수행한 모든 작업은 위치당 위양성률을 알고, 분석된 환자 수와 단백질 길이를 알며, 모델로부터 동일하게 분포된 독립적인 추출을 한다고 가정하고 알 수 없는 매개변수가 없기 때문에 가능했습니다. 이것은 확률 모델링 또는 생성 모델링(generative modeling)의 예시입니다: 모든 매개변수가 알려져 있고 수학적 이론을 통해 하향식 방식으로 연역(deduction)을 수행할 수 있습니다.\n우리는 노이즈에 대해 푸아송 분포를 가정하고, 모든 매개변수를 알고 있는 척하며 수학적 연역을 통해 결론을 내릴 수 있었습니다.\n대신에 환자 수와 단백질 길이는 알고 있지만 데이터의 분포를 모르는 좀 더 현실적인 상황에 있다면, 우리는 통계 모델링(statistical modeling)을 사용해야 합니다. 이 접근 방식은 2장에서 전개될 것입니다. 시작할 데이터만 있는 경우, 우리는 먼저 이를 설명하기 위해 적절한 분포를 적합(fit)시켜야 함을 보게 될 것입니다. 하지만 이 더 어려운 문제로 넘어가기 전에, 이산 분포에 대한 지식을 이분법적인 성공-또는-실패 결과 이상으로 확장해 봅시다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>이산 데이터를 위한 생성 모델</span>"
    ]
  },
  {
    "objectID": "01-chap.html#다항-분포-dna의-사례",
    "href": "01-chap.html#다항-분포-dna의-사례",
    "title": "3  이산 데이터를 위한 생성 모델",
    "section": "3.3 1.4 다항 분포: DNA의 사례",
    "text": "3.3 1.4 다항 분포: DNA의 사례\n\n3.3.0.1 결과가 두 가지보다 많은 경우.\n그림 1.9의 상자들과 같이 네 가지 가능한 결과를 모델링하거나, 네 가지 뉴클레오타이드 [A, C, G] 및 [T]의 카운트를 연구할 때, 우리는 이항 모델을 확장해야 합니다.\n\n그림 1.9: 상자들은 이산 범주형(categorical) 변수의 네 가지 결과 또는 수준을 나타냅니다. 오른쪽의 상자는 더 발생 가능성이 높은 결과를 나타냅니다.\n이항 분포를 사용할 때, 결과 1에 확률 \\(p=P(1)=p_1\\)을 할당하고 결과 0에 확률 \\(1-p=p(0)=p_0\\)을 할당함으로써 두 결과에 대해 균등하지 않은 확률을 고려할 수 있었음을 상기하세요. [A, C, G] 및 [T]와 같이 두 가지보다 많은 가능한 결과가 있을 때, 우리는 서로 다른 확률에 해당하는 서로 다른 크기의 상자들에 공을 던지는 것을 생각할 수 있으며, 이 확률들을 \\(p_A, p_C, p_G, p_T\\)로 레이블링할 수 있습니다. 이항 분포의 경우와 마찬가지로 가능한 모든 결과의 확률 합은 1입니다: \\(p_A+p_C+p_G+p_T=1\\).\n\n\n\n여러분은 여기서 은연중에 연속 분포인 균등 분포(uniform distribution)인 runif를 만나고 있습니다.\n\n\n여러분은 여기서 은연중에 연속 분포인 균등 분포, runif를 만나고 있습니다.\n태스크\nrunif라는 함수를 통해 \\(0\\)과 \\(1\\) 사이의 모든 가능한 숫자를 생성하는 난수 생성기를 실험해 보세요. 이를 사용하여 \\(p_{}=, p_{}=, p_{}=, p_{}=\\)인 4가지 수준(A, C, G, T)을 가진 확률 변수를 생성하세요.\n수학적 공식화. 다항 분포(Multinomial distributions)는 카운트를 집계하기 위한 가장 중요한 모델이며, R은 확률이 \\(p_1, …, p_m\\)인 \\(m\\)개의 상자로부터 \\(n\\)번 추출한 결과에 대한 카운트의 다항 벡터 \\((x_1, …, x_m)\\)의 확률을 계산하기 위해 일반 공식을 사용합니다.\n첫 번째 항은 다음과 같이 읽습니다: 상자 1이 확률 \\(p_1\\)을 갖고, 상자 2가 확률 \\(p_2\\), …, 상자 \\(m\\)이 확률 \\(p_m\\)을 가질 때, 상자 1에서 카운트 \\(x_1\\), 상자 2에서 \\(x_2\\), …, 상자 \\(m\\)에서 \\(x_m\\)을 관찰할 결합 확률.\n[\\[\\begin{align} P(x_1,x_2,...,x_m) &=\\frac{n!}{\\prod_{i=1}^m x_i!} \\prod_{i=1}^m p_i^{x_i}\\\\ &={{n}\\choose{x_1,x_2,...,x_m}} \\; p_1^{x_1}\\,p_2^{x_2}\\cdots p_m^{x_m}. \\end{align}\\]]\n대괄호 안의 항은 다항 계수(multinomial coefficient)라고 불리며 다음의 약어입니다: [{nx_1,x_2,…,x_m}=.] 따라서 이는 이항 계수의 일반화입니다 – \\(m=2\\)인 경우 이항 계수와 동일합니다.\n질문 1.7\n동일한 확률을 가진 네 개의 상자가 있다고 가정해 봅시다. 공식을 사용하여, 첫 번째 상자에서 4, 두 번째 상자에서 2, 그리고 나머지 두 상자에서는 아무것도 관찰되지 않을 확률은 얼마인가요?\n해결책\n[ P(4,2,0,0)= =. ]\ndmultinom(c(4, 2, 0, 0), prob = rep(1/4, 4))\n\n\n[1] 0.003662109\n우리는 우리가 보는 데이터가 각 상자가 동일한 확률 1/4을 갖는 가장 단순한 가능한 4-박스 모델과 일치하는지 확인하기 위해 시뮬레이션 실험을 자주 실행합니다. 어떤 의미에서 그것은 허수아비(strawman)입니다 (흥미로운 일이 일어나지 않는다는 가설). 2장에서 이에 대한 더 많은 예시를 볼 것입니다. 여기서는 이러한 카운트 벡터를 생성하기 위해 몇 가지 R 명령어를 사용합니다. 먼저 네 가지 종류의 동일한 확률을 가진 8개의 문자가 있다고 가정해 봅시다.\npvec = rep(1/4, 4)\nt(rmultinom(1, prob = pvec, size = 8))\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    1    3\n질문 1.8\nt() 함수를 사용하지 않고 코드를 시도해 보세요; t는 무엇의 약자인가요?\n질문 1.9\nrmultinom(n = 8, prob = pvec, size = 1)과 rmultinom(n = 1, prob = pvec, size = 8)의 차이를 어떻게 해석하시겠습니까? 힌트: 1.3.1절과 1.3.2절에서 우리가 한 일을 떠올려 보세요.\n\n\n3.3.1 1.4.1 검정력 시뮬레이션 (Simulating for power)\n과학자들이 실험을 계획할 때 종종 해결해야 하는 문제인 “표본 크기가 얼마나 커야 하는가?”와 관련된 방식으로 다항 분포에 몬테카를로를 사용하는 예시를 살펴봅시다.\n\n통계학자에게 표본 크기에 대해 물어보면, 그들은 항상 더 많은 데이터가 필요하다고 말할 것입니다. 표본 크기가 클수록 결과는 더 민감해집니다. 하지만 실험실 작업은 비용이 많이 들기 때문에, 고려해야 할 까다로운 비용-편익 상충 관계가 있습니다. 이것은 매우 중요한 문제이므로 책의 마지막 부분(13장)에서 한 장 전체를 할애했습니다.\n검정력(power)이라는 용어는 통계학에서 특별한 의미를 가집니다. 이는 무언가가 거기에 존재할 때 이를 감지할 확률로, 진양성률(true positive rate)이라고도 불립니다.\n관례적으로 실험자들은 실험을 계획할 때 80% (또는 그 이상)의 검정력을 목표로 합니다. 이는 동일한 실험을 여러 번 실행할 때, 의미 있는 결과를 내야 함에도 불구하고 약 20%의 확률로 실패할 것임을 의미합니다.\n우리가 수집한 DNA 데이터가 4개의 뉴클레오타이드 각각이 동일하게 발생할 가능성이 있는 \\((p_A, p_C, p_G, p_T)=(0.25, 0.25, 0.25, 0.25)\\)인 공정한(fair) 과정으로부터 온다는 귀무 가설을 \\(H_0\\)라고 부릅시다. 여기서 귀무(Null)는 단지 기준선, 즉 흥미로운 일이 전혀 일어나지 않는 상태를 의미합니다. 그것은 우리가 반증(또는 통계학자들의 용어로 “기각”)하려고 시도하는 허수아비이므로, 귀무 가설은 그로부터의 이탈이 흥미로울 수 있는 것이어야 합니다16.\n16 생물학을 조금이라도 안다면, 살아있는 유기체의 DNA가 그 귀무 가설을 따르는 경우가 거의 없음을 알 것입니다 – 따라서 이를 반증하는 것이 그리 흥미롭지 않을 수도 있습니다. 여기서는 계산을 설명하기 위해 이 귀무 가설을 사용하지만, 좋은 귀무 가설(그 기각이 흥미로운 것)을 선택하는 데는 과학적인 입력이 필요하다는 점을 상기시켜 주는 역할도 합니다.\n동일한 크기의 상자로 표현된 8개의 문자와 4개의 동일한 확률의 결과에 대한 R 명령어를 실행하여 보았듯이, 우리는 항상 각 상자에서 2개를 얻지는 않습니다. 단지 8개의 문자만 보고 뉴클레오타이드가 공정한 과정에서 왔는지 아닌지 말하는 것은 불가능합니다.\n길이가 \\(n=20\\)인 시퀀스를 살펴봄으로써, 뉴클레오타이드의 원래 분포가 공정한지 아니면 어떤 다른(“대립”) 과정으로부터 온 것인지 감지할 수 있는지 결정해 봅시다.\nrmultinom 함수를 사용하여 귀무 가설로부터 1000번의 시뮬레이션을 생성합니다. 공간을 절약하기 위해 처음 11개 열만 표시합니다.\nobsunder0 = rmultinom(1000, prob = pvec, size = 20)\ndim(obsunder0)\n\n\n[1]    4 1000\n\n\nobsunder0[, 1:11]\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]\n[1,]    6    5    6    8    4    6    2    7    5     4     4\n[2,]    6    6    3    7    3    3    8    4    3     3     5\n[3,]    3    3    6    2    8    3    5    7    4     7     6\n[4,]    5    6    5    3    5    8    5    2    8     6     5\n\n\n\n모든 열의 상단에 [,1][,2]… 형식의 인덱스가 있음에 주목하세요. 이들은 열 인덱스입니다. 행은 [1,][2,]…로 라벨링되어 있습니다. obsunder0 객체는 이전에 보았던 단순한 벡터가 아니라 행렬 형태의 숫자 배열입니다.\n\n\n모든 열의 상단에 [,1][,2]... 형식의 인덱스가 있음에 주목하세요. 이들은 열 인덱스입니다. 행은 [1,][2,]...로 라벨링되어 있습니다. obsunder0 객체는 이전에 보았던 단순한 벡터가 아니라 행렬 형태의 숫자 배열입니다.\n행렬 obsunder0의 각 열은 하나의 시뮬레이션된 인스턴스입니다. 상자 안의 숫자들이 많이 변하는 것을 볼 수 있습니다: 기대값은 5=20/4이지만 일부는 8만큼 큽니다.\n\n3.3.1.1 검정 만들기\n기억하세요: 우리는 이 값들이 공정한 과정에서 온 것임을 알고 있습니다. 분명히 프로세스의 기대값을 아는 것만으로는 충분하지 않습니다. 우리는 또한 얼마나 많은 가변성이 예상되는지, 그리고 어느 정도가 너무 큰지를 설명할 수 있는 가변성 척도가 필요합니다. 우리는 다음 통계량을 우리의 척도로 사용합니다. 이는 관측값과 기대값 사이의 차이의 제곱을 기대값으로 나눈 값들의 합으로 계산됩니다. 따라서 각 인스턴스에 대해,\n이 척도는 각 제곱 잔차(residuals)에 기대값에 상대적인 가중치를 부여합니다.\n[ {}=+ + + =_i ]\n처음 세 열의 생성된 데이터는 우리가 기대하는 것과 얼마나 다를까요? 다음과 같이 얻습니다:\nexpected0 = pvec * 20\nsum((obsunder0[, 1] - expected0)^2 / expected0)\n\n\n[1] 1.2\n\n\nsum((obsunder0[, 2] - expected0)^2 / expected0)\n\n\n[1] 1.2\n\n\nsum((obsunder0[, 3] - expected0)^2 / expected0)\n\n\n[1] 1.2\n척도값은 다를 수 있습니다 – 3개 이상의 열을 살펴볼 수 있으며, 1,000개 열 전체를 연구하는 방법을 알아볼 것입니다. 반복적인 입력을 피하기 위해, stat 공식(식 1.1)을 함수로 캡슐화합니다:\nstat = function(obsvd, exptd = 20 * pvec) {\n  sum((obsvd - exptd)^2 / exptd)\n}\nstat(obsunder0[, 1])\n\n\n[1] 1.2\n이 변동에 대한 더 완전한 그림을 얻기 위해, 우리는 1000개 인스턴스 모두에 대해 척도값을 계산하고 이 값들을 S0라고 부르는 벡터에 저장합니다: 여기에는 \\(H_0\\)하에서 생성된 값들이 포함됩니다. 우리는 그림 1.10에 표시된 S0 값들의 히스토그램을 우리 귀무 분포(null distribution)의 추정치로 간주할 수 있습니다.\nS0 = apply(obsunder0, 2, stat)\nsummary(S0)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   1.200   2.800   3.126   4.400  17.600 \n\n\nhist(S0, breaks = 25, col = \"lavender\", main = \"\")\n\n그림 1.10: 귀무(공정한) 분포하에서의 통계량 stat의 시뮬레이션된 값 S0의 히스토그램은 통계량 stat의 표집 분포(sampling distribution)에 대한 근사치를 제공합니다.\n\n\n\napply 함수는 배열의 행이나 열에 대한 루프의 축약형입니다. 여기서 두 번째 인수 2는 열에 대해 루프를 돌 것을 나타냅니다.\n\n\napply 함수는 배열의 행이나 열에 대한 루프의 축약형입니다. 여기서 두 번째 인수 2는 열에 대해 루프를 돌 것을 나타냅니다.\n요약 함수는 S0가 다양한 값의 범위를 가진다는 것을 보여줍니다. 시뮬레이션된 데이터로부터 우리는 예를 들어 95% 분위수(가장 큰 5%의 값과 작은 95%의 값을 구분하는 값)를 근사할 수 있습니다.\nq95 = quantile(S0, probs = 0.95)\nq95\n\n\n95% \n7.6 \n따라서 우리는 S0 값의 5%가 7.6보다 크다는 것을 알 수 있습니다. 우리는 이를 데이터 검정을 위한 임계값으로 제안할 것이며, 만약 가중 제곱합 stat이 7.6보다 크다면 뉴클레오타이드가 동일한 확률로 발생하는 공정한 과정에서 데이터가 왔다는 가설을 기각할 것입니다.\n\n\n3.3.1.2 검정력 결정하기\n우리는 가중 제곱합 차이에 기반한 우리 검정이, 데이터가 실제로 귀무 가설로부터 오지 않았을 때 이를 감지해낼 확률을 계산해야 합니다. 우리는 시뮬레이션을 통해 기각 확률을 계산합니다. pvecA로 매개변수화된 대립 과정(alternative process)으로부터 1000번의 시뮬레이션 인스턴스를 생성합니다.\n\npvecA = c(3/8, 1/4, 1/4, 1/8)\nobserved = rmultinom(1000, prob = pvecA, size = 20)\ndim(observed)\n\n\n[1]    4 1000\n\n\nobserved[, 1:7]\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]   10    4    8    8    4    7    7\n[2,]    3   10    5    6    6    7    2\n[3,]    5    3    5    6    4    2    6\n[4,]    2    3    2    0    6    4    5\n\n\napply(observed, 1, mean)\n\n\n[1] 7.469 4.974 5.085 2.472\n\n\nexpectedA = pvecA * 20\nexpectedA\n\n\n[1] 7.5 5.0 5.0 2.5\n귀무 가설로부터의 시뮬레이션과 마찬가지로, 관측값들은 상당히 다양합니다. 질문은: 우리 검정이 (1000번의 인스턴스 중) 얼마나 자주 데이터가 귀무 가설로부터 벗어났음을 감지해낼 것인가 하는 것입니다.\n통계량 값이 95번째 백분위수 내에 있기 때문에 검정은 첫 번째 관측치 (10, 3, 5, 2)를 기각하지 않습니다.\nstat(observed[, 1])\n\n\n[1] 7.6\n\n\nS1 = apply(observed, 2, stat)\nq95\n\n\n95% \n7.6 \n\n\nsum(S1 &gt; q95)\n\n\n[1] 199\n\n\npower = mean(S1 &gt; q95)\npower\n\n\n[1] 0.199\n\n\n\n우리는 수직 막대를 ~이 주어졌을 때 또는 ~에 대한 조건부로 읽습니다.\n\n\n우리는 수직 막대를 ~이 주어졌을 때 또는 조건부로 읽습니다.\n1000번의 시뮬레이션에 걸쳐 실행했을 때, 검정은 199개를 대립 분포로부터 온 것으로 식별했습니다. 따라서 우리는 확률 \\(P(H_0 ;|; H_A)\\)가 0.199임을 계산했습니다.\n시퀀스 길이 \\(n = 20\\)을 사용하면 공정한 생성 과정과 우리 대립 가설 사이의 차이를 감지할 수 있는 약 20%의 _검정력_을 가집니다.\n태스크\n실제로 앞서 언급했듯이, 수용 가능한 검정력 값은 0.8 이상입니다. 시뮬레이션 실험을 반복하고 검정력이 수용 가능한 수준이 되도록 보장하는 새로운 시퀀스 길이 \\(n\\)을 제안하세요.\n\n\n3.3.1.3 고전적 데이터에 대한 고전적 통계\n95번째 백분위수를 계산하기 위해 몬테카를로를 사용하여 데이터를 시뮬레이션할 필요는 없었습니다; 계산을 돕는 적절한 이론이 존재합니다.\n우리의 통계량 stat은 사실 (자유도가 3인) 카이제곱(chi-square) 분포라 불리는 잘 알려진 분포를 따르며 \\({}^2_3\\)로 쓰여집니다.\n\n그림 1.11: 우리는 확률 모델이 \\(F\\)라고 부르는 분포를 어떻게 갖는지 공부했습니다. \\(F\\)는 관례적으로 \\(\\)와 같은 그리스 문자로 표시되는 매개변수들에 의존하는 경우가 많습니다. 관측된 데이터는 갈색 화살표를 통해 생성되며 \\(x\\)와 같은 라틴 문자로 표시됩니다. 확률 계산에서 수직 막대는 ~라고 가정할 때 또는 조건부를 나타냅니다.\n우리는 2장에서 Q-Q 플롯을 사용하여 분포를 비교하는 방법을 살펴볼 것입니다 (그림 2.8 참조). 직접 만든 시뮬레이션을 실행하는 대신 더 표준적인 검정을 사용할 수도 있었습니다. 하지만 우리가 배운 절차는 카이제곱 분포가 적용되지 않는 많은 상황으로 확장됩니다. 예를 들어, 일부 상자가 매우 낮은 확률을 가지고 카운트가 대부분 0인 경우입니다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>이산 데이터를 위한 생성 모델</span>"
    ]
  },
  {
    "objectID": "01-chap.html#이-장의-요약",
    "href": "01-chap.html#이-장의-요약",
    "title": "3  이산 데이터를 위한 생성 모델",
    "section": "3.4 1.5 이 장의 요약",
    "text": "3.4 1.5 이 장의 요약\n우리는 몇 가지 기본 분포를 사용하여 다양한 이산 사건(events)의 확률을 계산하기 위해 수학 공식과 R을 사용했습니다:\n베르누이(Bernoulli) 분포는 우리의 가장 기본적인 구성 요소였습니다 – 이는 동전 던지기와 같은 단일 이진 시행을 나타내는 데 사용됩니다. 우리는 결과를 0과 1로 코딩할 수 있습니다. 우리는 \\(p\\)를 성공 확률(결과 1)이라고 부릅니다.\n이항(binomial) 분포는 \\(n\\)번의 이진 시행에서의 1의 개수에 사용되며, R 함수 dbinom을 사용하여 \\(k\\)번 성공할 확률을 계산할 수 있습니다. 또한 rbinom 함수를 사용하여 \\(n\\)번 시행하는 이항 실험을 시뮬레이션하는 방법도 보았습니다.\n푸아송(Poisson) 분포는 \\(p\\)가 작은 경우(1이 드물게 나타나는 경우)에 가장 적합합니다. 여기에는 오직 하나의 매개변수 \\(\\)만이 있으며, \\(=np\\)인 푸아송 분포는 \\(p\\)가 작을 때 \\((n, p)\\)에 대한 이항 분포와 거의 동일합니다. 우리는 시퀀스를 따라 에피토프를 테스트하는 어세이에서 무작위로 발생하는 위양성(false positives)의 수를 모델링하기 위해 푸아송 분포를 사용했으며, 위치당 위양성률 \\(p\\)가 작다고 가정했습니다. 우리는 모든 매개변수를 알고 있는 한, 그러한 매개변수 모델이 어떻게 극단적인 사건의 확률을 계산할 수 있게 해주는지 보았습니다.\n다항(multinomial) 분포는 세 가지 이상의 가능한 결과 또는 수준(levels)을 가진 이산 사건에 사용됩니다. 검정력 예제는 만약 우리가 다항 모델이 데이터와 일치하는지 테스트하고 싶다면, 얼마나 많은 데이터를 수집해야 할지 결정하기 위해 몬테카를로 시뮬레이션을 어떻게 사용할 수 있는지 보여주었습니다. 우리는 생성 모델에 대한 가정을 세움으로써 우리 데이터가 어떻게 생성되었는지에 대한 가설을 평가하기 위해 확률 분포와 확률 모델을 사용했습니다. 가설이 주어졌을 때 데이터를 볼 확률을 p-값(p-value)이라고 부릅니다. 이것은 가설이 참일 확률과는 다릅니다!\n\n\n\nP(H_0 | data)는 p-값 P(data | H_0)와 같지 않습니다.\n\n\n\\(P(H_0;|;)\\)는 p-값 \\(P(;|;H_0)\\)과 같지 않습니다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>이산 데이터를 위한 생성 모델</span>"
    ]
  },
  {
    "objectID": "01-chap.html#더-읽을거리",
    "href": "01-chap.html#더-읽을거리",
    "title": "3  이산 데이터를 위한 생성 모델",
    "section": "3.5 1.6 더 읽을거리",
    "text": "3.5 1.6 더 읽을거리\n\nFreedman, Pisani, Purves (1997)의 입문서는 여기서 언급하는 상자 모델 유형을 통해 확률에 대한 최고의 소개를 제공합니다.\nDurbin 등 (1998)의 저서는 많은 유용한 확률 분포를 다루고 있으며 부록에서 확률 이론의 이론적 배경과 생물학 서열에 대한 적용에 대한 보다 완전한 견해를 제공합니다.\n몬테카를로 방법은 현대 통계학에서 광범위하게 사용됩니다. Robert와 Casella (2009)는 R을 사용한 이러한 방법의 입문서를 제공합니다.\n6장에서는 가설 검정 주제를 다룰 것입니다. 또한 데이터 분석에서 자주 사용하는 더 발전된 확률 분포인 베타, 감마, 지수 분포 유형에 유용한 심화 자료로 Rice (2006)를 추천합니다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>이산 데이터를 위한 생성 모델</span>"
    ]
  },
  {
    "objectID": "01-chap.html#연습-문제",
    "href": "01-chap.html#연습-문제",
    "title": "3  이산 데이터를 위한 생성 모델",
    "section": "3.6 1.7 연습 문제",
    "text": "3.6 1.7 연습 문제\n연습 문제 1.1\nR은 알려진 모든 분포로부터 숫자를 생성할 수 있습니다. 우리는 이제 각 분포 유형에 맞춰진 특수한 R 함수들을 사용하여 임의의 이산 데이터를 생성하는 법을 압니다. 우리는 rpois, rbinom, rmultinom에서와 같이 r로 시작하는 함수들을 사용합니다. 만약 이러한 모델들 중 하나하에서 확률에 대한 이론적 계산이 필요하다면, 이산 이항 분포에서의 사건 확률을 계산하는 dbinom이나 연속 정규 분포에 대한 확률 밀도 함수를 계산하는 dnorm과 같이 d로 시작하는 함수들을 사용합니다. \\(P(X&gt;a)\\)와 같은 꼬리 확률을 계산할 때는 p로 시작하는 누적 분포 함수를 사용하는 것이 편리합니다. 위의 XXXX 부분을 대체할 수 있는 다른 이산 분포 두 가지를 찾아보세요.\n연습 문제 1.2\n이 장에서 우리는 확률이 셀 수 있는 값들의 세트에 집중되어 있는 이산 확률 변수에 집중했습니다. dbinom을 사용하여 이항 분포 \\(B(10, 0.3)\\)에 대해 \\(X=2\\)인 값에서의 확률 질량(probability mass)을 어떻게 계산하시겠습니까? dbinom을 사용하여 \\(P(X)\\)에 해당하는 값 2에서의 누적(cumulative) 분포를 계산하고, 다른 R 함수로 답을 확인해 보세요.\n해결책\ndbinom(2, size = 10, prob = 0.3)\n\n\n[1] 0.2334744\n\n\npbinom(2, size = 10, prob = 0.3)\n\n\n[1] 0.3827828\n\n\nsum(dbinom(0:2, size = 10, prob = 0.3))\n\n\n[1] 0.3827828\n연습 문제 1.3\n어떤 명령어 시퀀스가 계속 필요하다는 것을 알게 될 때마다, 이를 함수로 만드는 것이 좋습니다. 함수 본문(body)은 우리가 반복해서 수행하고 싶은 지시사항들을 포함하고, 함수 인수(arguments)는 우리가 변화시키고 싶은 것들을 받습니다. 비율이 lambda인 n개의 푸아송 변수들을 살펴볼 때 최댓값이 m만큼 클 확률을 계산하는 함수를 작성하세요.\n해결책\npoismax = function(lambda, n, m) {\n  epsilon = 1 - ppois(m - 1, lambda)\n  1 - exp( -n * epsilon)\n}\npoismax(lambda = 0.5, n = 100, m = 7)\n\n\n[1] 0.0001002329\n\n\npoismax(lambda = mean(e100), n = 100, m = 7)\n\n\n[1] 0.0001870183\n연습 문제 1.4\n인수에 대한 기본값(즉, 함수 호출 시 인수가 지정되지 않았을 때 사용되는 값)을 갖도록 함수를 다시 작성하세요.\n해결책\npoismax = function(lambda, n = 100, m = 7) {\n  1 - exp( -n * (1 - ppois(m - 1, lambda)))\n}\npoismax(0.5)\n\n\n[1] 0.0001002329\n\n\npoismax(0.5, m = 9)\n\n\n[1] 3.43549e-07\n연습 문제 1.5\n에피토프 예제에서, 시뮬레이션을 사용하여 100번의 시행 중 최댓값이 9 이상일 확률을 찾으세요. “확률이 0.000001보다 작다”는 것을 증명하려면 몇 번의 시뮬레이션이 필요할까요?\n연습 문제 1.6\nR에서 ?Distributions를 사용하여 가용한 분포 리스트를 확인해 보세요17. 다양한 분포에 대한 확률 질량 또는 밀도 함수 플롯을 만들고(dXXXX 함수 사용), 이산형이 아닌 분포 다섯 가지를 나열하세요.\n17 이들은 단지 기본 R 설치 시 함께 제공되는 것들입니다. 추가 패키지들에는 더 많이 있으며, CRAN 태스크 뷰: 확률 분포를 참조하세요.\n연습 문제 1.7\nPoisson(3) 확률 변수의 인스턴스 100개를 생성하세요. 평균은 얼마인가요? R 함수 var로 계산된 분산은 얼마인가요?\n연습 문제 1.8\nC. elegans 유전체 뉴클레오타이드 빈도: _C. elegans_의 미토콘드리아 서열이 뉴클레오타이드가 동일한 확률을 갖는다는 모델과 일치하나요?\n\n바이오컨덕터의 Biostrings 패키지에 있는 전 전용 함수를 사용하여 염색체 M의 뉴클레오타이드 빈도를 탐색하세요.\n시뮬레이션을 사용하여 C. elegans 데이터가 균등 모델(모든 뉴클레오타이드 빈도가 동일함)과 일치하는지 테스트하세요. 힌트: 이번이 바이오컨덕터를 처음으로 사용해 볼 기회입니다. 바이오컨덕터의 패키지 관리는 CRAN보다 더 엄격하게 통제되므로, 바이오컨덕터 패키지를 설치하려면 (BiocManager 패키지의) 특별한 install 함수를 사용해야 합니다.\n\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(c(\"Biostrings\", \"BSgenome.Celegans.UCSC.ce2\"))\n그 후에, 다른 R 패키지들을 로드하는 것과 같이 유전체 서열 패키지를 로드할 수 있습니다.\n해결책\nlibrary(\"BSgenome.Celegans.UCSC.ce2\")\nCelegans\n\n\n| BSgenome object for Worm\n| - organism: Caenorhabditis elegans\n| - provider: UCSC\n| - genome: ce2\n| - release date: Mar. 2004\n| - 7 sequence(s):\n|     chrI   chrII  chrIII chrIV  chrV   chrX   chrM                       \n| \n| Tips: call 'seqnames()' on the object to get all the sequence names, call\n| 'seqinfo()' to get the full sequence info, use the '$' or '[[' operator to\n| access a given sequence, see '?BSgenome' for more information.\n\n\nseqnames(Celegans)\n\n\n[1] \"chrI\"   \"chrII\"  \"chrIII\" \"chrIV\"  \"chrV\"   \"chrX\"   \"chrM\"  \n\n\nCelegans$chrM\n\n\n13794-letter DNAString object\nseq: CAGTAAATAGTTTAATAAAAATATAGCATTTGGGTT...TATTTATAGATATATACTTTGTATATATCTATATTA\n\n\nclass(Celegans$chrM)\n\n\n[1] \"DNAString\"\nattr(,\"package\")\n[1] \"Biostrings\"\n\n\nlength(Celegans$chrM)\n\n\n[1] 13794\n\n\nlibrary(\"Biostrings\")\nlfM = letterFrequency(Celegans$chrM, letters=c(\"A\", \"C\", \"G\", \"T\"))\nlfM\n\n\n   A    C    G    T \n4335 1225 2055 6179 \n\n\nsum(lfM)\n\n\n[1] 13794\n\n\nlfM / sum(lfM)\n\n\n         A          C          G          T \n0.31426707 0.08880673 0.14897782 0.44794838 \nC. elegans 염색체 M과 동일한 길이를 가진 무작위(각 글자가 동일한 확률을 가짐) 서열을 생성합니다:\nt(rmultinom(1, length(Celegans$chrM), p = rep(1/4, 4)))\n\n\n     [,1] [,2] [,3] [,4]\n[1,] 3409 3486 3476 3423\n기대 빈도는 단지 다음과 같습니다.\nlength(Celegans$chrM) / 4\n\n\n[1] 3448.5\n우리는 두 다항 출력 결과가 서로 얼마나 가까운지 측정하는 통계량을 계산할 것입니다. 관측된(o) 카운트와 기대되는(e) 카운트 사이의 차이의 제곱의 평균을 구하고 이를 e로 스케일링하겠습니다. 이 함수를 oestat이라고 부르겠습니다.\noestat = function(o, e) {\n  sum((o-e)^2 / e)\n}\noe = oestat(o = lfM, e = length(Celegans$chrM) / 4)\noe\n\n\n[1] 4386.634\n이것이 무작위성으로 설명될 수 있는 것보다 더 큰가요? 우리는 이미 위에서 귀무 모델하에서 기대할 수 있는 일련의 전형적인 카운트들을 보았습니다. 하지만 우리에게는 값들의 전체 세트(분포)가 필요합니다. 함수를 여러 번 평가해 주는 replicate 함수를 사용하여 이들을 계산합니다. 우리는 다음을 실행합니다:\nB = 10000\nn = length(Celegans$chrM)\nexpected = rep(n / 4, 4)\noenull = replicate(B,\n  oestat(e = expected, o = rmultinom(1, n, p = rep(1/4, 4))))\nDurbin, Richard, Sean Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis. Cambridge University Press.\nFreedman, David, Robert Pisani, and Roger Purves. 1997. Statistics. New York, NY: WW Norton.\nRice, John. 2006. Mathematical Statistics and Data Analysis. Cengage Learning.\nRobert, Christian, and George Casella. 2009. Introducing Monte Carlo Methods with R. Springer Science & Business Media.\n페이지는 R 버전 4.5.1 (2025-06-13)을 사용하여 2025-09-01 01:33에 빌드되었습니다. ## 1.6 요약\n이 장에서는 다음과 같은 핵심 개념들을 배웠습니다:\n\n이산 확률 변수: 카운트 데이터(돌연변이 수, 리드 수 등)를 모델링하는 데 사용됩니다.\n베르누이 및 이항 분포: 성공/실패와 같은 이진 사건의 횟수를 모델링합니다.\n푸아송 분포: 성공 확률이 매우 낮은 희귀 사건을 모델링하는 데 유용하며, 단일 매개변수 ()에 의존합니다.\n다항 분포: 결과가 세 가지 이상인 범주형 사건의 카운트를 모델링합니다.\nR을 이용한 시뮬레이션: , 등의 함수를 사용하여 확률 모델로부터 데이터를 생성하고 시각화할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>이산 데이터를 위한 생성 모델</span>"
    ]
  },
  {
    "objectID": "01-chap.html#요약-및-핵심-용어",
    "href": "01-chap.html#요약-및-핵심-용어",
    "title": "3  이산 데이터를 위한 생성 모델",
    "section": "3.7 1.6 요약 및 핵심 용어",
    "text": "3.7 1.6 요약 및 핵심 용어\n이 장에서 우리는 이산형 데이터(discrete data)를 모델링하는 데 사용되는 세 가지 중요한 확률 분포를 살펴보았습니다.\n\n베르누이(Bernoulli) 시행: 결과가 두 가지(성공/실패, 앞면/뒷면, 0/1)뿐인 단일 시행입니다.\n이항(Binomial) 분포 ((B(n, p))): (n)번의 독립적인 베르누이 시행에서 성공 확률이 (p)일 때, 총 성공 횟수 (k)의 확률 분포입니다.\n\n공식: (P(X=k) = {n k} p^k (1-p)^{n-k})\n평균: (np), 분산: (np(1-p))\n\n푸아송(Poisson) 분포 ((P())): 희귀한 사건이 발생하는 횟수를 모델링합니다. 이항 분포에서 (n)이 매우 크고 (p)가 매우 작을 때((np )) 좋은 근사치가 됩니다.\n\n공식: (P(X=k) = )\n평균: (), 분산: ()\n\n다항(Multinomial) 분포: 결과가 두 가지 이상(예: DNA의 A, C, G, T)인 시행을 (n)번 반복했을 때 각 결과의 카운트 분포입니다.\n\n이러한 생성 모델(generative models)을 사용하면 데이터를 시뮬레이션하고, 관측된 데이터가 우리가 가정한 모델(무작위성)과 일치하는지 통계적으로 평가할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>이산 데이터를 위한 생성 모델</span>"
    ]
  },
  {
    "objectID": "02-chap.html",
    "href": "02-chap.html",
    "title": "4  매개변수가 핵심입니다.",
    "section": "",
    "text": "4.1 2.2 통계 모델과 확률 모델의 차이점\n이전 장에서 생성 모델과 매개변수 값에 대한 지식은 우리가 의사 결정을 내리는 데 사용할 수 있는 확률을 제공했습니다 – 예를 들어, 우리가 정말로 에피토프(epitope)를 찾았는지 여부와 같은 것들입니다. 많은 실제 상황에서 생성 모델이나 매개변수 중 어느 것도 알려져 있지 않으며, 우리는 수집한 데이터를 사용하여 이를 추정해야 할 것입니다. 통계 모델링은 데이터를 바탕으로 그 데이터를 그럴듯하게 설명할 수 있을지도 모르는 모델을 향해 상향식(upwards)으로 작동합니다1. 이러한 상향 추론 단계를 통계적 추론(inference)이라고 합니다. 이 장에서는 추론의 구성 요소 역할을 하는 몇 가지 분포와 추정 메커니즘을 살펴볼 것입니다. 이 장의 예제들은 모두 모수적(parametric)이지만(즉, 통계 모델이 소수의 알려지지 않은 매개변수만을 가짐), 우리가 논의하는 원칙들은 일반화될 것입니다.\n1 현재의 모든 데이터를 완벽하게 설명하는 모델을 찾았더라도, 현실은 항상 더 복잡할 수 있습니다. 새로운 데이터 세트는 또 다른 모델이 필요하다는 결론을 내리게 할 수 있으며, 이는 현재 모델을 특수한 사례나 근사치로 포함할 수도 있습니다.\n통계적 환경에서, 우리는 데이터 (X)로부터 시작하여 이를 사용하여 매개변수들을 _추정_합니다. 이러한 추정치들은 ()와 같이 소위 ’햇(hats)’을 씌운 그리스 문자로 표시됩니다.\n이 장에서 우리는 다음을 수행할 것입니다:\n매개변수의 예시 : 단일 매개변수 ()는 푸아송 분포를 정의합니다. 그리스 문자 ()는 흔히 정규 분포의 평균을 나타내는 데 사용됩니다. 더 일반적으로, 우리는 확률 모델을 지정하는 데 필요한 매개변수들의 일반적인 튜플(tuple)을 나타내기 위해 그리스 문자 ()를 사용합니다. 예를 들어, 이항 분포의 경우 (=(n,p))는 양의 정수와 0과 1 사이의 실수라는 두 개의 숫자로 구성됩니다.\n우리는 1장에서 에피토프 예제의 모든 매개변수 값을 알고 있을 때 확률 모델을 사용하여 손에 든 데이터를 바탕으로 귀무 가설을 테스트할 수 있음을 보았습니다. 우리는 몇 가지 실제 예제와 컴퓨터 시뮬레이션을 통해 통계 모델링에 대한 서로 다른 접근 방식들을 살펴보겠지만, 우선 가용한 정보의 양에 따른 두 가지 상황을 구분하는 것부터 시작해 봅시다.\n확률적 분석은 데이터의 무작위성에 대한 좋은 생성 모델을 알고 있고 , 그 매개변수들의 실제 값을 알고 있을 때 가능합니다.\n그림 2.1: 1장에서 얻은 확률 모델. 데이터는 녹색 (x)로 표시됩니다. 만약 ()의 참값을 안다면, (x)의 모든 가능한 인스턴스에 대해, 특히 우리가 관찰한 (x)에 대해 (x)를 관찰할 확률을 계산할 수 있습니다.\n에피토프 예제에서, 위양성(false positives)이 위치당 Bernoulli(0.01)로 발생한다는 것과 분석된 환자 샘플의 수, 그리고 단백질의 길이를 안다는 것은 _알려지지 않은 매개변수가 없음_을 의미했습니다.\n그러한 경우, 우리는 그림 2.1에 도식화된 것과 같이 사건의 확률을 계산하기 위해 수학적 연역(deduction)을 사용할 수 있습니다. 에피토프 예제에서 우리는 주어진 매개변수 ()를 사용하여 푸아송 확률을 귀무 모델(null model)로 사용했습니다. 우리는 수학적 연역을 통해 7 이상의 최댓값을 볼 확률이 약 (10^{-4})임을 알아낼 수 있었고, 따라서 실제로 관찰된 데이터가 그 모델(또는 “귀무 가설”)하에서는 발생할 가능성이 매우 낮다는 결론을 내릴 수 있었습니다.\n이제 환자 수와 단백질 길이는 알고 있지만(이들은 실험 설계에 의해 주어짐), 분포 자체와 위양성률은 모른다고 가정해 봅시다. 일단 데이터를 관찰하고 나면, 확률 모델 (F)(푸아송, 정규, 이항)와 결과적으로 그 모델에 대해 누락된 매개변수(들)를 추정하기 위해 데이터로부터 상향(up)으로 나아가야 합니다. 이것이 우리가 이 장에서 설명할 통계적 추론(inference)의 유형입니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>매개변수가 핵심입니다.</span>"
    ]
  },
  {
    "objectID": "02-chap.html#통계-모델링의-간단한-예제",
    "href": "02-chap.html#통계-모델링의-간단한-예제",
    "title": "4  매개변수가 핵심입니다.",
    "section": "4.2 2.3 통계 모델링의 간단한 예제",
    "text": "4.2 2.3 통계 모델링의 간단한 예제\n\n4.2.0.1 데이터로부터 시작하기\n모델링 절차에는 두 가지 부분이 있습니다. 먼저 데이터 생성 과정을 모델링하기 위한 합리적인 확률 분포 가 필요합니다. 1장에서 보았듯이, 이산형 카운트 데이터는 이항, 다항 또는 푸아송 분포와 같은 간단한 확률 분포로 모델링될 수 있습니다. 정규 분포, 즉 종 모양 곡선은 연속형 측정값에 대한 좋은 모델인 경우가 많습니다. 분포는 또한 이러한 기초적인 분포들의 더 복잡한 혼합물일 수도 있습니다(4장에서 더 자세히 다룹니다).\n이전 장의 에피토프 예제를 다시 살펴보되, 우선 까다로운 이상치(outlier) 없이 시작해 봅시다.\nload(\"../data/e100.RData\")\ne99 = e100[-which.max(e100)]\n\n\n4.2.0.2 적합도(Goodness-of-fit) : 시각적 평가\n우리의 첫 번째 단계는 후보 분포들 중에서 적합한 것을 찾는 것입니다; 이를 위해 시각적 및 정량적 적합도 플롯을 참고해야 합니다. 이산형 데이터의 경우, 그림 2.2에서와 같이 빈도의 막대 그래프를 그릴 수 있습니다 (연속형 데이터의 경우에는 히스토그램을 살펴볼 것입니다).\nbarplot(table(e99), space = 0.8, col = \"chartreuse4\")\n\n그림 2.2: 이상치를 제외한 에피토프 데이터의 관찰된 분포.\n하지만 비교 대조군 없이 어떤 이론적 분포가 데이터에 가장 잘 맞는지 결정하기는 어렵습니다. 한 가지 시각적 적합도 다이어그램은 루토그램(rootogram) (Cleveland 1988)으로 알려져 있습니다; 이는 관측된 카운트가 있는 막대들을 이론적인 빨간색 점들로부터 매달아 놓습니다. 만약 카운트가 이론적 값과 정확히 일치한다면, 상자들의 바닥이 가로축과 정확히 정렬될 것입니다.\nlibrary(\"vcd\")\ngf1 = goodfit( e99, \"poisson\")\nrootogram(gf1, xlab = \"\", rect_gp = gpar(fill = \"chartreuse4\"))\n\n그림 2.3: 이론적 값의 제곱근을 빨간색 점으로 표시하고 관측 빈도의 제곱근을 아래로 내려온 직사각형으로 표시한 루토그램. (goodfit 함수가 어떤 ()를 사용하기로 결정했는지 아래에서 좀 더 살펴보겠습니다.)\n질문 2.1\n알려진 푸아송 변수를 사용하여 그러한 플롯이 어떻게 보이는지 확인하기 위해, () = 0.05인 rpois를 사용하여 100개의 푸아송 분포 숫자를 생성하고 그 루토그램을 그려보세요.\n해결책\nsimp = rpois(100, lambda = 0.05)\ngf2 = goodfit(simp, \"poisson\")\nrootogram(gf2, xlab = \"\")\n우리는 e99에 대한 루토그램이 푸아송 모델에 상당히 잘 맞는 것으로 보임을 알 수 있습니다. 하지만 이를 위해 우리가 이상치를 제거했다는 점을 기억하세요. 푸아송 분포는 흔히 푸아송 평균이라 불리는 하나의 매개변수 ()에 의해 완전히 결정됩니다. 데이터가 푸아송 분포를 따른다고 추측할 수 있는 대부분의 경우, 우리는 데이터로부터 푸아송 매개변수를 추정해야 할 것입니다.\n\n\n\n이 매개변수는 이론적 분포의 평균이기 때문에 푸아송 평균이라고 불리며, 결과적으로 표본 평균에 의해 추정됩니다. 이 단어의 중복 사용은 모든 사람에게 혼란을 줍니다.\n\n\n이 매개변수는 이론적 분포의 평균이기 때문에 푸아송 평균이라고 불리며, 결과적으로 표본 평균에 의해 추정됩니다. 이 단어의 중복 사용은 모든 사람에게 혼란을 줍니다.\n()를 추정하는 가장 일반적인 방법은 관찰된 데이터를 가장 발생 가능하게 만드는 값 ()를 선택하는 것입니다. 이를 최대 우도 추정량(maximum likelihood estimator) (Rice 2006, chap. 8, 5절)이라고 하며, 흔히 MLE로 축약합니다. 우리는 다음 섹션에서 다소 역설적인 이 아이디어를 설명할 것입니다.\n비록 우리가 확률 분포를 추측하기 전에 극단적인 관측값을 제외하긴 했지만, 나머지 분석을 위해 해당 데이터를 다시 포함시킬 것입니다. 실제로는 이상치가 존재하는지, 그리고 어떤 데이터 포인트가 이상치인지 알지 못할 것입니다. 이를 남겨두는 효과는 평균에 대한 우리 추정치를 더 높게 만드는 것입니다. 이는 결과적으로 귀무 모델하에서 7이라는 값을 관찰할 가능성을 더 높게 만들고, 그 결과 더 큰 p-값을 내놓게 됩니다. 따라서 이상치를 포함하더라도 결과 p-값이 작다면, 우리는 우리의 분석이 실재하는 무언가를 다루고 있다고 확신할 수 있습니다. 우리는 이러한 전술을 보수적(conservative)이라고 부릅니다: 우리는 무언가를 탐지하지 못하는 쪽으로, 즉 조심하는 쪽으로 실수를 범하는 것입니다.\n\n\n4.2.0.3 푸아송 분포의 매개변수 추정하기\n푸아송 평균의 어떤 값이 데이터를 가장 발생 가능하게 만들까요? 첫 번째 단계로, 결과들을 집계합니다.\ntable(e100)\n\n\ne100\n 0  1  2  7 \n58 34  7  1 \n그런 다음 푸아송 평균에 대해 여러 가지 서로 다른 값들을 시도해 보고 우리 데이터에 가장 잘 맞는 것이 무엇인지 살펴볼 것입니다. 만약 푸아송 분포의 평균 ()가 3이라면, 카운트는 다음과 같을 것입니다:\ntable(rpois(100, 3))\n\n\n 0  1  2  3  4  5  6  7 \n 4 12 23 24 14 16  4  3 \n이는 우리 데이터에서 보는 것보다 훨씬 더 많은 2와 3을 포함하고 있습니다. 따라서 카운트가 잘 일치하지 않으므로 ()이 우리 데이터를 생성했을 가능성은 낮다는 것을 알 수 있습니다.\n질문 2.2\n()의 서로 다른 값들을 사용하여 이 시뮬레이션을 반복해 보세요. 단순히 시행착오를 통해 관찰된 값에 가까운 카운트를 주는 값을 찾을 수 있나요?\n그래서 우리는 많은 가능한 값들을 시도해 보고 무차별 대입(brute force)으로 진행할 수 있습니다. 하지만 우리는 좀 더 우아한 작업을 수행하여, 어떤 값이 우리 데이터를 관찰할 확률을 최대화하는지 확인하기 위해 약간의 수학을 사용할 것입니다. 푸아송 매개변수의 값이 (m)일 때 데이터를 볼 확률을 계산해 봅시다. 데이터가 독립적인 추출로부터 유도되었다고 가정하므로, 이 확률은 단순히 개별 확률들의 곱입니다:\n[ P(58 , 34 , 7 , 7 ;|; (m)) = P(0){58}P(1){34}P(2){7}P(7){1}. ]\n(m=3)에 대해 이를 계산할 수 있습니다2.\n2 여기서 R의 벡터화를 어떻게 사용하는지 주목하세요: dpois 호출은 네 가지 서로 다른 숫자에 해당하는 네 개의 값을 반환합니다. 그런 다음 ^ 연산자를 사용하여 이들을 각각 58, 34, 7, 1의 거듭제곱으로 취하며, 다시 네 개의 값을 얻습니다. 마지막으로 prod 함수를 사용하여 이들을 하나의 숫자인 곱으로 합칩니다.\nprod(dpois(c(0, 1, 2, 7), lambda = 3) ^ (c(58, 34, 7, 1)))\n\n\n[1] 1.392143e-110\n질문 2.3\n위와 같이 (m=0, 1, 2)에 대해 확률을 계산하세요. (m)은 반드시 정수여야 하나요? 예를 들어 (m=0.4)에 대한 확률을 계산해 보세요.\n해결책\nprod(dpois(c(0, 1, 2, 7), lambda = 0.4) ^ (c(58, 34, 7, 1)))\n\n\n[1] 8.5483e-46\n이 확률은 주어진 데이터에 대한 ()의 우도 함수(likelihood function)이며, 다음과 같이 씁니다.\n여기서 (L)은 우도를 나타내고 (f(k)=e^{-} ,^k,/,k!)는 우리가 앞서 본 푸아송 확률입니다.\n[ L(,,x=(k_1,k_2,k_3,…))=_{i=1}^{100}f(k_i) ]\n백 개의 작은 숫자들을 곱하는 대신, 로그를 취하는 것이 편리합니다3. 로그 함수는 엄격하게 증가하므로, 만약 특정 구간 내에서 로그가 최댓값에 도달하는 지점이 있다면 그 지점은 확률에 대해서도 최댓값이 될 것입니다.\n3 이는 대개 종이와 연필 계산뿐만 아니라 컴퓨터 계산에서도 마찬가지입니다.\n4 여기서 우리는 데이터 포인트들에 대한 명시적인 루프 없이 계산을 작성할 수 있게 해주는 R의 벡터 구문을 다시 사용합니다. 위의 코드와 비교할 때, 여기서는 고유한 값들에 대해서만 dpois를 호출하기 위해 table 함수로 data를 집계하는 대신, 100개 데이터 포인트 각각에 대해 dpois를 호출합니다. 이는 결과는 동일하지만 코드를 읽기가 얼마나 쉬운지 또는 실행 시간이 얼마나 걸리는지가 다를 수 있는 대안적인 해결책들의 간단한 예시입니다.\n계산적인 설명부터 시작해 봅시다. 우리는 푸아송 매개변수의 많은 서로 다른 값들에 대해 우도를 계산합니다. 이를 위해 서로 다른 값들에 대한 데이터의 확률을 계산하는 작은 함수를 작성해야 합니다4.\nloglikelihood  =  function(lambda, data = e100) {\n  sum(log(dpois(data, lambda)))\n}\n이제 0.05부터 0.95까지 일련의 lambda 값들에 대해 우도를 계산할 수 있습니다 (그림 2.4).\nlambdas = seq(0.05, 0.95, length = 100)\nloglik = vapply(lambdas, loglikelihood, numeric(1))\nplot(lambdas, loglik, type = \"l\", col = \"red\", ylab = \"\", lwd = 2,\n     xlab = expression(lambda))\nm0 = mean(e100)\nabline(v = m0, col = \"blue\", lwd = 2)\nabline(h = loglikelihood(m0), col = \"purple\", lwd = 2)\nm0\n\n\n[1] 0.55\n\n그림 2.4: 빨간색 곡선은 로그 우도 함수입니다. 수직선은 m(평균) 값을 보여주고 가로선은 m의 로그 우도를 보여줍니다. m이 우도를 최대화하는 것으로 보입니다.\n질문 2.4\n위의 코드에서 vapply 함수는 무엇을 하나요? 힌트: 매뉴얼 페이지를 확인하세요.\n해결책\nvapply는 첫 번째 인수(이 경우 벡터 lambdas)를 가져와서, 각 벡터 요소에 대해 반복적으로 loglikelihood 함수(두 번째 인수)를 적용합니다. 결과적으로 결과값들의 벡터를 반환합니다. 이 함수는 또한 loglikelihood에 대한 각 개별 호출이 어떤 유형의 값을 반환할 것으로 기대되는지를 지정하는 세 번째 인수(이 경우 numeric(1))를 필요로 합니다: 즉, 단일 숫자입니다. (일반적으로 함수가 때때로 문자열이나 두 개의 숫자와 같은 다른 것을 반환할 수도 있는데, 그럴 경우 전체 결과들을 일관된 하나의 벡터로 조립할 수 없으며 vapply가 오류를 발생시킵니다.)\n사실 goodfit이라는 단축 함수가 있습니다.\ngf  =  goodfit(e100, \"poisson\")\nnames(gf)\n\n\n[1] \"observed\" \"count\"    \"fitted\"   \"type\"     \"method\"   \"df\"       \"par\"     \n\n\ngf$par\n\n\n$lambda\n[1] 0.55\ngoodfit의 출력은 리스트(list)라 불리는 복합 객체입니다. 그 구성 요소 중 하나는 par라고 불리며, 연구된 분포에 대해 적합된 매개변수(들)의 값을 포함합니다. 이 경우에는 ()에 대한 추정치인 단 하나의 숫자뿐입니다.\n질문 2.5\ngoodfit 함수의 출력에 있는 다른 구성 요소들은 무엇인가요?\n태스크\nm의 값을 이전에 ()로 사용했던 값인 0.5와 비교해 보세요. 0.5 대신 m을 사용하여 1장에서 했던 모델링을 다시 수행해 보세요.\n\n\n4.2.1 2.3.1 고전적 데이터에 대한 고전적 통계\n표본 평균이 (로그) 우도를 최대화한다는 우리 계산 결과에 대한 공식적인 증명은 다음과 같습니다.\n[ \\[\\begin{align} \\log L(\\lambda, x) &= \\sum_{i=1}^{100} - \\lambda + k_i\\log\\lambda - \\log(k_i!) \\\\ &= -100\\lambda + \\log\\lambda\\left(\\sum_{i=1}^{100}k_i\\right) + \\text{const.} \\end{align}\\] ]\n우리는 ()에 의존하지 않는 항들(비록 (x), 즉 (k_i)에는 의존할지라도)에 대해 포괄적으로 “const.”를 사용합니다. 이를 최대화하는 ()를 찾기 위해, ()에 대해 미분을 수행하고 이를 0으로 둡니다.\n[ \\[\\begin{align} \\frac{d}{d\\lambda}\\log L &= -100 + \\frac{1}{\\lambda} \\sum_{i=1}^{100}k_i \\stackrel{?}{=}0 \\\\ \\lambda &= \\frac{1}{100} \\sum_{i=1}^{100}k_i = \\bar{k} \\end{align}\\] ]\n여러분은 방금 (데이터로부터 시작하여) 모델 매개변수(들)를 추론하기 위한 통계적 접근 방식 의 첫 번째 단계들을 보았습니다: 이것이 데이터로부터 매개변수를 추정하는 통계적 추정(estimation) 입니다. 또 다른 중요한 구성 요소는 우리 데이터를 모델링하는 데 어떤 분포군을 사용할지 선택하는 것인데, 그 부분은 적합도(goodness of fit)를 평가함으로써 수행됩니다. 우리는 나중에 이를 접하게 될 것입니다.\n전통적인 통계적 검정(statistical testing) 프레임워크에서, 우리는 데이터에 대해 귀무 모델(null model)이라 불리는 단 하나의 모델을 고려합니다. 귀무 모델은 어떤 그룹이나 처리에 관계없이 모든 관측치가 동일한 무작위 분포로부터 온다는 것과 같은 “흥미롭지 않은” 기준선을 공식화합니다. 그런 다음 데이터가 그 모델과 호환될 확률을 계산함으로써 좀 더 흥미로운 일이 일어나고 있는지 테스트합니다. 종종 이것이 우리가 할 수 있는 최선인데, 왜냐하면 우리는 “흥미로운” 비귀무(non-null) 또는 대립 모델이 무엇인지 충분히 자세히 알지 못하기 때문입니다. 다른 상황에서는 나중에 보게 되겠지만 우리가 서로 비교할 수 있는 두 개의 경쟁 모델을 가집니다.\n질문 2.6\n알려진 분포를 사용하여 모델링하는 것의 가치는 무엇인가요? 예를 들어, 어떤 변수가 푸아송 분포를 가진다는 것을 아는 것이 왜 흥미로울까요?\n해결책\n모델은 데이터 생성 과정을 간결하면서도 표현력 있게 나타낸 것입니다. 예를 들어 푸아송 분포의 경우, 숫자 하나만 알면 우리가 앞서 보았듯이 극단적이거나 희귀한 사건의 확률을 포함하여 그 분포에 대한 모든 것을 알 수 있습니다.\n또 다른 유용한 방향은 회귀(regression) 입니다. 우리는 카운트 기반의 반응 변수(예: 시퀀싱 리드 카운트 결과)가 연속형 공변량(예: 온도나 영양분 농도)에 어떻게 의존하는지 알고 싶을 수 있습니다. 반응 변수 (y)가 방정식 (y = ax+b + e)를 통해 공변량 (x)에 의존하고, (우리가 추정해야 할) 매개변수 (a)와 (b), 그리고 확률 모델이 정규 분포인 잔차 (e)(이의 분산 또한 대개 추정해야 함)를 사용하는 선형 회귀를 이미 접해 보셨을 것입니다. 카운트 데이터의 경우에도 동일한 유형의 회귀 모델이 가능하지만, 잔차의 확률 분포는 비정규 분포여야 합니다. 그 경우 우리는 일반화 선형 모델(generalized linear models) 프레임워크를 사용합니다. 우리는 8장에서 RNA-Seq을 공부할 때, 그리고 9장에서 또 다른 유형의 차세대 시퀀싱 데이터인 16S rRNA 데이터를 공부할 때 그 예시들을 보게 될 것입니다.\n우리 확률 모델이 푸아송, 이항, 다항 분포 또는 다른 모수적 군(parametric family)을 포함한다는 것을 안다면, 모델의 매개변수에 관한 질문들에 빠르게 답할 수 있고 p-값이나 신뢰 구간과 같은 수치들을 계산할 수 있게 될 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>매개변수가 핵심입니다.</span>"
    ]
  },
  {
    "objectID": "02-chap.html#이항-분포와-최대-우도",
    "href": "02-chap.html#이항-분포와-최대-우도",
    "title": "4  매개변수가 핵심입니다.",
    "section": "4.3 2.4 이항 분포와 최대 우도",
    "text": "4.3 2.4 이항 분포와 최대 우도\n이항 분포에는 두 가지 매개변수가 있습니다: 대개 알려져 있는 시행 횟수 (n)과, 각 시행에서 1이 나타날 확률 (p)입니다. 이 확률은 종종 알려져 있지 않습니다.\n\n4.3.1 2.4.1 예제\n(n=120)명의 남성 샘플을 추출하여 적록 색맹 여부를 테스트한다고 가정해 봅시다. 피험자가 색맹이 아니면 0으로, 색맹이면 1로 데이터를 코딩할 수 있습니다. 우리는 다음과 같은 표로 데이터를 요약합니다:\ntable(cb)\n\n\ncb\n  0   1 \n110  10 \n질문 2.7\n이 데이터가 주어졌을 때 가장 발생 가능성이 높은 (p) 값은 무엇인가요?\n해결책\n(=)입니다.\nmean(cb)\n\n\n[1] 0.08333333\n\n\n\n하지만 주의하세요: 때때로 최대 우도 추정치는 추측하거나 계산하기가 더 어려우며 훨씬 덜 직관적이기도 합니다 (연습 문제 2.2 참조).\n\n\n하지만 주의하세요: 때때로 최대 우도 추정치는 추측하거나 계산하기가 더 어려우며 훨씬 덜 직관적이기도 합니다 (연습 문제 2.2 참조).\n이 특수한 사례에서는 여러분의 직관이 (=)라는 추정치를 줄 수 있는데, 이것이 최대 우도 추정치임이 밝혀졌습니다. 우리는 이것이 (필연적으로) 기저의 실제 값이 아니라 우리가 데이터로부터 얻은 추정치임을 상기하기 위해 글자 위에 햇(hat)을 씌웠습니다.\n이전의 푸아송 사례와 마찬가지로, 많은 가능한 (p) 값들에 대해 우도를 계산한다면 이를 플롯하고 최댓값이 어디에 떨어지는지 확인할 수 있습니다 (그림 2.5).\nprobs  =  seq(0, 0.3, by = 0.005)\nlikelihood = dbinom(sum(cb), prob = probs, size = length(cb))\nplot(probs, likelihood, pch = 16, xlab = \"성공 확률\",\n       ylab = \"우도\", cex=0.6)\nprobs[which.max(likelihood)]\n\n\n[1] 0.085\n\n그림 2.5: 확률의 함수로서의 우도 플롯. 우도는 ([0, 1]) 구간의 함수입니다. 여기서는 (p)의 더 큰 값들에 대한 우도가 사실상 0이므로 ([0, 0.3]) 범위를 확대해서 보았습니다.\n참고: 0.085는 우리가 예상했던 값인 (())과 정확히 일치하지 않는데, 이는 우리가 시도했던 값들의 집합(probs)이 ()이라는 정확한 값을 포함하지 않았기 때문에 차선책을 얻었기 때문입니다. 수치 최적화 방법을 사용하여 이를 극복할 수 있습니다.\n\n\n4.3.2 2.4.2 이항 분포의 우도\n\n\n\n최대 우도와는 다른 기준을 세워 다른 추정량을 도출할 수도 있습니다. 이들 역시 모두 햇(hats)을 씌웁니다. [4장](04-chap.html)에서 다른 예시들을 보게 될 것입니다.\n\n\n최대 우도와는 다른 기준을 세워 다른 추정량을 도출할 수도 있습니다. 이들 역시 모두 햇(hats)을 씌웁니다. 4장에서 다른 예시들을 보게 될 것입니다.\n우도(likelihood)와 확률(probability)은 동일한 수학적 함수이지만, 단지 해석하는 방식이 다를 뿐입니다 – 한 가지 경우에는 함수가 매개변수가 주어졌을 때 데이터의 특정 값 집합을 볼 확률이 얼마나 되는지를 알려줍니다; 다른 경우에는 데이터를 주어진 것으로 간주하고 그 데이터를 생성했을 법한 매개변수 값(들)을 묻습니다. (n=300)이라고 가정하고 (y=40)번의 성공을 관찰했다고 합시다. 그러면 이항 분포의 경우:\n[ f(p,|,n,y) = f(y,|,n,p)={n y} , p^y , (1-p)^{(n-y)}. ]\n다시 말하지만, 우도의 로그를 사용하는 것이 더 편리합니다.\n[ f(p |y) = + y(p) + (n-y)(1-p). ]\n다음은 이를 계산하는 데 사용할 수 있는 함수입니다5.\n5 실제로는 choose(n, y)를 명시적으로 계산하는 것을 피하려고 노력할 것인데, 이는 컴퓨터의 부동 소수점 연산 한계를 시험하는 매우 큰 숫자일 수 있기 때문입니다 (n=300 및 y=40의 경우 약 9.8e+49임). 이는 최대화에 영향을 주지 않는 (p)와 무관한 추가적인 오프셋일 뿐이므로, 스털링 공식을 사용하여 항을 근사화하거나 실제로 무시할 수 있습니다.\nloglikelihood = function(p, n = 300, y = 40) {\n  log(choose(n, y)) + y * log(p) + (n - y) * log(1 - p)\n}\n0부터 1까지의 (p) 범위에 대해 이를 플롯해 봅니다 (그림 2.6).\np_seq = seq(0, 1, by = 0.001)\nplot(p_seq, loglikelihood(p_seq), xlab = \"p\", ylab = \"log f(p|y)\", type = \"l\")\n\n그림 2.6: (n=300) 및 (y=40)에 대한 로그 우도 함수 플롯.\n최댓값은 40/300 = 0.1333… 지점에 위치하며 이는 직관과 일치하지만, 함수가 최댓값 주변에서 상당히 평평하여 (p)의 다른 값들도 거의 비슷하게 발생할 가능성이 있음을 알 수 있습니다. 우리는 이후 섹션에서 베이지안 방법을 사용하여 단일 최댓값만을 고르는 대신 (p)에 대한 값의 범위를 어떻게 다룰 수 있는지 살펴볼 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>매개변수가 핵심입니다.</span>"
    ]
  },
  {
    "objectID": "02-chap.html#더-많은-상자들-다항-데이터",
    "href": "02-chap.html#더-많은-상자들-다항-데이터",
    "title": "4  매개변수가 핵심입니다.",
    "section": "4.4 2.5 더 많은 상자들: 다항 데이터",
    "text": "4.4 2.5 더 많은 상자들: 다항 데이터\n\n4.4.1 2.5.1 DNA 카운트 모델링: 염기쌍\nDNA에는 아데닌(A), 시토신(C), 구아닌(G), 티민(T)의 네 가지 기본 분자가 있습니다. 뉴클레오타이드는 퓨린(A와 G)과 피리미딘(C와 T)의 두 그룹으로 분류됩니다. 이항 분포는 퓨린/피리미딘 그룹화에 대한 모델로 작동하겠지만, 만약 A, C, G, T를 사용하고 싶다면 작동하지 않을 것입니다; 이를 위해서는 1.4절의 다항 모델이 필요합니다. 이러한 빈도에서 나타나는 주목할 만한 패턴들을 살펴봅시다.\n\n\n4.4.2 2.5.2 뉴클레오타이드 편향\n이 섹션에서는 실제 예제에서 시뮬레이션을 통한 추정과 검정을 결합합니다. Staphylococcus aureus 박테리아 유전자의 DNA 한 가닥에 대한 데이터는 fasta 파일 staphsequence.ffn.txt에서 사용할 수 있으며, 바이오컨덕터 패키지 Biostrings의 함수를 사용하여 읽어 들일 수 있습니다.\nlibrary(\"Biostrings\")\nstaph = readDNAStringSet(\"../data/staphsequence.ffn.txt\", \"fasta\")\n첫 번째 유전자를 살펴봅시다:\nstaph[1]\n\n\nDNAStringSet object of length 1:\n    width seq                                               names               \n[1]  1362 ATGTCGGAAAAAGAAATTTGGGA...AAAAAGAAATAAGAAATGTATAA lcl|NC_002952.2_c...\n\n\nletterFrequency(staph[[1]], letters = \"ACGT\", OR = 0)\n\n\n  A   C   G   T \n522 219 229 392 \n질문 2.8\n왜 두 번째 줄에서 이중 대괄호를 사용했나요?\n해결책\n이중 대괄호 [[i]]는 i번째 유전자의 서열을 단일 DNAString 으로 추출하는 반면, 한 쌍의 단일 대괄호 [i]는 그 안에 단일 DNAString 이 들어있는 DNAStringSet 을 반환합니다. 만약 staph[1]의 길이를 보면 1인 반면, staph[[1]]의 길이는 1362입니다.\n질문 2.9\n연습 문제 1.8과 유사한 절차를 따라, 이 첫 번째 유전자에 대해 네 가지 뉴클레오타이드가 균등하게 분포되어 있는지 테스트해 보세요.\n서로 다른 물리적 특성 때문에, 진화적 선택은 뉴클레오타이드 빈도에 작용할 수 있습니다. 따라서 우리는 이 데이터의 처음 10개 유전자가 동일한 다항 분포로부터 오는지 물을 수 있습니다. 우리는 사전 참조 정보가 없으며, 처음 10개 유전자의 뉴클레오타이드가 동일한 비율로 나타나는지만을 결정하고 싶습니다. 만약 그렇지 않다면, 이는 이 10개 유전자에 대한 선택 압력이 다양하다는 증거를 제공할 것입니다.\nletterFrq = vapply(staph, letterFrequency, FUN.VALUE = numeric(4),\n         letters = \"ACGT\", OR = 0)\ncolnames(letterFrq) = paste0(\"gene\", seq(along = staph))\ntab10 = letterFrq[, 1:10]\ncomputeProportions = function(x) { x/sum(x) }\nprop10 = apply(tab10, 2, computeProportions)\nround(prop10, digits = 2)\n\n\n  gene1 gene2 gene3 gene4 gene5 gene6 gene7 gene8 gene9 gene10\nA  0.38  0.36  0.35  0.37  0.35  0.33  0.33  0.34  0.38   0.27\nC  0.16  0.16  0.13  0.15  0.15  0.15  0.16  0.16  0.14   0.16\nG  0.17  0.17  0.23  0.19  0.22  0.22  0.20  0.21  0.20   0.20\nT  0.29  0.31  0.30  0.29  0.27  0.30  0.30  0.29  0.28   0.36\n\n\np0 = rowMeans(prop10)\np0\n\n\n        A         C         G         T \n0.3470531 0.1518313 0.2011442 0.2999714 \n따라서 p0가 10개 유전자 전체에 대한 다항 확률의 벡터라고 가정하고, 이 가정하에서 관찰된 문자 빈도와 기대값 사이의 차이가 타당한 범위 내에 있는지 몬테카를로 시뮬레이션을 사용하여 테스트해 봅시다.\n우리는 확률 벡터 p0와 10개 열 각각의 뉴클레오타이드 카운트 합계 cs 사이의 외적(outer product)을 취하여 기대되는 카운트를 계산합니다.\ncs = colSums(tab10)\ncs\n\n\n gene1  gene2  gene3  gene4  gene5  gene6  gene7  gene8  gene9 gene10 \n  1362   1134    246   1113   1932   2661    831   1515   1287    696 \n\n\nexpectedtab10 = outer(p0, cs, FUN = \"*\")\nround(expectedtab10)\n\n\n  gene1 gene2 gene3 gene4 gene5 gene6 gene7 gene8 gene9 gene10\nA   473   394    85   386   671   924   288   526   447    242\nC   207   172    37   169   293   404   126   230   195    106\nG   274   228    49   224   389   535   167   305   259    140\nT   409   340    74   334   580   798   249   454   386    209\n우리는 이제 rmultinom 함수를 사용하여 올바른 열 합계를 가진 무작위 표를 만들 수 있습니다. 이 표는 실제 비율이 p0에 의해 주어진다는 귀무 가설에 따라 생성됩니다.\nrandomtab10 = sapply(cs, function(s) { rmultinom(1, s, p0) } )\nall(colSums(randomtab10) == cs)\n\n\n[1] TRUE\n이제 이를 B = 1000번 반복합니다. 각 표에 대해 1장의 1.4.1절(함수 stat)에서 정의한 검정 통계량을 계산하고 그 결과를 simulstat 벡터에 저장합니다. 이 값들은 모두 10개 유전자 각각에 대해 p0가 다항 비율 벡터라는 귀무 가설하에서 생성된 것이므로, 통칭하여 우리 귀무 분포를 구성합니다.\nstat = function(obsvd, exptd) {\n   sum((obsvd - exptd)^2 / exptd)\n}\nB = 1000\nsimulstat = replicate(B, {\n  randomtab10 = sapply(cs, function(s) { rmultinom(1, s, p0) })\n  stat(randomtab10, expectedtab10)\n})\nS1 = stat(tab10, expectedtab10)\nsum(simulstat &gt;= S1)\n\n\n[1] 0\n\n\nhist(simulstat, col = \"lavender\", breaks = seq(0, 75, length.out=50))\nabline(v = S1, col = \"red\")\nabline(v = quantile(simulstat, probs = c(0.95, 0.99)),\n       col = c(\"darkgreen\", \"blue\"), lty = 2)\n\n그림 2.7: simulstat의 히스토그램. S1의 값은 수직 빨간색 선으로 표시되었으며, 0.95 및 0.99 분위수(다음 섹션 참조)는 점선으로 표시되었습니다.\n히스토그램은 그림 2.7에 나와 있습니다. 우리는 귀무 모델 하에서 S1=70.1만큼 큰 값을 볼 확률이 매우 낮다는 것을 알 수 있습니다. 우리가 수행한 1000번의 시뮬레이션 중에서 S1만큼 큰 값이 나타난 경우는 0번이었습니다. 따라서 이 10개 유전자는 동일한 다항 모델로부터 온 것 같지 않습니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>매개변수가 핵심입니다.</span>"
    ]
  },
  {
    "objectID": "02-chap.html#분포",
    "href": "02-chap.html#분포",
    "title": "4  매개변수가 핵심입니다.",
    "section": "4.5 2.6 (^2) 분포",
    "text": "4.5 2.6 (^2) 분포\n사실, 우리는 이러한 시뮬레이션을 실행하지 않고도 통계 이론을 사용하여 동일한 결론에 도달할 수 있었습니다. simulstat 통계량의 이론적 분포는 매개변수가 30((=10(4-1)))인 (^2)(카이제곱) 분포6라고 불립니다. 우리는 이를 사용하여 S1 (=) 70.1만큼 높은 값을 가질 확률을 계산할 수 있습니다. 위에서 보았듯이, 작은 확률은 몬테카를로 방법으로 계산하기 어렵습니다: 계산의 해상도가 (1/B)이기 때문에 그보다 작은 확률은 추정할 수 없으며, 실제로 추정치의 불확실성은 그보다 큽니다. 따라서 어떤 이론이라도 적용 가능하다면 유용한 경향이 있습니다. 우리는 또 다른 시각적 적합도 도구인 분위수-분위수(quantile-quantile, QQ) 플롯을 사용하여 우리 사례에서 이론과 시뮬레이션이 얼마나 잘 일치하는지 확인할 수 있습니다. 두 분포를 비교할 때, 그것이 서로 다른 두 표본으로부터 온 것이든 아니면 하나의 표본 대 이론적 모델이든, 단순히 히스토그램만 보는 것은 충분히 정보가 많지 않습니다. 우리는 각 분포의 분위수(quantiles)를 기반으로 한 방법을 사용합니다.\n6 엄밀히 말하면, simulstat의 분포는 대략적으로 (^2) 분포로 설명됩니다; 이 근사는 표의 카운트가 클 때 특히 정확합니다.\n\n4.5.1 2.6.1 막간: 분위수와 분위수-분위수 플롯\n이전 장에서 우리는 100개의 표본 값 (x_{(1)}, x_{(2)}, …, x_{(100)})을 정렬했습니다. 만약 22번째 백분위수를 원한다면, 우리는 22번째와 23번째 값 사이의 임의의 값을 취할 수 있습니다. 즉, (x_{(22)} c_{0.22} &lt; x_{(23)})을 만족하는 임의의 값은 0.22 분위수 ((c_{0.22}))로 수용 가능합니다. 다시 말해, (c_{0.22})는 다음과 같이 정의됩니다.\n[ = 0.22. ]\n3.6.7절에서 우리는 경험적 누적 분포(empirical cumulative distribution) 함수(ECDF) ()를 소개할 것이며, 우리 분위수 (c_{0.22})의 정의가 (n(c{0.22}) = 0.22)로 쓰여질 수도 있음을 보게 될 것입니다. 그림 2.7의 simulstat 분포 히스토그램에는 분위수 (c_{0.95})와 (c_{0.99})도 표시되어 있습니다.\n질문 2.10\n\nsimulstat 값들과 무작위로 생성된 1000개의 (^2_{30}) 난수를 각각 50개의 빈(bin)을 가진 히스토그램으로 표시하여 비교해 보세요.\nsimulstat 값들의 분위수를 계산하고 이를 (_{30}^2) 분포의 분위수와 비교해 보세요. 힌트:\n\nqs = ppoints(100)\nquantile(simulstat, qs)\nquantile(qchisq(qs, df = 30), qs)\n\n\n\n여기서 명칭의 충돌이 발생합니다. 통계학자들은 방금 우리가 계산한 요약 통계량인 simulstat(가중 제곱 차이의 합)을 카이제곱 또는 (^2) 통계량이라고 부릅니다. 이론적 분포 (^2_)는 자유도라고 불리는 매개변수 ()를 가진 그 자체의 분포입니다. 카이제곱 또는 (^2)에 대해 읽을 때, 어떤 의미가 적절한지 파악하기 위해 문맥에 주의를 기울여야 합니다.\n\n\n여기서 명칭의 충돌이 발생합니다. 통계학자들은 방금 우리가 계산한 요약 통계량인 simulstat(가중 제곱 차이의 합)을 카이제곱 또는 (^2) 통계량 이라고 부릅니다. 이론적 분포 (^2_)는 자유도라고 불리는 매개변수 ()를 가진 그 자체의 분포입니다. 카이제곱 또는 (^2)에 대해 읽을 때, 어떤 의미가 적절한지 파악하기 위해 문맥에 주의를 기울여야 합니다.\n질문 2.11\n0.5 분위수의 또 다른 이름을 알고 있나요?\n해결책\n중앙값(median).\n질문 2.12\n위의 정의에서, 우리는 분위수가 단지 0.22뿐만 아니라 일반적으로 어떻게 정의되는지에 대해 약간 모호하게 설명했습니다. (1/n)의 배수가 아닌 숫자를 포함하여 0과 1 사이의 임의의 숫자에 대한 분위수는 어떻게 계산되나요?\n해결책\nquantile 함수의 매뉴얼 페이지와 type이라는 인수를 확인해 보세요.\n이제 분위수가 무엇인지 알았으므로 분위수-분위수 플롯을 그릴 수 있습니다. 귀무 가설하에서 우리가 시뮬레이션한 simulstat 값들의 분위수를 이론적인 귀무 분포인 (^2_{30})에 대해 플롯합니다 (그림 2.8).\nqqplot(qchisq(ppoints(B), df = 30), simulstat, main = \"\",\n  xlab = expression(\\chi[nu==30]^2), asp = 1, cex = 0.5, pch = 16)\nabline(a = 0, b = 1, col = \"red\")\n\n그림 2.8: 우리 시뮬레이션 통계량의 분포를 ({30}^2)와 비교한 분위수-분위수(QQ) 플롯. 가로축에는 (^2{30}) 분포의 이론적 분위수를, 세로축에는 샘플링된 분위수를 나타냅니다.\nsimulstat가 (^2_{30}) 분포에 의해 잘 설명된다고 확신했다면, 이를 사용하여 p-값, 즉 귀무 가설하에서 (카운트가 확률 (p_{} = 0.35, p_{} = 0.15, p_{} = 0.2, p_{} = 0.3)인 다항 분포를 따를 때) S1=70.1만큼 높은 값을 관찰할 확률을 계산할 수 있습니다.\n1 - pchisq(S1, df = 30)\n\n\n[1] 4.74342e-05\np-값이 이렇게 작으므로 귀무 가설은 일어날 법하지 않아 보입니다. 이 계산에는 1000번의 시뮬레이션이 필요하지 않았으며 훨씬 더 빨랐음을 주목하세요.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>매개변수가 핵심입니다.</span>"
    ]
  },
  {
    "objectID": "02-chap.html#샤가프의-법칙-chargaffs-rule",
    "href": "02-chap.html#샤가프의-법칙-chargaffs-rule",
    "title": "4  매개변수가 핵심입니다.",
    "section": "4.6 2.7 샤가프의 법칙 (Chargaff’s Rule)",
    "text": "4.6 2.7 샤가프의 법칙 (Chargaff’s Rule)\n뉴클레오타이드 빈도에서의 가장 중요한 패턴은 Chargaff에 의해 발견되었습니다 (Elson and Chargaff 1952).\n\nDNA 시퀀싱이 가용해지기 훨씬 전, 분자들의 무게를 사용하여 그는 뉴클레오타이드들이 동일한 빈도로 나타나는지 물었습니다. 그는 이를 테트라뉴클레오타이드 가설(tetranucleotide hypothesis)이라고 불렀습니다. 우리는 이를 (p_{} = p_{} = p_{} = p_{}) 인지 묻는 것으로 해석할 것입니다.\n불행하게도, Chargaff는 측정값 그 자체가 아니라 서로 다른 유기체에 존재하는 각 뉴클레오타이드 질량의 백분율 만을 발표했습니다.\nload(\"../data/ChargaffTable.RData\")\nChargaffTable\n\n\n                  A    T    C    G\nHuman-Thymus   30.9 29.4 19.9 19.8\nMycobac.Tuber  15.1 14.6 34.9 35.4\nChicken-Eryth. 28.8 29.2 20.5 21.5\nSheep-liver    29.3 29.3 20.5 20.7\nSea Urchin     32.8 32.1 17.7 17.3\nWheat          27.3 27.1 22.7 22.8\nYeast          31.3 32.9 18.7 17.1\nE.coli         24.7 23.6 26.0 25.7\n\n그림 2.9: ChargaffTable의 서로 다른 행들에 대한 막대 그래프. 패턴을 찾을 수 있나요?\n질문 2.13\n\n이 데이터가 동일한 확률을 가진 다항 범주들로부터 온 것처럼 보이나요?\n대안적인 패턴을 제안할 수 있나요?\n위의 시뮬레이션에서 영감을 얻어, 해당 패턴에 대한 정량적 분석을 수행할 수 있나요?\n\n해결책\n샤가프는 이 질문에 대한 답을 보고 , 유기체의 DNA에서 아데닌(A)의 양과 티민(T)의 양이 완벽하게 일치하도록 보장하는 염기쌍(base pairing) 이라는 패턴을 가정했습니다. 유사하게, 구아닌(G)의 양이 얼마든 시토신(C)의 양은 그와 같을 것입니다. 이것이 현재 샤가프의 법칙이라 불리는 것입니다. 반면에, 한 유기체 내에서의 C/G 양은 A/T 양과 상당히 다를 수 있으며 유기체들 전반에 걸쳐 뚜렷한 패턴은 없습니다. 샤가프의 법칙에 기초하여, 우리는 다음과 같은 통계량을 정의할 수 있습니다.\n[ (p_{} - p_{})^2 + (p_{} - p_{})^2, ]\n표의 모든 행에 대해 합산한 것입니다. 우리는 각 행에서 관찰된 확률들이 특별한 순서가 없어서 As와 Ts의 비율 또는 Cs와 Gs의 비율 사이에 특별한 관계가 없었다는 의미에서 뉴클레오타이드들이 ’교환 가능’했을 경우 데이터에 어떤 일이 일어날지에 대한 비교를 살펴볼 것입니다.\nstatChf = function(x){\n  sum((x[, \"C\"] - x[, \"G\"])^2 + (x[, \"A\"] - x[, \"T\"])^2)\n}\nchfstat = statChf(ChargaffTable)\npermstat = replicate(100000, {\n     permuted = t(apply(ChargaffTable, 1, sample))\n     colnames(permuted) = colnames(ChargaffTable)\n     statChf(permuted)\n})\npChf = mean(permstat &lt;= chfstat)\npChf\n\n\n[1] 0.00014\n\n\nhist(permstat, breaks = 100, main = \"\", col = \"lavender\")\nabline(v = chfstat, lwd = 2, col = \"red\")\n\n그림 2.10: 컬럼들의 행 단위 순열을 사용한 시뮬레이션을 통해 계산된 우리 통계량 statChf의 히스토그램. 관찰된 데이터에 대해 산출된 값은 빨간색 선으로 표시되었습니다.\n그림 2.10의 히스토그램은 빨간색 선이 그려진 관측값 11.1만큼 작은 값을 갖는 것이 매우 드물다는 것을 보여줍니다. 그와 같거나 더 작은 값을 관찰할 확률은 pChf=(1.4 ^{-4})입니다. 따라서 데이터는 샤가프의 통찰을 강력하게 뒷받침합니다.\n질문 2.14\npChf를 계산할 때, 우리는 귀무 분포에서 관측된 값보다 작은 값들만을 살펴보았습니다. 왜 여기서 우리는 단측(one-sided) 방식으로 이를 수행했나요?\n\n4.6.1 2.7.1 두 개의 범주형 변수\n지금까지 우리는 샘플이 서로 다른 상자들로 분류될 수 있는 사례들을 보았습니다: 예/아니오 이진 상자에 대한 이항 분포와 A, C, G, T 또는 aa, aA, AA와 같은 다른 유전자형과 같은 범주형 변수에 대한 다항 분포입니다. 하지만 우리가 일련의 피험자에 대해 눈 색깔과 머리카락 색깔과 같은 두 개(또는 그 이상)의 범주형 변수를 측정할 수도 있습니다. 그런 다음 눈 색깔과 머리카락 색깔의 모든 조합에 대한 카운트를 교차 집계할 수 있습니다. 우리는 분할표(contingency table) 라 불리는 카운트 표를 얻게 됩니다. 이 개념은 많은 생물학적 데이터 유형에 매우 유용합니다.\nHairEyeColor[,, \"Female\"]\n\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    36    9     5     2\n  Brown    66   34    29    14\n  Red      16    7     7     7\n  Blond     4   64     5     8\n질문 2.15\nR에서 HairEyeColor 객체를 탐색해 보세요. 어떤 데이터 유형, 모양, 차원을 가지고 있나요?\n해결책\n이는 세 가지 차원을 가진 수치형 배열(array)입니다:\nstr(HairEyeColor)\n\n\n 'table' num [1:4, 1:4, 1:2] 32 53 10 3 11 50 10 30 10 25 ...\n - attr(*, \"dimnames\")=List of 3\n  ..$ Hair: chr [1:4] \"Black\" \"Brown\" \"Red\" \"Blond\"\n  ..$ Eye : chr [1:4] \"Brown\" \"Blue\" \"Hazel\" \"Green\"\n  ..$ Sex : chr [1:2] \"Male\" \"Female\"\n\n\n## ?HairEyeColor\n\n4.6.1.1 색맹과 성별\n녹색맹(Deuteranopia)은 중간 파장에 민감한 원뿔 세포(녹색)가 없어서 발생하는 적록 색맹의 한 형태입니다. 녹색맹 환자는 오직 2~3가지의 서로 다른 색조만을 구별할 수 있는 반면, 정상 시력을 가진 사람은 7가지의 서로 다른 색조를 봅니다. 인간 피험자들을 대상으로 한 이 유형의 색맹 조사는 색맹 여부와 성별을 교차시킨 2원 표(two-way table)를 생성했습니다.\nload(\"../data/Deuteranopia.RData\")\nDeuteranopia\n\n\n          Men Women\nDeute      19     2\nNonDeute 1981  1998\n성별과 색맹 발생 사이에 관계가 있는지 어떻게 테스트할 수 있을까요? 우리는 성별에 대한 것과 색맹에 대한 것, 두 개의 독립적인 이항 분포를 사용한 귀무 모델을 가정합니다. 이 모델하에서 우리는 모든 셀의 다항 확률을 추정할 수 있으며, 관찰된 카운트를 기대되는 카운트와 비교할 수 있습니다. 이는 R의 chisq.test 함수를 통해 수행됩니다.\nchisq.test(Deuteranopia)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  Deuteranopia\nX-squared = 12.255, df = 1, p-value = 0.0004641\n작은 p-값은 귀무 모델하에서 그러한 표를 볼 확률이 매우 낮다는 것을 말해줍니다 – 즉, 여성과 남성 사이에서 녹색맹 비율이 동일하다는 가설하에서 말이죠.\n우리는 10.3.2절에서 피셔의 정확 검정(Fisher’s exact test, 초기하 검정으로도 알려짐)이라 불리는 이러한 유형의 데이터를 위한 또 다른 검정을 보게 될 것입니다. 이 검정은 유의미하게 발현된 유전자 리스트에서 특정 유형의 유전자들이 과다하게 나타나는지 테스트하는 데 널리 사용됩니다.\n\n\n\n4.6.2 2.7.2 특수한 다항 분포: 하디-와인버그 평형 (Hardy-Weinberg equilibrium)\n여기서 우리는 두 대립유전자 M과 N을 결합하여 생성된 세 가지 가능한 수준을 가진 다항 분포의 사용을 강조합니다. 모집단에서 대립유전자 M의 전체 빈도가 (p)이고, N의 빈도가 (q = 1-p)라고 가정합시다. 하디-와인버그 모델은 유전자형에서 두 대립유전자의 빈도가 독립적일 때, 소위 하디-와인버그 평형(Hardy-Weinberg equilibrium, HWE) 상태에서 (p)와 (q) 사이의 관계를 살펴봅니다. 이는 성별 간에 대립유전자가 균등하게 분포되어 있는 대규모 집단에서 무작위 교배가 이루어지는 경우에 해당합니다. 세 가지 유전자형의 확률은 다음과 같습니다:\n[ p_{}=p2,p_{}=q2,p_{}=2pq ]\n우리는 유전자형 MM, MN, NN에 대한 빈도 ((n_{},,n_{},,n_{}))과 총합 (S=n_{}+ n_{}+n_{}) 만을 관찰합니다. 범주들의 확률이 식 2.5에 의해 주어질 때 관찰된 데이터의 확률인 우도를 다항 분포 공식을 사용하여 쓸 수 있습니다.\n[ P(n_{},,n_{},,n_{};|;p) = {S n_{},n_{},n_{}} (p2){n_{}} ,, (2pq)^{n_{}} ,, (q2){n_{}}, ]\n그리고 HWE하에서의 로그 우도는 다음과 같습니다.\n[ L(p)=n_{}(p^2)+n_{} (2pq)+n_{}(q^2). ]\n로그 우도를 최대화하는 (p)의 값은 다음과 같습니다.\n[ p = . ]\n증명에 대해서는 (Rice 2006, chap. 8, 5절)를 참조하십시오. 주어진 데이터 ((n_{},,n_{},,n_{}))에서 로그 우도 (L)은 오직 하나의 매개변수 (p)의 함수입니다. 그림 2.11은 아래 코드에서 계산된 Mourant 데이터7의 216번째 행에 대한 (p)의 서로 다른 값들에 따른 로그 우도 함수를 보여줍니다.\n7 이는 R 패키지 HardyWeinberg를 통해 제공되는 Mourant, Kopec 및 Domaniewska-Sobczak (1976)의 혈액형 대립유전자 유전자형 빈도 데이터입니다.\nlibrary(\"HardyWeinberg\")\ndata(\"Mourant\")\nMourant[214:216,]\n\n\n    Population    Country Total  MM  MN  NN\n214    Oceania Micronesia   962 228 436 298\n215    Oceania Micronesia   678  36 229 413\n216    Oceania     Tahiti   580 188 296  96\n\n\nnMM = Mourant$MM[216]\nnMN = Mourant$MN[216]\nnNN = Mourant$NN[216]\nloglik = function(p, q = 1 - p) {\n  2 * nMM * log(p) + nMN * log(2*p*q) + 2 * nNN * log(q)\n}\nxv = seq(0.01, 0.99, by = 0.01)\nyv = loglik(xv)\nplot(x = xv, y = yv, type = \"l\", lwd = 2,\n     xlab = \"p\",ylab = \"로그 우도\")\nimax = which.max(yv)\nabline(v = xv[imax], h = yv[imax], lwd = 1.5, col = \"blue\")\nabline(h = yv[imax], lwd = 1.5, col = \"purple\")\n\n그림 2.11: 타히티(Tahiti) 데이터에 대한 로그 우도 플롯.\n다항 분포에서의 확률에 대한 최대 우도 추정치는 이항 분포의 경우와 마찬가지로 관찰된 빈도를 사용하여 얻어지지만, 추정치는 세 확률 사이의 관계를 고려해야 합니다. 우리는 HardyWeinberg 패키지의 af 함수를 사용하여 ({}), ({}) 및 (_{})을 계산할 수 있습니다.\nphat  =  af(c(nMM, nMN, nNN))\nphat\n\n\n        A \n0.5793103 \n\n\npMM   =  phat^2\nqhat  =  1 - phat\n하디-와인버그 평형 상태에서 기대되는 값은 다음과 같습니다.\npHW = c(MM = phat^2, MN = 2*phat*qhat, NN = qhat^2)\nsum(c(nMM, nMN, nNN)) * pHW\n\n\n    MM.A     MN.A     NN.A \n194.6483 282.7034 102.6483 \n이는 위에서 관찰된 값들과 비교될 수 있습니다. 우리는 이들이 관측된 값들과 꽤 가깝다는 것을 알 수 있습니다. 우리는 관측된 값들이 하디-와인버그 모델을 기각하게 하는지 여부를 시뮬레이션을 하거나 위에서와 같은 (^2) 검정을 수행하여 추가로 테스트할 수 있습니다. 하디-와인버그 적합도에 대한 시각적 평가는 de Finetti (Finetti 1926; Cannings and Edwards 1968)에 의해 고안되었습니다. 이는 각 대립유전자의 비율로 주어지는 좌표를 가진 지점에 모든 샘플을 배치합니다.\n\n4.6.2.1 하디-와인버그 평형과의 시각적 비교\n우리는 HWTernaryPlot 함수를 사용하여 데이터를 표시하고 이를 하디-와인버그 평형과 그래픽으로 비교합니다.\npops = c(1, 69, 128, 148, 192)\ngenotypeFrequencies = as.matrix(Mourant[, c(\"MM\", \"MN\", \"NN\")])\nHWTernaryPlot(genotypeFrequencies[pops, ],\n        markerlab = Mourant$Country[pops],\n        alpha = 0.0001, curvecols = c(\"red\", rep(\"purple\", 4)),\n        mcex = 0.75, vertex.cex = 1)\n\n그림 2.12: 이 de Finetti 플롯은 세 유전자형의 빈도를 삼각형의 각 꼭짓점에 대한 가중치로 사용하여 무게 중심으로서 점들을 보여줍니다. 하디-와인버그 모델은 빨간색 곡선이며, 채택 영역(acceptance region)은 두 보라색 선 사이에 있습니다. 우리는 미국(US)이 HW 평형 상태에서 가장 멀리 떨어져 있음을 알 수 있습니다.\n질문 2.16\n위의 코드와 같이 삼원 플롯(ternary plot)을 만든 다음, 다른 데이터 포인트들도 추가해 보세요. 무엇을 알 수 있나요? HWChisq 함수를 사용하여 여러분의 논의를 뒷받침할 수 있습니다.\n해결책\nHWTernaryPlot(genotypeFrequencies[-pops, ], \n              newframe = FALSE, alpha = 0.0001, cex = 0.5)\n질문 2.17\n각 유전자형에 대해 동일한 비율을 유지하면서 모든 전체 빈도를 50으로 나누고 삼원 플롯을 다시 만듭니다.\n\n점들은 어떻게 되나요?\n신뢰 영역(confidence regions)은 어떻게 되며 그 이유는 무엇인가요?\n\n해결책\nnewgf = round(genotypeFrequencies / 50)\nHWTernaryPlot(newgf[pops, ],\n              markerlab = Mourant$Country[pops],\n              curvecols = c(\"red\", rep(\"purple\", 4)),\n              alpha = 0.0001, mcex = 0.75, vertex.cex = 1)\n\n\n\n4.6.3 2.7.3 여러 다항 분포 연결하기: 서열 모티프와 로고\nKozak 모티프는 코딩 영역의 시작 코돈인 ATG 근처에서 발생하는 서열입니다. 시작 코돈 자체는 항상 고정된 철자를 가지고 있지만, 그 왼쪽으로 5개 위치에는 뉴클레오타이드 패턴이 나타나며 그곳의 글자들은 동일한 확률을 갖는 것과는 거리가 멉니다.\n우리는 모든 위치에서의 다항 확률을 제공하는 위치 가중치 행렬(position weight matrix, PWM) 또는 위치 특이적 점수 행렬(position-specific scoring matrix, PSSM)을 제공함으로써 이를 요약합니다. 이는 서열 로고(sequence logo) (그림 2.13)를 통해 그래픽으로 인코딩됩니다.\nlibrary(\"seqLogo\")\nload(\"../data/kozak.RData\")\nkozak\n\n\n  [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\nA 0.33 0.25  0.4 0.15 0.20    1    0    0 0.05\nC 0.12 0.25  0.1 0.40 0.40    0    0    0 0.05\nG 0.33 0.25  0.4 0.20 0.25    0    0    1 0.90\nT 0.22 0.25  0.1 0.25 0.15    0    1    0 0.00\n\n\npwm = makePWM(kozak)\nseqLogo(pwm, ic.scale = FALSE)\n\n그림 2.13: Kozak 모티프를 모델링하는 데 사용되는 위치 종속 다항 분포에 대한 서열 로고라 불리는 다이어그램입니다. 이는 각 위치에서의 변동량을 로그 스케일로 인코딩합니다. 큰 글자들은 어떤 뉴클레오타이드가 나타날지에 대해 불확실성이 없는 위치를 나타냅니다.\n지난 몇 개 섹션에서 우리는 우리가 접했던 다항 분포의 서로 다른 “상자”들이 동일한 확률을 갖는 경우가 매우 드물다는 것을 보았습니다. 다시 말해, 매개변수 (p_1, p_2, …)는 모델링되는 대상에 따라 대개 다릅니다. 동일하지 않은 빈도를 가진 다항 분포의 예로는 20가지의 서로 다른 아미노산, 혈액형, 머리카락 색깔 등이 있습니다.\n만약 여러 개의 범주형 변수가 있다면, 우리는 그것들이 독립적인 경우가 드물다는 것을 보았습니다(성별과 색맹, 머리카락과 눈 색깔, …). 우리는 나중에 9장에서 분할표의 다변량 분해를 사용하여 이러한 의존성의 패턴을 탐구할 수 있음을 보게 될 것입니다. 여기서는 범주형 변수 간의 의존성의 중요한 특수한 사례, 즉 (예를 들어 시간의 흐름이나 바이오폴리머를 따라) 범주형 변수들의 시퀀스(또는 “체인”)를 따라 발생하는 의존성을 살펴볼 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>매개변수가 핵심입니다.</span>"
    ]
  },
  {
    "objectID": "02-chap.html#순차적-의존성-모델링-마르코프-체인",
    "href": "02-chap.html#순차적-의존성-모델링-마르코프-체인",
    "title": "4  매개변수가 핵심입니다.",
    "section": "4.7 2.8 순차적 의존성 모델링: 마르코프 체인",
    "text": "4.7 2.8 순차적 의존성 모델링: 마르코프 체인\n내일의 날씨를 예측하고 싶을 때, 합리적으로 좋은 추측은 오늘 날씨와 대체로 같을 것이라는 점이며, 여기에 더해 다양한 종류의 가능한 변화들에 대한 확률을 서술할 수도 있을 것입니다1. 일기 예보를 위한 이 방법은 마르코프 가정(Markov assumption)의 한 예시입니다: 내일에 대한 예측은 오늘의 상태에만 의존하며, 어제나 3주 전의 상태에는 의존하지 않습니다 (우리가 잠재적으로 사용할 수 있는 모든 정보는 이미 오늘의 날씨에 포함되어 있습니다). 날씨 예제는 또한 그러한 가정이 반드시 정확할 필요는 없지만, 충분히 좋은 가정이어야 한다는 점을 강조합니다. 이 가정을 유한하고 너무 크지 않은 수 (k)에 대해 이전 (k)일 동안의 의존성으로 확장하는 것은 꽤 간단합니다. 마르코프 가정의 정수는 프로세스가 유한한 “기억”을 가지고 있어서 예측을 위해 유한한 시간만큼만 거슬러 올라가면 된다는 점입니다.\n시간적 시퀀스 대신에, 우리는 이를 생물학적 서열에 적용할 수 있습니다. DNA에서 우리는 뉴클레오타이드 쌍인 다이그램(digrams), 예를 들어 [CG, CA, CC] 및 [CT]가 동일하게 빈번하지 않도록 특정 순차적 패턴을 볼 수 있습니다. 예를 들어, 유전체의 일부 영역에서 우리는 독립성 하에서 예상되는 것보다 [CA]의 사례를 더 자주 보게 됩니다:\n[ P() P() , P(). ]\n우리는 서열에서의 이러한 의존성을 마르코프 체인(Markov chain) 으로 모델링합니다:\n[ P() = P() = P() = P(…) = P() , P(), ]\n여기서 N은 임의의 뉴클레오타이드를 나타내고, (P())는 “앞선 염기가 ()일 때 ()가 나타날 확률”을 나타냅니다. 그림 2.14는 그래프 상의 그러한 전이들에 대한 도식적 표현을 보여줍니다.\n\n그림 2.14: 4-상태 마르코프 체인의 시각화. 각 가능한 다이그램(예: CA)의 확률은 대응하는 노드 사이의 에지(edge) 가중치로 주어집니다. 따라서 예를 들어 CA의 확률은 에지 C -&gt; A로 주어집니다. 우리는 11장에서 이러한 유형의 네트워크 그래프를 그리기 위해 R 패키지를 사용하는 방법을 살펴볼 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>매개변수가 핵심입니다.</span>"
    ]
  },
  {
    "objectID": "02-chap.html#베이지안-사고방식",
    "href": "02-chap.html#베이지안-사고방식",
    "title": "4  매개변수가 핵심입니다.",
    "section": "4.8 2.9 베이지안 사고방식",
    "text": "4.8 2.9 베이지안 사고방식\n\n그림 2.15: 아래쪽으로 끝없는 거북이들. 분포의 매개변수에 대한 불확실성의 베이지안 모델링은, 매개변수에 따라 분포가 달라지는 확률 변수를 사용하고 그 매개변수 자체의 불확실성은 다시 확률 변수로 모델링함으로써 수행됩니다; 이들을 계층적 모델(hierarchical models)이라고 부릅니다.\n지금까지 우리는 모델의 매개변수와 그것들이 사용하는 분포, 즉 가능한 서로 다른 결과의 확률이 장기적인 빈도를 나타내는 고전적인 접근 방식을 따랐습니다. 매개변수는 –적어도 개념적으로는– 확실하고 알 수 있으며 고정된 것입니다. 우리는 그것들을 모를 수도 있으므로, 손에 든 데이터로부터 그것들을 추정합니다. 하지만 그러한 접근 방식은 우리가 이미 가지고 있을 수 있는 정보, 즉 우리가 현재의 데이터 세트를 보기 전 일지라도 매개변수에 대해 알려주거나 특정 매개변수 값 또는 그 조합이 다른 것보다 더 가능성 있게 만들 수 있는 정보를 고려하지 않습니다. 이를 위해서는 확률 모델(즉, 분포)을 사용하여 매개변수에 대한 사전 지식8을 표현하고, 현재 데이터를 사용하여 그러한 지식을 업데이트 하는(예를 들어 해당 분포를 이동시키거나 더 좁게 만드는 등) 다른 접근 방식이 필요합니다. 그러한 접근 방식이 바로 베이지안 패러다임(그림 2.15)에 의해 제공됩니다.\n8 어떤 사람들은 “우리들의 믿음(들)”이라고 말하기를 좋아합니다.\n베이지안 패러다임은 데이터를 수집하고 관찰하기 전 과 후 의 우리 지식을 모델링하기 위해 사전(prior) 및 사후(posterior) 분포를 사용하는 실용적인 접근 방식입니다. 이는 무한히 반복될 수 있습니다: 한 차례의 데이터 생성 후의 사후 분포는 다음 차례를 위한 사전 분포로 사용될 수 있습니다. 따라서 이는 서로 다른 소스로부터의 정보를 통합하거나 결합하는 데에도 특히 유용합니다.\n동일한 아이디어가 가설 검정에도 적용될 수 있는데, 여기서 우리는 가설 (H)라고 부를 수 있는 특정 진술이 참이라고 믿는지 결정하기 위해 데이터를 사용하고 싶어 합니다. 여기서 우리의 “매개변수”는 (H)가 참일 확률이며, 우리는 (P(H))9라고 쓰여진 사전 확률의 형태로 우리의 사전 지식을 공식화할 수 있습니다. 데이터를 확인한 후에는 사후 확률을 갖게 됩니다. 우리는 이를 (D)를 보았을 때 (H)일 확률인 (P(H,|,D))로 씁니다. 이는 데이터 (D)가 무엇이었는지에 따라 (P(H))보다 높거나 낮을 수 있습니다.\n9 소위 빈도론자(frequentist)에게 그러한 확률은 존재하지 않습니다. 그들의 관점은 비록 진실은 알려져 있지 않지만 가설은 실제로는 참이거나 거짓이라는 것입니다; 이를 “70% 참”이라고 부르는 것은 의미가 없습니다.\n\n4.8.1 2.9.1 예제: 일배체형 빈도\n수학적 형식주의를 최소한으로 유지하기 위해, Y 염색체의 결합된 시그니처(일배체형, haplotypes)를 사용하는 법의학 예제부터 시작하겠습니다.\n일배체형 이란 염색체 상에서 공간적으로 인접하여 대개 함께 유전되는(재조합에 의해 분리되지 않는 경향이 있음) 대립유전자(DNA 서열 변이체)들의 집합이며, 따라서 유전적으로 연결되어 있습니다. 이 사례에서 우리는 Y 염색체 상의 연결된 변이들을 살펴보고 있습니다.\n먼저 일배체형 빈도 분석의 동기를 살펴보고, 우도(likelihood)에 대한 아이디어를 다시 방문해 보겠습니다. 그 후, 알려지지 않은 매개변수들을 그 자체로 난수인 것으로 생각하고 사전 분포를 사용하여 그 불확실성을 모델링하는 방법을 설명하겠습니다. 그런 다음 관찰된 새로운 데이터를 확률 분포에 통합하고 매개변수에 대한 사후 신뢰 진술을 어떻게 계산하는지 살펴볼 것입니다.\n\n그림 2.16: DNA에서의 단일 반복 서열(short tandem repeat, STR)은 두 개 이상의 뉴클레오타이드 패턴이 반복되고 그 반복된 서열들이 서로 직접 인접해 있을 때 발생합니다. STR은 미세 부수체(microsatellite)라고도 알려져 있습니다. 패턴의 길이는 2에서 13 뉴클레오타이드 범위일 수 있으며, 반복 횟수는 개인마다 매우 다양합니다. STR 숫자는 유전적 시그니처로 사용될 수 있습니다.\n\n그림 2.17: 인간 Y 염색체 상의 단일 반복 서열(STR) 위치. 출처: https://strbase.nist.gov/ystrpos1.htm\n\n그림 2.18: FBI가 사용하는 데이터베이스에서의 Y STR 일배체형 조회.\n우리는 서로 다른 단일 반복 서열(STR) 세트로 구성된 특정 Y-일배체형들의 빈도에 관심이 있습니다. DNA 법의학에 사용되는 특정 위치에서의 STR 숫자 조합은 해당 특정 위치에서의 반복 횟수로 라벨링됩니다. 다음은 그러한 STR 일배체형 표의 짧은 발췌본입니다:\nhaplo6 = read.table(\"../data/haplotype6.txt\", header = TRUE)\nhaplo6\n\n\n  Individual DYS19 DXYS156Y DYS389m DYS389n DYS389p\n1         H1    14       12       4      12       3\n2         H3    15       13       4      13       3\n3         H4    15       11       5      11       3\n4         H5    17       13       4      11       3\n5         H7    13       12       5      12       3\n6         H8    16       11       5      12       3\n표는 일배체형 H1이 DYS19 위치에서 14번의 반복을 가지고, DXYS156Y 위치에서 12번의 반복을 가지는 식임을 보여줍니다. 관심 모집단에서 특정 일배체형의 기저 비율 (p)를 알아내기 위해 (n=300)명의 남성을 일배체형 분석하고 싶다고 가정해 봅시다; 그리고 그중 (y=40)명에게서 H1을 발견했다고 합시다. 우리는 이를 모델링하기 위해 이항 분포 (B(n,p))를 사용할 것이며, 여기서 (p)는 알려져 있지 않습니다.\n이러한 Y-STR 프로필을 사용하여 생성된 일배체형은 동일한 부계 혈통의 남성들 사이에 공유됩니다. 따라서 두 명의 서로 다른 남성이 동일한 프로필을 공유하는 것이 가능합니다.\n\n\n4.8.2 2.9.2 이항 분포에 대한 베이지안 패러다임의 시뮬레이션 연구\n우리 매개변수 (p)가 단 하나의 값(예: 최대 우도 추정치인 40/300)만을 가진다고 가정하는 대신, 베이지안 접근 방식은 이를 통계적 분포로부터의 추출로 볼 수 있게 해줍니다. 그 분포는 매개변수 (p)의 가능한 값들에 대한 우리의 믿음을 표현합니다. 원칙적으로 우리는 (p)에 대해 허용되는 가능한 값들을 가진 임의의 분포를 우리가 원하는 대로 사용할 수 있습니다. 여기서는 비율이나 확률을 나타내고 0과 1 사이의 값을 가지는 매개변수를 다루고 있으므로, 베타 분포(Beta distribution) 를 사용하는 것이 편리합니다. 그 밀도 공식은 다음과 같이 쓰여집니다.\n[ f_{,}(x) = (,)=. ]\n우리는 그림 2.19에서 이 함수가 두 매개변수 ()와 ()에 어떻게 의존하는지 볼 수 있으며, 이는 매우 유연한 분포군을 만듭니다(수많은 서로 다른 상황에 “적합”할 수 있음). 그리고 이는 멋진 수학적 속성을 가집니다: 만약 우리가 베타 모양의 (p)에 대한 사전 믿음으로 시작하여, (n)번의 이항 시행으로 이루어진 데이터 세트를 관찰하고, 그 후 우리의 믿음을 업데이트한다면, (p)에 대한 사후 분포 또한 (업데이트된 매개변수들에도 불구하고) 베타 분포를 가질 것입니다. 이것은 수학적 사실입니다. 우리는 여기서 이를 증명하지는 않겠지만, 시뮬레이션을 통해 보여줄 것입니다.\n\n그림 2.19: (, 20, 50) 및 (, 60, 150)인 베타 분포. 우리는 이들을 이항 실험에서의 성공 확률에 대한 사전 분포로 사용할 수 있습니다. 이 세 분포는 동일한 평균 (())을 가지지만, 평균 주변의 집중도는 다릅니다.\n\n\n4.8.3 2.9.3 (Y)의 분포\n주어진 (p)의 선택에 대해, 우리는 식 2.3에 의해 (Y)의 분포가 무엇인지 압니다. 하지만 만약 (p) 자체 또한 어떤 분포에 따라 변한다면 (Y)의 분포는 무엇일까요? 우리는 이를 (Y)의 주변 분포(marginal distribution)라고 부릅니다. 이를 시뮬레이션해 봅시다. 먼저 100,000개의 (p)에 대한 무작위 샘플 rp를 생성합니다. 그런 다음 각각에 대해 (Y)의 무작위 샘플을 생성하여 그림 2.20에 나타냅니다. 아래 코드에서는 시연을 위해 사전 분포의 매개변수로 50과 350을 사용합니다. 그러한 사전 분포는 이미 상당히 많은 정보를 담고 있으며(“뾰족함”), 예를 들어 이전 연구들에 기초한 우리의 믿음을 반영할 수 있습니다. 질문 2.20에서는 “더 부드러운”(정보가 적은) 사전 분포를 시도해 볼 기회가 있을 것입니다. 우리는 다시 vapply를 사용하여 익명의 x 함수를 rp의 모든 요소에 적용하여 결과적으로 동일한 길이의 또 다른 벡터 y를 얻습니다.\nrp = rbeta(100000, 50, 350)\ny = vapply(rp, \n           function(x) rbinom(1, prob = x, size = 300), \n           integer(1))\nhist(y, breaks = 50, col = \"orange\", main = \"\", xlab = \"\")\n\n그림 2.20: (Y)의 주변 분포.\n질문 2.18\nR의 벡터화 기능을 사용하고 rbinom(length(rp), rp, size = 300)이라고 씀으로써 위의 코드 청크에서와 동일한 결과를 얻을 수 있는지 확인해 보세요.\n해결책\nset.seed(0xbebe)\ny1 = vapply(rp, \n            function(x) rbinom(1, prob = x, size = 300), \n            integer(1))\nset.seed(0xbebe)\ny2 = rbinom(length(rp), rp, size = 300)\nstopifnot(identical(y1, y2))\n\n\n4.8.4 2.9.4 (Y=40)을 만족하는 모든 (p)의 히스토그램: 사후 분포\n이제 (Y)가 40인 결과들을 조건부로 하여 (p)의 사후 분포를 계산해 봅시다. 이를 이론적인 사후 분포인 densPostTheory와 비교해 볼 것인데, 이에 대해서는 아래에서 더 자세히 다룹니다. 결과는 그림 2.21에 나와 있습니다.\npPostEmp = rp[ y == 40 ]\nhist(pPostEmp, breaks = 40, col = \"chartreuse4\", main = \"\",\n  probability = TRUE, xlab = \"사후 p\")\n\np_seq = seq(0, 1, by = 0.001)\ndensPostTheory = dbeta(p_seq, 50 + 40, 350 + 260)\nlines(p_seq, densPostTheory, type = \"l\", lwd = 3)\n\n그림 2.21: (Y=40)인 분포의 값들만을 선택하면 (p)의 사후 분포를 얻을 수 있습니다. 히스토그램(초록색)은 사후 분포에 대한 시뮬레이션된 값들을 보여주고, 선은 이론적 매개변수를 사용한 베타 분포의 밀도를 나타냅니다.\n우리는 또한 위에서 계산된 두 분포의 평균을 확인하여 유효 숫자 4자리까지 가까움을 볼 수 있습니다.\nmean(pPostEmp)\n\n\n[1] 0.128726\n\n\ndp = p_seq[2] - p_seq[1]\nsum(p_seq * densPostTheory * dp)\n\n\n[1] 0.1285714\n이론적 밀도 densPostTheory의 평균을 근사하기 위해, 우리는 위에서 문자 그대로 적분인\n[ _0^1 p , f(p) , dp ]\n을 수치 적분, 즉 적분에 대한 sum을 사용하여 계산했습니다. 이는 특히 모델이 단일 스칼라 매개변수 (p)만을 포함하지 않고 많은 매개변수를 가져서 고차원 매개변수 벡터와 고차원 적분을 다루어야 할 때 항상 편리(하거나 실용적)하지는 않습니다. 만약 적분을 분석적으로 계산할 수 없다면, 우리는 몬테카를로 적분(Monte Carlo integration) 을 사용할 수 있습니다. 여러분은 위의 코드에서 몬테카를로 적분의 아주 간단한 사례를 이미 보셨는데, 거기서 우리는 pPostEmp로 사후 분포를 샘플링하고 R의 mean 함수를 호출하여 사후 평균을 계산하기 위해 통합을 수행했습니다. 이 경우, 대안적인 몬테카를로 알고리즘은 올바른 매개변수와 함께 rbeta 함수를 직접 사용하여 사후 샘플을 생성하는 것입니다.\npPostMC = rbeta(n = 100000, 90, 610)\nmean(pPostMC)\n\n\n[1] 0.1285718\n우리는 약간 다른 방식으로 생성된 몬테카를로 샘플 pPostMC와 pPostEmp 사이의 일치도를 분위수-분위수 플롯(quantile-quantile plot, QQ-plot , 그림 2.22)을 사용하여 확인할 수 있습니다.\nqqplot(pPostMC, pPostEmp, type = \"l\", asp = 1)\nabline(a = 0, b = 1, col = \"blue\")\n\n그림 2.22: 이론적 분포로부터 얻은 우리 몬테카를로 샘플 pPostMC와 우리 시뮬레이션 샘플 pPostEmp의 QQ(Quantile-Quantile) 플롯. 또한 이 두 분포 중 하나를 이론적 분포 함수 pbeta(., 90, 610)와 유사하게 비교할 수도 있습니다. 곡선이 (y=x) 선 위에 있다면 이는 좋은 일치를 나타냅니다. 꼬리 부분에는 약간의 무작위 차이가 있습니다.\n질문 2.19\npPostEmp로 이어지는 시뮬레이션과 pPostMC로 이어지는 몬테카를로 시뮬레이션의 차이는 무엇인가요?\n\n\n4.8.5 2.9.5 사후 분포 또한 베타 분포입니다.\n이제 우리는 사후 분포 또한 베타임을 확인했습니다. 우리 사례에서 그 매개변수 ()과 ()은 사전 매개변수 (, )에 관측된 성공 횟수 (y=40)과 관측된 실패 횟수 (n-y=260)를 각각 더함으로써 얻어졌습니다.\n[ (90,, 610)=(+y,+(n-y)). ]\n우리는 사후 분포에 의해 주어지는 불확실성과 함께 (p)에 대해 우리가 할 수 있는 최선의10 추정치를 제공하기 위해 이를 사용할 수 있습니다.\n10 사후 분포를 최대화하는 값을 최선의 추정치로 취할 수 있으며, 이를 MAP 추정치라고 합니다. 이 경우에는 (=)가 될 것입니다.\n\n\n4.8.6 2.9.6 두 번째 데이터 시리즈가 있다고 가정해 봅시다.\n이전 데이터를 본 후, 우리는 이제 새로운 사전 분포인 ((90, 610))을 가집니다. 우리가 (n=150)번의 관측을 하여 (y=25)번의 성공, 즉 125번의 실패를 얻은 새로운 데이터 세트를 수집한다고 가정해 봅시다. 이제 (p)에 대한 우리의 최선의 추측은 무엇이 될까요?\n이전과 동일한 추론을 사용하면, 새로운 사후 분포는 ((90+25=115,, 610+125=735))가 될 것입니다. 이 분포의 평균은 (=)이므로, (p)의 한 추정치는 0.135가 될 것입니다. 최대 사후(maximum a posteriori, MAP) 추정치는 ((115, 735))의 모드(mode), 즉 ()입니다. 이를 수치적으로 확인해 봅시다.\ndensPost2 = dbeta(p_seq, 115, 735)\nmcPost2   = rbeta(1e6, 115, 735)\nsum(p_seq * densPost2 * dp)   # 수치 적분에 의한 평균\n\n\n[1] 0.1352941\n\n\nmean(mcPost2)                 # MC에 의한 평균\n\n\n[1] 0.1352655\n\n\np_seq[which.max(densPost2)]   # MAP 추정치\n\n\n[1] 0.134\n질문 2.20\n우리의 원래 사전 분포를 덜 뾰족한(softer) 사전 분포로 대체하여 모든 계산을 다시 수행해 보세요. 즉, 사전 정보를 덜 사용한다는 의미입니다. 예를 들어, 균등 분포인 Beta(1,1)을 시도해 보세요. 이것이 최종 결과를 얼마나 변화시키나요?\n일반적인 규칙으로서, 사전 분포는 그것이 매우 뾰족하지 않은 한 사후 분포를 실질적으로 변화시키는 경우가 드뭅니다. 처음에 우리가 무엇을 기대할지 이미 상당히 확신하고 있었다면 그런 경우가 될 것입니다. 사전 분포가 영향을 미치는 또 다른 경우는 데이터가 매우 적을 때입니다.\n최선의 상황은 사전 분포를 압도할 만큼 충분한 데이터를 확보하여 사전 분포의 선택이 최종 결과에 큰 영향을 미치지 않도록 하는 것입니다.\n\n\n4.8.7 2.9.7 비율 매개변수에 대한 신뢰 진술\n이제 데이터가 주어졌을 때 비율 (p)가 실제로 무엇인지에 대해 결론을 내릴 차례입니다. 한 가지 요약 수치는 신뢰 구간(confidence interval)의 베이지안 대응물인 사후 신용 구간(posterior credibility interval)입니다. 우리는 사후 분포의 2.5% 및 97.5% 백분위수를 취할 수 있습니다: (P(q_{2.5%} p q_{97.5%})=0.95).\nquantile(mcPost2, c(0.025, 0.975))\n\n\n     2.5%     97.5% \n0.1131080 0.1590221 \n\n그림 2.23: Love, Huber, Anders (2014)의 예시는 초록색과 보라색 유전자에 대한 우도(실선, 1로 적분되도록 스케일 조정됨), 사후 분포(점선), 그리고 사전 분포(검은색 실선)의 플롯을 보여줍니다: 보라색 유전자의 더 높은 분산으로 인해 그 우도는 더 넓고 덜 뾰족하며(정보가 적음을 나타냄), 사전 분포가 초록색 유전자의 경우보다 그 사후 분포에 더 많은 영향을 미칩니다. 최댓값 지점에서의 초록색 사후 분포의 더 강한 곡률은 MAP 로그 폴드 변화(logarithmic fold change, LFC) 추정치(가로 오차 막대)에 대해 보고된 표준 오차가 더 작음을 의미합니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>매개변수가 핵심입니다.</span>"
    ]
  },
  {
    "objectID": "02-chap.html#예제-유전체에서의-뉴클레오타이드-패턴-발생",
    "href": "02-chap.html#예제-유전체에서의-뉴클레오타이드-패턴-발생",
    "title": "4  매개변수가 핵심입니다.",
    "section": "4.9 2.10 예제: 유전체에서의 뉴클레오타이드 패턴 발생",
    "text": "4.9 2.10 예제: 유전체에서의 뉴클레오타이드 패턴 발생\n지금까지 우리가 본 예제들은 이산형 카운트와 범주형 데이터의 분포에 집중되어 있었습니다. 이제 준연속형(quasi-continuous)인 거리 분포의 예제를 살펴봅시다. 유전체 서열에서의 특정 모티프 인스턴스 간 거리 분포에 대한 이 사례 연구는 우리가 바이오컨덕터에서의 구체적인 유전체 서열 조작을 탐색할 수 있게 해줄 것입니다.\nBiostrings 패키지는 서열 데이터 작업을 위한 도구들을 제공합니다. 필수적인 데이터 구조, 즉 R의 클래스(classes) 들은 DNAString 과 DNAStringSet 입니다. 이들은 우리가 하나 또는 여러 개의 DNA 서열을 효율적으로 다룰 수 있게 해줍니다.\nBiostrings 패키지는 또한 아미노산 서열과 더 일반적인 생물학 영감 서열들을 표현하기 위한 추가적인 클래스들을 포함하고 있습니다.\nlibrary(\"Biostrings\")\n질문 2.21\n튜토리얼 비네트(vignette)를 탐색하여 Biostrings 패키지에서 제공되는 유용한 데이터와 함수들을 일부 살펴보세요.\n해결책\n첫 번째 줄은 유전 코드 정보를 출력하고, 두 번째 줄은 IUPAC 뉴클레오타이드 모호성 코드를 반환합니다. 세 번째 줄은 Biostrings 패키지에서 가용한 모든 비네트를 나열하며, 네 번째 줄은 특정 비네트 하나를 표시합니다.\nGENETIC_CODE\nIUPAC_CODE_MAP\nvignette(package = \"Biostrings\")\nvignette(\"BiostringsQuickOverview\", package = \"Biostrings\")\n이 마지막 명령은 브라우저 창에서 설명서에 접근할 수 있는 목록을 열어줄 것입니다11. BSgenome 패키지는 많은 유전체에 대한 접근을 제공하며, 다음과 같이 입력하여 전체 유전체 서열을 포함하는 데이터 패키지들의 이름을 확인할 수 있습니다.\n11 비네트(Vignettes)는 예제와 사례 연구가 포함된 패키지 매뉴얼입니다.\nlibrary(\"BSgenome\")\nag = available.genomes()\nlength(ag)\n\n\n[1] 113\n\n\nag[1:2]\n\n\n[1] \"BSgenome.Alyrata.JGI.v1\"              \n[2] \"BSgenome.Amellifera.BeeBase.assembly4\"\n우리는 대장균(E.coli) 유전체에서의 AGGAGGT 모티프12 발생을 조사해 볼 것입니다. 우리는 특정 균주인 Escherichia coli str. K12 substr.DH10B13의 유전체 서열을 사용하며, 그 NCBI 기탁 번호는 NC_010473입니다.\n12 이것은 박테리아에서 단백질 합성을 개시하는 데 도움을 주는 Shine-Dalgarno 모티프입니다.\n13 실험실의 “일꾼”으로 알려져 있으며 실험에 자주 사용됩니다.\nlibrary(\"BSgenome.Ecoli.NCBI.20080805\")\nEcoli\nshineDalgarno = \"AGGAGGT\"\necoli = Ecoli$NC_010473\n우리는 countPattern 함수를 사용하여 너비 50,000의 윈도우에서 패턴의 발생 횟수를 셀 수 있습니다.\nwindow = 50000\nstarts = seq(1, length(ecoli) - window, by = window)\nends   = starts + window - 1\nnumMatches = vapply(seq_along(starts), function(i) {\n  countPattern(shineDalgarno, ecoli[starts[i]:ends[i]],\n               max.mismatch = 0)\n  }, numeric(1))\ntable(numMatches)\n\n\nnumMatches\n 0  1  2  3  4 \n48 32  8  3  2 \n질문 2.22\n이 표는 어떤 분포에 잘 맞을까요?\n해결책\n이러한 데이터에 대한 정량적 및 그래픽 평가(그림 2.24 참조)가 보여주듯이, 푸아송 분포가 좋은 후보입니다.\nlibrary(\"vcd\")\ngf = goodfit(numMatches, \"poisson\")\nsummary(gf)\n\n\n    Goodness-of-fit test for poisson distribution\n\n                      X^2 df  P(&gt; X^2)\nLikelihood Ratio 4.134932  3 0.2472577\n\n\ndistplot(numMatches, type = \"poisson\")\n\n그림 2.24: Ecoli$NC_010473 서열을 따른 모티프 카운트에 대한 푸아송 모델 평가.\n우리는 matchPattern 함수를 사용하여 일치하는 것들을 조사할 수 있습니다.\nsdMatches = matchPattern(shineDalgarno, ecoli, max.mismatch = 0)\nR 커맨드 라인에 sdMatches를 입력하여 이 객체에 대한 요약을 얻을 수 있습니다. 여기에는 원래 서열 상에서의 이른바 뷰(views) 들의 집합으로 표현된 모든 65개의 패턴 일치 위치들이 포함되어 있습니다. 이제 그들 사이의 거리는 얼마일까요?\nbetweenmotifs = gaps(sdMatches)\n따라서 이들은 사실 66개의 보충적인 영역들입니다. 이제 모티프들 사이의 간격(gap) 크기 분포에 대한 모델을 찾아봅시다. 만약 모티프들이 무작위 위치에서 발생한다면, 우리는 간격 길이가 지수 분포(exponential distribution)를 따를 것으로 기대합니다14. 아래 코드(그 출력은 그림 2.25에 나타남)는 이 가정을 평가합니다. 만약 지수 분포가 잘 맞는다면, 점들은 대략 직선 위에 놓여야 합니다. 지수 분포는 비율(rate)이라는 하나의 매개변수를 가지며, 데이터로부터 얻은 추정치에 해당하는 기울기를 가진 선 또한 표시되었습니다.\n14 여기서 왜 지수 분포가 적합한지 어떻게 추측할 수 있을까요? 서열을 따라 독립적이고 무작위적인 베르누이 발생이 있을 때마다, 간격 길이는 지수 분포를 따릅니다. 여러분은 방사성 붕괴에 익숙하실 텐데, 거기서 방출 사이의 대기 시간 또한 지수 분포를 따릅니다. 이 분포가 생소하시다면 위키백과에서 더 자세한 내용을 찾아보시기를 추천합니다.\nlibrary(\"Renext\")\nexpplot(width(betweenmotifs), rate = 1/mean(width(betweenmotifs)),\n        labels = \"fit\")\n\n그림 2.25: 모티프 간 간격들의 지수 분포(검은색 실선) 적합도 평가.\n질문 2.23\n그림 2.25의 적합된 선에서 분포의 오른쪽 꼬리, 즉 가장 큰 값들 부근에서 약간의 이탈이 있는 것으로 보입니다. 그 이유가 무엇일 수 있을까요?\n\n4.9.1 2.10.1 의존성이 있는 경우의 모델링\n2.8절에서 보았듯이, 뉴클레오타이드 서열은 종종 의존성을 갖습니다: 특정 위치에서 어떤 뉴클레오타이드를 보게 될 확률은 주변 서열에 의존하는 경향이 있습니다. 여기서 우리는 마르코프 체인(Markov chain) 을 사용하여 의존성 모델링을 실습해 볼 것입니다. 우리는 인간 유전체의 8번 염색체 영역들을 살펴보고, CpG15 섬(islands)이라 불리는 영역들과 나머지 영역들 사이의 차이점을 발견해 보려 합니다.\n15 CpG는 5’-C-phosphate-G-3’을 의미합니다; 즉, C가 가닥을 따라 인산염을 통해 G와 연결되어 있음을 뜻합니다 (이는 2.7절의 C-G 염기쌍과는 무관합니다). CpG 디뉴클레오타이드의 사이토신은 메틸화될 수 있으며, 이는 유전자 발현 수준을 변화시킵니다. 이러한 유형의 유전자 조절은 후생유전학(epigenetics) 의 일부입니다. 위키백과에서 더 많은 정보를 찾을 수 있습니다: CpG 사이트(CpG site) 및 후생유전학(epigenetics).\n우리는 유전체에서 섬들의 시작점과 끝점이 어디인지 알려주는 (Irizarry, Wu, Anders (2009)에 의해 생성된) 데이터를 사용하여, 뉴클레오타이드들과 ‘CG’, ‘CT’, ‘CA’, ’CC’와 같은 다이그램들의 빈도를 살펴볼 것입니다. 따라서 우리는 뉴클레오타이드 발생 사이에 의존성이 있는지, 그리고 있다면 이를 어떻게 모델링할지 물을 수 있습니다.\nlibrary(\"BSgenome.Hsapiens.UCSC.hg19\")\nchr8  =  Hsapiens$chr8\nCpGtab = read.table(\"../data/model-based-cpg-islands-hg19.txt\",\n                    header = TRUE)\nnrow(CpGtab)\n\n\n[1] 65699\n\n\nhead(CpGtab)\n\n\n    chr  start    end length CpGcount GCcontent pctGC obsExp\n1 chr10  93098  93818    721       32       403 0.559  0.572\n2 chr10  94002  94165    164       12        97 0.591  0.841\n3 chr10  94527  95302    776       65       538 0.693  0.702\n4 chr10 119652 120193    542       53       369 0.681  0.866\n5 chr10 122133 122621    489       51       339 0.693  0.880\n6 chr10 180265 180720    456       32       256 0.561  0.893\n\n\nirCpG = with(dplyr::filter(CpGtab, chr == \"chr8\"),\n         IRanges(start = start, end = end))\n\n\n\n우리는 dplyr 패키지로부터 명시적으로 filter 함수를 호출하기 위해 :: 연산자를 사용합니다 – 동일한 이름의 함수를 정의하고 있는 다른 패키지들이 로드되어 있을 수도 있기 때문입니다. 특히 filter 함수의 경우 이 명칭이 꽤 많은 다른 패키지들에서 사용되므로 이러한 예방 조치가 특히 권장됩니다. R 함수를 (:: 없이) 호출하는 일반적인 방식은 사람들을 이름으로 부르는 것과 유사하다고 생각할 수 있습니다; 반면 ::를 사용한 완전 수식 버전은 누군가를 성과 이름을 포함한 전체 이름으로 부르는 것에 해당합니다. 최소한 CRAN 및 바이오컨덕터 저장소 내에서는 이러한 완전 수식 이름들은 고유함이 보장됩니다.\n\n\n우리는 dplyr 패키지로부터 명시적으로 filter 함수를 호출하기 위해 :: 연산자를 사용합니다 – 동일한 이름의 함수를 정의하고 있는 다른 패키지들이 로드되어 있을 수도 있기 때문입니다. 특히 filter 함수의 경우 이 명칭이 꽤 많은 다른 패키지들에서 사용되므로 이러한 예방 조치가 특히 권장됩니다. R 함수를 (:: 없이) 호출하는 일반적인 방식은 사람들을 이름으로 부르는 것과 유사하다고 생각할 수 있습니다; 반면 ::를 사용한 완전 수식 버전은 누군가를 성과 이름을 포함한 전체 이름으로 부르는 것에 해당합니다. 최소한 CRAN 및 바이오컨덕터 저장소 내에서는 이러한 완전 수식 이름들은 고유함이 보장됩니다.\n위의 줄에서, 우리는 데이터 프레임 CpGtab을 8번 염색체로만 하위 집합화(filter)한 다음, 데이터 프레임의 동일한 이름의 열들에 의해 시작 및 종료 위치가 정의되는 IRanges 객체를 만듭니다. (인수들로부터 객체를 생성하는) IRanges 함수 호출에서, 첫 번째 start는 함수의 인수 이름이고, 두 번째 start는 filter의 출력으로 얻은 데이터 프레임의 열을 나타냅니다; end에 대해서도 마찬가지입니다. IRanges 는 수학적 구간들을 위한 일반적인 컨테이너입니다. 우리는 다음 줄로 생물학적 맥락을 만듭니다16.\n16 IRanges 에서의 “I”는 “구간(interval)”을 의미합니다; GRanges 에서의 “G”는 “유전체(genomic)”를 의미합니다.\ngrCpG = GRanges(ranges = irCpG, seqnames = \"chr8\", strand = \"+\")\ngenome(grCpG) = \"hg19\"\n이제 시각화해 봅시다; 그림 2.26의 출력을 보세요.\nlibrary(\"Gviz\")\nideo = IdeogramTrack(genome = \"hg19\", chromosome = \"chr8\")\nplotTracks(\n  list(GenomeAxisTrack(),\n    AnnotationTrack(grCpG, name = \"CpG\"), ideo),\n    from = 2200000, to = 5800000,\n    shape = \"box\", fill = \"#006400\", stacking = \"dense\")\n\n그림 2.26: Gviz를 이용한 8번 염색체의 선택된 영역에 있는 CpG 위치들의 플롯.\n우리는 이제 CpG 섬인 irCpG와 그 사이의 영역들(gaps(irCpG))에 해당하는 염색체 서열 상의 소위 뷰(views)들을 정의합니다. 결과로 나오는 객체 CGIview와 NonCGIview는 오직 좌표들만을 포함하며 서열 그 자체는 포함하지 않으므로 (이들은 큰 객체인 Hsapiens$chr8에 머뭅니다), 저장 공간 측면에서 상당히 가볍습니다.\nCGIview    = Views(unmasked(Hsapiens$chr8), irCpG)\nNonCGIview = Views(unmasked(Hsapiens$chr8), gaps(irCpG))\n우리는 데이터를 사용하여 CpG 섬과 비섬에서의 전이(transition) 횟수를 계산합니다.\nseqCGI      = as(CGIview, \"DNAStringSet\")\nseqNonCGI   = as(NonCGIview, \"DNAStringSet\")\ndinucCpG    = sapply(seqCGI, dinucleotideFrequency)\ndinucNonCpG = sapply(seqNonCGI, dinucleotideFrequency)\ndinucNonCpG[, 1]\n\n\n AA  AC  AG  AT  CA  CC  CG  CT  GA  GC  GG  GT  TA  TC  TG  TT \n389 351 400 436 498 560 112 603 359 336 403 336 330 527 519 485 \n\n\nNonICounts = rowSums(dinucNonCpG)\nIslCounts  = rowSums(dinucCpG)\n우리가 가진 4-상태 마르코프 체인의 경우, 행이 ‘from’ 상태이고 열이 ‘to’ 상태인 전이 행렬을 정의합니다.\nTI  = matrix( IslCounts, ncol = 4, byrow = TRUE)\nTnI = matrix(NonICounts, ncol = 4, byrow = TRUE)\ndimnames(TI) = dimnames(TnI) =\n  list(c(\"A\", \"C\", \"G\", \"T\"), c(\"A\", \"C\", \"G\", \"T\"))\n우리는 각 유형의 전이 횟수 카운트를 사용하여 빈도를 계산하고 이를 두 행렬에 담습니다.\n\n\n\n전이 확률은 확률이므로 행의 합이 1이어야 합니다.\n\n\n전이 확률은 확률이므로 행의 합이 1이어야 합니다.\nMI = TI /rowSums(TI)\nMI\n\n\n           A         C         G         T \nA 0.20457773 0.2652333 0.3897678 0.1404212 \nC 0.20128250 0.3442381 0.2371595 0.2173200 \nG 0.18657245 0.3145299 0.3450223 0.1538754 \nT 0.09802105 0.3352314 0.3598984 0.2068492 \n\n\nMN = TnI / rowSums(TnI)\nMN\n\n\n          A         C          G         T \nA 0.3351380 0.1680007 0.23080886 0.2660524 \nC 0.3641054 0.2464366 0.04177094 0.3476871 \nG 0.2976696 0.2029017 0.24655406 0.2528746 \nT 0.2265813 0.1972407 0.24117528 0.3350027\n질문 2.24\n서로 다른 행들에서 전이들이 다른가요? 이는 예를 들어 (P(,|,) P(,|,))인 상황을 의미합니다.\n해결책\n전이들이 다릅니다. 예를 들어, 섬(island) 내부의 전이 행렬(MI)에서 C에서 A로의 전이와 T에서 A로의 전이는 매우 다르게 보입니다 (0.201 대 0.098).\n질문 2.25\nCpG 섬에서의 서로 다른 뉴클레오타이드들의 상대적 빈도가 다른 곳과 비교했을 때 다른가요?\n해결책\nfreqIsl = alphabetFrequency(seqCGI, baseOnly = TRUE, collapse = TRUE)[1:4]\nfreqIsl / sum(freqIsl)\n\n\n        A         C         G         T \n0.1781693 0.3201109 0.3206298 0.1810901 \n\n\nfreqNon = alphabetFrequency(seqNonCGI, baseOnly = TRUE, collapse = TRUE)[1:4]\nfreqNon / sum(freqNon)\n\n\n        A         C         G         T \n0.3008292 0.1993832 0.1993737 0.3004139 \n이는 반대되는 패턴을 보여줍니다: CpG 섬에서는 C와 G가 약 0.32의 빈도를 가지는 반면, 비-CpG 섬에서는 A와 T가 약 0.30의 빈도를 가집니다.\n질문 2.26\n이러한 차이점들을 사용하여 주어진 서열이 CpG 섬으로부터 오는지 여부를 어떻게 결정할 수 있을까요?\n해결책\n관찰된 빈도와 freqIsl 및 freqNon 빈도 사이를 비교하기 위해 (^2) 통계량을 사용합니다. 더 짧은 서열의 경우, 이는 충분히 민감하지 않을 수 있으며 아래에 더 민감한 접근 방식이 제시됩니다.\n어떤 서열이 CpG 섬 내부에 있는지 아닌지 모르는 상태로 주어졌을 때, 우리는 그 서열이 다른 곳과 비교했을 때 CpG 섬에 속할 확률이 얼마나 되는지 물을 수 있습니다. 우리는 오즈 비(odds ratio)에 기반하여 점수를 계산합니다. 예제를 하나 들어봅시다: 우리 서열 (x)가 ACGTTATACTACG이고, 이것이 CpG 섬으로부터 오는지 여부를 결정하고 싶다고 가정해 봅시다.\n서열을 1차 마르코프 체인으로 모델링한다면, 그 서열이 CpG 섬으로부터 온다고 가정할 때 다음과 같이 쓸 수 있습니다:\n[ \\[\\begin{align} P_{\\text{i}}(x = \\mathtt{ACGTTATACTACG}) = \\; &P_{\\text{i}}(\\mathtt{A}) \\, P_{\\text{i}}(\\mathtt{AC})\\, P_{\\text{i}}(\\mathtt{CG})\\, P_{\\text{i}}(\\mathtt{GT})\\, P_{\\text{i}}(\\mathtt{TT}) \\times \\\\ &P_{\\text{i}}(\\mathtt{TA})\\, P_{\\text{i}}(\\mathtt{AT})\\, P_{\\text{i}}(\\mathtt{TA})\\, P_{\\text{i}}(\\mathtt{AC})\\, P_{\\text{i}}(\\mathtt{CG}). \\end{align}\\] ]\n우리는 이 확률을 섬이 아닌 곳에 대한 확률과 비교할 것입니다. 위에서 보았듯이, 이러한 확률들은 상당히 다른 경향이 있습니다. 우리는 그들의 비율을 취해서 그것이 1보다 큰지 작은지 볼 것입니다. 이 확률들은 많은 작은 항들의 곱이 될 것이며 매우 작아질 것입니다. 우리는 로그를 취함으로써 이 문제를 해결할 수 있습니다.\n[ \\[\\begin{align} \\log&\\frac{P(x\\,|\\, \\text{섬})}{P(x\\,|\\,\\text{비섬})}=\\\\\\\\log&\\left( \\frac{P_{\\text{i}}(\\mathtt{A})\\, P_{\\text{i}}(\\mathtt{A}\\rightarrow \\mathtt{C})\\, P_{\\text{i}}(\\mathtt{C}\\rightarrow \\mathtt{G})\\, P_{\\text{i}}(\\mathtt{G}\\rightarrow \\mathtt{T})\\, P_{\\text{i}}(\\mathtt{T}\\rightarrow \\mathtt{T})\\, P_{\\text{i}}(\\mathtt{T}\\rightarrow \\mathtt{A})} {P_{\\text{n}}(\\mathtt{A})\\, P_{\\text{n}}(\\mathtt{A}\\rightarrow \\mathtt{C})\\, P_{\\text{n}}(\\mathtt{C}\\rightarrow \\mathtt{G})\\, P_{\\text{n}}(\\mathtt{G}\\rightarrow \\mathtt{T})\\, P_{\\text{n}}( \\mathtt{T}\\rightarrow \\mathtt{T})\\, P_{\\text{n}}( \\mathtt{T}\\rightarrow \\mathtt{A})} \\right. \\times\\\\ &\\left.\\mathtt{A}\\rightarrow \\mathtt{T})\\, P_{\\text{i}}(\\mathtt{T}\\rightarrow \\mathtt{A})\\, P_{\\text{i}}(\\mathtt{A}\\rightarrow \\mathtt{C})\\, P_{\\text{i}}(\\mathtt{C}\\rightarrow \\mathtt{G})} {P_{\\text{n}}(\\mathtt{A}\\rightarrow \\mathtt{T})\\, P_{\\text{n}}(\\mathtt{T}\\rightarrow \\mathtt{A})\\, P_{\\text{n}}(\\mathtt{A}\\rightarrow \\mathtt{C})\\, P_{\\text{n}}(\\mathtt{C}\\rightarrow \\mathtt{G})} \\right) \\end{align}\\] ]\n이것이 로그 우도 비(log-likelihood ratio) 점수입니다. 계산 속도를 높이기 위해, 우리는 로그 비율들인 ((P_{}()/P_{}()),…, (P_{}()/P_{}()))을 한 번에 계산한 다음, 관련 있는 것들을 모두 더하여 우리 점수를 얻습니다.\n\n\n\n연구된 예제들과 많은 유용한 세부 사항들을 Durbin 등(1998)에서 찾아볼 수 있습니다.\n\n\n연구된 예제들과 많은 유용한 세부 사항들을 Durbin 등 (1998)에서 찾아볼 수 있습니다.\nalpha = log((freqIsl/sum(freqIsl)) / (freqNon/sum(freqNon)))\nbeta  = log(MI / MN)\n\n\n\nx = \"ACGTTATACTACG\"\nscorefun = function(x) {\n  s = unlist(strsplit(x, \"\"))\n  score = alpha[s[1]]\n  if (length(s) &gt;= 2)\n    for (j in 2:length(s))\n      score = score + beta[s[j-1], s[j]]\n  score\n}\nscorefun(x)\n\n\n         A \n-0.2824623 \n아래 코드에서, 우리는 seqCGI 객체의 2855개 서열 중 100 길이(len = 100)의 서열들을 뽑고, 그 다음 seqNonCGI 객체의 2854개 서열 중에서도 마찬가지로 뽑습니다 (각각은 DNAStringSet 입니다). generateRandomScores 함수의 처음 세 줄에서, 우리는 A, C, T, G 이외의 글자(예: 정의되지 않은 뉴클레오타이드에 사용되는 글자인 “.”)를 포함하는 서열들을 버립니다. 남은 서열들 중에서 서열 길이에 len을 뺀 값에 비례하는 확률로 샘플링한 다음, 그중에서 100 길이의 하위 서열을 뽑습니다. 하위 서열의 시작점들은 하위 서열이 전체 서열 내에 들어와야 한다는 제약 조건을 두고 균등하게 샘플링됩니다.\ngenerateRandomScores = function(s, len = 100, B = 1000) {\n  alphFreq = alphabetFrequency(s)\n  isGoodSeq = rowSums(alphFreq[, 5:ncol(alphFreq)]) == 0\n  s = s[isGoodSeq]\n  slen = sapply(s, length)\n  prob = pmax(slen - len, 0)\n  prob = prob / sum(prob)\n  idx  = sample(length(s), B, replace = TRUE, prob = prob)\n  ssmp = s[idx]\n  start = sapply(ssmp, function(x) sample(length(x) - len, 1))\n  scores = sapply(seq_len(B), function(i)\n    scorefun(as.character(ssmp[[i]][start[i]+(1:len)]))\n  )\n  scores / len\n}\nscoresCGI    = generateRandomScores(seqCGI)\nscoresNonCGI = generateRandomScores(seqNonCGI)\n\n\n\nrgs = range(c(scoresCGI, scoresNonCGI))\nbr = seq(rgs[1], rgs[2], length.out = 50)\nh1 = hist(scoresCGI,    breaks = br, plot = FALSE)\nh2 = hist(scoresNonCGI, breaks = br, plot = FALSE)\nplot(h1, col = rgb(0, 0, 1, 1/4), xlim = c(-0.5, 0.5), ylim=c(0,120))\nplot(h2, col = rgb(1, 0, 0, 1/4), add = TRUE)\n\n그림 2.27: generateRandomScores 함수에 의해 생성된 섬 및 비섬 점수. 이는 우리가 접하게 되는 첫 번째 혼합물(mixture) 사례입니다. 우리는 4장에서 이들을 다시 방문할 것입니다.\n우리는 이들을 우리의 훈련 데이터(training data) 로 간주할 수 있습니다: 유형을 알고 있는 데이터로부터, 우리는 우리 점수가 분류(discrimination)에 유용한지 확인할 수 있습니다 – 그림 2.27을 보세요.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>매개변수가 핵심입니다.</span>"
    ]
  },
  {
    "objectID": "02-chap.html#이-장의-요약",
    "href": "02-chap.html#이-장의-요약",
    "title": "4  매개변수가 핵심입니다.",
    "section": "4.10 2.11 이 장의 요약",
    "text": "4.10 2.11 이 장의 요약\n이 장에서 우리는 통계학의 기본적인 훈련(yoga)을 경험했습니다: 어떻게 데이터로부터 가능한 생성 분포로 거슬러 올라가는지, 그리고 이러한 분포를 정의하는 매개변수들을 어떻게 추정하는지를 보았습니다.\n통계 모델 우리는 범주형 결과(이항 및 다항) 실험을 위한 몇 가지 구체적인 통계 모델들을 보여주었습니다.\n적합도 우리는 다양한 시각화 기법을 사용하고 시뮬레이션 실험을 실행하여 우리 데이터가 공정한 4-박스 다항 모델에 적합될 수 있는지 테스트하는 방법을 보았습니다. 우리는 카이제곱 통계량을 접했고 QQ-플롯을 사용하여 시뮬레이션과 이론을 비교하는 법을 살펴보았습니다.\n추정 최대 우도와 베이지안 추정 절차에 대해 설명했습니다. 이러한 접근 방식들은 뉴클레오타이드 패턴 발견과 일배체형 추정을 포함하는 예제들을 통해 예시되었습니다.\n사전 및 사후 분포 일배체형과 같이 이전에 연구된 적이 있는 유형의 데이터를 평가할 때, 데이터의 사후 분포를 계산하는 것이 유익할 수 있습니다. 이는 간단한 계산을 통해 의사 결정 과정에 불확실성을 통합할 수 있게 해줍니다. 충분한 데이터가 있는 한 사전 분포의 선택은 결과에 거의 영향을 미치지 않습니다.\nCpG 섬과 마르코프 체인 우리는 DNA 서열을 따른 의존성이 마르코프 체인 전이에 의해 어떻게 모델링될 수 있는지 보았습니다. 우리는 이를 사용하여 긴 DNA 서열이 CpG 섬으로부터 오는지 여부를 결정할 수 있게 해주는 우도 비에 기반한 점수를 구축했습니다. 우리가 점수들의 히스토그램을 만들었을 때, 그림 2.27에서 주목할 만한 특징을 보았습니다: 그것은 두 개의 조각으로 만들어진 것처럼 보였습니다. 이 이봉성(bimodality) 은 혼합물과의 첫 번째 만남이었으며, 이들은 4장의 주제입니다.\n이것은 일부 훈련 데이터 상에서 모델을 구축하는 첫 번째 사례입니다: 우리가 CpG 섬 내부에 있다는 것을 알고 있는 서열들은 나중에 새로운 데이터를 분류하는 데 사용될 수 있습니다. 우리는 12장에서 이를 수행하는 훨씬 더 완전한 방법을 개발할 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>매개변수가 핵심입니다.</span>"
    ]
  },
  {
    "objectID": "02-chap.html#더-읽을거리",
    "href": "02-chap.html#더-읽을거리",
    "title": "4  매개변수가 핵심입니다.",
    "section": "4.11 2.12 더 읽을거리",
    "text": "4.11 2.12 더 읽을거리\n가장 훌륭한 기초 통계학 서적 중 하나는 Freedman, Pisani, Purves (1997)입니다. 이 책은 중요한 개념들을 설명하기 위해 상자 모델을 사용합니다. 통계학 수업을 들어본 적이 없거나 복습이 필요하다고 느낀다면, 이 책을 강력히 추천합니다. 많은 기초 통계학 수업들은 이산형 데이터에 대한 통계를 심도 있게 다루지 않습니다. 이 주제는 생물학적 응용 분야에 필요한 중요한 부분입니다. 이러한 유형의 분석들에 대한 한 권으로 된 입문서는 (Agresti 2007)에서 찾아볼 수 있습니다.\n여기서 우리는 간단하고 구조화되지 않은 다항 분포의 예제들을 들었습니다. 하지만 때때로 다항 분포의 범주들(또는 상자들)은 특정한 구조를 가집니다. 예를 들어, 64개의 가능한 코돈은 20개의 아미노산과 종결 코돈(61+3)을 코딩합니다. 그래서 우리는 아미노산 자체를 20 자유도의 다항 분포로 볼 수 있습니다. 각 아미노산 내에는 서로 다른 수의 범주를 가진 다항 분포들이 존재합니다 (프롤린은 CCA, CCG, CCC, CCT 네 가지를 가집니다, 연습 문제 2.3 참조). 일부 다변량 방법들은 서로 다르게 풍부한 아미노산들 내에서의 코돈 사용 간의 가변성을 분해하기 위해 특별히 고안되었으며 (Grantham et al. 1981; Perrière and Thiouluse 2002), 이는 수평적 유전자 전달과 번역 선택의 발견을 가능하게 합니다. 우리는 9장에서 범주형 데이터의 다변량 탐색을 깊이 파고들 때 해당 논문들에서 사용된 구체적인 방법들을 다룰 것입니다.\n불확실성을 정량화하기 위해 베이지안 패러다임을 성공적으로 사용한 많은 예시들이 있습니다. 최근 몇 년 동안 사후 분포의 계산은 마르코프 체인이나 무작위 보행(random walk), 또는 해밀턴 동역학(Hamiltonian dynamics)을 사용하는 특수한 유형의 몬테카를로 방법에 의해 혁신을 일으켰습니다. 이러한 방법들은 수차례의 반복 후에 올바른 사후 분포로 수렴하는 근사치를 제공합니다. 예제들과 더 많은 내용은 (Robert and Casella 2009; Marin and Robert 2007; McElreath 2015)를 참조하십시오.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>매개변수가 핵심입니다.</span>"
    ]
  },
  {
    "objectID": "02-chap.html#연습-문제",
    "href": "02-chap.html#연습-문제",
    "title": "4  매개변수가 핵심입니다.",
    "section": "4.12 2.13 연습 문제",
    "text": "4.12 2.13 연습 문제\n연습 문제 2.1\n1,000개의 글자 길이를 가진 유전자 서열을 따라 발생하는 돌연변이를 모델링하는 1,000개의 무작위 0/1 변수들을 생성하세요. 이들은 각각 (10^{-4})의 비율로 독립적으로 발생합니다. 그런 다음 1,000개 위치의 합을 구하여 1,000 길이의 서열에 얼마나 많은 돌연변이가 있는지 계산하세요.\n적합도 검정을 사용하여 이러한 돌연변이 합계에 대한 올바른 분포를 찾고 적합도의 품질을 시각화하는 플롯을 만드세요.\n연습 문제 2.2\n(0)과 (7) 사이에서 (n)개의 무작위 균등 분포 숫자를 생성하고 그중 최댓값을 반환하는 함수를 만드세요. (n=25)에 대해 함수를 실행하세요. 이 절차를 (B=100)번 반복하세요. 이러한 최댓값들의 분포를 플롯하세요.\n크기 25인 표본의 최대 우도 추정치(이를 ()라고 부릅시다)는 무엇인가요?\n이론적인 정당성과 실제 최댓값 ()를 찾아낼 수 있나요?\n연습 문제 2.3\n유전자의 코딩 영역에서 가져온 세 개의 뉴클레오타이드 시퀀스(코돈)는 20가지의 가능한 아미노산 중 하나로 전사될 수 있습니다. (4^3=64)가지의 가능한 코돈 서열이 있지만, 아미노산은 20가지뿐입니다. 우리는 유전 코드 가 중복(redundant)된다고 말합니다: 각 아미노산을 철자하는 방법은 여러 가지가 있습니다.\n다중성(동일한 아미노산을 코딩하는 코돈의 수)은 2에서 6까지 다양합니다. 각 아미노산의 서로 다른 코돈 철자들은 동일한 확률로 발생하지 않습니다. 표준 실험실 균주인 결핵균(H37Rv)에 대한 데이터를 살펴봅시다:\nmtb = read.table(\"../data/M_tuberculosis.txt\", header = TRUE)\nhead(mtb, n = 4)\n\n\n  AmAcid Codon Number PerThous\n1    Gly   GGG  25874    19.25\n2    Gly   GGA  13306     9.90\n3    Gly   GGT  25320    18.84\n4    Gly   GGC  68310    50.82\n아미노산 프롤린(proline)에 대한 코돈들은 (CC*) 형태이며, 결핵균(Mycobacterium tuberculosis)에서 다음과 같은 빈도로 발생합니다:\npro  =  mtb[ mtb$AmAcid == \"Pro\", \"Number\"]\npro/sum(pro)\n\n\n[1] 0.54302025 0.10532985 0.05859765 0.29305225\n\ntable을 사용하여 AmAcid와 Codon 변수를 집계하여 데이터 mtb를 탐색하세요.\nPerThous 변수는 어떻게 만들어졌나요?\n가능한 철자들 사이의 균등 분포로부터의 이탈이 가장 큰, 즉 가장 강한 코돈 편향(codon bias) 을 보이는 아미노산이 무엇인지 찾기 위해 여러분이 표에 적용할 수 있는 R 함수를 작성하세요.\n\n(*)은 정규 표현식의 컴퓨터 표기법을 사용하여 임의의 4개 글자 중 하나를 나타냅니다.\n연습 문제 2.4\n황색포도상구균(Staphylococcus Aureus) 서열을 따라 움직이는 윈도우에서 GC 함량을 표시하세요. 파일로부터 fasta 파일 서열을 읽어 들입니다.\nstaph = readDNAStringSet(\"../data/staphsequence.ffn.txt\", \"fasta\")\n\n전체 staph 객체를 살펴본 다음 세트의 처음 세 서열을 표시하세요.\n너비 100의 슬라이딩 윈도우를 따라 GC 함량을 찾으세요.\nb)의 결과를 표시하세요.\n서열을 따른 이러한 비율의 전반적인 추세를 어떻게 시각화할 수 있을까요?\n\n해결책\n\n데이터는 다음과 같이 표시됩니다:\n\nstaph[1:3, ]\n\n\nDNAStringSet object of length 3:\n    width seq                                               names               \n[1]  1362 ATGTCGGAAAAAGAAATTTGGGA...AAAAAGAAATAAGAAATGTATAA lcl|NC_002952.2_c...\n[2]  1134 ATGATGGAATTCACTATTAAAAG...TTTTACCAATCAGAACTTACTAA lcl|NC_002952.2_c...\n[3]   246 GTGATTATTTTGGTTCAAGAAGT...TCATTCATCAAGGTGAACAATGA lcl|NC_002952.2_c...\n\n\nstaph\n\n\nDNAStringSet object of length 2650:\n       width seq                                            names               \n   [1]  1362 ATGTCGGAAAAAGAAATTTGGG...AAAGAAATAAGAAATGTATAA lcl|NC_002952.2_c...\n   [2]  1134 ATGATGGAATTCACTATTAAAA...TTACCAATCAGAACTTACTAA lcl|NC_002952.2_c...\n   [3]   246 GTGATTATTTTGGTTCAAGAAG...ATTCATCAAGGTGAACAATGA lcl|NC_002952.2_c...\n   [4]  1113 ATGAAGTTAAATACACTCCAAT...CAAGGTGAAATTATAAAGTAA lcl|NC_002952.2_c...\n   [5]  1932 GTGACTGCATTGTCAGATGTAA...TATGCAAACTTAGACTTCTAA lcl|NC_002952.2_c...\n   ...   ... ...\n[2646]   720 ATGACTGTAGAATGGTTAGCAG...ACTCCTTTACTTGAAAAATAA lcl|NC_002952.2_c...\n[2647]  1878 GTGGTTCAAGAATATGATGTAA...CTCCAAAGGGTGAGTGACTAA lcl|NC_002952.2_c...\n[2648]  1380 ATGGATTTAGATACAATTACGA...CAATTCTGCTTAGGTAAATAG lcl|NC_002952.2_c...\n[2649]   348 TTGGAAAAAGCTTACCGAATTA...TTTAATAAAAAGATTAAGTAA lcl|NC_002952.2_c...\n[2650]   138 ATGGTAAAACGTACTTATCAAC...CGTAAAGTTTTATCTGCATAA lcl|NC_002952.2_c...\n\nletterFrequency 함수를 사용하여 빈도를 계산할 수 있습니다.\n\nletterFrequency(staph[[1]], letters = \"ACGT\", OR = 0)\n\n\n  A   C   G   T \n522 219 229 392 \n\n\nGCstaph = data.frame(\n  ID = names(staph),\n  GC = rowSums(alphabetFrequency(staph)[, 2:3] / width(staph)) * 100\n)\n\n플로팅은 다음과 같이 수행될 수 있으며, 여기서는 예시로 364번 서열을 사용합니다 (그림 2.28):\n\nwindow = 100\ngc = rowSums( letterFrequencyInSlidingView(staph[[364]], window,\n      c(\"G\",\"C\")))/window\nplot(x = seq(along = gc), y = gc, type = \"l\")\n\n그림 2.28: 황색포도상구균 유전체의 364번 서열을 따른 GC 함량.\n\n윈도우를 따라 lowess 함수를 사용하여 데이터를 평활화함으로써 전반적인 추세를 살펴볼 수 있습니다.\n\nplot(x = seq(along = gc), y = gc, type = \"l\")\nlines(lowess(x = seq(along = gc), y = gc, f = 0.2), col = 2)\n\n그림 2.29: 평활화가 적용된 그림 2.28과 유사한 플롯.\n우리는 나중에 서열을 따라 이동함에 따라 우리가 항상 여러 가능한 상태(states) 중 하나에 있다는 아이디어를 사용하여, 해당 윈도우가 비정상적으로 높은 GC 함량을 가지고 있는지 결정하는 적절한 방법을 보게 될 것입니다. 하지만 우리는 그 상태를 직접 관찰하지 못하고 오직 서열만을 관찰합니다. 그러한 모델들을 은닉 (상태) 마르코프 모델(hidden (state) Markov models) , 줄여서 HMM이라고 부릅니다 (위키백과 참조). 이러한 모델 명칭에서의 마르코프 는 이웃한 위치들 사이의 의존성을 모델링하는 방식을 뜻하며, 은닉 부분은 상태가 직접 관찰되지 않고 숨겨져 있음을 나타냅니다.\n연습 문제 2.5\n그림 2.19와 유사한 그림을 다시 그리되, 두 가지 다른 분포를 포함시키세요: 균등 분포(Beta(1,1)임)와 Beta(\\(1/2, 1/2\\))입니다. 무엇을 발견했나요?\n해결책\ndfbetas = data.frame(\n  p = rep(p_seq, 5),\n  dbeta = c(dbeta(p_seq, 0.5, 0.5), \n            dbeta(p_seq,   1,   1), \n            dbeta(p_seq,  10,  30),\n            dbeta(p_seq,  20,  60), \n            dbeta(p_seq,  50, 150)),\n  pars = rep(c(\"Beta(0.5,0.5)\", \"U(0,1)=Beta(1,1)\", \n               \"Beta(10,30)\", \"Beta(20,60)\", \n               \"Beta(50,150)\"), each = length(p_seq)))\nggplot(dfbetas) +\n  geom_line(aes(x = p, y = dbeta, colour = pars)) +\n  theme(legend.title = element_blank()) +\n  geom_vline(aes(xintercept = 0.25), colour = \"#990000\", linetype = \"dashed\")\n\n그림 2.30: 서로 다른 매개변수 선택에 대한 베타 밀도 함수.\n매개변수가 1보다 큰 베타 분포들이 단봉형(unimodal)인 반면, Beta(0.5, 0.5) 분포는 이봉형(bimodal)이고 Beta(1, 1)은 평평하며 모드가 없습니다.\n연습 문제 2.6\n베타 분포의 매개변수들에 대해 여러분만의 사전 분포를 선택해 보세요. https://jhubiostatistics.shinyapps.io/drawyourprior 에서 직접 그려볼 수 있습니다. 사전 분포를 설정했다면, (n=300)번의 시행 중 (Y=40)번의 성공을 보았던 2.9.1절의 데이터를 다시 분석하세요. 여러분의 사후 분포를 QQ-플롯을 사용하여 해당 섹션에서 우리가 얻었던 것과 비교해 보세요.\nAgresti, Alan. 2007. An Introduction to Categorical Data Analysis. John Wiley.\nCannings, Chris, and Anthony WF Edwards. 1968. “Natural Selection and the de Finetti Diagram.” Annals of Human Genetics 31 (4): 421–28.\nCleveland, William S. 1988. The Collected Works of John w. Tukey: Graphics 1965-1985. Vol. 5. CRC Press.\nDurbin, Richard, Sean Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis. Cambridge University Press.\nElson, D, and E Chargaff. 1952. “On the Desoxyribonucleic Acid Content of Sea Urchin Gametes.” Experientia 8 (4): 143–45.\nFinetti, Bruno de. 1926. “Considerazioni Matematiche Sull’ereditarieta Mendeliana.” Metron 6: 3–41.\nFreedman, David, Robert Pisani, and Roger Purves. 1997. Statistics. New York, NY: WW Norton.\nGrantham, Richard, Christian Gautier, Manolo Gouy, M Jacobzone, and R Mercier. 1981. “Codon Catalog Usage Is a Genome Strategy Modulated for Gene Expressivity.” Nucleic Acids Research 9 (1): 213–13.\nIrizarry, Rafael A, Hao Wu, and Andrew P Feinberg. 2009. “A Species- Generalized Probabilistic Model-Based Definition of CpG Islands.” Mammalian Genome 20 (9-10): 674–80.\nLove, Michael I, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation of Fold Change and Dispersion for RNA-seq Data with DESeq2.” Gnome Biology 15 (12): 1–21.\nMarin, Jean-Michel, and Christian Robert. 2007. Bayesian Core: A Practical Approach to Computational Bayesian Statistics. Springer Science & Business Media.\nMcElreath, Richard. 2015. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Chapman; Hall/CRC.\nMourant, AE, Ada Kopec, and K Domaniewska-Sobczak. 1976. “The Distribution of the Human Blood Groups 2nd Edition.” Oxford University Press London.\nPerrière, Guy, and Jean Thiouluse. 2002. “Use and Misuse of Correspondence Analysis in Codon Usage Studies.” Nucleic Acids Research 30 (20): 4548–55.\nRice, John. 2006. Mathematical Statistics and Data Analysis. Cengage Learning.\nRobert, Christian, and George Casella. 2009. Introducing Monte Carlo Methods with R. Springer Science & Business Media.\n페이지는 R 버전 4.5.1 (2025-06-13)을 사용하여 2025-09-01 01:33에 빌드되었습니다. ## 2.9 요약\n이 장에서는 데이터로부터 모델 파라미터를 추정하는 두 가지 주요 접근 방식인 최대 우도 추정(Maximum Likelihood Estimation, MLE)과 베이지안(Bayesian) 추론을 소개했습니다.\n\nMLE: 관측된 데이터를 얻을 가능성(우도)을 최대화하는 파라미터 값을 찾습니다.\n베이지안 추론: 파라미터에 대한 사전 지식(사전 분포)을 데이터(우도)와 결합하여 사후 분포를 업데이트합니다.\n\n또한 통계적 모델링에서 이항 분포, 푸아송 분포 등 다양한 확률 분포가 어떻게 사용되는지 살펴보았습니다. 이러한 기초 지식은 이후 장에서 다룰 더 복잡한 생물학적 데이터 분석의 토대가 됩니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>매개변수가 핵심입니다.</span>"
    ]
  },
  {
    "objectID": "02-chap.html#footnotes",
    "href": "02-chap.html#footnotes",
    "title": "4  매개변수가 핵심입니다.",
    "section": "",
    "text": "동일한 추론이 역으로도 적용될 수 있습니다: 우리는 오늘로부터 어제의 날씨를 “예측”할 수도 있습니다.↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>매개변수가 핵심입니다.</span>"
    ]
  },
  {
    "objectID": "03-chap.html",
    "href": "03-chap.html",
    "title": "5  고품질 그래픽 (High-Quality Graphics)",
    "section": "",
    "text": "5.1 3.2 기본 R 플로팅\n데이터 시각화에는 (적어도) 두 가지 유형이 있습니다. 첫 번째는 과학자가 데이터를 탐색하고 작동 중인 복잡한 과정에 대한 발견을 할 수 있게 해줍니다. 다른 유형의 시각화는 과학자가 다른 사람들에게 보여주고 궁극적으로 출판물에 포함할 수 있는, 결과에 대한 유익하고 명확하며 시각적으로 매력적인 삽화를 제공합니다.\n이 두 가지 유형의 시각화 모두 R로 만들 수 있습니다. 사실 R은 여러 그래픽 시스템을 제공합니다. 이는 R이 확장 가능하기 때문이며, 수년간 R 그래픽의 발전이 기존 함수를 대체하는 것이 아니라 주로 패키지를 추가함으로써 진행되었기 때문입니다. 각각의 서로 다른 그래픽 시스템은 저마다의 장점과 한계가 있습니다. 이 장에서 우리는 그중 두 가지를 알아볼 것입니다. 먼저, 기본 R 플로팅(plotting) 함수들을 간단히 살펴봅니다1. 그 후에는 ggplot2로 전환할 것입니다.\n1 이들은 모든 기본 R 설치 시 함께 제공되는 graphics 패키지에 들어 있습니다.\n그림 3.1: ZUSE 플로터 Z64 (1961년 발표). 출처: https://en.wikipedia.org/wiki/Plotter.\n기본 R 그래픽은 역사적으로 가장 먼저 등장했습니다: 단순하고 절차적이며, 캔버스에 그리는 것에서 개념적 동기를 얻었습니다. 서로 다른 유형의 플롯을 위한 특화된 함수들이 있습니다. 이들은 호출하기 쉽지만, 더 복잡한 플롯을 구축하기 위해 이들을 결합하거나 하나를 다른 것으로 교체하고 싶을 때, 금방 지저분해지거나 심지어 불가능해질 수도 있습니다. 사용자는 (개념적인) 캔버스에 직접 플롯을 그립니다 (플로터(plotter)라는 단어는 최초의 그래픽 장치들 중 일부로 거슬러 올라갑니다 – 그림 3.1 참조). 사용자는 여백, 축 레이블, 제목, 범례, 하위 패널에 할당할 공간과 같은 결정들을 명시적으로 다루어야 합니다; 일단 무언가가 “그려지면(plotted)” 이동하거나 지울 수 없습니다.\n좀 더 고수준의 접근 방식이 있습니다: 그래픽 문법(grammar of graphics) 에서는 그래픽이 모듈화된 논리적 조각들로부터 구축되므로, 우리가 인간 언어에서 문장의 일부를 갈아 끼울 수 있는 것처럼, 직관적이고 쉽게 해독할 수 있는 방식으로 데이터에 대한 서로 다른 시각화 유형들을 쉽게 시도해 볼 수 있습니다. 캔버스나 플로터라는 개념은 없습니다; 대신 사용자는 ggplot2 에 자신이 원하는 플롯에 대한 고수준의 설명을 R 객체 형태로 제공하며, 렌더링 엔진은 장면을 전체적으로 조망하여 그래픽을 배치하고 출력 장치에 렌더링합니다.\n이 장에서 우리는 다음을 수행할 것입니다:\n가장 기본적인 함수는 plot입니다. 아래 코드(그 결과는 그림 3.2에 나와 있음)에서는 DNA를 분해하는 효소인 디옥시리보뉴클레아제(DNase)의 활성을 정량화하는 데 사용된 효소 결합 면역 흡착 분석(ELISA) 데이터를 플로팅하는 데 사용됩니다. 데이터는 기본 R과 함께 편리하게 제공되는 R 객체 DNase에 모여 있습니다. DNase 객체는 어세이 실행(run)을 나타내는 Run, 사용된 단백질 농도인 conc, 그리고 측정된 광학 밀도인 density 열을 가진 데이터 프레임입니다.\n그림 3.2: DNase의 ELISA 어세이에 대한 농도 대 밀도 플롯.\n이 기본적인 플롯은 그림 3.3에 표시된 것처럼 xlab, ylab 및 pch(플롯 기호) 매개변수들을 사용하여 사용자 정의할 수 있습니다. 변수들에 대한 정보는 DNase 객체에 저장되어 있으며, attr 함수를 사용하여 이에 접근할 수 있습니다.\n그림 3.3: 동일한 데이터이지만 더 나은 축 레이블과 다른 플롯 기호를 사용한 모습.\n질문 3.1\n데이터 프레임 열에 긴 설명, 물리적 단위, 출처 정보 등과 같은 “메타데이터”를 주석으로 다는 것은 유용한 기능인 것 같습니다. DNase 객체에서와 같이 이러한 정보를 저장하는 방식이 R 생태계 전반에서 표준화되어 있거나 흔한가요? 이 작업을 수행하는 다른 표준화되거나 흔한 방식이 있나요?\n해결책\n일반적인 R data.frame 이나 tidyverse(data_frame , tibble)에는 이를 위한 훌륭하거나 널리 사용되는 인프라가 없습니다. 하지만 바이오컨덕터 패키지 S4Vectors 의 DataFrame 클래스를 살펴보세요. 이는 무엇보다도 SummarizedExperiment 의 행과 열에 주석을 다는 데 사용됩니다.\n산점도 외에도 내장 함수들을 사용하여 히스토그램과 박스플롯을 만들 수 있습니다 (그림 3.4).\n그림 3.4: (a) ELISA 어세이로부터 얻은 밀도 히스토그램, (b) 어세이 실행(run)별로 계층화된 이 값들의 박스플롯. 실행 정보가 텍스트 문자열로 저장되어 있어 상자들이 축을 따라 사전식 순서로 정렬되었습니다. 수치적 순서를 얻기 위해 R의 유형 변환 함수들을 사용할 수 있습니다.\n박스플롯은 좁은 공간에 여러 분포를 나란히 보여주는 데 편리합니다. 우리는 3.6절에서 여러 단변량 분포를 플로팅하는 것에 대해 더 자세히 살펴볼 것입니다.\n기본 R 플로팅 함수들은 데이터를 신속하게 대화형으로 탐색하는 데 훌륭합니다; 하지만 더 정교한 디스플레이를 만들고 싶다면 곧 한계에 부딪히게 됩니다. 우리는 ggplot2 패키지에 구현된, 논리적이고 우아한 방식으로 고품질 그래픽을 단계별로 구성할 수 있게 해주는 그래픽 문법(grammar of graphics)이라 불리는 시각화 프레임워크를 사용할 것입니다. 먼저 예제 데이터 세트를 소개하고 로드해 봅시다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>고품질 그래픽 (High-Quality Graphics)</span>"
    ]
  },
  {
    "objectID": "03-chap.html#기본-r-플로팅",
    "href": "03-chap.html#기본-r-플로팅",
    "title": "5  고품질 그래픽 (High-Quality Graphics)",
    "section": "",
    "text": "head(DNase)\n\n\n  Run       conc density\n1   1 0.04882812   0.017\n2   1 0.04882812   0.018\n3   1 0.19531250   0.121\n4   1 0.19531250   0.124\n5   1 0.39062500   0.206\n6   1 0.39062500   0.215\n\n\nplot(DNase$conc, DNase$density)\n\n\n\nplot(DNase$conc, DNase$density,\n  ylab = attr(DNase, \"labels\")$y,\n  xlab = paste(attr(DNase, \"labels\")$x, attr(DNase, \"units\")$x),\n  pch = 3,\n  col = \"blue\")\n\n\n\n\n\n\n\nhist(DNase$density, breaks=25, main = \"\")\nboxplot(density ~ Run, data = DNase)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>고품질 그래픽 (High-Quality Graphics)</span>"
    ]
  },
  {
    "objectID": "03-chap.html#예제-데이터-세트",
    "href": "03-chap.html#예제-데이터-세트",
    "title": "5  고품질 그래픽 (High-Quality Graphics)",
    "section": "5.2 3.3 예제 데이터 세트",
    "text": "5.2 3.3 예제 데이터 세트\n\n그림 3.5: 원시 내배엽(primitive endoderm)의 마커인 Serpinh1(파란색), Gata6(빨간색), Nanog(초록색)으로 염색된 E3.5 생쥐 배반포의 단일 단면 면역 형광 이미지.\nggplot2 의 기능을 제대로 시험해 보기 위해, 우리는 충분히 크고 복잡해서 여러 다른 각도에서 자르고 살펴볼 수 있는 데이터 세트가 필요합니다. 우리는 초기 발달 과정의 서로 다른 시점에서 얻은 약 100개의 개별 생쥐 배아 세포의 전사체를 보고하는 유전자 발현 마이크로어레이 데이터 세트를 사용할 것입니다. 포유류 배아는 수정란이라는 단일 세포에서 시작합니다. 동기화된 세포 분열의 물결을 통해, 수정란은 처음에는 서로 간에 뚜렷한 차이를 보이지 않는 세포 덩어리로 증식합니다. 하지만 어느 시점에서 세포들은 서로 다른 계통(lineages)을 선택합니다. 더 세부적인 명세화 과정을 거치면서, 완전한 유기체에 필요한 서로 다른 세포 유형과 조직들이 발생합니다. Ohnishi 등 (2014)에 의해 설명된 이 실험의 목적은 배아에서 발생하는 첫 번째 대칭성 붕괴 사건과 연관된 유전자 발현 변화를 조사하는 것이었습니다. 진행하면서 데이터를 더 자세히 설명하겠습니다. 자세한 내용은 논문과 바이오컨덕터 데이터 패키지 Hiiragi2013 의 문서에서 찾을 수 있습니다. 먼저 데이터를 로드합니다:\n\n데이터 객체가 더 설명적인 이름 대신 x라는 다소 일반적인 이름을 가진 것은 유감입니다. 이름 충돌을 피하기 위해, 다음과 같은 코드를 실행하는 것이 가장 실용적인 해결책일 수 있습니다: esHiiragi = x; rm(list=\"x\").\nlibrary(\"Hiiragi2013\")\n\n\nIn chunk 'loadHiiragi': Warning: replacing previous import 'boot::logit' by 'gtools::logit' whenloading 'Hiiragi2013'\n\n\nIn chunk 'loadHiiragi': Warning: replacing previous import 'boot::inv.logit' by 'gtools::inv.logit'when loading 'Hiiragi2013'\n\n\ndata(\"x\")\ndim(Biobase::exprs(x))\n\n\n[1] 45101   101\nR 프롬프트에 x를 입력하면 ExpressionSet 객체 x에 대한 더 자세한 요약을 출력할 수 있습니다. (바이오컨덕터 패키지 Biobase 의 exprs 함수를 통해 위에서 접근한) 데이터 행렬의 101개 열은 샘플(이들 각각은 단일 세포임)에 대응하며, 45,101개 행은 어레이(Affymetrix mouse4302 어레이)에 의해 프로브(probed)된 유전자들에 대응합니다. 데이터는 RMA 방법을 사용하여 정규화되었습니다 (Irizarry et al. 2003). 원시 데이터 또한 패키지 내(데이터 객체 a에 있음)와 EMBL-EBI의 ArrayExpress 데이터베이스(기탁 번호 E-MTAB-1681)에서 확인할 수 있습니다.\n샘플들에 대해 어떤 정보가 가용한지 살펴봅시다2.\n2 #CAB2D6 표기법은 색상의 RGB 좌표에 대한 16진수 표현입니다; 이에 대한 자세한 내용은 3.10.2절을 참조하세요.\nhead(pData(x), n = 2)\n\n\n        File.name Embryonic.day Total.number.of.cells lineage genotype\n1 E3.25  1_C32_IN         E3.25                    32               WT\n2 E3.25  2_C32_IN         E3.25                    32               WT\n          ScanDate sampleGroup sampleColour\n1 E3.25 2011-03-16       E3.25      #CAB2D6\n2 E3.25 2011-03-16       E3.25      #CAB2D6\n제공된 정보는 세포에 관한 정보(즉, 세포가 얻어진 배아의 나이, 크기 및 유전자형)와 기술적 정보(스캔 날짜, 원시 데이터 파일 이름)가 섞여 있습니다. 관례에 따라 생쥐 배아 발달 시간은 일 단위로 측정되며, 예를 들어 E3.5와 같이 보고됩니다. 또한 논문에서 저자들은 세포의 나이, 유전자형 및 계통에 기초하여 세포들을 8개의 생물학적 그룹(sampleGroup)으로 나누었고, 이 그룹들을 나타내기 위한 색상 체계(sampleColour3)를 정의했습니다. 다음 코드를 사용하여 (설명은 아래 참조), 각 그룹에 대한 요약 정보(세포 수와 선호하는 색상)를 포함하는 작은 데이터 프레임 groups를 정의합니다.\n3 데이터 세트의 이 식별자는 영국식 철자(colour)를 사용합니다. 이 책의 다른 곳에서는 미국식 철자(color)를 사용합니다. ggplot2 패키지는 일반적으로 두 철자 모두를 허용합니다.\nlibrary(\"dplyr\")\ngroups = group_by(pData(x), sampleGroup) |&gt;\n  summarise(n = n(), color = unique(sampleColour))\ngroups\n\n\n# A tibble: 8 × 3\n  sampleGroup         n color  \n  &lt;chr&gt;           &lt;int&gt; &lt;chr&gt;  \n1 E3.25              36 #CAB2D6\n2 E3.25 (FGF4-KO)    17 #FDBF6F\n3 E3.5 (EPI)         11 #A6CEE3\n4 E3.5 (FGF4-KO)      8 #FF7F00\n5 E3.5 (PE)          11 #B2DF8A\n6 E4.5 (EPI)          4 #1F78B4\n7 E4.5 (FGF4-KO)     10 #E31A1C\n8 E4.5 (PE)           4 #33A02C\n이름에 FGF4-KO가 포함된 그룹의 세포들은 세포 분화의 중요한 조절 인자인 FGF4 유전자가 녹아웃(knocked out)된 배아로부터 온 것입니다. E3.5부터 야생형(wildtype) 세포(FGF4 녹아웃이 없는 경우)는 첫 번째 대칭성 붕괴 사건을 겪으며, 다분화능 에피블라스트(pluripotent epiblast, EPI)와 원시 내배엽(primitive endoderm, PE)이라 불리는 서로 다른 세포 계통으로 분화됩니다.\n위의 코드 청크는 dplyr 패키지의 파이프 연산자 |&gt;와 함수 group_by, summarise를 처음 접하는 사례일 수 있으므로, 코드를 풀어보겠습니다. 먼저 파이프 |&gt; 4입니다. 일반적으로 파이프는 중첩된 함수 호출을 사람이 더 읽기 쉽게 만드는 데 유용합니다. 다음 두 줄의 R 코드는 서로 동일합니다.\n4 |&gt;는 2021년에 출시된 버전 4.1부터 기본 R에 포함된 파이프 연산자입니다. 패키지 magrittr 는 이와 유사하지만(동일하지는 않음) 오래전부터 %&gt;% 연산자를 제공해 왔으며, %&lt;&gt;%나 %T&gt;%와 같은 다른 여러 파이프 관련 연산자들도 제공합니다. 이 책의 많은 부분이 2021년 이전에 쓰였으므로, magrittr 의 %&gt;% 연산자가 많은 곳에서 사용되고 있습니다. 우리는 이 코드에서처럼 책을 유지보수하는 동안 가끔씩 %&gt;%를 |&gt;로 업데이트하고 있습니다.\nf(x) |&gt; g(y) |&gt; h()\nh(g(f(x), y))\n이는 ” f(x)를 평가한 다음, 그 결과를 첫 번째 인수로 하여 함수 g에 전달하고, y는 g의 두 번째 인수로 전달하라. 그런 다음 g의 출력을 함수 h에 전달하라”는 뜻입니다. 이를 무한히 반복할 수 있습니다. 특히 인수 x와 y 자체가 복잡한 표현식이거나 꽤 긴 함수 체인이 관여하는 경우, 첫 번째 버전이 더 읽기 쉬운 경향이 있습니다.\ngroup_by 함수는 후속되는 모든 작업이 전체 데이터 프레임에 한꺼번에 적용되는 것이 아니라, sampleGroup 요인(factor)에 의해 정의된 블록별로 적용되어야 한다는 메모를 데이터 프레임에 남깁니다. 마지막으로 summarise는 요약 통계량을 계산합니다; 이는 예를 들어 mean이나 sum이 될 수 있습니다; 이 사례에서 우리는 단순히 각 블록의 행 개수인 n()과 대표 색상을 계산했습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>고품질 그래픽 (High-Quality Graphics)</span>"
    ]
  },
  {
    "objectID": "03-chap.html#ggplot2",
    "href": "03-chap.html#ggplot2",
    "title": "5  고품질 그래픽 (High-Quality Graphics)",
    "section": "5.3 3.4 ggplot2",
    "text": "5.3 3.4 ggplot2\nggplot2 는 Leland Wilkinson이 그의 저서 (Wilkinson 2005)에서 창안한 그래픽 문법(grammar of graphics) 개념을 구현한 Hadley Wickham (Wickham 2016)의 패키지입니다. 우리는 이 장에서 그 기능 중 일부를 탐구할 것이며, 이 책의 나머지 부분에서 이것이 어떻게 사용될 수 있는지에 대한 많은 예시들을 보게 될 것입니다. 패키지에 대한 포괄적인 문서는 웹사이트에서 찾을 수 있습니다. 온라인 문서에는 이 장에서 소개된 각 그래픽 유형(및 그 이상)에 대한 예제 사용 사례들이 포함되어 있으며, 그림을 만들 때 매우 유용한 리소스입니다.\n패키지를 로드하고 그림 3.2의 간단한 플롯을 다시 만들어 보는 것부터 시작합시다.\nlibrary(\"ggplot2\")\nggplot(DNase, aes(x = conc, y = density)) + geom_point()\n\n그림 3.6: 기본 그래픽 그림 3.2와 유사한, 우리의 첫 번째 ggplot2 그림.\n우리는 방금 그래픽 문법을 사용하여 첫 번째 “문장”을 썼습니다. 이 문장을 분해해 봅시다. 먼저 데이터가 들어있는 데이터 프레임인 DNase를 지정했습니다. aes ( 미학(aesthetic) 의 약자) 인수는 우리가 어떤 변수들을 각각 (x\\)축과 (y\\)축에 매핑하고 싶은지를 명시합니다. 마지막으로 geom_point 함수를 호출한 결과를 더함으로써, 플롯이 (예를 들어 선이나 막대가 아닌) 점들을 사용하기를 원한다고 명시했습니다.\n이제 생쥐 단일 세포 데이터로 돌아가서, ggplot 함수를 사용하여 8개 그룹 각각의 샘플 수를 플롯해 봅시다. 결과는 그림 3.7에 나와 있습니다.\nggplot(groups, aes(x = sampleGroup, y = n)) +\n  geom_bar(stat = \"identity\")\n\n그림 3.7: 생쥐 단일 세포 데이터의 그룹 크기 표로부터 ggplot 함수로 생성된 막대 그래프.\ngeom_bar를 통해 우리는 이제 각 데이터 항목(groups의 각 행)이 막대(bar)로 표현되기를 원한다고 ggplot에게 말했습니다. 막대는 ggplot2 패키지 용어로 geom 이라 부르는 기하학적 객체의 한 예입니다. 우리는 이미 그림 3.6에서 geom_point 함수에 의해 표시된 점이라는 또 다른 객체를 보았습니다. 나중에 다른 많은 geom들을 만나게 될 것입니다. 우리는 aes를 사용하여 그룹이 (x\\)축을 따라, 크기가 (y\\)축을 따라 표시되기를 원한다고 명시했습니다. 마지막으로 geom_bar 함수에 stat = \"identity\" (즉, 아무것도 하지 않음) 인수를 제공했는데, 그렇지 않으면 함수가 데이터의 히스토그램을 계산하려 시도할 것이기 때문입니다 (stat의 기본값은 “count”입니다). stat은 통계량(statistic) 의 약자로, 데이터를 축소하는 모든 함수를 일컫는 말입니다. identity와 count 통계량 외에도 평활화(smoothing), 평균화, 구간화(binning) 또는 다른 방식으로 데이터를 축소하는 다른 연산들이 있습니다.\n데이터, 기하학적 객체, 통계량이라는 이러한 개념들은 영어 문장의 구성 요소인 명사, 동사, 부사와 마찬가지로 그래픽 문법을 구성하는 재료들입니다.\n태스크\n(x\\)와 (y\\) 미학을 뒤집어서 가로 막대 그래프를 만들어 보세요.\n그림 3.7의 플롯은 나쁘지 않지만, 몇 가지 개선할 점이 있습니다. 막대에 색상을 사용하면 어떤 막대가 어떤 그룹에 해당하는지 신속하게 파악하는 데 도움이 될 수 있습니다. 이는 여러 플롯에서 동일한 색상 체계를 사용할 때 특히 유용합니다. 이를 위해, sampleGroup의 각 가능한 값에 대해 우리가 원하는 색상을 포함하는 이름이 지정된 벡터 groupColor를 정의해 봅시다.5\n5 이 정보는 groups 데이터 프레임의 sampleGroup 및 color 열에 있는 정보와 완전히 동일합니다; 단지 ggplot2 가 이 정보를 이름이 지정된 벡터 형식으로 기대한다는 점에 맞추는 것뿐입니다.\ngroupColor = setNames(groups$color, groups$sampleGroup)\n그림 3.7에서 수정해야 할 또 다른 사항은 막대 레이블의 가독성입니다. 지금은 레이블들이 서로 겹치고 있는데, 이는 설명적인 이름을 가질 때 흔히 발생하는 문제입니다.\nggplot(groups, aes(x = sampleGroup, y = n, fill = sampleGroup)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = groupColor, name = \"Groups\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n그림 3.8: 그림 3.7과 유사하지만 색상이 입혀진 막대와 더 나은 막대 레이블을 사용한 모습.\n이것은 이제 이미 더 길고 복잡한 문장입니다. 이를 분석해 봅시다. 우리는 aes 함수에 sampleGroup에 기초하여 막대가 채워지기를(색상이 입혀지기를) 원한다는 것을 명시하는 fill 인수를 추가했습니다 (이 경우 공교롭게도 x 인수의 값과 같지만, 항상 그럴 필요는 없습니다). 또한 scale_fill_manual 함수 호출을 추가했는데, 이 함수는 가능한 변수 값으로부터 관련 색상으로의 매핑인 색상 맵(color map)을 이름이 지정된 벡터로 입력받습니다. 우리는 또한 이 색상 맵에 제목을 부여했습니다 (더 복잡한 플롯에서는 여러 개의 서로 다른 색상 맵이 관여할 수 있음에 유의하세요). 만약 scale_fill_manual 호출을 생략했다면, ggplot2 는 자체적인 기본 색상 선택을 사용했을 것입니다. 우리는 또한 theme 호출을 추가하여 (x\\)축 레이블을 90도 회전시키고 오른쪽 정렬(hjust; 기본값은 중앙 정렬)하도록 명시했습니다.\n\n5.3.1 3.4.1 데이터 흐름\n\nggplot 함수는 데이터를 데이터 프레임 형태로 기대합니다. 만약 데이터가 행렬이나 별도의 벡터, 또는 다른 유형의 객체에 들어있다면 변환해야 합니다. dplyr 이나 broom 등의 패키지가 이 목적을 위한 기능을 제공합니다. 우리는 이에 대해 13.10절에서 더 자세히 다룰 것이며, 책 전체에서 이러한 변환의 예시들을 보게 될 것입니다.\n이는 기본 R의 data.frame 뿐만 아니라 tidyverse의 tibble 패키지에 있는 tibble (및 동의어인 data_frame) 클래스들도 포함합니다.\nggplot 호출의 결과는 ggplot 객체입니다. 위에서의 코드 조각을 다시 떠올려 봅시다:\ngg = ggplot(DNase, aes(x = conc, y = density)) + geom_point()\n우리는 여기서 ggplot의 출력을 콘솔로 직접 보내 그림 3.6을 “출력”하는 대신, gg 객체에 할당했습니다. 이 상황은 R 콘솔에서 작업할 때 익숙한 것과 완전히 동일합니다: 1+1과 같은 표현식을 입력하고 “Enter”를 누르면 결과가 출력됩니다. 표현식이 s = 1+1과 같은 할당문인 경우, 부작용(side effect)이 일어나지만(메모리상의 객체에 s라는 이름이 바인딩됨) 아무것도 출력되지 않습니다. 유사하게, source로 호출된 스크립트의 일부로 표현식이 평가될 때도 출력되지 않습니다. 따라서 위 코드 역시 print 메서드가 호출되지 않았으므로 그래픽 출력을 생성하지 않습니다. gg를 출력하려면 (대화형 세션에서) 그 이름을 입력하거나 print를 호출하면 됩니다:\ngg\nprint(gg)\n\n\n5.3.2 3.4.2 그림 저장하기\nggplot2 에는 ggsave라는 내장 플롯 저장 함수가 있습니다:\nggplot2::ggsave(\"DNAse-histogram-demo.pdf\", plot = gg)\n플롯을 저장하는 두 가지 주요 방식은 벡터 그래픽(vector graphics)과 래스터(raster, 픽셀) 그래픽입니다. 벡터 그래픽에서 플롯은 점, 선, 곡선, 도형 및 타이포그래피 문자와 같은 일련의 기하학적 원형(primitives)으로 저장됩니다. R에서 플롯을 벡터 그래픽 형식으로 저장할 때 선호되는 형식은 PDF입니다. 래스터 그래픽에서 플롯은 도트 매트릭스 데이터 구조로 저장됩니다. 래스터 형식의 주요 한계는 가용한 픽셀 수에 따른 제한된 해상도입니다. R에서 래스터 그래픽 출력에 가장 흔히 사용되는 장치는 png입니다. 일반적으로 플롯을 벡터 그래픽 형식으로 저장하는 것이 바람직합니다. 나중에 언제든지 벡터 그래픽 파일을 원하는 해상도의 래스터 형식으로 변환할 수 있는 반면, 그 반대는 매우 어렵기 때문입니다. 발표 자료나 논문의 그림이 픽셀화 아티팩트 때문에 보기 흉해지는 것을 원치 않으실 것입니다!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>고품질 그래픽 (High-Quality Graphics)</span>"
    ]
  },
  {
    "objectID": "03-chap.html#그래픽-문법",
    "href": "03-chap.html#그래픽-문법",
    "title": "5  고품질 그래픽 (High-Quality Graphics)",
    "section": "5.4 3.5 그래픽 문법",
    "text": "5.4 3.5 그래픽 문법\nggplot2 의 그래픽 문법 구성 요소는 다음과 같습니다:\n\n하나 이상의 데이터 세트,\n데이터를 시각적으로 표현하는 역할을 하는 하나 이상의 기하학적 객체 – 예를 들어 점, 선, 직사각형, 등고선 등,\n데이터의 변수들이 기하학적 객체의 시각적 속성(미학)에 어떻게 매핑되는지에 대한 설명과 관련 척도(scale) (예: 선형, 로그, 순위),\n하나 이상의 좌표계,\n통계적 요약 규칙,\n패싯 명세(facet specification), 즉 동일한 데이터의 하위 집합들을 살펴보기 위해 여러 개의 유사한 서브플롯(subplots)을 사용하는 것,\n레이아웃과 렌더링에 영향을 미치는 선택적 매개변수들 (예: 텍스트 크기, 글꼴 및 정렬, 범례 위치).\n\n위의 예제들인 그림 3.7과 3.8에서 데이터 세트는 groupsize였고, 변수들은 수치값들과 groupsize의 이름들이었으며, 우리는 이들을 각각 (y\\)축과 (x\\)축 미학에 매핑했습니다. 척도는 (y\\)축에 대해서는 선형이었고 (x\\)축에 대해서는 순위 기반이었습니다 (막대들이 알파벳순으로 정렬되었고 각 막대는 동일한 너비를 가짐). 그리고 기하학적 객체는 직사각형 막대였습니다.\n위 목록의 4~7번 항목은 선택 사항입니다. 이를 지정하지 않으면 좌표계로 데카르트 좌표계가 사용되고, 통계적 요약은 사소한 것(즉, identity)이 사용되며, 패싯이나 서브플롯은 만들어지지 않습니다 (나중에 3.8절에서 예시를 볼 것입니다). 처음 세 가지 항목은 필수입니다: 유효한 ggplot2 “문장”은 각각을 적어도 하나씩 포함해야 합니다.\n사실 ggplot2 의 그래픽 문법 구현은 레이어(layers) (Wickham 2010)라 불리는 구조를 통해 동일한 유형의 컴포넌트를 여러 번 사용할 수 있게 해줍니다. 예를 들어, 아래 코드는 동일한 데이터에 대해 세 가지 유형의 기하학적 객체인 점, 선, 그리고 신뢰 구간(confidence band)을 동일한 플롯에서 사용합니다.\ndftx = data.frame(t(Biobase::exprs(x)), pData(x))\nggplot( dftx, aes( x = X1426642_at, y = X1418765_at )) +\n  geom_point( shape = 1 ) +\n  geom_smooth( method = \"loess\" )\n\n그림 3.9: 동일한 데이터의 서로 다른 통계량을 보여주는 세 개의 레이어를 가진 산점도: 점(geom_point), 부드러운 회귀선 및 신뢰 구간(뒤의 두 개는 geom_smooth로부터 생성됨).\n여기서 우리는 표현 데이터(Biobase::exprs(x))와 샘플 주석 데이터(pData(x))를 모두 모아서 dftx라는 데이터 프레임으로 만들어야 했습니다 – 이는 이것이 ggplot2 함수들이 입력으로 가장 쉽게 받아들이는 데이터 형식이기 때문입니다 (13.10절에서 더 자세히 다룹니다).\n우리는 색상을 사용하여 플롯을 더욱 개선할 수 있습니다 – 그림 3.9의 각 점이 하나의 샘플에 대응하므로, 객체 x에 있는 sampleColour 정보를 사용하는 것이 타당합니다.\nggplot(dftx, aes(x = X1426642_at, y = X1418765_at))  +\n  geom_point(aes(color = sampleGroup), shape = 19) +\n  scale_color_manual(values = groupColor, guide = \"none\") +\n  geom_smooth(method = \"loess\")\n\n그림 3.10: 그림 3.9와 같지만, 추가로 시점과 세포 계통(그림 3.8에서 정의됨)에 따라 점들에 색상이 입혀졌습니다. 이제 우리는 Timd2 유전자(()y축을 따른 프로브 1418765_at에 의해 타겟팅됨)의 발현값이 초기 시점에서는 일관되게 높은 반면, E3.5와 E4.5의 EPI 샘플에서는 발현이 감소하는 것을 볼 수 있습니다. FGF4-KO에서는 이러한 감소가 지연되어 E3.5에서도 여전히 발현이 높습니다. 반대로 Fn1 유전자(1426642_at, ()x축)는 초기 시점에는 꺼져 있다가 E3.5와 E4.5에서 올라갑니다. PE 샘플(초록색)은 높은 수준의 세포 간 가변성을 보여줍니다.\n질문 3.2\n위의 코드에서 우리는 geom_point 레이어에 대해서만 color 미학(aes)을 정의한 반면, x와 y 미학은 모든 레이어에 대해 정의했습니다. 만약 우리가 color 미학을 모든 레이어에 대해 설정한다면, 즉 ggplot의 인수 목록으로 옮긴다면 어떤 일이 일어날까요?\n질문 3.3\n그림 3.9와 3.10에서와 같이 산점도 데이터를 회귀선과 함께 시각화하는 것이 항상 의미가 있을까요?\n여담으로, 이러한 프로브 식별자들이 어떤 유전자를 타겟팅하는지, 그리고 그들이 무엇을 하는지 알고 싶다면 다음과 같이 호출할 수 있습니다:\n\n여기서는 (예를 들어 앞서 붙었던 “X”가 없는 “1426642_at”과 같이) 원래의 특징 식별자들을 사용해야 함에 유의하세요. 이것이 마이크로어레이 제조사, 바이오컨덕터 주석 패키지, 그리고 객체 x 내부에서 사용되는 표기법입니다. 우리가 앞서 dftx로 작업할 때 사용했던 선행 “X”는 data.frame 생성자 함수에 의해 dftx 생성 중에 삽입된 것인데, 이는 해당 함수의 check.names 인수가 기본적으로 TRUE로 설정되어 있기 때문입니다. 또는 check.names = FALSE로 설정하여 원래의 식별자 표기법을 유지할 수도 있었겠지만, 그럴 경우 R이 코드(예: aes() 호출 등)에서 이들을 올바르게 해석할 수 있도록 식별자 주위에 역따옴표(backtick)를 사용해야 합니다.\n패키지 이름을 포함한 완전 수식 이름(fully qualified name)으로 select 함수를 호출하기 위해 :: 연산자를 사용한 것에 주목하세요. 우리는 이미 2장에서 이를 접했습니다.\nlibrary(\"mouse4302.db\")\n\n\nAnnotationDbi::select(mouse4302.db,\n   keys = c(\"1426642_at\", \"1418765_at\"), keytype = \"PROBEID\",\n   columns = c(\"SYMBOL\", \"GENENAME\"))\n\n\n     PROBEID SYMBOL                                            GENENAME\n1 1426642_at    Fn1                                       fibronectin 1\n2 1418765_at  Timd2 T cell immunoglobulin and mucin domain containing 2\nggplot을 사용할 때, 대개 데이터, 미학, 기하학적 객체만 지정하면 됩니다. 대부분의 기하학적 객체는 데이터에 적합한 기본 통계 요약을 암묵적으로 호출합니다. 예를 들어 geom_smooth를 사용하면 ggplot2 는 기본적으로 stat = \"smooth\"를 사용하여 선을 표시합니다; geom_histogram을 사용하면 데이터가 구간화(binned)되고 그 결과가 막대 그래프 형태로 표시됩니다. 여기에 예시가 있습니다:\ndfx = as.data.frame(Biobase::exprs(x))\nggplot(dfx, aes(x = `20 E3.25`)) + geom_histogram(binwidth = 0.2)\n\n그림 3.11: E3.25 시점의 특정 샘플 하나(세포 번호 20)에 대한 프로브 강도 히스토그램.\n질문 3.4\n객체 dfx와 dftx 사이의 차이점은 무엇인가요? 왜 이 두 가지를 모두 만들어야 했을까요?\n위의 막대 그래프 예제로 돌아가 봅시다.\npb = ggplot(groups, aes(x = sampleGroup, y = n))\n이는 플롯 객체 pb를 생성합니다. 만약 이를 표시하려 한다면, 기하학적 객체를 지정하지 않았기 때문에 빈 플롯이 생성될 것입니다. 지금까지 우리의 pb 객체에 들어있는 전부는 데이터와 미학뿐입니다 (그림 3.12).\nclass(pb)\n\n\n[1] \"gg\"     \"ggplot\"\n\n\npb\n\n그림 3.12: pb: 기하학적 객체(geom)가 없으므로 플롯 영역이 비어 있습니다. 기본 스타일 매개변수 하에서 (x\\)축의 눈금 레이블은 읽을 수 없습니다.\n이제 + 연산자를 사용하여 플롯의 다른 구성 요소들을 간단히 덧붙일 수 있습니다 (그림 3.13):\npb = pb + geom_bar(stat = \"identity\")\npb = pb + aes(fill = sampleGroup)\npb = pb + theme(axis.text.x = element_text(angle = 90, hjust = 1))\npb = pb + scale_fill_manual(values = groupColor, name = \"Groups\")\npb\n\n그림 3.13: 그래픽 객체 bp의 완성된 모습.\n이미 어떤 방식으로든 생성된 그래픽 객체를 가져와서 더 정제해 나가는 이러한 단계별 구축 방식은, 그래픽을 생성하는 단일 함수 호출에 모든 지침을 한꺼번에 제공하는 것보다 더 편리하고 관리하기 쉬울 수 있습니다.\n우리는 매번 처음부터 플롯을 다시 만들 필요 없이, 부분적으로 완성된 객체를 저장한 다음 이를 다른 방식으로 수정함으로써 다양한 시각화 아이디어를 빠르게 시도해 볼 수 있습니다. 예를 들어, 막대 그래프의 대안적인 시각화를 만들기 위해 플롯을 극좌표계로 전환할 수 있습니다.\npb.polar = pb + coord_polar() +\n  theme(axis.text.x = element_text(angle = 0, hjust = 1),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank()) +\n  xlab(\"\") + ylab(\"\")\npb.polar\n\n그림 3.14: 극좌표계에서의 막대 그래프.\n위 코드에서 우리가 이전에 설정했던 theme 매개변수를 단순히 새로운 값으로 설정함으로써 덮어쓸 수 있음에 유의하세요 – 원래 theme을 설정했던 pb를 다시 생성할 필요가 없습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>고품질 그래픽 (High-Quality Graphics)</span>"
    ]
  },
  {
    "objectID": "03-chap.html#차원-데이터-시각화",
    "href": "03-chap.html#차원-데이터-시각화",
    "title": "5  고품질 그래픽 (High-Quality Graphics)",
    "section": "5.5 3.6 1차원 데이터 시각화",
    "text": "5.5 3.6 1차원 데이터 시각화\n생물학적 데이터 분석에서 흔한 작업 중 하나는 여러 단변량 측정값 샘플을 비교하는 것입니다. 이 섹션에서 우리는 그러한 샘플들을 시각화하고 비교하기 위한 몇 가지 가능성들을 탐구해 볼 것입니다. 예제로서 Fgf4, Gata4, Gata6, Sox2라는 네 가지 유전자의 강도 데이터를 사용하겠습니다6. 마이크로어레이 상에서 이들은 다음과 같이 표현됩니다:\n6 이 유전자들에 대해서는 (Ohnishi et al. 2014)에서 더 자세히 읽어볼 수 있습니다.\nselectedProbes = c( Fgf4 = \"1420085_at\", Gata4 = \"1418863_at\",\n                   Gata6 = \"1425463_at\",  Sox2 = \"1416967_at\")\n이 표현으로부터 데이터를 추출하여 데이터 프레임으로 변환하기 위해, 우리는 reshape2 패키지의 melt 함수를 사용합니다7.\n7 우리는 서로 다른 데이터 표현 방식의 개념과 메커니즘에 대해 13.10절에서 더 이야기할 것입니다.\nlibrary(\"reshape2\")\ngenes = melt(Biobase::exprs(x)[selectedProbes, ],\n             varnames = c(\"probe\", \"sample\"))\n정확성을 위해, 프로브 식별자와 함께 유전자 심볼을 제공하는 열을 추가합니다.\ngenes$gene =\n  names(selectedProbes)[match(genes$probe, selectedProbes)]\nhead(genes)\n\n\n       probe  sample    value  gene\n1 1420085_at 1 E3.25 3.027715  Fgf4\n2 1418863_at 1 E3.25 4.843137 Gata4\n3 1425463_at 1 E3.25 5.500618 Gata6\n4 1416967_at 1 E3.25 1.731217  Sox2\n5 1420085_at 2 E3.25 9.293016  Fgf4\n6 1418863_at 2 E3.25 5.530016 Gata4\n\n5.5.1 3.6.1 막대 그래프\n우리 데이터 프레임 genes와 같은 데이터를 표시하는 인기 있는 방법은 막대 그래프입니다 (그림 3.15).\nggplot(genes, aes(x = gene, y = value)) +\n  stat_summary(fun = mean, geom = \"bar\")\n\n그림 3.15: 네 프로브로부터 얻은 발현 측정값 분포의 평균을 보여주는 막대 그래프.\n그림 3.15에서 각 막대는 해당 유전자에 대한 값들의 평균을 나타냅니다. 이러한 플롯은 생물 과학 분야뿐만 아니라 대중 매체에서도 흔히 사용됩니다. 하지만 데이터를 평균이라는 단 하나의 숫자로 요약하는 것은 많은 정보를 잃게 만들며, 공간을 차지하는 정도에 비해 막대 그래프는 데이터를 시각화하는 효율적인 방법이 아닙니다8.\n8 사실 평균이 적절한 요약치가 아닌 경우(예: 심하게 치우치거나 다봉형인 분포, 또는 큰 이상치가 있는 데이터 세트의 경우), 이러한 유형의 시각화는 완전히 오도할 수 있습니다.\n때때로 오차 막대(error bars)를 추가하고 싶을 때가 있는데, ggplot2 에서 이를 수행하는 한 가지 방법은 다음과 같습니다.\nlibrary(\"Hmisc\")\nggplot(genes, aes( x = gene, y = value, fill = gene)) +\n  stat_summary(fun = mean, geom = \"bar\") +\n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\",\n               width = 0.25)\n\n그림 3.16: 평균의 표준 오차를 나타내는 오차 막대가 포함된 막대 그래프.\n여기서 우리는 레이어형 그래픽의 원리를 다시 보게 됩니다: 우리는 두 개의 요약 함수인 mean과 mean_cl_normal, 그리고 그에 연관된 두 개의 기하학적 객체인 bar와 errorbar를 사용했습니다. mean_cl_normal 함수는 Hmisc 패키지에서 온 것으로 평균의 표준 오차(또는 신뢰 한계, c onfidence l imits)를 계산합니다; 이는 간단한 함수이므로, 원한다면 베이스 R 표현식을 사용하여 직접 계산할 수도 있습니다. 우리는 또한 플롯을 더 시각적으로 돋보이게 하기 위해 막대에 색상을 입혔습니다.\n\n\n5.5.2 3.6.2 박스플롯\n박스플롯(Boxplots)은 막대 그래프와 비슷한 공간을 차지하면서도 훨씬 더 많은 정보를 제공합니다.\np = ggplot(genes, aes( x = gene, y = value, fill = gene))\np + geom_boxplot()\n\n그림 3.17: 박스플롯.\n그림 3.17에서 우리는 두 유전자(Gata4, Gata6)가 비교적 집중된 분포를 가지고 있으며 오직 몇몇 데이터 포인트만이 더 높은 값 쪽으로 뻗어 있음을 알 수 있습니다. Fgf4의 경우, 박스 안의 중앙값(수평 검은색 막대)이 박스의 아래쪽(또는 왼쪽) 측면에 더 가깝기 때문에 분포가 오른쪽으로 치우쳐(right-skewed) 있음을 볼 수 있습니다. 반대로 Sox2의 분포는 왼쪽으로 치우쳐 있습니다.\n\n\n5.5.3 3.6.3 점 도표와 벌떼 도표\n데이터 포인트의 수가 너무 많지 않다면, 앞서 보았던 요약 수치들 대신 데이터 포인트들을 직접 보여주는 것이 가능하며 좋은 실무입니다. 하지만 데이터를 직접 플로팅하면 점들이 서로 겹치는 경우가 많아 시각적으로 불쾌하거나 심지어 데이터를 가릴 수도 있습니다. 우리는 점들이 겹치지 않으면서도 최대한 원래 위치에 가깝게 배치되도록 시도해 볼 수 있습니다 (Wilkinson 1999).\np + geom_dotplot(binaxis = \"y\", binwidth = 1/6,\n       stackdir = \"center\", stackratio = 0.75,\n       aes(color = gene))\nlibrary(\"ggbeeswarm\")\np + geom_beeswarm(aes(color = gene))\n[](03-chap_files/figure-html/fig-graphics-oneddot-1.png “그림 3.18 (a):”)\n\n\n\n[](03-chap_files/figure-html/fig-graphics-oneddot-2.png “그림 3.18 (b):”)\n\n\n\n그림 3.18: (a) ggplot2 의 geom_dotplot으로 만든 점 도표(Dot plots). (b) ggbeeswarm 패키지의 geom_beeswarm으로 만든 벌떼 도표(Beeswarm plots).\n그림 3.18의 (a) 패널에 있는 플롯은 점들의 (y\\) 좌표를 구간화(discretize)하고(위에서 우리는 구간 크기를 1/6로 선택했습니다), 그 후 이들을 서로 옆으로 쌓아 올립니다.\n대안적인 방식은 ggbeeswarm 패키지의 geom_beeswarm에 의해 제공됩니다. 이 플롯은 그림 3.18의 (b) 패널에 나와 있습니다. 레이아웃 알고리즘은 점들 사이의 겹침을 피하는 것을 목표로 합니다. 만약 어떤 점이 기존의 점과 겹치게 되면, 겹침을 피할 수 있을 만큼만 (x\\)축을 따라 옆으로 이동시킵니다. 점 도표나 벌떼 도표를 보기 좋게 만들려면 대개 새로운 데이터 세트마다 레이아웃 매개변수들을 약간씩 조정해야 합니다.\n\n\n5.5.4 3.6.4 밀도 도표\n동일한 데이터를 표현하는 또 다른 방법은 밀도 도표(density plots)입니다. 여기서는 데이터 포인트들을 평활화하여 기저의 데이터 생성 밀도를 추정하려 시도합니다 (그림 3.19).\nggplot(genes, aes( x = value, color = gene)) + geom_density()\n\n그림 3.19: 밀도 도표.\n그림 3.17—3.19로부터 볼 수 있듯이, 박스플롯은 단봉형(unimodal) 데이터에 대해서는 상당히 잘 작동하지만 데이터 분포가 다봉형(multimodal)인 경우에는 오도할 수 있으며, 꼬리가 긴(long-tailed) 분포의 특성을 항상 공정하게 보여주지는 못합니다. 데이터 포인트나 그 밀도를 직접 보여줌으로써 그림 3.18—3.19는 그러한 특징들을 더 잘 보여줍니다. 예를 들어, 우리는 Fgf4와 Sox2 데이터가 이봉형임을 알 수 있고, Gata4와 Gata6은 대부분의 값들이 기준선 근처에 모여 있지만 특정 분율의 세포들은 넓은 값의 범위에 걸쳐 발현 수준이 높아져 있음을 볼 수 있습니다.\n하지만 밀도 추정에는 여러 복잡한 문제들이 따르는데, 특히 평활화 윈도우(smoothing window)를 선택해야 할 필요성이 있습니다. 데이터가 조밀한 영역에서 정점들을 포착할 만큼 충분히 작은 윈도우 크기는 다른 곳에서 불안정한(“꾸불꾸불한”) 추정치를 낳을 수 있습니다. 반면에 윈도우를 너무 크게 잡으면 정점들과 같은 밀도의 뚜렷한 특징들이 뭉개질 수 있습니다. 또한 밀도 선들은 밀도를 추정하는 데 얼마나 많은 데이터가 사용되었는지에 대한 정보를 전달하지 않으므로, 그림 3.19와 같은 플롯은 곡선들 간의 샘플 크기가 다를 때 특히 문제가 될 수 있습니다.\n\n\n5.5.5 3.6.5 바이올린 도표\n그림 3.19와 같은 밀도 도표는 밀도가 여러 개인 경우 복잡해 보일 수 있으며, 한 가지 아이디어는 박스플롯에서 영감을 얻은 레이아웃으로 밀도들을 배치하는 것입니다: 바로 바이올린 도표(violin plot)입니다 (그림 3.20). 여기서는 밀도 추정치를 사용하여 대칭적인 모양을 그리는데, 이것이 때때로 시각적으로 바이올린을 연상시킵니다.\np + geom_violin()\n\n그림 3.20: 바이올린 도표.\n\n\n5.5.6 3.6.6 능선 도표\n밀도 도표를 변주한 또 다른 방식은 능선 도표(ridgeline plots)입니다 (그림 3.21):\nlibrary(\"ggridges\")\nggplot(genes, aes(x = value, y = gene, fill = gene)) + \n  geom_density_ridges()\n\n그림 3.21: 능선 도표.\n이러한 유형의 디스플레이는 표시할 밀도의 개수가 수십 개에 달할 때 가장 적절할 것입니다.\ntop42 = order(rowMeans(Biobase::exprs(x)), decreasing = TRUE)[1:42]\ng42 = melt(Biobase::exprs(x)[rev(top42), ], varnames = c(\"probe\", \"sample\"))\nggplot(g42, aes(x = value, y = probe))\n\n그림 3.22: 그림 3.21과 유사하지만 유전자 수가 더 많은 경우.\n\n\n5.5.7 3.6.7 ECDF 도표\n1차원 확률 변수 (X\\)의 분포를 설명하는 가장 수학적으로 편리한 방법은 그 누적 분포 함수(cumulative distribution function, CDF) 입니다. 즉, 다음과 같이 정의되는 함수입니다:\n\\[ F(x) = P(Xx), \\]\n여기서 (x\\)는 실수축 전체의 값을 취합니다. (X\\)의 밀도는 (F\\)가 존재한다면 그 도함수입니다9. 식 3.1에서의 확률에 대한 유한 표본 버전은 경험적 누적 분포 함수(empirical cumulative distribution function, ECDF) 라고 불립니다:\n9 정의에 의해 (F\\)는 작은 (x\\) ((x-\\))에 대해 0으로 가고, 큰 (x\\) ((x+\\))에 대해 1로 갑니다.\n\\[ F_{n}(x) = = _{i=1}^n 𝟙({xx_i}), \\]\n여기서 (x_1, …, x_n\\)은 (X\\)로부터의 (n\\)번의 추출을 나타내고, (𝟙\\)은 인수의 표현식이 참이면 1이고 그렇지 않으면 0을 취하는 지시 함수(indicator function)입니다. 만약 이것이 추상적으로 들린다면, 다음 예제로부터 아마도 더 직관적인 이해를 얻을 수 있을 것입니다 (그림 3.23):\nsimdata = rnorm(70)\ntibble(index = seq(along = simdata),\n          sx = sort(simdata)) %&gt;%\nggplot(aes(x = sx, y = index)) + geom_step()\n\n그림 3.23: 정렬된 simdata 값 대 그들의 인덱스. 이것이 simdata의 경험적 누적 분포 함수입니다.\n정렬된 값들을 그들의 순위(rank)에 대해 플로팅하면 ECDF의 본질적인 특징들이 나타납니다 (그림 3.23). 실제로는 위의 코드에서와 같이 정렬 및 다른 단계들을 수동으로 수행할 필요 없이, 대신 stat_ecdf() 기하학적 객체를 사용할 것입니다. 우리 데이터의 ECDF가 그림 3.24에 나와 있습니다.\nggplot(genes, aes( x = value, color = gene)) + stat_ecdf()\n\n그림 3.24: 경험적 누적 분포 함수(ECDF).\nECDF는 몇 가지 멋진 속성을 가집니다:\n\n무손실(Lossless): ECDF (F_{n}(x)\\)는 원래 샘플 (x_1, …, x_n\\)에 포함된 모든 정보를 유지합니다. (값들의 순서는 중요하지 않다고 가정할 때)\n(n\\)이 커질수록, ECDF (F_{n}(x)\\)는 실제 CDF (F(x)\\)로 수렴합니다. 제한된 표본 크기 (n\\)에 대해서도 두 함수 사이의 차이는 작은 경향이 있습니다. 이는 경험적 밀도에 대해서는 해당되지 않음에 유의하세요! 평활화 없이는 유한 표본의 경험적 밀도는 디랙 델타(Dirac delta) 함수들의 합이며, 이는 시각화하기 어렵고 기저의 매끄럽고 실제적인 밀도와는 사뭇 다릅니다. 평활화를 하면 그 차이가 덜 두드러질 수 있지만 앞서 논의한 것처럼 이를 제어하기는 어렵습니다.\n\n\n그림 3.25: Lawrence 등 (2013) 논문의 그림 1 일부. 각 점은 종양-정상 쌍에 대응하며, 수직 위치는 엑솜(exome) 내의 체세포 돌연변이 총 빈도를 나타냅니다. 결과로 나타나는 곡선들은 본질적으로 ECDF 도표이며, 개념적으로 이 플롯은 그림 3.24와 유사합니다. 다만 그래프가 90도 회전되었고(즉, (x\\)축과 (y\\)축의 역할이 바뀜), 개별 종양 유형에 대한 곡선들이 서로 잘 구분되도록 수평으로 변위되었습니다.\n태스크\ntibbles. 위의 코드에서 우리는 tibble을 처음 보았습니다. tibble 패키지의 비네트를 살펴보고 그것이 무엇을 하는지 확인해 보세요.\n\n\n5.5.8 3.6.8 변환이 밀도에 미치는 영향\n히스토그램이나 밀도 도표를 보고 어떤 기저의 생물학적 현상에 대한 지표로서 이봉성(bimodality) (또는 다봉성)의 증거를 조사하고 싶은 유혹을 느낄 수 있습니다. 하지만 그러기 전에, 밀도의 모드(mode) 개수는 연쇄 법칙(chain rule) 에 의해 데이터가 변환되는 척도에 의존한다는 점을 기억하는 것이 중요합니다. 예를 들어, Hiiragi 데이터 세트의 어레이 중 하나에서 얻은 데이터를 살펴봅시다 (그림 3.26).\nggplot(dfx, aes(x = `64 E4.5 (EPI)`)) + geom_histogram(bins = 100)\nggplot(dfx, aes(x = 2 ^ `64 E4.5 (EPI)`)) + \n  geom_histogram(binwidth = 20) + xlim(0, 1500)\n[](03-chap_files/figure-html/fig-graphics-onedtrsf-1.png “그림 3.26 (a):”)\n\n\n\n[](03-chap_files/figure-html/fig-graphics-onedtrsf-2.png “그림 3.26 (b):”)\n\n\n\n그림 3.26: 로그 변환 여부에 따른 동일한 데이터의 히스토그램. (a) 데이터는 마이크로어레이 형광 강도를 로그(밑 2) 변환한 결과로 데이터 객체 x에 저장된 척도 그대로 보여줍니다 (Irizarry et al. 2003); (b) 이들을 다시 원래의 형광 척도로 되돌린 모습입니다. 공간을 더 잘 활용하기 위해 (x\\)축 범위를 1500으로 제한했습니다.\n질문 3.5\n(심화:) 확률 변수 (X\\)와, 변환된 확률 변수 (Y = f(X)\\)를 정의하는 비선형 1:1 변환 (f: x y\\)를 고려해 봅시다. (Y\\)의 밀도 함수가 (p(y)\\)라고 가정합시다. (X\\)의 밀도는 무엇인가요? (X\\)의 모드(또는 모드들)는 (Y\\)의 모드(들)와 어떤 관련이 있나요?\n힌트: 함수 (p\\)의 모드는 그 도함수 (p’=dp/dx\\)의 근(root)이라는 점에 유의하세요. (x_0\\)가 (X\\)의 모드라면 (y_0=f(x_0)\\)가 (Y\\)의 모드라는 사실이 일반적으로 성립할까요?\n해결책\n연쇄 법칙에 따라 (p(y), dy = p(f(x)),f’(x),dx\\)이므로, (X\\)의 밀도는 ((x) = p(f(x)),f’(x)\\)입니다. (\\)의 모드는 그 도함수 (d/dx\\)의 근이며, 즉 이들은 (p’(f(x)),f’^2(x) + p(f(x))f”(x) = 0\\)을 만족합니다. 합의 두 번째 항은 (f\\)가 아핀 선형(affine linear) ((f”\\))인 경우 사라지지만, 일반적으로는 두 밀도의 근들 사이에, 그리고 그에 따라 (X\\)와 (Y\\)의 모드들 사이에 간단한 관계는 없습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>고품질 그래픽 (High-Quality Graphics)</span>"
    ]
  },
  {
    "objectID": "03-chap.html#차원-데이터-시각화-산점도",
    "href": "03-chap.html#차원-데이터-시각화-산점도",
    "title": "5  고품질 그래픽 (High-Quality Graphics)",
    "section": "5.6 3.7 2차원 데이터 시각화: 산점도",
    "text": "5.6 3.7 2차원 데이터 시각화: 산점도\n산점도는 처리-반응 비교(그림 3.3에서와 같이), 변수들 사이의 연관성(그림 3.10에서와 같이), 또는 쌍체 데이터(예: 치료 전후 여러 환자의 질병 바이오마커)를 시각화하는 데 유용합니다. 우리는 플로팅 용지나 화면의 두 차원을 사용하여 두 변수를 나타냅니다. 야생형(wildtype)과 FGF4-KO 샘플 사이의 차등 발현을 살펴봅시다.\nscp = ggplot(dfx, aes(x = `59 E4.5 (PE)` ,\n                      y = `92 E4.5 (FGF4-KO)`))\nscp + geom_point()\n\n그림 3.27: 두 샘플에 대한 45,101개 발현 측정값의 산점도.\n레이블 59 E4.5 (PE)와 92 E4.5 (FGF4-KO)는 앞서 우리가 만들었던 데이터 프레임 dfx의 열 이름(샘플 이름)을 나타냅니다. 이들은 특수 문자(공백, 괄호, 하이픈)를 포함하고 숫자로 시작하기 때문에, R이 구문적으로 이해할 수 있도록 역따옴표(backtick)로 묶어줄 필요가 있습니다. 플롯은 그림 3.27에 나와 있습니다. 우리는 밀집된 점 구름을 얻게 되는데, 구름의 외곽 부분에서는 해석을 시도해 볼 수 있겠지만 플롯의 더 밀집된 영역 내에서 데이터가 어떻게 분포되어 있는지에 대해서는 시각적으로 전혀 알 방법이 없습니다.\n겹쳐그리기(overplotting) 문제를 개선하는 한 가지 쉬운 방법은 geom_point의 alpha 매개변수를 조정하여 점들의 투명도(알파 값)를 조절하는 것입니다 (그림 3.28).\nscp  + geom_point(alpha = 0.1)\n\n그림 3.28: 그림 3.27과 같지만 일부 겹쳐그리기를 해결하기 위해 반투명한 점들을 사용한 모습.\n이것은 이미 그림 3.27보다 낫지만, 더 밀집된 영역에서는 반투명한 점들조차 특징 없는 검은 덩어리로 겹쳐지는 반면, 더 고립된 바깥쪽 점들은 흐릿해집니다. 대안은 2D 밀도의 등고선 도표(contour plot)이며, 이는 그림 3.29에서와 같이 플롯의 모든 점들을 다 그릴 필요가 없다는 장점이 있습니다.\n하지만 그림 3.29를 보면, (상대적으로 적은 수의 점들을 포함하는) 오른쪽 하단의 점 구름이 더 이상 표현되지 않음을 알 수 있습니다. geom_density2d의 대역폭(bandwidth)과 구간화 매개변수들을 조정함으로써 이 문제를 어느 정도 극복할 수 있습니다 (그림 3.30, 왼쪽 패널).\nscp + geom_density2d()\n\n그림 3.29: 그림 3.27과 같지만 2D 밀도 추정치의 등고선 도표로 렌더링된 모습.\nscp + geom_density2d(h = 0.5, bins = 60)\nlibrary(\"RColorBrewer\")\ncolorscale = scale_fill_gradientn(\n    colors = rev(brewer.pal(9, \"YlGnBu\")),\n    values = c(0, exp(seq(-5, 0, length.out = 100))))\n\nscp + stat_density2d(h = 0.5, bins = 60,\n          aes( fill = after_stat(level)), geom = \"polygon\") +\n  colorscale + coord_fixed()\n[](03-chap_files/figure-html/fig-graphics-twodsp4-1.png “그림 3.30 (a):”)\n\n\n\n[](03-chap_files/figure-html/fig-graphics-twodsp4-2.png “그림 3.30 (b):”)\n\n\n\n그림 3.30: 2D 밀도 도표. (a) 그림 3.29와 같지만 더 작은 평활화 대역폭과 등고선을 위한 더 촘촘한 구간화를 사용한 모습. (b) 색상 채우기가 추가된 모습.\n그림 3.30의 오른쪽 패널에서와 같이, stat_density2d 함수(geom_density2d를 감싸는 래퍼 함수)를 명시적으로 호출하고 기하학적 객체로 polygon 을 사용함으로써 등고선 사이의 각 공간을 점들의 상대적 밀도로 채울 수 있습니다.\n우리는 RColorBrewer 패키지의 brewer.pal 함수를 사용하여 색상 척도를 정의했으며, coord_fixed 호출을 추가하여 플롯의 종횡비를 고정했습니다. 이는 두 변수에 대해 데이터 범위에서 (x\\) 및 (y\\) 좌표로의 매핑이 동일함을 보장하기 위함입니다. 이 두 가지 이슈 모두 더 깊이 살펴볼 가치가 있으며, 우리는 3.7.1절에서 플롯 모양에 대해, 3.9절에서 색상에 대해 더 이야기할 것입니다.\n그림 3.30의 밀도 기반 플로팅 방법들은 그림 3.27과 3.28의 겹쳐 그려진 점 구름보다 더 시각적으로 매력적이고 해석하기 쉽지만, 플롯의 희소한 영역에 있는 이상치 점들에 대한 정보를 많이 잃게 되므로 주의해서 사용해야 합니다. 한 가지 방법은 geom_point를 사용하여 그러한 점들을 다시 추가하는 것입니다.\n하지만 평활화의 한계를 피하는 아마도 최고의 대안은 육각형 구간화(hexagonal binning)일 것입니다 (Carr et al. 1987).\nscp + geom_hex() + coord_fixed()\nscp + geom_hex(binwidth = c(0.2, 0.2)) + colorscale +\n  coord_fixed()\n[](03-chap_files/figure-html/fig-graphics-twodsp6-1.png “그림 3.31 (a):”)\n\n\n\n[](03-chap_files/figure-html/fig-graphics-twodsp6-2.png “그림 3.31 (b):”)\n\n\n\n그림 3.31: 육각형 구간화. (a) 기본 매개변수 사용. (b) 더 미세한 구간 크기와 사용자 정의 색상 척도 사용.\n\n5.6.1 3.7.1 플롯 모양\n여러분의 플롯을 위해 적절한 모양을 선택하는 것은 정보가 잘 전달되도록 보장하는 데 중요합니다. 기본적으로 그래프의 높이와 너비 사이의 비율인 모양 매개변수(shape parameter)는 현재 플로팅 장치(device)에서 가용한 공간에 기초하여 ggplot2 에 의해 선택됩니다. 장치의 너비와 높이는 명시적으로 지정하거나 R에서 장치가 열릴 때 기본 매개변수10를 통해 지정됩니다. 게다가 그래프 크기는 그림 3.31의 색상 척도 바와 같이 추가적인 장식 요소들이 있느냐 없느냐에 따라서도 달라집니다.\n10 예를 들어 pdf와 png 함수의 매뉴얼 페이지를 참조하세요.\n산점도에 대해 적용할 수 있는 두 가지 간단한 규칙이 있습니다:\n\n만약 두 축의 변수들이 동일한 단위로 측정되었다면, 데이터 공간에서 물리적 공간으로의 매핑이 동일하도록 하세요 – 즉, coord_fixed를 사용하세요. 위의 산점도들에서 두 축은 모두 발현 수준 측정값의 로그(밑 2)입니다; 즉, 한 단위의 변화는 두 축 모두에서 동일한 의미(발현 수준의 두 배 증가)를 가집니다. 또 다른 사례는 주성분 분석(PCA)으로, 여기서 (x\\)축은 보통 성분 1을, (y\\)축은 성분 2를 나타냅니다. 축들이 입력 데이터 공간의 정규 직교(orthonormal) 회전으로부터 발생하므로, 축들의 척도가 일치하기를 원할 것입니다. 데이터의 분산은 (정의에 의해) 첫 번째 성분보다 두 번째 성분을 따라 더 작으므로(또는 기껏해야 같으므로), 잘 만들어진 PCA 플롯은 대개 높이보다 너비가 더 큽니다.\n만약 두 축의 변수들이 서로 다른 단위로 측정되었다면, 여전히 그들의 차원을 비교함으로써 이들을 서로 연관시킬 수 있습니다. ggplot2 를 포함한 R의 많은 플로팅 루틴의 기본값은 데이터 범위를 살펴보고 이를 가용한 플로팅 영역에 매핑하는 것입니다. 하지만 특히 데이터가 대략적으로 선을 따를 때, 그 선의 전형적인 기울기를 살펴보는 것이 유용할 수 있습니다. 이를 뱅킹(banking) 이라 부릅니다 (William S. Cleveland, McGill, and McGill 1988).\n\n뱅킹을 설명하기 위해, Cleveland의 논문에 나온 고전적인 태양 흑점 데이터를 사용해 봅시다.\nlibrary(\"ggthemes\")\nsunsp = tibble(year   = time(sunspot.year),\n               number = as.numeric(sunspot.year))\nsp = ggplot(sunsp, aes(x = year, y = number)) + geom_line()\nsp\nratio = with(sunsp, bank_slopes(year, number))\nsp + coord_fixed(ratio = ratio)\n[](03-chap_files/figure-html/fig-graphics-banking-1.png “그림 3.32 (a):”)\n\n\n\n[](03-chap_files/figure-html/fig-graphics-banking-2.png “그림 3.32 (b):”)\n\n\n\n그림 3.32: 태양 흑점 데이터. (a)에서 플롯 모양은 대략 정사각형에 가까우며, 이는 흔한 기본 선택입니다. (b)에서는 플롯 모양을 선택하기 위해 뱅킹 이라는 기술이 사용되었습니다. (참고: 이 플롯에서 눈금 레이블의 배치가 좋지 않으며 사용자 정의가 필요해 보입니다.)\n결과 플롯은 그림 3.32의 상단 패널에 나와 있습니다. 우리는 1700년대 초반, 1800년대 초반, 그리고 20세기 전환기 전후에 특히 낮은 최대 활동을 보이는 등 태양 흑점 활동 주기의 진폭에서 장기적인 변동을 명확하게 볼 수 있습니다. 하지만 이제 뱅킹을 시도해 봅시다.\n알고리즘은 어떻게 작동하나요? 곡선의 기울기들을 1 근처로 만드는 것을 목표로 합니다. 특히 bank_slopes는 절대 기울기의 중앙값을 계산하고, coord_fixed 호출을 통해 이 수치가 1이 되도록 플롯의 종횡비를 설정합니다. 결과는 그림 3.32의 하단 패널에 나와 있습니다. 다소 반직관적으로, 비록 플롯이 훨씬 작은 공간을 차지하지만 우리는 거기서 더 많은 것을 볼 수 있습니다! 특히, 급격한 상승과 더 완만한 하락을 보이는 태양 흑점 주기의 톱니 모양(saw-tooth shape)을 확인할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>고품질 그래픽 (High-Quality Graphics)</span>"
    ]
  },
  {
    "objectID": "03-chap.html#차원-이상의-시각화",
    "href": "03-chap.html#차원-이상의-시각화",
    "title": "5  고품질 그래픽 (High-Quality Graphics)",
    "section": "5.7 3.8 2차원 이상의 시각화",
    "text": "5.7 3.8 2차원 이상의 시각화\n때때로 우리는 세 개 이상의 변수 사이의 관계를 보여주고 싶어 합니다. 추가적인 차원을 포함하기 위한 명백한 선택지는 플롯 기호의 모양과 색상입니다. geom_point 기하학적 객체는 ( x와 y 외에) 다음과 같은 미학들을 제공합니다:\n\nfill\ncolor\nshape\nsize\nalpha\n\n이들은 geom_point 함수의 매뉴얼 페이지에서 살펴볼 수 있습니다. fill과 color는 객체의 채우기 색상과 외곽선 색상을 의미하며, alpha는 투명도를 나타냅니다. 앞서 그림 3.28 및 그 이후의 그림들에서 우리는 점 밀도를 반영하고 겹쳐그리기 효과를 피하기 위해 색상이나 투명도를 사용했습니다. 또한 이러한 속성들을 사용하여 데이터의 다른 차원들을 보여줄 수도 있습니다. 원칙적으로 우리는 위에 나열된 다섯 가지 미학 모두를 동시에 사용하여 최대 7차원 데이터를 보여줄 수도 있겠지만, 그러한 플롯은 해독하기가 매우 어려울 것입니다. 대개 우리는 이 미학들 중 한두 가지만 선택하여 데이터의 추가적인 한두 차원을 보여주는 것으로 제한하는 것이 좋습니다.\n\n5.7.1 3.8.1 패싯(Faceting)\n\n\n\n때때로 이러한 플롯 배열의 모습 때문에 트렐리스(trellis) 또는 래티스(lattice) 그래픽이라고도 불립니다. 패싯을 구현한 첫 번째 주요 R 패키지는 lattice였습니다. 이 책에서는 ggplot2를 통해 제공되는 패싯 기능들을 사용할 것입니다.\n\n\n때때로 이러한 모습 때문에 트렐리스(trellis) 또는 래티스(lattice) 그래픽이라고도 불립니다. 패싯을 구현한 첫 번째 주요 R 패키지는 lattice 였습니다. 이 책에서는 ggplot2 를 통해 제공되는 패싯 기능들을 사용할 것입니다.\n데이터의 추가적인 차원을 보여주는 또 다른 방법은, 우리 데이터의 하위 집합을 한 개(또는 그 이상의) 변수에 기초하여 반복적으로 “슬라이싱”하여 여러 개의 플롯을 보여줌으로써 각 부분을 따로 시각화하는 것입니다. 이를 패싯(faceting) 이라 부르며, 이를 통해 최대 4~5차원 데이터를 시각화할 수 있게 해줍니다. 따라서 우리는 예를 들어 패싯 변수의 범위에 걸쳐 다른 변수들 사이에서 관찰되는 패턴이 동일한지 아니면 다른지를 조사할 수 있습니다. 예제를 하나 살펴봅시다:\nlibrary(\"magrittr\")\ndftx$lineage %&lt;&gt;% sub(\"^$\", \"no\", .)\ndftx$lineage %&lt;&gt;% factor(levels = c(\"no\", \"EPI\", \"PE\", \"FGF4-KO\"))\n\nggplot(dftx, aes(x = X1426642_at, y = X1418765_at)) +\n  geom_point() + facet_grid( . ~ lineage )\n\n그림 3.33: 패싯(faceting) 의 예시: 그림 3.9와 동일한 데이터이지만 이제 범주형 변수 lineage에 의해 분리된 모습.\n결과는 그림 3.33에 나와 있습니다. 우리는 데이터를 나누고 싶은 변수를 지정하기 위해 R의 포뮬러(formula) 언어를 사용했으며, 개별 패널들이 서로 다른 열에 배치되도록 했습니다: facet_grid( . ~ lineage ). 사실 우리는 다음과 같이 두 개의 패싯 변수를 지정할 수도 있습니다; 그 결과는 그림 3.34에 나와 있습니다.\nggplot(dftx,\n  aes(x = X1426642_at, y = X1418765_at)) + geom_point() +\n   facet_grid( Embryonic.day ~ lineage )\n\n그림 3.34: 패싯 : 그림 3.9와 동일한 데이터이지만 범주형 변수 Embryonic.day (행)와 lineage (열)에 의해 분리된 모습.\n또 다른 유용한 함수는 facet_wrap입니다: 만약 패싯 변수의 수준(levels)이 너무 많아 모든 플롯을 한 행이나 한 열에 맞추기 어려운 경우, 이 함수를 사용하여 이들을 지정된 수의 열이나 행으로 감쌀 수 있습니다. 지금까지 우리는 범주형 변수에 의한 패싯을 보았지만, 연속형 변수를 수준별로 구간화하여 사용할 수도 있습니다. 함수 cut이 이 목적에 유용합니다.\nggplot(mutate(dftx, Tdgf1 = cut(X1450989_at, breaks = 4)),\n   aes(x = X1426642_at, y = X1418765_at)) + geom_point() +\n   facet_wrap( ~ Tdgf1, ncol = 2 )\n\n그림 3.35: 패싯 : 그림 3.9와 동일한 데이터이지만 연속형 변수 X1450989_at에 의해 분리되고 facet_wrap에 의해 배치된 모습.\n그림 3.35에서 네 패널의 점 개수가 서로 다른 것을 볼 수 있습니다. 이는 cut이 점 개수가 아니라 구간 길이를 동일하게 나누기 때문입니다. 만약 후자를 원한다면 cut과 함께 quantile을 사용하거나, 변수 값의 순위(rank)에 대해 구간을 나눌 수 있습니다.\n\n5.7.1.1 축 척도\n그림 3.33—3.35에서 축 척도는 모든 패널에서 동일합니다. 대신 facet_grid와 facet_wrap 함수의 scales 인수를 설정함으로써 이를 가변적으로 만들 수 있습니다. 이 인수는 각 패널에서 (x\\)축과 (y\\)축이 동일한 척도를 가질지, 아니면 각 패널의 데이터 범위에 맞춰질지를 제어하게 해줍니다. 여기에는 상충 관계(trade-off)가 있습니다: 가변적인 축 척도는 더 많은 세부 사항을 보게 해줄 수 있는 반면, 패널 간의 비교는 더 어려워집니다.\n\n\n5.7.1.2 암묵적 패싯\nfacet_grid나 facet_wrap을 명시적으로 호출하지 않고도 미학을 지정함으로써 플롯을 패싯할 수 있습니다. 암묵적 패싯의 매우 간단한 예는 그림 3.15—3.18에서와 같이 요인(factor)을 (x\\)축으로 사용하는 것입니다.\n\n\n\n5.7.2 3.8.2 대화형 그래픽\n지금까지 생성된 플롯들은 정적인 이미지들이었습니다. 여러분은 플롯을 대화형(interactive)으로 만듦으로써 엄청난 양의 정보와 표현력을 더할 수 있습니다. 우리는 여기서 대화형 시각화를 깊이 있게 전달하려 하지는 않지만, 몇 가지 중요한 리소스들에 대한 지표를 제공할 것입니다. 이는 역동적인 분야이므로, 독자들은 최근의 발전을 위해 R 생태계를 탐색해 보아야 합니다.\n\n5.7.2.1 shiny\nPosit(구 RStudio)의 shiny 는 R을 위한 웹 애플리케이션 프레임워크입니다. 대화형 요소가 플롯을 생성하는 R 코드를 직접 호출하기 때문에, 슬라이더, 선택기 및 다른 제어 요소들을 사용하여 표시된 플롯의 모든 측면을 변경할 수 있는 대화형 디스플레이를 쉽게 만들 수 있게 해줍니다. 몇 가지 훌륭한 예시들을 shiny 갤러리에서 확인해 보세요.\nshiny 기반의 대화형 시각화를 위한 그래픽 엔진으로 ggplot2 를 사용할 수 있으며, 사실 베이스 R 그래픽이나 다른 어떤 그래픽 패키지도 사용할 수 있습니다. 여기서 약간 어색할 수 있는 점은 대화형 옵션들을 기술하기 위해 사용되는 언어가 ggplot2 와 그래픽 문법을 통한 그래픽 생성과 분리되어 있다는 것입니다. ggvis 패키지는 이러한 한계를 극복하는 것을 목표로 합니다:\n\n\n5.7.2.2 ggvis\nggvis 11는 ggplot2 의 좋은 특징들을 대화형 그래픽 영역으로 확장하려는 시도입니다. R의 전통적인 그래픽 장치들(PDF, PNG 등)로 그래픽을 생성하는 ggplot2 와 대조적으로, ggvis 는 Vega라 불리는 JavaScript 인프라를 기반으로 하며, 그 플롯들은 HTML 브라우저에서 보도록 의도되었습니다. ggplot2 와 마찬가지로 ggvis 패키지 역시 그래픽 문법 개념으로부터 영감을 얻었지만, 구별되는 구문을 사용합니다. 이는 대화형 동작에 필요한 계산을 수행하기 위해 R과 연결하는 데 shiny 의 인프라를 활용합니다. 그 저자가 말했듯이12, “목표는 R의 최고(예: 상상할 수 있는 모든 모델링 함수들)와 웹의 최고(모든 사람이 웹 브라우저를 가지고 있음)를 결합하는 것입니다. 데이터 조작과 변환은 R에서 수행되고, 그래픽은 Vega를 사용하여 웹 브라우저에 렌더링됩니다.”\n11 이 글을 쓰는 시점(2017년 여름)에는 ggvis 개발의 초기 추진력이 유지될지 불분명하며, 현재의 기능과 성숙도는 아직 ggplot2 에 미치지 못합니다.\n12 https://ggvis.rstudio.com\nshiny 와 ggvis 의 대화형 기능의 결과로서, 사용자가 그래픽을 보는 동안 사용자의 행동에 응답하기 위해 기저 데이터 및 코드와 함께 실행 중인 R 인터프리터가 필요합니다. 이 R 인터프리터는 로컬 머신에 있을 수도 있고 서버에 있을 수도 있습니다; 두 경우 모두 뷰어 애플리케이션은 웹 브라우저이며, R과의 상호 작용은 웹 프로토콜(http 또는 https)을 통해 이루어집니다. 이는 물론 R에 의해 한 번 생성되어 실행 중인 R 인스턴스와의 연결 없이 PDF나 HTML 뷰어에서 볼 수 있는 자체 완비된 파일에 저장된 그래픽과는 다릅니다.\n\n\n5.7.2.3 plotly\n대화형 그래픽 생성을 위한 또 다른 훌륭한 웹 기반 도구는 plotly 입니다. https://plot.ly/r 에서 대화형 그래픽의 몇 가지 예시들을 온라인으로 볼 수 있습니다. R에서 여러분만의 대화형 플롯을 만들려면 다음과 같은 코드를 사용할 수 있습니다:\nlibrary(\"plotly\")\nplot_ly(economics, x = ~ date, y = ~ unemploy / pop)\nshiny 및 ggvis 와 마찬가지로 그래픽은 HTML 브라우저에서 볼 수 있습니다; 하지만 실행 중인 R 세션이 필수적으로 요구되지는 않습니다. 그래픽은 그 “로직”이 JavaScript, 또는 더 정확하게는 D3.js 시스템으로 코딩된 자체 완비된 HTML 문서로 구성될 수 있습니다.\n\n그림 3.36: rgl 을 이용한 volcano 데이터 렌더링. 오클랜드 화산 지대의 약 50개 화산 중 하나인 마웅가 화우(Mt Eden)의 지형 정보입니다.\n\n\n5.7.2.4 rgl, webgl\n3D 객체(예: 기하학적 구조)를 시각화하기 위해 rgl 패키지가 있습니다. 이는 (여러분의 화면 상의 특수한 그래픽 장치나 웹 브라우저를 통해) 장면을 회전시키고 확대/축소 등을 할 수 있는 대화형 뷰어 창을 생성합니다. 아래 코드에 의해 생성된 장면의 스크린샷이 그림 3.36에 나와 있습니다; 이러한 스크린샷은 snapshot3d 함수를 사용하여 생성할 수 있습니다.\ndata(\"volcano\")\nvolcanoData = list(\n  x = 10 * seq_len(nrow(volcano)),\n  y = 10 * seq_len(ncol(volcano)),\n  z = volcano,\n  col = terrain.colors(500)[cut(volcano, breaks = 500)]\n)\nlibrary(\"rgl\")\nwith(volcanoData, persp3d(x, y, z, color = col))\n위의 코드에서 베이스 R 함수 cut은 volcano 데이터의 값 범위를 1부터 500 사이의 정수들로 매핑하는 것을 계산하며13, 우리는 이를 색상 척도인 terrain.colors(500)를 인덱싱하는 데 사용합니다. 더 자세한 정보는 패키지의 훌륭한 비네트를 참조하세요.\n13 더 정확하게는, 해당 개수의 수준(levels)을 가진 요인(factor)을 반환하며, 우리는 이를 R이 정수들로 자동 변환하도록 합니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>고품질 그래픽 (High-Quality Graphics)</span>"
    ]
  },
  {
    "objectID": "03-chap.html#색상",
    "href": "03-chap.html#색상",
    "title": "5  고품질 그래픽 (High-Quality Graphics)",
    "section": "5.8 3.9 색상",
    "text": "5.8 3.9 색상\n플롯을 만들 때 중요한 고려 사항 중 하나는 거기서 사용하는 색상 선택입니다. 대부분의 R 사용자들은 그림 3.37에 표시된 것처럼 베이스 R 그래픽에서 사용하는 내장 색상 체계에 익숙할 것입니다.\npie(rep(1, 8), col=1:8)\n\n그림 3.37: 베이스 R 색상 팔레트의 처음 8가지 색상.\n이 색상 팔레트의 기원은 1980년대 하드웨어로 거슬러 올라갑니다. 당시 그래픽 카드들은 음극선관(CRT)의 세 가지 기본 색상 채널인 빨강, 초록, 파랑(RGB)을 각 픽셀이 사용하거나 사용하지 않게 하는 방식으로 색상을 다뤘습니다. 이는 RGB 색상 큐브의 여덟 모서리에서 (2^3=8\\)가지 조합을 낳았습니다14.\n14 따라서 그림 3.37의 8번째 색상은 흰색이어야 하지만, 대신 회색입니다.\nggplot2 에서 범주형 변수를 위한 처음 8가지 색상은 그림 3.38에 나와 있습니다:\nggplot(tibble(u = factor(1:8), v = 1), \n       aes(x = \"\",  y = v, fill = u)) +\n  geom_bar(stat = \"identity\", width = 1) + \n  coord_polar(\"y\", start = 0) + theme_void()\n\n그림 3.38: ggplot2 색상 팔레트의 처음 8가지 색상.\n이러한 기본값들은 간단한 사용 사례들에는 적절하지만, 종종 우리는 우리만의 선택을 하고 싶어 할 것입니다. 3.7절에서 우리는 이미 scale_fill_gradientn 함수를 보았는데, 이를 사용하여 RColorBrewer 패키지의 brewer.pal 함수에 의해 제공되는 일련의 색상 단계들을 보간함으로써 그림 3.30과 3.31에서 사용된 매끄러워 보이는 색상 그라디언트를 만들었습니다. 이 패키지는 목적에 맞게 설계된 일련의 색상 팔레트들을 정의합니다. 우리는 display.brewer.all 함수를 사용하여 이들을 한눈에 볼 수 있습니다 (그림 3.39).\ndisplay.brewer.all()\n\n그림 3.39: RColorBrewer 팔레트들.\n우리는 brewer.pal.info로부터 가용한 색상 팔레트들에 대한 정보를 얻을 수 있습니다.\nhead(brewer.pal.info)\n\n\n     maxcolors category colorblind\nBrBG        11      div       TRUE\nPiYG        11      div       TRUE\nPRGn        11      div       TRUE\nPuOr        11      div       TRUE\nRdBu        11      div       TRUE\nRdGy        11      div      FALSE\n\n\ntable(brewer.pal.info$category)\n\n\n div qual  seq \n   9    8   18 \n팔레트들은 세 가지 범주로 나뉩니다:\n\nqualitative (정성적): 고유한 순서가 없는 범주형 속성들을 위한 것. Paired 팔레트는 각각 두 개의 하위 범주로 나뉘는 최대 6개의 범주들을 지원합니다 – 예를 들어 전 과 후 , 처리군 과 대조군 등.\nsequential (순차적): 낮음 에서 높음 으로 가는 양적 속성들을 위한 것.\ndiverging (발산적): 자연스러운 중간 지점이나 중립적인 값이 있고 그 값으로부터 위아래로 벗어날 수 있는 양적 속성들을 위한 것; 그림 3.41에서 예시를 볼 것입니다.\n\n특정 팔레트로부터 색상들을 얻으려면 brewer.pal 함수를 사용합니다. 첫 번째 인수는 원하는 색상의 개수입니다 (이는 brewer.pal.info에 있는 가용한 최대 개수보다 작을 수 있습니다).\nbrewer.pal(4, \"RdYlGn\")\n\n\n[1] \"#D7191C\" \"#FDAE61\" \"#A6D96A\" \"#1A9641\"\n가용한 프리셋 색상 개수보다 더 많은 색상을 원한다면 (예를 들어 연속적인 색상의 히트맵을 그리기 위해), colorRampPalette 함수를 사용하여 보간할 수 있습니다15.\n15 colorRampPalette는 하나의 정수 매개변수를 받는 함수를 반환합니다. 표시된 코드에서 우리는 그 함수를 100이라는 인수와 함께 호출했습니다.\nmypalette  = colorRampPalette(\n    c(\"darkorange3\", \"white\",\"darkblue\")\n  )(100)\nhead(mypalette)\n\n\n[1] \"#CD6600\" \"#CE6905\" \"#CF6C0A\" \"#D06F0F\" \"#D17214\" \"#D27519\"\n\n\nimage(matrix(1:100, nrow = 100, ncol = 10), col = mypalette,\n        xaxt = \"n\", yaxt = \"n\", useRaster = TRUE)\n\n그림 3.40: darkorange3, white, darkblue 색상 사이를 보간하여 유도된 준연속 색상 팔레트.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>고품질 그래픽 (High-Quality Graphics)</span>"
    ]
  },
  {
    "objectID": "03-chap.html#히트맵",
    "href": "03-chap.html#히트맵",
    "title": "5  고품질 그래픽 (High-Quality Graphics)",
    "section": "5.9 3.10 히트맵",
    "text": "5.9 3.10 히트맵\n히트맵(Heatmaps)은 대규모의 행렬 형태 데이터 세트를 시각화하고 데이터에 있을 수 있는 패턴들에 대한 빠른 개요를 제공하는 강력한 방식입니다. R에는 여러 히트맵 그리기 함수들이 있습니다; 그중 편리하고 보기 좋은 결과물을 내는 것은 동명의 패키지16에 있는 pheatmap 함수입니다. 아래 코드에서 우리는 먼저 데이터 세트 x에서 가장 가변적인 상위 500개 유전자를 선택하고, 열들에 대한 평균을 뺌으로써 각 유전자(행)를 중앙화(center)하는 rowCenter 함수를 정의합니다. 기본적으로 pheatmap은 RcolorBrewer 의 RdYlBu 색상 팔레트를 colorRampPalette 함수와 함께 사용하여 11개 색상을 매끄러워 보이는 팔레트로 보간합니다 (그림 3.41).\n16 매우 다재다능하고 모듈화된 대안으로는 ComplexHeatmap 패키지가 있습니다.\nlibrary(\"pheatmap\")\ntopGenes = order(rowVars(Biobase::exprs(x)), decreasing = TRUE)[1:500]\nrowCenter = function(x) { x - rowMeans(x) }\npheatmap(rowCenter(dfx[topGenes, ]), \n  show_rownames = FALSE, \n  show_colnames = FALSE, \n  breaks = seq(-5, +5, length = 101),\n  annotation_col = pData(x)[, c(\"sampleGroup\", \"Embryonic.day\", \"ScanDate\", \"genotype\") ],\n  annotation_colors = list(\n    sampleGroup = groupColor,\n    genotype = c(`FGF4-KO` = \"chocolate1\", `WT` = \"azure2\"),\n    Embryonic.day = setNames(brewer.pal(9, \"Blues\")[c(3, 6, 9)], c(\"E3.25\", \"E3.5\", \"E4.5\")),\n    ScanDate = setNames(brewer.pal(nlevels(x$ScanDate), \"YlGn\"), levels(x$ScanDate))\n  )\n)\n\n그림 3.41: 상대적 발현값의 히트맵. 즉, 해당 유전자(행)의 모든 샘플(열)에 걸친 평균 발현과 비교한 로그 폴드 변화를 보여줍니다. 색상 척도는 중간 지점이 0인 발산 팔레트를 사용합니다.\n이 다소 거대해 보이는 pheatmap 호출을 잠시 분해해 봅시다. show_rownames와 show_colnames 옵션은 행과 열의 이름이 행렬의 측면에 출력될지 여부를 제어합니다. 우리 행렬은 가용한 플로팅 공간에 비해 크기 때문에 레이블들이 읽기 어려울 것이므로, 이를 억제하는 것이 좋습니다. annotation_col 인수는 샘플들에 대한 추가 정보를 담은 데이터 프레임을 받습니다. 이 정보는 히트맵 상단의 컬러 바에 표시됩니다. 행(유전자)들에 대해서도 측면에 컬러 바 주석을 달기 위한 유사한 annotation_row 인수가 있지만, 여기서는 사용하지 않았습니다. annotation_colors 인수는 주석 바를 위한 기본 색상 선택을 덮어쓸 수 있게 해주는 이름이 지정된 벡터들의 리스트입니다. pheatmap 함수는 더 많은 옵션들을 가지고 있으며, 여러분 자신의 데이터 시각화에 이를 사용하고 싶다면 공부할 가치가 있습니다.\n\n5.9.1 3.10.1 덴드로그램 순서\n그림 3.41에서 왼쪽과 상단의 트리들은 계층적 군집화 알고리즘의 결과를 나타내며 덴드로그램(dendrograms) 이라고도 불립니다. 행과 열의 순서는 덴드로그램에 기초합니다. 이는 히트맵의 시각적 영향력에 엄청난 효과를 미칩니다. 하지만 나타나는 패턴들 중 어떤 것이 실제적인 것이고 어떤 것이 임의적인 트리 레이아웃 결정의 결과인지 결정하기는 어려울 수 있습니다17. 다음 사항을 염두에 둡시다:\n17 우리는 5장에서 군집화와 군집 유의성을 평가하는 방법들에 대해 배울 것입니다.\n\n클러스터 덴드로그램에 따라 행과 열을 정렬하는 것(그림 3.41에서와 같이)은 하나의 임의적인 선택이며, 여러분은 다른 정렬을 택할 수도 있습니다.\n덴드로그램 순서로 정하기로 했더라도, 각 내부 분기(internal branch)에서 어느 쪽을 왼쪽 혹은 오른쪽으로 둘지는 본질적으로 임의적인 선택입니다. 왜냐하면 트리의 위상(topology)을 바꾸지 않고도 각 분기를 뒤집을 수 있기 때문입니다 (또한 그림 5.21을 참조하세요).\n\n질문 3.6\npheatmap 함수는 서브트리의 어느 분기가 왼쪽 혹은 오른쪽으로 갈지 결정하는 문제를 어떻게 처리하나요?\n해결책\n이는 기본적으로 pheatmap에서 사용되는 stats 패키지의 hclust 함수 매뉴얼 페이지에 설명되어 있습니다.\n질문 3.7\n다른 어떤 정렬 방식들을 생각할 수 있을까요?\n해결책\n제안된 방식들 중에는 외판원 문제(travelling salesman problem) (McCormick Jr, Schweitzer, and White 1972)나 첫 번째 주성분으로의 투영(예를 들어 pheatmap 매뉴얼 페이지의 예제들 참조) 등이 있습니다.\n질문 3.8\npheatmap 함수의 clustering_callback 인수를 확인해 보세요.\n\n\n5.9.2 3.10.2 색 공간\n인간의 색 지각 (Helmholtz 1867)은 3차원입니다18. 이 공간을 매개변수화하는 방법은 여러 가지가 있습니다. 위에서 우리는 이미 3.4절 시작 부분에서 groupColor의 내용을 출력할 때 RGB 색상 모델을 접했습니다. 이는 [0,1] 범위의 세 가지 값을 사용합니다:\n18 물리적으로는 빛의 파장이 무수히 많고 이들을 혼합하는 방법도 무수히 많기 때문에, 다른 종이나 로봇의 경우 색 공간은 더 적거나 더 많은 차원을 가질 수 있습니다.\ngroupColor[1]\n\n\n    E3.25 \n\"#CAB2D6\"\n여기서 CA는 빨간색 채널의 강도에 대한 16진수 표현이고, B2는 녹색, D6은 파란색 채널의 강도입니다. 십진수로는 이 수치들이 각각 202, 178, 214입니다. 이 값들의 범위가 0에서 255까지이므로, 이를 최댓값으로 나누면 RGB 삼중항(triplet)을 3차원 단위 큐브 내의 한 점으로 생각할 수도 있습니다.\n\n\n\n\n\n\n\n\n그림 3.42: HCL 색 공간의 원들. (a) 휘도(Luminance) (L\\)을 75로 고정했을 때, 각도 좌표 (H\\) (색조, hue)가 0에서 360까지 변하고 반지름 좌표 (C\\)가 0, 10, …, 60인 경우. (b) 크로마(Chroma) (C\\)를 50으로 고정했을 때, (H\\)는 위와 같고 반지름 좌표는 휘도 (L\\)로서 10, 20, …, 90인 경우.\nhcl 함수는 다른 좌표계를 사용합니다. 이 역시 세 가지 좌표로 구성됩니다: 색조(hue) (H\\) ( ([0, 360]) 범위의 각도), 크로마(chroma) (C\\) (양수), 그리고 휘도(luminance) (L\\) ( ([0, 100]) 범위의 값). (C\\)의 상한은 색조와 휘도에 따라 달라집니다.\nhcl 함수는 CIE-LUV19의 극좌표에 대응하며 영역 채우기(area fills)를 위해 설계되었습니다. 크로마와 휘도 좌표를 일정하게 유지하고 색조만 변화시킴으로써, 조화로운 색상 팔레트를 쉽게 만들 수 있고 밝은 색 영역이 어두운 영역보다 더 커 보이는 조사(irradiation) 착시를 피할 수 있습니다. 우리의 주의는 또한 선명한 색상에 끌리는 경향이 있는데, 크로마 값을 고정하면 모든 색상이 우리 눈에 똑같이 매력적으로 보이게 됩니다.\n19 CIE: Commission Internationale de l’Éclairage (국제 조명 위원회) – 자세한 내용은 위키백과 등을 참조하세요.\n색상환(color wheel)에서 색상을 고르는 방법은 여러 가지가 있습니다. 삼합색(Triads) 은 색상환에서 등간격으로 떨어진 세 가지 색상입니다; 예를 들어 (H=0, 120, 240\\)은 빨강, 초록, 파랑을 줍니다. 사합색(Tetrads) 은 색상환에서 등간격인 네 가지 색상이며, 일부 그래픽 아티스트들은 이 효과를 “역동적”이라고 설명합니다. 따뜻한 색(Warm colors) 은 노란색에 가까운 등간격 색상 세트이며, 차가운 색(Cool colors) 은 파란색에 가까운 등간격 색상 세트입니다. 유사 색(Analogous color) 세트는 색상환의 좁은 구간 내의 색상들을 포함합니다 (예: 노랑, 주황, 빨강 또는 초록, 청록, 파랑). 보색(Complementary colors) 은 색상환에서 정반대편에 위치한 색상들입니다. 사합색은 두 쌍의 보색으로 이루어집니다. 분할 보색(Split complementaries) 은 한 쌍의 보색 중 하나를 양옆으로 등간격인 두 색으로 나눈 세 가지 색상입니다 (예: (H=60, 240-30, 240+30\\)). 이는 두 가지 유사한 범주와 세 번째 다른 범주 사이의 차이를 강조할 때 유용합니다. 참고 문헌 (Mollon 1995; Ihaka 2003)에서 더 철저한 논의를 찾아볼 수 있습니다.\n\n5.9.2.1 선 대 영역\n선과 점의 경우, 배경과 강한 대비를 원하므로 흰색 배경에서는 상대적으로 어둡게(낮은 휘도 (L\\)) 만드는 것이 좋습니다. 영역 채우기의 경우, 채도가 낮거나 중간 정도인 더 밝은 파스텔 톤의 색상들이 보통 더 보기 좋습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>고품질 그래픽 (High-Quality Graphics)</span>"
    ]
  },
  {
    "objectID": "03-chap.html#데이터-변환",
    "href": "03-chap.html#데이터-변환",
    "title": "5  고품질 그래픽 (High-Quality Graphics)",
    "section": "5.10 3.11 데이터 변환",
    "text": "5.10 3.11 데이터 변환\n대부분의 점들이 한 영역에 뭉쳐 있고 가용 공간의 많은 부분이 비어 있는 플롯은 읽기 어렵습니다. 변수의 주변 분포 히스토그램이 날카로운 정점을 가지고 한쪽 또는 양쪽으로 긴 꼬리를 늘어뜨리고 있다면, 데이터를 변환하는 것이 도움이 될 수 있습니다. 이러한 고려 사항들은 (x\\) 및 (y\\) 미학뿐만 아니라 색상 척도에도 적용됩니다. 마이크로어레이 데이터를 다룬 이 장의 플롯들에서, 우리는 로그 변환20을 사용했습니다 – 그림 3.27과 같은 산점도의 (x\\) 및 (y\\) 좌표뿐만 아니라, 발현 폴드 변화를 나타내는 그림 3.41의 색상 척도에서도 마찬가지입니다. 로그 변환은 확실한 의미를 가지기 때문에 매력적입니다 – 로그 변환된 척도에서 동일한 양만큼 위나 아래로 움직이는 것은 원래 척도에서 동일한 배수 변화에 대응합니다: (\\log(ax)=\\log a+\\log x\\).\n20 ExpressionSet 객체 x 안의 데이터가 이미 로그 변환된 상태로 오기 때문에 우리는 이를 암묵적으로 사용했습니다.\n하지만 때때로 로그 변환만으로는 충분하지 않을 수 있는데, 예를 들어 데이터에 0이나 음수 값이 포함되어 있거나, 로그 척도에서도 데이터 분포가 매우 불균일한 경우입니다. 그림 3.43의 상단 패널로부터, A가 작을수록 분산이 더 큰 것처럼 보이는 등 M의 분포가 A에 의존한다는 인상을 받기 쉽습니다. 하지만 이는 전적으로 시각적 아티팩트일 뿐이며 하단 패널이 이를 확인해 줍니다: M의 분포는 A와 독립적이며, 상단 패널에서 보았던 겉보기 추세는 작은 A에서의 더 높은 점 밀도 때문에 발생한 것입니다.\ngg = ggplot(tibble(A = Biobase::exprs(x)[, 1], M = rnorm(length(A)))),\n            aes(y = M))\ngg + geom_point(aes(x = A), size = 0.2)\ngg + geom_point(aes(x = rank(A)), size = 0.2)\n\n\n\n\n\n\n\n\n그림 3.43: 순위 변환(rank transformation)이 의존성에 대한 시각적 지각에 미치는 영향.\n질문 3.9\n그림 3.31에서와 같이 밀도 기반 또는 구간화 기반 플로팅 방법을 사용함으로써 이러한 시각적 아티팩트를 피할 수 있을까요?\n질문 3.10\n순위 변환을 히트맵 등을 위한 색상 척도를 선택할 때도 적용할 수 있을까요? 이미지 처리에서의 히스토그램 평활화(histogram equalization) 는 무엇을 하나요?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>고품질 그래픽 (High-Quality Graphics)</span>"
    ]
  },
  {
    "objectID": "03-chap.html#수학-기호-및-기타-글꼴",
    "href": "03-chap.html#수학-기호-및-기타-글꼴",
    "title": "5  고품질 그래픽 (High-Quality Graphics)",
    "section": "5.11 3.12 수학 기호 및 기타 글꼴",
    "text": "5.11 3.12 수학 기호 및 기타 글꼴\n우리는 R 구문과 LaTeX 스타일 표기법이 혼합된 방식을 사용하여 플롯 레이블에 수학적 표기법을 사용할 수 있습니다 (자세한 내용은 help(\"plotmath\")를 참조하세요):\n\n그림 3.44: 그림 3.24와 유사하지만 “Bauhaus 93” 글꼴을 사용한 모습.\nvolume = function(rho, nu)\n            pi^(nu/2) * rho^nu / gamma(nu/2+1)\n\nggplot(tibble(nu    = 1:15,\n  Omega = volume(1, nu)), aes(x = nu, y = Omega)) +\ngeom_line() +\nxlab(expression(nu)) + ylab(expression(Omega)) +\ngeom_text(label =\n\"Omega(rho,nu)==frac(pi^frac(nu,2)~rho^nu, Gamma(frac(nu,2)+1))\",\n  parse = TRUE, x = 6, y = 1.5)\n 인 (\\) 차원 구의 부피 (\\) ( (, …, 15\\).”)\n그림 3.45: 반지름 (\\) 인 (\\) 차원 구의 부피 (\\) ( (, …, 15\\).\n그 결과는 그림 3.45에 나와 있습니다. 또한 세리프(serif) 글꼴인 Times와 같은 다른 글꼴로 전환하는 것도 쉽습니다 (그림 3.46).\nggplot(genes, aes( x = value, color = gene)) + stat_ecdf() +\n  theme(text = element_text(family = \"Times\"))\n\n그림 3.46: 그림 3.24와 유사하지만 다른 글꼴을 사용한 모습.\n사실 표준 R 설치 시 가용한 글꼴 세트는 제한적이지만, 다행히 R의 표준 PostScript 글꼴들 외의 글꼴들을 쉽게 사용할 수 있게 도와주는 extrafont 패키지가 있습니다. R 외부의 글꼴들을 먼저 R이 알 수 있게 해주어야 하므로 사용 전 약간의 추가 작업이 필요합니다. 글꼴들은 여러분의 운영 체제, 워드 프로세서 또는 다른 그래픽 애플리케이션과 함께 설치되어 있을 수 있습니다. 따라서 가용한 글꼴 세트와 물리적 위치는 표준화되어 있지 않으며, 운영 체제와 추가 설정에 따라 달라질 것입니다. extrafont 패키지를 불러온 후의 첫 세션에서, 여러분은 글꼴들을 가져와 패키지에 알리기 위해 font_import 함수를 실행해야 할 것입니다. 그 후 글꼴을 사용하고 싶은 각 세션마다, 하나 이상의 R 그래픽 장치에 이들을 등록하기 위해 loadfonts 함수를 호출해야 합니다. 마지막으로 fonttable 함수를 사용하여 가용한 글꼴들을 나열할 수 있습니다. 여러분의 머신에서 이를 어떻게 작동시킬지에 대해서는 extrafonts 패키지 문서를 참조해야 할 것입니다.\n태스크\n패키지 extrafont 를 사용하여 “Bauhaus 93” (또는 여러분의 시스템에서 가용한 다른 글꼴) 글꼴로 그림 3.46의 버전을 만들어 보세요.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>고품질 그래픽 (High-Quality Graphics)</span>"
    ]
  },
  {
    "objectID": "03-chap.html#유전체-데이터",
    "href": "03-chap.html#유전체-데이터",
    "title": "5  고품질 그래픽 (High-Quality Graphics)",
    "section": "5.12 3.13 유전체 데이터",
    "text": "5.12 3.13 유전체 데이터\n\n그림 3.47: Ensembl 유전체 브라우저의 스크린샷. 유전체 영역의 유전자 주석과 RNA-Seq 실험의 리드 파일업(read pile-up) 시각화를 보여줍니다.\n유전체 데이터를 시각화하는 데에는 이 장에서 논의한 일반적인 원칙들 외에도 몇 가지 구체적인 고려 사항들이 있습니다. 데이터는 대개 유전체 좌표(genomic coordinates)와 연관되어 있습니다. 사실 유전체 좌표는 유전체 데이터 통합을 위한 훌륭한 조직화 원칙을 제공합니다. 여러분은 아마도 그림 3.47과 같은 유전체 브라우저 디스플레이를 본 적이 있을 것입니다. 여기서는 공공 주석 정보뿐만 아니라 여러분의 데이터를 사용하여 이러한 플롯들을 프로그래밍 방식으로 생성하는 법을 짧게 보여줄 것입니다. 이는 짧은 맛보기일 뿐이며, 더 완전한 그림을 위해서는 Bioconductor와 같은 리소스를 참고하시기 바랍니다.\n유전체 데이터 시각화의 주요 도전 과제는 유전체의 크기입니다. 우리는 전체 유전체부터 뉴클레오타이드 수준에 이르기까지 다중 스케일(multiple scales)에서의 시각화가 필요합니다. 확대와 축소가 쉬워야 하며, 크기 스케일에 따라 서로 다른 시각화 전략들이 필요할 수 있습니다. 생물학적 분자들(유전체, 유전자, 전사체, 단백질)을 선형적인 방식으로 시각화하는 것이 편리할 수 있지만, 3D 물리적 세계에서의 그들의 배치 또한 (매우) 중요할 수 있습니다.\n몇 가지 재미있는 예시들로 시작해 봅시다: 인간 1번 염색체의 이데오그램(ideogram) 플롯(그림 3.48)과 유전체 전반의 RNA 편집 사이트 분포 플롯(그림 3.49).\nlibrary(\"ggbio\")\ndata(\"hg19IdeogramCyto\", package = \"biovizBase\")\nplotIdeogram(hg19IdeogramCyto, subchr = \"chr1\")\n\n그림 3.48: 인간 유전체의 1번 염색체: 이데오그램 플롯.\ndarned_hg19_subset500은 인간 유전체에서 선택된 500개의 RNA 편집 사이트를 나열합니다. 이는 파리, 생쥐, 인간의 RNA 편집 데이터베이스(DARNED, https://darned.ucc.ie/)로부터 얻어졌습니다. 그 결과는 그림 3.49에 나와 있습니다.\nlibrary(\"GenomicRanges\")\ndata(\"darned_hg19_subset500\", package = \"biovizBase\")\nautoplot(darned_hg19_subset500, layout = \"karyogram\",\n         aes(color = exReg, fill = exReg))\n\n그림 3.49: RNA 편집 사이트들이 표시된 카리오그램(Karyogram). exReg는 해당 사이트가 코딩 영역(C), 3’- 또는 5’-UTR에 있는지를 나타냅니다.\n질문 3.11\n그림 3.49에서 염색체들의 순서를 어떻게 바로잡고 염색체 길이에 대한 경고 메시지를 어떻게 없앨 수 있을까요?\n해결책\n인간 유전체의 hg19 어셈블리에서의 염색체 길이에 대한 정보는 (예를 들어) ideoCyto 데이터 세트에 저장되어 있습니다. 우리는 keepSeqlevels 함수를 사용하여 염색체들의 순서를 재조정합니다. 그림 3.50을 참조하세요.\ndata(\"ideoCyto\", package = \"biovizBase\")\ndn = darned_hg19_subset500\nseqlengths(dn) = seqlengths(ideoCyto$hg19)[names(seqlengths(dn))]\ndn = keepSeqlevels(dn, paste0(\"chr\", c(1:22, \"X\")))\nautoplot(dn, layout = \"karyogram\", aes(color = exReg, fill = exReg))\n\n그림 3.50: 그림 3.49의 개선된 버전.\n질문 3.12\ndarned_hg19_subset500은 어떤 유형의 객체인가요?\n해결책\ndarned_hg19_subset500[1:2,]\n\n\nGRanges object with 2 ranges and 10 metadata columns:\n      seqnames    ranges strand |       inchr       inrna         snp\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt; &lt;character&gt; &lt;character&gt;\n  [1]     chr5  86618225      - |           A           I        &lt;NA&gt;\n  [2]     chr7  99792382      - |           A           I        &lt;NA&gt;\n             gene      seqReg       exReg      source      ests      esta\n      &lt;character&gt; &lt;character&gt; &lt;character&gt; &lt;character&gt; &lt;integer&gt; &lt;integer&gt;\n  [1]        &lt;NA&gt;           O        &lt;NA&gt;    amygdala         0         0\n  [2]        &lt;NA&gt;           O        &lt;NA&gt;        &lt;NA&gt;         0         0\n           author\n      &lt;character&gt;\n  [1]    15342557\n  [2]    15342557\n  -------\n  seqinfo: 23 sequences from an unspecified genome; no seqlengths\n이는 유전체 좌표와 연관된 데이터를 저장하기 위한 바이오컨덕터 프로젝트의 특화된 클래스인 GRanges 객체입니다. 처음 세 개의 열은 필수적입니다: 포함된 바이오폴리머의 이름인 seqnames (우리의 사례에서는 인간 염색체 이름들), 구간의 유전체 좌표인 ranges (이 경우 각 구간은 단일 뉴클레오타이드를 나타내므로 길이는 모두 1입니다), 그리고 RNA가 전사되는 DNA 가닥인 strand입니다. 이 클래스와 연관된 인프라를 사용하는 방법에 대해 더 자세한 내용은 문서(예: GenomicRanges 패키지의 비네트)를 참조하세요. 유전체 연관 데이터 세트로 작업하고자 한다면 이를 배울 가치가 있는데, 이러한 데이터를 편리하고 효율적이며 안전하게 조작할 수 있게 해주고 많은 강력한 유틸리티를 제공하기 때문입니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>고품질 그래픽 (High-Quality Graphics)</span>"
    ]
  },
  {
    "objectID": "03-chap.html#이-장의-요약",
    "href": "03-chap.html#이-장의-요약",
    "title": "5  고품질 그래픽 (High-Quality Graphics)",
    "section": "5.13 3.14 이 장의 요약",
    "text": "5.13 3.14 이 장의 요약\n데이터를 “가공되지 않은” 상태로 보거나, 처리, 요약 및 추론의 다양한 단계에서 시각화하는 것은 응용 통계학뿐만 아니라 과학 전반에서 가장 중요한 활동 중 하나입니다. 연역적 이론이 많지 않기 때문에 교과서에서는 때때로 소홀히 다루어지기도 합니다. 하지만 수많은 좋은 (그리고 나쁜) 관행들이 있으며, 일단 주의를 기울이기 시작하면 특정 그래픽이 메시지를 전달하는 데 효과적인지, 아니면 강력하고 미적으로 매력적인 데이터 시각화를 만들기 위해 어떤 선택을 할 수 있는지 금방 알게 될 것입니다. 중요한 옵션들 중에는 플롯 유형 (ggplot2 에서 geom이라 부르는 것), 비율 (종횡비 포함), 그리고 색상이 있습니다. 그래픽 문법 은 그래픽에 대해 추론하고 데이터 시각화에 대한 우리의 의도를 컴퓨터에 전달하기 위한 강력한 개념 세트입니다.\n옵션에 대해 생각하지 않고 소프트웨어의 기본값만 사용하는 게으름 을 피하고, 플롯을 복잡하게 만들 뿐 실제 메시지는 없는 시각적 장식들을 잔뜩 추가하는 지나침 도 피하십시오. 자신만의 시각화를 만드는 것은 여러 면에서 좋은 글을 쓰는 것과 비슷합니다. (발표나 논문에서) 여러분의 메시지를 전달하는 것은 매우 중요하지만, 이를 위한 간단한 비법은 없습니다. 다른 사람들이 만든 수많은 시각화 자료를 주의 깊게 살펴보고, 요령을 터득하기 위해 자신만의 시각화를 직접 만드는 실험을 해본 다음, 여러분만의 스타일을 결정하십시오.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>고품질 그래픽 (High-Quality Graphics)</span>"
    ]
  },
  {
    "objectID": "03-chap.html#더-읽을거리",
    "href": "03-chap.html#더-읽을거리",
    "title": "5  고품질 그래픽 (High-Quality Graphics)",
    "section": "5.14 3.15 더 읽을거리",
    "text": "5.14 3.15 더 읽을거리\nggplot2 에 관한 가장 유용한 자료는 Wickham (2016)의 2판과 ggplot2 웹사이트입니다. 온라인에는 수많은 ggplot2 코드 스니펫들이 있으며, 약간의 연습을 거치면 검색 엔진을 통해 찾을 수 있을 것입니다. 하지만 비판적으로 정보를 확인하십시오: 온라인에서 찾는 코드 예제들은 때때로 패키지의 구버전을 참조하거나 품질이 좋지 않을 수 있습니다.\n이 시스템의 기초는 Wilkinson (2005)과 Tukey (1977; William S. Cleveland 1988)의 아이디어에 기반하고 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>고품질 그래픽 (High-Quality Graphics)</span>"
    ]
  },
  {
    "objectID": "03-chap.html#연습-문제",
    "href": "03-chap.html#연습-문제",
    "title": "5  고품질 그래픽 (High-Quality Graphics)",
    "section": "5.15 3.16 연습 문제",
    "text": "5.15 3.16 연습 문제\n연습 문제 3.1\n테마(themes) 를 사용하여 플롯의 시각적 외관을 변경해 보세요. 다음 예제를 실행하는 것부터 시작하십시오:\nggcars = ggplot(mtcars, aes(x = hp, y = mpg)) + geom_point()\nggcars\nggcars + theme_bw()\nggcars + theme_minimal()\n어떤 다른 테마들이 있나요? (힌트: ggplot2 온라인 문서의 themes 섹션을 살펴보세요.) 이것들이 테마의 전부인가요? (힌트: ggthemes 패키지를 살펴보세요.) 위의 플롯을 Economist 잡지 스타일로 만들어 보세요. 평활 선(smoothing line)을 추가해 보세요.\n연습 문제 3.2\n베이스 R에서 허용되는 색상 이름(color names) 은 무엇인가요? colors 함수의 매뉴얼 페이지를 살펴보세요. 예제와 데모를 실행해 보세요.\n인터넷 검색 엔진을 사용하여 “R color names”를 검색하고 검색 결과로 나오는 리소스들을 탐색해 보세요 (예: 모든 색상이 표시된 치트 시트 등).\n그림 3.41의 히트맵을 R 패키지 beyonce의 색상 팔레트를 사용하는 버전으로 만들어 보세요.\n연습 문제 3.3\nxkcd 웹툰 스타일의 플롯을 만들어 보세요. 다음 자료들을 참고하십시오:\n\nStackoverflow의 How can we make xkcd style graphs? 라는 제목의 스레드\n이 장의 시작 부분을 생성하는 코드가 포함된 이 책의 R 소스 코드\nxkcd 패키지의 비네트.\n\n\n연습 문제 3.4\nRStudio 웹사이트의 shiny 튜토리얼을 확인해 보세요. 이 장의 플롯 중 하나를 표시하지만, 표시되는 유전자 등을 제어할 수 있는 대화형 요소가 포함된 shiny 앱 을 작성해 보세요 (그림 3.33—3.35).\n연습 문제 3.5\n그래픽을 직렬화(serializing) 하는 방법, 즉 나중에 사용하기 위해 파일로 저장하거나 다른 소프트웨어에서 불러오기 위한 방법에는 어떤 것들이 있나요? 대화형 그래픽은 어떻게 직렬화할 수 있나요?\n연습 문제 3.6\n중요하고 잘 만들어진 그래픽이 반드시 복잡할 필요는 없습니다. 예를 들어, Comirnaty에 대한 유럽 의약품청(EMA)의 공공 평가 보고서21의 그림 9 (82페이지)와 XKCD 2400을 확인해 보세요.\n21 https://www.ema.europa.eu/en/medicines/human/EPAR/comirnaty 에서도 확인할 수 있습니다.\nCarr, Daniel B, Richard J Littlefield, WL Nicholson, and JS Littlefield. 1987. “Scatterplot Matrix Techniques for Large N.” Journal of the American Statistical Association 82 (398): 424–36.\nCleveland, William S. 1988. The Collected Works of John w. Tukey: Graphics 1965-1985. Vol. 5. CRC Press.\nCleveland, William S., Marylyn E. McGill, and Robert McGill. 1988. “The Shape Parameter of a Two-Variable Graph.” Journal of the American Statistical Association 83: 289–300.\nHelmholtz, H. von. 1867. Handbuch Der Physiologischen Optik. Leipzig: Leopold Voss.\nIhaka, Ross. 2003. “Color for Presentation Graphics.” In Proceedings of the 3rd International Workshop on Distributed Statistical Computing , edited by Kurt Hornik and Friedrich Leisch. Vienna, Austria: http://www.r-project.org/conferences/DSC-2003/Proceedings/; ISSN 1609-395X.\nIrizarry, R. A., B. Hobbs, F. Collin, Y. D. Beazer-Barclay, K. J. Antonellis, U. Scherf, and T. P. Speed. 2003. “Exploration, Normalization, and Summaries of High Density Oligonucleotide Array Probe Level Data.” Biostatistics 4 (2): 249–64.\nLawrence, Michael S., Petar Stojanov, Paz Polak, Gregory V. Kryukov, Kristian Cibulskis, Andrey Sivachenko, Scott L. Carter, et al. 2013. “Mutational Heterogeneity in Cancer and the Search for New Cancer-Associated Genes.” Nature 499 (7457): 214–18. https://doi.org/10.1038/nature12213.\nMcCormick Jr, William T, Paul J Schweitzer, and Thomas W White. 1972. “Problem Decomposition and Data Reorganization by a Clustering Technique.” Operations Research 20 (5): 993–1009.\nMollon, John. 1995. “Seeing Colour.” In Colour: Art and Science , edited by T. Lamb and J. Bourriau. Cambridge Unversity Press.\nOhnishi, Y., W. Huber, A. Tsumura, M. Kang, P. Xenopoulos, K. Kurimoto, A. K. Oles, et al. 2014. “Cell-to-Cell Expression Variability Followed by Signal Reinforcement Progressively Segregates Early Mouse Lineages.” Nature Cell Biology 16 (1): 27–37.\nTukey, John W. 1977. “Exploratory Data Analysis.” Massachusetts: Addison- Wesley.\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28.\n———. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer New York. http://had.co.nz/ggplot2/book.\nWilkinson, Leland. 1999. “Dot Plots.” The American Statistician 53 (3): 276.\n———. 2005. The Grammar of Graphics. Springer.\n페이지는 R 버전 4.5.1 (2025-06-13)을 사용하여 2025-09-01 01:33에 빌드되었습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>고품질 그래픽 (High-Quality Graphics)</span>"
    ]
  },
  {
    "objectID": "03-chap.html#요약",
    "href": "03-chap.html#요약",
    "title": "5  고품질 그래픽 (High-Quality Graphics)",
    "section": "5.16 3.7 요약",
    "text": "5.16 3.7 요약\n이 장에서는 R의 기본 플로팅 시스템과 ggplot2 패키지를 이용한 고품질 그래픽 작성법을 다루었습니다.\n\n데이터 시각화는 데이터의 패턴을 탐색하고 발견하는 중요한 도구일 뿐만 아니라, 결과를 효과적으로 소통하는 수단입니다.\nggplot2는 “그래픽 문법(Grammar of Graphics)”을 기반으로 하여, 데이터를 미적 속성(aesthetic mappings)에 매핑하고 기하학적 객체(geoms)를 층(layer)으로 쌓아 올리는 방식으로 복잡한 플롯을 직관적으로 생성할 수 있게 해줍니다.\n\n적절한 시각화 도구의 선택과 활용은 생물학적 데이터 분석에서 통찰력을 얻는 데 필수적입니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>고품질 그래픽 (High-Quality Graphics)</span>"
    ]
  },
  {
    "objectID": "04-chap.html",
    "href": "04-chap.html",
    "title": "6  유한 혼합 모델 (Finite Mixture Models)",
    "section": "",
    "text": "6.1 4.2 유한 혼합물\n생물학적 데이터 분석의 주요 과제 중 하나는 이질성(heterogeneity)을 다루는 것입니다. 우리가 관심을 갖는 양들은 종종 단순하고 단봉형(unimodal)인 “교과서적인 분포”를 보여주지 않습니다. 예를 들어, 2장의 마지막 부분에서 그림 2.27의 서열 점수 히스토그램이 CpG 섬과 비섬(non-islands)에 해당하는 두 개의 별개 모드(modes)를 가지고 있음을 보았습니다. 우리는 이 데이터를 몇 가지(이 경우 두 개) 성분의 단순한 혼합물로 볼 수 있습니다. 이를 유한 혼합물(finite mixtures)이라고 부릅니다. 다른 혼합물은 우리가 가진 관측치 수만큼이나 많은 성분을 포함할 수도 있습니다. 이를 무한 혼합물(infinite mixtures)이라고 부릅니다1.\n1 모델링 선택의 많은 부분이 그러하듯이, 혼합물의 적절한 복잡성은 보는 사람의 관점에 달려 있으며, 종종 데이터의 양과 우리가 달성하고자 하는 해상도 및 매끄러움(smoothness)에 따라 달라집니다.\n1장에서 우리는 푸아송 분포를 이용한 단순한 생성 모델이 에피토프(epitope) 검출에서 어떻게 유용한 추론으로 이어지는지 보았습니다. 불행히도, 그러한 단순한 모델로 실제 데이터에 만족스러운 적합(fit)을 얻는 것은 종종 불가능합니다. 하지만 정규 분포나 푸아송 분포와 같은 단순한 모델은 우리가 이 장에서 다룰 혼합 프레임워크를 사용하여 더 현실적인 모델을 구축하기 위한 구성 요소 역할을 할 수 있습니다. 혼합물은 유세포 분석(flow cytometry) 데이터, 생체 측정값, RNA-Seq, ChIP-Seq, 마이크로바이옴 및 현대 생명공학 기술을 사용하여 수집된 다른 많은 유형의 데이터에서 자연스럽게 발생합니다. 이 장에서는 단순한 예제들을 통해 혼합물을 사용하여 더 현실적인 분포 모델을 구축하는 방법을 배울 것입니다.\n이 장에서 우리는 다음을 수행할 것입니다:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>유한 혼합 모델 (Finite Mixture Models)</span>"
    ]
  },
  {
    "objectID": "04-chap.html#유한-혼합물",
    "href": "04-chap.html#유한-혼합물",
    "title": "6  유한 혼합 모델 (Finite Mixture Models)",
    "section": "",
    "text": "6.1.1 4.2.1 단순한 예제와 컴퓨터 실험\n여기에 두 개의 동일한 크기의 성분으로 구성된 혼합 모델의 첫 번째 예제가 있습니다. 생성 과정은 두 단계로 이루어집니다:\n공정한 동전을 던집니다.\n앞면이 나오면: 평균 1, 분산 0.25인 정규 분포에서 난수를 생성합니다.\n뒷면이 나오면: 평균 3, 분산 0.25인 정규 분포에서 난수를 생성합니다. 그림 4.1에 표시된 히스토그램은 다음 코드를 사용하여 이 두 단계를 10,000번 반복하여 생성되었습니다.\ncoinflips = (runif(10000) &gt; 0.5)\ntable(coinflips)\n\n\ncoinflips\nFALSE  TRUE \n 5003  4997 \noneFlip = function(fl, mean1 = 1, mean2 = 3, sd1 = 0.5, sd2 = 0.5) { if (fl) { rnorm(1, mean1, sd1) } else { rnorm(1, mean2, sd2) } } fairmix = vapply(coinflips, oneFlip, numeric(1)) library(“ggplot2”) library(“dplyr”) ggplot(tibble(value = fairmix), aes(x = value)) + geom_histogram(fill = “purple”, binwidth = 0.1)\n\n그림 4.1: 두 정규 분포의 공정한 혼합물로부터 얻은 10,000개의 난수 히스토그램. 히스토그램의 왼쪽 부분은 (A)에서 생성된 숫자들이 지배적이고, 오른쪽은 (B)에서 생성된 숫자들이 지배적입니다.\n질문 4.1\nR의 벡터화된 구문을 사용하여 vapply 루프를 제거하고 fairmix 벡터를 더 효율적으로 생성하려면 어떻게 해야 할까요?\n해결책\nmeans = c(1, 3)\nsds   = c(0.5, 0.5)\nvalues = rnorm(length(coinflips),\n               mean = ifelse(coinflips, means[1], means[2]),\n               sd   = ifelse(coinflips, sds[1],   sds[2]))\n질문 4.2\n개선된 코드를 사용하여 백만 번의 동전 던지기를 수행하고 200개의 빈(bin)을 가진 히스토그램을 만들어 보세요. 무엇을 알 수 있나요?\n해결책\nfair = tibble(\n  coinflips = (runif(1e6) &gt; 0.5),\n  values = rnorm(length(coinflips),\n                 mean = ifelse(coinflips, means[1], means[2]),\n                 sd   = ifelse(coinflips, sds[1],   sds[2])))\nggplot(fair, aes(x = values)) +\n     geom_histogram(fill = \"purple\", bins = 200)\n\n그림 4.2: 그림 4.1과 유사하지만 백만 개의 관측치를 사용한 경우.\n그림 4.2는 빈의 수와 빈당 관측치 수를 늘림에 따라 히스토그램이 매끄러운 곡선에 가까워지는 것을 보여줍니다. 이 매끄러운 한계 곡선을 확률 변수 fair$values의 밀도(density) 함수라고 부릅니다.\n정규 분포 (N(,)) 확률 변수의 밀도 함수는 명시적으로 쓸 수 있습니다. 우리는 보통 이를 다음과 같이 부릅니다.\n[ (x)=e{-()2}. ]\n질문 4.3\n\ncoinflips가 TRUE인 fair$values 값들에 대한 히스토그램을 그리세요. 힌트: aes 호출 시 y = after_stat(density)를 사용하고(이는 수직축이 빈도를 나타냄을 의미함), binwidth를 0.01로 설정하세요.\n((z))에 해당하는 선을 겹쳐서 그리세요.\n\n해결책\nggplot(dplyr::filter(fair, coinflips), aes(x = values)) +\n  geom_histogram(aes(y = after_stat(density)), fill = \"purple\", binwidth = 0.01) +\n  stat_function(fun = dnorm, color = \"red\",\n                args = list(mean = means[1], sd = sds[1]))\n\n그림 4.3: 정규 분포 (N(,2=0.52))로부터 얻은 50만 개의 난수 히스토그램. 곡선은 dnorm 함수를 사용하여 계산된 이론적 밀도 ((x))입니다.\n사실 우리는 fair$values 전체의 밀도(히스토그램이 나타내는 한계 곡선)에 대한 수학적 공식을 두 밀도의 합으로 쓸 수 있습니다.\n[ f(x)=_1(x)+_2(x), ]\n여기서 (_1)은 정규 분포 (N(_1=1,^2=0.25))의 밀도이고, (_2)는 정규 분포 (N(_2=3,^2=0.25))의 밀도입니다. 그림 4.4는 다음 코드를 통해 생성되었습니다.\nfairtheory = tibble(\n  x = seq(-1, 5, length.out = 1000),\n  f = 0.5 * dnorm(x, mean = means[1], sd = sds[1]) +\n      0.5 * dnorm(x, mean = means[2], sd = sds[2]))\nggplot(fairtheory, aes(x = x, y = f)) +\n  geom_line(color = \"red\", linewidth = 1.5) + ylab(\"mixture density\")\n\n그림 4.4: 혼합물의 이론적 밀도.\n이 경우, 두 성분 분포의 중첩이 거의 없기 때문에 혼합 모델이 매우 뚜렷하게 보입니다. 그림 4.4는 두 개의 뚜렷한 정점을 보여줍니다: 우리는 이를 이봉(bimodal) 분포라고 부릅니다. 실제로는 많은 경우 혼합 성분 사이의 분리가 그렇게 명확하지 않지만, 그럼에도 불구하고 이는 중요합니다.\n\n그림 4.5: 인식하기 더 어려운 두 정규 분포의 혼합물.\n질문 4.4\n그림 4.5는 분산이 동일한 두 정규 분포의 공정한 혼합물 히스토그램입니다. 성분 분포의 두 평균 매개변수를 추측할 수 있나요? 힌트: 시행착오를 통해 다양한 혼합물을 시뮬레이션하여 일치하는 히스토그램을 만들 수 있는지 확인할 수 있습니다. 이 장의 R 코드를 살펴보면 데이터가 정확히 어떻게 생성되었는지 알 수 있습니다.\n해결책\n다음 코드는 동전의 _앞면_에서 생성된 점은 빨간색으로, _뒷면_에서 생성된 점은 파란색으로 표시합니다. 그림 4.6에 표시된 출력 결과는 두 기저 분포를 보여줍니다.\nhead(mystery, 3)\n\n\n# A tibble: 3 × 2\n  coinflips values\n  &lt;lgl&gt;      &lt;dbl&gt;\n1 FALSE       2.40\n2 FALSE       1.66\n3 TRUE        1.22\nbr = with(mystery, seq(min(values), max(values), length.out = 30)) ggplot(mystery, aes(x = values)) + geom_histogram(data = dplyr::filter(mystery, coinflips), fill = “red”, alpha = 0.2, breaks = br) + geom_histogram(data = dplyr::filter(mystery, !coinflips), fill = “darkblue”, alpha = 0.2, breaks = br)\n\n그림 4.6: 그림 4.5의 혼합물이지만, 두 성분을 빨간색과 파란색으로 표시한 경우.\n그림 4.6에서 두 성분 분포의 막대는 서로 겹쳐서 표시됩니다. 성분을 표시하는 다른 방법은 아래 코드로 생성된 그림 4.7입니다.\nggplot(mystery, aes(x = values, fill = coinflips)) +\n  geom_histogram(data = dplyr::filter(mystery, coinflips),\n     fill = \"red\", alpha = 0.2, breaks = br) +\n  geom_histogram(data = dplyr::filter(mystery, !coinflips),\n     fill = \"darkblue\", alpha = 0.2, breaks = br) +\n  geom_histogram(fill = \"purple\", breaks = br, alpha = 0.2)\n\n그림 4.7: 그림 4.6과 유사하지만, 두 혼합 성분에 대해 누적 막대(stacked bars)를 사용한 경우.\n질문 4.5\n왜 그림 4.7의 막대 높이는 그림 4.5와 같지만, 그림 4.6의 막대 높이는 그렇지 않을까요?\n해결책\n그림 4.7과 4.5에서 각 카운트는 빈(bin) 안의 수직 공간의 서로 다른 조각을 차지합니다. 그림 4.6에서는 겹치는(더 어두운) 영역에서 동일한 빈에 떨어지는 일부 카운트들이 서로 겹쳐서 그려지게 됩니다.\n그림 4.6과 4.7에서는 데이터의 coinflips 열을 사용하여 성분들을 분리해낼 수 있었습니다. 실제 데이터에서는 이 정보가 누락되어 있습니다.\n\n\n\n유한 혼합물 주제에 대한 단행본 수준의 다룸 (McLachlan and Peel 2004).\n\n\n유한 혼합물 주제에 대한 단행본 수준의 연구 (McLachlan and Peel 2004).\n\n\n6.1.2 4.2.2 숨겨진 클래스 레이블 발견하기\n우리는 숨겨진 그룹화 값을 추론하기 위해 기댓값-최대화(Expectation-Maximization, EM) 알고리즘이라 불리는 방법을 사용합니다. EM 알고리즘은 솔루션의 한 부분을 안다고 가정하고 다른 부분을 계산하는 것과, 다른 부분을 안다고 가정하고 첫 번째 부분을 계산하는 것을 번갈아 가며 수행하는 인기 있는 반복 절차입니다. 더 구체적으로는 다음과 같이 교대로 수행합니다:\n\n각 관측치가 서로 다른 혼합 성분에 속할 확률을 안다고 가정하고, 이로부터 성분들의 매개변수를 추정합니다.\n혼합 성분들의 매개변수를 안다고 가정하고, 각 관측치가 각 성분에 속할 확률을 추정합니다.\n\n간단한 예제를 들어보겠습니다. 우리는 두 그룹에서 온다고 생각되는 일련의 객체들에 대해 변수 (x)를 측정하지만, 그룹 레이블은 알지 못합니다. 우리는 측정되지 않은(잠재된) 그룹 레이블인 (U)를 데이터에 추가(augmenting) 2하는 것으로 시작합니다.\n2 측정되지 않은 또 다른 변수를 추가하는 것으로, 이를 숨겨진 또는 잠재 변수(latent variable)라고 부릅니다.\n우리는 (U)의 값과 그룹들의 기저 분포에 대한 알려지지 않은 매개변수 ()를 찾는 데 관심이 있습니다. 우리는 2장에서 소개된 최대 우도(maximum likelihood) 접근법을 사용하여 데이터 (x)를 가장 발생 가능하게 만드는 매개변수를 추정할 것입니다. 우리는 확률 밀도를 다음과 같이 쓸 수 있습니다.\n[ p(x,u,|,) = p(x,|,u,),p(u,|,). ]\n\n6.1.2.1 정규 혼합물\n예를 들어, 우리는 식 4.1의 두 정규 분포를 이용한 이전의 혼합 모델을 다음과 같이 동일하지 않은 혼합 비율을 허용하도록 일반화할 수 있습니다.\n[ f(x)=_1(x)+(1-)_2(x), ]\n여기서 ()입니다. 앞서와 유사하게, (_k)는 각각 (k=1, 2)에 대한 정규 분포 (N(_k,_k^2))의 밀도입니다. 그러면 매개변수 벡터 ()는 두 개의 평균, 두 개의 표준 편차, 그리고 혼합 매개변수 ()로 구성된 5-튜플(tuple)입니다. 다시 말해, (=(_1,_2,_1,_2,))입니다. 여기에 그러한 모델에 따라 생성된 데이터의 예가 있습니다. 레이블은 (u)로 표시됩니다.\nmus = c(-0.5, 1.5)\nlambda = 0.5\nu = sample(2, size = 100, replace = TRUE, prob = c(lambda, 1-lambda))\nx = rnorm(length(u), mean = mus[u])\ndux = tibble(u, x)\nhead(dux)\n\n\n# A tibble: 6 × 2\n      u     x\n  &lt;int&gt; &lt;dbl&gt;\n1     2 0.303\n2     2 2.65 \n3     1 0.484\n4     2 3.04 \n5     2 1.10 \n6     2 1.96 \n만약 레이블 (u)를 알고 있다면, 우리는 각 그룹에 대해 별도의 MLE를 사용하여 매개변수를 추정할 수 있습니다. 전체 우도(likelihood)는 다음과 같습니다.\n[ p(x, u ,|, ) = ( _{{i:,u_i=1}} 1(x_i) ) ( {{i:,u_i=2}} _2(x_i) ). ]\n이 식의 최대화는 세 개의 독립적인 조각으로 나눌 수 있습니다: 식 4.4의 우변에 있는 첫 번째 괄호 안의 식을 최대화하여 (_1)과 (_1)을 찾고, 두 번째 조각을 최대화하여 (_2)와 (_2)를 찾으며, 레이블의 경험적 빈도로부터 ()를 찾습니다.\ngroup_by(dux, u) |&gt; summarize(mu = mean(x), sigma = sd(x))\n# A tibble: 2 × 3\n      u     mu sigma\n  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     1 -0.558  1.05\n2     2  1.41   1.04\ntable(dux$u) / nrow(dux)\n   1    2 \n0.55 0.45 \n질문 4.6\n혼합 비율 (=)은 알고 있지만 (u_i)는 모른다고 가정해 봅시다. 이 경우 밀도는 (_1(x)+_2(x))입니다. (로그) 우도를 직접 써보세요. 여기서 MLE를 명시적으로 구하는 것을 방해하는 요인은 무엇인가요?\n해결책\n정규 분포의 유한 혼합물에 대한 우도 계산에 대해서는, 예를 들어 Shalizi (2017)의 혼합 모델(Mixture Models) 장을 참조하세요. “혼합 모델을 추정하려 할 때, 우리는 사후 레이블 확률(posterior label probabilities)에 의해 주어지는 가중치를 사용하여 가중 최대 우도(weighted maximum likelihood)를 수행하게 됩니다. 거듭하지만, 이 확률들은 우리가 추정하려는 매개변수들에 의존하므로, 일종의 악순환이 생기는 것 같습니다.”\n많은 경우 우리는 (u) 레이블도, 혼합 비율도 알지 못합니다. 우리가 할 수 있는 일은 레이블에 대한 초기 추측에서 시작하여 이를 알고 있다고 가정하고 위와 같이 매개변수를 추정한 다음, 우리 추정치가 실질적으로 변하지 않을 때까지(즉, 수렴할 때까지) 그리고 우도가 최적값에 도달할 때까지 매 단계마다 레이블과 매개변수에 대한 현재의 최선의 추측을 업데이트하는 반복적인 사이클을 거치는 것입니다.\n사실 우리는 각 관측치에 대해 “딱딱한(hard)” 레이블 (u)(그룹 1 아니면 2 중 하나에 속함)를 부여하는 대신, 이를 합이 1이 되는 멤버십 가중치(membership weights)로 대체하는 더 정교한 작업을 수행할 수 있습니다. 혼합 모델 4.3은 다음을 제안합니다.\n[ w(x, 1) = ]\n이 식은 값 (x)를 가진 관측치가 첫 번째 혼합 성분에 의해 생성되었을 확률로 해석될 수 있으며, 두 번째 성분에 대해서는 유사하게 (w(x, 2) = 1 - w(x, 1))이 됩니다. 즉, ()가 아직 보지 못한 관측치가 혼합 성분 1로부터 나올 사전 확률(prior probability)이라면, (w(x,1))은 그 값 (x)를 관찰한 후의 대응하는 사후 확률(posterior probability)입니다. 이는 다음과 같은 반복 알고리즘을 제안합니다:\nE 단계 : ()(즉, 평균, 표준 편차 및 ())가 알려져 있다고 가정하고, 멤버십 가중치 4.5를 평가합니다.\nM 단계 : 각 관측치 (x_i)의 멤버십 가중치가 주어졌을 때, ()의 새롭고 개선된 최대 우도 추정치를 결정합니다.\n()와 우도가 수렴할 때까지 반복합니다. 이 시점에서 코드로 작성된 데모인 연습 문제 4.1을 확인해 보세요. 사실 이 알고리즘은 여기서 다루는 특정 응용 예제보다 훨씬 더 일반적입니다. (Bishop 2006)은 매우 읽기 쉬운 설명을 제시하고 있으며, 주요 내용은 다음과:\n우리의 목표는 관측 변수 (x), 관찰되지 않은 변수 (u) 및 일부 매개변수 ()를 포함하는 확률 모델의 주변 우도(marginal likelihood)를 최대화하는 것입니다. 우리의 단순한 예제에서 (u)는 두 가지 가능한 값을 가진 범주형 변수이고, (x)는 실수입니다. 일반적으로 (x)와 (u) 모두 모든 유형의 개별 변수들의 튜플(즉, 다변량)일 수 있습니다. 주변 우도는 (u)의 모든 가능한 값들에 대해 기댓값(즉, 가중 평균)을 취함으로써 계산됩니다:\n[ L_(; x) = _U p(x, u,|,) , U. ]\n우리의 구체적인 예제에서, 적분은 가능한 모든 멤버십 구성에 대해 (확률적으로) 평균을 내는 것에 해당하며, 따라서 멤버십 가중치를 고려한 가중 합이 됩니다.\n[ L_(; x) = {i=1}^n {u=1}^2 p(x_i, u,|,) , w(x_i, u,|,). ]\n이 수치를 직접 최대화하는 것은 다루기 힘든(intractable) 일입니다.\n데이터와 현재의 매개변수 추정치 (_t)가 주어졌을 때 우리가 파악할 수 있는 것은 잠재 변수의 조건부 분포인 (p(u,|,x, _t))입니다. 우리는 이를 사용하여 일반적인 매개변수 값 ()에 대해 평가된 완전 데이터 로그 우도(complete data log likelihood) (p(x, u,|,))의 기댓값을 찾을 수 있습니다. 이 기댓값은 종종 다음과 같이 표시됩니다.\n[ Q(, _t) = _U p(x, u,|,) , p(u,|, x, _t) , U. ]\nM 단계에서는 이 함수를 최대화하여 수정된 매개변수 추정치 (_{t+1})을 결정합니다.\n[ {t+1} = , Q(, _t). ]\nE 단계는 (Q)의 핵심 요소인 (p(u,|, x, _t))를 계산하는 것으로 구성됩니다.\n이 두 단계(E와 M)는 개선 사항이 작아질 때까지 반복됩니다; 이는 우리가 우도의 평평한 부분에 도달했으며 최댓값에 이르렀다는 수치적 지표입니다. 반복 궤적은 시작 지점에 따라 달라지겠지만, 도달하는 지점은 달라지지 않기를 바랍니다. 이는 산 정상에 오르는 것과 비슷합니다. 산 정상에 오르는 길은 시작 지점에 따라 다를 수 있고 경로도 다를 수 있지만, 산봉우리가 하나만 있고 여러 개가 아닌 이상 항상 정상으로 이어집니다. 따라서 예방 조치로서, 이러한 절차를 서로 다른 시작 지점에서 여러 번 반복하여 항상 동일한 답을 얻는지 확인하는 것이 좋습니다.\n질문 4.7\n여러 R 패키지에서 mclust, EMcluster, EMMIXskew를 포함한 EM 구현을 제공합니다. 하나를 선택하여 서로 다른 시작 값으로 EM 함수를 여러 번 실행해 보세요. 그런 다음 mixtools 패키지의 normalmixEM 함수를 사용하여 출력을 비교해 보세요.\n해결책\n여기서는 mixtools의 출력을 보여줍니다.\nlibrary(\"mixtools\")\ny = c(rnorm(100, mean = -0.2, sd = 0.5),\n      rnorm( 50, mean =  0.5, sd =   1))\ngm = normalmixEM(y, k = 2, \n                    lambda = c(0.5, 0.5),\n                    mu = c(-0.01, 0.01), \n                    sigma = c(3, 3))\n\n\nnumber of iterations= 134 \n\n\nwith(gm, c(lambda, mu, sigma, loglik))\n\n\n[1]    0.9263    0.0737   -0.1859    1.9216    0.6134    0.4746 -170.5662\nEM 알고리즘은 매우 유익합니다:\n\n우리는 관측치가 한 그룹에 속하는지 다른 그룹에 속하는지 결정하지 않고, 멤버십 확률을 가중치로 사용하여 여러 그룹에 참여할 수 있게 함으로써 더 미묘한 추정치를 얻는 “부드러운(soft)” 평균화의 첫 번째 사례를 보았습니다 (Slonim et al. 2005).\n미지수가 너무 많은 어려운 문제를 더 간단한 문제들을 번갈아 가며 해결함으로써 어떻게 다룰 수 있는지 보여줍니다.\n숨겨진 변수가 있는 데이터 생성 모델을 고려하면서도 그 매개변수를 추정할 수 있었습니다. 숨겨진 변수의 값을 명시적으로 확정하지 않고도 그렇게 할 수 있었습니다: 식 4.8의 기댓값 단계에서 멤버십 확률로 구체화된 이들에 대해 (가중) 평균을 취했습니다. 이 기본 아이디어는 매우 강력하여 기계 학습의 많은 고급 알고리즘의 출발점이 됩니다 (Bishop 2006).\n이는 모델 평균화(model averaging, Hoeting et al. 1999)의 더 일반적인 경우로 확장될 수 있습니다. 우리 데이터에 어떤 모델이 적합한지 확신할 수 없는 경우 여러 모델을 동시에 고려하는 것이 때때로 유익할 수 있습니다. 우리는 이들을 가중 모델로 결합할 수 있습니다. 가중치는 모델의 우도에 의해 제공됩니다.\n\n\n\n\n6.1.3 4.2.3 영-과잉 데이터(zero inflated data)를 위한 모델\n생태학적 및 분자적 데이터는 종종 카운트(counts)의 형태로 나타납니다. 예를 들어, 여러 장소 각각에 있는 여러 종 각각의 개체 수일 수 있습니다. 그러한 데이터는 종종 두 가지 시나리오의 혼합으로 볼 수 있습니다: 종이 존재하지 않으면 카운트는 반드시 0이지만, 만약 종이 존재한다면 우리가 관찰하는 개체 수는 무작위 샘플링 분포에 따라 달라지며, 이 분포에도 0이 포함될 수 있습니다. 우리는 이를 다음과 같이 혼합 모델로 모델링합니다:\n[ f_(x) = , (x) + (1-) , f_(x), ]\n여기서 ()는 모든 질량이 0에 집중되어 있는 확률 분포를 나타내는 디랙 델타(Dirac’s delta) 함수입니다. 첫 번째 혼합 성분에서 발생하는 0은 “구조적(structural)”이라고 불립니다: 우리 예제에서 이는 특정 종이 특정 서식지에 살지 않기 때문에 발생합니다. 두 번째 성분인 (f_) 역시 단순히 무작위 샘플링으로 인해 0이나 다른 작은 숫자들을 포함할 수 있습니다. R 패키지 pscl (Zeileis, Kleiber, and Jackman 2008)과 zicounts는 영-과잉(zero inflated) 카운트를 다루기 위한 많은 예제와 함수를 제공합니다.\n\n6.1.3.1 예시: ChIP-Seq 데이터\nChIP-Seq 데이터의 예제를 살펴봅시다. 이 데이터는 염색질 면역 침전(chromatin immunoprecipitation, ChIP)으로부터 얻은 DNA 조각의 서열입니다. 이 기술은 전사 인자, 뉴클레오솜, 히스톤 수정, 염색질 리모델링 효소, 샤페론, 중합효소 및 기타 단백질의 게놈 DNA 상의 위치를 매핑할 수 있게 해줍니다. 이는 DNA 요소 백과사전(ENCODE) 프로젝트에서 사용된 주요 기술이었습니다. 여기서는 mosaicsExample 패키지(Kuan et al. 2011)의 예제를 사용합니다. 이 예제는 GM12878 세포주에 적용된 STAT1 단백질 및 H3K4me3 히스톤 수정에 대한 항체의 ChIP-Seq으로부터 22번 염색체에서 측정된 데이터를 보여줍니다. 여기서는 binTFBS 객체를 생성하는 코드는 보여주지 않습니다; 이는 이 장의 소스 코드 파일에 나와 있으며 mosaics 패키지의 비네트를 따릅니다.\nbinTFBS\n\n\nSummary: bin-level data (class: BinData)\n----------------------------------------\n- # of chromosomes in the data: 1\n- total effective tag counts: 462479\n  (sum of ChIP tag counts of all bins)\n- control sample is incorporated\n- mappability score is NOT incorporated\n- GC content score is NOT incorporated\n- uni-reads are assumed\n----------------------------------------\n이 객체로부터 빈(bin)당 카운트의 히스토그램을 생성할 수 있습니다.\nbincts = print(binTFBS)\nggplot(bincts, aes(x = tagCount)) +\n  geom_histogram(binwidth = 1, fill = \"forestgreen\")\n\n그림 4.8: ChIP-Seq 데이터 세트에서 22번 염색체를 따라 200nt 윈도우에서 발견된 결합 부위의 수.\n그림 4.8에서 우리는 0이 아주 많다는 것을 알 수 있는데, 비록 이 플롯만으로는 다른 작은 숫자들((1,2,…))의 빈도를 고려할 때 0의 개수가 정말로 특별한지는 즉각적으로 명확하지 않습니다.\n질문 4.8\n\n(y)축에 상용로그(base 10) 스케일을 사용하여 카운트 히스토그램을 다시 그리세요.\n카운트가 0인 빈(bin)의 비율인 (_0)을 추정하세요.\n\n해결책\nggplot(bincts, aes(x = tagCount)) + scale_y_log10() +\n   geom_histogram(binwidth = 1, fill = \"forestgreen\")\n\n그림 4.9: 그림 4.8과 같지만, (y)축에 상용로그(base 10) 스케일을 사용한 모습. 0의 분율이 1, 2, …의 분율과 비교했을 때 높아 보입니다.\n\n\n\n6.1.4 4.2.4 두 개 이상의 성분\n지금까지 우리는 두 개의 성분을 가진 혼합물을 살펴보았습니다. 우리는 이를 더 확장할 수 있습니다. 예를 들어, 데옥시리보뉴클레오타이드 일인산(deoxyribonucleotide monophosphates)의 혼합물로부터 얻은 N=7,000개의 뉴클레오타이드 무게를 잴 때 (각 유형은 서로 다른 무게를 가지며, 동일한 표준 편차 sd=3으로 측정됨), 다음 코드로 생성된 그림 4.10과 같은 히스토그램을 관찰할 수 있습니다.\nmasses = c(A = 331, C = 307, G = 347, T = 322) probs = c(A = 0.12, C = 0.38, G = 0.36, T = 0.14) N = 7000 sd = 3 nuclt = sample(length(probs), N, replace = TRUE, prob = probs) quadwts = rnorm(length(nuclt), mean = masses[nuclt], sd = sd) ggplot(tibble(quadwts = quadwts), aes(x = quadwts)) + geom_histogram(bins = 100, fill = “purple”)\n\n그림 4.10: 7,000개 뉴클레오타이드 질량 측정 시뮬레이션.\n질문 4.9\n이 시뮬레이션 실험을 (N=1000)개의 뉴클레오타이드 측정으로 반복해 보세요. 히스토그램에서 무엇을 발견했나요?\n질문 4.10\n(N=7000)이지만 표준 편차가 10이면 어떻게 될까요?\n질문 4.11\n그림 4.10에서 시뮬레이션된 분포에 대한 이론적 밀도 곡선을 그리세요.\n이 경우, 우리는 충분한 수의 정밀한 측정값을 가지고 있기 때문에 네 개의 뉴클레오타이드를 구별하고 그림 4.10에 표시된 분포를 분해할 수 있습니다. 데이터가 적거나 측정 노이즈가 더 많다면, 네 개의 모드와 분포 성분이 덜 명확해질 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>유한 혼합 모델 (Finite Mixture Models)</span>"
    ]
  },
  {
    "objectID": "04-chap.html#경험적-분포와-비모수적-붓스트랩",
    "href": "04-chap.html#경험적-분포와-비모수적-붓스트랩",
    "title": "6  유한 혼합 모델 (Finite Mixture Models)",
    "section": "6.2 4.3 경험적 분포와 비모수적 붓스트랩",
    "text": "6.2 4.3 경험적 분포와 비모수적 붓스트랩\n이 섹션에서는 혼합 모델의 극단적인 사례를 고려할 것입니다. 여기서 우리는 (n)개의 데이터 포인트 표본을 (n)개의 점 질량(point masses)의 혼합물로 모델링할 것입니다. 우리는 여기서 거의 모든 데이터 세트를 사용할 수 있습니다; 구체적으로, 우리는 다윈(Darwin)의 Zea Mays 데이터3를 사용합니다. 이 데이터에서 그는 15쌍의 Zea Mays 식물의 높이를 비교했습니다 (15개의 자가 수분 식물 대 15개의 타가 수분 식물). 이 데이터는 HistData 패키지에서 사용할 수 있으며, 우리는 15개의 높이 차이 분포를 플롯합니다:\n3 이 데이터는 다윈에 의해 수집되었으며, 그는 사촌인 프랜시스 골턴(Francis Galton)에게 분석을 요청했습니다. R.A. 피셔는 동일한 데이터를 사용하여 쌍체 t-검정(paired t-test)으로 재분석했습니다 (Bulmer 2003). 우리는 이 예제로 13장에서 다시 돌아올 것입니다.\nlibrary(\"HistData\")\nZeaMays$diff\n\n\n [1]  6.125 -8.375  1.000  2.000  0.750  2.875  3.500  5.125  1.750  3.625\n[11]  7.000  3.000  9.375  7.500 -6.000\n\n\nggplot(ZeaMays, aes(x = diff, ymax = 1/15, ymin = 0)) +\n  geom_linerange(linewidth = 1, col = \"forestgreen\") + ylim(0, 0.1)\n\n그림 4.11: 관측된 표본은 각 값에서의 점 질량들의 혼합물로 보여질 수 있습니다 (실제 점 질량은 너비가 전혀 없는 막대일 것입니다).\n3.6.7절에서 우리는 크기 (n)인 표본에 대한 경험적 누적 분포 함수(ECDF)가 다음과 같음을 보았습니다.\n[ n(x)= {i=1}^n _{x x_i}, ]\n그리고 그림 3.24에서 ECDF 플롯을 보았습니다. 우리는 또한 우리 표본의 _밀도_를 다음과 같이 쓸 수 있습니다.\n[ n(x) ={i=1}^n _{x_i}(x) ]\n일반적으로 확률 분포의 밀도는 (존재한다면) 분포 함수의 도함수입니다. 우리는 여기서 이 원리를 적용했습니다: 식 4.11에 의해 정의된 분포의 밀도는 식 4.12입니다. 함수 (a)를 계단 함수 ({}{x a})의 “도함수”로 간주할 수 있기 때문에 이렇게 할 수 있었습니다: 이 함수는 계단이 있는 한 지점 (a)를 제외하고는 거의 모든 곳에서 완전히 평평하며, 그 지점에서의 값은 “무한”입니다. 식 4.12는 우리의 데이터 표본이 그림 4.11에서 보듯이 관측된 값 (x_1, x_2, …, x_n)에서의 (n)개의 점 질량(point masses)의 혼합물로 간주될 수 있음을 강조합니다.\n이것이 엄밀히 성립하려면 (표준 미적분학을 넘어서는) 약간의 고급 수학이 필요하지만, 여기서는 다루지 않겠습니다.\n\n그림 4.12: 통계량 ()의 값은 기저 분포 (F)로부터 생성된 데이터(회색 행렬들)로부터 추정됩니다. (F)로부터의 서로 다른 표본들은 서로 다른 데이터로 이어지고, 따라서 추정치 ()의 값도 달라집니다: 이를 표집 가변성(sampling variability)이라고 합니다. 모든 ()들의 분포가 표집 분포(sampling distribution)입니다.\n평균, 최솟값 또는 중앙값과 같은 우리 표본의 통계량은 이제 ECDF의 함수로 쓰일 수 있습니다. 예를 들어, ({x} = {x_i}(x),x)입니다. 또 다른 예로, (n)이 홀수라면 중앙값은 정렬된 리스트의 정중앙에 있는 값인 (x{()})입니다.\n통계량 ()의 실제 표집 분포는 이를 계산하기 위해 많은 서로 다른 데이터 표본들이 필요하기 때문에 알기 어려운 경우가 많습니다; 이는 그림 4.12에 나와 있습니다.\n붓스트랩(bootstrap) 원리는 원래 표본으로부터 구축된 경험적 분포로부터 뽑은 새로운 표본들을 생성함으로써 ()의 실제 표집 분포를 근사합니다 (그림 4.13). 우리는 데이터를 (()들의 혼합 분포로 간주하여) 재사용 하여 표본을 추출하고, 그로부터 계산된 통계량 (^*)의 표집 분포를 살펴봄으로써 새로운 “데이터 세트”들을 만듭니다. 이를 비모수적 붓스트랩(nonparametric bootstrap) 재표본 추출 접근법이라고 하며, 완전한 참고 문헌은 Bradley Efron과 Tibshirani (1994)를 참조하십시오. 이는 아무리 복잡하더라도 기본적으로 모든 통계량에 적용할 수 있는 매우 다재다능하고 강력한 방법입니다. 우리는 5장에서 이 방법의 응용 예시, 특히 군집화(clustering)에 대해 살펴볼 것입니다.\n\n그림 4.13: 붓스트랩은 (그림 4.12에서처럼) 기저의 실제 분포 (F)로부터가 아니라 경험적 분포 함수 (_n)으로부터 표본을 추출함으로써 표집 가변성을 시뮬레이션합니다.\n이러한 아이디어들을 사용하여, 그림 4.11에서 보았던 Zea Mays 차이의 중앙값에 대한 표집 분포를 추정해 봅시다. 우리는 이전 섹션들과 유사한 시뮬레이션을 사용합니다: 15개의 값들(이들 각각은 15-성분 혼합물의 한 성분임)로부터 크기가 15인 표본을 (B=1000)번 추출합니다. 그런 다음 이 15개 값들로 이루어진 1000개 표본 각각의 중앙값을 계산하고 그 분포를 살펴봅니다: 이것이 중앙값의 붓스트랩 표집 분포입니다.\nB = 1000\nmeds = replicate(B, {\n  i = sample(15, 15, replace = TRUE)\n  median(ZeaMays$diff[i])\n})\nggplot(tibble(medians = meds), aes(x = medians)) +\n  geom_histogram(bins = 30, fill = \"purple\")\n\n그림 4.14: Zea Mays 차이의 중앙값에 대한 붓스트랩 표집 분포.\n질문 4.12\n이 시뮬레이션들에 기초하여 중앙값에 대한 99% 신뢰 구간을 추정해 보세요. 이 구간과 0 사이의 중첩을 보고 무엇을 결론지을 수 있나요?\n질문 4.13\nbootstrap 패키지의 bootstrap 함수를 사용하여 median과 mean 모두에 대해 동일한 분석을 다시 수행해 보세요. 평균과 중앙값의 표집 분포 사이에서 어떤 차이점을 발견했나요?\n해결책\nlibrary(\"bootstrap\")\nbootstrap(ZeaMays$diff, B, mean)\nbootstrap(ZeaMays$diff, B, median)\n\n6.2.0.1 왜 비모수적(nonparametric)인가요?\n이론 통계학에서 비모수적 방법이란 무한히 많은 자유도나 알려지지 않은 매개변수 수를 가진 방법을 말합니다.\n\n실제로는 무한대까지 기다리지 않습니다; 매개변수 수가 가용한 데이터의 양만큼 많거나 그보다 많아지면 우리는 그 방법을 비모수적이라고 부릅니다. 붓스트랩은 (n)개의 성분을 가진 혼합물을 사용하므로, 크기 (n)인 표본에 대해 비모수적 방법의 요건을 갖춥니다.\n이름에도 불구하고, 비모수적 방법은 매개변수를 사용하지 않는 방법이 아닙니다: 모든 통계적 방법은 알려지지 않은 양을 추정합니다.\n질문 4.14\n표본이 (n=3)개의 서로 다른 값들로 구성되어 있다면, 몇 가지의 서로 다른 붓스트랩 재표본이 가능할까요? (n=15)인 경우에 대해서도 답해 보세요.\n해결책\n모든 붓스트랩 재표본의 집합은 합이 (n)인 (n)개 정수 벡터들의 집합과 동일합니다. 관측치 (x_1, x_2, …, x_n)이 붓스트랩 표본에 나타나는 횟수를 ( = (k_1, k_2, …, k_n))이라고 합시다. 각 (k_i)를 (다항 분포에서와 같이) 상자로 생각할 수 있으며, (n)개의 공을 떨어뜨릴 (n)개의 상자가 있습니다. 구성(configurations)의 수를 세는 방법은 (n)개의 공을 상자들에 나누는 방법의 수를 세는 것입니다. 즉, 공을 나타내는 o를 (n)번 쓰고 그 사이에 구분선 |를 (n-1)번 쓰는 것입니다. 따라서 우리는 o(공)나 |(구분선) 중 하나를 선택해야 하는 (2n-1)개의 자리를 채워야 합니다. (n=3)인 경우, 가능한 배치는 oo||o일 수 있으며, 이는 ( = (2,0,1))에 해당합니다. 일반적으로 이 숫자는 ({2n-1} )이며, 따라서 (n=3)과 (15)에 대한 답은 다음과 같습니다:\nc(N3 = choose(5, 3), N15 = choose(29, 15))\n      N3      N15 \n      10 77558760 \n질문 4.15\nbootstrap 패키지에 구현된 붓스트랩을 사용할 때 발생할 수 있는 두 가지 유형의 오류는 무엇인가요? 이들 중 하나를 개선하기 위해 어떤 매개변수를 수정할 수 있나요?\n해결책\n무작위 재표본 추출을 통한 데이터 하위 집합의 몬테카를로 시뮬레이션은 전수 조사 붓스트랩(exhaustive bootstrap)을 근사합니다 (Diaconis and Holmes 1994). bootstrap 함수의 nboot 인수 크기를 늘리면 몬테카를로 오류를 줄일 수 있지만, 전수 조사 붓스트랩 또한 여전히 정확하지는 않습니다: 우리는 여전히 실제 분포 대신 데이터의 근사적인 분포 함수를 사용하고 있기 때문입니다. 표본 크기가 작거나 원래 표본에 편향이 있는 경우, nboot를 아무리 크게 선택하더라도 근사는 여전히 상당히 나쁠 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>유한 혼합 모델 (Finite Mixture Models)</span>"
    ]
  },
  {
    "objectID": "04-chap.html#무한-혼합물",
    "href": "04-chap.html#무한-혼합물",
    "title": "6  유한 혼합 모델 (Finite Mixture Models)",
    "section": "6.3 4.4 무한 혼합물",
    "text": "6.3 4.4 무한 혼합물\n때때로 혼합물은 우리가 각 관측치에 레이블을 할당하려 하지 않더라도 유용할 수 있으며, 달리 말하면 관측치 수만큼 많은 ’레이블’을 허용하는 경우입니다. 만약 혼합 성분의 수가 관측치 수만큼 많다면(혹은 더 많다면), 우리는 무한 혼합물(infinite mixture) 을 가지고 있다고 말합니다. 몇 가지 예제를 살펴봅시다.\n\n6.3.1 4.4.1 정규 분포의 무한 혼합물\n\n그림 4.15: 라플라스는 이미 확률 밀도 [f_X(y)=(-),&gt;0] 가 그 위치 매개변수 ()로서 중앙값을 가지고, 척도 매개변수 ()를 추정하는 데 중앙값 절대 편차(MAD)가 사용될 수 있음을 알고 있었습니다.\n다음과 같은 2단계 데이터 생성 체계를 고려해 봅시다:\n레벨 1 지수 분포로부터 W들의 표본을 생성합니다.\nw = rexp(10000, rate = 1)\n레벨 2 이 w들은 rnorm을 사용하여 생성된 평균 ()인 정규 변수들의 분산 역할을 합니다.\nmu  = 0.3\nlps = rnorm(length(w), mean = mu, sd = sqrt(w))\nggplot(data.frame(lps), aes(x = lps)) +\n  geom_histogram(fill = \"purple\", binwidth = 0.1)\n\n그림 4.16: 라플라스 분포로부터 샘플링된 데이터.\n이것은 상당히 유용한 분포인 것으로 밝혀졌습니다. 이는 잘 이해된 속성들을 가지고 있으며, 중앙값이 그 위치 매개변수 ()의 좋은 추정량이고 중앙값 절대 편차가 척도 매개변수 ()를 추정하는 데 사용될 수 있음을 증명한 라플라스의 이름을 따서 명명되었습니다. 그림 4.15의 캡션에 있는 공식으로부터 우리는 (L_1) 거리(차이의 절댓값)가 라플라스 밀도에서 차지하는 위치가, 정규 밀도에서의 (L_2) 거리(차이의 제곱)의 위치와 유사함을 알 수 있습니다.\n반대로, 베이지안 회귀4에서 계수들에 라플라스 분포를 사전 분포로 갖는 것은 라쏘(lasso) (Tibshirani 1996)라고 불리는 (L_1) 페널티와 동일하며, 반면 사전 분포로 정규 분포를 갖는 것은 릿지(ridge) 회귀라고 불리는 (L_2) 페널티로 이어집니다.\n4 이에 익숙하지 않다면 걱정하지 마세요. 그 경우 이 문장은 그냥 건너뛰셔도 됩니다.\n질문 4.16\n대칭 라플라스 분포를 따르는 확률 변수를 정규 및 지수 확률 변수들의 함수로 써보세요.\n해결책\n우리는 분산이 지수 변수 (W)로 생성되는 계층적 모델을 다음과 같이 쓸 수 있습니다:\n[ X = Z, W Exp(1), Z N(0,1). ]\n\n6.3.1.1 비대칭 라플라스 (Asymmetric Laplace)\n라플라스 분포에서 정규 성분들의 분산은 (W)에 의존하는 반면, 평균은 영향을 받지 않습니다. 한 가지 유용한 확장은 성분들의 위치나 중심을 제어하는 또 다른 매개변수 ()를 추가하는 것입니다. 우리는 (W)가 지수 변수인 계층적 모델로부터 데이터 alps를 생성합니다; 그림 4.17에 표시된 출력 결과는 정규 분포 (N(+w,w)) 난수들의 히스토그램이며, 여기서 (w)들 자체는 코드에 표시된 대로 평균이 1인 지수 분포로부터 무작위로 생성되었습니다:\nmu = 0.3; sigma = 0.4; theta = -1\nw  = rexp(10000, 1)\nalps = rnorm(length(w), theta + mu * w, sigma * sqrt(w))\nggplot(tibble(alps), aes(x = alps)) +\n  geom_histogram(fill = \"purple\", binwidth = 0.1)\n\n그림 4.17: 비대칭 라플라스 분포로부터 생성된 데이터의 히스토그램 – 평균과 분산이 서로 종속적인 많은 정규 분포들의 척도 혼합물(scale mixture)입니다. 우리는 (X AL(, , ))라고 씁니다.\n데이터의 모든 인스턴스가 저마다의 평균과 분산을 가지는 이러한 계층적 혼합 분포는 많은 생물학적 환경에서 유용한 모델입니다. 예제들이 그림 4.18에 나와 있습니다.\n\n\nKristiansson 등 (2009)이 연구한 Saccharomyces cerevisiae 의 2000bp보다 짧은 프로모터의 길이.\n\n\n\n20,000개 유전자에 대한 마이크로어레이 유전자 발현 측정값의 로그 비(log-ratios) (Purdom and Holmes 2005).\n\n그림 4.18: 실제 데이터의 히스토그램. 두 분포 모두 비대칭 라플라스 분포로 모델링될 수 있습니다.\n질문 4.17\n마이크로어레이로부터 얻은 유전자 발현 값들의 로그 비를 살펴보면, 그림 4.18의 오른쪽에 표시된 것과 같은 분포를 얻게 됩니다. 데이터가 이러한 형태의 히스토그램을 가지는 이유를 어떻게 설명할 수 있을까요?\n라플라스 분포는 생성 과정에 대한 고려가 어떻게 분산과 평균이 연결되어 있는지를 나타내주는 한 예입니다. 비대칭 라플라스 분포 (AL(, , ))의 기댓값과 분산은 다음과 같습니다.\n[ E(X)=+(X)=2+2. ]\n(= 0)인 경우(대칭 라플라스 분포의 사례)를 제외하고는 분산이 평균에 의존한다는 점에 주목하세요. 이 분포를 유용하게 만드는 것은 바로 이 특징입니다. 평균-분산 의존성을 갖는 것은 마이크로어레이 형광 강도든, 질량 분석기의 피크 높이든, 고처리량 시퀀싱의 리드 카운트든 물리적 측정값들에서 매우 흔하게 나타나며, 이는 다음 섹션에서 살펴볼 것입니다.\n\n\n\n6.3.2 4.4.2 푸아송 변수들의 무한 혼합물\n\n그림 4.19: 호수에서 물고기를 어떻게 셀까요? M.C. 에셔.\n실제 세계의 카운트 데이터를 모델링하기 위해 이와 유사한 2단계 계층 모델이 종종 필요합니다. 하위 레벨에서는 단순한 푸아송 및 이항 분포들이 구성 요소 역할을 하지만, 그들의 매개변수들은 어떤 기저의(잠재된) 과정에 의존할 수 있습니다. 예를 들어 생태학에서 우리는 한 지역의 모든 호수에 있는 물고기 종의 변동에 관심이 있을 수 있습니다. 우리는 각 호수의 물고기 종을 샘플링하여 그들의 실제 풍부도를 추정하며, 이는 푸아송 분포로 모델링될 수 있습니다. 하지만 실제 풍부도는 호수마다 다를 것입니다. 그리고 만약 우리가 예를 들어 기후나 고도의 변화가 어떤 역할을 하는지 보고 싶다면, 우리는 그러한 체계적인 효과들을 무작위적인 호수 간 변동으로부터 분리해내야 합니다. 서로 다른 푸아송 비율 매개변수 ()들은 비율들의 분포로부터 오는 것으로 모델링될 수 있습니다. 그러한 계층 모델은 또한 계층 구조에 보충적인 단계들을 추가할 수 있게 해주는데, 예를 들어 우리는 다양한 유형의 물고기에 관심이 있을 수 있고 고도 및 다른 환경적 요인들을 별도로 모델링할 수도 있습니다.\n푸아송 변수들의 혼합물로 잘 모델링되는 샘플링 체계의 추가적인 예시로는 RNA-Seq과 같은 고처리량 시퀀싱 응용 분야가 있으며, 이는 8장에서 자세히 다룰 것입니다. 또한 미생물 생태학에서 사용되는 16S rRNA-Seq 데이터도 여기에 해당합니다.\n\n\n6.3.3 4.4.3 감마 분포: 두 개의 매개변수 (모양과 척도)\n이제 우리는 이전에 보지 못했던 새로운 분포를 알아가려 합니다. 감마 분포는 (단일 매개변수를 가진) 지수 분포의 확장이며, 두 개의 매개변수를 가지고 있어 더 유연합니다. 이는 종종 계층 모델의 상위 레벨을 위한 구성 요소로 유용합니다. 감마 분포는 양수 값을 가지며 연속적입니다. 지수 분포의 밀도는 0에서 최댓값을 갖고 값이 무한대로 갈수록 단순히 0을 향해 감소하는 반면, 감마 분포의 밀도는 어떤 유한한 값에서 최댓값을 가집니다. 시뮬레이션 예제를 통해 이를 탐구해 봅시다. 그림 4.20의 히스토그램들은 다음 코드 라인들에 의해 생성되었습니다:\nggplot(tibble(x = rgamma(10000, shape = 2, rate = 1/3)), aes(x = x)) + geom_histogram(bins = 100, fill= “purple”) ggplot(tibble(x = rgamma(10000, shape = 10, rate = 3/2)), aes(x = x)) + geom_histogram(bins = 100, fill= “purple”)\n\n\ngamma((2, 1/3))\n\n\n\ngamma((10, 3/2))\n\n그림 4.20: 감마 분포의 무작위 표본 히스토그램. 감마 분포는 유연한 2-매개변수 분포입니다: 위키백과 참조.\n\n6.3.3.1 감마-포아송 혼합물: 계층적 모델\n\n감마 분포로부터 매개변수 세트 (_1, _2, …)를 생성합니다.\n이들을 사용하여 각 (_i)에 대해 하나씩 포아송((_i)) 확률 변수 세트를 생성합니다.\n\nlambda = rgamma(10000, shape = 10, rate = 3/2)\ngp = rpois(length(lambda), lambda = lambda)\nggplot(tibble(x = gp), aes(x = x)) +\n  geom_histogram(bins = 100, fill= \"purple\")\n\n그림 4.21: 감마-포아송 계층 모델을 통해 생성된 gp의 히스토그램.\n결과값들은 감마-포아송 혼합물로부터 왔다고 말합니다. 그림 4.21은 gp의 히스토그램을 보여줍니다.\n질문 4.18\n\n그러한 감마-포아송 혼합물로부터 생성된 값들은 연속형인가요, 아니면 이산형인가요?\n이 분포의 또 다른 이름은 무엇인가요? 힌트: vcd 패키지의 goodfit 함수에 의해 제공되는 서로 다른 분포들을 시도해 보세요.\n\n해결책\nlibrary(\"vcd\")\nofit = goodfit(gp, \"nbinomial\")\nplot(ofit, xlab = \"\")\nofit$par\n\n\n$size\n[1] 9.911829\n\n$prob\n[1] 0.5963857\n\n그림 4.22: 적합도(Goodness of fit) 플롯. 루토그램 은 감마-포아송 분포(일명 음이항 분포)의 이론적 확률을 빨간색 점으로, 관측 빈도의 제곱근을 직사각형 막대의 높이로 보여줍니다. 막대들은 모두 가로축 가까이에서 끝나며, 이는 음이항 분포에 잘 맞음을 나타냅니다.\nR과 다른 여러 곳에서 감마-포아송 분포는 음이항 분포(negative binomial distribution) 라는 가명으로 통용되기도 합니다. 이 두 명칭은 동의어입니다; 두 번째 명칭은 식 4.15가 이항 분포의 확률과 어떤 형식적인 유사성을 가진다는 사실을 암시합니다. 첫 번째 명칭인 감마-포아송 분포는 그 생성 메커니즘을 더 잘 나타내며, 이것이 우리가 책의 나머지 부분에서 사용할 명칭입니다. 이는 이산 분포이며, 즉 (양의 실수축 전체를 커버하는 감마 분포와 대조적으로) 자연수 값만을 취한다는 뜻입니다. 그 확률 분포는 다음과 같습니다.\n[ (K=k)=(\n\\[\\begin{array}{c}k+a-1\\\\k\\end{array}\\]\n) , p^a , (1-p)^k, ]\n이는 두 매개변수 (a^+)와 (p)에 의존합니다. 동등하게, 두 매개변수는 평균 (=pa/(1-p))와 분산(dispersion) 이라 불리는 매개변수 (/a)로 표현될 수 있습니다. 분포의 분산(variance)은 이 매개변수들에 의존하며, (+^2)입니다.\n\n그림 4.23: 감마-포아송 분포를 생성하는 계층 모델의 시각화. 상단 패널은 평균 50(수직 검은색 선)과 분산 30인 감마 분포의 밀도를 보여줍니다. 하나의 특정 실험 반복에서 값 60이 실현되었다고 가정해 봅시다. 이것이 우리의 잠재 변수입니다. 관찰 가능한 결과는 해당 비율 매개변수를 가진 푸아송 분포에 따라 분포하며, 중간 패널에 표시되어 있습니다. 하나의 특정 실험에서 결과는 예를 들어 55일 수 있으며, 점선으로 표시된 초록색 선으로 나타나 있습니다. 전체적으로, 만약 우리가 이러한 두 개의 연속적인 무작위 과정을 여러 번 반복한다면, 결과는 하단 패널에 표시된 것과 같이 분포될 것입니다 – 이것이 감마-포아송 분포입니다.\n질문 4.19\n설명을 위한 시뮬레이션보다 분석적인 유도에 더 관심이 있다면, 감마-포아송 확률 분포의 수학적 유도를 직접 작성해 보세요.\n해결책\n최종 분포는 2단계 과정의 결과임을 상기하세요:\n\n밀도로부터 ((a,b)) 분포를 따르는 수 (x)를 생성합니다.\n\n[ f_(x, a, b)=,x{a-1},e{-b x}, ]\n여기서 ()는 소위 ()-함수이며, ((a)=_0x{a-1},e^{-x},x)입니다 (이러한 부수적인 관계가 있음에도 불구하고 감마 분포와 혼동해서는 안 됩니다).\n\n비율 (x)를 가진 푸아송 분포로부터 수 (k)를 생성합니다. 확률 분포는 다음과 같습니다.\n\n[ f_(k, =x)= ]\n만약 (x)가 오직 유한한 값들의 집합만을 취한다면, 우리는 각각의 가능한 경우에 대해 (f_)에 따른 그들의 확률로 가중치를 주어 합산함으로써 문제를 해결할 수 있을 것입니다. 하지만 (x)는 연속적이므로, 우리는 이산적인 합 대신 적분으로 써야 합니다. 우리는 (K)의 분포를 주변 분포(marginal)라고 부릅니다. 그 확률 질량 함수는 다음과 같습니다.\n[\n\\[\\begin{aligned} P(K=k)&=\\int_{x=0}^{\\infty} \\, f_\\text{Pois}(k, \\lambda=x)\\; f_\\Gamma(x, a, b) \\;dx\\\\ &= \\int_{x=0}^{\\infty} \\frac{x^{k}e^{-x}}{k!}\\,\\frac{b^a}{\\Gamma(a)} x^{a-1}e^{-bx}\\; dx \\end{aligned}\\]\n]\n항들을 정리하고 (x)와 무관한 항들을 적분 밖으로 꺼냅니다.\n[ P(K=k)= _{x=0}^{} x{k+a-1}e{-(b+1)x} dx ]\n우리는 감마 밀도의 합이 1이라는 것을 알기 때문에: (_0x{k+a-1}e^{-(b+1)x} dx = )\n[\n\\[\\begin{aligned} P(K=k) &= \\frac{\\Gamma(k+a)}{\\Gamma(a)\\,\\Gamma(k+1)}\\frac{b^a}{(b+1)^{a}(b+1)^k} ={k+a-1\\choose k}\\left(\\frac{b}{b+1}\\right)^a\\left(1-\\frac{b}{b+1}\\right)^k \\end{aligned}\\]\n]\n여기서 마지막 줄에는 ((v+1)=v!)임을 사용했습니다. 이는 크기 매개변수 (a)와 확률 (p=)을 가진 감마-포아송 분포인 식 4.15와 동일합니다.\n\n\n\n6.3.4 4.4.4 분산 안정화 변환 (Variance stabilizing transformations)\n우리가 실험 데이터를 분석할 때 제어해야 할 핵심 이슈 중 하나는 기저의 동일한 실제 값에 대한 반복 측정들 사이에, 즉 반복 실험들 사이에 가변성이 얼마나 되느냐 하는 것입니다. 이것이 우리가 어떤 실제적인 차이(예: 서로 다른 조건들 사이의 차이)를 얼마나 잘 볼 수 있는지를 결정할 것입니다. 이 장에서 공부한 계층 모델 유형을 통해 발생하는 데이터는 종종 매우 불균질한 분산을 가지는 것으로 나타나며, 이는 도전 과제가 될 수 있습니다. 우리는 그러한 경우에 분산 안정화 변환 (Anscombe 1948)이 어떻게 도움이 될 수 있는지 볼 것입니다. 비율이 10부터 100까지인 일련의 푸아송 변수들로부터 시작해 봅시다:\n데이터프레임(더 정확하게는 tibble) simdat를 어떻게 구성하는지 주목하세요: lapply 루프의 출력은 lam의 각 값에 대해 하나씩인 tibble 들의 리스트입니다. 파이프 연산자 |&gt;를 사용하여 이를 (dplyr 패키지의) bind_rows 함수로 보냅니다. 그 결과는 모든 리스트 원소들이 서로 깔끔하게 쌓인 데이터프레임이 됩니다.\nsimdat = lapply(seq(10, 100, by = 10), function(lam) tibble(n = rpois(200, lambda = lam), sqrt(n) = sqrt(n), lambda = lam)) |&gt; bind_rows() | tidyr::pivot_longer(cols = !lambda) ggplot(simdat, aes(x = factor(lambda), y = value)) + xlab(expression(lambda)) + geom_violin() + facet_grid(rows = vars(name), scales = “free”)\n\n그림 4.24: 여덟 가지 서로 다른 평균 lambda 선택에 대한 푸아송 분포 측정 데이터. 상단 패널에서 (y)축은 데이터에 비례하며, 하단 패널은 제곱근 스케일입니다. 첫 번째 사례에서는 분포 너비가 어떻게 변하는지 주목하세요. 반면 두 번째 사례에서는 그 정도가 덜합니다.\n그림 4.24의 상단 패널에서 우리가 보는 데이터는 이분산성(heteroscedasticity) 이라 불리는 사례입니다: 우리 데이터 공간의 서로 다른 영역에서 데이터의 표준 편차(또는 동등하게 분산)가 다릅니다. 특히 가로축을 따라 평균과 함께 증가합니다. 푸아송 분포의 경우, 우리는 표준 편차가 평균의 제곱근이라는 것을 실제로 알고 있습니다; 다른 유형의 데이터에 대해서는 다른 의존성이 있을 수 있습니다. 이는 우리가 이후 분석 기법(예: 회귀 분석 또는 통계적 검정)을 적용하고 싶을 때 문제가 될 수 있는데, 그러한 기법들은 분산이 동일하다는 가정에 기초하고 있기 때문입니다. 그림 4.24에서 각 람다 값에 대한 반복 실험 횟수는 꽤 큽니다. 실제로는 항상 그렇지는 않습니다. 게다가 데이터가 우리 시뮬레이션에서처럼 알려진 평균에 의해 명시적으로 계층화되어 있지도 않으므로, 비록 이분산성이 존재하더라도 이를 보기가 더 어려울 수 있습니다. 하지만 그림 4.24의 하단 패널에서 보듯이, 우리가 단순히 제곱근 변환을 적용한다면 변환된 변수들은 대략 동일한 분산을 가질 것입니다. 이는 각 관측치에 대한 기저의 평균을 모르더라도 작동하며, 제곱근 변환은 이 정보를 필요로 하지 않습니다. 우리는 다음과 같은 코드로 이를 좀 더 정량적으로 확인할 수 있는데, 이는 서로 다른 lambda 선택에 대해 샘플링된 값 n과 sqrt(n)의 표준 편차를 보여줍니다.\n제곱근 변환된 값들의 표준 편차는 일관되게 0.5 근처이므로, 우리는 단위 분산을 달성하기 위해 2*sqrt(n) 변환을 사용할 것입니다.\nsummarise(group_by(simdat, name, lambda), sd(value)) |&gt; tidyr::pivot_wider(values_from = `sd(value)`)\n\n\n# A tibble: 10 × 3\n   lambda     n `sqrt(n)`\n    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1     10  2.95     0.478\n 2     20  4.19     0.470\n 3     30  5.62     0.521\n 4     40  5.99     0.473\n 5     50  7.69     0.546\n 6     60  7.59     0.492\n 7     70  8.69     0.520\n 8     80  8.99     0.505\n 9     90  9.44     0.498\n10    100  9.84     0.495\n감마-포아송 분포를 사용한 또 다른 예제가 그림 4.25에 나와 있습니다. 우리는 감마-포아송 변수 u5를 생성하고 평균 주변의 95% 신뢰 구간을 플롯합니다.\n5 너무 조밀한 시퀀스를 만들지 않으면서도 평균값 mu에 대해 더 넓은 범위의 값을 포착하기 위해, 우리는 기하 급수 (_{i+1} = 2_i)를 사용합니다.\nmuvalues = 2^seq(0, 10, by = 1)\nsimgp = lapply(muvalues, function(mu) {\n  u = rnbinom(n = 1e4, mu = mu, size = 4)\n  tibble(mean = mean(u), sd = sd(u),\n         lower = quantile(u, 0.025),\n         upper = quantile(u, 0.975),\n         mu = mu)\n  } ) |&gt; bind_rows()\nhead(as.data.frame(simgp), 2)\n\n\n    mean       sd lower upper mu\n1 0.9965 1.106440     0     4  1\n2 2.0233 1.748503     0     6  2\n\n\nggplot(simgp, aes(x = mu, y = mean, ymin = lower, ymax = upper)) +\n  geom_point() + geom_errorbar()\n\n그림 4.25: 1부터 1024까지의 () 범위에 대한 감마-포아송 분포 측정 데이터.\n질문 4.20\n푸아송 분포 데이터에 대한 제곱근 함수와 유사하게, 이 데이터에 대해 분산을 안정화하는 변환을 어떻게 찾을 수 있을까요?\n해결책\n만약 우리가 mu[1]에 대응하는 (그리고 simgp$mean[1] 주변에 중심을 둔) 값들을 그들의 표준 편차 simgp$sd[1]로 나누고, mu[2]에 대응하는 값들을 그들의 표준 편차 simgp$sd[2]로 나누는 식이라면, 그 결과로 나오는 값들은 설계에 의해 1의 표준 편차(따라서 분산)를 가질 것입니다. 그리고 11개의 개별적인 변환을 정의하는 대신, 우리는 적절한 지점에서 적절한 기울기를 가진 하나의 조각별 선형(piecewise linear) 및 연속 함수를 정의함으로써 우리의 목표를 달성할 수 있습니다.\nsimgp = mutate(simgp, slopes = 1 / sd, trsf = cumsum(slopes * mean)) ggplot(simgp, aes(x = mean, y = trsf)) + geom_point() + geom_line() + xlab(““)\n\n그림 4.26: 그림 4.25의 데이터 분산을 안정화하는 조각별 선형 함수.\n우리는 그림 4.26에서 이 함수가 특히 하단 끝부분에서 제곱근 함수와 어느 정도 닮았음을 볼 수 있습니다. 상단 끝부분에서는 로그 함수와 더 비슷해 보입니다. 수학적으로 더 깊이 있는 분들은 이러한 수치적 계산의 우아한 확장이 소위 델타 방법(delta method) 이라 불리는 약간의 미적분학을 통해 다음과 같이 수행될 수 있음을 알 수 있을 것입니다.\n우리의 변환 함수를 (g)라고 부르고, 그것이 미분 가능하다고 가정합시다 (이는 매우 합리적인 가정입니다: 우리가 여기서 고려할 만한 거의 모든 함수는 미분 가능하니까요). 또한 우리 확률 변수를 (X_i)라고 부르고, 평균을 (_i), 분산을 (v_i)라고 합시다. 그리고 (v_i)와 (_i)가 함수 관계 (v_i = v(_i))에 의해 연결되어 있다고 가정합니다. 그러면 그 평균 (_i) 근처의 (X_i) 값들에 대해,\n[ g(X_i) = g(_i) + g’(_i) (X_i-_i) + … ]\n여기서 점들은 우리가 무시할 수 있는 고차 항들을 나타냅니다. 변환된 값들의 분산은 대략 다음과 같습니다.\n[ \\[\\begin{align} \\text{Var}(g(X_i)) &\\simeq g'(\\mu_i)^2 \\\\ \\text{Var}(X_i) &= g'(\\mu_i)^2 \\, v(\\mu_i), \\end{align}\\] ]\n여기서 우리는 (c)가 상수일 때마다 성립하는 규칙들인 ((X-c)=(X))와 ((cX)=c^2,(X))를 사용했습니다. 이것이 상수가 되어야 한다는 요구 조건은 다음과 같은 미분 방정식으로 이어집니다.\n[ g’(x) = . ]\n주어진 평균-분산 관계 (v())에 대해, 우리는 함수 (g)에 대해 이를 풀 수 있습니다. 몇 가지 간단한 사례들을 확인해 봅시다:\n\n만약 (v()=) (푸아송)라면, 우리는 제곱근 변환인 (g(x)=)를 얻게 됩니다.\n만약 (v()=,^2)라면, 미분 방정식 4.19를 푸는 것은 (g(x)=(x))를 줍니다. 이것이 왜 많은 데이터 분석 응용 분야에서 로그 변환이 그렇게 인기 있는지 설명해 줍니다: 데이터가 일정한 변동 계수(coefficient of variation)를 가질 때, 즉 표준 편차가 평균에 비례할 때마다 로그 변환은 분산 안정화 변환으로서 작용합니다.\n\n질문 4.21\n(v() = + ,^2)와 연관된 분산 안정화 변환은 무엇인가요?\n해결책\n이 함수 (v())을 사용하여 미분 방정식 4.19를 풀려면, 다음 적분을 계산해야 합니다.\n[ . ]\n닫힌 형태의 표현식은 (Bronštein and Semendjajew 1979)와 같은 참조 표에서 찾아볼 수 있습니다. 이 저자들은 다음과 같은 일반적인 해를 제공합니다.\n[ = (2+2ax+b) + , ]\n여기에 우리 특수 사례인 (a=, b=1, c=0)을 대입하면 분산 안정화 변환을 얻게 됩니다.\n[ \\[\\begin{align} g_\\alpha(x) &= \\frac{1}{2\\sqrt{\\alpha}} \\ln\\left(2\\sqrt{\\alpha x (\\alpha x+1)} + 2\\alpha x + 1\\right) \\\\ &= \\frac{1}{2\\sqrt{\\alpha}} {\\displaystyle \\operatorname {arcosh}} (2\\alpha x+1).\\ \\end{align}\\] ]\n식 4.22의 두 번째 줄에는 항등식 ({}(z) = (z+))을 사용했습니다. ()인 극한에서는 선형 근사 ((1+)=+O(^2))를 사용하여 (g_0(x)=)임을 알 수 있습니다. 만약 (g_)가 분산 안정화 변환이라면 임의의 수 쌍 (u)와 (v)에 대해 (ug_+v) 또한 마찬가지이며, 우리는 나중에 분명해질 이유로 추가적인 인자 ()을 넣기 위해 이 자유를 사용했음에 유의하세요. 여러분은 식 4.22의 함수 (g_)가 그 도함수를 계산함으로써 조건 4.19를 충족한다는 것을 확인할 수 있는데, 이는 초등적인 계산입니다. 이를 플롯해 봅시다:\nf = function(x, a) ifelse (a==0, sqrt(x), log(2sqrt(a)  sqrt(x(ax+1)) + 2ax+1) / (2sqrt(a))) x = seq(0, 24, by = 0.1) df = lapply(c(0, 0.052^(0:5)), function(a) tibble(x = x, a = a, y = f(x, a))) %&gt;% bind_rows() ggplot(df, aes(x = x, y = y, col = factor(a))) + geom_line() + labs(col = expression(alpha))\n\n그림 4.27: 서로 다른 () 선택에 대한 식 4.22 함수의 그래프.\n그리고 식 4.22의 두 항의 동등성을 경험적으로 확인합니다:\nf2 = function(x, a) ifelse (a==0, sqrt(x), acosh(2ax + 1) / (2*sqrt(a)))\nwith(df, max(abs(f2(x,a) - y)))\n[1] 8.881784e-16\n그림 4.27에서 보듯이, 작은 (x) 값들에 대해 (g_(x)) (()에 관계없이)인 반면, 큰 값 ((x))과 (&gt;0)에 대해서는 로그 함수처럼 행동합니다:\n[ \\[\\begin{align} &\\frac{1}{2\\sqrt{\\alpha}}\\ln\\left(2\\sqrt{\\alpha(\\alpha x^2+x)}+2\\alpha x+1\\right)\\\\ \\approx&\\frac{1}{2\\sqrt{\\alpha}} \\ln\\left(2\\sqrt{\\alpha^2x^2}+2\\alpha x\\right)\\\\ = &\\frac{1}{2\\sqrt{\\alpha}}\\ln\\left(4\\alpha x\\right)\\\\ = &\\frac{1}{2\\sqrt{\\alpha}}\\ln x+\\text{const.} \\end{align}\\] ]\n우리는 예를 들어 다음과 같이 이를 경험적으로 확인할 수 있습니다.\n  a = c(0.2, 0.5, 1)\n  f(1e6, a)\n\n\n[1] 15.196731 10.259171  7.600903\n\n\n  1/(2*sqrt(a)) * (log(1e6) + log(4*a))\n\n\n[1] 15.196728 10.259170  7.600902",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>유한 혼합 모델 (Finite Mixture Models)</span>"
    ]
  },
  {
    "objectID": "04-chap.html#이-장의-요약",
    "href": "04-chap.html#이-장의-요약",
    "title": "6  유한 혼합 모델 (Finite Mixture Models)",
    "section": "6.4 4.5 이 장의 요약",
    "text": "6.4 4.5 이 장의 요약\n우리는 생물학적 데이터를 모델링하기 위해 혼합물을 사용하는 동기 부여 예제와 방법들을 제시했습니다. 우리는 EM 알고리즘이 부분적이고 더 단순한 문제들 사이를 반복함으로써 추정하기 어려운 확률 모델을 데이터에 적합시키는 흥미로운 예시임을 보았습니다.\n\n6.4.0.1 유한 혼합 모델 (Finite mixture models)\n우리는 서로 다른 평균과 분산을 가진 두 개 이상의 정규 분포 혼합물을 모델링하는 방법을 보았습니다. 우리는 EM 알고리즘을 사용하여 잠재 변수를 모르더라도 그러한 혼합물로부터 얻은 주어진 데이터 표본을 분해하는 방법을 보았습니다. EM 접근법은 우리가 분포의 모수적 형태와 성분의 수를 알고 있을 것을 요구합니다. 5장에서 우리는 그러한 정보에 의존하지 않고도 데이터에서 그룹을 찾는 방법을 볼 것인데, 이를 군집화(clustering)라고 부릅니다. 우리는 군집화와 혼합 모델링 사이에 강력한 개념적 관계가 있음을 염두에 둘 수 있습니다.\n\n\n6.4.0.2 흔한 무한 혼합 모델 (Common infinite mixture models)\n무한 혼합 모델은 이항, 정규, 푸아송과 같은 더 기본적인 분포들로부터 새로운 분포들(감마-포아송이나 라플라스와 같은)을 구축하는 데 좋습니다. 흔한 예시들은 다음과 같습니다.\n\n정규 혼합물 (종종 평균과 분산에 대한 계층 모델을 가짐);\n베타-이항 혼합물 – 이항 분포에서의 확률 (p)가 ((a, b)) 분포에 따라 생성되는 경우;\n리드 카운트를 위한 감마-포아송 ( 8장 참조);\nPCR을 위한 감마-지수.\n\n\n\n6.4.0.3 응용\n실험적 가변성의 레이어가 여러 개 있을 때마다 혼합 모델은 유용합니다. 예를 들어, 가장 낮은 레이어에서 우리의 측정 정밀도는 기본적인 물리적 감지 한계에 의해 제한될 수 있으며, 이는 카운팅 기반 어세이의 경우 푸아송 분포로, 연속형 측정의 경우 정규 분포로 모델링될 수 있습니다. 그 위에는 기계 간 변동, 시약 변동, 작업자 변동 등의 레이어가 하나(또는 그 이상) 있을 수 있습니다.\n혼합 모델은 데이터에 이질적인 양의 가변성(분산)이 있는 경우가 많다는 사실을 반영합니다. 그러한 경우, 후속 시각화나 분석 전에 적절한 데이터 변환, 즉 분산 안정화 변환이 필요합니다. 우리는 8장에서 RNA-Seq에 대한 예제를 심도 있게 공부할 것이며, 이는 미생물 생태학에서의 차세대 리드 정규화에서도 유용함이 입증되었습니다 (McMurdie and Holmes 2014).\n혼합 모델링의 또 다른 중요한 응용은 다중 검정에서의 2-성분 모델이며 – 우리는 이에 대해 6장에서 다시 다룰 것입니다.\n\n\n6.4.0.4 ECDF와 붓스트랩\n우리는 관측된 표본을 혼합물로 사용함으로써 추정치의 표집 분포에 대해 알려주는 많은 시뮬레이션된 표본들을 생성할 수 있음을 보았습니다. 이 방법을 붓스트랩이라고 부르며, 닫힌 형태의 표현식이 가용하지 않을 때에도 추정치를 평가할 수 있는 방법을 제공하므로 우리는 여러 번 이 방법으로 돌아올 것입니다 (우리는 이를 비모수적이라고 부릅니다).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>유한 혼합 모델 (Finite Mixture Models)</span>"
    ]
  },
  {
    "objectID": "04-chap.html#더-읽을거리",
    "href": "04-chap.html#더-읽을거리",
    "title": "6  유한 혼합 모델 (Finite Mixture Models)",
    "section": "6.5 4.6 더 읽을거리",
    "text": "6.5 4.6 더 읽을거리\n유한 혼합 모델에 대한 완전한 단행본 수준의 연구는 McLachlan과 Peel (2004)에 의해 이루어졌습니다; EM 알고리즘에 대해서는 McLachlan과 Krishnan (2007)의 책 또한 참조하십시오. Majorize-Minimization (MM) 프레임워크 내에서 모든 EM 유형 알고리즘을 제시하는 최근의 책은 Lange (2016)에 의한 것입니다.\n사실 많은 자연 현상이 혼합물로 보일 수 있는 수학적 이유가 있습니다: 이는 관찰된 사건들이 교환 가능할(발생 순서가 중요하지 않을) 때 발생합니다. 이 이면의 이론은 상당히 수학적이며, 시작하기 좋은 방법은 위키백과 항목과 Diaconis 및 Freedman (1980)의 논문을 살펴보는 것입니다.\n특히, 우리는 고처리량 데이터를 위해 혼합물을 사용합니다. 여러분은 8장과 11장에서 그 예시들을 보게 될 것입니다.\n붓스트랩은 많은 상황에서 사용될 수 있으며 알아두면 매우 유용한 도구입니다. 친절한 설명은 (B. Efron and Tibshirani 1993)에서 제공됩니다.\n역사적으로 흥미로운 논문은 Anscombe (1948)에 의한 분산 안정화에 관한 독창적인 기사로, 그는 푸아송 및 감마-포아송 확률 변수에 대해 분산 안정화 변환을 만드는 방법들을 제안했습니다. 분산 안정화는 Rice (2006, chap. 6)와 Kéry 및 Royle (2015, 35)와 같은 이론 통계학의 많은 표준 교재들에서 델타 방법을 사용하여 설명됩니다.\nKéry와 Royle (2015)는 니치(niche) 및 공간 생태학에서의 풍부도 추정을 위한 계층 모델을 구축하기 위해 R을 사용하는 훌륭한 탐색을 제공합니다.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>유한 혼합 모델 (Finite Mixture Models)</span>"
    ]
  },
  {
    "objectID": "04-chap.html#연습-문제",
    "href": "04-chap.html#연습-문제",
    "title": "6  유한 혼합 모델 (Finite Mixture Models)",
    "section": "6.6 4.7 연습 문제",
    "text": "6.6 4.7 연습 문제\n연습 문제 4.1\nEM 알고리즘 단계별 수행. 예제 데이터 세트로 Myst.rds 파일에 있는 값들을 사용합니다. 언제나 그렇듯이, 먼저 데이터를 시각화하는 것이 좋습니다. 히스토그램은 그림 4.28에 나와 있습니다. 우리는 이 데이터를 알려지지 않은 평균과 표준 편차, 그리고 알려지지 않은 혼합 비율을 가진 두 정규 분포의 혼합물로 모델링할 것입니다. 두 성분을 A와 B라고 부르겠습니다.\nmx = readRDS(“../data/Myst.rds”)$yvar str(mx)\n num [1:1800] 0.3038 0.0596 -0.0204 0.1849 0.2842 ...\nggplot(tibble(mx), aes(x = mx)) + geom_histogram(binwidth = 0.025)\n우리는 먼저 mx에 있는 각 값들에 대해 각 성분에 대한 멤버십 가중치를 무작위로 할당하는 것으로 시작합니다.\nwA = runif(length(mx)) wB = 1 - wA\n또한 관리용 변수들을 설정해야 합니다: iter는 EM 알고리즘의 반복 횟수를 셉니다; loglik은 현재의 로그 우도를 저장합니다; delta는 이전 반복에서 현재 반복까지의 로그 우도 변화를 저장합니다. 우리는 또한 알고리즘의 매개변수인 tolerance, miniter, maxiter를 정의합니다.\niter      = 0\nloglik    = -Inf\ndelta     = +Inf\ntolerance = 1e-12\nminiter   = 50\nmaxiter   = 1000\n아래 코드를 공부하고 다음 질문들에 답하세요:\n\n어느 줄이 E 단계에 해당하고, 어느 줄이 M 단계에 해당하나요?\ntolerance, miniter, maxiter의 역할은 무엇인가요?\n여기서 우리가 수행하는 작업의 결과를 mixtools 패키지의 normalmixEM 함수 출력과 비교해 보세요.\n\nwhile((delta &gt; tolerance) && (iter &lt;= maxiter) || (iter &lt; miniter)) {\n  lambda = mean(wA)\n  muA = weighted.mean(mx, wA)\n  muB = weighted.mean(mx, wB)\n  sdA = sqrt(weighted.mean((mx - muA)^2, wA))\n  sdB = sqrt(weighted.mean((mx - muB)^2, wB))\n\n  pA   =    lambda    * dnorm(mx, mean = muA, sd = sdA)\n  pB   = (1 - lambda) * dnorm(mx, mean = muB, sd = sdB)\n  ptot = pA + pB\n  wA   = pA / ptot\n  wB   = pB / ptot\n\n  loglikOld = loglik\n  loglik = sum(log(pA + pB))\n  delta = abs(loglikOld - loglik)\n  iter = iter + 1\n}\niter\n\n\n[1] 447\nc(lambda, muA, muB, sdA, sdB)\n[1]  0.4756 -0.1694  0.1473  0.0983  0.1498\n\n그림 4.28: EM 알고리즘 예제 데이터인 mx의 히스토그램.\n해결책\nwhile 루프의 처음 다섯 줄은 최대화(Maximization) 단계 를 구현합니다. wA와 wB의 현재 값이 주어졌을 때, 우리는 최대 우도 추정량들을 사용하여 혼합 모델의 매개변수들을 추정합니다: 혼합 비율 lambda는 wA의 평균에 의해, 두 정규 분포 성분의 매개변수들(muA, sdA) 및 (muB, sdB)은 표본 평균과 표본 표준 편차에 의해 추정됩니다. 멤버십 가중치를 고려하기 위해, 우리는 가중 평균(weighted.mean 함수)과 가중 표준 편차를 사용합니다.\n그다음은 기댓값(Expectation) 단계 가 옵니다. 데이터 벡터 mx에 있는 각 요소들에 대해, 우리는 정규 밀도 함수 dnorm을 사용하여 생성 분포 모델 A와 B에 대한 확률 밀도 pA와 pB를 계산하며, 이들은 각각 혼합 비율 lambda와 (1-lambda)에 의해 가중치가 부여됩니다. 이로부터 우리는 식 4.5에 따라 업데이트된 멤버십 가중치 wA와 wB를 계산합니다.\n멤버십 가중치와 매개변수들이 주어졌을 때, 로그 우도 loglik은 쉽게 계산되며, while 루프는 이 단계들을 반복합니다.\n루프의 종료 기준은 우도의 변화인 delta에 기초합니다. 만약 이것이 tolerance보다 작아지면 루프는 끝날 수 있습니다. 이는 알고리즘이 수렴했는지 확인하는 간단한 방식입니다. iter에 대한 추가 조건들은 적어도 miniter번의 반복이 실행되도록 보장하며, 루프가 항상 maxiter번의 반복 후에 멈추도록 보장합니다. 후자는 알고리즘이 어떠한 경우에도 유한한 시간 내에 종료되도록 하기 위함입니다. (이러한 반복 알고리즘의 “전문적인” 구현체들은 대개 멈출 최적의 시점을 결정하기 위해 좀 더 세심하게 작동합니다.)\n마지막으로, 우리 추정치를 mixtools 패키지의 normalmixEM 함수로부터 얻은 추정치와 비교해 봅시다.\ngm = mixtools::normalmixEM(mx, k = 2)\nnumber of iterations= 215 \n\n\nwith(gm, c(lambda[1], mu, sigma))\n\n\n[1]  0.4757 -0.1694  0.1473  0.0983  0.1498\n연습 문제 4.2\n우리는 왜 종종 우도 그 자체보다 우도의 로그를 고려할까요? 예를 들어 위에서의 EM 코드에서, 왜 우리는 로그 스케일에서의 확률을 가지고 작업했을까요?\n해결책\n(데이터 포인트들이 독립적으로 샘플링될 때마다) 우도는 종종 곱(product)의 형태를 취합니다. 예를 들어 식 4.4의 경우가 그러합니다. 우도 최적화를 위해 미분을 계산할 때, 곱의 미분법(product rule) 적용이 필요할 것입니다. 로그 스케일에서는 곱이 합으로 변하며, 합의 미분은 단지 개별 항들의 미분의 합일 뿐입니다.\n추가적인 이유는 컴퓨터가 산술 연산을 구현하는 방식으로부터 옵니다. 컴퓨터는 대개 유한한 비트 수를 가진 숫자의 부동 소수점 표현을 사용합니다. 예를 들어 IEEE 754-2008 표준은 배정밀도(double-precision) 수에 대해 64비트를 사용합니다: 부호에 1비트, 가수(mantissa 또는 significand)에 52비트, 지수에 11비트입니다. 이러한 숫자들 사이의 곱셈은 지수의 덧셈을 수반하지만, 지수의 범위는 오직 (0)부터 (2^{11}-1=2047)까지입니다. 단지 수백 개의 데이터 포인트가 관여된 우도조차 산술 오버플로(overflow)나 정밀도 관련 문제들을 일으킬 수 있습니다. 곱이 합이 되는 로그 스케일에서는 연산 부하가 가수와 지수 사이에 더 잘 분배되는 경향이 있으며, 수백만 개의 데이터 포인트를 가진 로그 우도조차 합리적인 정밀도로 다뤄질 수 있습니다.\n로그 확률 벡터 정규화를 위한 Log-Sum-Exp 트릭에 관한 Gregory Gundersen의 포스팅 또한 참조하세요.\n연습 문제 4.3\n4.4.3절의 추정치들인 ofit$par에 의해 주어진 매개변수들을 가진 감마-포아송 분포의 이론적 값들과 추정에 사용된 데이터를 QQ-플롯을 사용하여 비교하세요.\n연습 문제 4.4\n회귀 분석을 위한 혼합 모델링 예제. flexmix 패키지 (Grün, Scharl, and Leisch 2012)는 우리가 데이터에 대해 군집화와 회귀 적합을 동시에 수행할 수 있게 해줍니다. flexmix 의 표준 M 단계인 FLXMRglm은 R의 일반화 선형 모델링 시설(glm 함수)에 대한 인터페이스입니다. 패키지와 예제 데이터 세트를 로드하세요.\nlibrary(\"flexmix\")\ndata(\"NPreg\")\n\n먼저 데이터를 플롯하고 점들이 어떻게 생성되었는지 추측해 보세요.\n다음 명령어들을 사용하여 2-성분 혼합 모델을 적합시키세요.\n\nm1 = flexmix(yn ~ x + I(x^2), data = NPreg, k = 2)\n\n혼합 성분들의 추정된 매개변수들을 살펴보고 실제 클래스 대 클러스터 멤버십을 교차 분류하는 정오표(truth table)를 만드세요. 객체 m1의 요약(summary)은 우리에게 무엇을 보여주나요?\n데이터를 다시 플롯하되, 이번에는 각 점에 추정된 클래스에 따른 색상을 입히세요.\n\n해결책\nggplot(NPreg, aes(x = x, y = yn)) + geom_point()\n\n그림 4.29: 점들은 두 가지 서로 다른 생성 과정으로부터 온 것처럼 보입니다. 하나는 선형이고 다른 하나는 이차(quadratic)입니다.\n성분들은 다음과 같습니다:\nmodeltools::parameters(m1, component = 1)\n\n\n                      Comp.1\ncoef.(Intercept) -0.20998685\ncoef.x            4.81807854\ncoef.I(x^2)       0.03613061\nsigma             3.47665584\n\n\nmodeltools::parameters(m1, component = 2)\n\n\n                     Comp.2\ncoef.(Intercept) 14.7167886\ncoef.x            9.8468507\ncoef.I(x^2)      -0.9683734\nsigma             3.4795657\n두 성분의 매개변수 추정치들 모두 실제 값들에 가깝습니다. 실제 클래스와 클러스터 멤버십의 교차 집계표는 다음과 같이 얻을 수 있습니다.\ntable(NPreg$class, modeltools::clusters(m1))\n     1  2\n  1 95  5\n  2  5 95\n우리 예제 데이터의 경우, 두 성분의 비율은 약 0.7이며, 이는 직선과 포물선이 교차하는 지점에서 클래스들이 중첩됨을 나타냅니다.\nsummary(m1)\n요약 정보는 추정된 사전 확률 (k), 두 클러스터에 할당된 관측치 수, (p{nk}&gt;) (기본값 (^{-4}))인 관측치 수, 그리고 후자 두 수치 사이의 비율을 보여줍니다. 잘 분리된 성분들의 경우, 0이 아닌 사후 확률 (p_{nk})를 가진 관측치들의 큰 비율이 해당 클러스터에 할당되어야 하며, 이는 1에 가까운 비율을 주게 됩니다.\nNPreg = mutate(NPreg, gr = factor(class))\nggplot(NPreg, aes(x = x, y = yn, group = gr)) +\n   geom_point(aes(colour = gr, shape = gr)) +\n   scale_colour_hue(l = 40, c = 180)\n\n그림 4.30: 각 점에 추정된 클래스에 따라 색상을 입혀 flexmix를 사용한 회귀 분석 예제. 교차하는 지점에서 우리는 ‘식별 가능성(identifiability)’ 문제를 가짐을 알 수 있습니다: 직선에 속하는 점들과 포물선에 속하는 점들을 구별할 수 없습니다.\n연습 문제 4.5\n다른 계층적 노이즈 모델들:\n분자 생물학 기술적 변동 모델링을 위해 다른 무한 혼합물들의 사용을 탐구한 두 논문을 찾으세요.\n해결책\nChen, Xie, Story (2011)의 논문은 비드 어레이(bead arrays)에서의 배경 노이즈 모델링을 위해 지수-푸아송 모델을 탐구합니다. Wills 등 (2013)은 여러 푸아송 혼합 모델들을 비교합니다.\nAnscombe, Francis J. 1948. “The Transformation of Poisson, Binomial and Negative-Binomial Data.” Biometrika , 246–54.\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\nBronštein, Il’ja N., and Konstantin A Semendjajew. 1979. Taschenbuch Der Mathematik. B.G. Teubner Verlagsgesellschaft, Leipzig; Verlag Nauka, Moscow.\nBulmer, Michael George. 2003. Francis Galton: Pioneer of Heredity and Biometry. JHU Press.\nChen, Min, Yang Xie, and Michael Story. 2011. “An Exponential-Gamma Convolution Model for Background Correction of Illumina BeadArray Data.” Communications in Statistics-Theory and Methods 40 (17): 3055–69.\nDiaconis, Persi, and David Freedman. 1980. “Finite Exchangeable Sequences.” The Annals of Probability , 745–64.\nDiaconis, Persi, and Susan Holmes. 1994. “Gray Codes for Randomization Procedures.” Statistics and Computing 4 (4): 287–302.\nEfron, Bradley, and Robert J Tibshirani. 1994. An Introduction to the Bootstrap. CRC press.\nEfron, B., and R. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman & Hall/CRC.\nGrün, Bettina, Theresa Scharl, and Friedrich Leisch. 2012. “Modelling Time Course Gene Expression Data with Finite Mixtures of Linear Additive Models.” Bioinformatics 28 (2): 222–28. https://doi.org/10.1093/bioinformatics/btr653.\nHoeting, Jennifer A, David Madigan, Adrian E Raftery, and Chris T Volinsky. 1999. “Bayesian Model Averaging: A Tutorial.” Statistical Science , 382–401.\nKéry, Marc, and J Andrew Royle. 2015. Applied Hierarchical Modeling in Ecology: Analysis of Distribution, Abundance and Species Richness in r and BUGS: Volume 1: Prelude and Static Models. Academic Press.\nKristiansson, Erik, Michael Thorsen, Markus J Tamás, and Olle Nerman. 2009. “Evolutionary Forces Act on Promoter Length: Identification of Enriched Cis- Regulatory Elements.” Molecular Biology and Evolution 26 (6): 1299–1307.\nKuan, Pei Fen, Dongjun Chung, Guangjin Pan, James A Thomson, Ron Stewart, and Sündüz Keleş. 2011. “A Statistical Framework for the Analysis of ChIP-Seq Data.” Journal of the American Statistical Association 106 (495): 891–903.\nLange, Kenneth. 2016. MM Optimization Algorithms. SIAM.\nMcLachlan, Geoffrey, and Thriyambakam Krishnan. 2007. The EM Algorithm and Extensions. Vol. 382. John Wiley & Sons.\nMcLachlan, Geoffrey, and David Peel. 2004. Finite Mixture Models. John Wiley & Sons.\nMcMurdie, Paul J, and Susan Holmes. 2014. “Waste Not, Want Not: Why Rarefying Microbiome Data Is Inadmissible.” PLoS Computational Biology 10 (4): e1003531.\nPurdom, Elizabeth, and Susan P Holmes. 2005. “Error Distribution for Gene Expression Data.” Statistical Applications in Genetics and Molecular Biology 4 (1).\nRice, John. 2006. Mathematical Statistics and Data Analysis. Cengage Learning.\nShalizi, Cosma. 2017. Advanced Data Analysis from an Elementary Point of View. Cambridge University Press. https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf.\nSlonim, Noam, Gurinder Singh Atwal, Gašper Tkačik, and William Bialek. 2005. “Information-Based Clustering.” PNAS 102 (51): 18297–302.\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society. Series B (Methodological) , 267–88.\nWills, Quin F, Kenneth J Livak, Alex J Tipping, Tariq Enver, Andrew J Goldson, Darren W Sexton, and Chris Holmes. 2013. “Single-Cell Gene Expression Analysis Reveals Genetic Associations Masked in Whole-Tissue Experiments.” Nature Biotechnology 31 (8): 748–52.\nZeileis, Achim, Christian Kleiber, and Simon Jackman. 2008. “Regression Models for Count Data in R.” Journal of Statistical Software 27 (8). http://www.jstatsoft.org/v27/i08/.\n페이지는 R 버전 4.5.1 (2025-06-13)을 사용하여 2025-09-01 01:33에 빌드되었습니다.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>유한 혼합 모델 (Finite Mixture Models)</span>"
    ]
  },
  {
    "objectID": "05-chap.html",
    "href": "05-chap.html",
    "title": "7  군집화 (Clustering)",
    "section": "",
    "text": "7.1 5.2 데이터란 무엇이며 왜 군집화하는가?\n세포, 질병, 유기체의 범주를 찾고 그 이름을 짓는 것은 자연 과학의 핵심 활동입니다. 4장에서 우리는 일부 데이터가 명확한 모수적 생성 모델을 가진 서로 다른 그룹이나 모집단으로부터의 혼합물로 모델링될 수 있음을 보았습니다. 우리는 그러한 예제들에서 성분들을 분리해 내기 위해 EM 알고리즘을 어떻게 사용할 수 있는지 보았습니다. 이제 우리는 클러스터(clusters) 가 반드시 예쁜 타원형1 모양을 가질 필요는 없는 경우로 그룹들을 풀어내는 아이디어를 확장해 보려 합니다.\n1 다변량 정규 분포를 이용한 혼합 모델링은 타원형 클러스터 경계를 함축합니다.\n군집화(Clustering)는 데이터(연속형 또는 준연속형)를 가져와서 새로운 범주형 그룹(group) 변수를 추가하며, 이는 때때로 중간(intermediate) 상태를 무시하는 대가를 치르더라도 의사 결정을 단순화할 수 있습니다. 예를 들어, 공복 혈당, 당화혈색소 및 섭취 2시간 후 혈장 포도당 수치와 연관된 복잡한 고차원 진단 측정값들을 단순히 환자를 당뇨병 “그룹”에 할당함으로써 의료 결정을 단순화합니다.\n이 장에서 우리는 저차원 및 고차원 비모수적(nonparametric) 환경 모두에서 의미 있는 클러스터나 그룹을 찾는 방법을 공부할 것입니다. 하지만 주의할 점이 있습니다: 군집화 알고리즘은 클러스터를 찾도록 설계되었으므로, 클러스터가 없는 곳에서도 클러스터를 찾아낼 것입니다2. 따라서 클러스터의 존재를 뒷받침하는 사전 도메인 지식이 없는 경우, 클러스터 검증(validation) 은 우리 프로세스의 필수적인 구성 요소입니다.\n2 이는 인간을 연상시킵니다: 우리는 무작위성 속에서도 패턴을 보기를 좋아합니다.\n이 장에서 우리는 다음을 수행할 것입니다:\n그림 5.1: 존 스노우의 콜레라 사례 지도: 각 집의 작은 막대 그래프는 진단된 사례들의 군집을 나타냅니다.\nDavid Freedman은 이 발견으로 이어진 모든 단계에 대해 훌륭하고 상세한 설명을 남겼습니다 (Freedman 1991).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>군집화 (Clustering)</span>"
    ]
  },
  {
    "objectID": "05-chap.html#데이터란-무엇이며-왜-군집화하는가",
    "href": "05-chap.html#데이터란-무엇이며-왜-군집화하는가",
    "title": "7  군집화 (Clustering)",
    "section": "",
    "text": "7.1.1 5.2.1 군집화는 때때로 발견으로 이어질 수 있습니다.\n존 스노우(John Snow)는 콜레라 사례 지도를 만들고 사례들의 클러스터 를 식별했습니다. 그는 그 후 펌프들의 위치에 대한 추가 정보를 수집했습니다. 조밀한 사례 클러스터들이 브로드 스트리트(Broadstreet) 펌프와 가깝다는 사실은 물이 범인일 가능성을 지목했습니다. 그는 콜레라 발생의 원인을 추론할 수 있게 해주는 별도의 정보원들을 수집했습니다.\n이제 그림 5.2에 표시된 또 다른 런던 지도를 살펴봅시다. 빨간색 점들은 제2차 세계대전 중 폭격을 받은 위치들을 나타냅니다. 전쟁 중에 분석 팀들에 의해 많은 이론이 내놓아졌습니다. 그들은 폭격 패턴(유틸리티 공장, 병기창과의 근접성, (…))에 대한 합리적인 설명을 찾으려 노력했습니다. 사실, 전쟁 후에 폭격은 특정 목표물을 타격하려는 시도 없이 무작위로 분포되었다는 것이 밝혀졌습니다.\n\n그림 5.2: 1940년 9월 7일 런던에 떨어진 폭탄의 위치 지도. 영국 국립 보존 기록관 웹사이트 http://bombsight.org에서 가져온 것입니다.\n군집화는 복잡한 다변량 데이터를 이해하는 데 유용한 기법이며, 이는 비지도(unsupervised) 3 학습입니다. 탐색적 기법은 데이터를 해석하는 데 중요할 수 있는 그룹화된 모습을 보여줍니다.\n3 모든 변수가 동일한 상태를 가지며, 설명 변수의 정보를 바탕으로 한 변수(감독 반응)의 값을 예측하거나 학습하려고 하지 않기 때문에 이렇게 불립니다.\n예를 들어, 군집화는 연구자들이 암 생물학에 대한 이해를 높일 수 있게 해주었습니다. 해부학적 위치와 조직 병리학적 소견으로는 동일해 보였던 종양들이 유전자 발현 데이터와 같은 분자적 시그니처에 따라 여러 클러스터로 나뉘었습니다 (Hallett et al. 2012). 결국 그러한 군집화는 새롭고 더 적절한 질병 유형의 정의로 이어질 수 있습니다. 적절성은 예를 들어 서로 다른 환자 결과(예후)와 연관되어 있다는 사실로 입증됩니다. 이 장에서 우리가 하고자 하는 것은 그림 5.3과 같은 그림들이 어떻게 구성되는지, 그리고 어떻게 해석해야 하는지를 이해하는 것입니다.\n\n그림 5.3: 유방암 샘플들(The Cancer Genome Atlas (TCGA) 및 Molecular Taxonomy of Breast Cancer International Consortium (METABRIC)에서 가져옴)은 그들의 miRNA 발현을 사용하여 그룹으로 나뉠 수 있습니다 (Aure et al. 2017). 저자들은 하단 플롯에서 서로 다른 그룹들의 생존 기간이 다르다는 것을 보여줍니다. 따라서 이러한 클러스터들은 생물학적 및 임상적으로 의미가 있었습니다. 이러한 분석의 약속은 그룹들을 사용하여 더 구체적이고 최적화된 치료를 제공할 수 있다는 것입니다.\n4장에서 우리는 이미 그룹을 찾아내기 위한 한 가지 기법인 EM 알고리즘을 공부했습니다. 이 장에서 우리가 탐구하는 기법들은 더 일반적이며 더 복잡한 데이터에 적용될 수 있습니다. 이들 중 상당수는 관측치 쌍 사이의 거리에 기초하며(이는 전체 대 전체일 수도 있고, 때로는 전체 대 일부일 수도 있음), 정규 분포, 감마-포아송 등과 같은 특정 분포군을 포함하는 데이터의 생성 메커니즘에 대해 명시적인 가정을 하지 않습니다. 문헌과 과학 소프트웨어 분야에는 군집화 알고리즘이 넘쳐나며, 이는 위협적으로 느껴질 수 있습니다. 사실 이는 데이터 유형의 다양성과 각 분야에서 추구하는 목표의 다양성과 연결되어 있습니다.\n태스크\nBiocViews 군집화 또는 CRAN의 군집 뷰를 찾아보고 군집화 도구를 제공하는 패키지 수를 세어 보세요.\n\n그림 5.4: 우리는 군집화 알고리즘에서 이루어지는 선택들을 취해진 단계에 따라 분해합니다: 관측치-특성 직사각형 테이블 (X)에서 시작하여, 관측치 간 거리 척도를 선택하고 거리 행렬을 계산합니다(여기서는 삼각형으로 도식화됨). 거리는 클러스터를 구성하는 데 사용됩니다. 왼쪽에는 계층적 군집 트리를 구축하는 응집형(agglomerative) 방법을 도식화했습니다. 오른쪽에는 데이터를 하위 집합으로 분리하는 분할형(partitioning) 방법을 도식화했습니다. 두 유형의 방법 모두 클러스터 수 (k)의 선택을 필요로 합니다. (k)-평균과 같은 분할 접근법의 경우 이 선택은 처음에 이루어져야 합니다. 계층적 군집화의 경우 분석이 끝날 때까지 이를 미룰 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>군집화 (Clustering)</span>"
    ]
  },
  {
    "objectID": "05-chap.html#유사성을-어떻게-측정하는가",
    "href": "05-chap.html#유사성을-어떻게-측정하는가",
    "title": "7  군집화 (Clustering)",
    "section": "7.2 5.3 유사성을 어떻게 측정하는가?",
    "text": "7.2 5.3 유사성을 어떻게 측정하는가?\n\n\n\n유유상종: 거리가 어떻게 측정되고 관측치 사이의 유사성이 어떻게 정의되느냐는 군집화 결과에 강력한 영향을 미칩니다.\n\n\n유유상종 : 거리가 어떻게 측정되고 관측치 사이의 유사성이 어떻게 정의되느냐는 군집화 결과에 강력한 영향을 미칩니다.\n우리의 첫 번째 단계는 유사함 이 무엇을 의미하는지 결정하는 것입니다. 새들을 비교하는 방법은 여러 가지가 있습니다: 예를 들어 크기와 무게를 사용한 거리는 식단이나 서식지를 사용한 거리와는 다른 군집화 결과를 줄 것입니다. 관련 특징(features)을 선택하고 나면, 여러 특징 간의 차이를 어떻게 하나의 숫자로 결합할지 결정해야 합니다. 여기에 몇 가지 선택지들이 있으며, 그 중 일부가 그림 5.5에 예시되어 있습니다.\n\n\n\n그림 5.5 (a):\n\n\n\n\n\n\n\n\n그림 5.5 (b):\n\n\n\n\n\n\n\n\n그림 5.5 (c):\n\n\n\n\n\n\n\n\n그림 5.5 (d):\n\n\n\n\n\n그림 5.5: 네 가지 서로 다른 거리에 따른 등거리 등고선 플롯: 임의의 한 곡선 상의 점들은 모두 중심점으로부터 동일한 거리에 있습니다.\n유클리드(Euclidean) 점 (A=(a_1,…,a_p))와 (B= (b_1,…,b_p)) 사이의 유클리드 거리는 모든 (p)개 좌표 방향에서의 차이의 제곱합의 제곱근입니다:\n[ d(A,B)=. ]\n맨해튼(Manhattan) 맨해튼, 시가(City Block), 택시 또는 (L_1) 거리는 모든 좌표에서의 절대 차이의 합을 취합니다.\n[ d(A,B)=|a_1-b_1|+|a_2-b_2|+… +|a_p-b_p|. ]\n최댓값(Maximum) 좌표 간의 절대 차이의 최댓값은 (L_) 거리라고도 불립니다:\n[ d_(A,B)= _{i}|a_i-b_i|. ]\n가중 유클리드 거리(Weighted Euclidean distance) 는 특징 공간의 서로 다른 방향에 서로 다른 가중치를 부여함으로써 일반적인 유클리드 거리를 일반화한 것입니다. 우리는 이미 2장에서 가중 유클리드 거리의 한 예인 (^2) 거리를 접했습니다. 이는 분할표의 행들을 비교하는 데 사용되며, 각 특징의 가중치는 기대값의 역수입니다. 마할라노비스(Mahalanobis) 거리는 서로 다른 특징들이 서로 다른 동적 범위를 가질 수 있고, 일부 특징들이 서로 양(+) 또는 음(-)의 상관관계를 가질 수 있다는 사실을 고려하는 또 다른 가중 유클리드 거리입니다. 이 경우 가중치는 특징들의 공분산 행렬로부터 유도됩니다. 질문 5.1도 참조하세요.\n민코프스키(Minkowski) 유클리드 거리에서처럼 지수를 2가 아닌 (m)으로 허용하면 다음과 같은 민코프스키 거리를 얻습니다.\n[ d(A,B) = ( (a_1-b_1)m+(a_2-b_2)m+… +(a_p-b_p)^m )^. ]\n편집(Edit), 해밍(Hamming) 이 거리는 문자 시퀀스를 비교하는 가장 간단한 방법입니다. 단순히 두 문자열 사이의 차이점의 개수를 셉니다. 이는 뉴클레오타이드나 아미노산 서열에 적용될 수 있습니다 – 비록 그 경우, 서로 다른 문자 치환은 물리적 또는 진화적 유사성을 고려하여 대개 서로 다른 거리 기여도와 연관되며, 삭제와 삽입도 허용될 수 있습니다.\n이진(Binary) 두 벡터가 좌표로 이진 비트를 가질 때, 우리는 0이 아닌 원소를 ’on’으로, 0인 원소를 ’off’로 생각할 수 있습니다. 이진 거리는 적어도 하나의 비트가 켜진 특징들 중에서 오직 하나의 비트만 켜진 특징들의 비율입니다.\n자카드 거리(Jaccard Distance) 생태학적 또는 돌연변이 데이터에서 특성이나 특징의 발생은 존재와 부재로 번역되어 1과 0으로 인코딩될 수 있습니다. 그러한 상황에서, 공동 발생(co-occurrence)은 종종 공동 부재(co-absence)보다 더 정보가 많습니다. 예를 들어, HIV의 돌연변이 패턴을 비교할 때, 두 서로 다른 균주에서의 돌연변이 공존은 그들의 공동 부재보다 더 중요한 관찰인 경향이 있습니다. 이러한 이유로 생물학자들은 자카드 지수(Jaccard index) 를 사용합니다. 우리의 두 관측 벡터를 (S)와 (T)라고 하고, (f_{11})을 특징이 (S)와 (T) 모두에서 공동 발생하는 횟수, (f_{10}) (및 (f_{01}))을 특징이 (S)에는 나타나지만 (T)에는 나타나지 않는(또는 그 반대) 횟수, 그리고 (f_{00})을 특징이 공동으로 부재하는 횟수라고 합시다. 자카드 지수는 다음과 같습니다.\n[ J(S,T) = , ]\n(즉, (f_{00})을 무시합니다), 그리고 자카드 비유사성(Jaccard dissimilarity) 은 다음과 같습니다.\n[ d_J(S,T) = 1-J(S,T) = . ]\n상관관계 기반 거리\n[ d(A,B)=. ]\n\n그림 5.6: 두 클러스터 중심으로부터 새로운 데이터 점(빨간색)의 거리를 측정하기 위한 마할라노비스 거리 사용 예시.\n질문 5.1\n그림 5.6의 두 클러스터 중심 중 빨간색 점은 어느 쪽에 더 가깝나요?\n해결책\n나이브한 대답은 유클리드 메트릭을 사용하여 그 점이 왼쪽 클러스터에 더 가깝다고 결정할 것입니다. 하지만 특징들이 서로 다른 범위와 상관관계를 가지고 있으며, 이들이 두 클러스터 사이에서조차 다르다는 것을 알 수 있으므로, 클러스터 특이적인 마할라노비스 거리를 사용하는 것이 타당합니다. 그림은 두 클러스터에 대한 등고선을 보여줍니다. 이들은 밀도 추정으로부터 얻어졌습니다. 마할라노비스 거리는 이러한 등고선들을 타원으로 근사합니다. 빨간색 점과 각 클러스터 중심 사이의 거리는 가로지르는 등고선의 수에 해당합니다. 오른쪽 그룹이 더 넓게 퍼져 있기 때문에, 빨간색 점은 사실 그 쪽에 더 가깝다는 것을 알 수 있습니다.\n\n그림 5.7: 거리의 하삼각 행렬은 다양한 R 패키지의 수백 가지 함수들에 의해 계산될 수 있습니다 (vegan 의 vegdist, cluster 의 daisy, gstudio 의 genetic_distance, ape 의 dist.dna, amap 의 Dist, ecodist 의 distance, distory 의 dist.multiPhylo, gdistance 의 shortestPath, % ade4 의 dudi.dist 및 dist.genet).\n\n7.2.1 5.3.1 R에서의 거리 관련 계산\nR의 dist 함수는 (n)개 객체 사이의 완전한 (n imes n) 거리 행렬이 필요로 할 (n^2)개 위치보다 적은 공간을 사용하도록 설계되었습니다. 이 함수는 6가지 거리 선택지(euclidean, maximum, manhattan, canberra, binary, minkowski) 중 하나를 계산하고 완전한 거리 행렬을 재구성하기에 충분한 값들의 벡터를 출력합니다. 함수는 (n imes(n-1)/2) 크기의 관련 벡터를 인코딩하는 dist 클래스의 특수 객체를 반환합니다. 다음은 (3 imes 3) 행렬에 대한 출력입니다:\nmx  = c(0, 0, 0, 1, 1, 1)\nmy  = c(1, 0, 1, 1, 0, 1)\nmz  = c(1, 1, 1, 0, 1, 1)\nmat = rbind(mx, my, mz)\ndist(mat)\n\n\n         mx       my\nmy 1.732051         \nmz 2.000000 1.732051\n\n\ndist(mat, method = \"binary\")\n\n\n          mx        my\nmy 0.6000000          \nmz 0.6666667 0.5000000\n특정 거리(예를 들어 관측치 1과 2 사이의 거리)에 접근하려면, dist 클래스 객체를 다시 행렬로 변환해야 합니다.\nload(\"../data/Morder.RData\")\nsqrt(sum((Morder[1, ] - Morder[2, ])^2))\n\n\n[1] 5.593667\n\n\nas.matrix(dist(Morder))[2, 1]\n\n\n[1] 5.593667\nHIV 균주들 사이에 위에서 정의한 자카드 거리를 어떻게 계산하는지 살펴봅시다.\nmut = read.csv(\"../data/HIVmutations.csv\")\nmut[1:3, 10:16]\n\n\n  p32I p33F p34Q p35G p43T p46I p46L\n1    0    1    0    0    0    0    0\n2    0    1    0    0    0    1    0\n3    0    1    0    0    0    0    0\n질문 5.2\nHIV 데이터 mut의 돌연변이들 사이의 자카드 거리(R 패키지 vegan 의 vegdist 함수로 사용 가능)와 상관관계 기반 거리를 비교해 보세요.\n해결책\nlibrary(\"vegan\")\nmutJ = vegdist(mut, \"jaccard\")\nmutC = sqrt(2 * (1 - cor(t(mut))))\nmutJ\n\n\n      1     2     3     4\n2 0.800                  \n3 0.750 0.889            \n4 0.900 0.778 0.846      \n5 1.000 0.800 0.889 0.900\n\n\nas.dist(mutC)\n\n\n     1    2    3    4\n2 1.19               \n3 1.10 1.30          \n4 1.32 1.13 1.30     \n5 1.45 1.19 1.30 1.32\n\n그림 5.8: 코페네틱 거리(cophenetic distance) 계산 예시 (xkcd).\n전통적인 벡터나 실수가 아닌 복잡한 객체들을 비유사성이나 거리를 사용하여 비교하는 것도 흥미로울 수 있습니다. 혼합 모달리티 데이터(범주형 요인과 연속형 변수 모두)에 대한 고워(Gower) 거리는 daisy 함수로 계산할 수 있습니다. 사실 거리는 단순히 ({R}^p) 상의 점들이나 문자 시퀀스뿐만 아니라 임의의 객체 쌍 사이에서도 정의될 수 있습니다. 예를 들어, 10장에서 보게 될 igraph 패키지의 shortest.paths 함수는 그래프 상의 정점들 사이의 거리를 계산하고, cophenetic 함수는 그림 5.8에 예시된 것처럼 트리의 잎들 사이의 거리를 계산합니다. 우리는 distory 패키지의 dist.multiPhylo를 사용하여 트리들 사이의 거리를 계산할 수 있습니다.\n그래프 간의 자카드 지수는 동일한 노드 위에 구축된 두 그래프를 살펴보고 공동 발생하는 에지의 수를 셈으로써 계산될 수 있습니다. 이는 igraph 패키지의 similarity 함수에 구현되어 있습니다. 거리와 비유사성은 이미지, 소리, 지도 및 문서를 비교하는 데에도 사용됩니다. 거리는 도메인 지식을 유용하게 아우를 수 있으며, 신중하게 선택된다면 불균질한 데이터를 포함하는 많은 어려운 문제들에 대한 해결책으로 이어질 수 있습니다. 여러분의 데이터에 적합한 “가까움” 또는 유사성의 개념이 무엇인지 스스로에게 묻는 것은, 9장에서 탐구할 것처럼 데이터를 표현하는 유용한 방법들을 제공할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>군집화 (Clustering)</span>"
    ]
  },
  {
    "objectID": "05-chap.html#비모수적-혼합-탐지",
    "href": "05-chap.html#비모수적-혼합-탐지",
    "title": "7  군집화 (Clustering)",
    "section": "7.3 5.4 비모수적 혼합 탐지",
    "text": "7.3 5.4 비모수적 혼합 탐지\n\n7.3.1 5.4.1 (k)-방법: (k)-평균, (k)-메도이드 및 PAM\n\n\n\n그룹의 중심을 때때로 메도이드(medoids)라고 부르며, 따라서 PAM(partitioning around medoids)이라는 이름이 붙었습니다.\n\n\n그룹의 중심을 때때로 메도이드(medoids)라고 부르며, 따라서 PAM(partitioning around medoids)이라는 이름이 붙었습니다.\n분할(Partitioning) 또는 반복 재배치(iterative relocation) 방법들은 고차원 설정에서 잘 작동하는데, 거기서는 4장에서 했던 것과 같은 방식으로 확률 밀도, EM 알고리즘 및 모수적 혼합 모델링을 쉽게 사용할 수 없기 4 때문입니다. 거리 측정 외에 이루어져야 할 주요 선택은 클러스터의 수인 (k)입니다. PAM(partitioning around medoids, Kaufman and Rousseeuw (2009)) 방법은 다음과 같습니다:\n4 이는 소위 차원의 저주(curse of dimensionality) 때문입니다. 우리는 이에 대해 12장에서 더 자세히 논의할 것입니다.\n\n일련의 (n)개 관측치에 대해 측정된 (p)개 특징들의 행렬에서 시작합니다.\n(n)개 관측치 중에서 (k)개의 뚜렷한 클러스터 중심 (“시드(seeds)”)을 무작위로 뽑습니다.\n나머지 각 관측치를 그것과 가장 가까운 중심을 가진 그룹에 할당합니다.\n각 그룹에 대해, 그룹 구성원들과의 거리 합이 최소가 되도록 그룹 내 관측치들 중에서 새로운 중심을 선택합니다; 이를 메도이드(medoid) 라고 부릅니다.\n그룹들이 안정화될 때까지 3단계와 4단계를 반복합니다.\n\n알고리즘이 실행될 때마다 2단계에서 서로 다른 초기 시드들이 뽑힐 것이며, 일반적으로 이는 서로 다른 최종 결과로 이어질 수 있습니다. 널리 쓰이는 구현체는 cluster 패키지의 pam 함수입니다.\n방법의 약간의 변형으로 메도이드를 클러스터의 산술 평균(질량 중심)으로 대체하는 것을 (k)-평균(((k)-means)이라고 합니다. PAM에서는 중심이 관측치인 반면, (k)-평균에서는 일반적으로 그렇지 않습니다. kmeans 함수는 모든 R 설치 시 stats 패키지와 함께 제공됩니다; 예시 실행 결과가 그림 5.9에 나와 있습니다.\n[](05-chap_files/figure-html/fig-clust-kmeansastep1-1.png “그림 5.9 (a):”)\n\n\n\n[](05-chap_files/figure-html/fig-clust-kmeansastep1-2.png “그림 5.9 (b):”)\n\n\n\n[](05-chap_files/figure-html/fig-clust-kmeansastep1-3.png “그림 5.9 (c):”)\n\n\n\n그림 5.9: (k)-평균 알고리즘의 예시 실행. 무작위로 선택된 초기 중심(검은 원)과 그룹(색상)이 (a)에 표시되어 있습니다. 그룹 멤버십은 중심까지의 거리에 따라 할당됩니다. 각 반복 (b)와 (c)에서, 그룹 중심이 재정의되고 점들이 클러스터 중심에 재할당됩니다.\n이른바 (k)-방법들은 군집화를 위한 가장 일반적인 오프더쉘프(off-the-shelf) 방법들입니다; 이들은 클러스터들의 크기가 비슷하고 볼록한(blob 모양) 경우에 특히 잘 작동합니다. 반면에, 실제 클러스터들의 크기가 매우 다르다면 큰 클러스터들이 쪼개지는 경향이 있으며, 비구형(non-spherical)이나 비타원형 모양을 가진 그룹들의 경우에도 마찬가지입니다.\n질문 5.3\n(k)-평균 알고리즘은 평균 지점을 계산하는 것과 점들을 클러스터에 할당하는 것을 번갈아 가며 수행합니다. 이 교대하는 반복적인 방법이 EM 알고리즘과는 어떻게 다른가요?\n해결책\nEM 알고리즘에서는 각 점이 자신에게 할당된 확률적 가중치를 통해 모든 그룹의 평균 계산에 참여합니다. (k)-평균 방법에서는 점들이 클러스터에 속하거나 속하지 않거나 둘 중 하나이므로, 각 점은 오직 하나의 클러스터 중심 계산에만 전적으로 참여합니다.\n\n\n7.3.2 5.4.2 재표본 추출을 통한 타이트한 클러스터\n서로 다른 초기 중심이나 재표본 추출된 데이터 세트를 사용하여 과정을 여러 번 반복하는 영리한 체계들이 있습니다. 동일한 데이터에 대해 군집화 절차를 여러 번 반복하되 시작 지점을 다르게 하는 것은 Diday와 Brito (1989)에 따르면 강한 형태(strong forms) 를 생성합니다. 데이터 세트를 반복적으로 하위 샘플링(subsampling)하고 군집화 방법을 적용하면 “거의 항상” 함께 묶이는 관측치 그룹들이 나타날 것입니다; 이들을 타이트한 클러스터(tight clusters) 라고 부릅니다 (Tseng and Wong 2005). 강한 형태나 타이트한 클러스터에 대한 연구는 클러스터 수의 선택을 용이하게 합니다. 최근에 개발된 clusterExperiment 패키지는 많은 서로 다른 군집화 결과들을 결합하고 비교하기 위해 만들어졌습니다. 여기서는 비네트에 나온 예시를 하나 들어보겠습니다. 단일 세포 RNA-Seq 실험은 개별 세포로부터 유전자 전사체를 나타내는 리드 카운트를 제공합니다. 단일 세포 해상도는 연구자로 하여금 무엇보다도 세포 계통(lineage)의 역학을 추적할 수 있게 해줍니다. 군집화는 그러한 데이터를 분석하는 데 매우 유용한 것으로 입증되었습니다.\n질문 5.4\nclusterExperiment 패키지의 비네트를 따라가 보세요. 앙상블 군집화 함수인 clusterMany를 호출하되, 개별 군집화 노력에는 pam을 사용하세요. 포함할 유전자 선택은 가변성이 가장 높은 60개, 100개 또는 150개 유전자로 설정하세요. (k)를 4에서 9까지 변화시키며 군집화 결과들을 플롯해 보세요. 무엇을 알 수 있나요?\n해결책\n다음 코드는 그림 5.10을 생성합니다.\nlibrary(\"clusterExperiment\")\nfluidigm = scRNAseq::ReprocessedFluidigmData()\nse = fluidigm[, fluidigm$Coverage_Type == \"High\"]\nassays(se) = list(normalized_counts = as.matrix(limma::normalizeQuantiles(assay(se))))\nce = clusterMany(se, clusterFunction = \"pam\", ks = c(5, 7, 9), run = TRUE,\n                 isCount = TRUE, reduceMethod = \"var\", nFilterDims = c(60, 100, 150))\n\n\n9 parameter combinations, 0 use sequential method, 0 use subsampling method\nRunning Clustering on Parameter Combinations...\ndone.\n\n\nclusterLabels(ce) = sub(\"FilterDims\", \"\", clusterLabels(ce))\nplotClusters(ce, whichClusters = \"workflow\", axisLine = -1)\n\n그림 5.10: 포함된 유전자 수와 클러스터 수 (k)의 변화에 따른 군집화 결과(행)의 비교. 히트맵의 각 열은 세포에 대응하며, 색상은 할당된 클러스터를 나타냅니다.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>군집화 (Clustering)</span>"
    ]
  },
  {
    "objectID": "05-chap.html#군집화-예시-유세포-분석-및-질량-분석",
    "href": "05-chap.html#군집화-예시-유세포-분석-및-질량-분석",
    "title": "7  군집화 (Clustering)",
    "section": "7.4 5.5 군집화 예시: 유세포 분석 및 질량 분석",
    "text": "7.4 5.5 군집화 예시: 유세포 분석 및 질량 분석\n\n\n\n유세포 분석을 위한 생물정보학적 방법들에 대한 리뷰는 (O’Neill et al. 2013)와 잘 관리된 위키백과 문서에서 찾을 수 있습니다.\n\n\n유세포 분석을 위한 생물정보학적 방법들에 대한 리뷰는 (O’Neill et al. 2013)와 잘 관리된 위키백과 문서에서 찾을 수 있습니다.\n단일 세포에 대한 측정값을 연구하면 세포 유형과 역학을 분석할 수 있는 초점과 해상도가 모두 향상됩니다. 유세포 분석(flow cytometry)은 약 10가지의 서로 다른 세포 마커를 동시에 측정할 수 있게 해줍니다. 질량 분석(mass cytometry)은 측정 컬렉션을 세포당 최대 80개의 단백질로 확장합니다. 이 기술의 특히 유망한 응용 분야는 면역 세포 역학 연구입니다.\n\n7.4.1 5.5.1 유세포 분석 및 질량 분석\n발달의 서로 다른 단계에서 면역 세포는 표면에 고유한 단백질 조합을 발현합니다. 이러한 단백질 마커는 CD (clusters of differentiation, 분화 클러스터)라고 불리며 유세포 분석(형광 사용, Hulett et al. (1969)) 또는 질량 분석(중원소 리포터의 단일 세포 원자 질량 분석법 사용, Bendall et al. (2012)))에 의해 수집됩니다. 흔히 사용되는 CD의 예로 CD4가 있는데, 이 단백질은 “CD4+”라고 불리는 보조 T 세포(helper T cells)에 의해 발현됩니다. 하지만 일부 세포는 CD4를 발현하지만(따라서 CD4+임), 실제로는 보조 T 세포가 아니라는 점에 유의하세요. 먼저 세포 분석(cytometry) 데이터를 위한 유용한 바이오컨덕터 패키지인 flowCore 와 flowViz 를 불러오고, 다음과 같이 예시 데이터 객체 fcsB를 읽어들입니다:\nlibrary(\"flowCore\")\nlibrary(\"flowViz\")\nfcsB = read.FCS(\"../data/Bendall_2011.fcs\", truncate_max_range = FALSE)\nslotNames(fcsB)\n\n\n[1] \"exprs\"       \"parameters\"  \"description\"\n그림 5.11은 fcsB 데이터에서 사용 가능한 두 변수의 산점도를 보여줍니다. (이러한 플롯을 만드는 방법은 아래에서 살펴보겠습니다.) 이 두 차원에서 명확한 이봉성과 군집화를 볼 수 있습니다.\n질문 5.5\n\nfcsB 객체의 구조를 살펴보세요(힌트: colnames 함수). 얼마나 많은 변수가 측정되었나요?\n처음 몇 행을 보기 위해 데이터를 하위 집합화해 보세요(힌트: Biobase::exprs(fcsB) 사용). 얼마나 많은 세포가 측정되었나요?\n\n\n\n7.4.2 5.5.2 데이터 전처리\n먼저 동위원소(isotopes)와 마커(항체) 사이의 매핑을 보고하는 테이블 데이터를 불러온 다음, fcsB의 열 이름에 있는 동위원소 이름을 마커 이름으로 바꿉니다. 이렇게 하면 후속 분석 및 플로팅 코드가 더 읽기 쉬워집니다:\nmarkersB = readr::read_csv(\"../data/Bendall_2011_markers.csv\")\nmt = match(markersB$isotope, colnames(fcsB))\nstopifnot(!any(is.na(mt)))\ncolnames(fcsB)[mt] = markersB$marker\n이제 그림 5.11을 생성할 준비가 되었습니다.\nflowPlot(fcsB, plotParameters = colnames(fcsB)[2:3], logy = TRUE)\n\n그림 5.11: 2차원에서 명확한 군집화를 보여주는 세포 측정값.\n그림 5.11에서처럼 데이터를 2차원으로 플롯하는 것만으로도 세포들이 하위 모집단으로 그룹화될 수 있음을 이미 보여줍니다. 때때로 단 하나의 마커만이 모집단을 정의하는 데 사용될 수 있는데, 그 경우 모집단을 분리하기 위해 단순한 직사각형 게이팅(rectangular gating) 이 사용됩니다. 예를 들어, CD4+ 세포는 CD4 마커에 대해 높은 값을 가진 하위 모집단을 취함으로써 게이팅될 수 있습니다. 세포 군집화는 데이터 변환을 신중하게 선택함으로써 개선될 수 있습니다. 그림 5.12의 왼쪽 부분은 변환 전의 단순한 1차원 히스토그램을 보여주며, 그림 5.12의 오른쪽에서는 변환 후의 분포를 봅니다. 이는 이봉성(bimodality)과 두 세포 모집단의 존재를 드러냅니다.\n데이터 변환: 하이퍼볼릭 아크사인(asinh). 유세포 분석과 질량 분석 데이터 모두를 여러 특수 함수 중 하나를 사용하여 변환하는 것이 표준입니다. 우리는 역 하이퍼볼릭 사인(asinh)의 예시를 취합니다: ((x) = ). 이로부터 우리는 큰 (x) 값에 대해 ((x))가 로그 함수처럼 행동하며 실제로는 ((x)+(2))와 거의 같음을 알 수 있습니다. 작은 (x)에 대해 이 함수는 (x)에 대해 거의 선형적입니다.\n태스크\n변환의 두 가지 주요 영역인 작은 값과 큰 값을 확인하기 위해 다음 코드를 실행해 보세요.\nv1 = seq(0, 1, length.out = 100)\nplot(v1, asinh(v1), type = 'l')\n\n\n plot(log(v1), asinh(v1), type = 'l')\n\n\nv3 = seq(30, 3000, length = 100)\nplot(log(v3), asinh(v3), type= 'l')\n이것은 4장과 8장에서도 언급된 분산 안정화 변환의 또 다른 예입니다. 그림 5.12는 flowCore 패키지의 arcsinhTransform 함수를 사용하는 다음 코드로 생성되었습니다.\nasinhtrsf = arcsinhTransform(a = 0.1, b = 1)\nfcsBT = transform(fcsB, transformList(colnames(fcsB)[-c(1, 2, 41)], asinhtrsf))\ndensityplot(~`CD3all`, fcsB)\ndensityplot(~`CD3all`, fcsBT)\n\n\n\n\n\n\n\n\n그림 5.12: 패널 (a)는 CD3all 변수의 히스토그램을 보여줍니다: 세포들이 0 근처에 군집해 있고 몇 개의 큰 값들이 있습니다. (b)에서는 asinh 변환 후 세포들이 군집을 이루어 두 그룹 또는 유형으로 나뉘는 것을 볼 수 있습니다.\n질문 5.6\n다음 코드는 (k)-평균을 사용하여 데이터를 2개의 그룹으로 나누기 위해 몇 개의 차원을 사용하나요?\nkf = kmeansFilter(\"CD3all\" = c(\"Pop1\",\"Pop2\"), filterId=\"myKmFilter\")\nfres = flowCore::filter(fcsBT, kf)\nsummary(fres)\n\n\nPop1: 33434 of 91392 events (36.58%)\nPop2: 57958 of 91392 events (63.42%)\n\n\nfcsBT1 = flowCore::split(fcsBT, fres, population = \"Pop1\")\nfcsBT2 = flowCore::split(fcsBT, fres, population = \"Pop2\")\n다음 코드로 생성된 그림 5.13은 CD3와 CD56 마커에 의해 확장된 두 차원으로 데이터를 단순 투영한 것을 보여줍니다:\nlibrary(\"flowPeaks\")\nfp = flowPeaks(Biobase::exprs(fcsBT)[, c(\"CD3all\", \"CD56\")])\nplot(fp)\n\n그림 5.13: 변환 후 이 세포들은 kmeans를 사용하여 군집화되었습니다.\n어느 영역에 밀집된 점들을 플롯할 때는 겹쳐그리기(overplotting)를 피해야 합니다. 3장에서 선호되는 기법들 중 일부를 보았습니다. 여기서는 등고선(contours)과 음영(shading)을 사용합니다. 다음과 같이 수행합니다:\nflowPlot(fcsBT, plotParameters = c(\"CD3all\", \"CD56\"), logy = FALSE)\ncontour(fcsBT[, c(40, 19)], add = TRUE)\n\n그림 5.14: 그림 5.13과 유사하지만 등고선을 사용했습니다.\n태스크\n바이오컨덕터 패키지 ggcyto 는 ggplot을 사용하여 각 환자의 데이터를 서로 다른 패싯(facet)에 그릴 수 있게 해줍니다. 다음과 같은 방식으로 이 접근법을 사용한 출력과 위에서 수행한 작업을 비교해 보세요:\nlibrary(\"ggcyto\")\nlibrary(\"labeling\")\n\np1 = ggcyto(fcsB, aes(x = CD4)) + geom_histogram(bins = 60)\np2 = ggcyto(fcsB, aes(x = CD8)) + geom_histogram(bins = 60)\np3 = ggcyto(fcsB, aes(x = CD4, y = CD8)) + geom_density2d(colour = \"black\")\n\nfcsBT = transform(fcsB, transformList(colnames(fcsB)[-c(1, 2, 41)], \n                                      arcsinhTransform(a = 0, b = 1)))\n                                      \np1t = ggcyto(fcsBT, aes(x = CD4))            + geom_histogram(bins = 90)\np2t = ggcyto(fcsBT, aes(x = CD4,y = CD8))    + geom_density2d(colour = \"black\")\np3t = ggcyto(fcsBT, aes(x = CD45RA,y = CD20))+ geom_density2d(colour = \"black\")\n\n\n7.4.3 5.5.3 밀도 기반 군집화(Density-based clustering)\n마커 수가 적고 세포 수가 많은 유세포 분석과 같은 데이터 세트는 밀도 기반 군집화에 적합합니다. 이 방법은 희소한 영역에 의해 분리된 고밀도 영역을 찾습니다. 이 방법은 클러스터가 반드시 볼록할 필요가 없는 경우에도 대처할 수 있다는 장점이 있습니다. 이러한 방법의 한 구현체로 dbscan이 있습니다. 다음 코드를 실행하여 예시를 살펴보겠습니다.\nlibrary(\"dbscan\")\nmc5 = Biobase::exprs(fcsBT)[, c(15,16,19,40,33)]\nres5 = dbscan::dbscan(mc5, eps = 0.65, minPts = 30)\nmc5df = data.frame(mc5, cluster = as.factor(res5$cluster))\ntable(mc5df$cluster)\n\n\n    0     1     2     3     4     5     6     7     8 \n76053  4031  5450  5310   257   160    63    25    43 \n\n\nggplot(mc5df, aes(x=CD4,    y=CD8,  col=cluster))+geom_density2d()\nggplot(mc5df, aes(x=CD3all, y=CD20, col=cluster))+geom_density2d()\n\n\n\n\n\n\n\n\n그림 5.15: 이 두 플롯은 5개의 마커를 사용하여 dbscan으로 군집화한 결과를 보여줍니다. 여기서는 데이터를 CD4-CD8 및 C3all-CD20 평면에 투영한 것만 보여줍니다.\n출력 결과는 그림 5.15에 나와 있습니다. 2D 투영에서의 클러스터 중첩을 통해 군집화의 다차원적 특성을 이해할 수 있습니다.\n질문 5.7\n입력 데이터에서 CD 마커 변수 하나를 추가하여 차원을 6으로 늘려보세요.\n그런 다음 eps를 변화시키면서, 적어도 두 개가 100개 이상의 점을 가진 4개의 클러스터를 찾아보세요.\n7개의 CD 마커 변수로 이 작업을 반복해 보세요. 무엇을 알 수 있나요?\n해결책\n다음 6개 마커를 사용한 예시입니다.\nmc6 = Biobase::exprs(fcsBT)[, c(15, 16, 19, 33, 25, 40)]\nres = dbscan::dbscan(mc6, eps = 0.65, minPts = 20)\nmc6df = data.frame(mc6, cluster = as.factor(res$cluster))\ntable(mc6df$cluster)\n\n\n    0     1     2     3     4     5     6 \n91068    34    61    20    67   121    21 \n우리는 eps=0.75일 때 eps=0.65일 때보다 충분히 큰 클러스터를 찾기가 더 쉽다는 것을 알 수 있으며, eps=0.55일 때는 불가능합니다. 차원수를 7로 늘리면 eps를 훨씬 더 크게 만들어야 합니다.\nmc7 = Biobase::exprs(fcsBT)[, c(11, 15, 16, 19, 25, 33, 40)]\nres = dbscan::dbscan(mc7, eps = 0.95, minPts = 20)\nmc7df = data.frame(mc7, cluster = as.factor(res$cluster))\ntable(mc7df$cluster)\n\n\n    0     1     2     3     4     5     6     7     8     9    10 \n90249    21   102   445   158   119    19   224    17    20    18 \n이는 소위 차원의 저주(curse of dimensionality) 가 실제로 작동하는 것을 보여주며, 이에 대해서는 12장에서 더 자세히 다룹니다.\n\n7.4.3.1 밀도 기반 군집화(dbscan)는 어떻게 작동하나요 ?\ndbscan 방법은 밀도 연결성(density-connectedness) 기준에 따라 고밀도 영역의 점들을 군집화합니다. 이 방법은 점들이 연결되어 있는지 확인하기 위해 반지름 ()인 작은 이웃 구(neighborhood spheres)를 살펴봅니다.\ndbscan의 기본 구성 요소는 밀도 도달 가능성(density-reachability) 개념입니다: 점 (q)가 점 (p)로부터 주어진 임계값 ()보다 멀리 있지 않고, (p)가 충분히 많은 점들에 둘러싸여 있어 (p)(및 (q))를 밀집 영역의 일부로 간주할 수 있다면, 점 (q)는 점 (p)로부터 직접 밀도 도달 가능(density-reachable)합니다. (p_1 = p)이고 (p_n = q)인 일련의 점 (p_1, …, p_n)이 있어서 각 (p_{i+1})이 (p_i)로부터 직접 밀도 도달 가능하다면, (q)는 (p)로부터 밀도 도달 가능 하다고 합니다.\n\n\n\n방법이 이웃에서 높은 점 밀도를 찾는 것이 중요합니다. 클러스터 사이의 공백(void) 또는 “누락된 점”에 의해 클러스터를 정의하려는 다른 방법들이 존재합니다. 그러나 이러한 방법들은 차원의 저주에 취약하며, 가짜 “공백”을 만들어낼 수 있습니다.\n\n\n방법이 이웃에서 높은 점 밀도를 찾는 것이 중요합니다. 클러스터 사이의 공백(void) 또는 “누락된 점”에 의해 클러스터를 정의하려는 다른 방법들이 존재합니다. 그러나 이러한 방법들은 차원의 저주에 취약하며, 가짜 “공백”을 만들어낼 수 있습니다.\n그러면 클러스터 는 다음 속성들을 만족하는 점들의 하위 집합입니다:\n\n클러스터 내의 모든 점은 서로 밀도 연결되어 있습니다.\n어떤 점이 클러스터의 임의의 점에 밀도 연결되어 있다면, 그 점 역시 클러스터의 일부입니다.\n점들의 그룹이 클러스터로 간주되려면 적어도 MinPts개의 점을 가져야 합니다.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>군집화 (Clustering)</span>"
    ]
  },
  {
    "objectID": "05-chap.html#계층적-군집화hierarchical-clustering",
    "href": "05-chap.html#계층적-군집화hierarchical-clustering",
    "title": "7  군집화 (Clustering)",
    "section": "7.5 5.6 계층적 군집화(Hierarchical clustering)",
    "text": "7.5 5.6 계층적 군집화(Hierarchical clustering)\n\n그림 5.16: 특성 유사성에 따라 유기체들을 군집화한 린네의 분류 체계의 일부.\n계층적 군집화는 유사한 관측치와 하위 클래스를 반복적으로 조립하는 상향식(bottom-up) 접근 방식입니다. 그림 5.16은 린네가 특정 특성에 따라 유기체들의 중첩된 클러스터를 어떻게 만들었는지 보여줍니다. 이러한 계층적 조직은 많은 분야에서 유용하게 사용되어 왔으며, 자연의 사다리(ladder of nature) 를 상정한 아리스토텔레스까지 거슬러 올라갑니다.\n덴드로그램 순서(Dendrogram ordering). 그림 5.17의 예에서 볼 수 있듯이, 레이블의 순서는 형제 쌍(sibling pairs) 내에서는 중요하지 않습니다. 수평 거리는 대개 무의미한 반면, 수직 거리는 어떤 정보를 인코딩합니다. 이러한 속성들은 계통발생학적(monophyletic)이지 않지만(즉, 동일한 서브트리나 클레이드(clade)에 속하지 않음) 플롯에서는 이웃으로 나타나는 대상(예를 들어 오른쪽 나무의 B와 D)에 대해 해석을 내릴 때 기억해야 할 중요한 사항입니다.\n\n그림 5.17: 동일한 계층적 군집 트리의 세 가지 표현 방식.\n하향식 계층 구조(Top-down hierarchies). 대안적인 하향식 접근 방식은 모든 객체를 가져와서 선택된 기준에 따라 순차적으로 분할합니다. 이러한 소위 재귀적 분할(recursive partitioning) 방법은 종종 의사 결정 나무(decision trees)를 만드는 데 사용됩니다. 이들은 예측(예를 들어 의료 진단이 주어졌을 때의 생존 기간)에 유용할 수 있습니다: 우리는 그러한 사례들에서 분할을 통해 불균질한 모집단을 더 균질한 하위 그룹으로 나누기를 희망합니다. 이 장에서 우리는 상향식 접근 방식에 집중합니다. 12장에서 비지도 학습과 분류에 대해 이야기할 때 분할 방식으로 다시 돌아올 것입니다.\n\n7.5.1 5.6.1 집계된 클러스터 간의 (비)유사성을 어떻게 계산하는가?\n\n그림 5.18: 단일 연결법(single linkage method)에서, 그룹 (C_1)과 (C_2) 사이의 거리는 두 그룹의 점들 사이의 가장 짧은 거리로 정의됩니다.\n상향식으로 작동하는 계층적 군집화 알고리즘은 가장 유사한 관측치들을 함께 그룹화함으로써 쉽게 시작할 수 있습니다. 하지만 우리에게는 단순히 모든 개별 객체 쌍 사이의 거리 그 이상의 것이 필요할 것입니다. 일단 합쳐지고 나면, 새로 형성된 클러스터와 다른 모든 점(또는 기존 클러스터) 사이의 거리가 어떻게 계산되는지 명시해야 합니다. 개별 객체 간 거리에 기반한 여러 선택지가 있으며, 각 선택은 서로 다른 유형의 계층적 군집화를 결과로 냅니다.\n최소 도약(minimal jump) 방법은 단일 연결법(single linkage) 또는 최근접 이웃(nearest neighbor) 방법이라고도 불리며, 클러스터 사이의 거리를 두 클러스터에 있는 임의의 두 점 사이의 가장 작은 거리로 계산합니다 (그림 5.18 참조):\n[ d_{12} = {i C_1, j C_2 } d{ij}. ]\n이 방법은 점들의 연속적인 줄(strings)처럼 보이는 클러스터를 만드는 경향이 있습니다. 클러스터 트리는 종종 빗(comb) 모양을 띱니다.\n\n그림 5.19: 완전 연결법(complete linkage method)에서, 그룹 (C_1)과 (C_2) 사이의 거리는 두 그룹의 점 쌍 사이의 최대 거리로 정의됩니다.\n최대 도약(maximum jump) (또는 완전 연결법(complete linkage)) 방법은 클러스터 사이의 거리를 두 클러스터에 있는 임의의 두 객체 사이의 가장 큰 거리로 정의합니다 (그림 5.19 참조):\n[ d_{12} = {i C_1, j C_2 } d{ij}. ]\n평균 연결법(average linkage) 방법은 위 두 방법의 중간 정도입니다 (여기서 (|C_k|)는 클러스터 (k)의 원소 수입니다):\n[ d_{12} = {i C_1, j C_2 } d{ij} ]\n\n그림 5.20: 와드 방법(Ward method)은 그룹 내 제곱합(검은색 에지)을 최소화하면서 그룹 간 제곱합(빨간색 에지)을 최대화합니다.\n와드 방법(Ward’s method) 은 분산 분석 접근 방식을 취하며, 목표는 클러스터 내의 분산을 최소화하는 것입니다. 이 방법은 매우 효율적이지만, 클러스터의 크기가 작아지는 경향이 있습니다.\n\n\n\n집계된 클러스터 간 거리를 정의하는 다양한 선택지의 장단점 (Chakerian and Holmes 2012). 방법\n장점\n단점\n\n\n\n\n단일 연결법\n클러스터 수\n빗 모양 트리\n\n\n완전 연결법\n콤팩트한 클래스\n하나의 관측치가 그룹을 바꿀 수 있음\n\n\n평균 연결법\n유사한 크기와 분산\n강건하지 않음\n\n\n중심(Centroid)\n이상치에 강건함\n클러스터 수가 적어짐\n\n\n와드(Ward)\n관성(inertia) 최소화\n클래스가 작아짐 (높은 가변성)\n\n\n\n\n그림 5.21: 계층적 군집화 출력은 모빌(mobile)과 유사한 속성을 가집니다: 브랜치들은 그들의 매달린 지점을 중심으로 자유롭게 회전할 수 있습니다.\n이것들이 계층적 군집 트리를 구축할 때 우리가 해야 할 선택들입니다. 분할 방법들과 비교할 때 계층적 군집화의 장점은 그룹화의 강도를 그래픽으로 진단할 수 있다는 것입니다: 트리의 내부 에지(inner edges)의 길이입니다.\n클러스터들의 크기가 거의 같다는 사전 지식이 있다면, 그룹 내 분산을 최소화하는 평균 연결법(average linkage)이나 와드 방법(Ward’s method)을 사용하는 것이 가장 좋은 전술입니다.\n질문 5.8\n세포군에 대한 계층적 군집화 Morder 데이터는 10명의 환자로부터 얻은 3가지 유형(naïve, effector, memory)의 T 세포에 대한 156개 유전자의 발현 측정값입니다(Holmes et al. 2005). pheatmap 패키지를 사용하여, 이 데이터의 유클리드 거리와 맨해튼 거리에 대해 덴드로그램이나 재정렬 없이 두 개의 단순한 히트맵을 만드세요.\n질문 5.9\n이제 이 두 거리를 사용한 계층적 군집 트리에서의 순서 차이를 살펴보세요. 어떤 차이점이 눈에 띄나요?\n해결책\n다음과 같이 코드를 실행합니다 (출력은 그림 5.22 참조):\nload(\"../data/Morder.RData\")\npheatmap(Morder, clustering_method = \"single\")\npheatmap(Morder, clustering_method = \"average\")\npheatmap(Morder, clustering_method = \"complete\")\n\n\n\n\n\n\n\n\n\n\n\n\n그림 5.22: 서로 다른 응집(agglomeration) 선택으로 만들어진 세 개의 계층적 군집 플롯. (a)의 단일 연결법(single linkage)에 대한 빗 모양 구조에 주목하세요. 평균 연결법 (b)와 완전 연결법 (c) 트리는 내부 분기의 길이에 의해서만 다릅니다.\n\n그림 5.23: 이 트리는 여러 가지 다른 방식으로 그려질 수 있습니다. 여기에 나타난 잎들의 순서는 ((8,11,9,10,7,5,6,1,4,2,3))입니다.\n질문 5.10\n계층적 군집 트리는 그림 5.21의 콜더 모빌(Calder mobile)과 같아서 많은 내부 피벗 점들을 중심으로 회전할 수 있으며, 주어진 트리와 일관성을 유지하면서 팁(tips) 순서를 제공하는 방법이 많습니다. 그림 5.23의 트리를 보세요. 이 트리와 일관성을 유지하면서 팁 레이블을 정렬할 수 있는 방법은 몇 가지가 있을까요?\n행 및/또는 열이 계층적 군집 트리에 기반하여 정렬된 히트맵을 흔히 볼 수 있습니다. 때때로 이것은 일부 클러스터를 매우 강력해 보이게 만듭니다 – 트리가 실제로 암시하는 것보다 더 강력하게 말이죠. 히트맵에서 행과 열을 정렬하는 대안적인 방법들이 있는데, 예를 들어 서열화 방법(ordination methods)5을 사용하여 순서를 찾는 NeatMap 패키지가 있습니다.\n5 이들은 9장에서 설명될 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>군집화 (Clustering)</span>"
    ]
  },
  {
    "objectID": "05-chap.html#클러스터-수-검증-및-선택",
    "href": "05-chap.html#클러스터-수-검증-및-선택",
    "title": "7  군집화 (Clustering)",
    "section": "7.6 5.7 클러스터 수 검증 및 선택",
    "text": "7.6 5.7 클러스터 수 검증 및 선택\n우리가 설명한 군집화 방법들은 다양한 제약 조건 하에서 데이터의 좋은 그룹화를 제공하도록 맞춤화되어 있습니다. 그러나 군집화 방법은 클러스터가 없더라도 항상 그룹을 제공한다는 점을 명심하세요. 만약 데이터에 실제 클러스터가 없다면, 계층적 군집 트리는 상대적으로 짧은 내부 분기를 보여줄 수 있지만, 이를 정량화하기는 어렵습니다. 일반적으로 보다 객관적인 기준으로 클러스터 선택을 검증하는 것이 중요합니다.\n군집화 결과의 품질을 평가하는 한 가지 기준은 그룹 내 거리를 작게 유지하면서 그룹 간 차이를 어느 정도까지 최대화하느냐 하는 것입니다 (그림 5.20에서 빨간색 선의 길이를 최대화하고 검은색 선의 길이를 최소화하는 것). 우리는 이를 그룹 내 제곱 거리 합 (within-groups sum of squared distances, WSS)으로 공식화합니다:\n[ k={}^k {x_i C} d^2(x_i, {x}_{}) ]\n여기서 (k)는 클러스터의 수, (C_)은 ()번째 클러스터에 있는 객체들의 집합, 그리고 ({x}_)은 ()번째 클러스터의 질량 중심(평균점)입니다. 우리는 동일한 클러스터 알고리즘에 대해 서로 다른 (k) 값에 걸쳐 이 수치를 비교하는 데 관심이 있으므로 식 5.4에서 WSS의 (k)에 대한 의존성을 명시합니다. 하지만 WSS 그 자체로는 충분한 기준이 되지 못합니다: WSS의 최솟값은 단순히 각 점을 개별 클러스터로 만듦으로써 얻어질 수 있기 때문입니다. WSS는 유용한 구성 요소이지만, 이 숫자만 보는 것보다 더 정교한 아이디어가 필요합니다.\n한 가지 아이디어는 (k)의 함수로서 (_k)를 살펴보는 것입니다. 이는 항상 감소 함수이겠지만, 급격히 감소하다가 완만해지는 뚜렷한 영역이 있다면, 우리는 이를 엘보우(elbow) 라고 부르며 이를 클러스터 수의 잠재적인 최적 지점으로 간주할 수 있습니다.\n질문 5.11\n**(_k)에 대한 대안적 표현.** R을 사용하여 클러스터 내의 모든 점 쌍 사이의 거리 합을 계산하고 이를 (_k)와 비교해 보세요. (_k)가 다음과 같이 쓰여질 수 있음을 알 수 있나요?\n[ k={}^k {x_i C} {x_j C} d^2(x_i,x_j), ]\n여기서 (n_)은 ()번째 클러스터의 크기입니다.\n질문 5.11은 클러스터 내 제곱합 (_k)가 클러스터 내의 모든 점과 중심 사이의 거리뿐만 아니라, 클러스터 내의 모든 점 쌍 사이의 평균 거리도 측정한다는 것을 보여줍니다.\n데이터에 적합한 클러스터 수를 결정하는 데 도움이 되는 다양한 지수와 통계량의 거동을 살펴볼 때, 정답을 실제로 알고 있는 경우를 살펴보는 것이 유용할 수 있습니다.\n시작하기 위해, 네 개의 그룹에서 나오는 데이터를 시뮬레이션합니다. 우리는 파이프(%&gt;%) 연산자와 dplyr 의 bind_rows 함수를 사용하여 각 클러스터에 해당하는 네 개의 tibble 을 하나의 큰 tibble 로 연결합니다.6\n6 파이프 연산자는 왼쪽에 있는 값을 오른쪽 함수로 전달합니다. 이는 코드에서 데이터의 흐름을 더 쉽게 따라갈 수 있게 해줍니다: f(x) %&gt;% g(y)는 g(f(x), y)와 동일합니다.\nlibrary(\"dplyr\")\nsimdat = lapply(c(0, 8), function(mx) {\n  lapply(c(0,8), function(my) {\n    tibble(x = rnorm(100, mean = mx, sd = 2),\n           y = rnorm(100, mean = my, sd = 2),\n           class = paste(mx, my, sep = \":\"))\n   }) %&gt;% bind_rows\n}) %&gt;% bind_rows\nsimdat\n\n\n# A tibble: 400 × 3\n        x      y class\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  \n 1 -2.42  -4.59  0:0    \n 2  1.89  -1.56  0:0    \n 3  0.558  2.17  0:0    \n 4  2.51  -0.873 0:0    \n 5 -2.52  -0.766 0:0    \n 6  3.62   0.953 0:0    \n 7  0.774  2.43  0:0    \n 8 -1.71  -2.63  0:0    \n 9  2.01   1.28  0:0    \n10  2.03  -1.25  0:0    \n# ℹ 390 more rows\n\n\nsimdatxy = simdat[, c(\"x\", \"y\")] # class 레이블 제외\n\n\nggplot(simdat, aes(x = x, y = y, col = class)) + geom_point() +\n  coord_fixed()\n\n그림 5.24: 클래스 레이블로 색상이 입혀진 simdat 데이터. 여기서는 우리가 데이터를 생성했으므로 레이블을 알고 있습니다 – 보통은 이를 알지 못합니다.\n우리는 (k)-평균 방법으로 얻은 클러스터들에 대해 그룹 내 제곱합을 계산합니다:\nwss = tibble(k = 1:8, value = NA_real_)\nwss$value[1] = sum(scale(simdatxy, scale = FALSE)^2)\nfor (i in 2:nrow(wss)) {\n  km  = kmeans(simdatxy, centers = wss$k[i])\n  wss$value[i] = sum(km$withinss)\n}\nggplot(wss, aes(x = k, y = value)) + geom_col()\n\n그림 5.25: (k)의 함수로서 WSS 통계량의 막대 그래프는 마지막 실질적인 도약이 (k=4) 직전에 있음을 보여줍니다. 이는 이 데이터에 대한 최선의 선택이 (k=4)임을 나타냅니다.\n질문 5.12\n\n위의 코드를 여러 번 실행하고 서로 다른 실행에서의 wss 값을 비교해 보세요. 왜 이들은 서로 다를까요?\nsimdat와 동일한 범위와 차원을 가진, 정규 분포 대신 균등 분포(uniform distributions)로부터 오는 데이터 세트를 만드세요. 이 데이터에 대해 WSS 값을 계산해 보세요. 무엇을 결론지을 수 있나요?\n\n질문 5.13\n이른바 칼린스키-하라바츠(Calinski-Harabasz) 지수는 WSS와 BSS (between group sums of squares, 그룹 간 제곱합)를 사용합니다. 이는 분산 분석에서 사용되는 (F) 통계량 — 인자에 의해 설명되는 평균 제곱합 대 평균 잔차 제곱합의 비율 — 에서 영감을 받았습니다:\n[ (k)= k = {}^k n_({x}_{}-{x})^2, ]\n여기서 ({x})는 전체 질량 중심(평균점)입니다. simdat 데이터에 대한 칼린스키-하라바츠 지수를 플롯해 보세요.\n해결책\n그림 5.26을 생성하는 코드는 다음과 같습니다.\nlibrary(\"fpc\")\nlibrary(\"cluster\")\nCH = tibble(\n  k = 2:8,\n  value = sapply(k, function(i) {\n    p = pam(simdatxy, i)\n    calinhara(simdatxy, p$cluster)\n  })\n)\nggplot(CH, aes(x = k, y = value)) + geom_line() + geom_point() +\n  ylab(\"CH index\")\n\n그림 5.26: simdat 데이터에 대해 계산된, 서로 다른 (k) 선택에 대한 그룹 간 분산과 그룹 내 분산의 비율인 칼린스키-하라바츠 지수.\n\n7.6.1 5.7.1 갭 통계량 (gap statistic) 사용하기\n그룹 내 제곱합의 로그(((_k)))를 취하고 이를 구조가 덜한 시뮬레이션 데이터의 평균과 비교하는 것은 (k)를 선택하는 좋은 방법이 될 수 있습니다. 이것이 Tibshirani, Walther, Hastie (2001)에 의해 도입된 갭 통계량(gap statistic) 의 기본 아이디어입니다. 우리는 클러스터 수인 여러 (k) 값에 대해 ((_k))를 계산하고, 이를 다양한 가능한 ‘군집되지 않은’ 분포를 가진 유사한 차원의 참조 데이터에서 얻은 것과 비교합니다. 위에서 했던 것처럼 균등하게 분포된 데이터를 사용하거나, 원래 데이터와 동일한 공분산 구조를 가진 시뮬레이션 데이터를 사용할 수 있습니다.\n\n이 알고리즘은 관측된 데이터에 대한 갭 통계량을 유사한 구조를 가진 시뮬레이션 데이터의 평균과 비교하는 몬테카를로 방법입니다.\n갭 통계량 계산 알고리즘 (Tibshirani, Walther, and Hastie 2001):\n\n데이터를 (k)개의 클러스터로 군집화하고 다양한 (k) 선택에 대해 (_k)를 계산합니다.\n균질한 분포로부터 몬테카를로 샘플링을 사용하여 (B)개의 그럴듯한 참조 데이터 세트를 생성하고, 이 새로운 시뮬레이션 데이터에 대해 위의 1단계를 다시 수행합니다. 그 결과 시뮬레이션 데이터에 대한 (B)개의 새로운 그룹 내 제곱합 (W_{kb}^*)((b=1,…,B))을 얻습니다.\n((k))-통계량을 계산합니다:\n\n[ (k) = _k - _k k ={b=1}^B W^*_{kb} ]\n군집화가 잘 되었다면 (즉, WSS가 더 작다면) 첫 번째 항이 두 번째 항보다 클 것으로 예상됩니다. 따라서 갭 통계량은 대부분 양수일 것이며 우리는 그 최댓값을 찾습니다.\n\n표준 편차\n\n[ k^2 = {b=1}B((W*_{kb})-_k)^2 ]\n를 사용하여 최적의 (k)를 선택하는 데 도움을 줄 수 있습니다. 여러 선택지가 있는데, 예를 들어 다음과 같은 최소의 (k)를 선택하는 것입니다.\n[ (k) (k+1) - s’{k+1} s’{k+1}=_{k+1}. ]\nThe packages cluster and clusterCrit provide implementations.\n질문 5.14\nMake a function that plots the gap statistic as in Figure 5.27. Show the output for the simdat example dataset clustered with the pam function.\n해결책\nlibrary(\"cluster\")\nlibrary(\"ggplot2\")\npamfun = function(x, k)\n  list(cluster = pam(x, k, cluster.only = TRUE))\n\ngss = clusGap(simdatxy, FUN = pamfun, K.max = 8, B = 50,\n              verbose = FALSE)\nplot_gap = function(x) {\n  gstab = data.frame(x$Tab, k = seq_len(nrow(x$Tab)))\n  ggplot(gstab, aes(k, gap)) + geom_line() +\n    geom_errorbar(aes(ymax = gap + SE.sim,\n                      ymin = gap - SE.sim), width=0.1) +\n    geom_point(size = 3, col=  \"red\")\n}\nplot_gap(gss)\n\n그림 5.27: 갭 통계량, 질문 5.14를 참조하세요.\nLet’s now use the method on a real example. We load the Hiiragi data that we already explored in Chapter 3 and will see how the cells cluster.\nlibrary(\"Hiiragi2013\")\n\n\nIn chunk 'Hiiragi': Warning: replacing previous import 'boot::logit' by 'gtools::logit' whenloading 'Hiiragi2013'\n\n\nIn chunk 'Hiiragi': Warning: replacing previous import 'boot::inv.logit' by 'gtools::inv.logit'when loading 'Hiiragi2013'\n\n\ndata(\"x\")\nWe start by choosing the 50 most variable genes (features)7.\n7 The intention behind this step is to reduce the influence of technical (or batch) effects. Although individually small, when accumulated over all the 45101 features in x, many of which match genes that are weakly or not expressed, without this feature selection step, such effects are prone to suppress the biological signal.\nselFeats = order(rowVars(Biobase::exprs(x)), decreasing = TRUE)[1:50]\nembmat = t(Biobase::exprs(x)[selFeats, ])\nembgap = clusGap(embmat, FUN = pamfun, K.max = 24, verbose = FALSE)\nk1 = maxSE(embgap$Tab[, \"gap\"], embgap$Tab[, \"SE.sim\"])\nk2 = maxSE(embgap$Tab[, \"gap\"], embgap$Tab[, \"SE.sim\"],\n           method = \"Tibs2001SEmax\")\nc(k1, k2)\n\n\n[1] 9 7\nThe default choice for the number of clusters, k1, is the first value of (k) for which the gap is not larger than the first local maximum minus a standard error (s) (see the manual page of the clusGap function). This gives a number of clusters (k = 9), whereas the choice recommended by Tibshirani, Walther, and Hastie (2001) is the smallest (k) such that ((k) (k+1) - s’_{k+1}), this gives (k = 7). Let’s plot the gap statistic (Figure 5.28).\nplot(embgap, main = \"\")\ncl = pamfun(embmat, k = k1)$cluster\ntable(pData(x)[names(cl), \"sampleGroup\"], cl)\n\n\n                 cl\n                   1  2  3  4  5  6  7  8  9\n  E3.25           23 11  1  1  0  0  0  0  0\n  E3.25 (FGF4-KO)  0  0  1 16  0  0  0  0  0\n  E3.5 (EPI)       2  1  0  0  0  8  0  0  0\n  E3.5 (FGF4-KO)   0  0  8  0  0  0  0  0  0\n  E3.5 (PE)        0  0  0  0  9  2  0  0  0\n  E4.5 (EPI)       0  0  0  0  0  0  0  4  0\n  E4.5 (FGF4-KO)   0  0  0  0  0  0  0  0 10\n  E4.5 (PE)        0  0  0  0  0  0  4  0  0\n\n그림 5.28: Hiiragi2013 데이터에 대한 갭 통계량.\n위에서 우리는 pamfun으로부터 얻은 군집화 결과와 데이터의 어노테이션에 포함된 샘플 레이블을 비교한 것을 볼 수 있습니다.\n질문 5.15\n가장 가변적인 상위 50개 유전자만 사용하는 대신 x의 모든 특성을 사용하면 결과가 어떻게 달라질까요?\n\n\n7.6.2 5.7.2 붓스트랩을 이용한 클러스터 검증\n\n\n\n\n\n\n\n\n그림 5.29: 동일한 분포 (F)로부터 얻은 서로 다른 표본들은 서로 다른 군집화 결과로 이어집니다. (a)에서 우리는 실제 표집 가변성을 봅니다. 붓스트랩은 (b)에서 보듯이 경험적 분포 함수 (_n)을 사용하여 하위 표본(subsamples)을 추출함으로써 이러한 표집 가변성을 시뮬레이션합니다.\n우리는 4장에서 붓스트랩 원리를 보았습니다: 이상적으로는 기저의 데이터 생성 프로세스로부터 많은 새로운 표본(데이터 세트)을 얻어 각각에 군집화 방법을 적용한 다음, 군집화를 비교하기 위해 위에서 사용했던 것과 같은 지수를 사용하여 클러스터가 얼마나 안정적인지 또는 얼마나 변하는지 보고 싶을 것입니다. 물론 우리에게는 이러한 추가 표본이 없습니다. 따라서 우리는 단순히 데이터의 서로 다른 무작위 하위 표본을 취하여 매번 얻는 서로 다른 군집화 결과를 비교함으로써 새로운 데이터 세트를 만들 것입니다. Tibshirani, Walther, Hastie (2001)는 갭 통계량을 사용하여 클러스터 수를 추론할 때 붓스트랩 재표본 추출을 사용할 것을 권장합니다.\n우리는 계속해서 Hiiragi2013 데이터를 사용하겠습니다. 여기서는 생쥐 배아의 배아기 3.5일(E3.5) 배반포의 내세포집단(ICM)이 다분화능 에피블라스트(EPI)와 원시 내배엽(PE)에 해당하는 두 개의 클러스터로 “자연스럽게” 나뉘는 반면, 배아기 3.25일(E3.25)의 데이터는 아직 이러한 대칭성 붕괴(symmetry breaking)를 보여주지 않는다는 가설에 대한 조사를 따라갑니다.\n우리는 군집화 과정에서 실제 그룹 레이블을 사용하지 않고, 결과의 최종 해석에서만 이를 사용할 것입니다. 우리는 (E3.5)와 (E3.25)라는 두 가지 서로 다른 데이터 세트에 각각 붓스트랩을 적용할 것입니다. 붓스트랩의 각 단계는 데이터의 무작위 하위 집합에 대한 군집화 결과를 생성할 것이며, 우리는 이를 클러스터 앙상블의 합의(consensus)를 통해 비교해야 할 것입니다. clue 패키지(Hornik 2005)에 이를 위한 유용한 프레임워크가 있습니다. Ohnishi 등 (2014)의 부록에서 가져온 clusterResampling 함수가 이 접근 방식을 구현합니다:\nclusterResampling = function(x, ngenes = 50, k = 2, B = 250,\n                             prob = 0.67) {\n  mat = Biobase::exprs(x)\n  ce = cl_ensemble(list = lapply(seq_len(B), function(b) {\n    selSamps = sample(ncol(mat), size = round(prob * ncol(mat)),\n                      replace = FALSE)\n    submat = mat[, selSamps, drop = FALSE]\n    sel = order(rowVars(submat), decreasing = TRUE)[seq_len(ngenes)]\n    submat = submat[sel,, drop = FALSE]\n    pamres = pam(t(submat), k = k)\n    pred = cl_predict(pamres, t(mat[sel, ]), \"memberships\")\n    as.cl_partition(pred)\n  }))\n  cons = cl_consensus(ce)\n  ag = sapply(ce, cl_agreement, y = cons)\n  list(agreements = ag, consensus = cons)\n}\nclusterResampling 함수는 다음 단계들을 수행합니다:\n\n복원 추출 없이 샘플의 67%를 선택하여 데이터(모든 E3.25 샘플 또는 모든 E3.5 샘플)의 무작위 하위 집합을 추출합니다.\n(하위 집합에서) 전체 분산 기준 상위 ngenes개의 특징을 선택합니다.\n(k)-평균 군집화를 적용하고, clue 패키지의 cl_predict 메서드를 사용하여 하위 집합에 포함되지 않았던 샘플들이 어느 클러스터 중심과 가장 가까운지에 따라 그들의 클러스터 멤버십을 예측합니다.\n1-3단계를 B번 반복합니다.\n합의 군집화(consensus clustering, cl_consensus)를 적용합니다.\nB개의 군집화 결과 각각에 대해 cl_agreement 함수를 통해 합의 결과와의 일치도를 측정합니다. 여기서 일치도가 높으면 1에 가까운 값을, 낮으면 더 작은 값을 갖습니다. 일치도가 전반적으로 높다면 (k)개 클래스로의 군집화는 안정적이고 재현 가능한 것으로 간주될 수 있습니다. 반대로 낮다면 샘플들을 (k)개 클러스터로 나누는 안정적인 분할이 뚜렷하지 않음을 의미합니다.\n\n합의 군집화를 위한 클러스터 간 거리 척도로 멤버십의 유클리드 비유사성이 사용됩니다. 즉, ()와 ()의 모든 열 순열 사이의 최소 제곱 차이합의 제곱근입니다(여기서 ()와 ()는 클러스터 멤버십 행렬임). 일치도 측정값으로는 (1 - d/m) 수치가 사용되는데, 여기서 (d)는 유클리드 비유사성이고 (m)은 최대 유클리드 비유사성의 상한선입니다.\niswt = (x$genotype == \"WT\")\ncr1 = clusterResampling(x[, x$Embryonic.day == \"E3.25\" & iswt])\ncr2 = clusterResampling(x[, x$Embryonic.day == \"E3.5\"  & iswt])\n결과는 그림 5.30에 나와 있습니다. 이들은 E3.5 데이터가 두 개의 클러스터로 나뉜다는 가설을 확인해 줍니다.\nag1 = tibble(agreements = cr1$agreements, day = \"E3.25\")\nag2 = tibble(agreements = cr2$agreements, day = \"E3.5\")\np1 &lt;- ggplot(bind_rows(ag1, ag2), aes(x = day, y = agreements)) +\n  geom_boxplot() +\n  ggbeeswarm::geom_beeswarm(cex = 1.5, col = \"#0000ff40\")\nmem1 = tibble(y = sort(cl_membership(cr1$consensus)[, 1]),\n              x = seq(along = y), day = \"E3.25\")\nmem2 = tibble(y = sort(cl_membership(cr2$consensus)[, 1]),\n              x = seq(along = y), day = \"E3.5\")\np2 &lt;- ggplot(bind_rows(mem1, mem2), aes(x = x, y = y, col = day)) +\n  geom_point() + facet_grid(~ day, scales = \"free_x\")\ngridExtra::grid.arrange(p1, p2, widths = c(2.4,4.0))\n\n그림 5.30: E3.25 및 E3.5 샘플을 이용한 클러스터 안정성 분석. 왼쪽: B개의 군집화 결과와 합의 결과 사이의 일치도에 대한 beeswarm 플롯; (1)은 완벽한 일치를 나타내며, 낮은 값은 낮은 일치도를 나타냅니다. 오른쪽: 합의 군집화의 멤버십 확률. E3.25의 경우 확률이 분산되어 있어 개별 군집화 결과들이 자주 일치하지 않음을 나타내는 반면, E3.5의 경우 분포가 이봉형(bimodal)이며 모호한 샘플은 하나뿐입니다.\n\n7.6.2.1 계산 및 메모리 문제\n\n\n\n계산 복잡도. 어떤 알고리즘이 O(n^k)라고 불리는 것은, n이 커짐에 따라 자원 소비(CPU 시간 또는 메모리)가 n^k에 비례하여 증가함을 의미합니다. 다른(때로는 상당한) 기본 비용이나 n의 더 낮은 거듭제곱에 비례하여 증가하는 비용이 있을 수 있지만, n이 무한대로 감에 따라 이들은 항상 지배적인 항에 비해 무시할 수 있는 수준이 됩니다.\n\n\n계산 복잡도(Computational complexity). 어떤 알고리즘이 (O(n^k))라고 불리는 것은, (n)이 커짐에 따라 자원 소비(CPU 시간 또는 메모리)가 (n^k)에 비례하여 증가함을 의미합니다. 다른(때로는 상당한) 기본 비용이나 (n)의 더 낮은 거듭제곱에 비례하여 증가하는 비용이 있을 수 있지만, (n)임에 따라 이들은 항상 지배적인 항에 비해 무시할 수 있는 수준이 됩니다.\n(n)개 객체의 전체 대 전체 거리 계산이 (시간과 메모리 측면에서) (O(n^2)) 작업임을 기억하는 것이 중요합니다. 전통적인 계층적 군집화 접근 방식(stats 패키지의 hclust 등)은 시간 측면에서 심지어 (O(n^3))입니다. (n)이 크다면 이는 비실용적일 수 있습니다8. 우리는 전체 대 전체 거리 행렬의 완전한 계산을 피할 수 있습니다. 예를 들어, (k)-평균은 각 객체와 클러스터 중심 사이의 거리만 추적하면 되므로 (O(n))의 계산만 필요하다는 장점이 있으며, 클러스터 중심의 수는 (n)이 증가하더라도 일정하게 유지됩니다.\n8 예를 들어 백만 개 객체에 대한 거리 행렬을 8바이트 부동 소수점으로 저장하면 약 4테라바이트를 차지하며, hclust와 같은 알고리즘은 각 반복이 1나노초만 걸린다는 낙관적인 가정 하에서도 30년 동안 실행될 것입니다.\nfastclust (Müllner 2013) 및 dbscan 과 같은 빠른 구현체들은 많은 수의 관측치를 처리하기 위해 신중하게 최적화되었습니다.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>군집화 (Clustering)</span>"
    ]
  },
  {
    "objectID": "05-chap.html#노이즈-제거-수단으로서의-군집화",
    "href": "05-chap.html#노이즈-제거-수단으로서의-군집화",
    "title": "7  군집화 (Clustering)",
    "section": "7.7 5.8 노이즈 제거 수단으로서의 군집화",
    "text": "7.7 5.8 노이즈 제거 수단으로서의 군집화\n어떤 기저의 실제 값(예를 들어 게놈의 DNA 서열로 표현되는 종)을 반영하지만 기술적 노이즈에 의해 변질된 측정값 세트를 생각해 봅시다. 군집화는 이러한 노이즈를 제거하는 데 사용될 수 있습니다.\n\n7.7.1 5.8.1 서로 다른 베이스라인 빈도를 가진 노이즈 섞인 관측치\n동일한 오차 분산으로 만들어진 관측치들의 이변량 분포(bivariate distribution)가 있다고 가정해 봅시다. 그러나 샘플링은 베이스라인 빈도가 매우 다른 두 그룹으로부터 이루어집니다. 더 나아가, 오차는 연속적이고 독립적인 이변량 정규 분포를 따른다고 가정합시다. 다음 코드에서 생성된 것처럼 우리는 seq1을 (10^{3})개, seq2를 (10^{5})개 가지고 있습니다:\nlibrary(\"mixtools\")\nseq1 = rmvnorm(n = 1e3, mu = -c(1, 1), sigma = 0.5 * diag(c(1, 1)))\nseq2 = rmvnorm(n = 1e5, mu =  c(1, 1), sigma = 0.5 * diag(c(1, 1)))\ntwogr = data.frame(\n  rbind(seq1, seq2),\n  seq = factor(c(rep(1, nrow(seq1)),\n                 rep(2, nrow(seq2))))\n)\ncolnames(twogr)[1:2] = c(\"x\", \"y\")\nlibrary(\"ggplot2\")\nggplot(twogr, aes(x = x, y = y, colour = seq,fill = seq)) +\n  geom_hex(alpha = 0.5, bins = 50) + coord_fixed()\n\n그림 5.31: 두 그룹 모두 동일한 분산의 노이즈 분포를 가지고 있음에도 불구하고, 그룹의 겉보기 반지름은 매우 다릅니다. seq2의 (10^{5})개 인스턴스는 (10^{3})개뿐인 seq1보다 오차가 발생할 기회가 훨씬 더 많습니다. 따라서 우리는 데이터를 군집화할 때 빈도가 중요하다는 것을 알 수 있습니다.\n관측된 값들은 그림 5.31과 같이 보일 것입니다.\n질문 5.16\n데이터 seq1과 seq2를 가져와서 그룹 중심으로부터의 거리에 따라 두 그룹으로 군집화해 보세요. 결과가 두 서열 유형 각각의 빈도에 의존해야 한다고 생각하시나요?\n해결책\n분류학적 군집화(taxonomic clustering), 즉 OTU(operational taxonomic unit) 군집화(Caporaso et al. 2010; P. D. Schloss et al. 2009) 방법들에서 종종 사용되는 이러한 접근 방식은 최적이 아닙니다.\n오직 유사성에만 기반한 방법들은 대표성(representativeness) 휴리스틱에 내재된 편향으로 인해 고통받습니다. 군집화 및 분류학적 할당에서 대표성과 거리 기반 휴리스틱만을 사용하려는 우리의 자연스러운 경향이 어떻게 편향된 결과로 이어질 수 있는지 설명하는 데 도움이 되는 인지 심리학의 세계로 잠시 외도해 봅시다.\n1970년대에 Tversky와 Kahneman (1975)은 우리가 일반적으로 가장 유사한 대표자(representatives) 를 살펴봄으로써 그룹을 할당한다고 지적했습니다. 군집화와 그룹 할당에서 이는 새로운 서열을 그 중심까지의 거리에 따라 그룹에 할당하는 것을 의미할 것입니다. 사실 이는 서로 다른 그룹의 보급률(prevalence) 차이를 무시하고 동일한 반지름을 가진 공을 취하는 것과 같습니다. 이러한 심리학적 오류는 많은 다양한 휴리스틱과 편향을 다루는 중요한 Science 논문(Tversky and Kahneman 1974)에서 처음 논의되었습니다.\n\n\n\n우리의 자연스러운 휴리스틱과 우리가 확률 계산을 할 때 그것들이 우리를 오도할 수 있는 방식에 대한 단행본 규모의 논의는 Kahneman (2011)을 참조하십시오(특히 14장과 15장을 추천합니다).\n\n\n우리의 자연스러운 휴리스틱과 우리가 확률 계산을 할 때 그것들이 우리를 오도할 수 있는 방식에 대한 단행본 규모의 논의는 Kahneman (2011)을 참조하십시오 (특히 14장과 15장을 추천합니다).\n태스크\n서열 길이 len=200인 n=2000개의 이진 변수를 시뮬레이션하여, n개 시퀀싱 리드들의 품질을 나타내도록 합니다. 단순함을 위해, 시퀀싱 오차가 확률 perr=0.001로 독립적이고 균일하게 발생한다고 가정합시다. 즉, 우리는 오직 베이스 콜링이 올바르게 되었는지(TRUE) 아니면 아닌지(FALSE)에만 관심이 있습니다.\nn    = 2000\nlen  = 200\nperr = 0.001\nseqs = matrix(runif(n * len) &gt;= perr, nrow = n, ncol = len)\n이제, 리드들 사이의 모든 쌍별 거리를 계산합니다.\ndists = as.matrix(dist(seqs, method = \"manhattan\"))\n서열 수 k (2부터 n까지)의 다양한 값들에 대해, 이 서열 세트의 최대 거리가 아래 코드에 의해 계산되어 그림 5.32에 표시됩니다.\nlibrary(\"tibble\")\ndfseqs = tibble(\n  k = 10 ^ seq(log10(2), log10(n), length.out = 20),\n  diameter = vapply(k, function(i) {\n    s = sample(n, i)\n    max(dists[s, s])\n    }, numeric(1)))\nggplot(dfseqs, aes(x = k, y = diameter)) + geom_point()+geom_smooth()\n\n그림 5.32: 서열 수의 함수로서의 서열 세트의 지름.\n우리는 이제 오차 확률을 통합하는 노이즈 제거 메커니즘을 사용하여 16S rRNA 리드 군집화를 개선할 것입니다.\n\n\n7.7.2 5.8.2 16S rRNA 서열 노이즈 제거\n데이터란 무엇인가? 박테리아의 16S rRNA 유전자에는 분류군 특이적인 소위 가변 영역(variable regions) 이 있습니다. 이들은 분류군(taxon) 9 식별을 가능하게 하는 지문을 제공합니다. 원시 데이터는 PCR로 증폭된 DNA 영역들의 품질 점수가 매겨진 서열들이 들어있는 FASTQ 파일입니다10. 우리는 데이터로부터 확률적 노이즈 모델을 구축하기 위해 반복적으로 교대하는 접근 방식11을 사용합니다. 우리는 이를 데 노보(de novo) 방법이라고 부르는데, 왜냐하면 군집화를 사용하고 클러스터 중심을 노이즈가 제거된 서열 변이(Amplicon Sequence Variants, ASVs, (Benjamin J. Callahan, McMurdie, and Holmes 2017) 참조)로 사용하기 때문입니다. 모든 노이즈가 제거된 변이들을 찾은 후, 우리는 서로 다른 샘플들에 걸친 그들의 카운트에 대한 분할표를 만듭니다. 우리는 10장에서 이러한 표들이 어떻게 네트워크와 그래프를 사용하여 기저의 박테리아 커뮤니티의 속성을 추론하는 데 사용될 수 있는지 보여줄 것입니다.\n9 calling different groups of bacteria taxa rather than species highlights the approximate nature of the concept, as the notion of species is more fluid in bacteria than, say, in animals.\n10 FASTQ 형식은 여기서 설명됩니다.\n11 4장에서 보았던 EM 알고리즘과 유사합니다.\n데이터 품질을 개선하기 위해, 종종 원시 데이터로부터 시작하여 모든 변동 원인들을 세심하게 모델링해야 합니다. 우리는 이를 처음부터 요리하기(cooking from scratch) 의 한 예로 생각할 수 있습니다 (Ben J. Callahan et al. (2016)의 자세한 내용과 연습 문제 5.5 참조).\n질문 5.17\n우리 샘플에 매우 다른 풍부도로 존재하는 길이 200인 두 서열(seq1과 seq2)이 있다고 가정해 봅시다. 기술적 시퀀싱 오차는 각 뉴클레오타이드에 대해 독립적인 Bernoulli(0.0005) 무작위 사건으로 발생한다고 들었습니다. 서열당 오차 수의 분포는 무엇일까요?\n해결책\n확률 이론은 200개의 독립적인 Poisson(0.0005)의 합이 Poisson(0.1)이 될 것임을 알려줍니다.\n우리는 또한 몬테카를로 시뮬레이션으로 이를 확인할 수 있습니다:\nsimseq10K = replicate(1e5, sum(rpois(200, 0.0005)))\nmean(simseq10K)\n\n\n[1] 0.10143\n\n\nvcd::distplot(simseq10K, \"poisson\")\n\n그림 5.33: simseq10K 데이터에 대한 distplot.\n그림 5.33은 분포가 푸아송 분포에 얼마나 가까운지 보여줍니다.\n\n\n7.7.3 5.8.3 서열 변이 추론\nDADA 방법(Divisive Amplicon Denoising Algorithm, Rosen et al. (2012))은 시퀀싱 오차와 실제 생물학적 변이를 구분하는 치환 오차의 매개변수화된 모델을 사용합니다. 이 모델은 ({ t C}) 대신 ({ t A})를 보게 될 확률과 같은 염기 치환 확률을 계산합니다. 이 확률들이 서열을 따른 위치와 독립적이라고 가정합니다. 오차율은 시퀀싱 실행(runs)과 PCR 프로토콜에 따라 상당히 다르기 때문에, 모델 매개변수들은 EM 유형의 접근 방식을 사용하여 데이터 자체로부터 추정됩니다. 리드는 현재 매개변수가 주어졌을 때 노이즈가 있거나 정확한 것으로 분류되며, 그에 따라 노이즈 모델 매개변수들이 업데이트됩니다12.\n12 대규모 데이터 세트의 경우, 노이즈 모델 추정 단계가 전체 세트에 대해 수행될 필요는 없습니다. 대규모 데이터 세트를 다룰 때의 팁과 도구에 대해서는 https://benjjneb.github.io/dada2/bigdata.html을 참조하세요.\n13 F는 순방향 가닥(forward strand), R은 역방향 가닥(reverse strand)을 나타냅니다.\n역중복(dereplicated) 서열들13 이 읽혀지고, 그 다음 다음 코드에서와 같이 dada 함수를 사용하여 분할형 노이즈 제거 및 추정이 실행됩니다:\nderepFs = readRDS(file=\"../data/derepFs.rds\")\nderepRs = readRDS(file=\"../data/derepRs.rds\")\nlibrary(\"dada2\")\nddF = dada(derepFs, err = NULL, selfConsist = TRUE)\nddR = dada(derepRs, err = NULL, selfConsist = TRUE)\nIn order to verify that the error transition rates have been reasonably well estimated, we inspect the fit between the observed error rates (black points) and the fitted error rates (black lines) (Figure 5.34).\nplotErrors(ddF)\n\n\nIn chunk 'fig-rerrorprofile1': Warning: Removed 82 rows containing missing values or values outside the scale range(`geom_line()`).\n\n그림 5.34: plotErrors(ddF)에 의해 제공된 순방향 전이 오차율. 이는 품질(quality)의 함수로서 각 유형의 뉴클레오타이드 전이 빈도를 보여줍니다.\n오차가 추정되고 나면, 서열 변이들을 찾기 위해 데이터에 대해 알고리즘이 다시 실행됩니다:\ndadaFs = dada(derepFs, err=ddF[[1]]$err_out, pool = TRUE)\ndadaRs = dada(derepRs, err=ddR[[1]]$err_out, pool = TRUE)\n참고: 서열 추론 함수는 두 가지 다른 모드로 실행될 수 있습니다: 샘플별 독립적 추론(pool = FALSE)과, 모든 샘플로부터 결합된 시퀀싱 리드들로부터의 풀링된 추론입니다. 독립적 추론은 두 가지 장점이 있습니다: 샘플 수의 함수로서 계산 시간이 선형적이며 메모리 요구 사항이 일정합니다. 풀링된 추론은 계산적으로 더 부담스럽지만, 개별 샘플에서는 한두 번만 발생하지만 모든 샘플 전체에서는 더 자주 발생하는 희귀 변이의 탐지를 개선할 수 있습니다. 이 데이터 세트는 특별히 크지 않으므로, 우리는 풀링된 추론을 수행했습니다.\n서열 추론은 데이터로부터 거의 모든 치환 및 인델(indel) 14 오차를 제거합니다. 우리는 추론된 순방향 및 역방향 서열들을 병합하며, 잔류 오차에 대한 최종 제어로서 완벽하게 겹치지 않는 쌍체 서열들을 제거합니다.\n14 인델(indel)이라는 용어는 삽입-삭제(insertion-deletion)를 나타냅니다; 짧은 문자열만큼 차이가 나는 두 서열을 비교할 때, 이것이 삽입인지 삭제인지는 관점의 문제이므로 이러한 이름이 붙었습니다.\nmergers = mergePairs(dadaFs, derepFs, dadaRs, derepRs)\n우리는 ASV 카운트의 분할표를 생성합니다. 이것은 “OTU 15 표”의 고해상도 아날로그입니다. 즉, 각 샘플에서 각 서열 변이가 관찰된 횟수를 셀에 포함하는 샘플별 특징 표입니다.\n15 운영상 분류 단위 (operational taxonomic units)\nseqtab.all = makeSequenceTable(mergers[!grepl(\"Mock\",names(mergers))])\n질문 5.18\ndadaRs와 mergers 객체의 구성 요소들을 살펴보세요.\n해결책\ndadaRs는 길이가 20인 리스트입니다. 그 요소들은 노이즈가 제거된 리드들을 포함하는 dada 클래스의 객체들입니다. 우리는 10장에서 서열을 정렬하고, 그들의 분류학적 정보를 할당하며, 하류 분석을 위해 샘플 정보와 결합하는 방법을 보게 될 것입니다.\n[1] 20\n\n\n[1] 20\n\n\n[1] \"list\"\n\n\n [1] \"F3D0\"   \"F3D1\"   \"F3D141\" \"F3D142\" \"F3D143\" \"F3D144\" \"F3D145\" \"F3D146\"\n [9] \"F3D147\" \"F3D148\" \"F3D149\" \"F3D150\" \"F3D2\"   \"F3D3\"   \"F3D5\"   \"F3D6\"  \n[17] \"F3D7\"   \"F3D8\"   \"F3D9\"   \"Mock\"  \n\n\n[1] \"list\"\n\n\n[1] 20\n키메라(Chimera)는 PCR 증폭 과정에서 두 개(드문 경우 그 이상)의 원래 서열이 융합되어 인위적으로 생성된 서열입니다. 노이즈 제거 워크플로를 완료하기 위해, 우리는 removeBimeraDenovo 함수를 호출하여 이들을 제거하고, 나중에 사용할 깨끗한 분할표를 남깁니다.\nseqtab = removeBimeraDenovo(seqtab.all)\n질문 5.19\n키메라가 왜 비교적 식별하기 쉽다고 생각하시나요?\nseqtab.all 데이터에서 리드들 중 키메라의 비율은 얼마였나요?\n고유한 서열 변이들 중 키메라의 비율은 얼마인가요?\n해결책\n여기서 우리는 일부 서열 변이들이 키메라인 것을 관찰했지만, 이들은 전체 리드의 7%만을 차지합니다.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>군집화 (Clustering)</span>"
    ]
  },
  {
    "objectID": "05-chap.html#이-장의-요약",
    "href": "05-chap.html#이-장의-요약",
    "title": "7  군집화 (Clustering)",
    "section": "7.8 5.9 이 장의 요약",
    "text": "7.8 5.9 이 장의 요약\n유유상종: 관측치를 비교하는 방법 우리는 장의 시작 부분에서 올바른 거리 를 찾는 것이 군집 분석의 필수적인 첫 단계임을 보았습니다; 이는 ’쓰레기를 넣으면 쓰레기가 나온다(garbage in, garbage out)’는 격언이 완전히 적용되는 사례입니다. 항상 과학적으로 의미 있는 거리를 선택하고 가능한 한 많은 거리로부터 얻은 출력들을 비교하세요; 때로는 동일한 데이터라도 서로 다른 과학적 목표가 추구될 때는 서로 다른 거리가 필요할 수 있습니다.\n군집화의 두 가지 방법 우리는 군집화에 두 가지 접근 방식이 있음을 보았습니다:\n\n클러스터 중심을 추정하고 점들을 그에 할당하는 것을 번갈아 가며 수행하는 (k)-평균 및 (k)-메도이드(PAM)와 같은 반복적 분할 접근 방식;\n먼저 점들을 응집시키고, 이어서 커지는 클러스터들을 계층적 군집 트리 로 표현될 수 있는 중첩된 세트 시퀀스로 응집시키는 계층적 군집화 접근 방식.\n\n생물학적 예시 군집화는 특히 면역학 및 단일 세포 데이터 분석에서 단일 세포 측정값의 잠재 클래스를 찾는 데 중요한 도구입니다. 우리는 희소성(sparsity)이 문제가 되지 않는 저차원 데이터에 대해 밀도 기반 군집화가 어떻게 유용한지 보았습니다.\n검증 군집화 알고리즘은 항상 클러스터를 내놓으므로, 우리는 그 품질을 평가하고 클러스터 수를 신중하게 선택해야 합니다. 그러한 검증 단계는 시각화 도구를 사용하고 데이터의 많은 재표본에 대해 군집화를 반복함으로써 수행됩니다. 우리는 WSS/BSS나 (())와 같은 통계량들이 그룹 구조를 이해하고 있는 데이터에 대한 시뮬레이션에서 어떻게 보정될 수 있는지, 그리고 새로운 데이터에 대해 클러스터 수를 선택하기 위한 유용한 벤치마크를 어떻게 제공할 수 있는지 보았습니다. 물론, 클러스터의 의미를 알리고 확인하기 위해 생물학적으로 관련 있는 정보를 사용하는 것이 항상 최선의 검증 접근 방식입니다.\n일반적으로 군집화 결과를 비교할 실측 자료(ground truth)는 거의 없습니다. “모든 모델은 틀렸지만, 일부는 유용하다”는 오래된 격언이 여기에도 적용됩니다. 좋은 군집화란 유용한 것으로 판명되는 군집화입니다.\n거리와 확률 마지막으로: 거리가 전부는 아닙니다. 우리는 군집화할 때 베이스라인 빈도와 국소 밀도를 고려하는 것이 얼마나 중요한지 보여주었습니다. 이는 실제 클래스나 분류군 그룹이 매우 다른 빈도로 발생하는 16S rRNA 서열 리드의 노이즈 제거를 위한 군집화와 같은 사례에서 필수적입니다.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>군집화 (Clustering)</span>"
    ]
  },
  {
    "objectID": "05-chap.html#더-읽을거리",
    "href": "05-chap.html#더-읽을거리",
    "title": "7  군집화 (Clustering)",
    "section": "7.9 5.10 더 읽을거리",
    "text": "7.9 5.10 더 읽을거리\nFinding groups in data 에 관한 완전한 교재로는 Kaufman과 Rousseeuw (2009)를 참조하십시오. clusterExperiment 패키지의 비네트는 7장에서 다룰 예비 차원 축소(PCA)를 포함하여, 여러 다양한 기법들을 사용하여 클러스터를 생성하는 전체 워크플로를 담고 있습니다. 인접한 생물학적 정보가 없는 상태에서 필요한 클러스터 수를 결정하는 방법에 대해서는 일치된 의견이 없습니다. 하지만 강한 형태(strong forms) 의 계층적 클러스터를 만드는 것은 사용자로 하여금 계층적 트리를 어디까지 자를지 결정하게 하고 이러한 내부 분기가 짧은 곳에서 자르지 않도록 주의하게 해준다는 장점이 있는 방법입니다. 단일 세포 RNA 실험 데이터에의 적용 사례는 clusterExperiment 의 비네트를 참조하십시오.\nHiiragi 데이터를 분석하면서 우리는 클러스터 확률을 사용했는데, 이는 이미 4장에서 언급된 개념으로, EM 알고리즘이 이를 기댓값 통계량을 계산하기 위한 가중치로 사용했습니다. 확률적 군집화 개념은 베이지안 비모수적 혼합물 프레임워크에서 잘 발달되어 있으며, 이는 4장에서 다룬 혼합 모델을 더 일반적인 설정으로 풍부하게 만듭니다. 유세포 분석을 위해 이 프레임워크를 사용한 실제 사례는 Dundar 등 (2014)을 참조하십시오. 고처리량 시퀀싱 리드의 노이즈 제거 및 특정 박테리아나 바이러스 균주로의 할당에 있어 군집화는 필수적입니다. 노이즈가 존재하는 상황에서, 크기가 매우 불균등한 실제 균주 그룹으로 군집화하는 것은 도전적인 일일 수 있습니다. 데이터를 사용하여 노이즈 모델을 만드는 것은 노이즈 제거와 클러스터 할당을 동시에 가능하게 합니다. Rosen 등 (2012)이나 Benjamin J. Callahan 등 (2016)에 의한 것과 같은 노이즈 제거 알고리즘들은 EM 방법으로부터 영감을 얻은 반복적 워크플로를 사용합니다 (McLachlan and Krishnan 2007).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>군집화 (Clustering)</span>"
    ]
  },
  {
    "objectID": "05-chap.html#연습-문제",
    "href": "05-chap.html#연습-문제",
    "title": "7  군집화 (Clustering)",
    "section": "7.10 5.11 연습 문제",
    "text": "7.10 5.11 연습 문제\n연습 문제 5.1\nWe can define the average dissimilarity of a point (x_i) to a cluster (C_k) as the average of the distances from (x_i) to all points in (C_k). Let (A(i)) be the average dissimilarity of all points in the cluster that (x_i) belongs to. Let (B(i)) be the lowest average dissimilarity of (x_i) to any other cluster of which (x_i) is not a member. The cluster with this lowest average dissimilarity is said to be the neighboring cluster of (x_i), because it is the next best fit cluster for point (x_i). The silhouette index is\n[ S(i)=. ]\nCompute the silhouette index for the simdat data we simulated in Section 5.7.\nlibrary(\"cluster\")\npam4 = pam(simdatxy, 4)\nsil = silhouette(pam4, 4)\nplot(sil, col=c(\"red\",\"green\",\"blue\",\"purple\"), main=\"Silhouette\")\nChange the number of clusters (k) and assess which (k) gives the best silhouette index.\nNow, repeat this for groups that have uniform (unclustered) data distributions over a whole range of values.\n연습 문제 5.2\nMake a “character” representation of the distance between the 20 locations in the dune data from the vegan package using the function symnum.\nMake a heatmap plot of these distances.\n연습 문제 5.3\nLoad the spirals data from the kernlab package. Plot the results of using (k)-means on the data. This should give you something similar to Figure 5.35.\n\n\n\n\n\n\n\n\nFigure 5.35: An example of non-convex clusters. In (a), we show the result of (k)-means clustering with (k=2). In (b), we have the output from dbscan. The colors represent the three clusters found by the algorithm for the settings .\nYou’ll notice that the clustering in Figure 5.35 seems unsatisfactory. Show how a different method, such as specc or dbscan, could cluster spirals data in a more useful manner.\nRepeat the dbscan clustering with different parameters. How robust is the number of groups?\n연습 문제 5.4\nLooking at graphical representations in simple two-dimensional maps can often reveal important clumping patterns. We saw an example for this with the map that enabled Snow to discover the source of the London cholera outbreak. Such clusterings can often indicate important information about hidden variables acting on the observations. Look at a map for breast cancer incidence in the US at: http://www.huffingtonpost.com/bill-davenhall/post_1663_b_817254.html (Mandal et al. 2009); the areas of high incidence seem spatially clustered. Can you guess the reason(s) for this clustering and high incidence rates on the West and East coasts and around Chicago?\n연습 문제 5.5\nAmplicon bioinformatics: from raw reads to dereplicated sequences. As a supplementary exercise, we provide the intermediate steps necessary to a full data preprocessing workflow for denoising 16S rRNA sequences. We start by setting the directories and loading the downloaded data:\nbase_dir = \"../data\"\nmiseq_path = file.path(base_dir, \"MiSeq_SOP\")\nfilt_path = file.path(miseq_path, \"filtered\")\nfnFs = sort(list.files(miseq_path, pattern=\"_R1_001.fastq\"))\nfnRs = sort(list.files(miseq_path, pattern=\"_R2_001.fastq\"))\nsampleNames = sapply(strsplit(fnFs, \"_\"), `[`, 1)\nif (!file_test(\"-d\", filt_path)) dir.create(filt_path)\nfiltFs = file.path(filt_path, paste0(sampleNames, \"_F_filt.fastq.gz\"))\nfiltRs = file.path(filt_path, paste0(sampleNames, \"_R_filt.fastq.gz\")\nfnFs = file.path(miseq_path, fnFs)\nfnRs = file.path(miseq_path, fnRs)\nprint(length(fnFs))\n\n\n[1] 20\nThe data are highly-overlapping Illumina Miseq (2) amplicon sequences from the V4 region of the 16S rRNA gene (Kozich et al. 2013). There were originally 360 fecal samples collected longitudinally from 12 mice over the first year of life. These were collected by P. D. Schloss et al. (2012) to investigate the development and stabilization of the murine microbiome. We have selected 20 samples to illustrate how to preprocess the data.\nWe will need to filter out low-quality reads and trim them to a consistent length. While generally recommended filtering and trimming parameters serve as a starting point, no two datasets are identical and therefore it is always worth inspecting the quality of the data before proceeding. We show the sequence quality plots for the two first samples in Figure 5.36. They are generated by:\nplotQualityProfile(fnFs[1:2]) + ggtitle(\"Forward\")\nplotQualityProfile(fnRs[1:2]) + ggtitle(\"Reverse\")\n\n\n\n\n\n\n\n\nFigure 5.36: Quality scores. The lines show positional summary statistics: green is the mean, orange is the median, and the dashed orange lines are the 25th and 75th quantiles.\nNote that we also see the background distribution of quality scores at each position in Figure 5.36 as a grey-scale heat map. The dark colors correspond to higher frequency.\n연습 문제 5.6\nGenerate similar plots for four randomly selected sets of forward and reverse reads. Compare forward and reverse read qualities; what do you notice?\n해결책\nii = sample(length(fnFs), 4)\nplotQualityProfile(fnFs[ii]) + ggtitle(\"Forward\")\n\n\n plotQualityProfile(fnRs[ii]) + ggtitle(\"Reverse\")\n연습 문제 5.7\nHere, the forward reads maintain high quality throughout, while the quality of the reverse reads drops significantly at about position 160. Therefore, we truncate the forward reads at position 240, and trimm the first 10 nucleotides as these positions are of lower quality. The reverse reads are trimmed at position 160. Combine these trimming parameters with standard filtering parameters remember to enforce a maximum of 2 expected errors per-read. (Hint: Trim and filter on paired reads jointly, i.e., both reads must pass the filter for the pair to pass. The input arguments should be chosen following the dada2 vignette carefully. We recommend filtering out all reads with any ambiguous nucleotides.)\n해결책\nMost Illumina sequencing data show a trend of decreasing quality towards the end of the reads.\nout = filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),\n        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,  trimLeft=10,\n        compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE\nhead(out)\n\n\n                              reads.in reads.out\nF3D0_S188_L001_R1_001.fastq       7793      7139\nF3D1_S189_L001_R1_001.fastq       5869      5314\nF3D141_S207_L001_R1_001.fastq     5958      5478\nF3D142_S208_L001_R1_001.fastq     3183      2926\nF3D143_S209_L001_R1_001.fastq     3178      2955\nF3D144_S210_L001_R1_001.fastq     4827      4323\nThe maxN parameter omits all reads with more than maxN = 0 ambiguous nucleotides and maxEE at 2 excludes reads with more than 2 expected errors.\nThe sequence data was imported into R from demultiplexed fastq files (i.e. one fastq for each sample) and simultaneously dereplicated to remove redundancy. Name the resulting objects by their sample provenance; they will have derep as their class.\nderepFs = derepFastq(filtFs, verbose = FALSE)\nderepRs = derepFastq(filtRs, verbose = FALSE)\nnames(derepFs) = sampleNames\nnames(derepRs) = sampleNames\n연습 문제 5.8\nUse R to create a map like the one shown in Figure 5.2. Hint: go to the website of the British National Archives and download street addresses of hits, use an address resolution service to convert these into geographic coordinates, and display these as points on a map of London.\n해결책\nSee the Gist https://gist.github.com/wolfganghuber/523622aff6a156d26e77a87ccbe7bd0d by Andrzej Oles.\nAure, Miriam Ragle, Valeria Vitelli, Sandra Jernström, Surendra Kumar, Marit Krohn, Eldri U Due, Tonje Husby Haukaas, et al. 2017. “Integrative Clustering Reveals a Novel Split in the Luminal A Subtype of Breast Cancer with Impact on Outcome.” Breast Cancer Research 19 (1): 44.\nBendall, Sean C, Garry P Nolan, Mario Roederer, and Pratip K Chattopadhyay. 2012. “A Deep Profiler’s Guide to Cytometry.” Trends in Immunology 33 (7): 323–32.\nCallahan, Ben J, Kris Sankaran, Julia A Fukuyama, Paul J McMurdie, and Susan P Holmes. 2016. “Bioconductor Workflow for Microbiome Data Analysis: From Raw Reads to Community Analyses.” F1000Research 5.\nCallahan, Benjamin J, Paul J McMurdie, and Susan P Holmes. 2017. “Exact Sequence Variants Should Replace Operational Taxonomic Units in Marker Gene Data Analysis.” ISME Journal , 1–5.\nCallahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy J Johnson, and Susan P Holmes. 2016. “DADA2: High Resolution Sample Inference from Amplicon Data.” Nature Methods , 1–4.\nCaporaso, J. G., J. Kuczynski, J. Stombaugh, K. Bittinger, F. D. Bushman, E. K. Costello, N. Fierer, et al. 2010. “QIIME Allows Analysis of High-Throughput Community Sequencing Data.” Nature Methods 7 (5): 335–36.\nChakerian, John, and Susan Holmes. 2012. “Computational Tools for Evaluating Phylogenetic and Hierarchical Clustering Trees.” Journal of Computational and Graphical Statistics 21 (3): 581–99.\nDiday, Edwin, and M Paula Brito. 1989. “Symbolic Cluster Analysis.” In Conceptual and Numerical Analysis of Data , 45–84. Springer.\nDundar, Murat, Ferit Akova, Halid Z. Yerebakan, and Bartek Rajwa. 2014. “A Non-Parametric Bayesian Model for Joint Cell Clustering and Cluster Matching: Identification of Anomalous Sample Phenotypes with Random Effects.” BMC Bioinformatics 15 (1): 1–15. https://doi.org/10.1186/1471-2105-15-314.\nFreedman, David A. 1991. “Statistical Models and Shoe Leather.” Sociological Methodology 21 (2): 291–313.\nHallett, Robin M, Anna Dvorkin-Gheva, Anita Bane, and John A Hassell. 2012. “A Gene Signature for Predicting Outcome in Patients with Basal-Like Breast Cancer.” Scientific Reports 2.\nHolmes, Susan, Michael He, Tong Xu, and Peter P Lee. 2005. “Memory t Cells Have Gene Expression Patterns Intermediate Between Naive and Effector.” PNAS 102 (15): 5519–23.\nHornik, Kurt. 2005. “A CLUE for CLUster Ensembles.” Journal of Statistical Software 14 (12).\nHulett, Henry R, William A Bonner, Janet Barrett, and Leonard A Herzenberg. 1969. “Cell Sorting: Automated Separation of Mammalian Cells as a Function of Intracellular Fluorescence.” Science 166 (3906): 747–49.\nKahneman, Daniel. 2011. Thinking, Fast and Slow. Macmillan.\nKaufman, Leonard, and Peter J Rousseeuw. 2009. Finding Groups in Data: An Introduction to Cluster Analysis. Vol. 344. John Wiley & Sons.\nKozich, James J, Sarah L Westcott, Nielson T Baxter, Sarah K Highlander, and Patrick D Schloss. 2013. “Development of a Dual-Index Sequencing Strategy and Curation Pipeline for Analyzing Amplicon Sequence Data on the MiSeq Illumina Sequencing Platform.” Applied and Environmental Microbiology 79 (17): 5112–20.\nMandal, Rakesh, Sophie St-Hilaire, John G Kie, and DeWayne Derryberry. 2009. “Spatial Trends of Breast and Prostate Cancers in the United States Between 2000 and 2005.” International Journal of Health Geographics 8 (1): 53.\nMcLachlan, Geoffrey, and Thriyambakam Krishnan. 2007. The EM Algorithm and Extensions. Vol. 382. John Wiley & Sons.\nMüllner, Daniel. 2013. “Fastcluster: Fast Hierarchical, Agglomerative Clustering Routines for r and Python.” Journal of Statistical Software 53 (9): 1–18.\nO’Neill, Kieran, Nima Aghaeepour, Josef Špidlen, and Ryan Brinkman. 2013. “Flow Cytometry Bioinformatics.” PLoS Computational Biology 9 (12): e1003365.\nOhnishi, Y., W. Huber, A. Tsumura, M. Kang, P. Xenopoulos, K. Kurimoto, A. K. Oles, et al. 2014. “Cell-to-Cell Expression Variability Followed by Signal Reinforcement Progressively Segregates Early Mouse Lineages.” Nature Cell Biology 16 (1): 27–37.\nRosen, Michael J, Benjamin J Callahan, Daniel S Fisher, and Susan P Holmes. 2012. “Denoising PCR-Amplified Metagenome Data.” BMC Bioinformatics 13 (1): 283.\nSchloss, P D, S L Westcott, T Ryabin, J R Hall, M Hartmann, E B Hollister, R A Lesniewski, et al. 2009. “Introducing mothur: Open-Source, Platform- Independent, Community-Supported Software for Describing and Comparing Microbial Communities.” Applied and Environmental Microbiology 75 (23): 7537–41.\nSchloss, P. D., A. M. Schuber, J. P. Zackular, K. D. Iverson, Young V. B., and Petrosino J. F. 2012. “Stabilization of the Murine Gut Microbiome Following Weaning.” Gut Microbes 3 (4): 383–93.\nTibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. “Estimating the Number of Clusters in a Data Set via the Gap Statistic.” JRSSB 63 (2): 411–23.\nTseng, George C, and Wing H Wong. 2005. “Tight Clustering: A Resampling-Based Approach for Identifying Stable and Tight Patterns in Data.” Biometrics 61 (1): 10–16.\nTversky, Amos, and Daniel Kahneman. 1974. “Heuristics and Biases: Judgement Under Uncertainty.” Science 185: 1124–30.\n———. 1975. “Judgment Under Uncertainty: Heuristics and Biases.” In Utility, Probability, and Human Decision Making , 141–62. Springer.\n페이지의 R 버전 4.5.1 (2025-06-13)을 사용하여 2025-09-01 01:33에 빌드되었습니다.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>군집화 (Clustering)</span>"
    ]
  },
  {
    "objectID": "06-chap.html",
    "href": "06-chap.html",
    "title": "8  가설 검정 (Hypothesis Testing)",
    "section": "",
    "text": "8.0.1 6.1.1 쏟아지는 데이터 속에서 정보 찾기\n가설 검정(Hypothesis testing)은 과학의 핵심적인 도구 중 하나입니다. 이는 유한한 데이터 표본을 바탕으로 결론을 내리거나 결정을 내리는 방법입니다. 예를 들어, 질병에 대한 새로운 치료법은 대개 임상 시험을 바탕으로 승인됩니다. 임상 시험은 해당 치료법이 다른 가용한 선택지에 비해 더 나은 효능을 보이는지, 그리고 부작용과의 상충 관계(trade-off)가 수용 가능한지를 결정하는 것을 목표로 합니다. 이러한 시험은 비용이 많이 들고 오랜 시간이 걸릴 수 있습니다. 따라서 모집할 수 있는 환자 수는 제한적이며, 우리는 관찰된 제한된 환자 반응 표본을 바탕으로 추론을 해야 합니다. 데이터에는 노이즈가 섞여 있습니다. 환자의 반응은 치료뿐만 아니라 우리가 통제할 수 없는 많은 다른 요인들에 의존하기 때문입니다. 신뢰할 수 있는 결론을 내리기 위해서는 표본 크기가 충분히 커야 합니다. 반면, 소중한 자원이나 시간을 낭비하지 않도록 표본 크기가 너무 커서도 안 됩니다. 예를 들어, 약값을 필요 이상으로 비싸게 만들거나 새로운 약의 혜택을 볼 수 있는 환자들에게 접근 기회를 박탈해서는 안 되기 때문입니다. 가설 검정의 메커니즘은 오늘날 훨씬 더 널리 사용되고 있긴 하지만, 주로 이러한 응용 분야를 염두에 두고 개발되었습니다.\n생물학적 데이터 분석(및 다른 많은 분야1)에서 가설 검정은 수천 또는 수백만 개의 가능한 가설들을 스크리닝하여 후속 연구를 진행할 가치가 있는 가설을 찾는 데 적용됩니다. 예를 들어, 연구자들은 유전적 변이와 표현형 간의 연관성이나, 유전자 발현 수준과 질병 간의 연관성을 스크리닝합니다. 여기서 “가치 있는” 것은 종종 “통계적으로 유의미한” 것으로 해석되지만, 이 두 개념이 분명히 같은 것은 아닙니다. 통계적 유의성은 데이터 기반 의사결정을 통해 흥미로운 것을 찾기 위한 필요조건이긴 하지만, 충분조건은 아니라고 말하는 것이 타당할 것입니다. 어쨌든, 이러한 대규모 연관성 스크리닝은 다중 가설 검정(multiple hypothesis testing)과 밀접하게 관련되어 있습니다.\n1 신용카드 사기 탐지, 이메일 스팸 탐지, (…)\n이 장에서 우리는 다음을 수행할 것입니다:\n그림 6.1: 현대 생물학의 고처리량 데이터는 수백만 개의 가설 검정을 통해 연관성을 스크리닝합니다. (출처: 바이엘)\n만약 통계적 검정(불확실성을 동반한 의사결정)이 단 한 번의 결정을 내릴 때도 어려운 과제처럼 느껴진다면, 마음을 단단히 먹으십시오: 유전체학이나 더 일반적으로 “빅데이터” 분야에서는 이를 한 번이 아니라 수천 번 또는 수백만 번 수행해야 합니다. 2장에서 우리는 에피토프 검출의 예와 단 한 곳이 아닌 여러 위치를 고려할 때의 어려움을 보았습니다. 유사하게, 전유전체 시퀀싱(whole genome sequencing)에서는 손에 든 DNA 시퀀싱 데이터와 참조 서열(또는 다른 시퀀싱 데이터 세트) 간의 차이에 대한 증거를 찾기 위해 게놈의 모든 위치를 스캔합니다. 인간 데이터를 보고 있다면 이는 약 60억 번의 검정에 해당합니다! 유전자 또는 화학 화합물 스크리닝에서는 대조군과 비교하여 각 시약이 어세이(assay)에서 효과를 나타내는지 테스트하는데, 이 역시 수만 번에서 수백만 번의 검정이 이루어집니다. 8장에서 우리는 측정된 수천 개의 유전자 각각에 대해 가설 검정을 적용하여 RNA-Seq 데이터의 차등 발현을 분석할 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>가설 검정 (Hypothesis Testing)</span>"
    ]
  },
  {
    "objectID": "06-chap.html#예시-동전-던지기",
    "href": "06-chap.html#예시-동전-던지기",
    "title": "8  가설 검정 (Hypothesis Testing)",
    "section": "8.1 6.2 예시: 동전 던지기",
    "text": "8.1 6.2 예시: 동전 던지기\n이제 단일 검정부터 시작하여 가설 검정에 대해 깊이 파고들어 봅시다. 메커니즘을 제대로 이해하기 위해 가장 간단한 예제 중 하나를 사용합니다: 어떤 동전이 공정한지4 확인하기 위해 동전을 던진다고 가정해 봅시다. 동전을 100번 던지고 매번 앞면이 나왔는지 뒷면이 나왔는지 기록합니다. 그러면 다음과 같은 기록을 갖게 될 것입니다:\nHHTTHTHTT…\n이를 R에서 시뮬레이션할 수 있습니다. 편향된 동전을 던진다고 가정하고, probHead를 1/2이 아닌 값으로 설정해 보겠습니다:\nset.seed(0xdada)\nnumFlips = 100\nprobHead = 0.6\ncoinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n  replace = TRUE, prob = c(probHead, 1 - probHead))\nhead(coinFlips)\n\n\n[1] \"T\" \"T\" \"H\" \"T\" \"H\" \"H\"\n이제 동전이 공정하다면 앞면이 절반 정도 나올 것으로 기대할 것입니다. 확인해 봅시다.\ntable(coinFlips)\n\n\ncoinFlips\n H  T \n59 41 \n결과가 50/50과는 다릅니다. 친구에게 동전이 공정한지 알려주지 않고 이 데이터를 보여주었다고 가정해 봅시다. 그 친구의 사전 가정, 즉 귀무 가설은 동전이 대체로 공정하다는 것입니다. 이 데이터가 동전이 공정하지 않다고 결론 내리기에 충분히 강력할까요? 친구는 무작위 표집에 따른 차이가 발생할 수 있다는 것을 알고 있습니다. 이를 결정하기 위해 공정한 동전에 대한5 우리의 검정 통계량 – 100번의 동전 던지기에서 나타난 앞면의 총 횟수 – 의 표집 분포를 살펴봅시다. 1장에서 보았듯이, 동전을 독립적으로 (n)번 던졌을 때 앞면의 횟수 (k)는 다음과 같습니다.\n5 공정하다는 것이 무엇을 의미하는지 아직 제대로 정의하지 않았습니다 – 앞면과 뒷면이 나올 확률이 같고, 각 동전 던지기의 결과가 이전 결과에 의존하지 않는다는 것이 합리적인 정의일 것입니다. 더 복잡한 응용 분야의 경우, 가장 적합한 귀무 가설을 정하는 데는 고민이 필요할 수 있습니다.\n[ P(K=k,|,n, p) = (\n\\[\\begin{array}{c}n\\\\k\\end{array}\\]\n) pk;(1-p){n-k}, ]\n여기서 (p)는 앞면의 확률입니다 (공정한 동전이라고 가정하면 0.5). 우리는 위 방정식의 좌변을 “매개변수 (n)과 (p)가 주어졌을 때, (K)의 관측값이 (k)일 확률”로 읽습니다. 통계학자들은 통계량이 가질 수 있는 모든 가능한 값과 실제로 관측된 값을 구분하는 것을 좋아하며6, 가능한 값들에 대해서는 대문자 (K)를 사용하고 (따라서 (K)는 0에서 100 사이의 무엇이든 될 수 있습니다), 관측된 값에 대해서는 소문자 (k)를 사용합니다.\n6 다시 말해, (K)는 우리 확률 모델의 추상적인 확률 변수인 반면, (k)는 그 실현값, 즉 특정한 데이터 포인트입니다.\n우리는 식 6.3을 그림 6.5에 플롯합니다; 확인을 위해 관측된 값인 numHeads를 파란색 수직선으로 표시합니다.\nlibrary(\"dplyr\")\nk = 0:numFlips\nnumHeads = sum(coinFlips == \"H\")\nbinomDensity = tibble(k = k,\n     p = dbinom(k, size = numFlips, prob = 0.5))\n\n\nlibrary(\"ggplot2\")\nggplot(binomDensity) +\n  geom_bar(aes(x = k, y = p), stat = \"identity\") +\n  geom_vline(xintercept = numHeads, col = \"blue\")\n\n그림 6.5: 식 6.3에 따른 (n=100) 및 (p=0.5) 매개변수의 이항 분포.\n만약 우리가 식 6.3을 몰랐다고 가정해 봅시다. 우리는 여전히 비교를 위해 몬테카를로 시뮬레이션을 사용할 수 있습니다:\nnumSimulations = 10000\noutcome = replicate(numSimulations, {\n  coinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n                     replace = TRUE, prob = c(0.5, 0.5))\n  sum(coinFlips == \"H\")\n})\nggplot(tibble(outcome)) + xlim(-0.5, 100.5) +\n  geom_histogram(aes(x = outcome), binwidth = 1, center = 50) +\n  geom_vline(xintercept = numHeads, col = \"blue\")\n\n그림 6.6: 10,000번의 시뮬레이션을 통한 이항 분포의 근사 (그림 6.5와 동일한 매개변수).\n예상대로, 앞면의 가장 발생 가능성이 높은 횟수는 동전 던지기 횟수의 절반인 50입니다. 하지만 우리는 50 근처의 다른 숫자들 또한 꽤 발생 가능하다는 것을 볼 수 있습니다. 관측된 값 59가 공정한 동전에서 볼 수 있는 법한 값들 중 하나인지, 아니면 그 편차가 우리가 이 동전이 편향되었다고 충분히 자신 있게 결론 내릴 만큼 큰지 어떻게 정량화할까요? 우리는 모든 가능한 (k) 값의 집합(0에서 100)을 두 개의 상보적인 부분 집합인 기각역(rejection region) 과 비기각역으로 나눕니다. 우리의 선택7은 기각역을 가능한 한 많은 (k)로 채우되, 귀무 가설을 가정했을 때 그들의 전체 확률을 어떤 임계값 ()(예를 들어 0.05) 미만으로 유지하는 것입니다.\n7 이에 대해서는 6.3.1절에서 더 다룹니다.\n아래 코드에서, 우리는 dplyr 패키지의 arrange 함수를 사용하여 p-값들을 낮은 것부터 높은 것 순으로 정렬한 다음, 이를 mutate로 전달하여 p-값들의 누적 합(cumsum)을 계산하고 이를 ()와 비교하여 정의된 또 다른 데이터 프레임 열 reject를 추가합니다. 따라서 논리형 벡터 reject는 전체 확률이 ()보다 작은 (k) 값들의 세트를 TRUE로 표시합니다. 이들은 그림 6.7에 표시되어 있으며, 우리의 기각역이 연속적이지 않음을 볼 수 있습니다 – 이는 매우 큰 (k) 값들과 매우 작은 (k) 값들을 모두 포함합니다.\nlibrary(\"dplyr\")\nalpha = 0.05\nbinomDensity = arrange(binomDensity, p) |&gt;\n        mutate(reject = (cumsum(p) &lt;= alpha))\n\nggplot(binomDensity) +\n  geom_bar(aes(x = k, y = p, col = reject), stat = \"identity\") +\n  scale_colour_manual(\n    values = c(`TRUE` = \"red\", `FALSE\" = \"darkgrey\")) +\n  geom_vline(xintercept = numHeads, col = \"blue\") +\n  theme(legend.position = \"none\")\n\n그림 6.7: 그림 6.5와 같지만, 전체 면적이 최대 ()가 되도록 가장 많은 수의 빈(bins)을 포함하도록 선택된 기각역(빨간색)이 표시된 모습.\n확률들에 대해 명시적으로 합계를 구하는 것은 번거로운 일이며, 여기서는 교육적인 가치를 위해 그렇게 했습니다. 1차원 분포의 경우, R은 밀도 함수(dbinom 등)뿐만 아니라 누적 분포 함수(pbinom)도 제공하며, 이들은 확률들에 대해 cumsum을 수행하는 것보다 더 정확하고 빠릅니다. 실제로는 이들을 사용해야 합니다.\n태스크\ndbinom과 cumsum을 사용하지 않고, 대신 pbinom을 사용하여 기각역을 계산하고 그림 6.7과 같은 플롯을 생성해 보세요.\n그림 6.7에서 관측된 값 59가 회색으로 칠해진 영역에 있음을 알 수 있으며, 따라서 우리는 ()의 유의 수준에서 이 데이터로부터 공정한 동전이라는 귀무 가설을 기각하지 않을 것입니다.\n질문 6.1\n귀무 가설을 기각하지 않는다는 사실이 그 동전이 공정하다는 것을 의미하나요?\n질문 6.2\n만약 우리가 더 많은 동전 던지기를 했다면 동전이 공정하지 않다는 것을 발견할 가능성이 더 높았을까요? 얼마나 더 해야 했을까요?\n질문 6.3\n만약 우리가 전체 절차를 반복하여 다시 동전을 100번 던진다면, 그때는 귀무 가설을 기각하게 될 수도 있을까요?\n질문 6.4\n그림 6.7의 기각역은 비대칭적입니다 – 왼쪽 부분은 (k=40)에서 끝나고, 오른쪽 부분은 (k=61)에서 시작합니다. 왜 그럴까요? 기각역을 정의하는 다른 유용한 방식들에는 어떤 것들이 있을까요?\n우리는 방금 이항 검정(binomial test)의 단계들을 거쳤습니다. 사실 이는 R에서 매우 빈번한 작업이므로 단일 함수인 binom.test로 캡슐화되어 있으며, 우리는 그 출력과 우리 결과를 비교할 수 있습니다.\nbinom.test(x = numHeads, n = numFlips, p = 0.5)\n\n\n    Exact binomial test\n\ndata:  numHeads and numFlips\nnumber of successes = 59, number of trials = 100, p-value = 0.08863\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4871442 0.6873800\nsample estimates:\nprobability of success \n                  0.59",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>가설 검정 (Hypothesis Testing)</span>"
    ]
  },
  {
    "objectID": "06-chap.html#가설-검정의-5단계",
    "href": "06-chap.html#가설-검정의-5단계",
    "title": "8  가설 검정 (Hypothesis Testing)",
    "section": "8.2 6.3 가설 검정의 5단계",
    "text": "8.2 6.3 가설 검정의 5단계\n가설 검정의 일반적인 원칙들을 요약해 봅시다:\n\n관심 있는 효과를 결정하고, 적절한 실험이나 연구를 설계하며, 데이터 요약 함수와 검정 통계량(test statistic) 을 선택합니다.\n귀무 가설(null hypothesis) 을 설정합니다. 이는 귀무 가설이 참이라는 가정하에 검정 통계량의 가능한 결과들과 그 확률인 귀무 분포(null distribution) 를 계산할 수 있게 해주는 단순하고 계산 가능한 현실 모델입니다.\n기각역(rejection region) 을 결정합니다. 즉, 전체 확률이 매우 작은8 가능한 결과들의 하위 집합을 선택합니다.\n실험을 수행하고 데이터를 수집합니다9; 검정 통계량을 계산합니다.\n결정을 내립니다: 검정 통계량이 기각역에 있다면 귀무 가설을 기각합니다10.\n\n8 이에 대해서는 6.3.1절을 참조하십시오.\n9 또는 다른 누군가가 이미 수행했다면 그들의 데이터를 다운로드하십시오.\n10 즉, 그것이 참일 가능성이 낮다고 결론 내립니다.\n이 이상적인 워크플로에서 우리는 데이터를 보기 전인 1~3단계에서 모든 중요한 결정을 내린다는 점에 주목하십시오. 서론(그림 1 및 2)에서 이미 언급했듯이, 이는 종종 현실적이지 않습니다. 우리는 6.6절에서 이 문제로 다시 돌아올 것입니다.\n우리가 위의 예에서 사용한 귀무 가설에도 이상화가 있었습니다: 우리는 공정한 동전이 정확히 0.5의 확률을 가져야 하며(예를 들어 0.500001이 아니라), 던지기 사이에 절대적으로 의존성이 없어야 한다고 가정했습니다. 공기 저항, 동전이 떨어지는 재료의 탄성 등 발생 가능한 효과들에 대해서는 걱정하지 않았습니다. 이는 귀무 가설이 이항 분포를 통해 계산 가능하다는 이점을 주었습니다. 여기서 이러한 이상화는 그리 논란의 여지가 없어 보일 수 있지만, 다른 상황에서는 귀무 가설이 얼마나 다루기 쉬운가와 얼마나 현실적인가 사이의 상충 관계(trade-off)가 더 클 수 있습니다. 문제는 귀무 가설이 처음부터 너무 이상화되어 있다면, 그것을 기각하는 것이 그리 흥미롭지 않다는 것입니다. 결과가 오도될 수 있으며, 확실히 우리는 시간을 낭비하고 있는 것입니다.\n우리 예에서의 검정 통계량은 앞면의 총 횟수였습니다. 만약 우리가 뒷면이 연속으로 50번 나오고, 그 다음 앞면이 연속으로 50번 나오는 것을 관찰했다고 가정해 봅시다. 우리의 검정 통계량은 결과의 순서를 무시하므로, 우리는 이것이 완벽하게 공정한 동전이라고 결론 내릴 것입니다. 하지만 만약 우리가 뒷면이 두 번 연속으로 나오는 횟수와 같은 다른 검정 통계량을 사용했다면, 이 동전에 뭔가 이상한 점이 있다는 것을 알아챘을지도 모릅니다.\n질문 6.5\n이 다른 검정 통계량의 귀무 분포는 무엇일까요?\n질문 6.6\n그 통계량에 기반한 검정이 일반적으로 더 선호될까요?\n해결책\n아니요, 동전 던지기 사이의 그러한 상관관계를 감지하는 데는 더 큰 검정력을 갖지만, 결과의 편향(bias)을 감지하는 데는 검정력이 더 낮습니다.\n우리가 방금 한 일은 두 가지 서로 다른 종류의 대립 가설(alternative hypotheses) 을 살펴본 것입니다. 첫 번째 종류의 대립 가설은 연속적인 동전 던지기가 여전히 서로 독립적이지만 앞면의 확률이 0.5가 아니라는 것이었습니다. 두 번째는 앞면의 전체 확률은 여전히 0.5일 수 있지만 연속적인 동전 던지기가 상관관계가 있다는 것이었습니다.\n질문 6.7\n1장의 충분 통계량(sufficient statistics) 개념을 상기해 보세요. 앞면의 총 횟수는 이항 분포에 대한 충분 통계량인가요? 왜 그것이 첫 번째 종류의 대립 가설에 대해서는 좋은 검정 통계량이 될 수 있지만, 두 번째 종류에 대해서는 그렇지 않을까요?\n그러므로 우리는 일반적으로 여러 가지 가능한 검정 통계량 선택지가 있음을 기억합시다 (원칙적으로는 데이터의 어떤 수치적 요약도 될 수 있습니다). 좋은 검정력11을 가진 검정을 얻기 위해서는 올바른 선택을 하는 것이 중요합니다. 올바른 선택이 무엇인지는 우리가 어떤 종류의 대립 가설을 예상하느냐에 달려 있습니다. 이를 미리 아는 것이 항상 쉬운 것은 아닙니다.\n11 1.4.1절 및 6.4절을 참조하십시오.\n12 가정이 정확히 참일 필요는 없습니다 – 이론의 예측이 진실에 대한 수용 가능한 근사치라면 충분합니다.\n일단 검정 통계량을 선택했다면 그것의 귀무 분포를 계산해야 합니다. 이는 종이와 연필로 할 수도 있고 컴퓨터 시뮬레이션으로 할 수도 있습니다. 종이와 연필을 이용한 해결책은 모수적(parametric)이며 (식 6.3과 같은) 닫힌 형태의 수학적 표현으로 이어지는데, 이는 귀무 가설의 모델 매개변수들((n, p) 등)의 범위에 대해 유효하다는 장점이 있습니다. 또한 특정 매개변수 세트에 대해 빠르게 계산될 수 있습니다. 하지만 항상 동전 던지기 예제처럼 쉬운 것은 아닙니다. 때로는 종이와 연필을 이용한 해결책을 계산하는 것이 불가능할 정도로 어려울 때도 있습니다. 다른 경우에는 단순화 가정이 필요할 수 있습니다. 한 예로 (이 장의 뒷부분에서 살펴볼) (t)-통계량에 대한 귀무 분포가 있습니다. 데이터가 독립적이고 정규 분포를 따른다고 가정하면 이를 계산할 수 있는데, 그 결과를 (t)-분포라고 부릅니다. 이러한 모델링 가정은 현실적일 수도 있고 그렇지 않을 수도 있습니다. 귀무 분포를 시뮬레이션하는 것은 잠재적으로 더 정확하고 현실적이며 아마도 더 직관적인 접근 방식을 제공합니다. 시뮬레이션의 단점은 시간이 꽤 오래 걸릴 수 있고, 매개변수 변화가 결과에 어떤 영향을 미치는지 체계적으로 이해하기 위해 추가적인 작업이 필요하다는 점입니다. 일반적으로 모수적 이론이 적용될 때는 이를 사용하는 것이 더 우아합니다12. 의심스러울 때는 시뮬레이션을 하거나, 둘 다 하십시오.\n\n8.2.1 6.3.1 기각역\n여러분의 검정에 적합한 기각역을 어떻게 선택할까요? 먼저, 그 크기는 얼마여야 할까요? 그것은 여러분이 선택한 유의 수준(significance level) 또는 위양성률(false positive rate) ()이며, 이는 귀무 가설이 참이더라도 검정 통계량이 이 영역에 떨어질 전체 확률입니다13.\n13 어떤 시점에 어떤 사람들이 특정 질문 세트에 대해 ()를 “작다”고 공모했습니다. 하지만 이 숫자에는 특별한 것이 없으며, 어떤 경우에도 결정 임계값에 대한 최선의 선택은 문맥에 따라 크게 달라질 수 있습니다 (Wasserstein and Lazar 2016; Altman and Krzywinski 2017).\n크기가 주어졌을 때, 다음 질문은 형태에 관한 것입니다. 주어진 크기에 대해 대개 여러 가지 가능한 형태가 존재합니다. 대립 가설이 참일 때 검정 통계량이 기각역에 떨어질 확률이 가능한 한 커야 한다는 요구 조건은 타당합니다. 다시 말해, 우리는 우리의 검정이 높은 검정력(power) , 즉 진양성률(true positive rate)을 갖기를 원합니다.\n그림 6.7의 기각역을 계산하는 코드에서 사용한 기준은 그 영역이 가능한 한 많은 k를 포함하도록 하는 것이었습니다. 이는 대립 분포에 대한 정보가 전혀 없는 상황에서는 어떤 k도 다른 k만큼이나 좋기 때문에, 그들의 전체 개수를 최대화하는 것입니다.\n이것의 결과로 그림 6.7에서 기각역은 분포의 두 꼬리(tails) 사이로 나뉩니다. 이는 우리가 불공정한 동전이 앞면이나 뒷면 어느 쪽으로든 편향될 수 있다고 예상하기 때문입니다. 만약 우리가 알고 있다면, 예를 들어 편향이 앞면 쪽일 것이라고 생각한다면 기각역을 적절한 쪽(예: 오른쪽 꼬리)에 모두 집중시킬 것입니다. 이러한 선택은 각각 양측(two-sided) 및 단측(one-sided) 검정이라고 불립니다. 보다 일반적으로, 대립 분포에 대한 가정이 있다면 이는 기각역 형태의 선택에 영향을 줄 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>가설 검정 (Hypothesis Testing)</span>"
    ]
  },
  {
    "objectID": "06-chap.html#오류의-유형",
    "href": "06-chap.html#오류의-유형",
    "title": "8  가설 검정 (Hypothesis Testing)",
    "section": "8.3 6.4 오류의 유형",
    "text": "8.3 6.4 오류의 유형\n검정의 메커니즘을 설정했으므로, 우리가 얼마나 잘하고 있는지 평가할 수 있습니다. 표 6.1은 실제 상황(귀무 가설이 실제로 참인지 여부)과 데이터를 본 후 귀무 가설을 기각할지 여부에 대한 우리의 결정을 비교합니다.\n\n\n\n검정 대 실제\n귀무 가설이 참\n…이 거짓\n\n\n\n\n귀무 가설 기각\n제1종 오류 (위양성)\n진양성\n\n\n기각하지 않음\n진음성\n제2종 오류 (위음성)\n\n\n\n표 6.1: 통계적 검정에서의 오류 유형.\n다른 유형의 오류를 증가시키는 대가로 두 가지 오류 유형 중 하나를 줄이는 것은 항상 가능합니다. 진짜 과제는 두 가지 사이에서 수용 가능한 상충 관계를 찾는 것입니다. 이것이 그림 6.2에 예시되어 있습니다. 우리는 임계값을 오른쪽으로 이동시켜 위양성률(false positive rate, FPR) 을 항상 낮출 수 있습니다. 우리는 더 “보수적”이 될 수 있습니다. 하지만 이는 더 높은 위음성률(false negative rate, FNR) 이라는 대가를 치러야 합니다. 유사하게, 임계값을 왼쪽으로 이동시켜 FNR을 낮출 수 있습니다. 하지만 이 역시 더 높은 FPR이라는 대가를 치러야 합니다. 용어에 대해 조금 설명하자면: FPR은 앞서 언급한 확률 ()와 같습니다. (1 - )는 검정의 특이도(specificity) 라고도 불립니다. FNR은 때때로 ()라고도 불리며, (1 - )는 검정의 검정력(power) , 민감도(sensitivity) 또는 진양성률(true positive rate) 이라고 불립니다.\n질문 6.8\n6.3절의 끝에서 단측 및 양측 검정에 대해 배웠습니다. 왜 이러한 구분이 존재할까요? 왜 더 넓은 범위의 대립 가설에 민감한 양측 검정을 항상 사용하지 않을까요?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>가설 검정 (Hypothesis Testing)</span>"
    ]
  },
  {
    "objectID": "06-chap.html#t-검정",
    "href": "06-chap.html#t-검정",
    "title": "8  가설 검정 (Hypothesis Testing)",
    "section": "8.4 6.5 t-검정",
    "text": "8.4 6.5 t-검정\n많은 실험 측정값은 유기수(rational numbers)로 보고되며, 우리가 할 수 있는 가장 간단한 비교는 두 그룹 사이의 비교입니다. 예를 들어, 어떤 물질로 처리된 세포와 그렇지 않은 세포를 비교하는 것입니다. 이러한 상황을 위한 기본적인 검정이 (t)-검정입니다. 검정 통계량은 다음과 같이 정의됩니다:\n[ t = c ; , ]\n여기서 (m_1)과 (m_2)는 두 그룹 값들의 평균이고, (s)는 통합 표준 편차(pooled standard deviation)이며, (c)는 표본 크기, 즉 두 그룹의 관측치 수 (n_1)과 (n_2)에 의존하는 상수입니다. 공식으로는14,\n14 모든 사람이 식 6.4는 기억하려고 노력해야 하지만, 많은 사람들은 필요할 때 식 6.5를 찾아보는 것으로 충분합니다.\n[ \\[\\begin{align} m_g &= \\frac{1}{n_g} \\sum_{i=1}^{n_g} x_{g, i} \\quad\\quad\\quad g=1,2\\\\ s^2 &= \\frac{1}{n_1+n_2-2} \\left( \\sum_{i=1}^{n_1}\n\\left(x_{1,i} - m_1\\right)^2 + \\sum_{j=1}^{n_2} \\left(x_{2,j} - m_2\\right)^2 \\right)\\\\ c &= \\sqrt{\\frac{n_1n_2}{n_1+n_2}} \\end{align}\\] ]\n여기서 (x_{g, i})는 (g)번째 그룹의 (i)번째 데이터 포인트입니다. R의 datasets 패키지에 있는 PlantGrowth 데이터를 사용하여 이를 시도해 봅시다.\nlibrary(\"ggbeeswarm\")\ndata(\"PlantGrowth\")\nggplot(PlantGrowth, aes(y = weight, x = group, col = group)) +\n  geom_beeswarm() + theme(legend.position = \"none\")\ntt = with(PlantGrowth, \n       t.test(weight[group ==\"ctrl\"],\n              weight[group ==\"trt2\"],\n              var.equal = TRUE))\ntt\n\n\n    Two Sample t-test\n\ndata:  weight[group == \"ctrl\"] and weight[group == \"trt2\"]\nt = -2.134, df = 18, p-value = 0.04685\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.980338117 -0.007661883\nsample estimates:\nmean of x mean of y \n    5.032     5.526 \n\n그림 6.8: PlantGrowth 데이터.\n질문 6.9\ntrt1과의 비교에서는 무엇을 얻나요? trt1 대 trt2는요?\np-값을 계산하기 위해 t.test 함수는 (t)-통계량(식 6.4)에 대한 점근적 이론을 사용합니다; 이 이론은 두 그룹의 평균이 같다는 귀무 가설하에서 통계량이 알려진 수학적 분포인 자유도가 (n_1+n_2-2)인 (t)-분포를 따른다고 명시합니다. 이 이론은 추가적인 기술적 가정, 즉 데이터가 독립적이고 동일한 표준 편차를 가진 정규 분포에서 나온다는 가정을 사용합니다. 우리는 이러한 가정들에 대해 걱정할 수 있습니다. 분명히 이 가정들은 성립하지 않습니다: 무게는 항상 양수이지만, 정규 분포는 전체 실수 축에 걸쳐 있습니다. 문제는 이론적 가정으로부터의 이러한 이탈이 실제로 차이를 만드느냐 하는 것입니다. 우리는 이를 알아내기 위해 순열 검정(permutation test)을 사용할 수 있습니다 (순열 검정의 이면에 있는 아이디어를 6.5.1절에서 좀 더 자세히 논의할 것입니다).\n질문 6.10\n위의 (t)-검정에 대해 동전 던지기의 그림 6.5 및 그림 6.6과 유사한 방식으로 이론적 및 시뮬레이션된 귀무 분포를 플롯해 보세요.\n해결책\nplgr = dplyr::filter(PlantGrowth, group %in% c(\"ctrl\", \"trt2\"))\n\nalpha  = 0.05\nxrange = 5 * c(-1, 1)\ndeckel = function(x) ifelse(x &lt; xrange[1], xrange[1], ifelse(x &gt; xrange[2], xrange[2], x))\n\nsim_null = tibble(\n  t = replicate(10000, t.test(weight ~ sample(group), var.equal = TRUE, data = plgr)$statistic)\n)\nsim_thresh = quantile(sim_null$t, c(alpha/2, 1-alpha/2))\nsim_null = mutate(sim_null, \n  t = deckel(t),        # 범위를 벗어난 데이터에 대한 경고 피하기\n  reject = ifelse(t &lt;= sim_thresh[1], \"low\", ifelse(t &gt; sim_thresh[2], \"high\", \"none\"))\n) \n\ntheo_thresh = qt(c(alpha/2, 1-alpha/2), df =  nrow(plgr) - 2)\ntheo_null = tibble(\n  t = seq(-5, 5, by = 0.05),\n  density = dt (x = t, df = nrow(plgr)  - 2),\n  reject = ifelse(t &lt;= theo_thresh[1], \"low\", ifelse(t &gt; theo_thresh[2], \"high\", \"none\"))\n)\n\np1 = ggplot(sim_null, aes(x = t, col = reject, fill = reject)) +\n       geom_bar(stat = \"bin\", breaks = seq(-5, 5, by = 0.2)) \np2 = ggplot(theo_null, aes(x = t, y = density, col = reject, fill = reject)) +\n       geom_area() \n\nfor (p in list(p1, p2))\n  print(p + \n        geom_vline(xintercept = tt$statistic, col = \"#101010\") +\n        scale_colour_manual(values = c(low = \"blue\", high = \"red\", none = \"darkgrey\")) +\n        scale_fill_manual(values = c(low = \"blue\", high = \"red\", none = \"darkgrey\")) + \n        xlim(xrange) + theme(legend.position = \"none\"))\n\n그림 6.9\n\n그림 6.10\n질문 6.11\n만약 우리가 (t) 대신 (t)의 절댓값을 고려했다면 앞선 질문의 해결책이 더 간단해졌을까요?\n해결책\n네, 만약 우리가 차이의 부호에 관심이 없다면, 우리는 직접적으로 ((t))에 대해 작업할 수 있으며, 아래쪽과 위쪽의 기각역 대신 단 하나의 기각역만을 가질 것입니다.\n(t)-검정은 여러 가지 변체로 제공되며, 이들은 모두 t.test 함수의 매개변수들을 통해 선택될 수 있습니다. 우리가 위에서 수행한 것은 양측(two-sided) 2표본 비쌍체(unpaired) 등분산(equal variance) 검정 이라고 불립니다. 양측 은 처리된 식물들의 무게가 처리되지 않은 것들보다 크거나 작을 때 모두 귀무 가설을 기각할 준비가 되어 있었다는 사실을 의미합니다.\n2표본(Two-sample) 15 은 우리가 두 그룹의 평균을 서로 비교했다는 것을 나타냅니다; 또 다른 옵션은 한 그룹의 평균을 주어진 고정된 숫자와 비교하는 것입니다.\n15 통계학에서 표본(sample) 이라는 용어가 생물학에서와는 다른 의미를 가진다는 점이 혼동될 수 있습니다. 생물학에서 표본(sample)은 어세이가 수행되는 단일 개체인 반면, 통계학에서는 측정값들의 집합, 예를 들어 식 6.5에서의 (n_1)-튜플 ((x_{1,1},…,x_{1,n_1}))을 의미하며, 이는 여러 개의 생물학적 표본들을 포함할 수 있습니다. 이러한 이중적인 의미가 혼란을 일으킬 수 있는 맥락에서는, 단일 생물학적 표본으로부터 얻은 데이터를 관측치(observation) 라고 부릅니다.\n비쌍체(Unpaired) 는 두 그룹의 측정값들 사이에 직접적인 1:1 매핑이 없었음을 의미합니다. 만약 반대로, 동일한 식물들에 대해 처리 전후로 데이터를 측정했다면, 개별 식물 내에서의 무게 변화를 살펴보는 쌍체(paired) 검정이 더 적절했을 것인데, 이는 식물들의 절대적인 무게보다는 변화에 주목하기 때문입니다.\n등분산(Equal variance) 은 (t)-통계량인 식 6.4가 계산되는 방식을 가리킵니다. 그 표현식은 각 그룹 내의 분산이 대략 같을 때 가장 적절합니다. 만약 분산들이 매우 다르다면, 대안적인 형태(Welch’s (t)-test)와 그와 연관된 점근적 이론이 존재합니다.\n독립성 가정. 이제 조금 특이한 시도를 해봅시다: 데이터를 복제해 보겠습니다.\nwith(rbind(PlantGrowth, PlantGrowth), \n       t.test(weight[group == \"ctrl\"],\n              weight[group == \"trt2\"],\n              var.equal = TRUE))\n\n\n    Two Sample t-test\n\ndata:  weight[group == \"ctrl\"] and weight[group == \"trt2\"]\nt = -3.1007, df = 38, p-value = 0.003629\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.8165284 -0.1714716\nsample estimates:\nmean of x mean of y \n    5.032     5.526 \n그룹 평균의 추정치(따라서 차이의 추정치)는 변하지 않았지만, p-값은 이제 훨씬 작아졌음에 주목하세요! 이로부터 우리는 두 가지 결론을 내릴 수 있습니다:\n\n(t)-검정의 검정력은 표본 크기에 의존합니다. 기저의 생물학적 차이가 동일하더라도, 더 많은 관측치를 가진 데이터 세트가 더 유의미한 결과를 내놓는 경향이 있습니다16.\n측정값들 사이의 독립성 가정은 정말 중요합니다. 동일한 데이터를 노골적으로 복제하는 것은 의존성의 극단적인 형태이지만, 어느 정도는 여러분이 서로 다른 수준의 반복(replication)을 섞어버릴 때도 동일한 일이 발생합니다. 예를 들어, 8개의 식물로부터 데이터를 얻었지만 각 식물에 대해 동일한 것을 두 번씩 측정했다면(기술적 반복), 이들이 이제 16개의 독립적인 측정값이라고 주장하는 것은 잘못된 것입니다.\n\n16 식 6.5에서 숫자 (n_1)과 (n_2)가 나타나는 방식으로부터도 이를 알 수 있습니다.\n\n8.4.1 6.5.1 순열 검정 (Permutation tests)\n위에서 우리가 (t)-통계량에 적용된 순열 검정의 결과와 모수적 (t)-검정의 결과를 대조했을 때 어떤 일이 일어났나요? 이들이 두 개의 서로 다른 검정이며, 그 결과의 유사성은 바람직하긴 하지만 우연이라는 점을 깨닫는 것이 중요합니다. 모수적 검정의 경우, (t)-통계량의 귀무 분포는 데이터의 가정된 귀무 분포(단위 공분산을 가진 ((n_1+n_2))차원 공간 (^{n_1+n_2})에서의 다변량 정규 분포)로부터 도출되며 연속적입니다: 즉, (t)-분포입니다. 이와 대조적으로, 우리 검정 통계량의 순열 분포는 단일 데이터 인스턴스((n_1+n_2)개의 관측치)로부터 관측치 레이블의 ((n_1+n_2)!)개 순열17의 유한 집합으로부터 얻어지므로 이산적입니다. 여기서 우리가 가정하는 전부는 귀무 가설하에서 변수 (X_{1,1},…,X_{1,n_1},X_{2,1},…,X_{2,n_2})가 교환 가능하다(exchangeable)는 것입니다. 논리적으로 이 가정은 모수적 검정의 가정에 의해 함축되지만, 더 약한 가정입니다. 순열 검정은 (t)-통계량을 사용하지만, (t)-분포(또는 정규 분포)는 사용하지 않습니다. 두 검정이 매우 유사한 결과를 주었다는 사실은 중심 극한 정리(Central Limit Theorem)의 결과입니다.\n17 또는 계산 시간을 절약하기 위해 무작위 하위 집합을 사용하기도 합니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>가설 검정 (Hypothesis Testing)</span>"
    ]
  },
  {
    "objectID": "06-chap.html#p-값-해킹-p-value-hacking",
    "href": "06-chap.html#p-값-해킹-p-value-hacking",
    "title": "8  가설 검정 (Hypothesis Testing)",
    "section": "8.5 6.6 P-값 해킹 (P-value hacking)",
    "text": "8.5 6.6 P-값 해킹 (P-value hacking)\n동전 던지기 예제로 돌아가 봅시다. 우리는 동전이 공정하지 않다는 것을 “알고” 있었음에도 불구하고(6.2절에서 probHead를 0.6으로 선택했으므로), 5% 수준에서 귀무 가설(동전이 공정하다는 가설)을 기각하지 않았습니다. 이제 우리가 서로 다른 검정 통계량들을 살펴보기 시작한다고 가정해 봅시다. 아마도 3번 이상 연속으로 앞면이 나온 횟수라든지, 처음 50번의 던지기에서 앞면의 횟수 등등 말이죠. 그러다 보면 어느 시점에는 단지 우연히라도 작은 p-값이 나오는 검정을 발견하게 될 것입니다 (결국 귀무 가설 – 공정한 동전 – 하에서 p-값이 0.05보다 작을 확률은 20분의 1입니다). 우리는 방금 p-값 해킹(p-value hacking) 18 이라 불리는 일을 저지른 것입니다 (Head et al. 2015). 무엇이 문제인지 아시겠지요: 우리 주장을 증명하려는 열의에 차서 데이터가 우리가 원하는 대로 해줄 때까지 데이터를 고문한 것입니다. 이와 관련된 전술로는 가설 전환(hypothesis switching) 또는 HARKing (결과가 알려진 후에 가설을 세우는 것)이 있습니다: 데이터 세트가 있고, 아마도 그것을 모으기 위해 많은 시간과 돈을 투자했으므로 결과가 필요합니다. 우리는 수많은 다양한 귀무 가설과 검정 통계량을 생각해 내고, 이를 테스트하고, 무언가 보고할 만한 것이 나올 때까지 반복합니다.\n18 http://fivethirtyeight.com/features/science-isnt-broken\n이러한 전술들은 6.3절에서 설명한 가설 검정의 규칙을 위반하는 것입니다. 거기서는 가설과 검정 방법을 먼저 선택하고 그 다음에 데이터를 수집하는 순차적인 절차를 제시했습니다. 그러나 2장에서 보았듯이, 실제로는 그러한 전술이 유혹적일 수 있습니다. 생물학적 데이터의 경우 데이터를 “정규화”하고, 변환하고, 배치 효과(batch effects)를 보정하고, 이상치를 제거하는 등 너무나 많은 다양한 선택지가 존재합니다. 주제는 복잡하고 정답이 정해져 있지 않습니다. Wasserstein과 Lazar (2016)는 과학계에서 p-값이 어떻게 사용되는지에 대한 문제점들과 몇 가지 오해들에 대해 읽기 쉬운 짧은 요약을 제공했습니다. 그들은 또한 p-값이 어떻게 유익하게 사용될 수 있는지도 강조했습니다. 핵심 메시지는 다음과 같습니다: 여러분의 데이터, 어떤 분석을 시도했는지, 그리고 어떻게 수행했는지에 대해 완전히 투명하게 공개하십시오. 분석 코드를 제공하십시오. 그러한 맥락 정보가 있어야만 p-값은 유용할 수 있습니다.\n착각을 피하십시오. 우리의 통계적 검정은 결코 귀무 가설이 참임을 증명하려는 시도가 아니라는 점을 명심하십시오 - 우리는 단순히 그것이 거짓이라는 증거가 있는지 없는지를 말하는 것뿐입니다. 만약 높은 p-값이 귀무 가설이 참이라는 것을 나타내는 것이라면, 우리는 완전히 미친 귀무 가설을 세우고, 전혀 무관한 실험을 하고, 결론을 내릴 수 없는 소량의 데이터를 수집하여, 0과 1 사이의 랜덤한 숫자인 (따라서 우리의 임계값 ()보다 클 확률이 높은) p-값을 찾음으로써 우리의 가설을 증명해 버릴 수 있을 것입니다!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>가설 검정 (Hypothesis Testing)</span>"
    ]
  },
  {
    "objectID": "06-chap.html#다중-검정-multiple-testing",
    "href": "06-chap.html#다중-검정-multiple-testing",
    "title": "8  가설 검정 (Hypothesis Testing)",
    "section": "8.6 6.7 다중 검정 (Multiple testing)",
    "text": "8.6 6.7 다중 검정 (Multiple testing)\n질문 6.12\nxkcd 만화 882를 찾아보세요. 왜 신문은 다른 색상들에 대한 결과는 보도하지 않았을까요?\n만화에서 묘사된 곤경은 생물학의 고처리량 데이터에서도 발생합니다. 그것도 아주 강력하게 말이죠! 여러분은 단지 20가지 색상의 젤리빈뿐만 아니라, 예를 들어 두 조건 사이의 차등 발현을 위해 테스트된 20,000개의 유전자나, DNA 돌연변이가 일어났을지도 모르는 게놈 상의 60억 개의 위치를 다루게 될 것입니다. 그렇다면 우리는 이를 어떻게 처리해야 할까요? 통계적 검정 결과와 실제 상황을 연관시킨 표(표 6.1)를 다시 살펴봅시다. 이번에는 모든 것을 많은 가설의 관점에서 구성해 보겠습니다.\n\n\n\n검정 대 실제\n귀무 가설이 참\n…이 거짓\n합계\n\n\n\n\n기각됨\n(V)\n(S)\n(R)\n\n\n기각되지 않음\n(U)\n(T)\n(m-R)\n\n\n합계\n(m_0)\n(m-m_0)\n(m)\n\n\n\n표 6.2: 다중 검정에서의 오류 유형. 문자는 각 유형의 오류가 발생한 횟수를 나타냅니다.\n\n(m): 전체 검정(및 귀무 가설)의 수\n(m_0): 참인 귀무 가설의 수\n(m-m_0): 거짓인 귀무 가설의 수\n(V): 위양성(false positives)의 수 (제1종 오류의 척도)\n(T): 위음성(false negatives)의 수 (제2종 오류의 척도)\n(S), (U): 진양성 및 진음성의 수\n(R): 기각 횟수\n\n이 장의 나머지 부분에서 우리는 제1종 및 제2종 오류를 관리하는 다양한 방법들을 살펴봅니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>가설 검정 (Hypothesis Testing)</span>"
    ]
  },
  {
    "objectID": "06-chap.html#가족오류율-the-family-wise-error-rate",
    "href": "06-chap.html#가족오류율-the-family-wise-error-rate",
    "title": "8  가설 검정 (Hypothesis Testing)",
    "section": "8.7 6.8 가족오류율 (The family wise error rate)",
    "text": "8.7 6.8 가족오류율 (The family wise error rate)\n가족오류율(family wise error rate, FWER) 은 (V&gt;0)일 확률, 즉 하나 이상의 위양성 오류를 저지를 확률입니다. 우리는 이를 위양성 오류를 전혀 저지르지 않을 확률의 여사건으로 계산할 수 있습니다19.\n19 독립을 가정함.\n[ \\[\\begin{align} P(V&gt;0) &= 1 - P(\\text{$m_0$개의 귀무 가설 중 어느 것도 기각되지 않음}) \\\\ &= 1 - (1 - \\alpha)^{m_0} \\to 1 \\quad\\text{as } m_0\\to\\infty. \\end{align}\\] ]\n어떤 고정된 ()에 대해, 이 확률은 (m_0)가 (1/)의 규모가 되자마자 무시할 수 없을 정도로 커지며, (m_0)가 커짐에 따라 1을 향해 갑니다. 이러한 관계는 방대한 잠재적 일치 데이터베이스를 검색하는 DNA 매칭과 같은 실험에서 심각한 결과를 초래할 수 있습니다. 예를 들어, 두 사람의 DNA 프로필이 무작위 오차에 의해 일치할 확률이 100만 분의 1이고, 여러분의 DNA를 80만 개의 프로필이 담긴 데이터베이스와 대조한다면, 데이터베이스 내에서 (즉, 여러분이 그 안에 없더라도) 무작위로 일치할 확률은 다음과 같습니다:\n1 - (1 - 1/1e6)^8e5\n\n\n[1] 0.5506712\n상당히 높습니다. 그리고 데이터베이스에 몇 백만 개의 프로필이 더 포함된다면, 잘못된 일치는 사실상 피할 수 없습니다.\n질문 6.13\n(m_0)가 클 때 식 6.6의 확률이 실제로 1에 매우 가까워진다는 것을 증명해 보세요.\n\n8.7.1 6.8.1 본페로니 방법 (Bonferroni method)\nFWER을 제어하고 싶다면 가설별 ()를 어떻게 선택해야 할까요? 위의 계산들은 ()와 (m_0)의 곱이 합리적인 어림값이 될 수 있음을 시사합니다. 대개 우리는 (m_0)를 모르지만, (m)은 알고 있으며 (m_0 m)이므로 (m)은 (m_0)의 상한선입니다. 본페로니 방법은 간단히 말해, 우리가 ({}) 수준에서 FWER 제어를 원한다면 가설별 임계값을 (= {}/m)으로 선택해야 한다는 것입니다. 예제를 통해 이를 확인해 봅시다.\nm = 10000\nggplot(tibble(\n  alpha = seq(0, 7e-6, length.out = 100),\n  p     = 1 - (1 - alpha)^m),\n  aes(x = alpha, y = p)) +  geom_line() +\n  xlab(expression(alpha)) +\n  ylab(\"Prob( no false rejection )\") +\n  geom_hline(yintercept = 0.05, col = \"red\")\n\n그림 6.11: 본페로니 방법. 플롯은 (m=10000)에 대해 ()의 함수로서 식 6.6의 그래프를 보여줍니다.\n그림 6.11에서 검은색 선은 (0.05 값에 해당하는) 빨간색 선과 (^{-6})에서 교차하는데, 이는 본페로니 방법에 의해 함축된 (0.05/m) 값보다 아주 약간 큽니다.\n질문 6.14\n왜 두 값이 정확히 같지 않을까요?\n하지만 이 방법의 잠재적인 단점은 (m_0)가 크면 기각 임계값이 매우 작아진다는 것입니다. 이는 무언가를 감지할 기회를 가지려면 개별 검정의 검정력이 매우 높아야 함을 의미합니다. 종종 FWER 제어는 너무 엄격하여, 데이터를 생성하고 모으는 데 들인 시간과 비용을 비효율적으로 사용하게 만들 수 있습니다. 이제 우리는 제1종 오류를 제어하는 더 미묘한 방법들이 있다는 것을 보게 될 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>가설 검정 (Hypothesis Testing)</span>"
    ]
  },
  {
    "objectID": "06-chap.html#허위-발견율-the-false-discovery-rate",
    "href": "06-chap.html#허위-발견율-the-false-discovery-rate",
    "title": "8  가설 검정 (Hypothesis Testing)",
    "section": "8.8 6.9 허위 발견율 (The false discovery rate)",
    "text": "8.8 6.9 허위 발견율 (The false discovery rate)\n데이터를 살펴봅시다. 합성 글루코코르티코이드인 덱사메타손(dexamethasone)을 처리하거나 처리하지 않은 4개의 1차 인간 기도 평활근 세포주의 유전자 발현 측정값(유전자 수준 카운트)이 포함된 RNA-Seq 데이터 세트 airway를 불러옵니다. 우리는 8장에서 더 자세히 다룰 DESeq2 방법을 사용할 것입니다. 지금은 이 방법이 각 유전자에 대해 차등 발현 검정을 수행한다고만 말해두면 충분합니다. 개념적으로 검정되는 귀무 가설은 (t)-검정의 그것과 유사하지만, 카운트 데이터를 다루기 때문에 세부 사항은 약간 더 복잡합니다.\nlibrary(\"DESeq2\")\nlibrary(\"airway\")\ndata(\"airway\")\naw   = DESeqDataSet(se = airway, design = ~ cell + dex)\naw   = DESeq(aw)\nawde = as.data.frame(results(aw)) |&gt; dplyr::filter(!is.na(pvalue))\n태스크\nawde의 내용을 살펴보세요.\n태스크\n(선택 사항) 위의 코드 청크가 무엇을 하는지에 대한 더 많은 정보를 얻으려면 DESeq2 비네트 및/또는 8장을 참고하십시오.\n\n8.8.1 6.9.1 p-값 히스토그램\np-값 히스토그램 은 다중 검정을 포함하는 모든 분석에서 중요한 무결성 검사(sanity check)입니다. 이는 두 가지 성분으로 구성된 혼합물입니다:\nnull: 귀무 가설이 참인 검정에서 얻은 p-값들.\nalt: 귀무 가설이 거짓인 검정에서 얻은 p-값들. 이 두 성분의 상대적 크기는 참인 귀무 가설과 참인 대립 가설의 비율(즉, (m_0)와 (m))에 달려 있으며, 종종 히스토그램에서 시각적으로 추정할 수 있습니다. 분석의 통계적 검정력이 높다면 두 번째 성분(“alt”)은 대부분 작은 p-값들로 구성되며, 즉 히스토그램에서 0 근처의 정점으로 나타납니다. 일부 대립 가설에 대해 검정력이 높지 않다면 이 정점이 오른쪽으로 뻗어 나가는, 즉 “어깨(shoulder)” 모양을 가질 것으로 예상합니다. “null” 성분의 경우 (연속형 데이터와 검정 통계량에 대한 p-값의 정의에 따라) ([0,1])에서의 균등 분포를 예상합니다. airway 데이터에 대한 p-값 히스토그램을 그려봅시다.\nggplot(awde, aes(x = pvalue)) +\n  geom_histogram(binwidth = 0.025, boundary = 0)\n\n그림 6.12: airway 데이터에 대한 p-값 히스토그램.\n그림 6.12에서 우리는 예상된 혼합물을 봅니다. 또한 null 성분이 정확히 평평(균등)하지 않음을 볼 수 있는데, 이는 데이터가 카운트(counts)이기 때문입니다. 카운트가 높을 때는 준연속(quasi-continuous)으로 보이지만, 카운트가 낮은 검정의 경우 데이터의 이산성(discreteness)과 그 결과로 나타나는 p-값이 히스토그램 오른쪽의 뾰족한 부분(spikes)으로 나타납니다.\n이제 우리가 p-값이 ()보다 작은 모든 검정을 기각한다고 가정해 봅시다. 우리는 다음 코드로 생성된 그림 6.13과 같은 플롯을 통해 허위 발견 비율(false discovery proportion)의 추정치를 시각적으로 결정할 수 있습니다.\nalpha = binw = 0.025\npi0 = 2 * mean(awde$pvalue &gt; 0.5)\nggplot(awde,\n  aes(x = pvalue)) + geom_histogram(binwidth = binw, boundary = 0) +\n  geom_hline(yintercept = pi0 * binw * nrow(awde), col = \"blue\") +\n  geom_vline(xintercept = alpha, col = \"red\")\n\n그림 6.13: p-값 히스토그램을 통한 FDR의 시각적 추정.\n우리는 첫 번째 빈 ([0, ])에 4772개의 p-값이 있음을 알 수 있는데, 이 중 약 945개는 null일 것으로 예상됩니다(파란색 선으로 표시됨). 따라서 우리는 잘못된 기각의 비율을 다음과 같이 추정할 수 있습니다.\npi0 * alpha / mean(awde$pvalue &lt;= alpha)\n\n\n[1] 0.1980092\n허위 발견율(False discovery rate, FDR) 은 다음과 같이 정의됩니다.\n[ = !, ]\n여기서 (R)과 (V)는 표 6.2에서와 같습니다. 분모의 식은 (R=0)인 경우에도 FDR이 잘 정의되도록 보장합니다(그 경우 함축적으로 (V=0)이 됨). 모든 귀무 가설이 참인 경우, 즉 (V=R)인 경우 FDR은 FWER과 동일해진다는 점에 유의하세요. ()는 기댓값(expected value) 을 의미합니다. 즉, FDR은 특정한 하나의 실험 결과와 연관된 수치가 아닙니다. 오히려 우리가 선택한 검정 방법과 기각 규칙이 주어졌을 때, (적어도 개념적으로는) 그 실험을 수많은 횟수 반복했을 때 나타나는 발견들 중 제1종 오류가 차지하는 평균적인 비율을 의미합니다.\n20 FDR은 기댓값이기 때문에 최악의 경우(worst case)를 제어하는 것은 아닙니다: 어떤 단일 실험에서의 이른바 허위 발견 비율(false discovery proportion, FDP), 즉 (()가 없는) 실현된 값 (v/r)은 기댓값보다 훨씬 높거나 낮을 수 있습니다.\n\n\n8.8.2 6.9.2 FDR 제어를 위한 벤자미니-호크버그 알고리즘\n지난 섹션의 “시각적 FDR” 방법보다 더 우아한 대안이 있습니다. 벤자미니와 호크버그 (1995)에 의해 도입된 이 절차는 다음과 같은 단계들을 거칩니다:\n\n먼저, p-값들을 오름차순으로 정렬합니다: (p_{(1)} … p_{(m)})\n그런 다음 우리가 목표로 하는 FDR 수준인 ()에 대해, 다음을 만족하는 가장 큰 (k)를 찾습니다: (p_{(k)} ; k / m)\n마지막으로 가설 (1, …, k)를 기각합니다.\n\n우리는 간단한 그래픽 설명을 통해 우리 RNA-Seq p-값들에 이 절차를 적용했을 때 어떻게 작동하는지 볼 수 있습니다:\nphi  = 0.10\nawde = mutate(awde, rank = rank(pvalue))\nm    = nrow(awde)\n\nggplot(dplyr::filter(awde, rank &lt;= 7000), aes(x = rank, y = pvalue)) +\n  geom_line() + geom_abline(slope = phi / m, col = \"red\")\n\n그림 6.14: 벤자미니-호크버그 절차의 시각화. 하위 7000개 p-값들을 확대한 모습입니다.\n이 방법은 검은색 선(우리 p-값들)과 빨간색 선(기울기 (/ m))이 교차하는 가장 오른쪽 지점을 찾습니다. 그런 다음 그 왼쪽의 모든 검정들을 기각합니다.\nkmax = with(arrange(awde, rank),\n         last(which(pvalue &lt;= phi * rank / m)))\nkmax\n\n\n[1] 4099\n질문 6.15\nkmax의 값을 위에서의 4772라는 숫자(그림 6.13)와 비교해 보세요. 왜 이들이 다를까요?\n질문 6.16\nR과 함께 제공되는 p.adjust 함수의 method=\"BH\" 옵션과 연관된 코드를 살펴보세요. 그것이 위에서 우리가 수행한 것과 어떻게 비교되나요?\n질문 6.17\nSchweder와 Spjøtvoll 플롯: Schweder와 Spjøtvoll (1982)의 그림 1-3을 확인해 보세요. awde 데이터에 대해 유사한 플롯을 만들어 보세요. 그것이 그림 6.14 및 6.13과 어떤 관련이 있나요?\n해결책\n벤자미니와 호크버그 (1995)보다 13년 앞서, Schweder와 Spjøtvoll (1982)은 참인 귀무 가설의 분율을 추정할 수 있게 해주는 관측된 (p)-값들의 진단 플롯을 제안했습니다. (p)-값 (p_i)를 가진 일련의 가설 검정 (H_1, …, H_m)에 대해, 그들은 다음을 플롯할 것을 제안했습니다.\n[ ( 1-p_i, N(p_i) ) i , …, m, ]\n여기서 (N(p))는 (p)보다 큰 (p)-값들의 개수입니다. awde$pvalue에 이 진단 플롯을 적용한 결과가 그림 6.15에 나와 있습니다. 모든 귀무 가설이 참일 때, 각 (p)-값은 ([0,1])에서 균등하게 분포하며, 결과적으로 표본 ((p_1, …, p_m))의 경험적 누적 분포는 선 (F(t)=t)에 가까울 것으로 기대됩니다. 대칭성에 의해, 이는 ((1 - p_1, …, 1 - p_m))에도 똑같이 적용됩니다. (일반성을 잃지 않고) 처음 (m_0)개의 귀무 가설이 참이고 나머지 (m-m_0)개가 거짓일 때, ((1-p_1, …, 1-p_{m_0}))의 경험적 누적 분포는 다시 선 (F_0(t)=t)에 가까울 것으로 기대됩니다. 반면에 ((1-p_{m_0+1}, …, 1-p_{m}))의 경험적 누적 분포는 (F_0) 아래에 머물다가 (t)가 1에 가까워짐에 따라 가파르게 증가하는 함수 (F_1(t))에 가까울 것으로 기대됩니다. 실제로는 어떤 귀무 가설이 참인지 모르기 때문에, 우리는 다음과 같은 식에 가까울 것으로 기대되는 경험적 누적 분포를 가진 혼합물만을 관찰하게 됩니다.\n[ F(t) = F_0(t) + F_1(t). ]\n그러한 상황이 그림 6.15에 나와 있습니다. 만약 작은 (t)에 대해 (F_1(t)/F_0(t))가 작다면 (즉, 검정들이 합리적인 검정력을 가진다면), 혼합 분율 ()은 플롯의 왼쪽 부분에 선을 적합시키고 오른쪽에서의 그 높이를 확인 함으로써 추정될 수 있습니다. 그러한 적합 결과가 빨간색 선으로 표시되어 있습니다. 여기서 우리는 카운트 데이터가 모두 매우 작은 숫자가 아닌(baseMean&gt;=1) 검정들에 집중했는데, 이는 이들에 대해 p-값 귀무 분포가 충분히 균등 분포에 가깝기 때문입니다 (위에서 언급한 이산성을 보이지 않음). 하지만 여러분은 모든 유전자에 대해 동일한 플롯을 만들어 볼 수도 있습니다.\nawdef = awde |&gt;\n  dplyr::filter(baseMean &gt;=1) |\n  arrange(pvalue) |\n  mutate(oneminusp = 1 - pvalue,\n         N = n() - row_number())\nalpha = 0.05 jj = round(nrow(awdef) * c(1, 0.5)) slope = with(awdef, diff(N[jj]) / diff(oneminusp[jj])) ggplot(awdef) + geom_point(aes(x = oneminusp, y = N), size = 0.15) + xlab(expression(1-p[i])) + ylab(expression(N(p[i]))) + geom_abline(intercept = 0, slope = slope, col = “red3”) + geom_hline(yintercept = slope, linetype = “dotted”) + geom_vline(xintercept = 1, linetype = “dotted”) + geom_text(x = 0, y = slope, label = paste(round(slope)), hjust = -0.1, vjust = -0.25)\n\n그림 6.15: Schweder와 Spjøtvoll 플롯.\nawdef에는 22,853개의 행이 있으며, 따라서 이 단순한 추정치에 따르면 22853-17302=5551개의 대립 가설이 존재합니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>가설 검정 (Hypothesis Testing)</span>"
    ]
  },
  {
    "objectID": "06-chap.html#국소-fdr-the-local-fdr",
    "href": "06-chap.html#국소-fdr-the-local-fdr",
    "title": "8  가설 검정 (Hypothesis Testing)",
    "section": "8.9 6.10 국소 FDR (The local FDR)",
    "text": "8.9 6.10 국소 FDR (The local FDR)\n\n그림 6.16: http://xkcd.com/1132 로부터 – 빈도론자는 현재 가용한 데이터만을 가질 수 있는 반면, 베이지안은 세계에 대한 이해나 이전 경험을 활용할 수 있습니다. 베이지안으로서 그녀는 우리 태양의 질량이 노바(nova)가 되기에는 너무 작다는 물리학 지식을 충분히 알고 있을 것입니다. 설령 물리학을 모르더라도, 그녀는 경험적 베이지안(empirical Bayesian) 이 되어 태양이 폭발하지 않았던 수많은 이전 날들로부터 사전 확률을 가져올 수 있습니다.\n이 장의 첫머리에 있는 xkcd 만화는 다중 검정 문제를 오차를 축적하는 방식이라는 다소 불길한 해석으로 끝을 맺었지만, 그림 6.16은 다중 검정의 기회를 강조합니다: 우리가 많은 검정을 수행할 때, 우리는 다중성을 사용하여 단일 검정으로 가능한 것 이상의 이해를 높일 수 있습니다.\n\n그림 6.17: 국소 허위 발견율과 두 그룹 모델. (f_{}(p))의 일부 선택 및 (_0=0.6)을 사용함; 밀도(상단) 및 분포 함수(하단).\n그림 6.13의 히스토그램으로 돌아가 봅시다. 개념적으로 우리는 이를 이른바 두 그룹 모델(two-groups model, Efron 2010)의 관점에서 생각할 수 있습니다:\n[ f(p)= _0 + (1-0) f{}(p), ]\n여기서 (f(p))는 분포의 밀도(데이터가 무한하고 빈이 무한히 작을 때 히스토그램의 모습)이고, (0)는 균등 성분의 크기를 나타내는 0과 1 사이의 숫자이며, (f{})는 대립 성분입니다. 이는 우리가 이미 4장에서 보았던 혼합 모델입니다. 혼합 밀도와 주변 밀도 (f(p))는 그림 6.17의 상단 패널에 시각화되어 있습니다: 파란색 영역들은 함께 (f_{}(p))의 그래프에 해당하고, 회색 영역들은 (f_{}(p) = _0)에 해당합니다. 이제 특정 컷오프 (p) (예를 들어 그림 6.17에서처럼 (p=0.1))를 고려한다면, 이 컷오프에서 기각하는 가설이 위양성일 확률을 다음과 같이 계산할 수 있습니다. 우리는 컷오프에서의 (f) 값(빨간색 선)을 귀무 가설의 기여분(연한 빨간색, (_0))과 대립 가설의 기여분(더 짙은 빨간색, ((1-0) f{}(p)))으로 분해합니다. 그러면 국소 허위 발견율(local false discovery rate) 은 다음과 같습니다.\n[ (p) = . ]\n정의에 따라 이 수치는 0과 1 사이입니다. 그림 6.17에서 ()이 (p)의 단조 증가 함수라는 점에 주목하세요. 이는 fdr이 가장 작은 (p)에 대해 가장 낮아야 하고, 그 다음 점차 커져서 가장 오른쪽 끝에서 1에 도달해야 한다는 우리의 직관과 일치합니다. 우리는 빨간색 선뿐만 아니라 곡선 아래의 면적에 대해서도 유사한 분해를 할 수 있습니다. 이는 다음과 같습니다.\n[ F(p) = _0^p f(t),dt, ]\n그리고 전체 면적 (F(p))에 대한 짙은 회색 영역((_0)에 (p)를 곱한 값)의 비율은 꼬리 영역 허위 발견율(tail area false discovery rate) (Fdr21)입니다.\n21 관습적으로 소문자 약어 fdr은 국소(local)를, 대문자 약어 Fdr은 식 6.10의 두 그룹 모델 문맥에서의 꼬리 영역 허위 발견율을 위해 사용합니다. 약어 FDR은 원래의 정의인 식 6.7을 위해 사용되는데, 이는 좀 더 일반적입니다. 즉, 식 6.10의 모델링 가정에 의존하지 않습니다.\n[ (p) = . ]\n우리는 그림 6.21의 진단을 위해 (F)의 데이터 버전을 사용할 것입니다.\nqvalue 와 fdrtool 패키지는 이러한 모델을 데이터에 적합시키는 기능을 제공합니다.\nlibrary(\"fdrtool\")\nft = fdrtool(awde$pvalue, statistic = \"pvalue\")\nfdrtool 에서는 위에서 우리가 (_0)라고 불렀던 것을 eta0라고 부릅니다:\nft$param[,\"eta0\"]\n\n\n     eta0 \n0.8822922 \n질문 6.18\n위의 fdrtool 호출로 생성된 플롯들은 무엇을 보여주나요?\n태스크\n리스트 ft의 다른 요소들을 탐색해 보세요.\n질문 6.19\n경험적 베이즈(empirical Bayes) 방법에서 경험적(empirical) 은 무엇을 의미하나요?\n\n8.9.1 6.10.1 국소 대 전체\nFDR(또는 Fdr)은 집합의 속성입니다. 이는 다중 검정 분석 과정에서 이루어진 기각 전체의 집합에 적용되는 단일 숫자입니다. 대조적으로 fdr은 국소적인 속성입니다. 이는 개별 가설에 적용됩니다. fdr이 밀도 플롯의 (x)축을 따른 각 지점에 대해 계산되었던 반면, Fdr은 빨간색 선의 왼쪽 면적들에 의존했던 그림 6.17을 상기해 보십시오.\n질문 6.20\n경제학에서의 총비용(total cost) 과 한계비용(marginal cost) 개념을 확인해 보세요. Fdr 및 fdr과의 유사성을 찾을 수 있나요?\n해결책\n(m)개의 제품 세트를 생산하는 생산 공정에서 총비용은 관련된 모든 비용의 합입니다. 제품의 평균 비용은 가상의 수치로, 총비용을 (m)으로 나눈 것입니다. 한계비용은 제품 하나를 추가로 만드는 데 드는 비용이며, 종종 평균 비용과는 매우 다릅니다. 예를 들어, 피아노로 베토벤 소나타 한 곡을 연주하는 법을 배우는 데는 초보자에게 상당한 시간이 걸릴 수 있지만, 그것을 한 번 더 연주하는 데는 비교적 적은 추가 노력이 필요합니다: 즉, 한계비용이 고정비용(따라서 총비용)보다 훨씬 적습니다. 한계비용이 평균 비용보다 높은 경우의 예로는 달리기가 있습니다: 운동화를 신고 나가서 10km를 달리는 것은 대부분의 사람들에게 꽤 견딜 만한(아마도 즐거운) 일이겠지만, 추가로 10km를 더 달릴 때마다 불균형적으로 큰 불쾌감이 더해질 수 있습니다.\n\n\n8.9.2 6.10.2 용어\n역사적으로 다중 검정 보정(multiple testing correction) 과 조정된 p-값(adjusted p-value) 이라는 용어가 프로세스와 출력물에 사용되어 왔습니다. 허위 발견율의 문맥에서 이러한 용어들은 도움이 되지 않을 뿐만 아니라 혼란을 줄 수 있습니다. 우리는 이 용어들의 사용을 피할 것을 권장합니다. 이 용어들은 우리가 p-값 세트 ((p_1, …, p_m))로 시작하여 어떤 전형적인 절차를 적용하고, “보정된” 또는 “조정된” p-값 세트 ((p_1^{}, …, p_m^{}))를 얻는다는 것을 암시합니다. 하지만 벤자미니-호크버그 방법의 결과물은 p-값이 아니며, FDR, Fdr, fdr 역시 p-값이 아닙니다. FDR과 Fdr은 집합의 속성임을 기억하십시오. 이를 개별 검정과 연관 짓는 것은 평균 비용과 한계 비용을 혼동하는 것만큼이나 말이 되지 않습니다. Fdr과 fdr 역시 상당한 양의 모델링 가정에 의존합니다. 다음 섹션에서 여러분은 벤자미니-호크버그 방법이 유일한 방법이 아니라는 것과, 다중 검정 절차에 입력되는 가설 및 p-값 세트와 그 출력물 사이의 추정되는 직접적인 대응 관계를 더욱 멀어지게 만드는 중요하고 유용한 확장들이 있다는 것을 보게 될 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>가설 검정 (Hypothesis Testing)</span>"
    ]
  },
  {
    "objectID": "06-chap.html#독립-가설-가중치-부여-independent-hypothesis-weighting",
    "href": "06-chap.html#독립-가설-가중치-부여-independent-hypothesis-weighting",
    "title": "8  가설 검정 (Hypothesis Testing)",
    "section": "8.10 6.11 독립 가설 가중치 부여 (Independent hypothesis weighting)",
    "text": "8.10 6.11 독립 가설 가중치 부여 (Independent hypothesis weighting)\n지금까지 살펴본 벤자미니-호크버그 방법과 두 그룹 모델은 가설의 교환 가능성(exchangeability) 을 암시적으로 가정합니다: 우리가 사용하는 전부는 p-값뿐입니다. 이들 외에 어떠한 추가 정보도 고려하지 않습니다. 이것이 항상 최적인 것은 아니며, 여기서는 이를 개선하는 방법들을 공부할 것입니다.\n예를 들어봅시다. 직관적으로, 더 많은 수의 리드(reads)가 매핑된 유전자의 신호 대 잡음비(signal-to-noise ratio)는 리드가 적은 유전자보다 더 좋을 것이며, 이는 우리 검정의 검정력에 영향을 줄 것입니다. 관측치 전반에 걸친 정규화된 카운트의 평균을 살펴봅시다. DESeq2 패키지에서 이 수치는 baseMean이라 불립니다.\nawde$baseMean[1]\n\n\n[1] 708.6022\n\n\ncts = counts(aw, normalized = TRUE)[1, ]\ncts\n\n\nSRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 \n  663.3142   499.9070   740.1528   608.9063   966.3137   748.3722   836.2487 \nSRR1039521 \n  605.6024 \n\n\nmean(cts)\n\n\n[1] 708.6022\n다음으로 유전자 전체에 대한 이 수치의 히스토그램을 생성하고, p-값에 대해 플롯합니다(그림 6.18 및 6.19).\nggplot(awde, aes(x = asinh(baseMean))) +\n  geom_histogram(bins = 60)\n\n그림 6.18: baseMean의 히스토그램. 0에 가까운 값부터 약 330,000까지 넓은 동적 범위를 포괄하고 있음을 알 수 있습니다.\nggplot(awde, aes(x = rank(baseMean), y = -log10(pvalue))) +\n  geom_hex(bins = 60) +\n  theme(legend.position = \"none\")\n\n그림 6.19: baseMean의 순위(rank) 대 p-값의 상용로그 역수(negative logarithm)의 산점도. baseMean 값이 작을 때는 작은 p-값이 나타나지 않습니다. 모든 관측치에 걸친 리드 카운트가 일정 크기 이상인 유전자에 대해서만 차등 발현 검정이 작은 p-값을 내놓을 수 있는 검정력을 갖습니다.\n질문 6.21\n히스토그램에 왜 () 변환을 사용했을까요? 변환을 하지 않았을 때, 로그 변환을 했을 때, 이동된 로그 변환(즉, ((x+)))을 했을 때는 어떤 모습일까요?\n질문 6.22\n산점도에서 왜 p-값에 (-_{10})을 사용했을까요? baseMean에는 왜 순위 변환을 사용했을까요?\n편의상, 우리는 baseMean을 동일한 크기의 6개 그룹에 해당하는 요인 변수 group으로 이산화(discretize)합니다.\nawde = mutate(awde, stratum = cut(baseMean, include.lowest = TRUE,\n  breaks = signif(quantile(baseMean,probs=seq(0,1,length.out=7)),2)))\n그림 6.20과 6.21에서 우리는 stratum별로 계층화된 p-값의 히스토그램과 ECDF를 봅니다.\nggplot(awde, aes(x = pvalue)) + facet_wrap( ~ stratum, nrow = 4) +\n  geom_histogram(binwidth = 0.025, boundary = 0)\n\n그림 6.20: airway 데이터의 p-값 히스토그램. baseMean 값이 증가함에 따라 정의된 동일한 크기의 그룹들로 계층화된 모습.\nggplot(awde, aes(x = pvalue, col = stratum)) +\n  stat_ecdf(geom = \"step\") + theme(legend.position = \"bottom\")\n\n그림 6.21: 그림 6.20과 동일한 데이터를 ECDF로 표시한 모습.\n이러한 계층(strata)들에 대해 두 그룹 모델을 따로 적합시킨다면, (0)와 (f{})에 대해 상당히 다른 추정치를 얻게 될 것입니다. 가장 낮게 발현된 유전자들의 경우 DESeq2 검정의 검정력이 낮으며, p-값들은 본질적으로 모두 귀무(null) 성분에서 나옵니다. 평균 발현량이 높아질수록 히스토그램에서 작은 p-값 정점의 높이가 높아지는데, 이는 검정의 검정력이 증가함을 반영합니다.\n이를 다중 검정 처리를 개선하는 데 사용할 수 있을까요? 그것이 가능하다는 것이 밝혀졌습니다. 한 가지 접근 방식은 독립 가설 가중치 부여(independent hypothesis weighting, IHW) 입니다 (Ignatiadis et al. 2016; Ignatiadis and Huber 2021)22.\n22 이 외에도 여러 접근 방식이 있습니다. 예를 들어 Korthauer 등 (2019)의 벤치마크 연구나 Ignatiadis와 Huber (2021) 논문의 인용 문헌들을 참조하십시오.\nlibrary(\"IHW\")\nihw_res = ihw(awde$pvalue, awde$baseMean, alpha = 0.1)\nrejections(ihw_res)\n\n\n[1] 4892\n이를 일반적인 (가중치를 부여하지 않은) 벤자미니-호크버그 방법과 비교해 봅시다:\npadj_BH = p.adjust(awde$pvalue, method = \"BH\")\nsum(padj_BH &lt; 0.1)\n\n\n[1] 4099\n가설 가중치를 사용하면 더 많은 기각(rejections)을 얻습니다. 이 데이터의 경우 차이가 두드러지긴 하지만 극적이진 않은데, 이는 이미 신호 대 잡음비가 꽤 높기 때문입니다. 처음부터 검정력이 낮은 다른 상황(예: 반복 수가 적거나, 데이터에 노이즈가 더 많거나, 처리 효과가 덜 급격한 경우)에서는 IHW를 사용하는 효과가 더 뚜렷할 수 있습니다.\nihw 함수에 의해 결정된 가중치들을 살펴볼 수 있습니다(그림 6.22).\nplot(ihw_res)\n\n그림 6.22: ihw 함수에 의해 결정된 가설 가중치. 여기서 함수의 기본 설정은 22개의 계층(strata)을 선택한 반면, 위에서의 수동 탐색(그림 6.20, 6.21)에서는 6개를 사용했습니다. 실제로는 이는 사소한 세부 사항입니다.\n직관적으로 여기서 일어나는 일은, IHW가 baseMean이 높은 가설 계층에 더 많은 가중치를 부여하고, 카운트가 매우 낮은 계층에는 낮은 가중치를 부여하기로 선택한 것입니다. 벤자미니-호크버그 방법은 특정한 제1종 오류 예산을 가지고 있는데, 이를 모든 가설에 똑같이 분배하는 대신, 여기서는 어차피 작은 fdr을 가질 가능성이 거의 없는 계층에서 예산을 가져와 많은 가설이 작은 fdr에서 기각될 수 있는 계층에 “투자”하는 것입니다.\n질문 6.23\n왜 그림 6.22는 하나의 곡선이 아니라 5개의 곡선을 보여주나요?\np-값 외에 추가적인 요약 통계량(우리의 경우 baseMean)에 의한 계층화 가능성은 많은 다중 검정 상황에서 존재합니다. 비공식적으로 말하자면, 우리는 그러한 이른바 공변량(covariate) 이 다음과 같아야 합니다:\n\n귀무 가설하에서 우리의 p-값과 통계적으로 독립적이어야 하지만,\n두 그룹 모델에서 사전 확률 (0) 및/또는 검정력(대립 가설 밀도 (f{})의 형태)에 대한 정보를 제공해야 합니다.\n\n이러한 요구 사항은 그림 6.18—6.21과 같은 진단 플롯을 통해 평가할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>가설 검정 (Hypothesis Testing)</span>"
    ]
  },
  {
    "objectID": "06-chap.html#이-장의-요약",
    "href": "06-chap.html#이-장의-요약",
    "title": "8  가설 검정 (Hypothesis Testing)",
    "section": "8.11 6.12 이 장의 요약",
    "text": "8.11 6.12 이 장의 요약\n우리는 단일 가설 검정 이면의 개념들을 탐구한 다음 다중 검정 으로 넘어갔습니다. 우리는 수많은 검정으로부터 나온 결과의 전체 분포를 고려할 수 있게 되면 단일 검정에서 얻은 단일 p-값을 해석할 때의 몇 가지 한계들을 어떻게 극복할 수 있는지 보았습니다. 또한 p-값 외에도 우리 데이터에 종종 추가적인 요약 통계량이 있다는 것도 보았습니다. 우리는 이를 정보성 공변량(informative covariates)이라 불렀으며, 이를 사용하여 p-값에 가중치를 부여하고 전반적으로 더 많거나 더 나은 발견을 얻는 방법을 살펴보았습니다.\n다중 검정 시나리오에서의 가설 검정 사용은 단일 검정 사례와는 상당히 다릅니다: 후자의 경우, 가설 검정은 말 그대로 (이상적으로는 사전에 지정된 가설과 데이터 분석 계획을 가진) 길고 비용이 많이 드는 데이터 획득 캠페인의 최종 결과이자 정점일 수 있습니다. 다중 검정의 경우, 그 결과는 종종 중간 단계일 뿐입니다: 방대한 초기 집합을 스크리닝하여 선택된, 가장 가치 있는 가설들의 하위 집합입니다. 이 하위 집합은 이후 더 세심한 분석을 통해 후속 연구가 진행됩니다.\n우리는 허위 발견율(FDR)의 개념을 보았습니다. 이것이 선택된 가설들의 하위 집합에 대한 평균적인 속성이라는 점을 명심하는 것이 중요합니다. 다른 평균들과 마찬가지로, 개별 가설에 대해서는 아무것도 말해주지 않습니다. 그리고 개별 가설에 실제로 적용되는 국소 허위 발견율(fdr)이라는 개념이 있습니다. 그러나 두 그룹 모델이 보여주었듯이, 국소 허위 발견율은 p-값과는 상당히 무관합니다. p-값에 대한 많은 혼란과 좌절은 사람들이 fdr이 담당하는 목적을 위해 p-값을 사용하고 싶어 한다는 사실에서 비롯되는 것 같습니다. 응용 과학의 아주 많은 부분이 국소 허위 발견율이 아닌 p-값에 집중하고 있다는 것은 아마도 역사적 일탈일지 모릅니다. 반면에 실무적인 이유도 있는데, p-값은 즉시 계산되는 반면, fdr은 강력한 모델링 가정 없이는 데이터로부터 추정하거나 제어하기 어렵기 때문입니다.\n우리는 진단 플롯의 중요성을 확인했습니다. 특히 다중 검정 분석을 마주할 때는 항상 p-값 히스토그램을 살펴보아야 합니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>가설 검정 (Hypothesis Testing)</span>"
    ]
  },
  {
    "objectID": "06-chap.html#더-읽을거리",
    "href": "06-chap.html#더-읽을거리",
    "title": "8  가설 검정 (Hypothesis Testing)",
    "section": "8.12 6.13 더 읽을거리",
    "text": "8.12 6.13 더 읽을거리\n\n다중 검정에 대한 포괄적인 교과서적 처리는 Efron (2010)에 의해 제공됩니다.\n임상 시험에서의 결과 전환(Outcome switching): http://compare-trials.org\n가설 가중치에 대해서는 IHW 비네트, IHW 논문 (Ignatiadis et al. 2016) 및 그 안의 참고 문헌들을 참조하십시오.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>가설 검정 (Hypothesis Testing)</span>"
    ]
  },
  {
    "objectID": "06-chap.html#연습-문제",
    "href": "06-chap.html#연습-문제",
    "title": "8  가설 검정 (Hypothesis Testing)",
    "section": "8.13 6.14 연습 문제",
    "text": "8.13 6.14 연습 문제\n연습 문제 6.1\n여러분의 과학적 전문 분야에서 다중 검정에 의존하는 응용 사례를 하나 찾아보십시오. 전형적인 데이터 세트를 찾아 p-값 히스토그램을 그려보세요. 가설들이 모두 교환 가능한가요, 아니면 하나 이상의 정보성 공변량이 있나요? 계층화된 히스토그램을 그려보세요.\n연습 문제\n수리 통계학자들은 대립 가설에 비해 왜 검정의 귀무 가설에 그렇게 많이 집중할까요?\n연습 문제 6.2\n우리는 어떻게 귀무 가설이 참임을 증명할 수 있을까요? 아니면 대립 가설이 참임을요?\n연습 문제 6.3\n6.5절 끝부분의 데이터 복제보다 덜 극단적인 상관된 검정 통계량의 예를 만들어 보세요. 오직 참인 귀무 가설들로만 데이터를 시뮬레이션하고, 어떤 연속형 제어 매개변수의 함수로서 데이터가 완전히 독립적인 반복(열)에서 고도로 상관된 상태로 변하도록 만드십시오. 이 제어 매개변수의 함수로서 (예를 들어 p-값 히스토그램을 통해) 제1종 오류 제어를 확인해 보세요.\n연습 문제 6.4\n발표된 문헌 중에서 p-값 해킹, 결과 전환, HARKing이 작용한 것처럼 보이는 사례를 하나 찾아보십시오.\n연습 문제 6.5\nFDR은 기댓값(expectation value)입니다. 즉, 절차의 평균적인 거동을 제어하고 싶을 때 사용됩니다. 최악의 경우(worst case)를 제어하기 위한 방법들이 있을까요?\n연습 문제 6.6\n벤자미니-호크버그 알고리즘의 메모리 및 시간 복잡도는 얼마인가요? IHW 방법은 어떤가요? 검정의 수 (m)의 함수로서 다항 함수를 적합시킬 수 있나요? 힌트: 가설 검정의 수를 늘려가며 데이터를 시뮬레이션하고, pryr::object_size나 동명의 패키지에 있는 microbenchmark와 같은 함수를 사용하여 시간 및 메모리 소비를 측정하고, 이들을 이중 로그 플롯(double-logarithmic plot)에서 (m)에 대해 플롯해 보세요.\nAltman, Naomi, and Martin Krzywinski. 2017. “Points of Significance: Interpreting p Values.” Nature Methods 14 (3): 213–14. https://doi.org/10.1038/nmeth.4210.\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society B 57: 289–300.\nEfron, Bradley. 2010. Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction. Cambridge University Press.\nHead, Megan L, Luke Holman, Rob Lanfear, Andrew T Kahn, and Michael D Jennions. 2015. “The Extent and Consequences of p-Hacking in Science.” PLoS Biology 13 (3): e1002106.\nIgnatiadis, Nikolaos, and Wolfgang Huber. 2021. “Covariate Powered Cross- Weighted Multiple Testing.” Journal of the Royal Statistical Society: Series B 83: 720–51. https://doi.org/10.1111/rssb.12411.\nIgnatiadis, Nikolaos, Bernd Klaus, Judith Zaugg, and Wolfgang Huber. 2016. “Data-Driven Hypothesis Weighting Increases Detection Power in Genome-Scale Multiple Testing.” Nature Methods 13: 577–80.\nKorthauer, K., P. K. Kimes, C. Duvallet, A. Reyes, A. Subramanian, M. Teng, C. Shukla, E. J. Alm, and S. C. Hicks. 2019. “A practical guide to methods controlling false discoveries in computational biology.” Genome Biology 20 (1): 118.\nSchweder, T., and E. Spjøtvoll. 1982. “Plots of P-values to Evaluate Many Tests Simultaneously.” Biometrika 69: 493–502. https://doi.org/10.1093/biomet/69.3.493.\nStorey, John D. 2003. “The Positive False Discovery Rate: A Bayesian Interpretation and the q-Value.” The Annals of Statistics 31 (6). https://doi.org/10.1214/aos/1074290335.\nWasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA’s Statement on p-Values: Context, Process, and Purpose.” The American Statistician.\n페이지는 R 버전 4.5.1 (2025-06-13)을 사용하여 2025-09-01 01:33에 빌드되었습니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>가설 검정 (Hypothesis Testing)</span>"
    ]
  },
  {
    "objectID": "07-chap.html",
    "href": "07-chap.html",
    "title": "9  다변량 분석 (Multivariate Analysis)",
    "section": "",
    "text": "9.1 7.2 데이터란 무엇인가? 행렬과 그 동기\n많은 데이터 세트는 동일한 대상(환자, 샘플 또는 유기체)에 대해 측정된 여러 변수로 구성됩니다. 예를 들어, 우리는 천 명의 환자에 대해 키, 몸무게, 나이와 같은 생체 특징뿐만 아니라 혈압, 혈당, 심박수와 같은 임상 변수 및 유전 데이터를 가질 수 있습니다. 다변량 분석(multivariate analysis)의 존재 이유(raison d’être)는 측정된 서로 다른 변수들 간의 연결 또는 연관성을 조사하는 것입니다. 대개 데이터는 각 대상에 대해 하나의 행, 각 변수가 하나의 열을 가진 표 형식의 데이터 구조로 보고됩니다. 이하에서는 각 변수가 수치형인 특수한 경우에 초점을 맞출 것이며, 따라서 R에서 데이터 구조를 행렬(matrix) 로 나타낼 수 있습니다.\n행렬의 열들이 모두 서로 독립적(무관)이라면, 우리는 단순히 각 열을 별도로 공부하고 표준 “단변량(univariate)” 통계 처리를 하나씩 수행할 수 있습니다; 이들을 행렬로 공부해서 얻는 이점은 없을 것입니다.\n더 자주, 패턴과 의존성이 존재할 것입니다. 예를 들어 세포 생물학에서 우리는 증식률(proliferation rate)이 많은 유전자의 발현에 동시에 영향을 미친다는 것을 알고 있습니다. 환자 유래 세포의 많은 샘플(행)에 대해 25,000개의 유전자(열) 발현을 공부하면서, 우리는 많은 유전자가 함께 작용하여 양(+)의 상관관계를 갖거나 음(-)의 상관관계를 갖는다는 것을 알아차립니다. 각 유전자를 별도로만 공부한다면 우리는 많은 중요한 정보를 놓칠 것입니다. 유전자들 사이의 중요한 연결은 우리가 데이터를 전체적으로 고려할 때만 감지 가능합니다: 각 행은 동일한 관찰 단위에 대해 이루어진 많은 측정값들을 나타냅니다. 하지만 한꺼번에 고려해야 할 25,000차원의 변동을 갖는 것은 벅찬 일입니다; 우리는 너무 많은 정보를 잃지 않으면서 우리 데이터를 더 적은 수의 가장 중요한 차원1으로 줄이는 방법을 보여줄 것입니다.\n1 우리는 아래에서 이러한 차원 축소(dimension reduction)의 아이디어를 훨씬 더 자세히 설명할 것입니다. 당분간은 우리가 4차원 세계에 살고 있다는 것을 기억합시다.\n이 장에서는 고처리량 실험에서 마주치는 다변량 데이터 행렬의 많은 예시뿐만 아니라, 여러분의 직관을 높여줄 좀 더 기초적인 예시들을 제시합니다. 우리는 이 장에서 차원 축소 방법인 주성분 분석(Principal Component Analysis), 약어로 PCA 에 초점을 맞출 것입니다. 우리는 알고리즘에 대한 기하학적 설명뿐만 아니라 PCA 분석의 출력을 해석하는 데 도움이 되는 시각화 자료들을 제공할 것입니다.\n이 장에서 우리는 다음을 수행할 것입니다:\n먼저, 측정값 표를 나타내는 데 사용되는 직사각형 행렬(matrices) 의 예시들을 살펴보겠습니다. 각 행렬에서 행과 열은 특정 개체를 나타냅니다.\n거북이(Turtles): 기본 원리를 이해하는 데 도움이 될 간단한 데이터 세트는 거북이(painted turtles)에 대한 세 가지 차원의 생체 측정값 행렬입니다 (Jolicoeur and Mosimann 1960).\n마지막 세 열은 길이 측정값(밀리미터 단위)인 반면, 첫 번째 열은 각 동물의 성별을 알려주는 요인 변수입니다.\n운동선수(Athletes): 이 행렬은 스포츠 세계의 흥미로운 예시입니다. 10종 경기(decathlon)의 10개 종목에 대한 33명 선수의 성적을 보고합니다: m100, m400 및 m1500은 각각 100미터, 400미터, 1500미터 달리기 시간(초)입니다; m110은 110미터 허들 완주 시간입니다; pole은 장대높이뛰기 높이, high와 long은 각각 높이뛰기와 멀리뛰기 결과이며 모두 미터 단위입니다; weight, disc, javel은 선수들이 던진 포환, 원반, 창의 거리(미터)입니다. 처음 세 명의 선수에 대한 변수들은 다음과 같습니다:\n세포 유형(Cell Types): Holmes 등 (2005)은 서로 다른 피험자로부터 정렬된 T 세포 모집단의 유전자 발현 프로필을 연구했습니다. 열은 유전자 발현 측정값의 하위 집합이며, 세포 유형 간에 차등 발현을 보이는 156개 유전자에 대응합니다.\n박테리아 종 풍부도(Bacterial Species Abundances): 카운트 행렬은 미생물 생태학 연구에서 사용됩니다(4장에서 보았듯이). 여기에서 열은 수치 태그로 식별되는 서로 다른 박테리아 종(또는 운영상의 분류 단위인 OTU)을 나타냅니다. 행은 그것들이 측정된 샘플에 따라 레이블이 붙어 있으며, (정수) 숫자는 각 샘플에서 각 OTU가 관찰된 횟수를 나타냅니다.\n행렬 항목이 0이 되는 경향에 주목하세요. 우리는 이러한 데이터를 희소(sparse) 하다고 부릅니다.\nmRNA 리드(reads): RNA-Seq 전사체 데이터는 각 생물학적 샘플에서 각 유전자2에 매핑된 시퀀싱 리드의 수를 보고합니다. 우리는 8장에서 이 유형의 데이터를 자세히 공부할 것입니다.\n2 또는 엑손과 같은 유전자 하위 구조.\nRNA-Seq 분야에서는 –위의 airway 데이터와 같이– 행에 유전자를, 열에 샘플을 보고하는 것이 관례입니다. 이러한 서로 다른 관례는 쉽게 오류를 초래하므로, 주의를 기울일 가치가 있습니다3. 단백질 프로필(Proteomic profiles): 여기서 열은 정렬된 질량 분석(mass spectroscopy) 피크 또는 그들의 (m/z)-비율을 통해 식별된 분자들입니다; 행렬의 항목들은 측정된 강도입니다4.\n3 바이오컨덕터 프로젝트는 이러한 관례가 명시적으로 고정된 데이터 컨테이너를 정의함으로써 사용자와 개발자가 그러한 모호함을 피할 수 있도록 돕고자 합니다. 8장에서 우리는 SummarizedExperiment 클래스의 예시를 보게 될 것입니다.\n4 더 자세한 내용은 예를 들어 위키백과를 참조하세요.\n여기서 보았던 많은 행렬들에서, 샘플(피험자)과 측정된 특징들에 대한 중요한 정보는 행이나 열 이름에 저장되어 있으며, 종종 어떤 임시방편적인 문자열 연결을 통해 이루어집니다. 이는 가용한 모든 정보를 저장하기에 최선의 장소가 아니며, 금방 한계에 부딪히고 오류를 일으키기 쉽습니다. 훨씬 더 나은 접근 방식은 바이오컨덕터의 SummarizedExperiment 클래스입니다.\n태스크\n질량 분석 실행 중 특정 (m/z) 점수에 대해 피크가 감지되지 않았을 때, metab에는 0이 기록되었습니다. 유사하게, GPOTUs나 airway 객체에서의 0은 일치하는 시퀀싱 리드가 감지되지 않았을 때 발생합니다. 이러한 데이터 행렬들에서 0의 빈도를 표로 만들어 보세요.\n질문 7.1",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>다변량 분석 (Multivariate Analysis)</span>"
    ]
  },
  {
    "objectID": "07-chap.html#데이터란-무엇인가-행렬과-그-동기",
    "href": "07-chap.html#데이터란-무엇인가-행렬과-그-동기",
    "title": "9  다변량 분석 (Multivariate Analysis)",
    "section": "",
    "text": "turtles = read.table(\"../data/PaintedTurtles.txt\", header = TRUE)\nturtles[1:4, ]\n\n\n  sex length width height\n1   f     98    81     38\n2   f    103    84     38\n3   f    103    86     42\n4   f    105    86     40\n\n\ndata(\"olympic\", package = \"ade4\")\nathletes = setNames(olympic$tab, \n  c(\"m100\", \"long\", \"weight\", \"high\", \"m400\", \"m110\", \"disc\", \"pole\", \"javel\", \"m1500\"))\nathletes[1:3, ]\n\n\n   m100 long weight high  m400  m110  disc pole javel  m1500\n1 11.25 7.43  15.48 2.27 48.90 15.13 49.28  4.7 61.32 268.95\n2 10.87 7.45  14.97 1.97 47.71 14.46 44.36  5.1 61.76 273.02\n3 11.18 7.44  14.20 1.97 48.29 14.81 43.66  5.2 64.16 263.20\n\nload(\"../data/Msig3transp.RData\")\nround(Msig3transp,2)[1:5, 1:6]\n\n\n             X3968 X14831 X13492 X5108 X16348  X585\nHEA26_EFFE_1 -2.61  -1.19  -0.06 -0.15   0.52 -0.02\nHEA26_MEM_1  -2.26  -0.47   0.28  0.54  -0.37  0.11\nHEA26_NAI_1  -0.27   0.82   0.81  0.72  -0.90  0.75\nMEL36_EFFE_1 -2.24  -1.08  -0.24 -0.18   0.64  0.01\nMEL36_MEM_1  -2.68  -0.15   0.25  0.95  -0.20  0.17\n\ndata(\"GlobalPatterns\", package = \"phyloseq\")\nGPOTUs = as.matrix(t(phyloseq::otu_table(GlobalPatterns)))\nGPOTUs[1:4, 6:13]\n\n\nOTU Table:          [4 taxa and 8 samples]\n                     taxa are rows\n        246140 143239 244960 255340 144887 141782 215972 31759\nCL3          0      7      0    153      3      9      0     0\nCC1          0      1      0    194      5     35      3     1\nSV1          0      0      0      0      0      0      0     0\nM31Fcsw      0      0      0      0      0      0      0     0\n\n\n\n행렬 항목이 0이 되는 경향에 주목하세요. 우리는 이러한 데이터를 희소(sparse)하다고 부릅니다.\n\n\n\n\n\nlibrary(\"SummarizedExperiment\")\ndata(\"airway\", package = \"airway\")\nassay(airway)[1:3, 1:4]\n\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513\nENSG00000000003        679        448        873        408\nENSG00000000005          0          0          0          0\nENSG00000000419        467        515        621        365\n\n\n\nmetab = t(as.matrix(read.csv(\"../data/metabolites.csv\", row.names = 1)))\nmetab[1:4, 1:4]\n\n\n         146.0985388 148.7053275 310.1505057 132.4512963\nKOGCHUM1    29932.36    17055.70     1132.82    785.5129\nKOGCHUM2    94067.61    74631.69    28240.85   5232.0499\nKOGCHUM3   146411.33   147788.71    64950.49  10283.0037\nWTGCHUM1   229912.57   384932.56   220730.39  26115.2007\n\n\n\n여기서 보았던 많은 행렬들에서, 샘플(피험자)과 측정된 특징들에 대한 중요한 정보는 행이나 열 이름에 저장되어 있으며, 종종 어떤 임시방편적인 문자열 연결을 통해 이루어집니다. 이는 가용한 모든 정보를 저장하기에 최선의 장소가 아니며, 금방 한계에 부딪히고 오류를 일으키기 쉽습니다. 훨씬 더 나은 접근 방식은 바이오컨덕터의 SummarizedExperiment 클래스입니다.\n\n\n\n\n\n\n\n이러한 데이터 행렬의 열들은 보통 무엇이라고 불리나요?\n이러한 각 예시에서 행렬의 행은 무엇인가요?\n행렬의 셀은 무엇을 나타내나요?\n데이터 행렬의 이름이 athletes이고 다섯 번째 선수의 세 번째 변수 값을 보고 싶다면, R에 무엇을 입력하시겠습니까?\n\n\n9.1.1 7.2.1 저차원 데이터 요약 및 준비\n\n그림 7.1: xkcd: 저차원이란 무엇을 의미할까요? 우리는 3차원에 살고 있으며, 시간을 포함하면 4차원입니다. 평면은 2차원이고, 선은 1차원입니다. 점은 0차원이라고 합니다. 만화에서 참조한 재미있는 소설은 Abbott (1884)를 참조하세요.\n만약 우리가 하나의 변수만을 공부한다면, 즉 거북이 행렬의 세 번째 열5만 본다면, 우리는 1차원 데이터를 보고 있는 것입니다. 그러한 벡터, 예를 들어 모든 거북이의 무게는 3.6절에서 보았던 히스토그램과 같은 플롯을 통해 시각화될 수 있습니다. 우리가 평균이나 중앙값과 같은 하나의 숫자 요약치를 계산한다면, 우리는 우리 1차원 데이터의 0차원 요약치를 만든 것입니다. 이것은 이미 차원 축소의 한 예입니다.\n5 행렬 (X)의 세 번째 열은 수학적으로 ({x}_{})로 표기하거나 R에서 X[, 3]을 사용하여 접근합니다.\n3장에서 우리는 2차원 산점도를 공부했습니다. 관측치가 너무 많을 때, 데이터를 (육각형) 빈으로 그룹화하는 것이 유익할 수 있음을 보았습니다: 이들은 2차원 히스토그램입니다. 일련의 관측치에 대해 함께 측정된 두 변수((x)와 (y))를 고려할 때, 상관 계수 는 변수들이 어떻게 공동 변동(co-vary)하는지를 측정합니다. 이는 2차원 데이터의 단일 숫자 요약치입니다. 그 공식은 요약치인 ({x})와 ({y})를 포함합니다:\n[ = { } ]\nIn R, we use the cor function to calculate its value. Applied to a matrix this function computes all the two way correlations between continuous variables. In Chapter 9 we will see how to analyse multivariate categorical data.\n질문 7.2\n거북이 데이터의 측정값들 사이의 모든 상관관계 행렬을 계산해 보세요. 무엇을 발견했나요?\n해결책\n범주형 변수를 제외하고 행렬을 계산합니다.\ncor(turtles[, -1])\n\n\n          length     width    height\nlength 1.0000000 0.9783116 0.9646946\nwidth  0.9783116 1.0000000 0.9605705\nheight 0.9646946 0.9605705 1.0000000\n우리는 이 정사각 행렬이 대칭이며 값들이 모두 1에 가깝다는 것을 알 수 있습니다. 대각선 값은 항상 1입니다.\n다음 두 질문에서 살펴보는 것과 같은 시각적 디스플레이를 사용하여 이러한 간단한 1차원 및 2차원 요약 통계량을 확인하는 것으로 다차원 분석을 시작하는 것이 항상 유익합니다.\n질문 7.3\n\n거북이에 대한 세 가지 생체 측정값에 대해, 대각선에 1차원 히스토그램을 포함한 모든 쌍별 산점도를 생성하세요. GGally 패키지를 사용하세요.\n이 데이터의 기저에 있는 “진정한 차원”을 추측해 보세요.\n\n해결책\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"GGally\")\nggpairs(turtles[, -1], axisLabels = \"none\")\n\n그림 7.2를 보면, 세 변수 모두 높은 상관관계를 가지고 있으며 대부분 거북이의 크기 로 해석될 수 있는 동일한 “기저” 변수를 반영하는 것으로 보입니다.\n질문 7.4\nathletes 데이터의 변수들 사이의 모든 쌍별 상관관계를 계산하고 그 행렬을 히트맵으로 표시해 보세요. 무엇을 알 수 있나요?\n해결책\nlibrary(\"pheatmap\")\npheatmap(cor(athletes), cell.width = 10, cell.height = 10)\n\n그림 7.3은 10개의 변수가 달리기, 던지기, 점프라는 그룹으로 어떻게 군집화되는지 보여줍니다.\n\n\n9.1.2 7.2.2 데이터 전처리\n많은 경우, 서로 다른 변수들은 서로 다른 단위로 측정되므로, 서로 다른 기준선과 서로 다른 스케일(scales)6을 가집니다. 이들은 원래의 형태로는 직접적으로 비교할 수 없습니다.\n6 스케일의 일반적인 척도는 범위와 표준 편차입니다. 예를 들어, 110미터 허들 시간은 표준 편차 0.51과 함께 14.18에서 16.2 사이인 반면, 1500미터 완주 시간은 표준 편차 13.66과 함께 256.64에서 303.17 사이입니다; 이는 10배 이상 더 큽니다. 더욱이 athletes 데이터는 서로 다른 단위(초, 미터)의 측정값도 포함하고 있는데, 이들의 선택은 임의적입니다(길이는 센티미터나 피트로 기록될 수도 있고, 시간은 밀리초로 기록될 수도 있습니다).\n따라서 PCA 및 다른 많은 방법들의 경우, 비교를 의미 있게 만들기 위해 수치들을 어떤 공통적인 스케일로 변환해야 합니다. 중앙화(Centering) 는 평균을 빼는 것을 의미하며, 중앙화된 데이터의 평균은 원점에 위치하게 됩니다. 스케일링(Scaling) 또는 표준화(Standardizing) 는 표준 편차로 나누는 것을 의미하며, 새로운 표준 편차는 (1)이 됩니다. 사실 우리는 상관 계수를 계산할 때(식 7.1) 이미 이러한 연산들을 접했습니다: 상관 계수는 단순히 중앙화되고 스케일링된 변수들의 벡터 곱입니다. 이러한 연산을 수행하기 위해 R에는 scale 함수가 있으며, 행렬이나 데이터 프레임이 주어졌을 때의 기본 거동은 모든 열의 평균을 0으로, 표준 편차를 1로 만드는 것입니다.\n질문 7.5\n\n거북이 데이터의 평균과 표준 편차를 계산한 다음, scale 함수를 사용하여 연속형 변수들을 중앙화하고 표준화하세요. 이를 scaledTurtles라고 부르고, scaledTurtles의 평균과 표준 편차의 새로운 값을 확인해 보세요.\n거북이 데이터의 스케일링 및 중앙화된 너비(width)와 높이(height) 변수의 산점도를 만들고 점들에 성별에 따른 색상을 입히세요.\n\n해결책\napply(turtles[,-1], 2, sd)\n\n\n   length     width    height \n20.481602 12.675838  8.392837 \n\n\napply(turtles[,-1], 2, mean)\n\n\n   length     width    height \n124.68750  95.43750  46.33333 \n\n\nscaledTurtles = scale(turtles[, -1])\napply(scaledTurtles, 2, mean)\n\n\n       length         width        height \n-1.432050e-18  1.940383e-17 -2.870967e-16 \n\n\napply(scaledTurtles, 2, sd)\n\n\nlength  width height \n     1      1      1 \n\n\ndata.frame(scaledTurtles, sex = turtles[, 1]) %&gt;%\n  ggplot(aes(x = width, y = height, group = sex)) +\n    geom_point(aes(color = sex)) + coord_fixed()\n\n우리는 이미 4장과 5장에서 log와 asinh 함수를 사용한 다른 데이터 변환 선택지들을 접했습니다. 이러한 변환의 목적은 (대개) 분산 안정화(variance stabilization)입니다. 즉, 동적 범위의 서로 다른 부분에서 서로 다른 변수 가 아닌 동일한 변수 의 반복 측정값 분산을 더 비슷하게 만드는 것입니다. 이와 대조적으로 위에서 설명한 표준화 변환은 서로 다른 변수 의 스케일(평균과 표준 편차로 측정됨)을 동일하게 만드는 것을 목표로 합니다.\n때로는 변수들이 진정으로 중요도가 다르기 때문에 서로 다른 스케일로 남겨두는 것이 더 나을 수도 있습니다. 만약 원래의 스케일이 유의미하다면, 데이터를 있는 그대로 두어야 합니다. 다른 경우에, 변수들은 사전적으로 알려진 서로 다른 정밀도를 가집니다. 9장에서 우리는 그러한 변수들에 가중치를 부여하는 여러 방법들을 살펴볼 것입니다.\n데이터를 전처리한 후, 우리는 차원 축소 를 통한 데이터 단순화 를 수행할 준비가 되었습니다.\n\n도움이 될 만한 챕터가 포함된 유용한 책으로는 기초적인 설명을 담은 Flury (1997)와 상세한 수학적 접근 방식을 담은 Mardia, Kent, Bibby (1979)가 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>다변량 분석 (Multivariate Analysis)</span>"
    ]
  },
  {
    "objectID": "07-chap.html#차원-축소",
    "href": "07-chap.html#차원-축소",
    "title": "9  다변량 분석 (Multivariate Analysis)",
    "section": "9.2 7.3 차원 축소",
    "text": "9.2 7.3 차원 축소\n우리는 여러 가지 서로 다른 관점에서 차원 축소를 설명할 것입니다. 이는 1901년에 Karl Pearson (Pearson 1901)에 의해 두 변수 산점도를 단일 좌표로 줄이는 방법으로 발명되었습니다. 1930년대에는 통계학자들에 의해 동일한 대상에 대해 수행된 일련의 심리 테스트 결과를 요약하는 데 사용되었습니다 (Hotelling 1933); 이로써 한꺼번에 테스트된 많은 변수들을 요약하는 종합 점수(overall scores)를 제공하게 되었습니다.\n\n\n\nPrincipal과 principle은 서로 다른 뜻을 가진 두 개의 다른 단어입니다. 따라서 이들을 혼동하지 마세요. PCA의 경우, 그것은 항상 principal입니다.\n\n\nPrincipal 과 principle 은 서로 다른 뜻을 가진 두 개의 다른 단어입니다. 따라서 이들을 혼동하지 마세요. PCA의 경우, 그것은 항상 principal 입니다.\n이러한 주요(principal) 점수라는 아이디어가 주성분 분석(Principal Component Analysis, 약어로 PCA)이라는 이름에 영감을 주었습니다. PCA는 군집화에서와 마찬가지로 모든 변수를 동일한 상태(status) 를 가진 것으로 취급하기 때문에 비지도 학습(unsupervised learning) 기법이라 불립니다. 우리는 한 특정한 변수의 값을 다른 변수들로부터 예측하거나 설명하려는 것이 아니라, 오히려 모든 변수들에 대한 기저 구조의 수학적 모델을 찾으려는 것입니다. PCA는 기본적으로 변수들 사이의 관계와 관측치들 사이의 관계를 유용한 방식으로 보여주는 맵(maps)을 생성하는 탐색적 기법입니다.\n우리는 먼저 이 다변량 분석이 데이터에 무엇을 하는지에 대한 맛보기를 제공하겠습니다. 선형 대수학을 통한 이러한 방법들의 우아한 수학적 공식화가 존재하지만, 여기서는 그 사용을 최소화하고 시각화와 데이터 예시에 집중할 것입니다.\n우리는 고차원 공간의 점들을 낮은 차원으로 투영하는 기하학적 투영(projections) 을 사용합니다. 그림 7.5는 벡터 ({v})에 의해 생성된 선 위로 점 (A)를 투영하는 것을 보여줍니다.\n\n그림 7.5: 점 (A)가 벡터 (v)에 의해 생성된 빨간색 선 위로 투영됩니다. 점선 투영선은 빨간색 선에 수직(또는 직교(orthogonal) )합니다. 투영선과 빨간색 선의 교차점을 점 (A)의 벡터 (v)에 의해 생성된 빨간색 선 위로의 직교 투영이라고 부릅니다.\nPCA는 선형(linear) 기법으로, 변수들 사이의 선형 관계를 찾고 원래 변수들의 선형 함수인 새로운 변수들을 사용한다는 것을 의미합니다 ((f(ax+by)=af(x)+b(y))). 선형성 제약 조건은 계산을 특히 쉽게 만듭니다. 9장에서 우리는 비선형 기법들을 살펴볼 것입니다.\n\n9.2.1 7.3.1 저차원 투영\n여기에 athletes 데이터를 사용하여 2차원 데이터를 선 위로 투영하는 한 가지 방법을 보여줍니다. 아래 코드는 그림 7.6을 생성하는 데 사용된 전처리 및 플로팅 단계를 제공합니다:\nathletes = data.frame(scale(athletes))\nath_gg = ggplot(athletes, aes(x = weight, y = disc)) +\n  geom_point(size = 2, shape = 21)\nath_gg + geom_point(aes(y = 0), colour = \"red\") +\n  geom_segment(aes(xend = weight, yend = 0), linetype = \"dashed\")\n\n그림 7.6: 가로 (x)축(()y=0으로 정의됨)으로의 투영을 빨간색으로 보여주는 두 변수의 산점도이며 투영선은 점선으로 나타납니다.\n태스크\n\n그림 7.6에서 빨간색 점들의 분산을 계산해 보세요.\n(y)축으로의 투영선과 투영된 점들을 보여주는 플롯을 만들어 보세요.\n수직 (y)축으로 투영된 점들의 분산을 계산해 보세요.\n\n\n\n9.2.2 7.3.2 2차원 데이터를 어떻게 선으로 요약할까요?\n일반적으로 우리가 2차원(평면)에서 1차원(선)으로 투영할 때 점들에 대한 정보를 잃게 됩니다. 그림 7.6에서 weight 변수에 대해 했던 것처럼 원래의 좌표만 사용한다면, 우리는 disc 변수에 대한 모든 정보를 잃게 됩니다. 우리의 목표는 두 변수 모두에 대해 가능한 한 많은 정보를 유지하는 것입니다. 사실 점 구름을 선 위로 투영하는 방법은 여러 가지가 있습니다. 하나는 회귀선(regression lines) 이라 알려진 것을 사용하는 것입니다. R에서 이러한 선들이 어떻게 구축되는지 살펴보겠습니다.\n\n9.2.2.1 한 변수를 다른 변수에 대해 회귀 분석하기\n선형 회귀(linear regression)를 보셨다면, 산점도를 요약하는 선을 계산하는 방법을 이미 알고 계실 것입니다; 선형 회귀 는 한 방향, 즉 반응 변수 방향의 잔차 제곱합을 최소화하는 것을 우선시하는 지도(supervised) 방법입니다.\n\n\n9.2.2.2 weight에 대한 disc 변수의 회귀 분석.\n그림 7.7에서는 회귀선을 찾기 위해 lm(선형 모델) 함수를 사용합니다. 그 기울기와 절편은 결과 객체 reg1의 coefficients 슬롯에 있는 값들에 의해 주어집니다.\nreg1 = lm(disc ~ weight, data = athletes)\na1 = reg1$coefficients[1] # intercept\nb1 = reg1$coefficients[2] # slope\npline1 = ath_gg + geom_abline(intercept = a1, slope = b1,\n    col = \"blue\", linewidth = 1.5)\npline1 + geom_segment(aes(xend = weight, yend = reg1$fitted),\n    colour = \"red\", arrow = arrow(length = unit(0.15, \"cm\")))\n\n그림 7.7: 파란색 선은 (빨간색으로 표시된) 수직 잔차의 제곱합을 최소화합니다.\n\n\n9.2.2.3 discus에 대한 weight의 회귀 분석.\n그림 7.8은 두 변수의 역할을 바꾸었을 때 생성되는 선을 보여줍니다; weight가 반응 변수가 됩니다.\nreg2 = lm(weight ~ disc, data = athletes)\na2 = reg2$coefficients[1] # intercept\nb2 = reg2$coefficients[2] # slope\npline2 = ath_gg + geom_abline(intercept = -a2/b2, slope = 1/b2,\n    col = \"darkgreen\", linewidth = 1.5)\npline2 + geom_segment(aes(xend=reg2$fitted, yend=disc),\n    colour = \"orange\", arrow = arrow(length = unit(0.15, \"cm\")))\n\n그림 7.8: 녹색 선은 (주황색으로 표시된) 수평 잔차의 제곱합을 최소화합니다.\n각각의 회귀선인 그림 7.7과 7.8은 disc와 weight 사이의 대략적인 선형 관계를 제공합니다. 하지만 그 관계는 우리가 어느 변수를 예측 변수로 선택하고 어느 변수를 반응 변수로 선택하느냐에 따라 달라집니다.\n질문 7.6\n그림 7.7의 파란색 회귀선 상에 있는 투영된 점들의 분산은 얼마나 큰가요? 이를 원래의 축인 weight와 disc로 투영했을 때의 데이터 분산과 비교해 보세요.\n해결책\n피타고라스의 정리는 직각 삼각형의 빗변 길이의 제곱이 다른 두 변의 길이의 제곱의 합과 같다는 것을 알려주며, 우리는 이를 다음과 같이 적용합니다:\nvar(athletes$weight) + var(reg1$fitted)\n\n\n[1] 1.650204\n변수들을 스케일링했으므로, 원래의 축인 weight와 disc를 따른 점들의 분산은 1입니다.\n\n\n9.2.2.4 양방향 거리를 최소화하는 선\n그림 7.9는 데이터 점들로부터의 직교(수직) 투영의 제곱합을 최소화하도록 선택된 선을 보여줍니다; 우리는 이를 주성분(principal component) 선이라고 부릅니다. 우리가 선을 맞춘 세 가지 방식(그림 7.7–7.9)이 모두 한 플롯에 표시된 결과가 그림 7.10에 나와 있습니다.\nxy = cbind(athletes$disc, athletes$weight)\nsvda = svd(xy)\npc = xy %*% svda$v[, 1] %*% t(svda$v[, 1])\nbp = svda$v[2, 1] / svda$v[1, 1]\nap = mean(pc[, 2]) - bp * mean(pc[, 1])\nath_gg + geom_segment(xend = pc[, 1], yend = pc[, 2]) +\n  geom_abline(intercept = ap, slope = bp, col = \"purple\", linewidth = 1.5)\n\n그림 7.9: 보라색 주성분 선은 직교 투영의 제곱합을 최소화합니다.\n\n그림 7.10: 파란색 선은 수직 잔차의 제곱합을 최소화하고, 녹색 선은 수평 잔차를 최소화하며, 주성분 이라 불리는 보라색 선은 직교 투영을 최소화합니다. 세 선의 기울기 순서에 주목하세요.\n질문 7.7\n\n보라색 선의 기울기에 어떤 특징이 있나요?\n원래의(스케일링되지 않은) 변수들에 대해 플롯을 다시 그려보세요. 어떤 일이 일어나나요?\n\n해결책\n여기서 계산된 선들은 단위의 선택에 의존합니다. 두 변수 모두에 대해 표준 편차를 1로 만들었기 때문에, PCA 선은 두 회귀선 사이의 정확히 중간을 가르는 대각선입니다. 데이터가 평균을 뺌으로써 중앙에 맞춰졌으므로, 선은 원점 ((0,0))을 통과합니다.\n질문 7.8\n보라색 선 위에 있는 점들의 분산을 계산해 보세요.\n해결책\n우리는 플롯을 만들 때 점들의 좌표를 계산했으며, 이들은 pc 벡터에 있습니다:\napply(pc, 2, var)\n\n\n[1] 0.9031761 0.9031761\n\n\nsum(apply(pc, 2, var))\n\n\n[1] 1.806352\n우리는 이 축을 따른 분산이 질문 7.6에서 계산한 다른 분산들보다 크다는 것을 알 수 있습니다.\n피타고라스의 정리는 여기서 두 가지 흥미로운 사실을 알려줍니다:\n\n수평 및 수직 방향 모두에서 최소화하고 있다면, 사실 우리는 각 점으로부터 선으로의 직교 투영을 최소화하고 있는 것입니다.\n점들의 전체 가변성은 점들을 무게 중심(데이터가 중앙화되어 있다면 원점(0,0))으로 투영한 제곱합으로 측정됩니다. 이를 점 구름의 전체 분산 또는 관성(inertia) 이라고 합니다. 이 관성은 선 위로의 투영 제곱합과 그 선을 따른 분산의 합으로 분해될 수 있습니다. 고정된 분산에 대해, 투영 거리를 최소화하는 것은 또한 그 선을 따른 분산을 최대화합니다. 종종 우리는 첫 번째 주성분을 분산이 최대인 선으로 정의합니다.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>다변량 분석 (Multivariate Analysis)</span>"
    ]
  },
  {
    "objectID": "07-chap.html#새로운-선형-결합",
    "href": "07-chap.html#새로운-선형-결합",
    "title": "9  다변량 분석 (Multivariate Analysis)",
    "section": "9.3 7.4 새로운 선형 결합",
    "text": "9.3 7.4 새로운 선형 결합\n이전 섹션에서 찾은 PC 선은 다음과 같이 쓸 수 있습니다.\n이미지 출처: Sara Holmes\n[ PC = + . ]\n주성분은 원래 측정된 변수들의 선형 결합(linear combinations) 이며, 새로운 좌표계 를 제공합니다. 선형 결합 이 실제로 무엇인지 이해하기 위해 비유를 들어보겠습니다. 건강한 주스 믹스를 만들 때 다음과 같은 레시피를 따를 것입니다:\n[ \\[\\begin{align} V &= 2 \\times \\text{Beet} + 1 \\times \\text{Carrot} \\\\ &\\+ \\tfrac{1}{2} \\text{Gala} + \\tfrac{1}{2} \\text{GrannySmith} \\\\ &\\+ 0.02 \\times \\text{Ginger} + 0.25 \\times \\text{Lemon}. \\end{align}\\] ]\n이 레시피는 개별 주스 유형(원래 변수들)의 선형 결합입니다. 결과는 새로운 변수 (V)이며, 계수 ((2,1,,,0.02,0.25))는 로딩(loadings) 이라고 불립니다.\n질문 7.9\n주스 한 잔의 칼로리를 어떻게 계산하시겠습니까?\n\n9.3.1 7.4.1 최적의 선\n변수들의 선형 결합은 우리가 2차원 산점도 평면에서 선을 구축했던 것과 같은 방식으로 고차원에서의 선을 정의합니다. 그 사례에서 보았듯이, 데이터를 투영할 선을 선택하는 방법은 여러 가지가 있지만, 우리의 목적에 부합하는 ‘최선의’ 선이 존재합니다.\n모든 변수에 걸친 모든 점의 전체 분산은 분해될 수 있습니다. PCA에서 우리는 점들과 임의의 선 사이의 거리 제곱합이 선까지의 거리와 선을 따른 분산으로 분해될 수 있다는 사실을 이용합니다.\n우리는 주성분이 선까지의 거리를 최소화하며, 또한 선을 따른 투영의 분산을 최대화한다는 것을 보았습니다.\n왜 선을 따른 분산을 최대화하는 것이 좋은 아이디어일까요? 3차원에서 2차원으로의 투영에 대한 또 다른 예시를 살펴봅시다. 사실, 인간의 시각은 그러한 차원 축소에 의존합니다:\n\n\n\n그림 7.11: 수수께끼의 실루엣.\n\n\n질문 7.10\n그림 7.11에는 3차원 물체의 2차원 투영이 있습니다. 이 물체는 무엇일까요?\n질문 7.11\n그림 7.11과 7.13 중 어느 투영이 더 정보가 많다고 생각하시나요? 그 이유는 무엇인가요?\n해결책\n그림자의 면적을 최대화하는 투영이 더 많은 ’정보’를 보여준다고 주장할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>다변량 분석 (Multivariate Analysis)</span>"
    ]
  },
  {
    "objectID": "07-chap.html#pca-워크플로",
    "href": "07-chap.html#pca-워크플로",
    "title": "9  다변량 분석 (Multivariate Analysis)",
    "section": "9.4 7.5 PCA 워크플로",
    "text": "9.4 7.5 PCA 워크플로\n\n그림 7.12: PCA 처리 과정 중 많은 선택이 이루어져야 합니다.\nPCA는 가장 큰 관성/가변성을 보여주는 축을 찾고, 그 방향의 가변성을 제거한 다음, 그 다음으로 좋은 직교 축을 찾는 과정을 반복하는 원리에 기반합니다. 사실 반복해서 실행할 필요 없이, 특잇값 분해(Singular Value Decomposition, SVD) 라고 불리는 하나의 선형 대수 연산으로 모든 축을 찾을 수 있습니다(이에 대한 자세한 내용은 아래에서 다룰 것입니다).\n그림 7.12의 다이어그램에서, 먼저 평균과 분산이 계산되고 공분산 행렬로 직접 작업할지 아니면 상관 행렬로 작업할지 선택해야 함을 알 수 있습니다. 다음 단계는 데이터에 유의미하다고 간주되는 성분의 수인 (k)를 선택하는 것입니다. 우리는 (k)를 근사(approximation)의 계수(rank)라고 말합니다. 최적의 (k)를 선택하는 것은 어려운 문제이며, 아래에서 어떻게 접근해야 할지 논의할 것입니다. (k)를 선택하려면 연속적인 주성분들에 의해 설명되는 분산의 플롯을 살펴보아야 합니다. 일단 (k)를 선택했다면, 새로운 (k)차원 하위 공간으로의 데이터 투영을 진행할 수 있습니다.\nPCA 워크플로의 최종 결과는 변수와 샘플 모두에 대한 유용한 맵(maps) 입니다. 이러한 맵들이 어떻게 구축되는지 이해하면 그로부터 얻을 수 있는 정보를 극대화할 수 있을 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>다변량 분석 (Multivariate Analysis)</span>"
    ]
  },
  {
    "objectID": "07-chap.html#pca의-내부-구조-계수-축소-rank-reduction",
    "href": "07-chap.html#pca의-내부-구조-계수-축소-rank-reduction",
    "title": "9  다변량 분석 (Multivariate Analysis)",
    "section": "9.5 7.6 PCA의 내부 구조: 계수 축소 (Rank reduction)",
    "text": "9.5 7.6 PCA의 내부 구조: 계수 축소 (Rank reduction)\n이 섹션은 선형 대수학 배경 지식이 희미한 기억으로만 남아 있는 분들을 위한 짧은 섹션입니다. 너무 많은 수식 없이 PCA의 기초가 되는 특잇값 분해 방법에 대한 직관을 제공하고자 합니다.\n\n그림 7.13: 그림 7.11에서 보여준 것과 동일한 물체의 또 다른 2차원 투영. 여기서의 관점이 더 정보가 많습니다. 일반적으로 점들의 퍼짐(다시 말해 분산)이 최대가 되도록 관점을 선택하는 것이 가장 많은 정보를 제공합니다. 우리는 가능한 한 많은 변동을 보고 싶어 하며, 그것이 바로 PCA가 하는 일입니다.\n행렬의 특잇값 분해는 수평 및 수직 벡터(특잇값 벡터라 불림)와 정규화 값(특잇값이라 불림)을 찾습니다. 이전과 마찬가지로, 분해를 생성하는 데 사용되는 실제 역설계(reverse engineering)를 수행하기 전에 순방향 생성(forward-generative) 설명을 먼저 제공하겠습니다. 각 단계의 의미를 보정하기 위해, 실제 데이터의 복잡함으로 넘어가기 전에 인위적인 예시부터 시작하겠습니다.\n\n9.5.1 7.6.1 계수 1 행렬 (Rank-one matrices)\n간단한 생성 모델은 행렬의 계수(rank of a matrix) 의 의미를 보여주고 우리가 이를 실제로 어떻게 찾는지 설명해 줍니다. 두 개의 벡터 (u)(1열 행렬)와 (v^t=t(v))(1행 행렬 — 1열 행렬 (v)의 전치)가 있다고 가정해 봅시다. 예를 들어, (u =(\n\\[\\begin{smallmatrix} 1\\\\2\\\\3\\\\4 \\end{smallmatrix}\\]\n))이고 (v =(\n\\[\\begin{smallmatrix} 2\\\\4\\\\8 \\end{smallmatrix}\\]\n))입니다. (v)의 전치는 (v^t = t(v) = (2; 4; 8))로 쓰여집니다. 우리는 다음과 같이 (u)의 복사본에 (v^t)의 각 원소를 차례로 곱합니다:\n단계 0:\n\n\n\nX\n2\n4\n8\n\n\n\n\n1\n\n\n\n\n\n2\n\n\n\n\n\n3\n\n\n\n\n\n4\n\n\n\n\n\n\n단계 1:\n\n\n\nX\n2\n4\n8\n\n\n\n\n1\n2\n\n\n\n\n2\n4\n\n\n\n\n3\n6\n\n\n\n\n4\n8\n\n\n\n\n\n단계 2:\n\n\n\nX\n2\n4\n8\n\n\n\n\n1\n2\n4\n\n\n\n2\n4\n8\n\n\n\n3\n6\n12\n\n\n\n4\n8\n16\n\n\n\n\n단계 3:\n\n\n\nX\n2\n4\n8\n\n\n\n\n1\n2\n4\n8\n\n\n2\n4\n8\n16\n\n\n3\n6\n12\n24\n\n\n4\n8\n16\n32\n\n\n\n따라서 행렬 (X)의 ((2,3)) 항목(entry), 즉 (x_{2,3})은 (u_2)에 (v_3)를 곱하여 얻어집니다. 우리는 이를 다음과 같이 쓸 수 있습니다:\n[ X=\n\\[\\begin{pmatrix} 2&4&8\\\\ 4&8&16\\\\ 6 &12&24\\\\ 8&16&32\\\nend{pmatrix} = u * t(v)= u * v^t \\tag{7.3}\\]\n\n여기서 우리가 얻은 행렬 \\(X\\)는 \\(u\\)와 \\(v\\) 모두 하나의 열을 가지므로 계수(rank)가 1이라고 합니다.\n\n\n\n질문 7.12\n\n왜 \\(X = u*v^t\\)라고 쓰는 것이 전체 행렬 \\(X\\)를 모두 적는 것보다 더 경제적이라고 말할 수 있을까요?\n\n\n\n해결책\n\n\n\n\\(X\\)는 \\(4\\times3=12\\)개의 원소를 가지는 반면, \\(u\\)와 \\(v\\)를 통해서는 단지 \\(4+3=7\\)개의 숫자로 표현될 수 있습니다. 이러한 압축은 \\(u\\)나 \\(v\\)가 더 길어질수록 더욱 인상적입니다.\n\n반면에, 아래에 주어진 3행 4열(12개 숫자)의 또 다른 행렬 \\(X\\)를 단순화하기 위해 그 과정을 거꾸로 수행하고 싶다고 가정해 봅시다. 정보의 손실 없이 항상 이를 벡터들의 곱으로 비슷하게 표현할 수 있을까요? 그림 7.14와 7.15에 표시된 다이어그램에서, 색상이 칠해진 상자들은 행렬 (7.4)의 셀에 있는 숫자들에 비례하는 면적을 가집니다.\n\n[![](imgs/SVD-mosaicXplot0.png)](imgs/SVD-mosaicXplot0.png \"그림 7.14: 일부 특수한 행렬들은 분해하기 쉬운 숫자들을 포함하고 있습니다. 이 다이어그램의 각 색상 사각형은 그 안의 숫자에 해당하는 면적을 가집니다.\")\n\n그림 7.14: 일부 특수한 행렬들은 분해하기 쉬운 숫자들을 포함하고 있습니다. 이 다이어그램의 각 색상 사각형은 그 안의 숫자에 해당하는 면적을 가집니다.\n\n\n\n질문 7.13\n\n여기에 우리가 분해하고 싶은 행렬 \\(X\\)가 있습니다.\n\n\\[ \\begin{array}{rrrrr} \\hline \\large X & x_{.,1} & x_{.,2} & x_{.,3} & x_{.,4} \\\\ \\hline x_{1.} & 780 & 936 & 1300 & 728\\\\ x_{2.} & 75 & 90 & 125 & 70\\\\ x_{3.} & 540 & 648 & 900 & 504\\\\ \\hline \\end{array} \\tag{7.4}\\]\n\n\\(X\\)는 그림 7.14에서 일련의 사각형들로 다시 그려졌습니다. 사각형의 변의 값들을 곱했을 때 해당 숫자들이 나오도록 하려면 흰색 \\(u\\)와 \\(v\\) 상자에 어떤 숫자들을 넣을 수 있을까요?\n\n\\(X\\)처럼 완벽하게 \"직사각형\"인 특수한 성질을 가진 행렬을 계수가 1이라고 합니다. 우리는 \\(X\\)의 숫자들을 사각형의 면적으로 나타낼 수 있으며, 여기서 사각형의 변들은 측면 벡터(\\(u\\)와 \\(v\\))에 있는 값들에 의해 주어집니다.\n\n![](imgs/SVD-mosaicXplot1.png):\\n\")\n\n(a)\n\n![](imgs/SVD-mosaicXplot2.png):\\n\")\n\n(b)\n\n![](imgs/SVD-mosaicXplot3.png):\\n\")\n\n(c)\n\n그림 7.15: 셀 안의 숫자들은 (a), (b), (c)에서 상응하는 여백(margins)의 곱과 같습니다. 우리는 여러 가지 방식으로 곱을 통해 셀을 만들 수 있습니다. (c)에서는 여백의 노름(norm)이 1이 되도록 강제합니다.\n\n그림 7.15에서 우리는 \\(X\\)의 분해가 유일하지 않다는 것을 알 수 있습니다: 벡터 \\(u\\)와 \\(v\\)에 대해 여러 가지 후보 선택지가 있습니다. 우리는 각 벡터 원소의 제곱합이 1이 되도록 요구함으로써 선택을 유일하게 만들 것입니다 (벡터 \\(v\\)와 \\(u\\)가 노름 1을 갖는다고 말합니다). 그러면 우리는 각 곱에 곱해줄, \\(X\\)의 \"전체적인 스케일\"을 나타내는 추가적인 숫자 하나를 추적해야 합니다. 이것이 우리가 왼쪽 상단 모서리에 넣은 값입니다. 이를 첫 번째 특잇값 \\(s_1\\)이라고 부릅니다. 아래 R 코드에서, 우리는 먼저 `u`, `v`, `s1`의 값을 알고 있다고 가정하고 시작하겠습니다. 나중에 우리를 위해 이들을 찾아주는 함수를 볼 것입니다. R에서 곱셈과 노름 속성을 확인해 봅시다:\n\n    \n    \n    X = matrix(c(780,  75, 540,\n                 936,  90, 648,\n                1300, 125, 900,\n                 728,  70, 504), nrow = 3)\n    u = c(0.8196, 0.0788, 0.5674)\n    v = c(0.4053, 0.4863, 0.6754, 0.3782)\n    s1 = 2348.2\n    sum(u^2)\n    \n    \n    [1] 1\n    \n    \n    sum(v^2)\n    \n    \n    [1] 1\n    \n    \n    s1 * u %*% t(v)\n    \n    \n         [,1] [,2] [,3] [,4]\n    [1,]  780  936 1300  728\n    [2,]   75   90  125   70\n    [3,]  540  648  900  504\n    \n    \n    X - s1 * u %*% t(v)\n    \n    \n             [,1]   [,2]   [,3]   [,4]\n    [1,] -0.03419 0.0745 0.1355 0.1221\n    [2,]  0.00403 0.0159 0.0252 0.0186\n    [3,] -0.00903 0.0691 0.1182 0.0982\n\n\n\n질문 7.14\n\nR에서 `svd(X)`를 시도해 보세요. `svd` 함수의 출력 구성 요소를 주의 깊게 살펴보세요. 이 호출로 생성된 행렬 열의 노름(norm)을 확인하세요. 위의 `s1` = 2348.2 값은 어디에서 왔습니까?\n\n\n\n해결책\n\n\n\n    \n    \n    svd(X)$u[, 1]\n    svd(X)$v[, 1]\n    sum(svd(X)$u[, 1]^2)\n    sum(svd(X)$v[, 1]^2)\n    svd(X)$d\n\n사실, 이 특정한 경우에는 운이 좋았습니다: 우리는 (우리가 신경 쓰는 수치적 정밀도 범위 내에서) 두 번째와 세 번째 특잇값이 0인 것을 볼 수 있습니다. 그래서 우리는 \\(X\\)가 **계수(rank)** 가 1이라고 말합니다. 일반적인 행렬 \\(X\\)의 경우, 이와 같은 두 벡터 곱으로 정확하게 작성할 수 있는 경우는 드뭅니다. 다음 하위 섹션에서는 계수가 1이 아닐 때 \\(X\\)를 어떻게 분해할 수 있는지 보여줍니다: 더 많은 조각이 필요할 뿐입니다.\n\n### 7.6.2 유일한 방식으로 그러한 분해를 어떻게 찾을 수 있습니까?\n\n위의 분해에는 수평 및 수직 특잇값 벡터와 대각선 모서리(특잇값이라 함)의 세 가지 요소가 있었습니다. 이들은 특잇값 분해 함수(`svd`)를 사용하여 찾을 수 있습니다. 예를 들면:\n\n    \n    \n    Xtwo = matrix(c(12.5, 35.0, 25.0, 25, 9, 14, 26, 18, 16, 21, 49, 32,\n           18, 28, 52, 36, 18, 10.5, 64.5, 36), ncol = 4, byrow = TRUE)\n    USV = svd(Xtwo)\n\n\n\n질문 7.15\n\n`svd` 함수를 호출한 결과인 `USV` 객체를 보십시오. 구성 요소는 무엇입니까?\n\n\n\n해결책\n\n\n\n    \n    \n    names(USV)\n    \n    \n    [1] \"d\" \"u\" \"v\"\n    \n    \n    USV$d\n    \n    \n    [1] 1.350624e+02 2.805191e+01 3.111680e-15 2.290270e-15\n\n따라서 135.1이 첫 번째 특잇값 `USV$d[1]`입니다.\n\n\n\n질문 7.16\n\n각각의 연속적인 특잇값 벡터 쌍이 `Xtwo`에 대한 근사를 어떻게 개선하는지 확인해 보세요. 세 번째와 네 번째 특잇값에 대해 무엇을 알 수 있나요?\n\n\n\n해결책\n\n\n\n    \n    \n    Xtwo - USV$d[1] * USV$u[, 1] %*% t(USV$v[, 1])\n    Xtwo - USV$d[1] * USV$u[, 1] %*% t(USV$v[, 1]) -\n           USV$d[2] * USV$u[, 2] %*% t(USV$v[, 2])\n\n세 번째와 네 번째 특잇값은 매우 작아서 (반올림 오차 범위 내에서) 근사를 개선하지 못하므로, 우리는 `Xtwo`의 계수가 2라고 결론 내릴 수 있습니다.\n\n다시 말하지만, `Xtwo`와 같은 계수가 2인 행렬을 계수 1인 행렬들의 합으로 쓰는 방법은 여러 가지가 있습니다: 유일성을 보장하기 위해 우리는 특잇값 벡터에 또 다른 7 조건을 부과합니다. 특잇값 분해의 출력 벡터들은 노름이 1일 뿐만 아니라, \\(U\\) 행렬의 각 열 벡터는 이전의 모든 열 벡터들과 직교(orthogonal)합니다. 우리는 이를 \\(u_{\\cdot 1} \\perp u_{\\cdot 2}\\)라고 쓰며, 이는 동일한 위치에 있는 값들의 곱의 합이 0임을 의미합니다: \\(\\sum_i u_{i1} u_{i2} = 0\\). \\(V\\) 행렬에 대해서도 마찬가지입니다.\n\n7 위에서 우리는 벡터의 노름을 1로 선택했었습니다.\n\n\n\n태스크\n\n\\(U\\)와 \\(V\\) 행렬의 교차 곱(cross product)을 계산하여 정규 직교성(orthonormality)을 확인해 보세요:\n\n    \n    \n    t(USV$u) %*% USV$u\n    t(USV$v) %*% USV$v\n\n우리의 `scaledTurtles` 행렬을 특잇값 분해해 봅시다.\n\n    \n    \n    turtles.svd = svd(scaledTurtles)\n    turtles.svd$d\n    \n    \n    [1] 11.746475  1.419035  1.003329\n    \n    \n    turtles.svd$v\n    \n    \n              [,1]       [,2]        [,3]\n    [1,] 0.5787981  0.3250273  0.74789704\n    [2,] 0.5779840  0.4834699 -0.65741263\n    [3,] 0.5752628 -0.8127817 -0.09197088\n    \n    \n    dim(turtles.svd$u)\n    \n    \n    [1] 48  3\n\n\n\n질문 7.17\n\n`svd` 출력으로부터 거북이 행렬에 대해 무엇을 결론지을 수 있나요?\n\n\n\n해결책\n\n\n\n`turtles.svd$v`의 첫 번째 열은 세 변수에 대한 계수가 거의 동일함을 보여줍니다. 다른 눈에 띄는 \"우연\"들은 다음과 같습니다:\n\n    \n    \n    sum(turtles.svd$v[,1]^2)\n    \n    \n    [1] 1\n    \n    \n    sum(turtles.svd$d^2) / 47\n    \n    \n    [1] 3\n\n계수들이 실제로는 \\(\\sqrt{1/3}\\)이며 특잇값의 제곱합이 \\((n-1)\\times p\\)와 같음을 알 수 있습니다.\n\n### 7.6.3 특잇값 분해 (Singular value decomposition)\n\n![](imgs/SumRankOneD.png)\n\n\\(X\\)는 계수 1인 조각들로 가산적으로(additively) 분해됩니다. 각 \\(u\\) 벡터는 \\(U\\) 행렬로 결합되고, 각 \\(v\\) 벡터는 \\(V\\) 행렬로 결합됩니다. **특잇값 분해** 는 다음과 같습니다:\n\n\\[ \\mathbf{X} = U S V^t, V^t V={\\mathbb I}, U^t U={\\mathbb I}, \\tag{7.5}\\]\n\n여기서 \\(S\\)는 특잇값들의 대각 행렬(diagonal matrix)이고, \\(V^t\\)는 \\(V\\)의 전치 행렬이며, \\({\\mathbb I}\\)는 단위 행렬(Identity matrix)입니다. 식 7.5는 원소별로 다음과 같이 쓰여질 수 있습니다:\n\n\\[ X_{ij} = u_{i1}s_1v_{1j} + u_{i2}s_2v_{2j} + u_{i3}s_3v_{3j} +... + u_{ir}s_rv_{rj}, \\]\n\n\\(U\\)와 \\(V\\)는 그들 자신의 교차 곱이 단위 행렬이므로 정규 직교(orthonormal) 8라고 합니다.\n\n8 정규 분포(normal distribution)와는 아무런 관련이 없으며, 직교(orthogonal)하면서 노름이 1임을 나타냅니다.\n\n### 7.6.4 주성분 (Principal components)\n\n특잇값 분해(R의 `svd` 함수에서 제공)에서 얻은 특잇값 벡터는 원래의 변수 앞에 붙여서 우리가 주성분이라고 부르는 더 정보가 많은 변수들을 만드는 계수들을 포함하고 있습니다. 우리는 이를 다음과 같이 씁니다:\n\n\\[ Z_1=v_{11} X_{\\cdot 1} +v_{21} X_{\\cdot 2} + v_{31} X_{\\cdot 3}+ \\cdots + v_{p1} X_{\\cdot p}. \\]\n\n만약 `usv = svd(X)`라면, \\((v_{11},v_{21},v_{31},...)\\)은 `usv$v`의 첫 번째 열에 의해 주어집니다; 유사하게 \\(Z_2\\)는 `usv$v`의 두 번째 열에 의해 주어지는 식입니다. \\(p\\)는 \\(X\\)의 열의 수이자 \\(V\\)의 행의 수입니다. 이러한 새로운 변수 \\(Z_1, Z_2, Z_3, ...\\)는 크기가 감소하는 분산을 가집니다: \\(s_1^2 \\geq s_2^2 \\geq s_3^2 \\geq ...\\).\n\n\n\n질문 7.18\n\n첫 번째 특잇값 `d[1]`과 `u[,1]`을 곱하여 거북이 데이터의 첫 번째 주성분을 계산해 보세요. 이를 계산하는 또 다른 방법은 무엇일까요?\n\n\n\n해결책\n\n\n\n다음 코드를 사용하여 이를 보여줍니다:\n\n    \n    \n    US = turtles.svd$u[,1, drop = FALSE] %*% turtles.svd$d[1]\n    XV = scaledTurtles %*% turtles.svd$v[, 1, drop = FALSE]\n    max(abs(US-XV))\n\n우리는 또한 행렬 대수를 사용하여 \\(XV\\)와 \\(US\\)가 같음을 알 수 있습니다. \\(V\\)가 직교하므로 \\(V^t V={\\mathbb I}\\)이고 \\(XV = USV^tV=US\\,{\\mathbb I}\\)임을 기억하세요.\n\n_참고:_ 아래 코드의 첫 번째 줄에 있는 `drop = FALSE` 인수는 선택된 행렬 열이 _matrix_ / _array_ 클래스 속성을 유지하도록 하여 행렬 곱셈 연산이 가능하도록 보장합니다. 대안으로 일반 곱셈 연산자 `*`를 사용할 수도 있습니다. 두 번째 줄에서 `drop = FALSE`는 엄밀히 말해 필요하지 않지만 대칭을 위해 넣었습니다.\n\n여기에 두 가지 유용한 사실이 있습니다. 먼저 말로 설명하고, 그 다음 수학적 기호로 나타내겠습니다.\n\n주성분의 수 \\(k\\)는 항상 원래 변수의 수나 관측치 수보다 적게 선택됩니다. 우리는 문제의 차원을 \"낮추고\" 있는 것입니다:\n\n\\[ k\\leq \\min(n,p). \\]\n\n주성분 변환은 첫 번째 주성분이 가능한 가장 큰 분산을 갖도록(즉, 데이터의 가변성을 가능한 한 많이 설명하도록) 정의되며, 각 후속 성분은 이전 성분들과 직교해야 한다는 제약 조건 하에서 순차적으로 가장 높은 분산을 갖습니다:\n\n\\[ \\max_{aX \\perp bX}\\mbox{var}(\\mbox{Proj}_{aX} (X)), \\qquad \\mbox{여기서 } bX=\\mbox{이전 성분들} \\]\n\n## 7.7 주평면에 관측치 플롯하기\n\n우리는 `discus`와 `weight` 변수가 있는 두 변수 athletes 데이터를 다시 살펴봅니다. 7.3.2절에서 우리는 첫 번째 주성분을 계산했고 이를 그림 7.10에서 보라색 선으로 나타냈습니다. 우리는 \\(Z_1\\)이 대각선에 의해 주어진 선형 결합임을 보여주었습니다. 계수들의 제곱합이 1이 되어야 하므로, 우리는 다음과 같은 식을 얻습니다:\n\\[Z_1=-0.707*\\text{athletes\\$disc}- 0.707*\\text{athletes\\$weight}.\\]\n\n이는 두 좌표가 \\(c_1=0.7071\\) 및 \\(c_2=0.7071\\)인 것과 같습니다.\n\n\n\n질문 7.19\n\n`svd` 함수의 출력 중 어떤 부분이 PC **로딩(loadings)** 이라고도 알려진 첫 번째 PC 계수로 우리를 인도하나요?\n\n우리가 두 변수 `discus`와 `weight`에 적용했던 `svda`를 사용한다는 점에 유의하세요.\n\n\n\n해결책\n\n\n\n    \n    \n    svda$v[,1]\n    \n    \n    [1] -0.7071068 -0.7071068\n\n보라색 선을 가로 \\(x\\)축으로 만들어 `(discus, weight)` 평면을 회전시키면, 첫 번째 **주평면(principal plane)** 이라고 알려진 것을 얻게 됩니다.\n\n    \n    \n    ppdf = tibble(PC1n = -svda$u[, 1] * svda$d[1],\n                  PC2n =  svda$u[, 2] * svda$d[2])\n    gg = ggplot(ppdf, aes(x = PC1n, y = PC2n)) +\n        geom_point() +\n        geom_hline(yintercept = 0, color = \"purple\", linewidth = 1.5, alpha = 0.5) +\n        xlab(\"PC1 \")+ylab(\"PC2\") + xlim(-3.5, 2.7) + ylim(-2, 2) + coord_fixed()\n    gg + geom_point(aes(x = PC1n, y = 0), color = \"red\") +\n         geom_segment(aes(xend = PC1n, yend = 0), color = \"red\")\n    gg + geom_point(aes(x = 0, y = PC2n), color = \"blue\") +\n         geom_segment(aes(yend = PC2n, xend = 0), color = \"blue\") +\n         geom_vline(xintercept = 0, color = \"skyblue\", linewidth = 1.5, alpha = 0.5)\n\n[![](07-chap_files/figure-html/fig-pcablue-1.png)](07-chap_files/figure-html/fig-pcablue-1.png \"그림 7.16 (a): \")\n\n(a)\n\n[![](07-chap_files/figure-html/fig-pcablue-2.png)](07-chap_files/figure-html/fig-pcablue-2.png \"그림 7.16 (b): \")\n\n(b)\n\n그림 7.16: 원래 변수가 두 개뿐인 경우, PCA 변환은 단순한 회전입니다. 새로운 좌표는 항상 가로 및 세로 축으로 선택됩니다.\n\n\n\n질문 7.20\n\n  1. 그림 7.16에서 빨간색 세그먼트의 제곱합의 평균은 무엇과 같습니까?  \n\n  2. 이것이 빨간색 점들의 분산과 비교하여 어떤가요?  \n\n  3. 그림 7.16에서 파란색 세그먼트와 빨간색 세그먼트의 표준 편차 비율을 계산해 보세요. 이를 첫 번째와 두 번째 특잇값의 비율과 비교해 보세요.\n\n\n\n해결책\n\n\n\n  1. 빨간색 세그먼트의 제곱합은 두 번째 특잇값의 제곱에 해당합니다:\n\n    \n    \n    sum(ppdf$PC2n^2)\n    \n    \n    [1] 6.196729\n    \n    \n    svda$d[2]^2\n    \n    \n    [1] 6.196729\n\n빨간색 세그먼트의 평균이 0이므로, 위의 수치들은 또한 분산에 비례합니다:\n\n    \n    \n    mean(ppdf$PC2n)\n    \n    \n    [1] 5.451106e-16\n    \n    \n    var(ppdf$PC2n) * (nrow(ppdf)-1)\n    \n    \n    [1] 6.196729\n\n  2. 빨간색 점들의 분산은 `var(ppdf$PC1n)`이며, 이는 첫 번째 PC의 설계상 우리가 a)에서 계산한 것보다 큽니다.\n\n    \n    \n    var(ppdf$PC1n)\n    \n    \n    [1] 1.806352\n    \n    \n    var(ppdf$PC2n)\n    \n    \n    [1] 0.1936478\n\n  3. 우리는 수직축과 수평축 상의 점들에 의해 설명되는 표준 편차의 비율을 다음과 같이 계산합니다:\n\n    \n    \n    sd(ppdf$PC1n) / sd(ppdf$PC2n)\n    \n    \n    [1] 3.054182\n    \n    \n    svda$d[1] / svda$d[2]\n    \n    \n    [1] 3.054182\n\n\n\n태스크\n\n`prcomp`를 사용하여 athletes 데이터의 처음 두 열에 대한 PCA를 수행하고, 출력을 살펴보세요. 특잇값 분해와 비교해 보세요.\n\n### 7.7.1 거북이 데이터의 PCA\n\n이제 거북이 데이터에 대해 완전한 PCA 분석을 수행해 보고자 합니다. 우리는 이미 1차원 및 2차원 데이터에 대한 요약 통계량을 살펴보았습니다. 이제 이러한 재스케일링된 데이터의 \"진정한\" 차원성에 대한 질문에 답해 볼 것입니다.\n\nIn the following code, we use the function `princomp`. Its return value is a list of all the important pieces of information needed to plot and interpret a PCA.\n\n[![사실 PCA는 매우 기초적인 기법이라서 다양한 R 패키지에 많은 서로 다른 구현체들이 존재합니다. 불행하게도 입력 인수와 출력의 형식 및 명칭이 표준화되어 있지 않으며, 일부는 출력의 스케일링에 대해 서로 다른 관례를 사용하기도 합니다. 우리는 이러한 선택들에 익숙해지기 위해 몇 가지 서로 다른 것들을 실험해 볼 것입니다.](imgs/devil.png)](imgs/devil.png \"사실 PCA는 매우 기초적인 기법이라서 다양한 R 패키지에 많은 서로 다른 구현체들이 존재합니다. 불행하게도 입력 인수와 출력의 형식 및 명칭이 표준화되어 있지 않으며, 일부는 출력의 스케일링에 대해 서로 다른 관례를 사용하기도 합니다. 우리는 이러한 선택들에 익숙해지기 위해 몇 가지 서로 다른 것들을 실험해 볼 것입니다.\")\n\n사실 PCA는 매우 기초적인 기법이라서 다양한 R 패키지에 많은 서로 다른 구현체들이 존재합니다. 불행하게도 입력 인수와 출력의 형식 및 명칭이 표준화되어 있지 않으며, 일부는 출력의 스케일링에 대해 서로 다른 관례를 사용하기도 합니다. 우리는 이러한 선택들에 익숙해지기 위해 몇 가지 서로 다른 것들을 실험해 볼 것입니다.\n\n    \n    \n    cor(scaledTurtles)\n    \n    \n              length     width    height\n    length 1.0000000 0.9783116 0.9646946\n    width  0.9783116 1.0000000 0.9605705\n    height 0.9646946 0.9605705 1.0000000\n    \n    \n    pcaturtles = princomp(scaledTurtles)\n    pcaturtles\n    \n    \n    Call:\n    princomp(x = scaledTurtles)\n    \n    Standard deviations:\n       Comp.1    Comp.2    Comp.3\n    1.6954576 0.2048201 0.1448180\n    \n     3  variables and  48 observations.\n    \n    \n    library(\"factoextra\")\n    fviz_eig(pcaturtles, geom = \"bar\", bar_width = 0.4) + ggtitle(\"\")\n\n[![](07-chap_files/figure-html/fig-PCAturtles-1.png)](07-chap_files/figure-html/fig-PCAturtles-1.png \"그림 7.17: 스크리 플롯(screeplot)은 표준화된 거북이 데이터(scaledTurtles)에 대한 고윳값을 보여줍니다: 하나의 큰 값과 두 개의 작은 값이 있습니다. 데이터는 (거의) 1차원적입니다. 우리는 왜 이 차원을 크기의 축(axis of size)이라고 부르는지 보게 될 것인데, 이는 생체 데이터에서 흔히 나타나는 현상입니다 \\Политика конфиденциальности\\).\")\n\n그림 7.17: 스크리 플롯(screeplot)은 표준화된 거북이 데이터(`scaledTurtles`)에 대한 고윳값을 보여줍니다: 하나의 큰 값과 두 개의 작은 값이 있습니다. 데이터는 (거의) 1차원적입니다. 우리는 왜 이 차원을 **크기의 축(axis of size)** 이라고 부르는지 보게 될 것인데, 이는 생체 데이터에서 흔히 나타나는 현상입니다 ([Jolicoeur and Mosimann 1960](16-chap.html#ref-Jolicoeur1960)).\n\n\n\n질문 7.21\n\n많은 PCA 함수들이 서로 다른 시기에 서로 다른 분야에서 일했던 다양한 팀들에 의해 만들어졌습니다. 이는 특히 명명 규칙이 다르기 때문에 혼란을 줄 수 있습니다. 세 가지를 비교해 봅시다. 다음 코드 라인들을 실행하고 결과 객체들을 살펴보세요:\n\n    \n    \n    svd(scaledTurtles)$v[, 1]\n    prcomp(turtles[, -1])$rotation[, 1]\n    princomp(scaledTurtles)$loadings[, 1]\n    library(\"ade4\")\n    dudi.pca(turtles[, -1], nf = 2, scannf = FALSE)$c1[, 1]\n\n`prcomp`와 `princomp` 함수에서 스케일링(scaling)을 비활성화하면 어떤 일이 일어나나요?\n\n이후 내용에서는 항상 행렬 \\(X\\)가 중앙화되고 스케일링된 행렬을 나타낸다고 가정합니다.\n\n\n\n질문 7.22\n\n`prcomp` 함수(결과를 `res`라고 합시다)에서 새로운 변수들에 대한 관측치의 좌표는 결과의 `scores` 슬롯에 있습니다. 거북이 데이터에 대한 PC1을 살펴보고 이를 `res$scores`와 비교해 보세요. 표준 편차 `sd1`을 `res` 객체에 있는 값 및 점수(scores)들의 표준 편차와 비교해 보세요.\n\n\n\n해결책\n\n\n\n    \n    \n    res = princomp(scaledTurtles)\n    PC1 = scaledTurtles %*% res$loadings[,1]\n    sd1 = sqrt(mean(res$scores[, 1]^2))\n\n\n\n질문 7.23\n\n`res$scores` 행렬의 직교성(orthogonality)을 확인해 보세요. 왜 이것이 **정규 직교(orthonormal)** 한다고 말할 수 없을까요?\n\n이제 우리는 PC 점수(\\(US\\))와 로딩 계수(\\(V\\))를 결합할 것입니다. 샘플과 변수가 모두 표시된 플롯을 **바이플롯(biplots)** 이라고 합니다. 이는 다음 **[factoextra](https://cran.r-project.org/web/packages/factoextra/)** 패키지 함수를 사용하여 한 줄로 수행할 수 있습니다.\n\n    \n    \n    fviz_pca_biplot(pcaturtles, label = \"var\", habillage = turtles[, 1]) +\n      ggtitle(\"\")\n\n[![](07-chap_files/figure-html/fig-turtlebiplot-1.png)](07-chap_files/figure-html/fig-turtlebiplot-1.png \"그림 7.18: 변수와 관측치를 모두 보여주는 처음 두 차원의 바이플롯. 화살표는 변수를 나타냅니다. 거북이들은 성별에 따라 레이블이 붙어 있습니다. 가로 방향으로 길게 뻗은 것은 두 번째 고윳값보다 훨씬 큰 첫 번째 고윳값의 크기 때문입니다.\")\n\n그림 7.18: 변수와 관측치를 모두 보여주는 처음 두 차원의 바이플롯. 화살표는 변수를 나타냅니다. 거북이들은 성별에 따라 레이블이 붙어 있습니다. 가로 방향으로 길게 뻗은 것은 두 번째 고윳값보다 훨씬 큰 첫 번째 고윳값의 크기 때문입니다.\n\n[![PCA를 플로팅할 때 종횡비(aspect ratio)에 주의하십시오. 두 성분의 노름이 비슷한 경우는 드물기 때문에, 정사각형 형태의 플롯은 예외적일 것입니다. 가로(첫 번째) 주성분이 두 번째보다 더 중요하다는 것을 보여주는 길쭉한 형태의 플롯이 더 일반적입니다. 이는 예를 들어 플롯에서 점들 사이의 거리를 해석할 때 중요합니다.](imgs/devil.png)](imgs/devil.png \"PCA를 플로팅할 때 종횡비(aspect ratio)에 주의하십시오. 두 성분의 노름이 비슷한 경우는 드물기 때문에, 정사각형 형태의 플롯은 예외적일 것입니다. 가로(첫 번째) 주성분이 두 번째보다 더 중요하다는 것을 보여주는 길쭉한 형태의 플롯이 더 일반적입니다. 이는 예를 들어 플롯에서 점들 사이의 거리를 해석할 때 중요합니다.\")\n\nPCA를 플로팅할 때 종횡비(aspect ratio)에 주의하십시오. 두 성분의 노름이 비슷한 경우는 드물기 때문에, 정사각형 형태의 플롯은 예외적일 것입니다. 가로(첫 번째) 주성분이 두 번째보다 더 중요하다는 것을 보여주는 길쭉한 형태의 플롯이 더 일반적입니다. 이는 예를 들어 플롯에서 점들 사이의 거리를 해석할 때 중요합니다.\n\n\n\n질문 7.24\n\nPC1이 가로축인 PCA 플롯에서 너비보다 높이가 더 길게 나타나는 것이 가능할까요?\n\n\n\n해결책\n\n\n\nPC1 방향으로의 점들의 분산은 \\(\\lambda_1=s_1^2\\)이며 이는 항상 \\(\\lambda_2=s_2^2\\)보다 큽니다. 따라서 PCA 플롯은 항상 높이보다 너비가 더 넓을 것입니다.\n\n\n\n질문 7.25\n\n그림 7.18을 보고 답해 보세요: a) 수컷과 암컷 거북이 중 어느 쪽이 더 큰 경향이 있나요?  \nb) 화살표들은 상관관계에 대해 무엇을 말해주나요?\n\n\n\n질문 7.26\n\n각 새로운 좌표의 분산을 PCA `dudi.pca` 함수가 반환하는 고윳값(eigenvalues)과 비교해 보세요.\n\n\n\n해결책\n\n\n\n    \n    \n    pcadudit = dudi.pca(scaledTurtles, nf = 2, scannf = FALSE)\n    apply(pcadudit$li, 2, function(x) sum(x^2)/48)\n    \n    \n         Axis1      Axis2\n    2.93573765 0.04284387\n    \n    \n    pcadudit$eig\n    \n    \n    [1] 2.93573765 0.04284387 0.02141848\n\n이제 이른바 상관관계 원(correlation circle)을 그려서 기존 변수와 새로운 변수 사이의 관계를 살펴봅니다. 여기서는 종횡비가 1이며 변수들은 그림 7.19에서 보듯이 화살표로 표현됩니다. 화살표의 길이는 첫 번째 주평면으로의 투영 품질을 나타냅니다:\n\n    \n    \n    fviz_pca_var(pcaturtles, col.circle = \"black\") + ggtitle(\"\") +\n      xlim(c(-1.2, 1.2)) + ylim(c(-1.2, 1.2))\n\n[![](07-chap_files/figure-html/fig-\nturtlesCirclef-1.png)](07-chap_files/figure-html/fig-turtlesCirclef-1.png \"그림 7.19: 원래 변수들을 보여주는 \"상관관계 원\"의 일부. 서로 간의 상관관계 및 새로운 주성분과의 상관관계는 벡터들 사이의 각도와 축과 벡터 사이의 각도로 주어집니다.\")\n\n그림 7.19: 원래 변수들을 보여주는 \"상관관계 원\"의 일부. 서로 간의 상관관계 및 새로운 주성분과의 상관관계는 벡터들 사이의 각도와 축과 벡터 사이의 각도로 주어집니다.\n\n\n\n질문 7.27\n\n우리 거북이 데이터 행렬의 행 수와 다음 숫자들 사이의 관계를 설명해 보세요:\n\n    \n    \n    svd(scaledTurtles)$d/pcaturtles$sdev\n    sqrt(47)\n\n\n\n해결책\n\n\n\n분산-공분산 행렬을 계산할 때, 많은 구현체들은 분모로 \\(1/(n-1)\\)을 사용합니다. 여기서 \\(n=48\\)이므로 분산의 합은 48/47의 인자만큼 차이가 납니다.\n\n이 데이터는 때때로 데이터의 거의 모든 변동이 더 낮은 차원의 공간에 캡처될 수 있다는 좋은 예입니다: 여기서는 3차원 데이터가 본질적으로 하나의 선으로 대체될 수 있습니다. 다음을 명심하세요: \\(X^tC=VSU^tUS=VS^2.\\) **주성분** 은 행렬 \\(C=US\\)의 열들입니다. \\(U\\)(\\(\\text{svd}\\) 함수의 출력에서 `USV$u`로 주어지는 행렬)의 \\(p\\)개 열들은 노름 \\((s_1^2, s_2^2, ..., s_p^2)\\)을 갖도록 스케일이 조정됩니다. 각 열은 자신이 설명해야 할 **책임** 이 있는 서로 다른 분산을 가집니다. 이들은 감소하는 숫자들이 될 것임에 주목하세요.\n\n만약 첫 번째 것만 원한다면 그것은 단지 \\(c_1=s_1 u_1\\)입니다. \\(||c_1||^2=s_1^tu_1 u_1^t s_1= s_1^2 u_1^tu_1=s_1^2=\\lambda_1\\)임을 주목하세요.\n\n만약 행렬 \\(X\\)가 \\(n\\)개의 서로 다른 샘플이나 표본에 대한 연구로부터 나왔다면, 주성분은 그림 7.16에서처럼 이 \\(n\\)개 점들에 대한 새로운 좌표를 제공합니다. 이들은 때때로 PCA 함수의 결과에서 **점수(scores)** 라고 불립니다.\n\n[![](imgs/xkcdEigenVectors.jpg)](imgs/xkcdEigenVectors.jpg \"그림 7.20: 또 다른 훌륭한 xkcd의 시각: 이번에는 고유벡터입니다.\")\n\n그림 7.20: 또 다른 훌륭한 xkcd의 시각: 이번에는 고유벡터입니다.\n\n더 자세한 예제로 들어가기 전에, SVD와 PCA가 제공하는 것을 요약해 봅시다:\n\n  * 각 주성분은 해당 고윳값(상응하는 특잇값의 제곱)으로 측정되는 분산을 가집니다.\n\n  * 새로운 변수들은 서로 직교하도록 만들어집니다. 이들은 또한 중앙화되어 있으므로, 이는 서로 상관관계가 없음을 의미합니다. 정규 분포 데이터의 경우, 이는 또한 서로 독립임을 의미합니다.\n\n  * 변수들이 스케일링되었을 때, 모든 변수의 분산 합은 변수의 수(\\( = p\\))와 같습니다. 분산의 합은 교차 곱 행렬의 대각선을 더하여 계산됩니다9.\n\n  * 주성분들은 고윳값의 크기 순서대로 정렬됩니다. 우리는 몇 개의 성분을 유지할지 결정하기 전에 항상 스크리 플롯을 확인합니다. 또한 그림 7.18에서 했던 것처럼 각 PC 축에 그것이 설명하는 분산의 비율을 주석으로 다는 것이 가장 좋은 관행입니다.\n\n**고유 분해(Eigen Decomposition):** X와 그 자신과의 교차 곱은\n\\[X^tX=VSU^tUSV^t=VS^2V^t=V\\Lambda V^t\\]\n를 만족하며, 여기서 \\(V\\)는 대칭 행렬 \\(X^tX\\)의 고유벡터 행렬이라 불리고 \\(\\Lambda\\)는 \\(X^tX\\)의 고윳값들의 대각 행렬입니다.\n\n9 이 대각 원소들의 합을 행렬의 **대각합(trace)** 이라고 부릅니다.\n\n\n\n태스크\n\n위키백과에서 고윳값(eigenvalue)을 찾아보세요. 공식을 사용하지 않고 이를 정의하는 문장을 찾아보세요. 왜 고유벡터가 신데렐라에서 사용될 수 있을까요(약간 억지스럽게 말이죠)? (그림 7.20의 xkcd 만화를 참조하세요.)\n\n### 7.7.2 완전한 분석: 10종 경기 선수들\n\n우리는 이 장의 앞부분에서 이 데이터를 살펴보기 시작했습니다. 여기서는 단계별로 완전한 다변량 분석을 수행해 볼 것입니다. 먼저, 이변량 연관성을 포착하는 (소수점 이하 2자리까지 반올림된) 상관 행렬을 다시 한번 살펴봅시다. 우리는 이미 그림 7.3에서 이를 컬러 히트맵으로 플롯했습니다.\n\n    \n    \n    cor(athletes) |&gt; round(2)\n    \n    \n            m100  long weight  high  m400  m110  disc  pole javel m1500\n    m100    1.00 -0.54  -0.21 -0.15  0.61  0.64 -0.05 -0.39 -0.06  0.26\n    long   -0.54  1.00   0.14  0.27 -0.52 -0.48  0.04  0.35  0.18 -0.40\n    weight -0.21  0.14   1.00  0.12  0.09 -0.30  0.81  0.48  0.60  0.27\n    high   -0.15  0.27   0.12  1.00 -0.09 -0.31  0.15  0.21  0.12 -0.11\n    m400    0.61 -0.52   0.09 -0.09  1.00  0.55  0.14 -0.32  0.12  0.59\n    m110    0.64 -0.48  -0.30 -0.31  0.55  1.00 -0.11 -0.52 -0.06  0.14\n    disc   -0.05  0.04   0.81  0.15  0.14 -0.11  1.00  0.34  0.44  0.40\n    pole   -0.39  0.35   0.48  0.21 -0.32 -0.52  0.34  1.00  0.27 -0.03\n    javel  -0.06  0.18   0.60  0.12  0.12 -0.06  0.44  0.27  1.00  0.10\n    m1500   0.26 -0.40   0.27 -0.11  0.59  0.14  0.40 -0.03  0.10  1.00\n\n그다음 고윳값에 대한 스크리 플롯을 살펴봅시다. 이는 이 데이터의 본질을 나타내기 위해 계수 \\(k\\)를 선택하는 데 도움을 줄 것입니다.\n\n    \n    \n    pca.ath = dudi.pca(athletes, scannf = FALSE)\n    pca.ath$eig\n    \n    \n     [1] 3.4182381 2.6063931 0.9432964 0.8780212 0.5566267 0.4912275 0.4305952\n     [8] 0.3067981 0.2669494 0.1018542\n    \n    \n    fviz_eig(pca.ath, geom = \"bar\", bar_width = 0.3) + ggtitle(\"\")\n\n[![](07-chap_files/figure-html/fig-screeplota-1.png)](07-chap_files/figure-html/fig-screeplota-1.png \"그림 7.21: athletes 데이터의 스크리 플롯은 데이터 변동의 대부분이 처음 두 주성분에 의해 확장된 2차원 평면에 캡처될 수 있음을 나타냅니다.\")\n\n그림 7.21: `athletes` 데이터의 스크리 플롯은 데이터 변동의 대부분이 처음 두 주성분에 의해 확장된 2차원 평면에 캡처될 수 있음을 나타냅니다.\n\nThe screeplot in Figure 7.21 shows a clear drop in the eigenvalues after the second one. This indicates that a good approximation will be obtained at rank 2. Let’s look at an interpretation of the first two axes by projecting the loadings of the original variables onto the two new ones, the principal components.\n\n    \n    \n    fviz_pca_var(pca.ath, col.var = \"blue\", repel = TRUE) + ggtitle(\"\")\n\n[![](07-chap_files/figure-html/fig-athletecorr-1.png)](07-chap_files/figure-html/fig-athletecorr-1.png \"그림 7.22: 원래 변수들의 상관관계 원.\")\n\n그림 7.22: 원래 변수들의 상관관계 원.\n\nThe correlation circle Figure 7.22 displays the projection of the original variables onto the two first new axes. The angles between vectors are interpreted as correlations. On the right side of the plane, we have the track and field events (m110, m100, m400, m1500), and on the left, we have the throwing and jumping events. Maybe there is an opposition of skills as characterized in the correlation matrix. We did see the correlations were negative between variables from these two groups. How can we interpret this?\n\nIt seems that those who throw the best have lower scores in the track competitions. In fact, if we look at the original measurements, we can see what is happening. The athletes who run in short times are the stronger ones, as are the ones who throw or jump longer distances. We should probably change the scores of the track variables and redo the analysis.\n\n\n\n질문 7.28\n\n최고의 운동 성적이 동일한 방향으로 변하도록, 즉 대부분 양(+)의 상관관계를 갖도록 하려면 변수들을 어떻게 변환해야 할까요?\n\n\n\n해결책\n\n\n\n달리기 성적의 부호를 바꾸면 거의 모든 변수가 양의 상관관계를 갖게 됩니다.\n\n    \n    \n    runningvars = grep(\"^m\", colnames(athletes), value = TRUE)\n    runningvars\n    \n    \n    [1] \"m100\"  \"m400\"  \"m110\"  \"m1500\"\n    \n    \n    athletes[, runningvars] = -athletes[, runningvars]\n    cor(athletes) |&gt; round(2)\n    \n    \n           m100 long weight high  m400 m110  disc pole javel m1500\n    m100   1.00 0.54   0.21 0.15  0.61 0.64  0.05 0.39  0.06  0.26\n    long   0.54 1.00   0.14 0.27  0.52 0.48  0.04 0.35  0.18  0.40\n    weight 0.21 0.14   1.00 0.12 -0.09 0.30  0.81 0.48  0.60 -0.27\n    high   0.15 0.27   0.12 1.00  0.09 0.31  0.15 0.21  0.12  0.11\n    m400   0.61 0.52  -0.09 0.09  1.00 0.55 -0.14 0.32 -0.12  0.59\n    m110   0.64 0.48   0.30 0.31  0.55 1.00  0.11 0.52  0.06  0.14\n    disc   0.05 0.04   0.81 0.15 -0.14 0.11  1.00 0.34  0.44 -0.40\n    pole   0.39 0.35   0.48 0.21  0.32 0.52  0.34 1.00  0.27  0.03\n    javel  0.06 0.18   0.60 0.12 -0.12 0.06  0.44 0.27  1.00 -0.10\n    m1500  0.26 0.40  -0.27 0.11  0.59 0.14 -0.40 0.03 -0.10  1.00\n    \n    \n    pcan.ath = dudi.pca(athletes, nf = 2, scannf = FALSE)\n    pcan.ath$eig\n    \n    \n     [1] 3.4182381 2.6063931 0.9432964 0.8780212 0.5566267 0.4912275 0.4305952\n     [8] 0.3067981 0.2669494 0.1018542\n\n이제 모든 음의 상관관계는 상당히 작아졌습니다. 행렬의 고윳값은 위의 부호 반전에 의해 영향을 받지 않으므로 스크리 플롯은 변하지 않을 것입니다. 바뀌는 출력값은 부호를 반전시킨 변수들에 대한 주성분 로딩(loadings) 계수의 부호뿐입니다.\n\n    \n    \n    fviz_pca_var(pcan.ath, col.var = \"blue\", repel = TRUE) + ggtitle(\"\")\n\n[![](07-chap_files/figure-html/fig-athletecorrn-1.png)](07-chap_files/figure-html/fig-athletecorrn-1.png \"그림 7.23: 달리기 변수들의 부호를 바꾼 후의 상관관계 원.\")\n\n그림 7.23은 변환된 변수들의 상관관계 원을 보여줍니다. 이제 우리는 넓은 공통의 전체 축을 가지고 있음을 알 수 있습니다: 모든 화살표가 대략 같은 방향을 가리키고 있습니다.\n\n이제 다음 코드를 사용하여 주평면에 투영된 운동선수들을 플롯합니다:\n\n    \n    \n    fviz_pca_ind(pcan.ath, repel = TRUE) + ggtitle(\"\")\n\n[![](07-chap_files/figure-html/fig-athletepc-1.png)](07-chap_files/figure-html/fig-athletepc-1.png \"그림 7.24: 운동선수들의 투영을 보여주는 첫 번째 주평면. 숫자들의 조직 방식에서 무언가 주목할 점이 있나요?\")\n\n그림 7.24: 운동선수들의 투영을 보여주는 첫 번째 주평면. 숫자들의 조직 방식에서 무언가 주목할 점이 있나요?\n\n\n\n질문 7.29\n\n그림 7.24에 표시된 운동선수들 자체를 살펴보면, 약간의 순서 효과를 알 수 있습니다. 그림 7.24에서 운동선수들의 성적과 그들의 번호 매기기 사이의 관계가 보이나요?\n\n\n\n해결책\n\n\n\n번호 순서대로 점들을 연결해 보면, 번호가 무작위로 매겨졌을 때보다 플롯의 한쪽 면에서 더 많은 시간을 보낸다는 것을 깨닫게 될 것입니다.\n\n`olympic` 데이터 세트에는 보완적인 정보가 포함되어 있음이 밝혀졌습니다. `score`라고 불리는 추가 벡터 변수는 1988년 올림픽 남자 10종 경기의 최종 점수를 보고합니다.\n\n    \n    \n    olympic$score\n    \n    \n     [1] 8488 8399 8328 8306 8286 8272 8216 8189 8180 8167 8143 8114 8093 8083 8036\n    [16] 8021 7869 7860 7859 7781 7753 7745 7743 7623 7579 7517 7505 7422 7310 7237\n    [31] 7231 7016 6907\n\n그러면 운동선수들의 첫 번째 주성분 좌표와 이 점수를 비교하는 산점도를 살펴봅시다. 이는 그림 7.25에 나와 있습니다. 두 변수 사이의 강한 상관관계를 볼 수 있습니다. 1번 선수(실제로 올림픽 10종 경기 금메달을 딴 선수)가 가장 높은 점수를 얻었지만, PC1에서 가장 높은 값을 가진 것은 아님을 알 수 있습니다. 왜 그렇다고 생각하시나요?\n\n    \n    \n    ggplot(data = tibble(pc1 = pcan.ath$li[, 1], score = olympic$score, label = rownames(athletes)),\n           mapping = aes(y = score, x = pc1)) +\n       geom_text(aes(label = label)) + stat_smooth(method = \"lm\", se = FALSE)\n\n[![](07-chap_files/figure-html/fig-\nAthleteScorePCA-1.png)](07-chap_files/figure-html/fig-AthleteScorePCA-1.png \"그림 7.25: olympic$score와 첫 번째 주성분 사이의 산점도. 점들은 데이터 세트에서의 순서로 레이블이 붙어 있습니다. 강한 상관관계를 볼 수 있습니다. 왜 완벽한 선형 적합이 아닐까요?\")\n\n그림 7.25: `olympic$score`와 첫 번째 주성분 사이의 산점도. 점들은 데이터 세트에서의 순서로 레이블이 붙어 있습니다. 강한 상관관계를 볼 수 있습니다. 왜 완벽한 선형 적합이 아닐까요?\n\n### 7.7.3 차원 수 k를 어떻게 선택할까요?\n\n[![](07-chap_files/figure-html/fig-screeploteq-1.png)](07-chap_files/figure-html/fig-screeploteq-1.png \"그림 7.26: '위험할 정도로' 비슷한 분산들을 보여주는 스크리 플롯. 80%의 분산을 엄격한 임계값으로 끊기로 선택한다면 불안정한 PC 플롯을 얻게 될 것입니다. 그러한 임계값이 없다면, 3개의 유사한 고윳값을 가진 3D 하위 공간에 대응하는 축들은 불안정하며 개별적으로 해석될 수 없습니다.\")\n\n그림 7.26: '위험할 정도로' 비슷한 분산들을 보여주는 스크리 플롯. 80%의 분산을 엄격한 임계값으로 끊기로 선택한다면 불안정한 PC 플롯을 얻게 될 것입니다. 그러한 임계값이 없다면, 3개의 유사한 고윳값을 가진 3D 하위 공간에 대응하는 축들은 불안정하며 개별적으로 해석될 수 없습니다.\n\n우리는 예제들에서 PCA의 첫 번째 단계가 새로운 변수들의 분산(**고윳값** 과 같음)에 대한 스크리 플롯을 만드는 것임을 보았습니다. 이 플롯을 보기 전까지는 얼마나 많은 차원이 필요한지 결정할 수 없습니다. 그 이유는 주성분이 명확하게 정의되지 않는 상황이 있기 때문입니다: 그림 7.26과 같은 스크리 플롯을 주면서 두세 개의 연속된 PC가 매우 유사한 분산을 가질 때, 유사한 고윳값 그룹에 대응하는 하위 공간이 존재합니다. 이 경우 이는 \\(u_2, u_3, u_4\\)에 의해 생성된 3D 공간이 될 것입니다. 이 벡터들은 개별적으로는 의미가 없으며 그 로딩을 해석할 수 없습니다. 이는 하나의 관측치에서 아주 약간의 변화만 있어도 완전히 다른 세 개의 벡터 세트를 얻을 수 있기 때문입니다. 이들은 동일한 3D 공간을 생성하겠지만, 매우 다른 로딩을 가질 수 있습니다. 우리는 이러한 PC들이 **불안정하다** 고 말합니다.\n\n## 7.8 탐색적 도구로서의 PCA: 추가 정보 사용하기\n\n우리는 회귀 분석과 달리 PCA가 모든 변수를 동일하게 취급한다는 것을 보았습니다(동등한 표준 편차를 갖도록 전처리된 범위 내에서). 그러나 결과를 해석하는 데 도움을 주기 위해 다른 연속형 변수나 범주형 요인을 플롯에 매핑하는 것은 여전히 가능합니다. 종종 우리는 샘플에 대한 보완적인 정보를 가지고 있습니다. 예를 들어 당뇨병 데이터의 진단 레이블이나 T 세포 유전자 발현 데이터의 세포 유형 등이 있습니다.\n\n여기서는 우리의 해석에 정보를 주기 위해 그러한 추가 변수들을 어떻게 사용할 수 있는지 살펴봅니다. 그러한 이른바 _메타데이터(metadata)_ 를 저장하기에 가장 좋은 장소는 데이터 객체의 적절한 슬롯(바이오컨덕터 _SummarizedExperiment_ 클래스 등)입니다; 두 번째로 좋은 장소는 수치 데이터도 포함하는 데이터 프레임의 추가 열입니다. 실제로 그러한 정보는 종종 행렬의 행 이름에 어느 정도 암호 같은 방식으로 저장됩니다. 아래에서는 후자의 시나리오에 직면해야 하며, 우리는 `substr` 기교를 사용하여 세포 유형을 추출하고 그림 7.27의 스크리 플롯과 그림 7.28의 PCA를 보여줍니다.\n\n    \n    \n    pcaMsig3 = dudi.pca(Msig3transp, center = TRUE, scale = TRUE,\n                        scannf = FALSE, nf = 4)\n    fviz_screeplot(pcaMsig3) + ggtitle(\"\")\n\n[![](07-chap_files/figure-html/fig-tcellexpr-1.png)](07-chap_files/figure-html/fig-tcellexpr-1.png \"그림 7.27: T 세포 발현 PCA 스크리 플롯.\")\n\n그림 7.27: T 세포 발현 PCA 스크리 플롯.\n\n    \n    \n    ids = rownames(Msig3transp)\n    celltypes = factor(substr(ids, 7, 9))\n    status = factor(substr(ids, 1, 3))\n    table(celltypes)\n    \n    \n    celltypes\n    EFF MEM NAI\n     10   9  11\n    \n    \n    cbind(pcaMsig3$li, tibble(Cluster = celltypes, sample = ids)) %&gt;%\n    ggplot(aes(x = Axis1, y = Axis2)) +\n      geom_point(aes(color = Cluster), size = 5) +\n      geom_hline(yintercept = 0, linetype = 2) +\n      geom_vline(xintercept = 0, linetype = 2) +\n      scale_color_discrete(name = \"Cluster\") + coord_fixed()\n\n[![](07-chap_files/figure-html/fig-tcelltypes-1-1.png)](07-chap_files/figure-html/fig-tcelltypes-1-1.png \"그림 7.28: 세 가지 별도의 T 세포 유형(effector, naïve, memory) 각각의 특이성에 관여하는 156개 유전자 하위 집합에 대한 유전자 발현 PCA. 다시 한 번, 첫 번째 축이 분산의 많은 부분을 설명하므로 플롯이 첫 번째 축을 따라 길게 뻗어 있음을 알 수 있습니다. T 세포 중 하나가 레이블이 잘못 붙은 것으로 보인다는 점에 주목하세요.\")\n\n그림 7.28: 세 가지 별도의 T 세포 유형(effector, naïve, memory) 각각의 특이성에 관여하는 156개 유전자 하위 집합에 대한 유전자 발현 PCA. 다시 한 번, 첫 번째 축이 분산의 많은 부분을 설명하므로 플롯이 첫 번째 축을 따라 길게 뻗어 있음을 알 수 있습니다. T 세포 중 하나가 레이블이 잘못 붙은 것으로 보인다는 점에 주목하세요.\n\n### 7.8.1 질량 분석 데이터 분석\n\n이러한 데이터는 관련 특성을 열로, 샘플을 행으로 하는 원하는 행렬을 얻기 전에 섬세한 전처리가 필요합니다. 가공되지 않은 질량 분석 측정값에서 시작하여, 단계에는 관련 특성의 피크 추출, 여러 샘플에 걸친 정렬 및 피크 높이 추정이 포함됩니다. 끔찍한 세부 사항에 대해서는 바이오컨덕터 **[xcms](https://bioconductor.org/packages/xcms/)** 패키지의 비네트를 참고하시기 바랍니다. 우리는 그러한 방식으로 생성된 데이터 행렬을 `mat1xcms.RData` 파일로부터 불러옵니다. 아래 코드의 출력은 그림 7.29와 7.30에 있습니다.\n\n    \n    \n    load(\"../data/mat1xcms.RData\")\n    dim(mat1)\n    \n    \n    [1] 399  12\n    \n    \n    pcamat1 = dudi.pca(t(mat1), scannf = FALSE, nf = 3)\n    fviz_eig(pcamat1, geom = \"bar\", bar_width = 0.7) + ggtitle(\"\")\n\n[![](07-chap_files/figure-html/fig-xset3scree-1.png)](07-chap_files/figure-html/fig-xset3scree-1.png \"그림 7.29: 생쥐 데이터에 대한 고윳값을 보여주는 스크리 플롯.\")\n\n그림 7.29: 생쥐 데이터에 대한 고윳값을 보여주는 스크리 플롯.\n\n    \n    \n    dfmat1 = cbind(pcamat1$li, tibble(\n        label = rownames(pcamat1$li),\n        number = substr(label, 3, 4),\n        type = factor(substr(label, 1, 2))))\n    pcsplot = ggplot(dfmat1,\n      aes(x=Axis1, y=Axis2, label=label, group=number, colour=type)) +\n     geom_text(size = 4, vjust = -0.5)+ geom_point(size = 3)+ylim(c(-18,19))\n    pcsplot + geom_hline(yintercept = 0, linetype = 2) +\n      geom_vline(xintercept = 0, linetype = 2)\n\n[![](07-chap_files/figure-html/fig-\nStretchedbiplot-1.png)](07-chap_files/figure-html/fig-Stretchedbiplot-1.png \"그림 7.30: mat1 데이터에 대한 첫 번째 주평면. 분산의 59%를 설명합니다.\")\n\n그림 7.30: `mat1` 데이터에 대한 첫 번째 주평면. 분산의 59%를 설명합니다.\n\n\n\n질문 7.30\n\n그림 7.30을 보면, 샘플들이 평면에 무작위로 배치된 것처럼 보이나요? 레이블에 의해 설명되는 어떤 구조를 알아차릴 수 있나요?\n\n\n\n해결책\n\n\n\n이 플롯을 만들어 보면 답이 (더욱) 명확해집니다. 녹아웃(Knockouts)은 항상 쌍을 이루는 야생형(wildtype) 샘플 아래에 있습니다. 우리는 다음 장에서 지도 방식의 다변량 방법(supervised multivariate methods)을 살펴볼 때 이 예시를 다시 방문할 것입니다.\n\n    \n    \n    pcsplot + geom_line(colour = \"red\")\n\n### 7.8.2 바이플롯 및 스케일링\n\n이전 예제에서는 측정된 변수의 수가 너무 많아서 변수와 샘플을 유용하게 동시에 플로팅할 수 없었습니다. 이 예제에서는 범주형 `wine.class` 변수도 있는 서로 다른 와인에 대해 화학적 측정이 이루어진 간단한 데이터 세트의 PCA 바이플롯을 그립니다. 우리는 2차원 상관관계와 변수들의 히트맵을 살펴보는 것으로 분석을 시작합니다.\n\n    \n    \n    library(\"pheatmap\")\n    load(\"../data/wine.RData\")\n    load(\"../data/wineClass.RData\")\n    wine[1:2, 1:7]\n    \n    \n      Alcohol MalicAcid  Ash AlcAsh  Mg Phenols Flav\n    1   14.23      1.71 2.43   15.6 127    2.80 3.06\n    2   13.20      1.78 2.14   11.2 100    2.65 2.76\n    \n    \n    pheatmap(1 - cor(wine), treeheight_row = 0.2)\n\n[![](07-chap_files/figure-html/fig-WineHeatplot-1.png)](07-chap_files/figure-html/fig-WineHeatplot-1.png \"그림 7.31: 1과 상관관계의 차이는 변수 간의 거리로 사용될 수 있으며, 변수 간 연관성을 히트맵으로 만드는 데 사용됩니다.\")\n\n그림 7.31: 1과 상관관계의 차이는 변수 간의 거리로 사용될 수 있으며, 변수 간 연관성을 히트맵으로 만드는 데 사용됩니다.\n\nA **바이플롯(biplot)** 은 관측치의 공간과 변수의 공간을 동시에 표현한 것입니다. 그림 7.32와 같은 PCA 바이플롯의 경우, 화살표는 처음 두 개의 새로운 축에 의해 정의된 평면으로 투영된 기존 변수들의 방향을 나타냅니다. 여기서 관측치는 단지 색칠된 점들이며, 색상은 어떤 유형의 와인이 플롯되고 있는지에 따라 선택되었습니다. 우리는 샘플 지점들과 관련하여 변수들의 방향을 해석할 수 있습니다. 예를 들어 파란색 점들은 barbera 그룹에서 온 것이며 다른 와인들보다 더 높은 말산(Malic Acid) 함량을 보여줍니다.\n\n    \n    \n    winePCAd = dudi.pca(wine, scannf=FALSE)\n    table(wine.class)\n    \n    \n    wine.class\n        barolo grignolino    barbera\n            59         71         48\n    \n    \n    fviz_pca_biplot(winePCAd, geom = \"point\", habillage = wine.class,\n       col.var = \"violet\", addEllipses = TRUE, ellipse.level = 0.69) +\n       ggtitle(\"\") + coord_fixed()\n\n[![](07-chap_files/figure-html/fig-WineBiplot2-1.png)](07-chap_files/figure-html/fig-WineBiplot2-1.png \"그림 7.32: 세 가지 유형의 와인(barolo, grignolino, barbera)에 대한 타원을 포함한 PCA 바이플롯. 각 타원에 대해, 축의 길이는 하나의 표준 편차로 주어집니다. Phenols, Flav, Proa 벡터들 사이의 작은 각도는 이들이 강하게 상관되어 있음을 나타내는 반면, Hue와 Alcohol은 상관관계가 없습니다.\")\n\n그림 7.32: 세 가지 유형의 와인(barolo, grignolino, barbera)에 대한 타원을 포함한 PCA 바이플롯. 각 타원에 대해, 축의 길이는 하나의 표준 편차로 주어집니다. `Phenols`, `Flav`, `Proa` 벡터들 사이의 작은 각도는 이들이 강하게 상관되어 있음을 나타내는 반면, `Hue`와 `Alcohol`은 상관관계가 없습니다.\n\nmultivariate plot들의 해석은 가능한 한 많은 가용 정보를 사용할 것을 요구합니다; 여기서는 샘플들과 그 그룹들뿐만 아니라 변수들도 사용하여 와인들 간의 주요 차이점들을 이해했습니다.\n\n### 7.8.3 가중 PCA의 예시\n\n때때로 우리는 서로 다른 그룹이나 관측치들 사이의 가변성을 보고 싶어 하지만, 그들에게 가중치를 부여하고 싶을 때가 있습니다. 이는 예를 들어 그룹들의 크기가 매우 다를 때 유용할 수 있습니다. [3장](03-chap.html)에서 보았던 Hiiragi 데이터를 다시 한번 살펴봅시다. 아래 코드에서는 야생형(WT) 샘플들과 전체 분산이 가장 높은 상위 100개 특징들을 선택합니다.\n\n    \n    \n    data(\"x\", package = \"Hiiragi2013\")\n    xwt = x[, x$genotype == \"WT\"]\n    sel = order(rowVars(Biobase::exprs(xwt)), decreasing = TRUE)[1:100]\n    xwt = xwt[sel, ]\n    tab = table(xwt$sampleGroup)\n    tab\n    \n    \n         E3.25 E3.5 (EPI)  E3.5 (PE) E4.5 (EPI)  E4.5 (PE)\n            36         11         11          4          4\n    \n    \n    xwt$weight = 1 / as.numeric(tab[xwt$sampleGroup])\n    pcaMouse = dudi.pca(as.data.frame(t(Biobase::exprs(xwt))),\n      row.w = xwt$weight,\n      center = TRUE, scale = TRUE, nf = 2, scannf = FALSE)\n    fviz_eig(pcaMouse) + ggtitle(\"\")\n\n[![](07-chap_files/figure-html/fig-resPCADscree-1.png)](07-chap_files/figure-html/fig-resPCADscree-1.png \"그림 7.33: Hiiragi 데이터의 가중 PCA에서 얻은 스크리 플롯. 두 번째 고윳값 이후의 급격한 하락은 2차원 PCA가 적절함을 시사합니다.\")\n\n그림 7.33: Hiiragi 데이터의 가중 PCA에서 얻은 스크리 플롯. 두 번째 고윳값 이후의 급격한 하락은 2차원 PCA가 적절함을 시사합니다.\n\n    \n    \n    fviz_pca_ind(pcaMouse, geom = \"point\", col.ind = xwt$sampleGroup) +\n      ggtitle(\"\") + coord_fixed()\n\n우리는 `tab`으로부터 그룹들이 다소 불균등하게 대표되어 있음을 알 수 있습니다. 이를 설명하기 위해, 우리는 각 샘플에 해당 그룹 크기의 역수로 가중치를 다시 부여합니다. **[ade4](https://cran.r-project.org/web/packages/ade4/)** 패키지의 `dudi.pca` 함수에는 가중치를 입력할 수 있는 `row.w` 인수가 있습니다. 코드의 출력은 그림 7.33 및 7.34에 있습니다.\n\n[![](07-chap_files/figure-html/fig-resPCADplot-1.png)](07-chap_files/figure-html/fig-resPCADplot-1.png \"그림 7.34: Hiiragi 데이터에 대한 가중 PCA 출력. 샘플들은 그룹에 따라 색상이 입혀져 있습니다.\")\n\n그림 7.34: Hiiragi 데이터에 대한 가중 PCA 출력. 샘플들은 그룹에 따라 색상이 입혀져 있습니다.\n\n## 7.9 이 장의 요약\n\n**행렬 전처리** 다변량 데이터 분석은 \"의식적인\" 전처리를 필요로 합니다. 모든 평균, 분산 및 1차원 히스토그램을 확인한 후, 우리는 데이터를 어떻게 재스케일링하고 중앙화하는지 보았습니다.\n\n**새로운 변수로의 투영** 우리는 고차원 데이터를 너무 많은 정보를 잃지 않으면서 낮은 차원(2D 평면과 3D가 가장 자주 사용됨)으로 투영하는 방법을 보았습니다. PCA는 원래의(기존) 변수들의 선형 결합인, 새로운 \"더 정보가 많은\" 변수들을 찾습니다.\n\n**행렬 분해** PCA는 SVD라 불리는 행렬 \\(X\\)의 분해를 찾는 것에 기반합니다. 이 분해는 낮은 계수의 근사를 제공하며 \\(X^tX\\)의 고유 분해와 동일합니다. 특잇값의 제곱은 새로운 변수들의 고윳값 및 분산과 같습니다. 우리는 데이터의 신호를 재현하는 데 얼마나 많은 축이 필요한지 결정하기 전에 이러한 값들을 체계적으로 플롯했습니다.\n\n매우 가까운 두 고윳값은 매우 불안정한 점수(scores) 또는 PC 점수를 발생시킬 수 있습니다. 고윳값의 스크리 플롯을 살펴보고 이러한 가까운 고윳값에 해당하는 축들을 분리하는 것을 피하는 것이 항상 필요합니다. 이를 위해 여러 R 패키지에서 사용 가능한 대화형 3D 또는 4D 투영을 사용해야 할 수도 있습니다.\n\n**바이플롯(Biplot) 표현** 관측치의 공간은 자연스럽게 \\(p\\)차원 공간입니다 (\\(p\\)개의 원래 변수가 좌표를 제공합니다). 변수의 공간은 \\(n\\)차원입니다. 우리가 공부한 두 분해(특잇값 / 고윳값 및 특잇값 벡터 / 고유벡터)는 이 두 공간 모두에 대해 새로운 좌표를 제공하며, 때때로 우리는 하나를 다른 하나의 쌍대(dual)라고 부릅니다. 우리는 동일한 고유벡터 상에 관측치와 변수 모두의 투영을 플롯할 수 있습니다. 이는 PCA 출력을 해석하는 데 유용한 바이플롯을 제공합니다.\n\n**다른 그룹 변수의 투영** PCA의 해석은 관측치에 대한 중복되거나 인접한 데이터를 통해서도 용이해질 수 있습니다.\n\n## 7.10 더 읽을거리\n\n특잇값 분해에 대한 이해를 깊게 하는 가장 좋은 방법은 Strang ([2009](16-chap.html#ref-Strang:09))의 7장을 읽는 것입니다. 책 전체가 행렬의 계수의 의미와 행 공간과 열 공간 사이의 쌍대성([Holmes 2006](16-chap.html#ref-frenchway))을 이해하는 데 필요한 선형 대수학의 기초를 다집니다.\n\nPCA 및 관련 방법들에 대해 완전한 교과서들이 쓰여졌습니다. Mardia, Kent, Bibby ([1979](16-chap.html#ref-Mardia))는 선형 대수와 행렬을 사용하여 고전적인 방식으로 모든 다변량 방법을 다루는 표준 텍스트입니다. 데이터가 다변량 정규 분포에서 나온다는 모수적 가정을 함으로써, Mardia, Kent, Bibby ([1979](16-chap.html#ref-Mardia))는 성분 수에 대한 추론적 검정과 주성분에 대한 한계 속성도 제공합니다. Jolliffe ([2002](16-chap.html#ref-Jolliffe))는 광범위한 예제와 함께 PCA와 관련된 모든 것을 다루는 단행본입니다.\n\n우리는 관측치와 변수에 대한 가중치에 보충 정보를 통합할 수 있습니다. 이는 1970년대 프랑스 데이터 과학자들에 의해 도입되었습니다. 리뷰는 Holmes ([2006](16-chap.html#ref-frenchway))를, 추가 예제는 [9장](09-chap.html)을 참조하십시오.\n\nPCA의 해석과 안정성에 대한 개선은 선형 결합에 나타나는 0이 아닌 계수의 수를 최소화하는 페널티를 추가함으로써 얻어질 수 있습니다. Zou, Hastie, Tibshirani ([2006](16-chap.html#ref-Zou2006))와 Witten, Tibshirani, Hastie ([2009](16-chap.html#ref-Witten2009))는 희소(sparse) 버전의 주성분 분석을 개발했으며, 그들의 패키지 **[elasticnet](https://cran.r-project.org/web/packages/elasticnet/)** 및 **[PMA](https://cran.r-project.org/web/packages/PMA/)** 는 R에서의 구현체를 제공합니다.\n\n## 7.11 연습 문제\n\n\n\n연습 문제 7.1\n\n[SVD에 관한 위키백과 문서](http://en.wikipedia.org/wiki/Singular_value_decomposition)의 섹션 1, 2, 3을 읽어 svd에 관한 내용을 복습하세요. 또한 [행렬의 고유 분해에 관한 위키백과 문서](http://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix)의 섹션 1, 2, 2.1을 읽어 관련 고유 분해에 대해 읽어보는 것도 유익할 것입니다. 우리는 \\(n\\)행 \\(p\\)열의 계수 1인 행렬 \\(X\\)를 다음과 같이 분해할 수 있음을 알고 있습니다:\n\n\\[ **X** = \\begin{pmatrix} x_{11} & x_{12} & ... & x_{1p}\\\\ x_{21} & x_{22} & ... & x_{2p}\\\\ \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\ x_{n1} & x_{n2} & ... & x_{np} \\end{pmatrix}\\]\n[ X =\n\\[\\begin{pmatrix} x_{11} & x_{12} & ... & x_{1p}\\\\ x_{21} & x_{22} & ... & x_{2p}\\\\ \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ \\ x_{n1} & x_{n2} & ... & x_{np} \\end{pmatrix}\\]\n=\n\\[\\begin{pmatrix} u_{11} \\\\ u_{21} \\\\ \\vdots \\\\ u_{n1} \\\\ \\end{pmatrix}\\]\n\n\\[\\begin{pmatrix} v_{11} & v_{21} & \\cdots & v_{p1} \\end{pmatrix}\\]\n]\n\n(X)에 모든 값이 0인 행이나 열이 없다면, 이 분해는 유일할까요?\n계수가 1인 행렬을 생성해 보세요. 먼저 2부터 30까지 2씩 증가하는 길이 15의 벡터와, 3, 6, 9, 12의 값을 가진 길이 4의 벡터를 만든 다음, 이들의 외적(outer product)을 구하세요.\n\nu = seq(2, 30, by = 2)\nv = seq(3, 12, by = 3)\nX1 = u %*% t(v)\n왜 t(v)를 취해야 할까요?\n\n이제 Materr라고 부르는 행렬 형태의 노이즈를 추가하여 “거의 계수가 1인” 행렬을 만듭니다.\n\nMaterr = matrix(rnorm(60,1),nrow=15,ncol=4)\nX = X1+Materr\nggplot을 사용하여 (X)를 시각화하세요.\n\n계수가 2인 행렬에 대해서도 동일한 분석을 수행해 보세요.\n\n해결책\nX1은 다음과 같이 계산될 수도 있음에 주목하세요:\nouter(u, v)\n\n\nggplot(data=data.frame(X), aes(x=X1, y=X2, col=X3, size=X4)) + geom_point()\n여기서 우리는 데이터가 모든 4개 차원에서 선형으로 보임을 알 수 있습니다. 이것이 계수가 1이라는 것이 의미하는 바입니다. 이제 계수가 2인 행렬을 고려해 봅시다.\nn = 100\np = 4\nY2 = outer(rnorm(n), rnorm(p)) + outer(rnorm(n), rnorm(p))\nhead(Y2)\n\n\n            [,1]       [,2]         [,3]        [,4]\n[1,] -0.44143871  2.3213197  0.433215525 -1.35523790\n[2,]  0.79620920 -1.0748037  1.217052906 -1.13096295\n[3,]  0.16787281  0.2259296  0.547203332 -0.75836031\n[4,]  0.87269426 -1.9208649  0.856966180 -0.38621340\n[5,]  0.03751521 -0.1480678 -0.005217966  0.05864122\n[6,]  0.50195482 -2.0409896 -0.108241027  0.85336630\n\n\nggplot(data=data.frame(Y2), aes(x=X1, y=X2, col=X3, size=X4)) + geom_point()\n데이터를 처음 두 좌표(R에서 행렬을 데이터 프레임으로 변환할 때 기본적으로 X1 및 X2라고 불림)로 투영하면 데이터가 두 차원 모두에서 변하므로, 이제 분명히 적어도 두 개의 차원이 존재합니다. 따라서 다음 단계는 두 개 이상의 차원이 있는지 결정하는 것입니다. 오른쪽 상단의 점들이 여러분과 가장 가깝고(가장 큼), 플롯에서 아래쪽과 왼쪽으로 갈수록 그 점들은 더 멀어집니다. 왼쪽에는 가장 파란 점들이 있고 오른쪽으로 갈수록 선형적으로 더 어두워지는 것 같습니다. 아마 짐작하시겠지만, “고차원”이 단지 4차원을 의미할 때조차 고차원에서 저차원 공간을 시각적으로 발견하는 것은 매우 어렵습니다. 이것이 우리가 특잇값 분해에 의존하는 한 이유입니다.\nsvd(Y2)$d # two non-zero eigenvalues\n\n\n[1] 2.637465e+01 1.266346e+01 3.144564e-15 1.023131e-15\n\n\nY = Y2 + matrix(rnorm(n*p, sd=0.01),n,p) # add some noise to Y2\nsvd(Y)$d # four non-zero eigenvalues (but only 2 big ones)\n\n\n[1] 26.39673712 12.68547439  0.10735103  0.09104741\n여기서 우리는 0이 아닌 2개의 차원과 거의 0인 2개의 차원(“Y2”의 경우, 컴퓨터 허용 오차의 제곱근 내에서 0임)을 갖습니다.\n연습 문제 7.2\n\n먼저 그림 7.35에 표시된 것과 같은 상관관계가 높은 이변량 데이터 행렬을 생성하세요.\n힌트: mvrnorm 함수를 사용하세요.\n\n특잇값을 살펴보고 행렬의 계수를 확인하세요.\n\nPCA를 수행하고 회전된 주성분 축을 보여주세요.\n\n해결책\n\n다음을 사용하여 상관관계가 있는 이변량 정규 데이터를 생성합니다:\n\nlibrary(\"MASS\")\nmu1 = 1; mu2 = 2; s1=2.5; s2=0.8; rho=0.9;\nsigma = matrix(c(s1^2, s1*s2*rho, s1*s2*rho, s2^2),2)\nsim2d = data.frame(mvrnorm(50, mu = c(mu1,mu2), Sigma = sigma))\nsvd(scale(sim2d))$d\n\n\n[1] 9.647686 2.218592\n\n\nsvd(scale(sim2d))$v[,1]\n\n\n[1] 0.7071068 0.7071068\n\nprcomp를 사용하여 PCA를 수행하고 점수(scores)를 통해 원하는 회전을 제공합니다.\n\nrespc = princomp(sim2d)\ndfpc  = data.frame(pc1=respc$scores[,1], \n                   pc2=respc$scores[,2])\n\nggplot(data.frame(sim2d), aes(x=X1,y=X2)) + geom_point()\nggplot(dfpc, aes(x=pc1, y=pc2)) + geom_point() + coord_fixed(2)\n\n\n( ext{})\n\n\n\n( ext{})\n\n그림 7.35: 산점도 (A)에 표시된 원래 데이터와 주성분 회전 (B)을 사용하여 얻은 플롯.\n연습 문제 7.3\n그림 7.35의 (a) 부분은 매우 길쭉한 플로팅 영역을 보여주는데, 그 이유는 무엇인가요?\ncoord_fixed() 옵션을 사용하지 않고 정사각형 플로팅 존을 사용하면 어떤 일이 발생하나요? 왜 이것이 오해를 불러일으킬 수 있을까요?\n연습 문제 7.4\nHiiragi 데이터를 다시 살펴보고 가중 및 비가중 접근 방식을 비교해 봅시다.\n\n가중치를 부여하지 않은 Hiiragi 데이터 xwt에 대해 상관관계 원을 만드세요. 어떤 유전자들이 첫 번째 주평면에 가장 잘 투영(가장 좋은 근사)되나요?\n첫 번째 평면에서 대부분의 분산을 설명하는 극단적인 유전자 변수들의 레이블을 보여주는 바이플롯을 만드세요. 샘플 포인트들도 추가하십시오.\n\nAbbott, Edwin A. 1884. Flatland: A Romance of Many Dimensions. OUP Oxford.\nFlury, Bernard. 1997. A First Course in Multivariate Statistics. Springer.\nHolmes, Susan. 2006. “Multivariate Analysis: The French way.” In Probability and Statistics: Essays in Honor of David a. Freedman , edited by D. Nolan and T. P. Speed. Vol. 56. IMS Lecture Notes–Monograph Series. Beachwood, OH: IMS. http://www.imstat.org/publications/lecnotes.htm.\nHolmes, Susan, Michael He, Tong Xu, and Peter P Lee. 2005. “Memory t Cells Have Gene Expression Patterns Intermediate Between Naive and Effector.” PNAS 102 (15): 5519–23.\nHotelling, Harold. 1933. “Analysis of a Complex of Statistical Variables into Principal Components.” Journal of Educational Psychology 24 (6): 417–41.\nJolicoeur, Pierre, and James E Mosimann. 1960. “Size and Shape Variation in the Painted Turtle. A Principal Component Analysis.” Growth 24: 339–54.\nJolliffe, Ian. 2002. Principal Component Analysis. Wiley Online Library.\nMardia, Kanti, John T Kent, and John M Bibby. 1979. Multiariate Analysis. New York: Academic Press.\nPearson, Karl. 1901. “LIII. On Lines and Planes of Closest Fit to Systems of Points in Space.” The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 2 (11): 559–72.\nStrang, Gilbert. 2009. Introduction to Linear Algebra. Fourth. Wellesley- Cambridge Press.\nWitten, Daniela M, Robert Tibshirani, and Trevor Hastie. 2009. “A Penalized Matrix Decomposition, with Applications to Sparse Principal Components and Canonical Correlation Analysis.” Biostatistics , kxp008.\nZou, Hui, Trevor Hastie, and Robert Tibshirani. 2006. “Sparse Principal Component Analysis.” Journal of Computational and Graphical Statistics 15 (2): 265–86.\n페이지는 R 버전 4.5.1 (2025-06-13)을 사용하여 2025-09-01 01:33에 빌드되었습니다.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>다변량 분석 (Multivariate Analysis)</span>"
    ]
  },
  {
    "objectID": "08-chap.html",
    "href": "08-chap.html",
    "title": "10  고처리량 카운트 데이터 (High-Throughput Count Data)",
    "section": "",
    "text": "10.1 8.2 몇 가지 핵심 개념\n생명공학의 많은 측정 장치들은 분자의 대규모 병렬 샘플링과 카운팅에 기초합니다. 한 가지 예로 고처리량 DNA 시퀀싱(high-throughput DNA sequencing)이 있습니다. 그 응용 분야는 데이터 출력 방식에 따라 크게 두 가지 주요 클래스로 나뉩니다: 첫 번째 경우, 관심 있는 출력은 서열 그 자체이며, 아마도 그들의 다형성(polymorphisms)이나 이전에 보았던 다른 서열들과의 차이일 것입니다. 두 번째 경우, 서열 그 자체는 어느 정도 잘 이해되어 있으며(예를 들어, 잘 조립되고 어노테이션된 게놈을 가지고 있음), 우리의 관심은 샘플 내에서 서로 다른 서열 영역들이 얼마나 풍부하게 존재하는지에 있습니다.\n예를 들어, RNA-Seq (Ozsolak and Milos 2011)에서는 세포 집단이나 조직에서 발견되는 RNA 분자들의 서열을 분석합니다.\n엄밀히 말하면, 우리는 RNA의 서열을 분석하는 것이 아니라 역전사를 통해 얻은 상보적 DNA(cDNA)의 서열을 분석합니다. 전체 RNA 풀은 폴리-A 선택(poly-A selection)이나 리보솜 RNA 고갈(ribosomal RNA depletion)과 같은 생화학적 수단을 통해 관심 있는 하위 집합(예: 메신저 RNA)으로 줄어들 수 있습니다. 단일 세포와 대량의 세포를 분석할 수 있게 해주는 RNA-Seq의 민감한 변체들이 존재합니다.\nChIP-Seq 에서는 특정 DNA 결합 단백질에 결합된 DNA 영역(면역 침전을 통해 선택됨)의 서열을 분석합니다. RIP-Seq 에서는 특정 RNA 결합 단백질에 결합된 RNA 분자나 그 영역을, DNA-Seq 에서는 게놈 DNA의 서열을 분석하며 이질적인 세포 집단에서의 유전적 변이의 유병률(예: 종양의 클론 구성)에 관심을 가집니다. 고처리량 염색질 구조 포착(HiC)에서는 DNA의 3차원 공간적 배치를 매핑하는 것을 목표로 하며, 유전적 스크리닝 (변동을 위해 RNAi나 CRISPR-Cas9 라이브러리를 사용하고 리드아웃을 위해 고처리량 시퀀싱을 사용하는 경우)에서는 유전자 넉다운(knockdown), 넉아웃(knockout) 또는 수정에 따른 세포의 증식이나 생존에 관심을 가집니다. 마이크로바이옴 분석에서는 복잡한 미생물 서식지에 있는 서로 다른 미생물 종의 풍부도를 연구합니다.\n이상적으로는 샘플 내의 관심 있는 모든 분자를 시퀀싱하고 세고 싶을 것입니다. 일반적으로 이는 불가능합니다: 생화학적 프로토콜은 100% 효율적이지 않으며, 일부 분자나 중간 생성물은 과정 중에 소실됩니다. 게다가 대개 그럴 필요조차 없습니다. 대신, 우리는 통계적 표본(statistical sample) 을 시퀀싱하고 셉니다. 표본 크기는 분석되는 서열 풀의 복잡성에 따라 달라질 것이며, 수만 개에서 수십억 개에 이를 수 있습니다. 이러한 데이터의 샘플링 특성은 이를 분석할 때 중요합니다. 우리는 샘플링이 충분히 대표성이 있어 흥미로운 경향과 패턴을 식별할 수 있기를 희망합니다.\n이 장에서 우리는 RNA-Seq과 같은 고처리량 시퀀싱 응용 분야의 카운트 데이터(count data)에 익숙해질 것입니다. 데이터를 해석하기 위해 데이터의 근간이 되는 샘플링 프로세스를 이해하고 모델링할 것입니다. 우리의 주요 목표는 서로 다른 조건(예: 처리되지 않은 군 대 처리된 군)의 샘플들 사이에서 체계적인 변화를 감지하고 정량화하는 것이며, 이때의 과제는 그러한 체계적인 변화를 동일한 조건 내에서의 샘플링 변동 및 실험적 가변성과 구별하는 것입니다. 이를 위해 다음과 같은 필요한 통계적 개념과 도구들을 갖출 것입니다:\n사실 이러한 개념들은 훨씬 더 넓은 범위의 응용 분야를 가집니다: 이들은 어떤 실험적 공변량의 함수로서 노이즈가 섞여 있거나 하는 다른 유형의 데이터에도 적용될 수 있습니다. 특히 일반화 선형 모델의 프레임워크는 상당히 추상적이고 일반적이지만, 이는 많은 서로 다른 데이터 유형에 맞춰 조정될 수 있다는 장점이 있습니다. 따라서 우리는 바퀴를 새로 발명할 필요 없이 관련된 광범위한 도구와 진단 기능을 즉시 즐길 수 있습니다.\n보너스로, 5장과 7장에서 보았던 비지도 학습 방법들에 데이터를 적합하게 만들고 데이터 시각화를 더 쉽게 만들어주는 데이터 변환에 대해서도 살펴볼 것입니다.\n시작하기 전에 몇 가지 주요 용어를 정리해 봅시다.\n1 https://www.illumina.com/techniques/sequencing.html를 참조하십시오.\n2 특정 응용 분야의 경우, 가장 적절한 접근 방식과 선택에 대해 최신 문헌을 확인하는 것이 가장 좋습니다.\n3 예: RNA-Seq의 경우, 게놈과 그 전사체에 대한 어노테이션.\n시퀀싱과 카운팅 사이에는 함께 속하는 서열들을 모으는 중요한 집계(aggregation) 또는 군집화 단계가 포함됩니다: 예를 들어, (RNA-Seq에서) 동일한 유전자에 속하는 모든 리드나, (ChIP-Seq에서) 동일한 결합 영역에 속하는 모든 리드를 모으는 것입니다. 실험의 목적에 따라 이에 대한 여러 접근 방식과 선택 사항이 있습니다2. 방법에는 참조 서열(reference sequence)에 대한 명시적인 정렬(alignment) 또는 해시 기반 매핑3, 그리고 리드들의 참조 독립적인 서열 유사성 기반 군집화가 포함됩니다 — 특히 메타제노믹스나 메타전사체학에서와 같이 명확한 참조 서열이 없는 경우에 그렇습니다. 우리는 서로 다른 대립유전자(alleles)나 이소형(isoforms)을 별도로 고려할지, 아니면 이들을 하나의 동등 클래스(equivalence class)로 병합할지 선택해야 합니다. 단순함을 위해 이 장에서는 특정 응용 분야에 따라 다양한 대상이 될 수 있음에도 불구하고 이러한 운영상의 집계 단위에 대해 유전자(gene) 라는 용어를 사용하겠습니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>고처리량 카운트 데이터 (High-Throughput Count Data)</span>"
    ]
  },
  {
    "objectID": "08-chap.html#몇-가지-핵심-개념",
    "href": "08-chap.html#몇-가지-핵심-개념",
    "title": "10  고처리량 카운트 데이터 (High-Throughput Count Data)",
    "section": "",
    "text": "시퀀싱 라이브러리(sequencing library) 는 시퀀싱 장비의 입력으로 사용되는 DNA 분자들의 집합입니다.\n단편(Fragments) 은 시퀀싱되는 분자들입니다. 현재 가장 널리 사용되는 기술1은 길이가 약 300–1000 뉴클레오타이드인 분자만 처리할 수 있기 때문에, 이들은 관심 있는 (일반적으로 더 긴) DNA나 cDNA 분자를 단편화하여 얻어집니다.\n리드(read) 는 단편으로부터 얻은 서열입니다. 현재 기술로 리드는 단편 전체가 아니라 그 단편의 한쪽 끝 또는 양쪽 끝만을 포괄하며, 양쪽의 리드 길이는 최대 약 150 뉴클레오타이드입니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>고처리량 카운트 데이터 (High-Throughput Count Data)</span>"
    ]
  },
  {
    "objectID": "08-chap.html#카운트-데이터",
    "href": "08-chap.html#카운트-데이터",
    "title": "10  고처리량 카운트 데이터 (High-Throughput Count Data)",
    "section": "10.2 8.3 카운트 데이터",
    "text": "10.2 8.3 카운트 데이터\n예시 데이터 세트를 불러와 봅시다. 이 데이터는 실험 데이터 패키지인 pasilla 에 들어 있습니다.\nfn = system.file(\"extdata\", \"pasilla_gene_counts.tsv\",\n                  package = \"pasilla\", mustWork = TRUE)\ncounts = as.matrix(read.csv(fn, sep = \"\\t\", row.names = \"gene_id\"))\n\n\n\n여기서 보여주는 코드에서는 pasilla 패키지와 함께 제공되는 파일을 찾기 위해 system.file 함수를 사용합니다. 여러분 자신의 데이터로 작업할 때는 counts 행렬을 직접 준비해야 할 것입니다.\n\n\n여기서 보여주는 코드에서는 system.file 함수를 사용하여 pasilla 패키지와 함께 제공되는 파일을 찾습니다. 여러분 자신의 데이터로 작업할 때는 counts 행렬을 직접 준비해야 할 것입니다.\n데이터는 탭으로 구분된 파일에 직사각형 표 형태로 저장되어 있으며, 이를 counts 행렬로 읽어 들였습니다.\ndim(counts)\n\n\n[1] 14599     7\n\n\ncounts[ 2000+(0:3), ]\n\n\n            untreated1 untreated2 untreated3 untreated4 treated1 treated2\nFBgn0020369       3387       4295       1315       1853     4884     2133\nFBgn0020370       3186       4305       1824       2094     3525     1973\nFBgn0020371          1          0          1          1        1        0\nFBgn0020372         38         84         29         28       63       28\n            treated3\nFBgn0020369     2165\nFBgn0020370     2120\nFBgn0020371        0\nFBgn0020372       27\n이 행렬은 각 샘플에서 각 유전자에 대해 관찰된 리드의 수를 집계합니다. 우리는 이를 카운트 테이블(count table) 이라고 부릅니다. 유전자에 해당하는 14,599개의 행과 샘플에 해당하는 7개의 열로 이루어져 있습니다. 파일에서 데이터를 불러올 때 좋은 타당성 검사는 데이터의 일부를 출력해 보는 것인데, 위에서 했던 것처럼 아주 앞부분뿐만 아니라 중간의 무작위 지점도 확인해 보는 것이 좋습니다.\n이 표는 정수 값들의 행렬입니다: 행렬의 (i)번째 행과 (j)번째 열의 값은 샘플 (j)에서 유전자 (i)에 매핑된 리드가 몇 개인지를 나타냅니다. 이 장에서 논의할 통계적 샘플링 모델은 이 값들이 시퀀싱 리드의 직접적이고 “가공되지 않은(raw)” 카운트라는 사실에 의존합니다 — 정규화된 카운트나 포괄된 염기쌍의 수와 같은 파생된 수치가 아닙니다. 그러한 수치들은 무의미한 결과로 이어질 뿐입니다.\n\n10.2.1 8.3.1 카운트 데이터의 과제들\n우리가 이러한 카운트 데이터로 극복해야 할 과제들은 무엇일까요?\n\n데이터는 0부터 수백만까지 넓은 동적 범위를 가집니다. 동적 범위의 서로 다른 부분에서 데이터의 분산, 그리고 더 일반적으로 분포의 형태가 매우 다릅니다. 우리는 이분산성(heteroskedasticity) 이라고 불리는 이러한 현상을 고려해야 합니다.\n데이터는 음수가 아닌 정수이며, 그 분포는 대칭적이지 않습니다 — 따라서 정규 분포나 로그-정규 분포 모델은 적합도가 낮을 수 있습니다.\n우리는 체계적인 샘플링 편향을 이해하고 이를 보정해야 합니다. 혼란스럽게도 이를 흔히 정규화(normalization) 라고 부릅니다. 예로는 실험의 전체 시퀀싱 깊이(두 라이브러리에서 한 유전자의 실제 풍부도가 같더라도, 시퀀싱된 전체 리드 수에 따라 해당 유전자에 대해 서로 다른 리드 수를 예상함), 또는 서로 다른 샘플링 확률(생물학적 샘플 내에서 두 유전자의 실제 풍부도가 같더라도 길이, GC 함량, 2차 구조, 결합 파트너와 같은 생물물리학적 특성이 다르면 서로 다른 리드 수를 예상함) 등이 있습니다.\n우리는 샘플링의 확률적 특성뿐만 아니라 다른 확률적 실험 변동의 원인들도 이해해야 합니다. 생물학적 샘플 수가 많은 연구의 경우 이는 대개 간단하며, 재표본 추출이나 순열 기반 방법들에 의존할 수도 있습니다. 하지만 설계된 실험(designed experiments)의 경우 표본 크기가 제한적인 경향이 있습니다.\n\n\n\n\n실험(experiments)과 연구(studies) 사이에는 중요한 개념적 및 실무적 차이가 있습니다 – 13장을 참조하십시오.\n\n\n실험(experiments)과 연구(studies) 사이에는 중요한 개념적 및 실무적 차이가 있습니다 – 13장을 참조하십시오.\n예를 들어, pasilla 데이터에는 untreated 군에서 4개의 반복(replicates)이, treated 군에서 3개의 반복이 있습니다. 이는 재표본 추출이나 순열 기반 방법이 충분한 검정력을 갖지 못함을 의미합니다. 진행하기 위해 우리는 분포에 대한 가정을 해야 합니다. 본질적으로 그러한 가정들이 하는 일은 소수의 분포 매개변수로부터 분포의 꼬리 부분에 있는 드문 사건의 확률 — 즉, 비정상적으로 높거나 낮은 카운트 — 을 계산할 수 있게 해주는 것입니다.\n\n하지만 그것만으로도 부족한 경우가 많은데, 특히 분산(dispersion) 매개변수4의 추정은 작은 표본 크기에서 어렵습니다. 이 경우, 비슷한 위치에 있는 유전자들은 비슷한 분산을 가진다는 것과 같은 추가적인 가정을 해야 합니다. 이를 유전자 간 정보 공유(sharing of information across genes)라고 하며, 8.10.1절에서 다시 다룰 것입니다.\n\n4 분포는 다양한 방식으로 매개변수화될 수 있습니다; 종종 매개변수들은 위치(location)의 척도와 분산(dispersion)의 척도에 대응합니다; 친숙한 위치의 척도는 평균이고, 친숙한 분산의 척도는 분산(또는 표준 편차)이지만, 일부 분포의 경우 다른 척도들이 사용되기도 합니다.\n\n\n10.2.2 8.3.2 RNA-Seq: 유전자 구조, 스플라이싱, 이소형은 어떠한가요?\n진핵생물의 유전자는 복잡합니다: 대부분의 유전자는 여러 개의 엑손으로 구성되며, mRNAs는 스플라이싱(splicing)이라는 과정을 통해 엑손들이 연결되어 만들어집니다. 대안적 스플라이싱(alternative splicing)과 전사 시작 및 종료 지점의 다양한 선택은 동일한 유전자 좌위(locus)로부터 여러 개의 대안적 이소형(alternative isoforms) 생성을 가능하게 합니다. 고처리량 시퀀싱을 사용하여 전사체의 이소형 구조를 탐지하는 것이 가능합니다. 특정 이소형에 특징적인 단편들로부터 이소형 특이적 풍부도를 탐지하는 것도 가능합니다. 전체 길이 이소형의 비교적 짧은 단편들만을 제공하는 현재의 RNA-Seq 데이터로는 전체 길이 이소형 구조와 풍부도를 조립하고 분리해내는 것이 어려운 경향이 있습니다 (Steijger et al. 2013). 이 때문에 국소적인 판단(예: 개별 엑손의 포함 또는 제외)만을 내리는 좀 더 겸손한 목표를 가진 절차들이 공식화되었으며 (Anders, Reyes, and Huber 2012), 이들이 더 강건(robust)할 수 있습니다. 미래의 기술들은 전체 길이 전사체의 서열을 분석할 것으로 기대할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>고처리량 카운트 데이터 (High-Throughput Count Data)</span>"
    ]
  },
  {
    "objectID": "08-chap.html#카운트-데이터-모델링",
    "href": "08-chap.html#카운트-데이터-모델링",
    "title": "10  고처리량 카운트 데이터 (High-Throughput Count Data)",
    "section": "10.3 8.4 카운트 데이터 모델링",
    "text": "10.3 8.4 카운트 데이터 모델링\n\n10.3.1 8.4.1 분산 (Dispersion)\n유전자 1에 해당하는 단편 (n_1)개, 유전자 2에 해당하는 단편 (n_2)개 등을 포함하며 전체 라이브러리 크기가 (n = n_1+n_2+)인 시퀀싱 라이브러리를 생각해 봅시다. 우리는 라이브러리를 시퀀싱하여 무작위로 샘플링된 (r)개 단편의 정체를 결정합니다. 이 숫자들의 크기 정도를 살펴보면 반가운 단순화가 가능해집니다:\n\n유전자의 수는 수만 개입니다;\n(n)의 값은 준비에 사용된 세포의 양에 따라 달라지지만, 벌크(bulk) RNA-Seq의 경우 수십억 또는 수조 개에 달할 것입니다;\n리드 수 (r)은 보통 수천만 개이며, 따라서 (n)보다 훨씬 작습니다.\n\n이로부터 우리는 주어진 리드가 (i)번째 유전자에 매핑될 확률이 (p_i=n_i/n)이며, 이것이 다른 모든 리드에 대한 결과와 거의 독립적이라는 결론을 내릴 수 있습니다. 따라서 우리는 (i)번째 유전자에 대한 리드 수를 포아송 분포로 모델링할 수 있으며, 이때 포아송 프로세스의 비율(rate) 은 (i)번째 유전자에 대한 단편의 초기 비율인 (p_i)와 (r)의 곱, 즉 (_i=rp_i)가 됩니다.\n\n\n\n원칙적으로 우리는 여기서 비복원 추출과 다항 분포를 고려해야 합니다: i번째 유전자에 대한 리드를 샘플링할 확률은 동일한 유전자와 다른 유전자들이 이미 몇 번 샘플링되었는지에 달려 있습니다. 그러나 이러한 의존성은 무시할 수 있을 정도로 작아서 무시할 것입니다. 이는 n이 r보다 훨씬 크고, 유전자의 수가 많으며, 각 n_i가 n에 비해 작기 때문입니다.\n\n\n원칙적으로 우리는 여기서 비복원 추출(sampling without replacement) 과 다항 분포를 고려해야 합니다: (i)번째 유전자에 대한 리드를 샘플링할 확률은 동일한 유전자와 다른 유전자들이 이미 몇 번 샘플링되었는지에 달려 있습니다. 그러나 이러한 의존성은 무시할 수 있을 정도로 작아서 무시할 것입니다. 이는 (n)이 (r)보다 훨씬 크고, 유전자의 수가 많으며, 각 (n_i)가 (n)에 비해 작기 때문입니다.\n실제로 우리는 대개 단일 라이브러리 내의 리드 카운트를 모델링하는 데 관심이 있는 것이 아니라, 라이브러리 간의 카운트를 비교하는 데 관심이 있습니다. 즉, 우리는 서로 다른 생물학적 조건 사이에서 — 예를 들어 약물 처리를 한 세포주와 하지 않은 동일한 세포주 사이에서 — 보이는 차이가 “우연히” 예상되는 것보다 큰지, 즉 생물학적 반복 사이에서도 예상할 수 있는 것보다 큰지 알고 싶어 합니다. 경험적으로 반복 실험들은 포아송 분포가 예측하는 것보다 더 많이 변하는 것으로 나타납니다. 직관적으로 일어나는 일은 (p_i)와 따라서 (_i)가 생물학적 반복 사이에서도 변한다는 것입니다; 아마도 세포가 자란 온도가 약간 달랐거나, 첨가된 약물의 양이 몇 퍼센트 차이 났거나, 배양 시간이 약간 더 길었을 수 있습니다. 이를 설명하기 위해 우리는 그 위에 또 다른 모델링 층을 추가해야 합니다. 우리는 이미 4장에서 계층 모델과 혼합물을 보았습니다. 감마-포아송(일명 음이항) 분포가 우리의 모델링 요구에 적합하다는 것이 밝혀졌습니다. 평균과 분산 모두를 나타내는 단일 () 대신, 이 분포는 두 개의 매개변수를 가집니다. 원칙적으로 이들은 각 유전자마다 다를 수 있으며, 우리는 데이터로부터 이들을 어떻게 추정할지의 문제로 다시 돌아올 것입니다.\n\n\n10.3.2 8.4.2 정규화 (Normalization)\n종종 데이터 생성에 영향을 준 체계적인 편향들이 있으며, 이를 고려할 가치가 있습니다. 불행히도, 그러한 분석 측면에 대해 정규화(normalization) 라는 용어가 흔히 사용되지만, 이는 오해의 소지가 있습니다: 이는 정규 분포, 벡터 공간의 노름(norm), 또는 법선 벡터(normal vectors)와는 아무런 관련이 없습니다. 오히려 우리가 목표로 하는 것은 체계적인 편향의 성격을 식별하고 그 크기를 추정하여, 데이터의 모델 기반 분석에서 이를 고려하는 것입니다.\n가장 중요한 체계적 편향은 각 샘플의 총 리드 수의 차이에서 비롯됩니다. 만약 우리가 한 라이브러리에서 다른 라이브러리보다 더 많은 리드를 가졌다면, 다른 모든 조건이 동일할 때 카운트들은 어떤 비례 계수 (s)에 비례할 것이라고 가정할 수 있습니다. 나이브하게는, 각 샘플에 대한 (s)의 적절한 추정치가 단순히 모든 유전자의 카운트 합계에 의해 주어진다고 제안할 수 있습니다. 하지만 우리는 더 잘 할 수 있음이 밝혀졌습니다. 이를 이해하기 위해 장난감 예제가 도움이 됩니다.\n\n그림 8.1: 크기 인자(Size factor) 추정. 점들은 가상의 유전자들에 해당하며, 두 샘플에서의 그들의 카운트는 (x) 및 (y) 좌표로 표시됩니다. 선들은 본문에서 설명된 크기 인자 추정의 두 가지 서로 다른 방식을 나타냅니다.\n그림 8.1에 표시된 것처럼 5개의 유전자와 두 개의 샘플이 있는 데이터 세트를 고려해 봅시다. 만약 우리가 각 샘플에 대해 카운트 합계로 (s)를 추정한다면, 파란색 선의 기울기가 그들의 비율을 나타냅니다. 이에 따르면 유전자 C는 샘플 1에 비해 샘플 2에서 하향 조절(down-regulated)된 반면, 다른 유전자들은 모두 어느 정도 상향 조절되었습니다. 만약 우리가 대신 그들의 비율이 빨간색 선에 대응하도록 (s)를 추정한다면, 우리는 여전히 유전자 C가 하향 조절되었다고 결론 내릴 것이지만 다른 유전자들은 변하지 않은 것으로 결론 내릴 것입니다. 두 번째 버전이 더 파시모니어스(parsimonious)하며 과학자들에 의해 종종 선호됩니다. 빨간색 선의 기울기는 강건 회귀(robust regression)를 통해 얻을 수 있습니다. 이것이 DESeq2 방법이 하는 일입니다.\n질문 8.1\n8.3절의 예제 데이터 세트 count에 대해, DESeq2 의 estimateSizeFactorsForMatrix 출력은 단순히 열 합계를 구했을 때와 어떻게 비교되나요?\n해결책\n아래 코드에 의해 생성된 그림 8.2를 보세요. 이 경우에는 큰 차이가 없으며 결과들이 거의 비례합니다.\nlibrary(\"tibble\")\nlibrary(\"ggplot2\")\nlibrary(\"DESeq2\")\nggplot(tibble(\n  `size factor` = estimateSizeFactorsForMatrix(counts),\n  `sum` = colSums(counts)), aes(x = `size factor`, y = `sum`)) +\n  geom_point()\n\n그림 8.2: pasilla 데이터에 대한 크기 인자 대 합계.\n태스크\n이 책의 R 소스 코드를 찾아서 그림 8.1을 생성하는 코드를 살펴보세요.\n질문 8.2\npasilla 데이터 세트에서의 생물학적 반복 실험들에 대해 평균-분산 관계를 플롯하세요.\n해결책\n다음 코드에 의해 생성된 그림 8.3을 보세요.\nlibrary(\"matrixStats\")\nsf = estimateSizeFactorsForMatrix(counts)\nncounts  = counts / matrix(sf,\n   byrow = TRUE, ncol = ncol(counts), nrow = nrow(counts))\nuncounts = ncounts[, grep(\"^untreated\", colnames(ncounts)),\n                     drop = FALSE]\nggplot(tibble(\n        mean = rowMeans(uncounts),\n        var  = rowVars( uncounts)),\n     aes(x = log(mean), y = log(var))) +\n  geom_hex() + coord_fixed() + theme(legend.position = \"none\") +\n  geom_abline(slope = 1:2, color = c(\"forestgreen\", \"red\"))\n\n그림 8.3: (크기 인자 보정된) counts 데이터에 대한 분산 대 평균. 축들은 로그 스케일입니다. 기울기가 1(초록색)과 2(빨간색)이며 원점을 지나는 선들도 표시되었습니다.\n초록색 선(기울기 1)은 분산(()v)이 평균(()m)과 같을 때 우리가 기대하는 것인데, 이는 푸아송 분포를 따르는 확률 변수의 경우와 같습니다: ()v=m. 우리는 이것이 하위 범위의 데이터에 대략적으로 들어맞는 것을 볼 수 있습니다. 빨간색 선(기울기 2)은 이차 평균-분산 관계인 ()v=m^2 에 대응합니다; 이와 평행한 선들(미표시)은 다양한 ()c 값에 대해 ()v = cm^2 를 나타낼 것입니다. 우리는 상위 범위의 데이터에서 어떤 ()c &lt; 1 값에 대해 이차 관계가 데이터에 대략적으로 들어맞는다는 것을 볼 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>고처리량 카운트 데이터 (High-Throughput Count Data)</span>"
    ]
  },
  {
    "objectID": "08-chap.html#기본적인-분석",
    "href": "08-chap.html#기본적인-분석",
    "title": "10  고처리량 카운트 데이터 (High-Throughput Count Data)",
    "section": "10.4 8.5 기본적인 분석",
    "text": "10.4 8.5 기본적인 분석\n\n10.4.1 8.5.1 예제 데이터 세트: pasilla 데이터\n8.3절에서 보았던 pasilla 데이터로 돌아가 봅시다. 이 데이터는 Drosophila melanogaster 세포 배양 실험으로부터 얻은 것으로, 스플라이싱 인자인 pasilla 유전자의 RNAi 넉다운이 세포의 전사체에 미치는 효과를 조사했습니다 (Brooks et al. 2011). 우리가 로드한 카운트 테이블의 헤더에는 두 가지 실험 조건인 untreated 와 treated 가 표시되어 있습니다. 이들은 각각 음성 대조군과 pasilla 에 대한 siRNA 처리에 해당합니다. 이 데이터 세트에 포함된 7개 샘플의 실험 메타데이터는 스프레드시트 형태의 표로 제공되며, 이를 로드합니다.\n\n\n\n여기서 보여주는 코드에서는 pasilla 패키지와 함께 제공되는 pasilla_sample_annotation.csv 파일을 로드합니다. 우리는 system.file 함수를 사용하여 그 위치를 찾습니다. 여러분 자신의 데이터로 작업할 때는 이와 유사한 파일이나, 또는 직접 pasillaSampleAnno와 같은 데이터 프레임을 준비해야 할 것입니다.\n\n\n여기서 보여주는 코드에서는 pasilla 패키지와 함께 제공되는 pasilla_sample_annotation.csv 파일을 로드합니다. 우리는 system.file 함수를 사용하여 그 위치를 찾습니다. 여러분 자신의 데이터로 작업할 때는 이와 유사한 파일이나, 또는 직접 pasillaSampleAnno와 같은 데이터 프레임을 준비해야 할 것입니다.\nannotationFile = system.file(\"extdata\",\n  \"pasilla_sample_annotation.csv\",\n  package = \"pasilla\", mustWork = TRUE)\npasillaSampleAnno = readr::read_csv(annotationFile)\npasillaSampleAnno\n\n\n# A tibble: 7 × 6\n  file    condition type  `number of lanes` total number of read…¹ `exon counts`\n  &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                          &lt;dbl&gt;\n1 treate… treated   sing…                 5 35158667                    15679615\n2 treate… treated   pair…                 2 12242535 (x2)               15620018\n3 treate… treated   pair…                 2 12443664 (x2)               12733865\n4 untrea… untreated sing…                 2 17812866                    14924838\n5 untrea… untreated sing…                 6 34284521                    20764558\n6 untrea… untreated pair…                 2 10542625 (x2)               10283129\n7 untrea… untreated pair…                 2 12214974 (x2)               11653031\n# ℹ abbreviated name: ¹​`total number of reads`\n여기서 보듯이, 전체 데이터 세트는 두 개의 배치(batch)로 생성되었습니다. 첫 번째 배치는 단일 리드(single-read) 시퀀싱을 거친 3개의 시퀀싱 라이브러리로 구성되었고, 두 번째 배치는 쌍말단(paired-end) 시퀀싱이 사용된 4개의 라이브러리로 구성되었습니다. 종종 그렇듯이, 우리는 약간의 데이터 가공(wrangling)이 필요합니다: type 열의 하이픈(-)을 언더스코어(_)로 바꿉니다. 왜냐하면 DESeq2 에서는 요인 수준(factor levels)에 산술 연산자를 사용하는 것을 권장하지 않기 때문입니다. 그리고 type과 condition 열을 요인(factors)으로 변환하면서, 우리가 선호하는 수준의 순서를 명시적으로 지정합니다(기본값은 알파벳순입니다).\nlibrary(\"dplyr\")\npasillaSampleAnno = mutate(pasillaSampleAnno,\ncondition = factor(condition, levels = c(\"untreated\", \"treated\")),\ntype = factor(sub(\"-.*\", \"\", type), levels = c(\"single\", \"paired\")))\n우리는 설계(design)가 관심 요인인 condition과 “성가신 요인(nuisance factor)”인 type 사이에서 대략적으로 균형을 이루고 있음에 주목합니다:\nwith(pasillaSampleAnno,\n       table(condition, type))\n\n\n           type\ncondition   single paired\n  untreated      2      2\n  treated        1      2\nDESeq2 는 작업하는 데이터 세트를 저장하기 위해 DESeqDataSet 이라 불리는 특수 데이터 컨테이너를 사용합니다. 이러한 특수 컨테이너 — 또는 R 용어로 클래스(classes) — 의 사용은 관련 데이터를 함께 묶어두는 데 도움이 되기 때문에 바이오컨덕터 프로젝트의 공통적인 원칙입니다. 이러한 방식은 matrix 나 dataframe과 같은 기본 R 데이터 유형만을 사용하는 것에 비해 사용자가 클래스를 이해하기 위해 초기에 약간 더 많은 시간을 투자해야 하지만, 데이터의 관련 부분들 사이의 동기화 손실로 인한 버그를 피하는 데 도움이 됩니다. 또한 기본 용어로 항상 표현한다면 상당히 장황해질 수 있는 일반적인 연산들의 추상화와 캡슐화를 가능하게 합니다5. DESeqDataSet 은 바이오컨덕터의 SummarizedExperiment 클래스의 확장입니다. SummarizedExperiment 클래스는 다른 많은 패키지에서도 사용되므로, 이를 다루는 법을 익히면 상당히 다양한 도구들을 사용할 수 있게 될 것입니다.\n5 또 다른 장점은 클래스가 유효성(validity) 메서드를 포함할 수 있다는 것인데, 이는 데이터가 항상 특정 기대치(예: 카운트는 양의 정수여야 함, 카운트 행렬의 열이 샘플 어노테이션 데이터 프레임의 행과 일치해야 함 등)를 충족하는지 확인해 줍니다.\n6 아래 코드에서 counts 객체의 열 이름과 pasillaSampleAnno 데이터 프레임의 file 열을 일치시키기 위해 추가적인 작업이 필요함에 주목하세요. 특히 file 열에서 왠지 모르게 사용된 “fb”를 제거해야 합니다. 이러한 데이터 가공은 매우 흔한 일입니다. 데이터를 DESeqDataSet 객체에 저장하는 이유 중 하나는 일단 저장하고 나면 더 이상 그런 걱정을 할 필요가 없기 때문입니다.\n우리는 생성자 함수 DESeqDataSetFromMatrix를 사용하여 카운트 데이터 행렬 counts와 샘플 주석 데이터 프레임 pasillaSampleAnno로부터 DESeqDataSet 을 만듭니다6.\nmt = match(colnames(counts), sub(\"fb$\", \"\", pasillaSampleAnno$file))\nstopifnot(!any(is.na(mt)))\n\npasilla = DESeqDataSetFromMatrix(\n  countData = counts,\n  colData   = pasillaSampleAnno[mt, ],\n  design    = ~ condition)\nclass(pasilla)\n\n\n[1] \"DESeqDataSet\"\nattr(,\"package\")\n[1] \"DESeq2\"\n\n\nis(pasilla, \"SummarizedExperiment\")\n\n\n[1] TRUE\nSummarizedExperiment 클래스 — 따라서 DESeqDataSet — 는 카운트 행렬 행(rows)의 어노테이션을 저장하기 위한 기능도 포함하고 있습니다. 지금은 counts 표의 행 이름에 있는 유전자 식별자로 만족하겠습니다.\n질문 8.3\n우리는 어떻게 SummarizedExperiment 객체의 행 메타데이터에 접근할 수 있을까요? 즉, 어떻게 읽어내고, 어떻게 변경할 수 있을까요?\n해결책\nSummarizedExperiment 클래스와 rowData 및 rowData&lt;- 메서드의 매뉴얼 페이지를 확인해 보세요.\n\n\n10.4.2 8.5.2 DESeq2 방법\n이러한 준비를 마친 후, 우리는 이제 곧바로 차등 발현 분석으로 뛰어들 준비가 되었습니다. 우리의 목표는 처리된 세포와 처리되지 않은 세포 사이에서 풍부도가 차이 나는 유전자를 식별하는 것입니다. 이를 위해 우리는 6.5절에서 접했던 (t)-검정과 개념적으로 유사하지만 수학적으로는 좀 더 복잡한 검정을 적용할 것입니다. 이러한 세부 사항은 일단 미뤄두고 8.7절에서 다시 다룰 것입니다. 일련의 표준 분석 단계들이 DESeq라는 단일 함수로 묶여 있습니다.\npasilla = DESeq(pasilla)\nDESeq 함수는 단순히 estimateSizeFactors (8.4.2절에서 논의한 정규화를 위해), estimateDispersions (분산 추정) 및 nbinomWaldTest (차등 풍부도에 대한 가설 검정) 함수를 순서대로 호출하는 래퍼(wrapper)입니다. 검정은 요인 condition의 두 수준인 untreated와 treated 사이에서 이루어지는데, 이는 우리가 design=~condition 인수를 통해 pasilla 객체를 구성할 때 지정한 것이기 때문입니다. 그들의 거동을 수정하거나 사용자 정의 단계를 삽입하고 싶다면 언제든지 이 세 함수를 개별적으로 호출할 수 있습니다. 결과를 살펴봅시다.\nres = results(pasilla)\nres[order(res$padj), ] |&gt; head()\n\n\nlog2 fold change (MLE): condition treated vs untreated \nWald test p-value: condition treated vs untreated \nDataFrame with 6 rows and 6 columns\n             baseMean log2FoldChange     lfcSE      stat       pvalue\n            &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;    &lt;numeric&gt;\nFBgn0039155   730.596       -4.61901 0.1687068  -27.3789 4.88599e-165\nFBgn0025111  1501.411        2.89986 0.1269205   22.8479 1.53430e-115\nFBgn0029167  3706.117       -2.19700 0.0969888  -22.6521 1.33042e-113\nFBgn0003360  4343.035       -3.17967 0.1435264  -22.1539 9.56283e-109\nFBgn0035085   638.233       -2.56041 0.1372952  -18.6490  1.28772e-77\nFBgn0039827   261.916       -4.16252 0.2325888  -17.8965  1.25663e-71\n                    padj\n               &lt;numeric&gt;\nFBgn0039155 4.06661e-161\nFBgn0025111 6.38497e-112\nFBgn0029167 3.69104e-110\nFBgn0003360 1.98979e-105\nFBgn0035085  2.14354e-74\nFBgn0039827  1.74316e-68\n\n\n10.4.3 8.5.3 결과 탐색하기\n차등 발현 분석 후의 첫 번째 단계는 다음의 서너 가지 기본 플롯을 시각화하는 것입니다:\n\np-값의 히스토그램 (그림 8.4),\nMA 플롯 (그림 8.5), 그리고\n서열화 플롯(ordination plot) (그림 8.6).\n추가로, 히트맵 (그림 8.7)도 유익할 수 있습니다.\n\n이들은 필수적인 데이터 품질 평가 척도입니다 — 13.6절에서 제공된 품질 평가 및 제어에 대한 일반적인 조언도 여기에서 똑같이 적용됩니다.\np-값 히스토그램은 직관적입니다 (그림 8.4).\nggplot(as(res, \"data.frame\"), aes(x = pvalue)) +\n  geom_histogram(binwidth = 0.01, fill = \"Royalblue\", boundary = 0)\n\n그림 8.4: 차등 발현 분석의 p-값 히스토그램.\n분포는 두 가지 주요 성분을 보여줍니다: 0과 1 사이의 값을 갖는 균등한 배경(background)과, 왼쪽의 작은 p-값들의 정점입니다. 균등한 배경은 차등 발현되지 않는 유전자에 해당합니다. 대개 이것이 유전자의 대다수입니다. 왼쪽의 정점은 차등 발현되는 유전자에 해당합니다7. 우리가 이미 6장에서 보았듯이, 배경의 수준과 정점의 높이 사이의 비율은 가장 왼쪽 빈(bin)에 있는 유전자를 차등 발현된 것으로 판정할 때 수반될 허위 발견율(FDR)에 대한 대략적인 지표를 제공합니다. 우리의 경우, 가장 왼쪽 빈은 0과 0.01 사이의 모든 p-값을 포함하며 이는 993개 유전자에 해당합니다. 배경 수준은 약 100이므로, 가장 왼쪽 빈의 모든 유전자를 판정하는 것과 관련된 FDR은 약 10%가 될 것입니다.\n7 여기서 보여주는 데이터의 경우, 히스토그램은 중간이나 오른쪽에 몇 개의 고립된 정점들도 포함하고 있습니다; 이들은 카운트가 적은 유전자들로부터 유래하며 데이터의 이산성을 반영합니다.\n때때로 배경 분포가 균등하지 않고 오른쪽으로 갈수록 증가하는 기울어진 형태를 보이는 경우가 있습니다. 이는 대개 배치 효과의 징후입니다; 연습 문제 8.1에서 이를 더 탐구해 볼 수 있습니다.\nMA 플롯을 생성하기 위해 우리는 DESeq2 패키지의 plotMA 함수를 사용할 수 있습니다 (그림 8.5).\nplotMA(pasilla, ylim = c( -2, 2))\n\n그림 8.5: MA 플롯: 폴드 변화(fold change) 대 크기 인자로 정규화된 카운트의 평균. 두 축 모두 로그 스케일이 사용되었습니다. 기본적으로 조정된 p-값이 0.1보다 작으면 점들은 빨간색으로 표시됩니다. (y)축 범위를 벗어나는 점들은 삼각형으로 표시됩니다.\n7장에서 보았던 것과 유사한 PCA 플롯을 생성하기 위해 우리는 DESeq2 의 plotPCA 함수를 사용할 수 있습니다 (그림 8.6).\npas_rlog = rlogTransformation(pasilla)\nplotPCA(pas_rlog, intgroup=c(\"condition\", \"type\")) + coord_fixed()\n\n그림 8.6: PCA 플롯. 7개의 샘플이 처음 두 주성분에 의해 확장된 2D 평면에 표시되어 있습니다.\n이전 장에서 보았듯이, 이 유형의 플롯은 실험 공변량의 전체적인 효과를 시각화하거나 배치 효과를 탐지하는 데 유용합니다. 여기서 첫 번째 주축인 PC1은 주로 관심 있는 실험 공변량(untreated / treated)과 일치하며, 두 번째 축은 대략 시퀀싱 프로토콜(single / paired)과 일치합니다.\n우리는 데이터 변환의 일종인 정규화 로그(regularized logarithm) 또는 rlog 를 사용했는데, 이에 대해서는 8.10.2절에서 더 자세히 살펴볼 것입니다.\n질문 8.4\nPCA 플롯의 축들이 항상 알려진 실험 공변량과 일치해야 하나요?\n히트맵은 카운트 테이블을 포함한 행렬 형태의 데이터 세트에 대한 개요를 빠르게 얻을 수 있는 강력한 방법이 될 수 있습니다. 아래에서 rlog 변환된 데이터로부터 히트맵을 만드는 방법을 볼 수 있습니다. counts(pasilla)만큼 큰 행렬의 경우 전체를 플롯하는 것은 실용적이지 않으므로, 평균 발현량이 가장 높은 상위 30개 유전자의 하위 행렬을 플롯합니다.\nlibrary(\"pheatmap\")\nselect = order(rowMeans(assay(pas_rlog)), decreasing = TRUE)[1:30]\npheatmap( assay(pas_rlog)[select, ],\n     scale = \"row\",\n     annotation_col = as.data.frame(\n        colData(pas_rlog)[, c(\"condition\", \"type\")] ))\n\n그림 8.7: 상위 30개 유전자의 정규화 로그 변환된 데이터 히트맵.\n그림 8.7에서 pheatmap은 비지도 군집화(unsupervised clustering)를 통한 덴드로그램에 따라 행과 열을 정렬했습니다. 열의 군집화 결과는 type 인자에 의해 지배되고 있습니다. 이는 우리가 위에서 수행했던 기본적인 차등 발현 분석이 아마도 너무 나이브했으며, 조건들 간의 차등 발현 유전자를 테스트할 때 이 강력한 “성가신” 인자를 조정해야 함을 강조합니다. 우리는 8.9절에서 이 작업을 수행할 것입니다.\n태스크\n가장 가변적인 상위 30개 유전자를 선택하여 그림 8.7과 유사한 플롯을 생성해 보세요. 무엇이 다른가요? 평균이 매우 높은 유전자들과 분산이 매우 높은 유전자들은 서로 어떤 관계가 있나요? 그들의 데이터는 어떻게 보이나요?\n\n\n10.4.4 8.5.4 결과 내보내기\n정렬 및 필터링이 가능한 열과 플롯이 포함된 결과의 HTML 보고서는, DESeq 함수에 의해 처리된 DESeqDataSet 에 대해 ReportingTools 패키지를 사용하여 내보낼 수 있습니다. 코드 예시는 ReportingTools 패키지의 RNA-Seq differential expression 비네트나, DESeqDataSet 클래스에 대한 publish 메서드 매뉴얼 페이지를 참조하세요.\n결과의 CSV 파일은 write.csv (또는 readr 패키지의 해당 함수)를 사용하여 내보낼 수 있습니다.\nwrite.csv(as.data.frame(res), file = \"treated_vs_untreated.csv\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>고처리량 카운트 데이터 (High-Throughput Count Data)</span>"
    ]
  },
  {
    "objectID": "08-chap.html#기본-선택-사항에-대한-비판-및-가능한-수정-사항",
    "href": "08-chap.html#기본-선택-사항에-대한-비판-및-가능한-수정-사항",
    "title": "10  고처리량 카운트 데이터 (High-Throughput Count Data)",
    "section": "10.5 8.6 기본 선택 사항에 대한 비판 및 가능한 수정 사항",
    "text": "10.5 8.6 기본 선택 사항에 대한 비판 및 가능한 수정 사항\n\n10.5.1 8.6.1 ‘소수 변화’ 가정 (The few changes assumption)\nDESeq2 (및 다른 많은 차등 발현 방법들)에서 기본 정규화와 분산 추정의 기저에는 대부분의 유전자가 차등 발현되지 않는다는 가정이 깔려 있습니다.\n\n\n\n정규화의 경우, 분산 추정의 경우만큼은 아니지만, 이 가정을 약간 완화할 수 있습니다: 많은 유전자가 변하더라도 상향 및 하향 방향으로 균형을 이루고 있다면 여전히 유효합니다.\n\n\n정규화의 경우, 분산 추정의 경우만큼은 아니지만, 이 가정을 약간 완화할 수 있습니다: 많은 유전자가 변하더라도 상향 및 하향 방향으로 균형을 이루고 있다면 여전히 유효합니다.\n이 가정은 종종 합리적이지만(잘 설계된 실험은 대개 구체적인 질문을 던지므로, 모든 것이 한꺼번에 변하지는 않습니다), 만약 이 가정이 성립하지 않는다면 어떻게 해야 할까요? 그럴 때는 모든 유전자의 데이터에 대해 이러한 연산들을 적용하는 대신, 사전 생물학적 지식이나 명시적으로 통제된 외부 “스파이크-인(spiked-in)” 특징들을 통해 해당 가정이 성립한다고 믿는 (“음성 대조군”) 유전자들의 하위 집합을 식별해야 할 것입니다.\n태스크\n미리 정의된 유전자 하위 집합으로부터만 추정된 크기 인자와 분산 매개변수를 사용하여 DESeq2 워크플로를 실행해 보세요.\n\n\n10.5.2 8.6.2 점 가설 (Point-like null hypothesis)\n기본적으로 DESeq 함수는 각 유전자가 조건들 간에 동일한 풍부도를 가진다는 귀무 가설에 대해 검정합니다; 이는 단순하고 실용적인 선택입니다. 사실 표본 크기가 제한적이라면, 통계적으로 유의미한 것이 생물학적으로도 흥미로운 강도를 가지는 경향이 있습니다. 하지만 표본 크기가 증가함에 따라, 생물학적 관련성은 크지 않으면서도 통계적 유의성은 나타날 수 있습니다. 예를 들어, 많은 유전자들이 하류의 간접적인 효과에 의해 약간 섭동될 수 있습니다. 우리는 더 허용적인, 구간 기반의 귀무 가설을 사용하도록 검정을 수정할 수 있습니다; 우리는 8.10.4절에서 이를 더 탐구할 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>고처리량 카운트 데이터 (High-Throughput Count Data)</span>"
    ]
  },
  {
    "objectID": "08-chap.html#다요인-설계-및-선형-모델",
    "href": "08-chap.html#다요인-설계-및-선형-모델",
    "title": "10  고처리량 카운트 데이터 (High-Throughput Count Data)",
    "section": "10.6 8.7 다요인 설계 및 선형 모델",
    "text": "10.6 8.7 다요인 설계 및 선형 모델\n\n10.6.1 8.7.1 다요인 설계란 무엇인가?\nsiRNA를 이용한 pasilla 유전자 넉다운 외에도, 우리가 특정 약물의 효과도 테스트하고 싶다고 가정해 봅시다. 우리는 세포를 음성 대조군으로 처리하거나, pasilla에 대한 siRNA로 처리하거나, 약물로 처리하거나, 또는 둘 다로 처리하는 실험을 구상할 수 있습니다. 이 실험을 분석하기 위해 다음과 같은 표기법을 사용할 수 있습니다:\n[ y = _0 + x_1 _1 + x_2 2 + x_1x_2{12}. ]\n이 방정식은 다음과 같이 해석될 수 있습니다. 좌변인 (y)는 관심 있는 실험 측정값입니다. 우리의 경우, 이는 유전자의 적절하게 변환된 발현 수준입니다 (이에 대해서는 8.8.3절에서 논의할 것입니다). RNA-Seq 실험에는 수많은 유전자가 있으므로, 각 유전자에 대해 하나씩 식 8.1과 같은 수많은 방정식을 갖게 될 것입니다. 계수 (_0)는 음성 대조군에서의 측정 기본 수준이며; 종종 절편(intercept) 이라고 불립니다.\n\n\n\n때때로 식 8.1은 _0와 곱해지는 추가 항 x_0를 포함하여 쓰여지기도 하는데, 이때 x_0는 항상 1로 이해됩니다. 이렇게 하면 절편을 별개의 사례로 다루는 대신 다른 함께 일관되게 처리할 수 있어 이후의 표기법과 장부 관리가 더 쉬워집니다.\n\n\n때때로 식 8.1은 (_0)와 곱해지는 추가 항 (x_0)를 포함하여 쓰여지기도 하는데, 이때 (x_0=1)은 항상 성립하는 것으로 이해됩니다. 이렇게 하면 절편을 별개의 사례로 다루는 대신 다른 ()들과 함께 일관되게 처리할 수 있어 이후의 표기법과 장부 관리가 더 쉬워집니다.\n설계 요인 (x_1)과 (x_2)는 이진 지시 변수(binary indicator variables)입니다: (x_1)은 siRNA가 형질감염(transfected)되었으면 1, 아니면 0의 값을 가지며, 유사하게 (x_2)는 약물이 투여되었는지 여부를 나타냅니다. siRNA만 사용된 실험에서는 (x_1=1)이고 (x_2=0)이므로, 식 8.1의 세 번째와 네 번째 항은 사라집니다. 그러면 방정식은 (y=_0+_1)로 단순화됩니다. 이는 (_1)이 처리군과 대조군 사이의 차이를 나타냄을 의미합니다. 만약 우리의 측정값이 로그 스케일이라면,\n[ \\[\\begin{align} \\beta_1 = y-\\beta_0 &=\\log_2(\\text{expression}_{\\text{treated}}) -\\log_2(\\text{expression}_{\\text{untreated}})\\\\ &=\\log_2\\frac {\\text{expression}_{\\text{treated}}} {\\text{expression}_{\\text{untreated}}} \\end{align}\\] ]\n은 siRNA 처리에 의한 로그 폴드 변화(logarithmic fold change)입니다. 정확히 같은 방식으로, (_2)는 약물 처리에 의한 로그 폴드 변화입니다. 세포에 siRNA와 약물을 모두 처리하면 어떻게 될까요? 그 경우 (x_1=x_2=1)이며, 식 8.1은 다음과 같이 다시 쓰여질 수 있습니다:\n[ _{12} = y - (_0 + _1 + _2). ]\n이는 (_{12})가 관찰된 결과인 (y)와, 개별 처리들로부터 예상되는 결과(기본 수준에 siRNA 단독 효과 (_1) 및 약물 단독 효과 (_2)를 더하여 얻음) 사이의 차이임을 의미합니다.\n우리는 (_{12})를 siRNA와 약물의 상호작용(interaction) 효과라고 부릅니다. 이는 물리적인 상호작용과는 아무런 관련이 없으며, 이 용어는 두 가지 서로 다른 실험 요인의 효과가 단순히 더해지는 것이 아니라 더 복잡한 방식으로 결합됨을 나타냅니다.\n\n\n\n덧셈은 로그 스케일에서 이루어지며, 이는 원래 스케일에서의 곱셈에 해당함에 유의하세요.\n\n\n덧셈은 로그 스케일에서 이루어지며, 이는 원래 스케일에서의 곱셈에 해당함에 유의하세요.\n예를 들어, 만약 약물의 타겟과 siRNA의 타겟이 동등하여 세포에 동일한 효과를 준다면, 우리는 생물학적으로 (_1=2)일 것으로 예상합니다. 또한 우리는 그들의 조합이 추가적인 효과를 주지 않을 것으로 예상하므로, ({12}=-_1)이 됩니다. 반면에, 만약 약물의 타겟과 siRNA의 타겟이 서로 완충 작용을 할 수 있는 평행한 경로에 있다면, (_1)과 (2)는 둘 다 비교적 작겠지만 결합된 효과는 시너지 작용을 하여 ({12})가 클 것으로 예상할 수 있습니다.\n우리가 항상 상호작용에 관심을 갖는 것은 아닙니다. 많은 실험들이 여러 요인을 가지고 설계되지만 각각의 개별 효과에 가장 큰 관심을 가집니다. 그 경우, 조합 처리는 실험 설계에 포함되지 않을 수 있으며, 분석에 사용할 모델은 식 8.1에서 가장 오른쪽 항을 제거한 버전입니다.\n우리는 실험 설계를 설계 행렬(design matrix) 로 간결하게 인코딩할 수 있습니다. 예를 들어, 위에서 설명한 조합 실험의 경우 설계 행렬은 다음과 같습니다:\n[\n\\[\\begin{array}{c|c|c} x_0 & x_1 & x_2\\ \\hline 1&0&0\\ 1&1&0\\ 1&0&1\\ 1&1&1\\end{array} \\tag{8.4}\\]\\]\n설계 행렬의 열들은 실험 요인들에 대응하고, 행들은 서로 다른 실험 조건들(우리의 경우 4가지)을 나타냅니다. 만약 조합 처리가 수행되지 않는다면, 설계 행렬은 8.4의 처음 세 행으로만 축소됩니다.\n\n\n10.6.2 8.7.2 노이즈와 반복 실험(Replicates)은 어떠한가요?\n식 8.1은 관측된 데이터를 서로 다른 실험 변수들에 의해 발생한 효과들로 개념적으로 분해하는 방법을 제공합니다. 만약 우리의 데이터((y)들)가 절대적으로 정확하다면, 우리는 (x)들로 표현되는 네 가지 가능한 실험 조건 각각에 대해 하나씩 선형 방정식 시스템을 세우고 ()들을 풀 수 있을 것입니다.\n물론, 우리는 대개 노이즈의 영향을 받는 실제 데이터를 분석하고자 합니다. 우리는 반복 실험(replicates)을 필요로 하여 노이즈 수준을 추정하고 추정된 ()들의 불확실성을 평가합니다. 그래야만 조건 사이에서 관찰된 변화가 단지 실험적 또는 자연적 변동에 의해 발생하는 것보다 유의미하게 큰지 경험적으로 평가할 수 있습니다. 우리는 방정식을 약간 확장해야 합니다:\n[ y_{j} = x_{j0} ; 0 + x{j1} ; 1 + x{j2} ; 2 + x{j1},x_{j2},_{12} + _j. ]\n우리는 인덱스 (j)와 새로운 항 (j)를 추가했습니다. 인덱스 (j)는 이제 우리의 개별 반복 실험들을 명시적으로 셉니다; 예를 들어, 네 가지 조건 각각에 대해 세 번의 반복 실험을 수행한다면 (j)는 1부터 12까지 셉니다. 설계 행렬은 이제 12개의 행을 가지며, (x{jk})는 행렬의 (j)번째 행과 (k)번째 열의 값입니다.\n[](imgs/devil.png ” 0가 절편이므로 모든 j에 대해 x{j0}=1임을 기억하세요.”)\n(0)가 절편이므로 모든 (j)에 대해 (x{j0}=1)임을 기억하세요.\n우리가 잔차(residuals) 라고 부르는 추가 항 (_j)는 반복 실험들 사이의 차이를 흡수하기 위해 존재합니다. 하지만 한 가지 추가적인 모델링 구성 요소가 필요합니다: 12개의 방정식 시스템 8.5는 추가 정보 없이는 과소 결정(underdetermined)될 것인데, 왜냐하면 방정식의 수(12개, 각 (j)에 대해 하나씩)보다 변수의 수(12개의 입실론과 4개의 베타)가 더 많기 때문입니다. 이를 해결하기 위해 우리는 (_j)가 작을 것을 요구합니다. 이를 극복하기 위해 널리 쓰이는 한 가지 방법은 — 다른 방법들도 만나게 되겠지만 — 잔차 제곱합을 최소화하는 것입니다:\n[ _j _j^2 . ]\n이 요구 조건이 충족되면, ()들은 각 실험 요인의 평균적 효과를 나타내고, 잔차 (_j)는 반복 실험들 사이의 평균 주변의 실험적 변동을 반영하게 됩니다. 최소제곱법(least sum of squares) 적합 이라 불리는 이 접근 방식은 간단한 행렬 대수로 달성될 수 있기 때문에 수학적으로 편리합니다. 이것이 R 함수 lm이 하는 일입니다.\n질문 8.5\n식 8.5를 쓰는 대안적인 방법은 다음과 같습니다:\n[ y_{j} = k x{jk} ; _k + _j. ]\n이것이 어떻게 식 8.5와 매핑될 수 있을까요? 즉, 상호작용 항 (x_{j1},x_{j2},_{12})는 어떻게 된 것인가요?\n해결책\n이것은 정말 사소한 표기법의 문제입니다: 합은 (k=0,…,3)에 대해 확장되며, (k=0,1,2)에 대한 항들은 우리가 이미 알고 있는 것과 정확히 같습니다. 우리는 ({12}) 대신 ({3})라고 쓰고, (x_{j3})는 (x_{j1}x_{j2})로 정의됩니다. 일반적인 표기법 8.7은 선형 모델을 구현하는 컴퓨터 소프트웨어와 수학적 증명에서 사용하기에 실용적입니다. 또한 선형 모델의 “과학적 내용”이 그 설계 행렬 (X)에 응축되어 있음을 강조합니다.\n태스크\n목적 함수 8.6이 성립하도록 식 8.5를 데이터에 적합시켰다면, 적합 잔차 (_j)의 평균이 0임을 보이세요.\n\n\n10.6.3 8.7.3 분산 분석 (Analysis of variance)\n8.5와 같은 모델을 선형 모델(linear model) 이라고 부르며, 종종 기준 8.6이 데이터를 적합시키는 데 사용됨을 암시합니다. 이 접근 방식은 우아하고 강력하지만, 초보자가 그 모든 측면을 이해하는 데는 시간이 좀 걸릴 수 있습니다. 각기 다른 실험 조건에 대해 단순히 반복 실험들의 평균을 취하고 이 값들을 조건들 사이에서 비교하는 것에 비해 어떤 이점이 있을까요? 단순한 경우에 후자의 접근 방식은 직관적이고 효과적일 수 있습니다. 하지만 서로 다른 그룹에서 반복 실험의 수가 모두 같지 않거나, 하나 이상의 (x)-변수가 연속형 값을 가질 때 한계에 부딪히게 됩니다. 이러한 경우, 결국 데이터에 8.5와 같은 것을 적합시키는 것으로 귀결될 것입니다. 8.5를 생각하는 유용한 방법은 분산 분석(analysis of variance), 약어로 ANOVA라는 용어에 담겨 있습니다. 사실 식 8.5가 하는 일은 우리가 실험 과정에서 관찰한 (y)의 가변성을 기초적인 성분들로 분해하는 것입니다: 기본 수준 값 (_0), 첫 번째 변수의 효과에 의해 발생하는 가변성 (_1), 두 번째 변수의 효과에 의해 발생하는 가변성 (2), 상호작용의 효과에 의해 발생하는 가변성 ({12}), 그리고 설명되지 않는 가변성입니다. 이들 중 마지막 것을 우리는 흔히 노이즈(noise) 라고 부르고, 다른 것들은 체계적 가변성(systematic variability) 이라고 부릅니다.\n\n\n\n노이즈와 체계적 가변성의 구분은 보는 사람의 관점에 달려 있으며, 현실이 아니라 우리의 모델에 달려 있습니다.\n\n\n노이즈와 체계적 가변성의 구분은 보는 사람의 관점에 달려 있으며, 현실이 아니라 우리의 모델에 달려 있습니다.\n\n\n10.6.4 8.7.4 강건성 (Robustness)\n합 8.6은 데이터의 이상치(outliers)에 민감합니다. 이상치를 가진 단 하나의 측정값 (y_{j})가 () 추정치를 다른 반복 실험들에 의해 함축된 값들로부터 멀리 끌어당길 수 있습니다. 이는 최소제곱법에 기반한 방법들이 낮은 붕괴점(breakdown point) 을 갖는다는 잘 알려진 사실입니다: 단 하나의 데이터 포인트만 이상치여도 전체 통계 결과가 강력하게 영향을 받을 수 있습니다. 예를 들어, (n)개 숫자 세트의 평균은 ()의 붕괴점을 갖는데, 이는 숫자들 중 단 하나만 바꿈으로써 평균을 임의로 바꿀 수 있음을 의미합니다. 반면에 중앙값(median)은 훨씬 더 높은 붕괴점을 갖습니다. 숫자 하나를 바꾸는 것은 종종 아무런 영향을 주지 않으며, 영향이 있더라도 그 효과는 순위의 중간에 있는 데이터 포인트들의 범위(즉, () 순위에 인접한 것들)로 제한됩니다. 중앙값을 임의로 높게 바꾸려면 관측치의 절반을 바꾸어야 합니다. 우리는 중앙값을 강건(robust) 하다고 부르며, 그 붕괴점은 ()입니다. 숫자 세트 (y_1, y_2, …)의 중앙값이 합 (_j|y_j-_0|)을 최소화한다는 점을 기억하세요.\n이상치에 대해 더 높은 수준의 강건성을 달성하기 위해, 최소화의 목적 함수로 제곱합 8.6 대신 다른 선택지들이 사용될 수 있습니다. 그중에는 다음과 같은 것들이 있습니다:\n[ \\[\\begin{align} R &= \\sum_j |\\varepsilon_j| & \\text{최소 절대 편차 (Least absolute deviations)} \\\\ R &= \\sum_j \\rho_s(\\varepsilon_j) & \\text{M-추정 (M-estimation)} \\\\ R &= Q_{\\theta}\\,( \\{\\varepsilon_1^2, \\varepsilon_2^2,... \\} ) & \\text{LTS, LQS} \\\\ R &= \\sum_j w_j \\varepsilon_j^2 & \\text{일반화 가중 회귀 (general weighted regression)} \\end{align}\\] ]\n여기서 (R)은 최소화되어야 할 양입니다. 식 8.8의 첫 번째 선택지는 최소 절대 편차(least absolute deviations) 회귀라고 불립니다. 이는 중앙값의 일반화로 볼 수 있습니다. 개념적으로는 단순하고 언뜻 보기에 매력적이지만, 제곱합보다 최소화하기가 더 어렵고, 특히 데이터가 제한적이거나 모델에 잘 맞지 않을 때 덜 안정적이고 덜 효율적일 수 있습니다8. 식 8.8의 두 번째 선택지인 M-추정(M-estimation) 은 제한된 범위의 ()에 대해서는 이차 함수(quadratic function)처럼 보이지만, 절댓값 (||)이 스케일 매개변수 (s)보다 큰 경우에는 기울기가 더 작아지거나, 평평해지거나, 심지어 다시 0으로 떨어지는 페널티 함수 (_s)를 사용합니다 (최소제곱 회귀는 (_s()=^2)인 특수한 경우입니다). 이 이면의 의도는 이상치, 즉 큰 잔차를 가진 데이터 포인트의 효과를 낮게 가중하는 것입니다 (Huber 1964). (s)의 선택이 이루어져야 하며 이것이 무엇을 이상치로 부를지 결정합니다. 심지어 0 근처에서 (_s)가 이차 함수여야 한다는 요구 조건을 버릴 수도 있으며(그의 이계 도함수가 양수이기만 하다면), 문헌에서는 다양한 (_s) 함수 선택지가 제안되었습니다. 그 목표는 데이터가 모델에 잘 맞을 때와 그곳에서 추정기에 바람직한 통계적 속성(예: 편향과 효율성)을 부여하면서도, 그렇지 않은 데이터 포인트의 영향력을 제한하거나 무효화하고 계산을 다루기 쉽게 유지하는 것입니다.\n8 위키백과 문서에서 개요를 제공합니다.\n질문 8.6\nM-추정기를 위해 Huber (1964)가 제안한 함수 (_s())의 그래프를 그려보세요.\n해결책\nHuber의 논문은 75페이지에서 다음과 같이 정의합니다:\n[ _s() = {\n\\[\\begin{array}{cc} \\frac{1}{2}\\varepsilon^2, \\quad\\text{for }|\\varepsilon|&lt; s\\\\ s|\\varepsilon|-\\frac{1}{2}s^2, \\quad\\text{for }|\\varepsilon|\\ge s\\\\ \\end{array}\\]\n. ]\n아래 코드로 생성된 그래프는 그림 8.8에 나와 있습니다.\nrho = function(x, s)\n  ifelse(abs(x) &lt; s, x^2 / 2,  s * abs(x) - s^2 / 2)\n\ndf = tibble(\n  x        = seq(-7, 7, length.out = 100),\n  parabola = x ^ 2 / 2,\n  Huber    = rho(x, s = 2))\n\nggplot(reshape2::melt(df, id.vars = \"x\"),\n  aes(x = x, y = value, col = variable)) + geom_line()\n\n그림 8.8: (s=2)를 선택했을 때의 (_s()) 그래프.\n식 8.8의 세 번째 선택지는 최소제곱법을 또 다른 방식으로 일반화합니다. 최소 분위수 제곱(least quantile of squares, LQS) 회귀에서는 제곱 잔차의 합을 분위수로 대체합니다. 예를 들어, 중앙값인 (Q_{50}), 또는 90%-분위수인 (Q_{90})입니다 (Peter J. Rousseeuw 1987). 이의 변형인 최소 절단 제곱합(least trimmed sum of squares, LTS) 회귀에서는 모든 잔차가 아니라, 가장 작은 잔차들의 분율 (0)에 대해서만 제곱 잔차의 합을 사용합니다. 두 경우 모두의 동기는 이상치 데이터 포인트가 큰 잔차를 낳으며, 이들이 드문 한 분위수나 절단된 합에 영향을 주지 않는다는 것입니다.\n하지만 대가가 있습니다: 최소제곱 최적화 8.6은 직관적인 선형 대수를 통해 수행될 수 있는 반면, M-추정, LQS 및 LTS 회귀를 위해서는 더 복잡한 반복 최적화 알고리즘이 필요합니다.\n식 8.8의 마지막 접근 방식은 이상치를 하향 가중하는 훨씬 더 복잡한 방식을 나타냅니다. 이는 우리가 각 관측치에 어느 정도의 가중치 (w_j)를 주고 싶은지(아마도 이상치에 낮은 가중치를 주는 방식) 결정할 수 있는 어떤 방법을 가지고 있다고 가정합니다. 예를 들어, 8.10.3절에서 우리는 DESeq2 패키지에서 사용되는 접근 방식을 마주하게 될 것인데, 여기서는 쿡의 거리(Cook’s distance) 라 불리는 척도를 사용하여 각 데이터 포인트가 적합된 계수 ()들에 미치는 영향력을 평가합니다. 쿡의 거리가 너무 크다고 판단되는 데이터 포인트들에 대해서는 가중치 (w_j)를 0으로 설정하고, 다른 데이터 포인트들은 (w_j=1)을 갖게 됩니다. 결과적으로 이는 이상치 데이터 포인트들을 버리고 나머지에 대해 일반 회귀를 수행하는 것과 같습니다. 가중치를 가지고 가는 추가적인 계산 노력은 무시할 수 있으며, 최적화는 여전히 직관적인 선형 대수입니다.\n이상치 강건성에 대한 이러한 모든 접근 방식들은 어느 정도의 주관성을 도입하며 충분한 반복 실험에 의존합니다. 주관성은 내려야 하는 매개변수 선택에 반영됩니다: 식 8.8 (2)에서의 (s), 8.8 (3)에서의 (), 8.8 (4)에서의 가중치 등입니다. 한 과학자의 이상치가 다른 과학자의 노벨상이 될 수도 있습니다. 반면에, 이상치 제거가 허술한 실험에 대한 구제책이 되거나 희망 섞인 관측에 대한 정당화가 되어서는 안 됩니다.\n태스크\n위의 강건 회귀 방법들의 구현체를 R 및 CRAN 패키지들에서 찾아보세요. 시작하기 좋은 곳은 CRAN 태스크 뷰: 강건 통계 방법(Robust Statistical Methods)입니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>고처리량 카운트 데이터 (High-Throughput Count Data)</span>"
    ]
  },
  {
    "objectID": "08-chap.html#일반화-선형-모델",
    "href": "08-chap.html#일반화-선형-모델",
    "title": "10  고처리량 카운트 데이터 (High-Throughput Count Data)",
    "section": "10.7 8.8 일반화 선형 모델",
    "text": "10.7 8.8 일반화 선형 모델\n우리는 다음 응용 예제로 넘어가기 전에 두 가지 이론적 개념을 더 탐구해야 합니다. 식 8.5와 같은 형태의 방정식은 결과 변수 (y)의 기댓값을 설계 행렬의 선형 함수로 모델링하며, 최소제곱 기준 8.6(또는 그 강건한 변체)에 따라 데이터에 적합됩니다. 우리는 이제 이러한 가정들을 일반화하고자 합니다.\n\n10.7.1 8.8.1 변환된 스케일에서 데이터 모델링하기\n우리는 데이터를 얻은 그대로의 스케일이 아니라 어떤 변환(예를 들어 로그) 후에 고려하는 것이 유익할 수 있음을 이미 보았습니다. 이 아이디어는 일반화될 수 있는데, 맥락에 따라 다른 변환들이 유용하기 때문입니다. 예를 들어, 선형 모델 8.5는 질병 위험 지표와 같이 ([0,1]) 구간 내에 한정된 결과들을 모델링하는 데 직접적으로 유용하지는 않을 것입니다. 선형 모델에서 (y)의 값은 원칙적으로 실수축 전체를 포괄합니다. 하지만 만약 우리가 우변의 표현식을 시그모이드(sigmoid) 함수, 예를 들어 (f(y) = 1/(1+e^{-y}))로 변환한다면, 이 함수의 범위9는 0과 1 사이로 한정되어 그러한 결과를 모델링하는 데 사용될 수 있습니다.\n9 이는 로지스틱(logistic) 함수라 불리며 (Verhulst 1845), 연관된 회귀 모델을 로지스틱 회귀 라고 합니다.\n\n\n10.7.2 8.8.2 다른 오차 분포\n다른 일반화는 최소화 기준 8.6에 관한 것입니다. 사실, 이 기준은 특정한 확률 모델과 최대 우도 원리로부터 도출될 수 있습니다 (우리는 이미 2장에서 이를 접했습니다). 이를 보기 위해, 다음과 같은 확률 모델을 고려해 봅시다:\n[ p(_j) = (-), ]\n즉, 우리는 잔차들이 평균 0, 표준 편차 ()인 정규 분포를 따른다고 믿습니다. 그렇다면 좋은 모델(즉, 좋은 ()들의 세트)에 대해 이러한 확률들이 커야 한다고 요구하는 것이 타당합니다. 형식적으로는,\n[ _j p(_j) . ]\n질문 8.7\n우도 8.10을 최대화하는 것이 제곱 잔차의 합 8.6을 최소화하는 것과 동등함을 보이세요.\n해결책\n식 8.9를 8.10에 대입하고 로그를 취해 보세요.\n핵심 개념들을 다시 정리해 봅시다: 식 8.10의 좌변, 즉 잔차들 확률의 곱은 모델 매개변수 (_1, _2, …)와 데이터 (y_1, y_2, …) 모두의 함수입니다; 이를 (f(,y))라고 부릅시다. 만약 우리가 모델 매개변수 ()가 주어져 고정되어 있다고 생각한다면, 축소된 함수 (f(y))는 단순히 데이터의 확률을 나타냅니다. 우리는 이를 예를 들어 데이터를 시뮬레이션하는 데 사용할 수 있을 것입니다. 만약 반대로 우리가 데이터를 주어진 것으로 간주한다면, (f())는 모델 매개변수들의 함수이며, 이를 우도(likelihood) 라고 부릅니다. 두 번째 관점이 우리가 식 8.6(따라서 8.10)을 최적화할 때 취하는 관점이며, 따라서 이런 방식으로 얻은 ()들을 최대 우도 추정치(maximum-likelihood estimates) 라고 부릅니다.\n\n\n\n우리가 최대 우도 원리를 통해 최소제곱 회귀를 정당화하기 위해 정규 분포를 편리한 논거로 사용할 수는 있지만, 최소제곱 회귀가 유용한 결과를 주기 위해 데이터가 반드시 정규 분포를 따라야 하는 것은 아님을 기억하는 것이 좋습니다. 사실 최소제곱 적합은 데이터가 비정규 분포인 경우에도 대한 유용한 추정치를 제공하는 경우가 많지만, 이는 구체적인 상황에 따라 달라집니다.\n\n\n우리가 최대 우도 원리를 통해 최소제곱 회귀를 정당화하기 위해 정규 분포를 편리한 논거로 사용할 수는 있지만, 최소제곱 회귀가 유용한 결과를 주기 위해 데이터가 반드시 정규 분포를 따라야 하는 것은 아님을 기억하는 것이 좋습니다. 사실 최소제곱 적합은 데이터가 비정규 분포인 경우에도 ()들에 대한 유용한 추정치를 제공하는 경우가 많지만, 이는 구체적인 상황에 따라 달라집니다.\n우리가 이제 할 수 있는 일반화는 다른 확률 모델을 사용하는 것입니다. 우리는 식 8.9 대신 정규 분포 이외의 다른 분포들의 밀도를 사용할 수 있습니다. 예를 들어, 카운트 데이터를 다룰 수 있기 위해 우리는 감마-포아송 분포를 사용할 것입니다.\n\n\n10.7.3 8.8.3 카운트 데이터를 위한 일반화 선형 모델\nDESeq2 에서의 차등 발현 분석은 다음과 같은 형태의 일반화 선형 모델을 사용합니다:\n[ \\[\\begin{align} K_{ij} & \\sim \\text{GP}(\\mu_{ij}, \\alpha_i) \\\\ \\mu_{ij} &= s_j\\, q_{ij} \\\\ \\log_2(q_{ij}) &= \\sum_k x_{jk} \\beta_{ik}. \\end{align}\\] ]\n하나씩 풀어봅시다. 유전자 (i), 샘플 (j)에 대한 카운트 (K_{ij})는 평균 ({ij})와 분산(dispersion) (i)를 가진 두 개의 매개변수가 있는 감마-포아송(GP) 분포를 사용하여 모델링됩니다. 기본적으로 분산은 각 유전자 (i)마다 다르지만 모든 샘플에 걸쳐 동일하므로 인덱스 (j)가 없습니다. 식 8.11의 두 번째 줄은 평균이 샘플 특이적 크기 인자 (s_j)10와, 샘플 (j)에서의 유전자 (i)에 대한 단편들의 실제 기대 농도에 비례하는 (q{ij})로 구성됨을 나타냅니다. (q{ij})의 값은 세 번째 줄의 선형 모델에 의해 연결 함수(link function) 인 (2)를 통해 주어집니다. 설계 행렬 ((x{jk}))는 모든 유전자에 대해 동일합니다 (따라서 (i)에 의존하지 않습니다). 그 행 (j)는 샘플들에 대응하고, 열 (k)는 실험 요인들에 대응합니다. 가장 단순한 경우인 쌍체 비교의 경우, 설계 행렬은 오직 두 개의 열만을 가지며, 그중 하나는 모든 곳이 1로 채워져 있고 (8.7.1절의 (0)에 대응), 다른 하나는 샘플이 어느 그룹에 속하느냐에 따라 0 또는 1을 포함합니다. 계수 ({ik})는 설계 행렬 (X)의 각 열에 대해 유전자 (i)에 대한 (_2) 폴드 변화를 제공합니다.\n10 이 모델은 샘플 및 유전자 의존적인 정규화 인자 (s_{ij})를 사용하도록 일반화될 수 있습니다. 이는 DESeq2 패키지의 문서에 설명되어 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>고처리량 카운트 데이터 (High-Throughput Count Data)</span>"
    ]
  },
  {
    "objectID": "08-chap.html#pasilla-데이터의-2-요인-분석",
    "href": "08-chap.html#pasilla-데이터의-2-요인-분석",
    "title": "10  고처리량 카운트 데이터 (High-Throughput Count Data)",
    "section": "10.8 8.9 pasilla 데이터의 2-요인 분석",
    "text": "10.8 8.9 pasilla 데이터의 2-요인 분석\n우리가 이미 8.5절에서 고려했던 siRNA 처리 외에도, pasilla 데이터는 수행된 시퀀싱 유형을 나타내는 또 다른 공변량인 type을 가지고 있습니다.\n우리는 8.5.3절의 탐색적 데이터 분석(EDA) 플롯에서 후자가 데이터에 상당한 체계적 효과를 미쳤음을 보았습니다. 우리의 8.5절 기본 분석은 이를 고려하지 않았지만, 이제 그렇게 할 것입니다. 이는 조건들 간의 유전자 차등 발현 중 어떤 것이 처리에 기인한 것이고, 어떤 것이 시퀀싱 유형에 의해 혼동되거나 가려진 것인지에 대한 더 정확한 그림을 얻는 데 도움이 될 것입니다.\npasillaTwoFactor = pasilla\ndesign(pasillaTwoFactor) = formula(~ type + condition)\npasillaTwoFactor = DESeq(pasillaTwoFactor)\n두 변수 type과 condition 중에서 주된 관심사는 condition이며, DESeq2 에서의 관례는 이를 포뮬러의 끝에 두는 것입니다. 이 관례는 모델 적합에는 아무런 영향을 주지 않지만, 이후의 결과 보고를 단순화하는 데 도움이 됩니다. 다시 한 번, 우리는 각 유전자의 통계량을 포함하는 데이터 프레임을 반환하는 results 함수를 사용하여 결과에 접근합니다.\nres2 = results(pasillaTwoFactor)\nhead(res2, n = 3)\n\n\nlog2 fold change (MLE): condition treated vs untreated \nWald test p-value: condition treated vs untreated \nDataFrame with 3 rows and 6 columns\n             baseMean log2FoldChange     lfcSE       stat    pvalue      padj\n            &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt;  &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\nFBgn0000003  0.171569      0.6745518  3.871091  0.1742537  0.861666        NA\nFBgn0000008 95.144079     -0.0406731  0.222215 -0.1830351  0.854770  0.951975\nFBgn0000014  1.056572     -0.0849880  2.111821 -0.0402439  0.967899        NA\ntype 변수와 연관된 (_2) 폴드 변화, p-값 및 조정된 p-값들을 가져오는 것도 가능합니다. results 함수는 변수 이름, 폴드 변화의 분자에 해당하는 수준, 그리고 분모에 해당하는 수준을 사용자가 지정할 수 있게 해주는 contrast 인수를 받습니다.\nresType = results(pasillaTwoFactor,\n  contrast = c(\"type\", \"single\", \"paired\"))\nhead(resType, n = 3)\n\n\nlog2 fold change (MLE): type single vs paired \nWald test p-value: type single vs paired \nDataFrame with 3 rows and 6 columns\n             baseMean log2FoldChange     lfcSE      stat    pvalue      padj\n            &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\nFBgn0000003  0.171569      -1.611546  3.871083 -0.416304  0.677188        NA\nFBgn0000008 95.144079      -0.262255  0.220686 -1.188362  0.234691  0.543822\nFBgn0000014  1.056572       3.290586  2.087243  1.576522  0.114905        NA\n그렇다면 type을 성가신 요인(때로는 좀 더 정중하게 차단 요인(blocking factor) 이라고도 부름)으로 고려한 이번 분석을 통해, 8.5절의 단순한 두 그룹 비교와 비교하여 무엇을 얻었을까요? 두 분석으로부터 얻은 p-값들을 서로에 대해 플롯해 봅시다.\ntrsf = function(x) ifelse(is.na(x), 0, (-log10(x)) ^ (1/6))\nggplot(tibble(pOne = res$pvalue,\n              pTwo = res2$pvalue),\n    aes(x = trsf(pOne), y = trsf(pTwo))) +\n    geom_hex(bins = 75) + coord_fixed() +\n    xlab(\"단일 요인 분석 (condition)\") +\n    ylab(\"2개 요인 분석 (type + condition)\") +\n    geom_abline(col = \"orange\")\n\n그림 8.9: 단일 요인(condition) 모델과 2개 요인(type + condition) 모델로부터 얻은 p-값들의 비교. 축들은 시각화 목적으로 p-값의 동적 범위를 압축하는, 임의로 선택된 단조 감소 변환인 ((-_{10}p)^{1/6}) 에 해당합니다. 결합 분포가 대각선(bisector) 위쪽에 놓이는 경향을 볼 수 있으며, 이는 2-요인 분석에서의 작은 p-값들이 일반적으로 1-요인 분석에서의 그것들보다 더 작음을 나타냅니다.\n그림 8.9에서 볼 수 있듯이, 2-요인 분석에서의 p-값들은 1-요인 분석의 것들과 유사하지만 일반적으로 더 작습니다. 더 정교한 분석을 통해, 비록 미미하긴 하지만 검정력이 증가했습니다. 각 경우에 대해 특정 유의성 임계값을 통과하는 유전자 수를 셈으로써 이를 확인할 수도 있습니다:\ncompareRes = table(\n   `단순 분석` = res$padj &lt; 0.1,\n   `2-요인 분석` = res2$padj &lt; 0.1 )\naddmargins( compareRes )\n\n\n               2-요인 분석\n단순 분석 FALSE TRUE  Sum\n      FALSE  6973  289 7262\n      TRUE     25 1036 1061\n      Sum    6998 1325 8323\n2-요인 분석은 10%의 FDR 임계값에서 1325개의 차등 발현 유전자를 찾아낸 반면, 1-요인 분석은 1061개를 찾아냈습니다. 2-요인 분석을 통해 검출력이 증가했습니다. 일반적으로 이러한 이득은 데이터에 따라 훨씬 더 크거나 더 작을 수 있습니다. 적절한 모델 선택은 실험 설계와 데이터 품질에 대한 정보에 입각한 적응을 필요로 합니다.\n질문 8.8\ntype 변수를 고려하지 않았을 때 왜 유의미한 유전자가 더 적게 검출될까요? 더 일반적으로, 차단 요인(blocking factors)을 고려하는 것(또는 고려하지 않는 것)의 이점에 대해 이것이 시사하는 바는 무엇인가요?\n해결책\n차단 요인을 모델링하지 않는다면, 그 요인에 의한 데이터의 가변성은 ()들에 의해 흡수되어야 합니다. 이는 ()들이 차단 요인을 포함한 모델에서보다 일반적으로 더 크다는 것을 의미합니다. 더 높은 수준의 노이즈는 ()-추정치의 더 높은 불확실성으로 이어집니다. 반면에, 차단 요인을 포함한 모델은 추정해야 할 매개변수가 더 많습니다. 통계적 용어로 말하자면, 적합에 쓰이는 “자유도”가 더 적습니다. 이러한 두 효과는 서로 상충하며, 어느 쪽이 우세할지, 그리고 어떤 모델링 선택이 더 많거나 적은 유의미한 결과를 낼지는 데이터에 달려 있습니다.\n질문 8.9\n혼동(Confounding)이란 무엇인가요? 차단 요인을 고려하지 않는 것이 더 많은 유전자를 검출하는 결과로 이어질 수도 있을까요?\n해결책\n네. 만약 condition과 type 변수가 지금처럼 잘 균형 잡혀 있지 않고, 부분적으로나 완전히 혼동되어 있다고 상상해 보세요. 그 경우 type에 의한 데이터 차이가 type-효과를 흡수할 수 있는 모델이 없다면 condition의 효과로 오인될 수 있습니다. 과학적으로 그러한 실험(및 분석)은 꽤나 당혹스러운 일이 될 수 있습니다.\n질문 8.10\n약물 처리가 있는 것과 없는 것 각각 10개의 서로 다른 세포주가 있는 쌍체(paired) 실험 설계를 고려해 보세요. 이는 어떻게 분석되어야 할까요?\n해결책\n만약 우리가 단순히 (처리군 대 대조군) 두 그룹 비교를 수행한다면, 많은 처리 효과들이 세포주 간의 강력한 변동에 묻혀버릴 수 있습니다. 하지만 우리는 단순히 세포주 정체성을 차단 요인으로 추가함으로써 쌍체 분석을 설정할 수 있습니다. (그러면 세포주는 우리가 지금까지 보았던 0 대 1 지시 변수가 아니라 10개 수준을 가진 R 요인 이 됩니다; R의 선형 모델링 시설과 DESeq2 또한 이를 처리하는 데 아무런 문제가 없습니다.)\n질문 8.11\n데이터에 영향을 미치는 “숨겨진” 요인이 의심되지만 기록되어 있지 않다면 무엇을 할 수 있을까요? (때때로 이러한 기록되지 않은 공변량들을 배치 효과(batch effects) 라고 부르기도 합니다.)\n해결책\n비지도 방식으로 차단 요인들을 식별하려 시도하는 방법들이 있습니다. 예를 들어 Leek와 Storey (2007; Stegle et al. 2010)를 참조하세요.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>고처리량 카운트 데이터 (High-Throughput Count Data)</span>"
    ]
  },
  {
    "objectID": "08-chap.html#추가적인-통계적-개념들",
    "href": "08-chap.html#추가적인-통계적-개념들",
    "title": "10  고처리량 카운트 데이터 (High-Throughput Count Data)",
    "section": "10.9 8.10 추가적인 통계적 개념들",
    "text": "10.9 8.10 추가적인 통계적 개념들\n\n10.9.1 8.10.1 유전자 간 분산 정보 공유\n우리는 이미 그림 6.16에서 베이지안(또는 경험적 베이지안) 분석에 대한 설명을 보았습니다. 아이디어는 우리 추정치를 개선하기 위해 (우리가 사전에 알고 있거나, 다른 유사한 데이터의 분석으로부터 얻은) 추가 정보를 사용하는 것입니다. 이 아이디어는 데이터 자체가 상대적으로 노이즈가 많을 때 특히 유용합니다. DESeq2 는 분산 매개변수(식 8.11의 세 번째 줄에 있는 ()들)와 선택적으로 로그 폴드 변화(()들)를 추정하기 위해 경험적 베이즈 접근 방식을 사용합니다. 사전 분포(priors)는 두 경우 모두 모든 유전자에 걸친 최대 우도 추정치(MLEs)의 분포로부터 얻어집니다. 이 분포들이 모두 단봉형(uni-modal)인 것으로 나타나는데; ()들의 경우 0 근처에 정점이 있고, ()의 경우 전형적인 분산 값에 정점이 있습니다. 경험적 베이즈 메커니즘은 경험적 사전 분포의 선명도와 ML 추정치의 정밀도에 의존하는 양만큼 각 유전자별 MLE를 해당 정점 쪽으로 수축(shrinks)시킵니다 (후자의 정밀도가 높을수록 수축은 덜 일어납니다). 수학적인 설명은 (Michael I. Love, Huber, and Anders 2014)에 있으며, 그림 8.10은 ()들에 대한 접근 방식을 시각화합니다.\n태스크\n심화: 그림 8.10을 생성하는 R 코드를 확인해 보세요.\n\n\n\n\n\n\n\n\n그림 8.10: DESeq2 에서의 경험적 사전 분포 사용을 통한 로그 폴드 변화 추정치의 수축 추정. 비슷한 평균 카운트와 MLE 로그 폴드 변화를 가진 두 유전자가 초록색과 파란색으로 하이라이트 되었습니다. 이 유전자들에 대한 정규화된 카운트 (a)는 파란색 유전자의 낮은 분산과 초록색 유전자의 높은 분산을 드러냅니다. (b)에는 초록색과 파란색 유전자에 대한 정규화된 우도(실선)와 사후 분포(점선)의 밀도 플롯이 나와 있습니다. 추가로, 검은색 실선은 모든 유전자의 MLE들로부터 추정된 사전 분포를 보여줍니다. 초록색 유전자의 더 높은 분산 때문에 그 우도는 더 넓고 덜 선명하며(정보가 적음을 나타냄), 사전 분포가 파란색 유전자의 경우보다 사후 분포에 더 많은 영향을 미칩니다.\n\n\n10.9.2 8.10.2 카운트 데이터 변환\n차등 발현 테스트를 위해 우리는 원시 카운트(raw counts)에 대해 연산을 수행하고 이산 분포를 사용합니다. 하지만 시각화나 군집화와 같은 다른 하류(downstream) 분석을 위해서는 변환된 버전의 카운트 데이터를 사용하는 것이 유용할 수 있습니다.\n아마도 가장 명백한 변환 선택은 로그일 것입니다. 하지만 한 유전자의 카운트 값이 0이 될 수 있으므로, 어떤 이들은 다음과 같은 형태의 의사 카운트(pseudocounts) 사용을 옹호합니다:\n[ y = _2(n + 1)y = _2(n + n_0), ]\n여기서 (n)은 카운트 값들을 나타내고 (n_0)은 어떻게든 선택된 양의 상수입니다.\n이론적인 정당성이 더 크고, 위에서의 (n_0)에 해당하는 매개변수를 선택하는 합리적인 방법을 제공하는 두 가지 대안적인 접근 방식을 살펴봅시다. 한 방법은 샘플 간 차이에 대한 사전 분포를 통합하고, 다른 방법은 분산 안정화 변환 개념을 사용합니다.\n\n10.9.2.1 분산 안정화 변환 (Variance-stabilizing transformation)\n우리는 이미 4.4.4절에서 분산 안정화 변환 을 탐구했습니다. 거기서 우리는 이산적인 확률 변수 세트에 대해 조각별 선형 변환을 계산했고 (그림 4.26), 또한 미적분학을 사용하여 감마-포아송 혼합물에 대한 매끄러운 분산 안정화 변환을 도출하는 방법도 보았습니다. 이러한 계산들은 DESeq2 패키지에 구현되어 있습니다 (Anders and Huber 2010):\nvsp = varianceStabilizingTransformation(pasilla)\n첫 번째 샘플을 예로 들어 데이터에 미치는 효과를 탐구해 보고, 이를 (_2) 변환과 비교해 봅시다; 플롯은 그림 8.11에 나와 있으며 다음과 같이 만들어졌습니다:\nj = 1\nggplot(\n  tibble(\n    counts = rep(assay(pasilla)[, j], 2),\n    transformed = c(\n      assay(vsp)[, j],\n      log2(assay(pasilla)[, j])\n      ),\n    transformation = rep(c(\"VST\", \"log2\"), each = nrow(pasilla))\n  ),\n  aes(x = counts, y = transformed, col = transformation)) +\n  geom_line() + xlim(c(0, 600)) + ylim(c(0, 9))\n\n그림 8.11: 한 샘플의 데이터에 대한 분산 안정화 변환 그래프와, 비교를 위한 (_2) 변환 그래프. 분산 안정화 변환은 카운트가 0에 가까울 때도 유한한 값과 유한한 기울기를 가지는 반면, (_2)의 기울기는 작은 카운트에서 매우 가팔라지며 카운트 0에서는 정의되지 않습니다. 큰 카운트에서는 두 변환이 본질적으로 동일합니다.\n\n\n10.9.2.2 정규화 로그(rlog) 변환\n데이터 변환을 도출하는 두 번째 방법이 있습니다. 이는 개념적으로 분산 안정화와는 구별됩니다. 대신, 우리가 8.10.1절에서 이미 탐구했던 수축 추정(shrinkage estimation)을 기반으로 합니다. 이는 각 샘플에 대해 별도의 항을 가지고 데이터로부터 추정된 계수들에 대한 사전 분포를 가지고 “사소한” 모델을 적합시킴으로써 원래 카운트 데이터를 (2)와 유사한 스케일로 변환하는 방식으로 작동합니다. 적합은 8.10.1절에서 논의했던 것과 동일한 정규화(regularization)를 채용합니다. 변환된 데이터 (q{ij})는 식 8.11의 세 번째 줄에 의해 정의되는데, 여기서 설계 행렬 ((x_{jk}))는 (K (K+1)) 크기이며 (여기서 (K)는 샘플 수) 다음과 같은 형태를 가집니다:\n[ X=(\n\\[\\begin{array}{ccccc}1&1&0&0&\\cdot\\\\1&0&1&0&\\cdot\\\\1&0&0&1&\\cdot\\\\\\\\ \\cdot&\\cdot&\\cdot&\\cdot&\\cdot\\end{array}\\]\n). ]\n사전 분포 없이는 이 설계 행렬이 유일한 해를 낳지 못하겠지만, 절편이 아닌 ()들에 사전 분포를 추가함으로써 유일한 해를 찾을 수 있게 됩니다.\nDESeq2 에서는 이 기능이 rlogTransformation 함수에 구현되어 있습니다. 실제로는 rlog 변환 또한 대략적으로 분산 안정화 효과를 내는 것으로 나타나지만, 8.10.2절의 분산 안정화 변환과 대조적으로 서로 다른 샘플들의 크기 인자가 매우 뚜렷한 데이터에 대해 더 잘 대처합니다.\n질문 8.12\n이동 로그 8.12, 정규화 로그 변환 및 분산 안정화 변환에 대해 반복 실험 간의 평균 대 표준 편차를 플롯하세요.\n해결책\n그림 8.12를 보세요.\nlibrary(\"vsn\")\nrlp = rlogTransformation(pasilla)\n\nmsd = function(x)\n  meanSdPlot(x, plot = FALSE)$gg + ylim(c(0, 1)) +\n     theme(legend.position = \"none\")\n\ngridExtra::grid.arrange(\n  msd(log2(counts(pasilla, normalized = TRUE) + 1)) +\n    ylab(\"sd(log2)\"),\n  msd(assay(vsp)) + ylab(\"sd(vst)\"),\n  msd(assay(rlp)) + ylab(\"sd(rlog)\"),\n  ncol = 3\n)\n\n그림 8.12: 이동 로그 (_2(n+1)), 분산 안정화 변환(vst), 그리고 rlog에 대해, (샘플 간 계산된) 유전자별 표준 편차(sd) 대 평균의 순위(rank). 가장 왼쪽의 (,500)개 유전자에 대해서는 카운트가 모두 0이므로 표준 편차가 0임에 주목하세요. 평균-sd 의존성은 0이 아닌 카운트를 가진 유전자들에서 더 흥미로워집니다. 이동 로그가 사용되었을 때 약하게 검출된(하지만 모든 카운트가 0은 아닌) 유전자들에 대해 높은 표준 편차 값이 나타나는 것에 주목하고, 이를 분산 안정화 변환의 상대적으로 평평한 모양과 비교해 보세요.\n\n\n\n10.9.3 8.10.3 이상치 다루기\n데이터에는 때때로 실험 또는 연구 설계와 명백히 무관해 보이며 이상치로 간주될 수 있는 고립된 매우 큰 카운트 인스턴스들이 포함되어 있습니다. 드문 기술적 또는 실험적 아티팩트, 유전적으로 다른 샘플들의 경우 리드 매핑 문제, 그리고 실재하지만 드문 생물학적 사건 등 이상치가 발생하는 원인은 많습니다. 많은 경우 사용자들이 일관된 거동을 보이는 유전자들에 주로 관심을 갖는 것으로 보이며, 이것이 기본적으로 그러한 이상치에 영향을 받는 유전자들이 DESeq에 의해 제외되는 이유입니다. 함수는 모든 유전자와 모든 샘플에 대해 쿡의 거리(Cook’s distance) (Cook 1977)라 불리는 이상치 진단 검정을 계산합니다. 쿡의 거리는 단일 샘플이 한 유전자에 대한 적합된 계수들에 얼마나 많은 영향을 미치고 있는지를 측정하며, 쿡의 거리의 큰 값은 이상치 카운트를 나타내기 위해 의도되었습니다. DESeq2 는 자동으로 임계값 이상의 쿡의 거리를 가진 유전자들을 표시하고 그 p-값과 조정된 p-값들을 NA로 설정합니다.\n기본 임계값은 표본 크기와 추정할 매개변수 수에 달려 있습니다; DESeq2 는 (F(p, m-p)) 분포(이때 (p)는 절편을 포함한 매개변수 수이고 (m)은 샘플 수)의 99% 분위수를 사용합니다.\n질문 8.13\n문서를 확인하여 기본 임계값을 어떻게 변경할 수 있는지, 그리고 이상치 제거 기능을 어떻게 완전히 비활성화할 수 있는지 알아보세요. 계산된 쿡의 거리에는 어떻게 접근할 수 있나요?\n자유도가 많은 경우 –즉, 추정할 매개변수 수보다 샘플 수가 훨씬 많은 경우– 단 하나의 카운트 이상치 때문에 유전자 전체를 분석에서 제거하는 것은 바람직하지 않을 수 있습니다. 대안적인 전략은 이상치 카운트를 모든 샘플에 대한 절단 평균(trimmed mean)으로 대체하고 해당 샘플의 크기 인자로 조정하는 것입니다. 이 접근 방식은 보수적입니다: 이상치 값을 귀무 가설에 의해 예측된 값으로 대체하기 때문에 위양성으로 이어지지 않을 것입니다.\n\n\n10.9.4 8.10.4 임계값 위 또는 아래의 (_2) 폴드 변화 검정\n8.6절에서 제기했던 문제로 돌아가 봅시다: 통계적으로 유의미하지만 매우 작은 효과가 아니라, 충분히 강한 크기를 가진 효과를 검출하고 싶다는 우리의 요구 조건을 어떻게 검정에 반영할 것인가 하는 문제입니다. results 함수의 두 인수가 임계값 기반 왈드 검정(Wald tests)을 가능하게 합니다: lfcThreshold는 음수가 아닌 임계값 수치를 받고, altHypothesis는 검정의 종류를 지정합니다. 다음 네 가지 값 중 하나를 가질 수 있습니다 (여기서 ()는 name 인수에 의해 지정된 (_2) 폴드 변화이고, ()는 lfcThreshold를 나타냅니다):\n\ngreater: (&gt; )\nless: (&lt; (-))\ngreaterAbs: (|| &gt; ) (양측 검정)\nlessAbs: (|| &lt; ) (p-값은 상단 및 하단 검정의 최댓값임)\n\n이들은 다음 코드와 시각적으로 그림 8.13의 MA 플롯들을 통해 시연됩니다. ( DESeq2 패키지에 정의된 plotMA 메서드는 베이스 그래픽을 사용함에 유의하세요.)\npar(mfrow = c(4, 1), mar = c(2, 2, 1, 1))\nmyMA = function(h, v, theta = 0.5) {\n  plotMA(pasilla, lfcThreshold = theta, altHypothesis = h,\n         ylim = c(-2.5, 2.5))\n  abline(h = v * theta, col = \"dodgerblue\", lwd = 2)\n}\nmyMA(\"greaterAbs\", c(-1, 1))\nmyMA(\"lessAbs\",    c(-1, 1))\nmyMA(\"greater\",          1)\nmyMA(\"less\",         -1   )\n\n그림 8.13: 임계값에 대한 (_2) 폴드 변화 검정의 MA 플롯. 위에서 아래로, 검정들은 altHypothesis = \"greaterAbs\", \"lessAbs\", \"greater\", 그리고 \"less\"에 대한 것입니다.\nMA 플롯 대신 결과 테이블을 생성하려면, plotMA와 동일한 인수들(ylim 제외)을 results 함수에 제공하면 됩니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>고처리량 카운트 데이터 (High-Throughput Count Data)</span>"
    ]
  },
  {
    "objectID": "08-chap.html#이-장의-요약",
    "href": "08-chap.html#이-장의-요약",
    "title": "10  고처리량 카운트 데이터 (High-Throughput Count Data)",
    "section": "10.10 8.11 이 장의 요약",
    "text": "10.10 8.11 이 장의 요약\n우리는 고처리량 시퀀싱(및 유사한 데이터 유형)으로부터 얻은 카운트 테이블을 차등 풍부도에 대해 분석하는 방법을 보았습니다. 우리는 선형 모델의 강력하고 우아한 프레임워크를 기반으로 구축했습니다. 이 프레임워크에서 우리는 기본적인 두 그룹 비교뿐만 아니라 더 복잡한 다요인 설계, 또는 세 개 이상의 수준을 가지거나 연속형인 공변량이 있는 실험들을 분석할 수 있습니다. 일반적인 선형 모델에서 데이터의 기댓값 주변 표집 분포는 독립적이고 동일한 분산을 가진 정규 분포인 것으로 가정됩니다. 카운트 데이터의 경우, 분포는 이산적이며 비대칭적인(skewed) 경향이 있고 동적 범위 전체에 걸쳐 매우 다른 분산들을 가집니다. 따라서 우리는 일반 선형 모델의 일반화인 일반화 선형 모델(GLMs) 을 채용했으며, 특히 데이터로부터 추정해야 할 분산 매개변수를 가진 감마-포아송 분포 데이터를 고려했습니다.\n샘플링 깊이는 대개 시퀀싱 실행(반복)마다 다르기 때문에, 우리는 이 가변적인 매개변수의 효과를 추정하고 우리 모델에서 이를 고려해야 합니다. 우리는 크기 인자(size factors) (s_i)를 통해 이 작업을 수행했습니다. 종종 분석의 이 부분을 정규화(normalization) 라 부릅니다 (그 용어가 특별히 기술적이진 않지만, 불행히도 이제 문헌에 잘 정착되었습니다).\n설계된 실험의 경우, 반복 실험의 수는 대개 유전자 하나만의 데이터로부터 분산 매개변수(그리고 어쩌면 모델 계수들조차)를 추정하기에는 너무 적습니다 (그리고 그래야 합니다). 따라서 우리는 편향(bias)의 상대적으로 작은 비용으로 정밀도(precision)의 큰 이득을 약속하는 수축(shrinkage) 또는 경험적 베이즈 기법들을 사용합니다.\nGLM은 데이터를 원래의 스케일에서 모델링하게 해주지만, 때때로 데이터를 좀 더 등분산적이고 범위를 더 균일하게 채우는 스케일로 변환하는 것이 유용할 때가 있습니다 – 예를 들어 데이터를 플로팅하거나, 범용적인 군집화, 차원 축소 또는 학습 방법들에 데이터를 노출시키기 위해서입니다. 이를 위해 우리는 분산 안정화 변환을 보았습니다.\n여기서 연습했던 것과 같은 차등 발현 검정에 대한 주요하고 상당히 타당한 비판은, 귀무 가설 – 효과 크기가 정확히 0임 – 이 거의 결코 참이 아니며, 따라서 우리 접근 방식이 어떤 유전자가 차등 발현되는지에 대한 일관된 추정치를 제공하지 못한다는 것입니다. 실제로는 통계적 유의성뿐만 아니라 효과 크기를 함께 고려함으로써 이를 극복할 수 있습니다. 게다가, 우리는 “띠 모양(banded)” 귀무 가설을 사용하는 방법도 보았습니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>고처리량 카운트 데이터 (High-Throughput Count Data)</span>"
    ]
  },
  {
    "objectID": "08-chap.html#더-읽을거리",
    "href": "08-chap.html#더-읽을거리",
    "title": "10  고처리량 카운트 데이터 (High-Throughput Count Data)",
    "section": "10.11 8.12 더 읽을거리",
    "text": "10.11 8.12 더 읽을거리\n\nDESeq2 방법은 Michael I. Love, Huber, Anders (2014)의 논문에 설명되어 있으며, 소프트웨어의 실무적인 측면은 패키지 비네트에 나와 있습니다. 관련 접근 방식에 대해서는 edgeR 패키지와 논문 (Robinson, McCarthy, and Smyth 2009) 또한 참조하세요.\n강건 회귀와 이상치 탐지에 관한 고전적인 교재로는 Peter J. Rousseeuw와 Leroy (1987)의 책이 있습니다. 더 최근의 발전에 대해서는 CRAN 태스크 뷰: 강건 통계 방법(Robust Statistical Methods)이 시작하기 좋은 곳입니다.\nhttps://www.bioconductor.org/help/workflows/rnaseqGene 에 있는 바이오컨덕터 RNA-Seq 워크플로 (Michael I. Love et al. 2015)는 여기서 우리가 생략했던 RNA-Seq 관련 여러 이슈들을 다룹니다.\n우리가 본 일반화 선형 모델을 확장하여 RNA-Seq 데이터로부터 대안적 엑손 사용(alternative exon usage)을 탐지하는 방법이 DEXSeq 논문 (Anders, Reyes, and Huber 2012)에 제시되어 있으며, 생물학적 발견에 이러한 아이디어들을 적용한 사례들이 Reyes 등 (2013)과 Reyes 및 Huber (2017)에 의해 설명되었습니다.\nRIP-Seq, CLIP-Seq과 같은 일부 시퀀싱 기반 어세이의 경우, 생물학적 분석 목표는 입력(input) 과 면역 침전물(immunoprecipitate, IP) 사이의 비율이 조건 간에 변했는지 테스트하는 것으로 귀결됩니다. 바이오컨덕터 포럼에 올라온 Mike Love의 포스팅이 명확하고 빠른 하우투(how-to)를 제공합니다: https://support.bioconductor.org/p/61509.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>고처리량 카운트 데이터 (High-Throughput Count Data)</span>"
    ]
  },
  {
    "objectID": "08-chap.html#연습-문제",
    "href": "08-chap.html#연습-문제",
    "title": "10  고처리량 카운트 데이터 (High-Throughput Count Data)",
    "section": "10.12 8.13 연습 문제",
    "text": "10.12 8.13 연습 문제\n연습 문제 8.1\n작은 p-값들의 고갈. 작은 p-값들이 고갈된 모습을 보여주는 p-값 히스토그램에 대한 다음의 간단한 생성 모델을 고려해 보세요. 그림 8.14에는 테스트된 2-수준 범주형 변수인 groups와의 연관성이 없는 상태에서 수행된 차등 발현 분석(이 사례에서는 단순 (t)-검정)으로부터 얻은 p-값들이 나와 있습니다. x1에 대해서는 히스토그램이 대략적으로 균등한 반면, x2에 대해서는 작은 p-값들이 고갈되어 있습니다. 이는 groups와 직교(orthogonal)하면서 균형 잡힌 배치(batch, 동명의 변수로 인코딩됨)가 검정 통계량의 분모를 부풀리는 추가적인 가변성을 도입하기 때문입니다.\nlibrary(\"magrittr\")\nng = 10000\nns = 12\nx1 = x2 = matrix(rnorm(ns * ng), ncol = ns, nrow = ng)\ngroup = factor(letters[1 + seq_len(ns) %% 2])  %T&gt;% print\n\n\n [1] b a b a b a b a b a b a\nLevels: a b\n\n\nbatch = factor(ifelse(seq_len(ns) &lt;= ns/2, \"B1\", \"B2\")) %T&gt;% print\n\n\n [1] B1 B1 B1 B1 B1 B1 B2 B2 B2 B2 B2 B2\nLevels: B1 B2\n\n\ntable(group, batch)\n\n\n     batch\ngroup B1 B2\n    a  3  3\n    b  3  3\n\n\nx2[, batch==\"B2\"] = x2[, batch==\"B2\"] + 2 * rnorm(ng)\npvals = rbind(\n  cbind(type = \"x1\", genefilter::rowttests(x1, fac = group)),\n  cbind(type = \"x2\", genefilter::rowttests(x2, fac = group)))\nggplot(pvals, aes(x = p.value)) + \n  geom_histogram(binwidth = 0.02, boundary = 0) +\n  facet_grid(type ~ .)\n(t)-검정을 선형 모델로 대체해 보세요. 먼저 오직 group만을 인자로 하는 모델을, 두 번째로는 (R의 포뮬러 언어로) group + batch인 모델을 사용하세요. x1과 x2 두 경우 모두에서 group의 계수에 대한 p-값 히스토그램이 균등함을 보이세요.\n\n그림 8.14: x1과 x2에 대해 수행된 검정들의 p-값 (코드 참조).\n연습 문제 8.2\nedgeR. edgeR 패키지를 사용하여 8.5절의 분석을 수행하고 그 결과를 비교하세요: (_{10}) p-값들의 산점도를 그리고, 큰 차이가 있는 유전자들을 골라 원시 데이터를 시각화하여 무슨 일이 일어나고 있는지 살펴보세요. 이에 기초하여 차이점들을 설명할 수 있나요?\n연습 문제 8.3\n강건성. 예제 ((x, y)) 데이터 세트(예를 들어 mtcars 데이터)에 대해 선형 회귀를 수행하고 데이터와 함께 적합된 선을 표시하는 shiny 앱을 작성하세요. 넓은 범위(원래 데이터 범위 밖으로 몇 배 확장)에서 점들 중 하나를 (x)- 및/또는 (y)- 방향으로 움직일 수 있게 해주는 위젯을 추가하세요. lm, rlm 및 여러 서로 다른 method 선택지를 가진 lqs (MASS 패키지에 들어있음) 중에서 선택할 수 있게 해주는 라디오 버튼 위젯을 추가하세요. 보너스: robustbase 패키지의 함수들을 추가해 보세요.\n해결책\n앱의 ui.R 파일용 코드:\nlibrary(\"shiny\")\nshinyUI(fluidPage(\n  titlePanel(\"Breakdown\"),\n  sidebarLayout(\n    sidebarPanel(     # 이상치 이동(shift) 선택\n      sliderInput(\"shift\", \"Outlier:\", min = 0, max = 100, value = 0),\n      radioButtons(\"method\", \"Method:\",\n                   c(\"Non-robust least squares\" = \"lm\",\n                     \"M-estimation\" = \"rlm\"))\n    ),\n    mainPanel(       # 적합 결과 표시\n      plotOutput(\"regPlot\")\n    )\n  )\n))\n앱의 server.R 파일용 코드:\nlibrary(\"shiny\")\nlibrary(\"ggplot2\")\nlibrary(\"MASS\")\nshinyServer(function(input, output) {\n  output$regPlot = renderPlot({\n    whpt = 15\n    mtcars_new = mtcars\n    mtcars_new$mpg[whpt] = mtcars_new$mpg[whpt] + input$shift\n    reg = switch(input$method,\n      lm = lm(mpg ~ disp, data = mtcars_new),\n      rlm = rlm(mpg ~ disp, data = mtcars_new),\n      stop(\"Unimplemented method:\", input$method)\n    )\n    ggplot(mtcars_new, aes(x = disp, y = mpg)) + geom_point() +\n      geom_abline(intercept = reg$coefficients[\"(Intercept)\"],\n                  slope = reg$coefficients[\"disp\"], col = \"blue\")\n  })\n})\n물론 훨씬 더 많은 기능을 추가할 수 있습니다.\nAnders, Simon, and Wolfgang Huber. 2010. “Differential Expression Analysis for Sequence Count Data.” Genome Biology 11: R106. http://genomebiology.com/2010/11/10/R106.\nAnders, Simon, Alejandro Reyes, and Wolfgang Huber. 2012. “Detecting differential usage of exons from RNA-Seq data.” Genome Research 22 (10): 2008–17.\nBrooks, Angela N, Li Yang, Michael O Duff, Kasper D Hansen, Jung W Park, Sandrine Dudoit, Steven E Brenner, and Brenton R Graveley. 2011. “Conservation of an RNA Regulatory Map Between Drosophila and Mammals.” Genome Research , 193–202. https://doi.org/10.1101/gr.108662.110.\nCook, R. Dennis. 1977. “Detection of Influential Observation in Linear Regression.” Technometrics.\nHuber, Peter J. 1964. “Robust Estimation of a Location Parameter.” The Annals of Mathematical Statistics 35: 73–101.\nLeek, Jeffrey T., and John D. Storey. 2007. “Capturing heterogeneity in gene expression studies by surrogate variable analysis.” PLoS Genetics 3 (9): 1724–35.\nLove, Michael I., Simon Anders, Vladislav Kim, and Wolfgang Huber. 2015. “RNA- Seq Workflow: Gene-Level Exploratory Analysis and Differential Expression.” F1000Research 4 (1070). https://doi.org/10.12688/f1000research.7035.1.\nLove, Michael I, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation of Fold Change and Dispersion for RNA-seq Data with DESeq2.” Gnome Biology 15 (12): 1–21.\nOzsolak, Fatih, and Patrice M Milos. 2011. “RNA sequencing: advances, challenges and opportunities.” Nature Reviews Genetics 12: 87–98.\nReyes, Alejandro, Simon Anders, Robert J. Weatheritt, Toby J Gibson, Lars M. Steinmetz, and Wolfgang Huber. 2013. “Drift and Conservation of Differential Exon Usage Across Tissues in Primate Species.” Proceedings of the National Academy of Sciences 110 (38): 15377–82. https://doi.org/10.1073/pnas.1307202110.\nReyes, Alejandro, and Wolfgang Huber. 2017. “Alternative Start and Termination Sites of Transcription Drive Most Transcript Isoform Differences Across Human Tissues.” Nucleic Acids Research 46 (2): 582–92. https://doi.org/10.1093/nar/gkx1165.\nRobinson, M. D., D. J. McCarthy, and G. K. Smyth. 2009. “edgeR: A Bioconductor Package for Differential Expression Analysis of Digital Gene Expression Data.” Bioinformatics 26 (1): 139–40. https://doi.org/10.1093/bioinformatics/btp616.\nRousseeuw, Peter J. 1987. “Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis.” Journal of Computational and Applied Mathematics 20: 53–65.\nRousseeuw, Peter J., and Annick M. Leroy. 1987. Robust Regression and Outlier Detection. Wiley. https://doi.org/10.1002/0471725382.\nStegle, O., L. Parts, R. Durbin, and J. Winn. 2010. “A Bayesian framework to account for complex non-genetic factors in gene expression levels greatly increases power in eQTL studies.” PLoS Computational Biology 6 (5): e1000770.\nSteijger, T., J. F. Abril, P. G. Engstrom, F. Kokocinski, T. J. Hubbard, R. Guigo, J. Harrow, et al. 2013. “Assessment of transcript reconstruction methods for RNA-seq.” Nature Methods 10 (12): 1177–84.\nVerhulst, Pierre-François. 1845. “Recherches mathématiques Sur La Loi d’accroissement de La Population.” Nouveaux Mémoires de l’Académie Royale Des Sciences Et Belles-Lettres de Bruxelles 18: 1–42.\n페이지는 R 버전 4.5.1 (2025-06-13)을 사용하여 2025-09-01 01:33에 빌드되었습니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>고처리량 카운트 데이터 (High-Throughput Count Data)</span>"
    ]
  },
  {
    "objectID": "09-chap.html",
    "href": "09-chap.html",
    "title": "11  이질적 데이터의 다변량 분석 (Multivariate Analysis for Heterogeneous Data)",
    "section": "",
    "text": "11.1 9.2 다차원 척도법과 서열화 (Ordination)\n실제 상황에는 종종 포인트 클라우드, 그래디언트, 그래프, 어트랙션 포인트, 노이즈 및 서로 다른 공간적 환경이 포함되는데, 이는 마치 단단한 뼈대, 파도, 태양, 그리고 찌르레기들이 있는 이 사진과도 비슷합니다.\n7장에서 우리는 열이 연속형 변수인 직사각형 행렬을 요약하는 방법을 보았습니다. 우리가 만들었던 지도들은 모든 열이 의미 있는 분산을 가질 때 행렬 (X)에서 가장 중요한 신호(signal) 성분을 분리해 내는 것을 목표로 하는 주성분 분석과 같은 비지도 차원 축소 기법들을 사용했습니다.\n여기서 우리는 이러한 아이디어들을 연속형 변수와 범주형 변수가 결합된 더 복잡한 불균질 데이터로 확장합니다. 실제로 때때로 우리의 관측치는 개별 변수나 좌표 세트로 쉽게 설명될 수 없지만, 그들 사이의 거리나 (비)유사성을 결정하거나, 또는 그래프나 트리를 사용하여 그들 사이의 관계를 설명하는 것이 가능합니다. 예로는 종 계통수에서의 종이나 생물학적 서열이 있습니다. 생물학 이외의 예로는 텍스트 문서나 영화 파일이 있는데, 여기서 우리는 그것들 사이의 (비)유사성을 결정하는 합리적인 방법을 가질 수는 있지만 명백한 변수나 좌표는 없습니다.\n이 장에는 더 고급 기법들이 포함되어 있으며, 이를 위해 종종 기술적인 세부 사항은 생략합니다. 여기까지 온 만큼, 예제를 통한 실습 경험과 광범위한 참고 문헌을 제공하여 여러분이 비선형 다변량 분석에서 더 ’최첨단’인 기법 중 일부를 이해하고 사용할 수 있기를 바랍니다.\n이 장에서 우리는 다음을 수행할 것입니다:\n때때로 데이터는 특징 공간의 점으로 표현되지 않습니다. 이는 약물, 이미지, 트리 또는 다른 복잡한 객체 사이의 (비)유사성 행렬이 제공될 때 발생할 수 있는데, 이들은 ({R}^n) 상의 명백한 좌표를 가지고 있지 않습니다.\n5장에서 우리는 거리로부터 클러스터 를 생성하는 방법을 보았습니다. 여기서 우리의 목표는 PCA의 처음 몇몇 주축으로 만든 것과 유사하게, 저차원 공간(예: 평면)의 맵에 데이터를 시각화하는 것입니다.\n우리는 지리 데이터를 사용하는 직관적인 예제로 시작합니다. 그림 9.1에는 우크라이나의 도시와 장소들 사이의 거리에 대한 히트맵과 군집화가 나와 있습니다1.\n1 이 데이터의 출처는 data 폴더의 ukraine-dists.R 스크립트에 설명되어 있습니다.\n그림 9.1: ukraine_dists 거리 행렬의 히트맵. 거리는 킬로미터 단위로 측정됩니다. 함수가 도시들의 순서를 재배열하고 가장 가까운 것들을 그룹화했습니다.\n쌍별 거리를 포함하는 ukraine_dists 외에도, 위에서 로드한 RData 파일에는 경도와 위도가 포함된 ukraine_coords 데이터 프레임도 포함되어 있습니다; 우리는 이를 나중에 실측 자료(ground truth)로 사용할 것입니다. 거리가 주어지면 다차원 척도법(MDS)은 그들의 상대적 위치에 대한 “지도”를 제공합니다. 도시들이 2D 평면상에서 갖는 유클리드 거리가 주어진 거리 행렬을 정확하게 재현하도록 배치하는 것은 불가능할 것입니다: 도시들은 평면이 아니라 지구의 곡면 위에 있기 때문입니다. 그럼에도 불구하고, 우리는 데이터를 잘 표현하는 2차원 임베딩을 찾을 수 있을 것으로 기대할 수 있습니다. 생물학적 데이터의 경우, 우리의 2차원 임베딩은 이보다 훨씬 더 모호할 가능성이 높습니다. 우리는 다음 함수를 호출합니다:\n이 장에서 여러 번 재사용하여 cmdscale 함수의 호출 결과로부터 스크리 플롯(screeplot)을 만드는 함수를 정의합니다:\n그림 9.2: 처음 네 개의 고윳값에 대한 스크리 플롯. 처음 두 개의 고윳값 이후에 뚜렷한 감소가 있으며, 이는 데이터가 2차원 임베딩으로 잘 설명됨을 나타냅니다.\n질문 9.1\ncmdscale 함수에 의해 출력된 모든 고윳값들을 살펴보세요: 무엇을 발견했나요?\n해결책\n다음을 실행하면:\n그림 9.3: 모든 고윳값에 대한 스크리 플롯.\nPCA와 달리 일부 음의 고윳값들이 있음을 알 수 있습니다. 이는 cmdscale이 작동하는 방식 때문입니다.\ncmdscale 함수의 주요 출력은 2차원 임베딩의 좌표이며, 이를 그림 9.4에 나타냈습니다 (알고리즘의 작동 방식은 다음 섹션에서 논의할 것입니다).\n그림 9.4: 거리 기반의 MDS 지도.\n상대적인 위치는 정확하지만 지도의 방향이 관습적이지 않다는 점에 유의하세요: 크림 반도가 상단에 있습니다. 이는 거리로부터 평면 임베딩을 재구성하는 방법들에서 흔히 나타나는 현상입니다. 점들 사이의 거리는 회전과 반사(축 뒤집기)에 대해 불변이기 때문에, 회전이나 반사를 통해 연관되는 어떠한 솔루션도 다른 솔루션만큼 좋습니다. cmdscale과 같은 함수들은 똑같이 최적인 솔루션들 중 하나를 선택할 것이며, 그 구체적인 선택은 데이터의 미세한 세부 사항이나 사용 중인 컴퓨팅 플랫폼에 따라 달라질 수 있습니다. 여기서 우리는 (y)축의 부호를 반전시켜 결과를 더 관습적인 방향으로 변환할 수 있습니다. 그림 9.5에서 지도를 다시 그리고, 이를 ukraine_coords 데이터 프레임의 실제 경도 및 위도와 비교합니다 (그림 9.6).\n그림 9.5: 그림 9.4와 같지만, (y)축이 뒤집힌 모습.\n그림 9.6: ukraine_coords 데이터 프레임에서 가져온 실제 위도와 경도.\n질문 9.2\n우리는 종횡비에 주의를 기울이지 않고 그림 9.6의 오른쪽 패널에 경도와 위도를 그렸습니다. 이 플롯에 적절한 종횡비는 얼마일까요?\n해결책\n경도의 1도 변화에 해당하는 거리와 위도의 1도 변화에 해당하는 거리 사이에는 단순한 관계가 없으므로 선택하기가 어렵습니다. 지구가 구형이고 반지름이 6371km라는 단순화된 가정하에서도 복잡합니다: 위도의 1도는 항상 111km 거리((6371/360))에 해당하며, 적도에서의 경도 1도 마찬가지입니다. 하지만 적도에서 멀어지면 경도 1도는 점점 더 짧은 거리에 해당하게 됩니다(극점에서는 거리가 아예 없습니다). 실용적으로 그림 9.6과 같은 디스플레이의 경우, 가장 북쪽 지점과 남쪽 지점의 중간 정도인 48도에 대한 코사인 값을 종횡비로 선택할 수 있습니다.\n질문 9.3\n그림 9.6에 국제 경계와 강 같은 지리적 특징을 추가해 보세요.\n해결책\n국제 경계를 다각형으로 추가하는 아래 코드가 시작점이 될 수 있습니다 (그림 9.7).\n벡터 및 래스터 데이터 유형을 포함하여 지리 공간 데이터를 위해 R에서 사용할 수 있는 추가 인프라가 많이 있습니다.\n그림 9.7: 그림 9.6에 국제 경계가 추가된 모습.\n참고: MDS는 PCA와 유사한 출력을 생성하지만, 데이터(샘플 포인트)에 대한 한 종류의 ‘차원’만 가집니다. ’쌍대(dual)’ 차원이 없으며, 바이플롯(biplots)이나 로딩(loading) 벡터가 없다는 점입니다. 이는 지도를 해석할 때 단점이 됩니다. 극단적인 점들과 그들 사이의 차이점들을 주의 깊게 살펴봄으로써 해석을 용이하게 할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>이질적 데이터의 다변량 분석 (Multivariate Analysis for Heterogeneous Data)</span>"
    ]
  },
  {
    "objectID": "09-chap.html#다차원-척도법과-서열화-ordination",
    "href": "09-chap.html#다차원-척도법과-서열화-ordination",
    "title": "11  이질적 데이터의 다변량 분석 (Multivariate Analysis for Heterogeneous Data)",
    "section": "",
    "text": "library(\"pheatmap\")\ndata(\"ukraine_dists\", package = \"MSMB\")\nas.matrix(ukraine_dists)[1:4, 1:4]\n\n\n               Kyiv    Odesa Sevastopol Chernihiv\nKyiv         0.0000 441.2548   687.7551  128.1287\nOdesa      441.2548   0.0000   301.7482  558.6483\nSevastopol 687.7551 301.7482     0.0000  783.6561\nChernihiv  128.1287 558.6483   783.6561    0.0000\n\n\npheatmap(as.matrix(ukraine_dists), \n  color = colorRampPalette(c(\"#0057b7\", \"#ffd700\"))(50),\n  breaks = seq(0, max(ukraine_dists)^(1/2), length.out = 51)^2,\n  treeheight_row = 10, treeheight_col = 10)\n\n\n\nukraine_mds = cmdscale(ukraine_dists, eig = TRUE)\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nplotscree = function(x, m = length(x$eig)) {\n  ggplot(tibble(eig = x$eig[seq_len(m)], k = seq(along = eig)),\n    aes(x = k, y = eig)) + theme_minimal() +\n    scale_x_discrete(\"k\", limits = as.factor(seq_len(m))) + \n    geom_bar(stat = \"identity\", width = 0.5, fill = \"#ffd700\", col = \"#0057b7\")\n}\n\n\nplotscree(ukraine_mds, m = 4)\n\n\n\n\n\n\nukraine_mds$eig |&gt; signif(3)\n\n\n [1]  3.91e+06  1.08e+06  3.42e+02  4.84e-01  2.13e-01  3.83e-05  5.90e-06\n [8]  5.82e-07  8.79e-08  4.94e-08  6.52e-10  2.84e-10  1.84e-10  5.22e-11\n[15]  4.89e-11  4.57e-11 -3.26e-12 -2.55e-11 -5.90e-11 -6.55e-11 -1.40e-10\n[22] -1.51e-10 -3.46e-10 -3.76e-10 -4.69e-10 -2.24e-09 -1.51e-08 -9.60e-05\n[29] -2.51e-04 -1.41e-02 -1.19e-01 -3.58e+02 -8.85e+02\n\n\nplotscree(ukraine_mds)\n\n\n\n\nukraine_mds_df = tibble(\n  PCo1 = ukraine_mds$points[, 1],\n  PCo2 = ukraine_mds$points[, 2],\n  labs = rownames(ukraine_mds$points)\n)\nlibrary(\"ggrepel\")\ng = ggplot(ukraine_mds_df, aes(x = PCo1, y = PCo2, label = labs)) +\n  geom_point() + geom_text_repel(col = \"#0057b7\") + coord_fixed() \ng\n\n\n\ng %+% mutate(ukraine_mds_df, PCo1 = PCo1, PCo2 = -PCo2)\n\n\ndata(\"ukraine_coords\", package = \"MSMB\")\nprint.data.frame(ukraine_coords[1:4,  c(\"city\", \"lat\", \"lon\")])\n\n\n        city      lat      lon\n1       Kyiv 50.45003 30.52414\n2      Odesa 46.48430 30.73229\n3 Sevastopol 44.60544 33.52208\n4  Chernihiv 51.49410 31.29433\n\n\nggplot(ukraine_coords, aes(x = lon, y = lat, label = city)) +\n  geom_point() + geom_text_repel(col = \"#0057b7\")\n\n\n\n\n\n\n\n\n\n\nlibrary(\"maps\")\nua_borders = dplyr::filter(map_data(\"world\"), region == \"Ukraine\")\nggplot(ukraine_coords, aes(x = lon, y = lat)) + \n  geom_polygon(data = ua_borders, aes(x = long, y = lat, group = subregion), fill = \"#ffd700\", color = \"#0057b7\") +\n  geom_point() + \n  geom_text_repel(aes(label = city)) +\n  coord_fixed(1/cos(48/180*pi))\n\n\n\n\n\n11.1.1 9.2.1 방법은 어떻게 작동하나요?\n좌표가 알려진 점들로부터 실제로 시작했다면 어떤 일이 일어날지 살펴봅시다2. 우리는 이 좌표들을 점의 수만큼의 행을 가진 행렬의 두 열에 넣습니다. 이제 이 좌표들을 바탕으로 점들 사이의 거리를 계산합니다. 좌표 (X)에서 거리로 가려면 다음과 같이 씁니다: [d^2_{i,j} = (x_i^1 - x_j1)2 + + (x_i^p - x_jp)2.] 우리는 제곱 거리 행렬을 R에서 DdotD라고 부르고 텍스트에서는 (DD)라고 부를 것입니다. 우리는 그 거리의 제곱이 관측된 (DD)에 가능한 한 가까운 점들을 찾고 싶습니다.3\n이는 (D)와 그 자신과의 행렬 곱인 (DD) 또는 (D^2)과는 다릅니다.\n3 여기서는 우리 도시의 경도와 위도를 데카르트 좌표로 사용하고 지구 표면의 곡률을 무시함으로써 약간의 ’남용’을 저지르고 있습니다. 하버사인(Haversine) 공식에 대한 정보를 인터넷에서 찾아보세요.\nX = with(ukraine_coords, cbind(lon, lat * cos(48)))\nDdotD = as.matrix(dist(X)^2)\n상대적인 거리는 데이터의 원점에 의존하지 않습니다. 우리는 (H=I-^t)로 정의된 중앙화 행렬(centering matrix) (H)를 사용하여 데이터를 중앙에 맞춥니다. 다음을 사용하여 (H)의 중앙화 속성을 확인해 봅시다:\nn = nrow(X)\nH = diag(rep(1,n))-(1/n) * matrix(1, nrow = n, ncol = n)\nXc = sweep(X,2,apply(X,2,mean))\nXc[1:2, ]\n\n\n            lon          \n[1,] -1.1722946 -1.184705\n[2,] -0.9641429  1.353935\n\n\nHX = H %*% X\nHX[1:2, ]\n\n\n            lon          \n[1,] -1.1722946 -1.184705\n[2,] -0.9641429  1.353935\n\n\napply(HX, 2, mean)\n\n\n          lon               \n-1.618057e-15  1.747077e-16 \n질문 9.4\nDdotD의 오른쪽과 왼쪽에 모두 중앙화 행렬을 적용하여 얻은 행렬을 B0라고 합시다. (HX) 행렬에 의해 주어지는 원점에 중앙화된 점들을 고려하고 그 교차 곱(cross product)을 계산하여 이를 B2라고 부릅시다. B0를 B2와 같게 만들려면 B0에 무엇을 해야 할까요?\n해결책\nB0 = H  %*% DdotD %*% H\nB2 = HX %*% t(HX)\nB2[1:3, 1:3] / B0[1:3, 1:3]\n\n\n     [,1] [,2] [,3]\n[1,] -0.5 -0.5 -0.5\n[2,] -0.5 -0.5 -0.5\n[3,] -0.5 -0.5 -0.5\n\n\nmax(abs(-0.5 * B0 - B2))\n\n\n[1] 9.237056e-14\n따라서, 행들 사이의 제곱 거리((DD))와 중앙화된 행렬의 교차 곱 (B=(HX)(HX)^t)가 주어졌을 때, 우리는 다음을 보였습니다:\n[ -\frac{1}{2} H(DD) H=B ]\n이는 항상 참이며, 처음에 (DD)가 주어졌을 때 식 9.1을 만족하는 (X)를 역설계하는 데 이를 사용합니다.\n\n11.1.1.1 특잇값 벡터를 사용하여 (DD)에서 (X) 구하기.\n식 9.1에서 정의된 (B)의 고유 분해(eigen-decomposition)를 취함으로써 행렬 (DD)에서 (X)로 거꾸로 갈 수 있습니다. 이는 또한 우리가 (X) 행렬에 대해 몇 개의 좌표 또는 열을 원하는지 선택할 수 있게 해줍니다. 이는 PCA가 최선의 계수(rank) (r) 근사를 제공하는 방식과 매우 유사합니다.\n참고 : PCA에서와 마찬가지로, 이를 (HX)의 특잇값 분해(또는 (HX(HX)^t)의 고유 분해)를 사용하여 다음과 같이 쓸 수 있습니다:\n[ HX^{(r)} = US{(r)}Vt S^{(r)} r ,]\n이는 (r)차원 유클리드 공간에서의 최선의 근사 표현을 제공합니다.\n[S^{(r)} =\n\\[\\begin{pmatrix} s_1 & 0 & 0 & 0 & ...\\\\ 0 & s_2 & 0 & 0 & ...\\\\\n0 & 0 & ... & ... & ...\\\\ 0 & 0 & ... & s_r & ...\\\\ ...& ...& ...& 0 & 0 \\\\\n\\end{pmatrix}\\]\n] 이 방법은 종종 주좌표 분석(Principal Coordinates Analysis, PCoA) 이라고 불리며, 이는 PCA와의 연결성을 강조합니다.\n\n\n11.1.1.2 고전적 MDS 알고리즘.\n요약하자면, 점 간 제곱 거리의 (n n) 행렬 (DD)가 주어졌을 때, 다음 연산들을 통해 점들과 그들의 좌표 ()를 찾을 수 있습니다:\n\n점 간 제곱 거리를 이중 중앙화(double center)하고 (-)을 곱합니다:\n(B = -H DD H).\n(B)를 대각화합니다: (B = U U^t).\n()를 추출합니다: ( = U ^{1/2}).\n\n\n\n11.1.1.3 적절한 기저 차원 찾기.\n예시로서, 유사성(거리를 대신함)은 있지만 자연적인 기저 유클리드 공간은 없는 객체들을 살펴보겠습니다.\n1950년대의 한 심리학 실험에서, Ekman (1954)은 31명의 피험자에게 14가지 서로 다른 색상의 유사성을 평가하도록 요청했습니다. 그의 목표는 색상 인지의 기저 차원을 이해하는 것이었습니다. 유사성 또는 혼동 행렬(confusion matrix)은 0과 1 사이의 값을 갖도록 스케일이 조정되었습니다. 자주 혼동되는 색상들은 1에 가까운 유사성을 가졌습니다. 우리는 값들을 1에서 뺌으로써 데이터를 비유사성(dissimilarity)으로 변환합니다:\nekm = read.table(\"../data/ekman.txt\", header=TRUE)\nrownames(ekm) = colnames(ekm)\ndisekm = 1 - ekm - diag(1, ncol(ekm))\ndisekm[1:5, 1:5]\n\n\n     w434 w445 w465 w472 w490\nw434 0.00 0.14 0.58 0.58 0.82\nw445 0.14 0.00 0.50 0.56 0.78\nw465 0.58 0.50 0.00 0.19 0.53\nw472 0.58 0.56 0.19 0.00 0.46\nw490 0.82 0.78 0.53 0.46 0.00\n\n\ndisekm = as.dist(disekm)\n우리는 MDS 좌표와 고윳값을 계산합니다. 우리는 그림 9.8에 표시된 스크리 플롯에 고윳값들을 결합합니다:\nmdsekm = cmdscale(disekm, eig = TRUE)\nplotscree(mdsekm)\n\n그림 9.8: 스크리 플롯은 현상이 주로 2차원적임을 보여줍니다.\n우리는 다음과 같이 처음 두 개의 주좌표를 사용하여 서로 다른 색상들을 플롯합니다:\ndfekm = mdsekm$points[, 1:2] |&gt;\n  `colnames&lt;-`(paste0(\"MDS\", 1:2)) |&gt;\n  as_tibble() |&gt;\n  mutate(\n    name = rownames(ekm),\n    rgb = photobiology::w_length2rgb(as.numeric(sub(\"w\", \"\", name))))\nggplot(dfekm, aes(x = MDS1, y = MDS2)) +\n  geom_point(col = dfekm$rgb, size = 4) +\n  geom_text_repel(aes(label = name)) + coord_fixed()\n\n그림 9.9: 처음 두 차원에서의 산점도 배치는 말편자(horseshoe) 모양을 하고 있습니다. 레이블과 색상은 아치형 구조가 파장에 대응함을 보여줍니다.\n그림 9.9는 새로운 좌표계에서의 Ekman 데이터를 보여줍니다. 설명을 요약하는 눈에 띄는 패턴이 있습니다. 점들의 이러한 말편자 또는 아치 구조는 종종 데이터에 순차적인 잠재적 순서나 그래디언트가 존재함을 나타내는 지표입니다 (Diaconis, Goel, and Holmes 2008). 우리는 9.5절에서 이를 다시 살펴볼 것입니다.\n\n\n\n11.1.2 9.2.2 MDS의 강건한 버전들\n다차원 척도법은 (DD)에 의해 주어지는 제곱 거리와 새로운 좌표를 가진 점들 사이의 제곱 거리 간의 차이를 최소화하는 것을 목표로 합니다. 불행히도, 이 목적 함수는 이상치에 민감한 경향이 있습니다: 다른 모든 점들과의 거리가 큰 단 하나의 데이터 포인트가 분석 전체를 지배하고 왜곡할 수 있습니다. 종종 우리는 좀 더 강건한(robust) 무언가를 사용하고 싶어 하며, 이를 달성하는 한 가지 방법은 거리의 실제 값을 무시하고 원래 거리와 새로운 거리의 상대적 순위(rankings)가 가능한 한 비슷할 것만을 요구하는 것입니다. 이러한 순위 기반 접근 방식은 강건합니다: 이상치에 대한 민감도가 줄어듭니다.\n강건성(Robustness): 소수의 이상치에 의해 너무 크게 영향을 받지 않는다면 그 방법은 강건합니다. 예를 들어, (n)개 숫자 세트의 중앙값은 우리가 20개 숫자를 임의로 크게 바꾸더라도 크게 변하지 않습니다; 중앙값을 급격하게 이동시키려면 숫자들의 절반 이상을 바꾸어야 합니다. 이와 대조적으로, 평균은 단 하나의 숫자만 조작해도 크게 바꿀 수 있습니다. 우리는 중앙값의 붕괴점(breakdown point) 이 1/2인 반면, 평균의 붕괴점은 오직 (1/n)이라고 말합니다. 평균과 중앙값 모두 분포의 위치(location) (즉, 숫자들의 “전형적인” 값이 무엇인지)에 대한 추정량이지만, 중앙값이 더 강건합니다. 중앙값은 순위에 기반합니다; 더 일반적으로, 순위에 기반한 방법들은 종종 실제 값에 기반한 방법들보다 더 강건합니다. 많은 비모수적 검정들은 데이터를 그들의 순위로 축소하는 것에 기반합니다.\n우리는 측정값의 ’스케일’에 대해 확신이 없을 때 강건한 방법들이 얼마나 유용한지 보여주기 위해 Ekman 데이터를 사용할 것입니다. 비계량 다차원 척도법(non-metric multidimensional scaling, NMDS) 이라 불리는 강건한 서열화는, 오직 새로운 맵에서 재구성된 거리의 순서 가 원래 거리 행렬의 순서와 동일하도록 점들을 새로운 공간에 임베딩하려고 시도합니다.\n비계량 MDS는 행렬 (d)에 주어진 비유사성의 변환 (f)와, 이 새로운 맵에서의 거리 ()가 (f(d))가 되도록 하는 낮은 차원 공간(맵)에서의 좌표 세트를 찾습니다. 근사의 품질은 표준화된 잔차 제곱합(stress) 함수로 측정될 수 있습니다:\n[ ^2=. ]\nNMDS는 처음에 기저 차원수를 지정해야 하고 그 숫자에 따라 거리의 재구성을 최대화하도록 최적화가 실행된다는 점에서 순차적(sequential)이지 않습니다. PCA에서 제공되는 것처럼 개별 축에 의해 설명되는 변동의 백분율이라는 개념은 없습니다. 하지만 모든 연속적인 (k) 값((k=1, 2, 3, …))에 대해 프로그램을 실행하고 스트레스(stress)가 얼마나 잘 떨어지는지 살펴봄으로써 유사-스크리 플롯(simili-screeplot)을 만들 수 있습니다. 여기 이러한 연속적인 근사치들과 그들의 적합도를 살펴보는 예시가 있습니다. 군집화를 위한 진단 사례에서와 마찬가지로, 우리는 스트레스가 급격히 떨어진 후 의 축의 개수를 택할 것입니다.\nNMDS 결과의 각 계산은 무작위적이면서 (k) 값에 의존하는 새로운 최적화를 필요로 하므로, 우리는 4장에서 군집화를 위해 했던 것과 유사한 절차를 사용합니다. 우리는 metaMDS 함수를 각 네 가지 가능한 (k) 값에 대해 예를 들어 100번씩 실행하고 스트레스 값들을 기록합니다.\nlibrary(\"vegan\")\nnmds.stress = function(x, sim = 100, kmax = 4) {\n  sapply(seq_len(kmax), function(k)\n    replicate(sim, metaMDS(x, k = k, autotransform = FALSE)$stress))\n}\nstress = nmds.stress(disekm, sim = 100)\ndim(stress)\n결과의 박스플롯을 살펴봅시다. 이는 (k)를 선택하기 위한 유용한 진단 플롯이 될 수 있습니다 (그림 9.10).\ndfstr = reshape2::melt(stress, varnames = c(\"replicate\",\"dimensions\"))\nggplot(dfstr, aes(y = value, x = dimensions, group = dimensions)) +\n  geom_boxplot()\n\n그림 9.10: 스트레스의 안정성을 평가하기 위해 각 차원에서 여러 번의 반복 실험이 수행되었습니다. 우리는 2차원 이상에서 스트레스가 급격히 떨어지는 것을 볼 수 있으며, 이는 여기서 2차원 솔루션이 적절함을 나타냅니다.\n또한 예를 들어 (k=2)에 대해 셰퍼드 플롯(Shepard plot) 이라고 알려진 것을 사용하여 거리와 그 근사치를 비교할 수 있습니다:\nnmdsk2 = metaMDS(disekm, k = 2, autotransform = FALSE)\nstressplot(nmdsk2, pch = 20)\n\n그림 9.11: 셰퍼드 플롯은 원래의 거리 또는 비유사성(가로축)을 재구성된 거리(세로축, 이 경우 (k=2))와 비교합니다.\n그림 9.11의 셰퍼드 플롯과 그림 9.10의 스크리 플롯 모두 Ekman의 색상 혼동 연구에 대해 2차원 솔루션을 가리킵니다. 두 가지 서로 다른 MDS 프로그램인 고전적 계량 최소제곱 근사법과 비계량 순위 근사법의 출력을 비교해 봅시다. 그림 9.12의 오른쪽 패널은 비계량 순위 근사의 결과를 보여주며, 왼쪽 패널은 그림 9.9와 동일합니다. 두 경우 모두 투영이 거의 동일합니다. 이 데이터의 경우, 유클리드 다차원 척도법을 사용하든 비계량 다차원 척도법을 사용하든 거의 차이가 없습니다.\nnmdsk2$points[, 1:2] |&gt; \n  `colnames&lt;-`(paste0(\"NmMDS\", 1:2)) |&gt;\n  as_tibble() |&gt; \n  bind_cols(dplyr::select(dfekm, rgb, name)) |&gt;\n  ggplot(aes(x = NmMDS1, y = NmMDS2)) +\n    geom_point(col = dfekm$rgb, size = 4) +\n    geom_text_repel(aes(label = name))\n\n\n\n\n\n\n\n\n그림 9.12: (a) 고전적 다차원 척도법(그림 9.9와 동일)과 (b) 비계량 버전의 출력 비교.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>이질적 데이터의 다변량 분석 (Multivariate Analysis for Heterogeneous Data)</span>"
    ]
  },
  {
    "objectID": "09-chap.html#연속-정보-또는-보조-정보",
    "href": "09-chap.html#연속-정보-또는-보조-정보",
    "title": "11  이질적 데이터의 다변량 분석 (Multivariate Analysis for Heterogeneous Data)",
    "section": "11.2 9.3 연속 정보 또는 보조 정보",
    "text": "11.2 9.3 연속 정보 또는 보조 정보\n\n\n\n메타데이터: 생물학적 서열 분석이나 어세이를 위한 많은 프로그램들과 워크플로들은 환경적이고 맥락적인 정보를 어세이 데이터나 서열 리드들과 분리하며, 이를 메타데이터라고 부릅니다. 우리는 샘플과 공변량들 사이의 정확한 연결이 중요하기 때문에 그러한 실무를 권장하지 않습니다. 어세이와 공변량 사이의 연결이 끊어지면 나중의 분석이 불가능해집니다. 임상 이력, 시간, 배치 또는 위치와 같은 공변량들은 중요하며 데이터의 구성 요소로 간주되어야 합니다.\n\n\n메타데이터(Metadata): 생물학적 서열 분석이나 어세이를 위한 많은 프로그램들과 워크플로들은 환경적이고 맥락적인 정보를 어세이 데이터나 서열 리드들과 분리하며, 이를 메타데이터 라고 부릅니다. 우리는 샘플과 공변량들 사이의 정확한 연결이 중요하기 때문에 그러한 실무를 권장하지 않습니다. 어세이와 공변량 사이의 연결이 끊어지면 나중의 분석이 불가능해집니다. 임상 이력, 시간, 배치 또는 위치와 같은 공변량들은 중요하며 데이터의 구성 요소로 간주되어야 합니다.\n3장에서 우리는 서로 다른 이종 데이터 유형인 범주형 요인, 텍스트 및 연속형 값을 결합할 수 있게 해주는 R의 data.frame 클래스를 소개했습니다. 데이터 프레임의 각 행은 객체나 레코드에 대응하고, 열은 서로 다른 변수나 특징들에 대응합니다.\n샘플 배치, 측정 날짜, 서로 다른 프로토콜에 대한 추가 정보는 종종 메타데이터 라 불리기도 합니다; 만약 메타데이터가 어쩐지 덜 중요하다는 것을 암시하는 것이라면 이는 잘못된 명칭일 수 있습니다. 그러한 정보는 분석에 통합되어야 할 실제 데이터 입니다. 우리는 보통 이를 data.frame 이나 유사한 R 클래스에 저장하며, 이를 주요 어세이 데이터와 강력하게 연결합니다.\n\n11.2.1 9.3.1 데이터에서의 알려진 배치(batches)\n여기서는 Holmes 등 (2011)에 의해 Phylochip (Brodie et al. 2006) 마이크로어레이로부터 얻은 박테리아 풍부도 데이터에 대해 수행된 분석 예시를 보여줍니다. 이 실험은 건강한 쥐 그룹과 과민성 대장 증후군을 앓고 있는 그룹 사이의 차이를 탐지하기 위해 설계되었습니다 (Nelson et al. 2010). 이 예시는 실험 데이터 분석에서 성가신 배치 효과가 어떻게 명백해지는지를 보여줍니다. 이는 데이터 분석의 모범 사례가 순차적이어야 한다는 것과, 실험 설계상의 심각한 문제가 발생할 때마다 이를 조정하기 위해 데이터를 수집되는 대로 분석하는 것이 사후에 처리하는 것보다 더 낫다는 사실을 잘 보여줍니다4.\n4 피셔(Fisher)의 용어, 13장을 참조하십시오.\n데이터 수집이 이 프로젝트에서 시작되었을 때, 1일째와 2일째의 데이터가 전달되었고 우리는 그림 9.14에 나타난 플롯을 만들었습니다. 이는 분명한 날짜(day) 효과를 보여주었습니다. 이 효과의 원인을 조사한 결과, 1일째와 2일째에 사용된 프로토콜과 칩(array)이 모두 달랐음을 발견했습니다. 이는 변동의 원인에 대한 불확실성을 초래하며, 우리는 이를 효과의 혼동(confounding) 이라고 부릅니다.\n\n\n\nBioconductor 컨테이너: 이 데이터는 배치 정보를 실제 데이터와 결합하는 다소 어색한 방식의 예입니다. 날짜 정보가 어레이 데이터와 결합되어 숫자로 인코딩되었으며, 이는 연속형 변수와 혼동될 수 있습니다. 다음 섹션에서는 Bioconductor의 SummarizedExperiment라는 컨테이너를 사용하여 불균질한 데이터를 저장하고 조작하는 더 나은 실무 방식을 살펴볼 것입니다.\n\n\nBioconductor 컨테이너: 이 데이터는 배치 정보를 실제 데이터와 결합하는 다소 어색한 방식의 예입니다. day 정보가 어레이 데이터와 결합되어 숫자로 인코딩되었으며, 이는 연속형 변수와 혼동될 수 있습니다. 다음 섹션에서는 Bioconductor의 SummarizedExperiment 라는 컨테이너를 사용하여 불균질한 데이터를 저장하고 조작하는 더 나은 실무 방식을 살펴볼 것입니다.\n우리는 데이터를 불러오고 이 섹션에서 사용할 패키지들을 로드합니다:\nIBDchip = readRDS(\"../data/vsn28Exprd.rds\")\nlibrary(\"ade4\")\nlibrary(\"factoextra\")\nlibrary(\"sva\")\n질문 9.5\nIBDchip의 클래스는 무엇인가요? 행렬의 마지막 행을 살펴보세요. 무엇을 알 수 있나요?\n해결책\nclass(IBDchip)\n\n\n[1] \"matrix\" \"array\" \n\n\ndim(IBDchip)\n\n\n[1] 8635   28\n\n\ntail(IBDchip[,1:3])\n\n\n                                 20CF     20DF     20MF\nbm-026.1.sig_st              7.299308 7.275802 7.383103\nbm-125.1.sig_st              8.538857 8.998562 9.296096\nbru.tab.d.HIII.Con32.sig_st  6.802736 6.777566 6.859950\nbru.tab.d.HIII.Con323.sig_st 6.463604 6.501139 6.611851\nbru.tab.d.HIII.Con5.sig_st   5.739235 5.666060 5.831079\nday                          2.000000 2.000000 2.000000\n\n\ntable(IBDchip[nrow(IBDchip), ])\n\n\n 1  2  3 \n 8 16  4 \n데이터는 28개 샘플에서 측정된 8634개 분류군(taxa)의 정규화된 풍부도 측정값입니다. 우리는 순위-임계값(rank-threshold) 변환을 사용하여, 가장 풍부한 상위 3000개 분류군에는 3000에서 1까지의 점수를 부여하고 나머지(낮은 풍부도) 분류군은 모두 1의 점수를 갖게 합니다. 또한 요인(factor)으로 간주되어야 할 (어색하게 배치된) day 변수를 적절한 분석 데이터와 분리합니다5:\n5 아래에서 우리는 이러한 데이터를 Bioconductor의 SummarizedExperiment 로 정리하는 방법을 보여줄 것인데, 이는 이러한 데이터를 저장하는 훨씬 더 합리적인 방식입니다.\nassayIBD = IBDchip[-nrow(IBDchip), ]\nday      = factor(IBDchip[nrow(IBDchip), ])\n연속형의 정규화된 데이터를 사용하는 대신, 값을 순위로 대체하는 강건한 분석을 사용합니다. 낮은 풍부도 값들은 실제 존재하는 것으로 생각되는 예상 분류군 수를 반영하도록 선택된 임계값으로 인코딩된 동점(ties)으로 간주됩니다:\nrankthreshPCA = function(x, threshold = 3000) {\n  ranksM = apply(x, 2, rank)\n  ranksM[ranksM &lt; threshold] = threshold\n  ranksM = threshold - ranksM\n  dudi.pca(t(ranksM), scannf = FALSE, nf = 2)\n}\npcaDay12 = rankthreshPCA(assayIBD[, day != 3])\nfviz_eig(pcaDay12, bar_width = 0.6) + ggtitle(\"\")\n\n그림 9.13: 스크리 플롯은 샘플들이 2차원 임베딩으로 유용하게 표현될 수 있음을 보여줍니다.\nday12 = day[ day!=3 ]\nrtPCA1 = fviz(pcaDay12, element = \"ind\", axes = c(1, 2), geom = c(\"point\", \"text\"),\n  habillage = day12, repel = TRUE, palette = \"Dark2\",\n  addEllipses = TRUE, ellipse.type = \"convex\") + ggtitle(\"\") +\n  coord_fixed() \nrtPCA1\n\n그림 9.14: 우리는 색상을 사용하여 서로 다른 날짜를 식별하고 샘플 레이블도 유지했습니다. 또한 각 날짜에 대해 볼록 껍질(convex hulls)을 추가했습니다. 그룹 평균은 더 큰 기호(원, 삼각형 또는 사각형)를 가진 점으로 표시됩니다.\n질문 9.6\n왜 순위에 임계값(threshold)을 사용하나요?\n해결책\n실제로 존재하지 않는 종들의 경우 노이즈 수준의 낮은 풍부도가 나타나는데, 이들이 전체의 절반 이상입니다. 이러한 관측치들에 대해 어떠한 유의미한 이유 없이 순위에서 큰 도약이 쉽게 일어날 수 있습니다. 따라서 우리는 낮은 풍부도에 대해 많은 수의 동점(ties)을 만듭니다.\n그림 9.14는 샘플들이 샘플링 날짜에 따라 자연스럽게 두 개의 서로 다른 그룹으로 나뉘는 것을 보여줍니다. 이 효과를 발견한 후, 우리는 이러한 뚜렷한 클러스터들을 설명할 수 있는 차이점들을 조사했습니다. 두 가지 서로 다른 프로토콜이 사용되었고 (1일째는 프로토콜 1, 2일째는 프로토콜 2), 불행히도 그 이틀 동안 사용된 어레이의 출처도 달랐습니다 (1일째는 어레이 1, 2일째는 어레이 2).\n혼동 효과를 풀어내기 위해 4개 샘플로 구성된 세 번째 데이터 세트를 수집해야 했습니다. 3일째에는 어레이 2가 프로토콜 2와 함께 사용되었습니다. 그림 9.15는 다음 코드로 생성된 모든 샘플을 포함하는 새로운 PCA 플롯을 보여줍니다:\npcaDay123 = rankthreshPCA(assayIBD)\nfviz(pcaDay123, element = \"ind\", axes = c(1, 2), geom = c(\"point\", \"text\"),\n  habillage = day, repel = TRUE, palette = \"Dark2\",\n  addEllipses = TRUE, ellipse.type = \"convex\") + \n  ggtitle(\"\") + coord_fixed()\n\n\n\n\n\n\n\n\n그림 9.15: 3일간의 분석을 처음 이틀간의 분석과 비교해 보면, 두 번째 축 좌표의 부호가 반전된 것을 알 수 있습니다: 이는 생물학적 관련성이 없습니다. 중요한 발견은 그룹 3이 그룹 1과 크게 겹친다는 것인데, 이는 가변성을 만든 것이 2일째의 프로토콜 변경이었음을 나타냅니다.\n질문 9.7\n어떤 상황에서 다음 코드를 사용하여 그룹 평균 주변에 신뢰 타원(confidence ellipses)을 만드는 것이 더 선호될까요?\nfviz_pca_ind(pcaDay123, habillage = day, labelsize = 3,\n  palette = \"Dark2\", addEllipses = TRUE, ellipse.level = 0.69)\n\n그림 9.16: 3개 그룹의 경우 고윳값 스크리 플롯은 그림 9.13에 표시된 2개 그룹의 경우와 매우 유사합니다.\n이 시각화를 통해 우리는 원래의 실험 설계에 있었던 결함을 발견할 수 있었습니다. 처음 두 배치는 모두 IBS 쥐와 건강한 쥐에 대해 균형이 잡혀 있었습니다. 하지만 그들은 매우 다른 수준의 가변성과 전반적인 다변량 좌표를 보여줍니다. 사실, 두 개의 혼동된 효과가 있습니다. 그 이틀 동안 어레이와 프로토콜이 모두 달랐습니다. 우리는 3일째에 세 번째 배치 실험을 실행해야 했으며 (보라색으로 표시됨), 이는 1일째의 프로토콜과 2일째의 어레이를 사용했습니다. 세 번째 그룹은 배치 1과 충실하게 겹치며, 이는 프로토콜의 변경이 가변성의 원인이었음을 말해줍니다.\n\n\n11.2.2 9.3.2 배치 효과 제거하기\nassayIBD의 연속형 측정값과 보충(supplementary) 배치 번호를 요인으로 결합함으로써, PCA 맵은 귀중한 조사 도구를 제공했습니다. 이것은 보충 점(supplementary points)6을 사용한 좋은 예입니다. 평균-중심 점들은 세 그룹 각각에 있는 점들의 그룹 평균을 사용하여 만들어지며 플롯에서 추가 마커 역할을 합니다.\n6 새로운 관측 점이 행렬 분해에 사용되지 않기 때문에 이를 보충 점이라고 부릅니다.\n우리는 모든 배치가 원점에 중심을 맞추도록 그룹 평균을 뺌으로써 세 그룹을 재정렬하기로 결정할 수 있습니다. 약간 더 효과적인 방법은 sva 패키지에서 제공하는 ComBat 함수를 사용하는 것입니다. 이 함수는 유사하지만 약간 더 정교한 방법 (경험적 베이즈 혼합 접근법 (Leek et al. 2010))을 사용합니다. 우리는 강건한 PCA를 다시 수행하여 데이터에 미치는 효과를 확인할 수 있습니다 (그림 9.17의 결과 참조):\nmodel0 = model.matrix(~1, day)\ncombatIBD = ComBat(dat = assayIBD, batch = day, mod = model0)\npcaDayBatRM = rankthreshPCA(combatIBD)\nfviz(pcaDayBatRM, element = \"ind\", geom = c(\"point\", \"text\"),\n  habillage = day, repel=TRUE, palette = \"Dark2\", addEllipses = TRUE,\n  ellipse.type = \"convex\", axes =c(1,2)) + coord_fixed() + ggtitle(\"\")\n\n그림 9.17: 배치 효과가 제거된 수정된 데이터는 이제 거의 원점에 중심을 두고 심하게 겹쳐진 세 개의 배치 그룹을 보여줍니다.\n\n\n11.2.3 9.3.3 하이브리드 데이터 및 Bioconductor 컨테이너\n배치와 처리 정보를 복합 객체의 구획으로 결합하는 더 합리적인 방법은 SummarizedExperiment 클래스를 사용하는 것입니다. 이는 행이 관심 있는 특징(예: 유전자, 전사체, 엑손 등)을 나타내고 열이 샘플을 나타내는 분석(assay)을 위한 특수 슬롯(slots) 을 포함합니다. 특징에 대한 보충 정보는 rowData 함수를 사용하여 액세스할 수 있는 DataFrame 객체에 저장될 수 있습니다. DataFrame 의 각 행은 SummarizedExperiment 객체의 상응하는 행에 있는 특징에 대한 정보를 제공합니다.\n\n\n\n여기서 혼동스러운 표기법상의 유사성이 발생하는데, SummarizedExperiment 프레임워크에서 DataFrame은 data.frame과 다릅니다.\n\n\n여기서 혼동스러운 표기법상의 유사성이 발생하는데, SummarizedExperiment 프레임워크에서 DataFrame은 data.frame 과 다릅니다.\n여기서 우리는 날짜와 처리라는 두 공변량을 colData 객체에 삽입하고, 이를 분석 데이터와 결합하여 새로운 SummarizedExperiment 객체를 만듭니다.\nlibrary(\"SummarizedExperiment\")\ntreatment  = factor(ifelse(grepl(\"Cntr|^C\", colnames(IBDchip)), \"CTL\", \"IBS\"))\nsampledata = DataFrame(day = day, treatment = treatment)\nchipse = SummarizedExperiment(assays  = list(abundance = assayIBD),\n                              colData = sampledata)\n이는 모든 관련 데이터를 함께 유지하는 가장 좋은 방법이며, 모든 정보를 올바르게 정렬된 상태로 유지하면서 데이터를 신속하게 필터링할 수 있게 해줍니다.\n\n\n\nRStudio의 Environment 창을 사용하여 복합 객체를 탐색할 수 있습니다. chipse에서 일부 슬롯은 비어 있음을 볼 수 있습니다.\n\n\nRStudio의 Environment 창을 사용하여 복합 객체를 탐색할 수 있습니다. chipse에서 일부 슬롯은 비어 있음을 볼 수 있습니다.\n질문 9.8\n2일째에 생성된 샘플들의 하위 집합을 선택하여 새로운 SummarizedExperiment 객체를 만드세요.\n해결책\nchipse[, day == 2]\n\n\nclass: SummarizedExperiment \ndim: 8634 16 \nmetadata(0): \nassays(1): abundance\nrownames(8634): 01010101000000.2104_gPM_GC 01010101000000.2141_gPM_GC\n  ... bru.tab.d.HIII.Con323.sig_st bru.tab.d.HIII.Con5.sig_st\nrowData names(0): \ncolnames(16): 20CF 20DF ... IBSM IBSP\ncolData names(2): day treatment\nDataFrame 의 열들은 관심 있는 특징들의 서로 다른 속성들을 나타냅니다 (예: 유전자 또는 전사체 ID 등). 여기에 단일 세포 실험에서 얻은 하이브리드 데이터 컨테이너의 예시가 있습니다 (더 자세한 내용은 Perraudeau 등 (2017)의 바이오컨덕터 워크플로를 참조하십시오).\ncorese = readRDS(\"../data/normse.rds\")\nnorm = assays(corese)$normalizedValues\n워크플로에 명시된 전처리 및 정규화 단계들을 거친 후, 우리는 747개 세포에서 측정된 1000개의 가장 가변적인 유전자들을 유지합니다.\n질문 9.9\n세포들은 몇 개의 서로 다른 배치들에 속하나요?\n해결책\nlength(unique(colData(corese)$Batch))\n\n\n[1] 18\n우리는 정규화된 데이터의 PCA를 살펴보고 배치 효과가 제거되었음을 그래픽으로 확인할 수 있습니다:\nrespca = dudi.pca(t(norm), nf = 3, scannf = FALSE)\nplotscree(respca, 15)\nPCS = respca$li[, 1:3]\n\n그림 9.18: 정규화된 데이터의 PCA 스크리 플롯.\n\n\n\n워크플로에서 했던 것과 같이 클러스터들을 위한 색상들을 설정했습니다 (코드는 여기서 보여주지 않습니다).\n\n\n워크플로에서 했던 것과 같이 클러스터들을 위한 색상들을 설정했습니다 (코드는 여기서 보여주지 않습니다).\n그림 9.18의 스크리 플롯은 우리가 축 2와 3을 분리해서는 안 된다는 것을 보여주므로, 우리는 rgl 패키지를 사용하여 3차원 플롯을 만들 것입니다. 우리는 다음과 같은 대화형 코드를 사용합니다:\nlibrary(\"rgl\")\nbatch = colData(corese)$Batch\nplot3d(PCS,aspect=sqrt(c(84,24,20)),col=col_clus[batch])\nplot3d(PCS,aspect=sqrt(c(84,24,20)),\ncol = col_clus[as.character(publishedClusters)])\n[](imgs/plotnormpcabatch1.png “그림 9.19 (a):”)\n\n\n\n: “)\n\n\n\n그림 9.19: 3차원 rgl 플롯의 2차원 스크린샷. 점들은 (a)에서는 배치 번호에 따라, (b)에서는 원래의 군집화에 따라 색상이 입혀졌습니다. 배치 효과가 효과적으로 제거되었고 세포들이 원래의 군집화를 보여줌을 알 수 있습니다.\n참고: 물론 책이라는 매체는 여기서 한계가 있는데, 우리가 plot3d 함수를 사용할 때 나타나는 대화형 동적 플롯에서 얻을 수 있는 깊이감을 충분히 전달하지 못하는 두 개의 정적인 투영만을 보여주고 있기 때문입니다. 독자들께서는 이러한 패키지들을 직접 실험해 보시기를 권장하며, 이들이 데이터에 대한 훨씬 더 직관적인 경험을 제공할 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>이질적 데이터의 다변량 분석 (Multivariate Analysis for Heterogeneous Data)</span>"
    ]
  },
  {
    "objectID": "09-chap.html#대응-분석-correspondence-analysis",
    "href": "09-chap.html#대응-분석-correspondence-analysis",
    "title": "11  이질적 데이터의 다변량 분석 (Multivariate Analysis for Heterogeneous Data)",
    "section": "11.3 9.4 대응 분석 (Correspondence analysis)",
    "text": "11.3 9.4 대응 분석 (Correspondence analysis)\n\n11.3.1 9.4.1 분할표와 대응 분석\n생물학적 환경에는 범주형 데이터가 넘쳐납니다: 서열 상태(CpG/non-CpG), 표현형, 분류군 등은 2장에서 보았듯이 종종 요인(factors)으로 코딩됩니다. 그러한 두 변수를 교차 집계하면 분할표(contingency table) 를 얻게 됩니다; 이는 두 표현형(성별과 색맹이 그러한 예시였습니다)의 공동 발생(co-occurrence)을 세어 얻은 결과입니다. 우리는 첫 번째 단계가 두 범주형 변수의 독립성을 살펴보는 것임을 보았습니다; 독립성에 대한 표준적인 통계적 척도는 카이제곱 거리(chisquare distance) 를 사용합니다. 이 양은 연속형 측정에서 사용했던 분산을 대체할 것입니다.\n표의 열과 행은 동일한 ’상태’를 가지며 우리는 지도 방식/회귀 방식의 설정에 있지 않습니다. 우리는 샘플/변수 구분을 보지 않을 것입니다; 결과적으로 행과 열은 동일한 상태를 가지며 우리는 행과 열 모두를 ’중앙화’할 것입니다. 이러한 대칭성은 우리가 두 차원이 모두 동일한 플롯에 나타나는 바이플롯(biplots) 을 사용하는 것으로도 이어질 것입니다.\n표 9.1: 샘플별 돌연변이 행렬.\n\n\n\n환자\nMut1\nMut2\nMut3\n…\n\n\n\n\nAHX112\n0\n0\n0\n\n\n\nAHX717\n1\n0\n1\n\n\n\nAHX543\n1\n0\n0\n\n\n\n\n\n11.3.1.1 데이터를 표 형식으로 변환하기.\n만약 데이터가 각 피험자(또는 샘플)가 해당 범주형 변수들의 수준들과 연관된 긴 리스트 형태로 수집되었다면, 우리는 이를 분할표로 변환하고 싶을 수 있습니다. 여기에 예시가 있습니다. 표 9.1에서 HIV 돌연변이들은 이진(0/1) 지시 변수로 표로 정리되어 있습니다. 이 데이터는 표 9.2에 표시된 돌연변이 공동 발생 행렬 로 변환됩니다.\n표 9.2: HIV 돌연변이들을 교차 집계하여 보여주는 2원 공동 발생 표.\n\n\n\n환자\nMut1\nMut2\nMut3\n…\n\n\n\n\nMut1\n853\n29\n10\n\n\n\nMut2\n29\n853\n52\n\n\n\nMut3\n10\n52\n853\n\n\n\n\n질문 9.10\n이러한 교차 집계에서 어떤 정보가 손실되나요?\n언제 이것이 중요할까요?\n여기에 HIV 데이터베이스 (Rhee et al. 2003)로부터 얻은 공동 발생 데이터가 있습니다. 이 돌연변이들 중 일부는 함께 발생하는 경향이 있습니다.\n질문 9.11\n돌연변이들의 독립성 가설을 테스트하세요.\n대응 분석이 어떻게 작동하는지에 대한 세부 사항을 설명하기 전에, 여러 대응 분석 함수들 중 하나의 출력을 살펴봅시다. 우리는 PCA에서 했던 것과 유사한 절차에 따라 돌연변이들을 저차원 투영으로 플롯하기 위해 ade4 패키지의 dudi.coa를 사용합니다.\ncooc = read.delim2(\"../data/coccurHIV.txt\", header = TRUE, sep = \",\")\ncooc[1:4, 1:11]\n\n\n    X4S X6D X6K X11R X20R X21I X35I X35L X35M X35T X39A\n4S    0  28   8    0   99    0   22    5   15    3   45\n6D   26   0   0   34  131    0  108    4   30   13   84\n6K    7   0   0    6   45    0    5   13   38   35   12\n11R   0  35   7    0  127   12   60   17   15    6   42\n\n\nHIVca = dudi.coa(cooc, nf = 4, scannf = FALSE)\nfviz_eig(HIVca, geom = \"bar\", bar_width = 0.6) + ggtitle(\"\")\n\n그림 9.20: HIV 돌연변이들 사이의 의존성은 명백히 3차원적인 현상이며, 처음 세 개의 고윳값은 데이터에 뚜렷한 신호가 있음을 보여줍니다.\n\n그림 9.21: 대화형 3D 플로팅 함수(plot3d)의 출력 스크린샷.\n스크리 플롯을 살펴본 후, 기저 변동의 차원성이 분명히 3차원임을 알 수 있으며, 이 세 차원을 플롯합니다. 이상적으로는 그림 9.21에 표시된 것과 같이 rgl 패키지에 의해 제공되는 대화형 3차원 플로팅 함수를 사용하여 수행될 것입니다.\n질문 9.12\ncar 와 rgl 패키지를 사용하여 그림 9.21과 유사한 3D 산점도를 만드세요.\nrgl 의 plot3d 함수에서 aspect=FALSE를 사용했을 때 얻은 플롯과 비교해 보세요.\n점 구름을 회전시키면서 어떤 구조를 발견하셨나요?\n해결책\nlibrary(\"rgl\")\nCA1=HIVca$li[,1];CA2=HIVca$li[,2];CA3=HIVca$li[,3]\nplot3d(CA1,CA2,CA3,aspect=FALSE,col=\"purple\")\n\n\nfviz_ca_row(HIVca,axes = c(1, 2),geom=\"text\", col.row=\"purple\",\n  labelsize=3)+ggtitle(\"\") + xlim(-0.55, 1.7) + ylim(-0.53,1.1) + \n  theme_bw() +  coord_fixed()\nfviz_ca_row(HIVca,axes = c(1, 3), geom=\"text\",col.row=\"purple\",\n    labelsize=3)+ggtitle(\"\")+ xlim(-0.55, 1.7)+ylim(-0.5,0.6) + \n    theme_bw() + coord_fixed()\n\n\n\n\n\n\n\n\n그림 9.22: 대응 분석의 첫 번째 고유벡터에 해당하는 가로축과, (a)에서는 두 번째 축, (b)에서는 세 번째 축인 세로축으로 정의된 돌연변이들의 두 평면 지도; 높이의 차이에 주목하세요.\n질문 9.13\n그림 9.22의 하단 그림에 표시된 것과 같이 세로축의 스케일을 존중하면서 대응 분석의 축 1과 3에 의해 정의된 평면을 플로팅하는 코드를 보여주세요.\n해결책\nfviz_ca_row(HIVca, axes=c(1, 3), geom=\"text\", col.row=\"purple\", labelsize=3) +\n  ggtitle(\"\") + theme_minimal() + coord_fixed()\n이 첫 번째 예제는 PCA가 연속형 변수들을 투영하는 것과 유사한 방식으로, 하나의 범주형 변수(돌연변이들)의 모든 서로 다른 수준들을 매핑하는 방법을 보여주었습니다. 이제 이를 두 개 이상의 범주형 변수로 확장하는 방법을 살펴보겠습니다.\n\n\n\n11.3.2 9.4.2 머리카락 색깔, 눈 색깔 및 표현형 공동 발생\n분석을 자세히 따라갈 수 있도록 작은 표를 고려하겠습니다. 데이터는 표 9.3에 표시된 학생들의 머리카락 색깔과 눈 색깔 표현형 공동 발생에 대한 분할표입니다. 2장에서 우리는 가능한 의존성을 탐지하기 위해 (^2) 검정을 사용했습니다.\nHairColor = HairEyeColor[,,2]\nchisq.test(HairColor)\n\n\n    Pearson's Chi-squared test\n\ndata:  HairColor\nX-squared = 106.66, df = 9, p-value &lt; 2.2e-16\n표 9.3: 학생들의 머리카락 색깔과 눈 색깔 분할표.\n\n\n\n갈색\n파란색\n헤이즐\n초록색\n\n\n\n\n\n검은색\n36\n9\n5\n2\n\n\n갈색\n66\n34\n29\n14\n\n\n빨간색\n16\n7\n7\n7\n\n\n금발\n4\n64\n5\n8\n\n\n\n하지만 머리카락과 눈 색깔 사이의 비독립성 을 진술하는 것만으로는 충분하지 않습니다. 우리는 의존성이 어디에서 발생하는지에 대한 더 자세한 설명이 필요합니다: 어떤 머리카락 색깔이 초록색 눈과 더 자주 발생하나요? 일부 변수 수준들은 독립적인가요? 사실 우리는 SVD의 특수한 가중치 버전을 사용하여 독립성으로부터의 이탈을 연구할 수 있습니다. 이 방법은 PCA와 MDS를 분할표로 확장한 단순한 것으로 이해될 수 있습니다.\n\n11.3.2.1 독립성: 계산 및 시각화.\n우리는 행과 열의 합을 계산하는 것으로 시작합니다; 우리는 이를 사용하여 두 표현형이 독립적일 경우 예상되는 표를 만듭니다. 이 예상 표를 HCexp라고 부릅시다.\nrowsums = as.matrix(apply(HairColor, 1, sum))\nrowsums\n\n\n      [,1]\nBlack   52\nBrown  143\nRed     37\nBlond   81\n\n\ncolsums = as.matrix(apply(HairColor, 2, sum))\nt(colsums)\n\n\n     Brown Blue Hazel Green\n[1,]   122  114    46    31\n\n\nHCexp = rowsums %*%t (colsums) / sum(colsums)\n이제 표의 각 셀에 대해 척도 조정된 잔차의 합인 (^2)(카이제곱) 통계량을 계산합니다:\nsum((HairColor  - HCexp)^2/HCexp)\n\n\n[1] 106.6637\n우리는 이 잔차들을 예상 표로부터 연구할 수 있으며, 먼저 수치적으로 확인한 다음 그림 9.23에서 살펴보겠습니다.\nround(t(HairColor-HCexp))\n\n\n       Hair\nEye     Black Brown Red Blond\n  Brown    16    10   2   -28\n  Blue    -10   -18  -6    34\n  Hazel    -3     8   2    -7\n  Green    -3     0   3     0\n\n\nlibrary(\"vcd\")\nmosaicplot(HairColor, shade=TRUE, las=1, type=\"pearson\", cex.axis=0.7, main=\"\")\n\n그림 9.23: 독립성으로부터의 이탈 시각화. 이제 상자들의 크기는 실제 관측된 카운트에 비례하며, 더 이상 ‘직사각형’ 속성을 갖지 않습니다. 독립성으로부터의 이탈은 각 상자에 대해 카이제곱 거리로 측정되며, 잔차가 크고 양(+)수인지 여부에 따라 색상이 입혀집니다. 짙은 파란색은 양의 연관성을 나타내며(예: 파란색 눈과 금발), 빨간색은 음의 연관성을 나타냅니다(예: 금발과 갈색 눈).\n\n\n11.3.2.2 수학적 공식화.\n우리가 방금 R에서 수행한 계산들을 좀 더 수학적인 형식으로 정리하면 다음과 같습니다. (I)개의 행과 (J)개의 열을 가지고 전체 표본 크기가 (n={i=1}^I {j=1}^J n_{ij}= n_{})인 일반적인 분할표 ({N})에 대해, 만약 두 범주형 변수가 독립적이라면 각 셀의 빈도는 대략 다음과 같을 것입니다:\n[ n_{ij} = n ]\n이는 다음과 같이 쓰여질 수도 있습니다:\n[ {N} = {c r’} n, c= {{N}} {}_m ; ; r’= {N}’ {}_p ]\n독립성으로부터의 이탈은 (^2) 통계량으로 측정됩니다:\n[ {X}^2=_{i,j} {n} ]\n두 변수가 독립적이지 않다는 것을 확인하고 나면, 연관성을 시각화하기 위해 (^2) 거리를 사용하는 가중 다차원 척도법을 사용합니다.\n대응 분석 함수들: vegan 의 CCA, FactoMineR 의 CA, phyloseq 의 ordinate, ade4 의 dudi.coa.\n이 방법은 대응 분석(Correspondence Analysis, CA) 또는 쌍대 척도법(Dual Scaling) 이라고 불리며, 이를 구현하는 여러 R 패키지가 있습니다.\n여기서는 머리카락 색깔과 눈 색깔에 대한 단순한 바이플롯을 만듭니다.\nHC = as.data.frame.matrix(HairColor)\ncoaHC = dudi.coa(HC,scannf=FALSE,nf=2)\nround(coaHC$eig[1:3]/sum(coaHC$eig)*100)\n\n\n[1] 89 10  2\n\n\nfviz_ca_biplot(coaHC, repel=TRUE, col.col=\"brown\", col.row=\"purple\") +\n  ggtitle(\"\") + ylim(c(-0.5,0.5))\n\n그림 9.24: CA 플롯은 데이터와 독립성 가정하의 예상 값 사이의 카이제곱 거리의 상당 부분을 나타냅니다. 첫 번째 축은 검은 머리와 금발 학생 사이의 대비를 보여주며, 이는 갈색 눈과 파란 눈의 대립에 의해 반영됩니다. CA에서 두 범주는 대칭적인 역할을 하며, 우리는 파란 눈과 금발의 근접성을 이들 범주의 강력한 공동 발생을 의미하는 것으로 해석할 수 있습니다.\n질문 9.14\n카이제곱 통계량의 몇 퍼센트가 대응 분석의 처음 두 축에 의해 설명되나요?\n질문 9.15\n적절한 scaling 매개변수 값을 사용하여 vegan 패키지의 CCA로 얻은 결과와 비교해 보세요.\n해결책\nlibrary(\"vegan\")\nres.ca = vegan::cca(HairColor)\nplot(res.ca, scaling=3)\n\n\n11.3.2.3 바이플롯 해석하기\nCA는 특별한 무게 중심(barycentric) 속성을 가집니다: 바이플롯 스케일링은 행 지점들이 각자의 가중치를 가진 채 열 수준들의 무게 중심에 놓이도록 선택됩니다. 예를 들어, 파란 눈(Blue eyes) 열 지점은 (검정, 갈색, 빨강, 금발)의 무게 중심에 위치하며 그 가중치는 (9, 34, 7, 64)에 비례합니다. 금발(Blond) 행 지점은 매우 무겁게 가중치가 부여되며, 이것이 그림 9.24에서 금발과 파란 눈이 매우 가깝게 나타나는 이유입니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>이질적 데이터의 다변량 분석 (Multivariate Analysis for Heterogeneous Data)</span>"
    ]
  },
  {
    "objectID": "09-chap.html#시간-및-기타-중요한-그래디언트-찾기",
    "href": "09-chap.html#시간-및-기타-중요한-그래디언트-찾기",
    "title": "11  이질적 데이터의 다변량 분석 (Multivariate Analysis for Heterogeneous Data)",
    "section": "11.4 9.5 시간… 및 기타 중요한 그래디언트 찾기",
    "text": "11.4 9.5 시간… 및 기타 중요한 그래디언트 찾기\n지난 섹션에서 공부한 모든 방법들은 흔히 서열화(ordination) 방법으로 알려져 있습니다. 군집화 가 우리에게 숨겨진 요인/범주형 변수를 탐지하고 해석할 수 있게 해준 것과 같은 방식으로, 서열화는 데이터에서 숨겨진 순서, 그래디언트(gradient) 또는 잠재 변수를 탐지하고 해석할 수 있게 해줍니다.\n\n생태학자들은 대응 분석과 주성분 분석에서 관측 점들에 의해 형성되는 아치 구조를 생태학적 그래디언트로 해석해 온 오랜 역사를 가지고 있습니다 (Prentice 1977). 먼저 대응 분석을 수행하는 매우 간단한 데이터 세트를 통해 이를 설명해 보겠습니다.\n서열화(seriation) 또는 연대 측정의 첫 번째 예시는 Kendall (1969)에 의한 고고학 유물 연구였습니다. 그는 도자기의 특징 유무를 사용하여 도자기의 연대를 측정했습니다. 이러한 소위 서열화 방법은 오늘날 예를 들어 단일 세포 데이터의 발달 궤적을 추적할 때도 여전히 유효합니다.\nload(\"../data/lakes.RData\")\nlakelike[1:3,1:8]\n\n\n     plant1 plant2 plant3 plant4 plant5 plant6 plant7 plant8\nloc1      6      4      0      3      0      0      0      0\nloc2      4      5      5      3      4      2      0      0\nloc3      3      4      7      4      5      2      1      1\n\n\nreslake=dudi.coa(lakelike,scannf=FALSE,nf=2)\nround(reslake$eig[1:8]/sum(reslake$eig),2)\n\n\n[1] 0.56 0.25 0.09 0.03 0.03 0.02 0.01 0.00\n우리는 행-위치 점들(그림 9.25 (a))과 위치 및 식물 종 모두의 바이플롯을 그림 9.25 (b)의 하단에 플롯합니다; 이 플롯은 다음과 같이 만들어졌습니다:\nfviz_ca_row(reslake,repel=TRUE)+ggtitle(\"\")+ylim(c(-0.55,1.7))\nfviz_ca_biplot(reslake,repel=TRUE)+ggtitle(\"\")+ylim(c(-0.55,1.7))\n\n\n\n\n\n\n\n\n그림 9.25: 호수 근처의 위치들은 (a)에서 보듯이 아치를 따라 정렬되어 있습니다. 바이플롯 (b)에서는 파란색 점들에 가장 가까운 빨간색 삼각형들을 살펴봄으로써 어떤 위치에서 어떤 식물이 가장 빈번한지 알 수 있습니다.\n질문 9.16\n나타난 그대로의 원시 행렬 lakes를 다시 살펴보면, 그 항목들에서 어떤 패턴이 보이나요?\n만약 식물들이 실제 분류군 이름에 따라 정렬되었다면 어떤 일이 일어났을까요?\n\n11.4.1 9.5.1 세포 발달의 역학\n이제 Moignard 등 (2015)에 의해 발표된 더 흥미로운 데이터 세트를 분석해 보겠습니다. 이 논문은 혈액 세포 발달의 역학을 설명합니다. 데이터는 배아기 E7.0일과 E8.25일 사이의 5개 모집단으로부터 얻은, 혈액 및 내피 세포 잠재력을 가진 3,934개 세포의 단일 세포 유전자 발현 측정값입니다.\n\n그림 9.26: 여기서 연구된 네 가지 세포 모집단은 세 가지 순차적 상태(PS, NP, HF)와 두 가지 가능한 최종 분기(4SG 및 4SFG(^{-}))를 대표합니다.\n세포들을 비교하기 위해 여러 가지 서로 다른 거리를 사용할 수 있음을 4장에서 상기해 보세요. 여기서는 3,934개 세포 사이의 (L_2) 거리와 (_1) 거리를 모두 계산하는 것부터 시작합니다.\nMoignard = readRDS(\"../data/Moignard.rds\")\ncellt = rowData(Moignard)$celltypes\ncolsn = c(\"red\", \"purple\", \"orange\", \"green\", \"blue\")\nblom = assay(Moignard)\ndist2n.euclid = dist(blom)\ndist1n.l1     = dist(blom, \"manhattan\")\n이 두 거리 행렬에 대한 고전적 다차원 척도법은 다음과 같이 수행될 수 있습니다:\nce1Mds = cmdscale(dist1n.l1,     k = 20, eig = TRUE)\nce2Mds = cmdscale(dist2n.euclid, k = 20, eig = TRUE)\nperc1  = round(100*sum(ce1Mds$eig[1:2])/sum(ce1Mds$eig))\nperc2  = round(100*sum(ce2Mds$eig[1:2])/sum(ce2Mds$eig))\n우리는 기저 차원을 살펴보고, 그림 9.27에서 두 개 차원이 분산의 상당 부분을 제공할 수 있음을 확인합니다.\nplotscree(ce1Mds, m = 4)\nplotscree(ce2Mds, m = 4)\n\n\n\n\n\n\n\n\n그림 9.27: (_1) (a) 및 (L_2) (b) 거리에 대한 MDS의 스크리 플롯. 우리는 고윳값들이 매우 유사하며 둘 다 2차원 현상을 가리키고 있음을 알 수 있습니다.\n처음 2개 좌표는 세포 간에 (_1) 거리가 사용되었을 때 가변성의 78%를 차지하고, (L_2) 거리가 사용되었을 때는 57%를 차지합니다. 그림 9.28 (a)에서 세포 간의 (_1) 거리에 대한 MDS의 첫 번째 평면을 봅니다:\nc1mds = ce1Mds$points[, 1:2] |&gt;\n        `colnames&lt;-`(paste0(\"L1_PCo\", 1:2)) |&gt;\n        as_tibble()\nggplot(c1mds, aes(x = L1_PCo1, y = L1_PCo2, color = cellt)) +\n  geom_point(aes(color = cellt), alpha = 0.6) +\n  scale_colour_manual(values = colsn) + guides(color = \"none\")\nc2mds = ce2Mds$points[, 1:2] |&gt;\n        `colnames&lt;-`(paste0(\"L2_PCo\", 1:2)) |&gt;\n        as_tibble()\nggplot(c2mds, aes(x = L2_PCo1, y = L2_PCo2, color = cellt)) +\n  geom_point(aes(color = cellt), alpha = 0.6) +\n   scale_colour_manual(values = colsn) + guides(color = \"none\")\n\n\n\n\n\n\n\n\n그림 9.28: 생성된 2차원 MDS 플롯에서 세포 유형 (파란색: PS, 초록색: NP, 노란색: HF, 빨간색: 4SG, 보라색: 4SFG(^{-}))에 따라 색상이 입혀진 Moignard 세포 데이터. (a)는 (_1) 거리를, (b)는 (L_2) 거리를 사용한 경우입니다.\n그림 9.28 (b)는 같은 방식으로 만들어졌으며, (L_2) 거리에 대한 MDS를 사용하여 생성된 2차원 투영을 보여줍니다.\n그림 9.28은 두 거리(L1과 L2) 모두 세포들이 따르는 기저의 그래디언트에 대해 매우 유사한 표현을 가진 동일한 첫 번째 평면을 제공함을 보여줍니다.\n우리는 그림 9.28로부터 세포들이 우리가 살펴본 저차원에서 균일하게 분포되어 있지 않음을 알 수 있으며, 점들의 뚜렷한 조직화를 볼 수 있습니다. 빨간색으로 표시된 4SG 유형의 모든 세포들은 길쭉한 클러스터를 형성하며 다른 세포 유형들과 훨씬 덜 섞여 있습니다.\n\n\n11.4.2 9.5.2 국소적, 비선형적 방법들\n다차원 척도법과 비계량 다차원 척도법은 모든 거리를 가능한 한 정확하게 표현하는 것을 목표로 하며, 멀리 떨어진 점들 사이의 큰 거리가 표현을 왜곡합니다. 그래디언트나 저차원 매니폴드(manifolds)를 찾을 때, 우리 자신을 서로 가까이 있는 점들의 근사로 제한하는 것이 유익할 수 있습니다. 이는 국소적인 (작은) 거리들을 잘 표현하려 노력하고 멀리 떨어진 점들 사이의 거리를 너무 정확하게 근사하려 하지 않는 방법들을 요구합니다.\n최근 몇 년 동안 그러한 방법들에서 상당한 진전이 있었습니다. 계산된 커널(kernels) 의 사용은 멀리 떨어진 점들의 중요성을 감소시킬 수 있게 해줍니다. 방사 기저 커널(radial basis kernel)은 다음과 같은 형태입니다:\n[ 1-(-), ^2 ]\n이는 큰 거리들을 대폭 할인하는 효과가 있습니다. 점 간 거리의 정밀도는 종종 작은 범위에서 더 좋기 때문에 이는 매우 유용할 수 있습니다; 그러한 방법들의 여러 예시들이 이 장의 끝에 있는 연습 문제 9.6에서 다뤄집니다.\n질문 9.17\n왜 우리는 1과 지수 함수(exponential) 사이의 차이를 취하나요?\n(x)와 (y) 사이의 거리가 매우 클 때 어떤 일이 일어나나요?\n\n11.4.2.1 t-SNE.\n이 널리 사용되는 방법은 위에서 정의한 커널에 유연성을 더하며 (^2) 매개변수가 국소적으로 변할 수 있게 해줍니다 (평균이 1이 되도록 하는 정규화 단계가 있습니다). t-SNE 방법은 고차원 공간에서의 점들의 위치로부터 시작하여, 확률이 점들의 근접성이나 유사성에 비례하도록 점 쌍들의 집합에 대한 확률 분포를 도출합니다. 그런 다음 이 분포를 사용하여 저차원에서 데이터 세트의 표현을 구성합니다. 이 방법은 강건하지 않으며 클러스터들을 인위적으로 분리하는 속성을 가지고 있습니다; 하지만 이 속성은 복잡한 상황을 명확하게 하는 데 도움이 될 수도 있습니다. 이 방법은 그래프(또는 네트워크) 레이아웃 알고리즘과 유사한 방법으로 생각할 수 있습니다. 이들은 매우 가까운 (네트워크에서: 연결된) 점들 사이의 관계를 명확히 하기 위해 데이터를 늘리지만, 더 멀리 떨어진 (네트워크에서: 연결되지 않은) 점들 사이의 거리는 플롯의 서로 다른 영역에서 동일한 척도로 해석될 수 없습니다. 특히, 이러한 거리들은 국소적인 점 밀도에 의존하게 될 것입니다. 여기에 세포 데이터에 대한 t-SNE 출력의 예시가 있습니다:\nlibrary(\"Rtsne\")\nrestsne = Rtsne(blom, dims = 2, perplexity = 30, verbose = FALSE,\n                max_iter = 900)\ndftsne = restsne$Y[, 1:2] |&gt;\n         `colnames&lt;-`(paste0(\"axis\", 1:2)) |&gt;\n         as_tibble()\nggplot(dftsne,aes(x = axis1, y = axis2, color = cellt)) +\n  geom_point(aes(color = cellt), alpha = 0.6) +\n   scale_color_manual(values = colsn) + guides(color = \"none\")\nrestsne3 = Rtsne(blom, dims = 3, perplexity = 30, verbose = FALSE,\n                 max_iter = 900)\ndftsne3 = restsne3$Y[, 1:3] |&gt;\n          `colnames&lt;-`(paste0(\"axis\", 1:3)) |&gt; \n          as_tibble()\nggplot(dftsne3,aes(x = axis3, y = axis2, group = cellt)) +\n      geom_point(aes(color = cellt), alpha = 0.6) +\n      scale_colour_manual(values = colsn) + guides(color = \"none\")\n\n\n\n\n\n\n\n\n그림 9.29: 여기서 연구된 네 가지 세포 모집단은 세 가지 순차적 상태(PS, NP, HF)와 두 가지 가능한 최종 분기(4SG 및 4SFG(^{-}))를 대표합니다. 왼쪽 플롯은 당혹감(perplexity)을 30으로 설정하고 t-SNE에 대해 2개 차원을 선택하여 얻은 것입니다. 하단 플롯은 3개 차원을 선택하여 얻은 것인데, 여기서 수평축으로 표시된 것이 세 번째 t-SNE 축임을 알 수 있습니다.\n이 경우 MDS와 t-SNE 사이의 미세한 차이를 확인하려면 3D 플로팅을 사용하는 것이 정말 필요합니다.\n태스크\nrgl 패키지를 사용하여 3개의 t-SNE 차원을 살펴보고 디스플레이에 올바른 세포 유형 색상을 추가해 보세요.\n이러한 3D 스냅샷 중 두 개가 그림 9.30에 나와 있습니다. MDS 플롯에서보다 보라색 점들이 훨씬 더 강력하게 그룹화된 것을 볼 수 있습니다.\n참고: 복잡성(complexity) 및 () 매개변수에 대한 t-SNE 방법의 민감도에 대해 더 자세히 알아보려면 방문할 가치가 있는 사이트가 http://distill.pub/2016/misread-tsne 에 있습니다.\n[](imgs/tsnemoignard3scrop.png “그림 9.30 (a):”)\n\n\n\n[](imgs/tsnemoignard3crop.png “그림 9.30 (b):”)\n\n\n\n그림 9.30: 3차원 t-SNE 레이아웃에서 세포 유형 (파란색: PS, 초록색: NP, 노란색: HF, 빨간색: 4SG, 보라색: 4SFG(^-))에 따라 색상이 입혀진 Moignard 세포 데이터. 보라색 세포(4SFG(^{-}))가 점 구름 상단의 외곽 껍질에 분리되어 있는 것을 볼 수 있습니다.\n질문 9.18\n9.2절의 우크라이나 거리 데이터에 대해 2차원 t-SNE 임베딩을 시각화해 보세요.\n해결책\nukraine_tsne = Rtsne(ukraine_dists, is_distance = TRUE, perplexity = 8)\nukraine_tsne_df = tibble(\n  PCo1 = ukraine_tsne$Y[, 1],\n  PCo2 = ukraine_tsne$Y[, 2],\n  labs = attr(ukraine_dists, \"Labels\")\n)\nggplot(ukraine_tsne_df, aes(x = PCo1, y = PCo2, label = labs)) +\n  geom_point() + geom_text_repel(col = \"#0057b7\") + coord_fixed()\n\n그림 9.31: 우크라이나의 t-SNE 지도.\n관련 상태 공간에서 점들이 따르는 비선형 궤적을 추정하기 위한 다른 여러 비선형 방법들이 있습니다. 여기에 몇 가지 예시가 있습니다:\nRDRToolbox 국소 선형 임베딩(LLE) 및 isomap\ndiffusionMap 이 패키지는 점들 사이의 연결을 마르코프 커널(Markovian kernel)로 모델링합니다.\nkernlab 커널 방법들\nLPCM-package 국소 주성분 곡선 (Local principal curves)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>이질적 데이터의 다변량 분석 (Multivariate Analysis for Heterogeneous Data)</span>"
    ]
  },
  {
    "objectID": "09-chap.html#다중-표multitable-기법",
    "href": "09-chap.html#다중-표multitable-기법",
    "title": "11  이질적 데이터의 다변량 분석 (Multivariate Analysis for Heterogeneous Data)",
    "section": "11.5 9.6 다중 표(Multitable) 기법",
    "text": "11.5 9.6 다중 표(Multitable) 기법\n현재의 연구들은 종종 서로 다른 실험 조건에 따른 미생물, 유전체 및 대사 측정값의 변동을 정량화하려고 시도합니다. 그 결과, 동일한 생물학적 샘플에 대해 여러 어세이(assays)를 수행하고 미생물, 유전자 또는 대사산물 중 어떤 특징들이 서로 다른 샘플 조건과 연관되어 있는지 묻는 것이 일반적입니다. 이러한 질문들에 접근하는 방법은 여러 가지가 있습니다. 어떤 것을 적용할지는 연구의 초점에 달려 있습니다.\n\n11.5.1 9.6.1 공변동, 관성, 공동 관성 및 RV 계수\n물리학에서와 마찬가지로, 우리는 관성을 ‘가중치가 부여된’ 점들 사이의 거리 합으로 정의합니다. 이를 통해 우리는 분할표의 카운트에 대한 관성을 (카이제곱 통계량에서처럼) 관측 빈도와 예상 빈도 사이의 거리 제곱의 가중 합으로 계산할 수 있습니다.\n분산-관성의 또 다른 일반화는 유용한 계통 발생 다양성 지수(Phylogenetic diversity index)입니다 (트리를 통해 분류군 하위 집합 사이의 거리 합을 계산함). 다른 유용한 일반화에는 표준 공간 통계학에서 가져온 그래프 상의 점들의 가변성을 사용하는 것이 포함됩니다.\n만약 우리가 동일한 10개 위치에서 측정된 두 개의 표준화된 변수를 함께 연구하고 싶다면, 그들의 공분산(covariance) 을 사용합니다. (x)가 표준화된 pH를 나타내고 (y)가 표준화된 습도를 나타낸다면, 우리는 다음과 같이 평균을 사용하여 그들의 공변동(covariation)을 측정합니다:\n[ (x,y) = (x1y1 + x2y2 + x3y3 + + x10y10). ]\n만약 (x)와 (y)가 같은 방향으로 함께 변한다면, 이 수치는 커질 것입니다. 우리는 8장에서 정의한 상관 계수가 다변량 분석에 얼마나 유용했는지 보았습니다. 다중 표(multitable) 일반화도 그만큼 유용할 것입니다.\n\n\n11.5.2 9.6.2 만텔(Mantel) 계수 및 거리 상관관계 검정\n\n\n\n만텔 계수를 사용할 때는 몇 가지 주의사항이 있습니다. Guillot 및 Rousset (2013)의 비판적 리뷰를 참조하십시오.\n\n\n만텔 계수를 사용할 때는 몇 가지 주의사항이 있습니다. Guillot 및 Rousset (2013)의 비판적 리뷰를 참조하십시오.\n연관성 척도의 가장 초기 버전 중 하나인 만텔 계수는 아마도 현재 특히 생태학 분야에서 가장 인기 있는 방법이기도 할 것입니다 (Josse and Holmes 2016). (X) 및 (Y)와 관련된 두 비유사성 행렬 (D^X)와 (D^Y)가 주어졌을 때, R의 dist 함수가 하는 것처럼 이 행렬들을 벡터로 만들고 그들의 선형 상관관계를 계산합니다. 전형적인 응용 사례는 예를 들어 17개 서로 다른 위치의 토양 화학 성분으로부터 (D^X)를 계산하고, 동일한 17개 위치 사이에서 자카드 지수로 측정된 식물 발생의 비유사성을 나타내기 위해 (D^Y)를 사용하는 것입니다. 만텔 계수는 수학적으로 다음과 같이 정의됩니다:\n[ r_m(X, Y) = { }, ]\n여기서 ({d}^X) (각각 ({d}^Y))는 (d^X) (각각 (d^Y))와 관련된 비유사성 행렬의 상삼각 원소들의 평균입니다. 만텔 계수와 RV 또는 dCov와 같은 다른 계수들의 주요 차이점은 이중 중앙화(double centering)가 없다는 것입니다. 거리 행렬 내부의 의존성 때문에, 만텔 상관관계의 귀무 분포와 통계적 유의성은 일반적인 상관 계수처럼 간단하게 평가될 수 없습니다. 대신, 대개 순열 검정(permutation testing)을 통해 평가됩니다. 역사적 배경과 현대적 구현에 대한 리뷰는 Josse와 Holmes (2016)를 참조하십시오. 이 계수와 관련 검정들은 ade4 (Chessel, Dufour, and Thioulouse 2004), vegan 및 ecodist (Goslee, Urban, et al. 2007)와 같은 여러 R 패키지에 구현되어 있습니다.\n\n\n11.5.3 9.6.3 RV 계수\n두 벡터가 아닌 두 데이터 표의 유사성에 대한 전역적인 측정은, 표들 사이의 내적(inner product)에 의해 제공되는 공분산의 일반화를 통해 수행될 수 있습니다. 이는 상관 계수와 비슷하지만 표를 위한 수치인 0과 1 사이의 숫자인 RV 계수를 제공합니다.\n[ RV(A,B)= ]\nMatrixCorrelation 패키지에서 행렬 상관관계에 대한 다른 여러 척도들을 사용할 수 있습니다.\n만약 두 행렬 사이의 연결을 확인했다면, 우리는 그 연결을 이해하기 위한 방법을 찾아야 합니다. 그러한 방법 중 하나가 다음 섹션에서 설명됩니다.\n\n\n11.5.4 9.6.4 정전 상관 분석 (Canonical correlation analysis, CCA)\nCCA는 1930년대에 Hotelling에 의해 두 개의 연속형 변수 세트 (X)와 (Y) 사이의 연관성을 찾기 위해 개발된 PCA와 유사한 방법입니다. 그 목표는 두 번째 변수 세트의 선형 투영과 최대의 상관관계를 갖는 첫 번째 변수 세트의 선형 투영을 찾는 것입니다.\n표현 특이적인 세부 사항(노이즈)을 버리고 동일한 현상에 대한 두 가지 관점의 상관된 함수(공변량)를 찾음으로써, 상관관계의 원인이 되는 기저의 숨겨져 있지만 영향력 있는 요인들을 밝혀낼 것으로 기대됩니다.\n두 개의 행렬을 고려해 봅시다:\n\n(np) 행렬 (X), 그리고\n(nq) 행렬 (Y).\n\n(X)의 (p)개 열과 (Y)의 (q)개 열은 변수들에 대응하며, 행들은 동일한 (n)개의 실험 단위에 대응합니다. 행렬 (X)의 (j)번째 열을 (X_j), 마찬가지로 행렬 (Y)의 (k)번째 열을 (Y_k)로 표시합니다. 일반성을 잃지 않고 (X)와 (Y)의 열들은 표준화(평균 0, 분산 1)되었다고 가정합니다.\n고전적 CCA는 (p n) 및 (q n)이고, 행렬 (X)와 (Y)가 각각 최대 열 계수(full column rank) (p)와 (q)를 갖는다고 가정합니다. 이하에서 CCA는 반복 알고리즘을 통해 해결되는 문제로 제시됩니다. CCA의 첫 번째 단계는 다음과 같이 정의된 선형 결합 (U)와 (V) 사이의 상관관계를 최대화하는 두 벡터 (a =(a_1,…,a_p)^t)와 (b =(b_1,…,b_q)^t)를 찾는 것으로 구성됩니다:\n[\n\\[\\begin{aligned}\nU=Xa&=a_1 X_1 +a_2 X_2 +\\cdots a_p X_p\\\\\nV=Yb&=b_1 Y_1 +b_2 Y_2 +\\cdots b_q Y_q\n\\end{aligned}\\]\n]\n이때 벡터 (a)와 (b)는 ((U) = (V) = 1)이 되도록 정규화되었다고 가정합니다. 다시 말해, 문제는 다음을 만족하는 (a)와 (b)를 찾는 것입니다:\n[ 1 = (U, V) = {a,b} (Xa, Yb)(Xa)=(Yb) = 1. ]\n결과로 나오는 변수 (U)와 (V)를 첫 번째 정전 변수(canonical variates)라고 부르며 (_1)을 첫 번째 정전 상관관계(canonical correlation)라고 부릅니다.\n참고: 고차원의 정전 변수와 정전 상관관계는 단계별 문제로 찾아낼 수 있습니다. (s = 1,…,p)에 대해, 우리는 다음을 최대화함으로써 상응하는 벡터 ((a^1, b^1), …, (a^p, b^p))와 함께 양의 상관관계 (_1 _2 … _p)를 순차적으로 찾을 수 있습니다:\n[ s = (Us,Vs) = {as,bs} (Xas,Ybs)(Xa^s) = (Yb^s)=1 ]\n이때 다음과 같은 추가 제약 조건이 따릅니다:\n[ (Us,Ut) = (V^s, V^t)=0 t &lt; s p. ]\n우리는 CCA를 우리가 최대화하는 분산이 두 행렬 사이의 ’공분산’인 PCA의 일반화로 생각할 수 있습니다 (더 자세한 내용은 Holmes (2006)를 참조하십시오).\n\n\n11.5.5 9.6.5 희소 정전 상관 분석 (Sparse canonical correlation analysis, sCCA)\n각 표의 변수 수가 매우 많을 때 상관관계가 매우 높은 두 벡터를 찾는 것은 너무 쉬우며 불안정할 수 있습니다: 자유도가 너무 많기 때문입니다.\n\n\n\n우리는 12장에서 정규화의 많은 예시와 과적합(overfitting)의 위험을 보게 될 것입니다.\n\n\n우리는 12장에서 정규화의 많은 예시와 과적합(overfitting)의 위험을 보게 될 것입니다.\n그럴 때는 0이 아닌 계수의 수를 최소한으로 유지하는 페널티를 추가하는 것이 유익합니다. 이 접근 방식은 희소 정전 상관 분석(sparse CCA 또는 sCCA)이라고 불리며, 샘플 간의 탐색적 비교와 흥미로운 공변동(co-variation)을 가진 특징을 식별하는 데 모두 잘 어울리는 방법입니다. 우리는 PMA 패키지의 구현체를 사용할 것입니다.\n여기서는 Kashyap 등 (2013)에 의해 수집된 두 개의 표가 있는 데이터 세트를 연구합니다. 하나는 박테리아 풍부도의 분할표이고 다른 하나는 대사산물의 풍부도 표입니다. 12개의 샘플이 있으므로 (n = 12)입니다. 대사산물 표는 (p = 637)개 특징에 대한 측정값을 가지고 있고 박테리아 풍부도는 총 (q = 20,609)개의 OTU를 가졌는데, 이를 약 200개로 필터링할 것입니다. 먼저 데이터를 불러오는 것부터 시작합니다.\nlibrary(\"genefilter\")\nload(\"../data/microbe.rda\")\nmetab = read.csv(\"../data/metabolites.csv\", row.names = 1) |&gt; as.matrix()\n우리는 먼저 관심 있는 박테리아와 대사산물로 필터링하여, 많은 샘플에서 0인 것들을 (수동으로) 제거하고 큰 값들에 대해서는 50이라는 상한 임계값을 부여합니다. 꼬리가 긴(heavy tails) 분포를 완화하기 위해 데이터를 변환합니다.\nlibrary(\"phyloseq\")\nmetab   = metab[rowSums(metab == 0) &lt;= 3, ]\nmicrobe = prune_taxa(taxa_sums(microbe) &gt; 4, microbe)\nmicrobe = filter_taxa(microbe, filterfun(kOverA(3, 2)), TRUE)\nmetab   = log(1 + metab, base = 10)\nX       = log(1 + as.matrix(otu_table(microbe)), base = 10)\n예비 분석의 두 번째 단계는 ade4 패키지의 RV.test를 사용하여 두 행렬 사이에 어떤 연관성이 있는지 살펴보는 것입니다:\ncolnames(metab) = colnames(X)\npca1 = dudi.pca(t(metab), scal = TRUE, scann = FALSE)\npca2 = dudi.pca(t(X), scal = TRUE, scann = FALSE)\nrv1 = RV.rtest(pca1$tab, pca2$tab, 999)\nrv1\n\n\nMonte-Carlo test\nCall: RV.rtest(df1 = pca1$tab, df2 = pca2$tab, nrepet = 999)\n\nObservation: 0.8400429 \n\nBased on 999 replicates\nSimulated p-value: 0.002 \nAlternative hypothesis: greater \n\n    Std.Obs Expectation    Variance \n6.231661953 0.314166070 0.007121318 \n우리는 이제 희소 CCA를 적용할 수 있습니다. 이 방법은 샘플보다 측정된 특징이 더 많을 수 있는 고차원 데이터 표 전반에서 특징 세트들을 비교합니다. 이 과정에서 가장 많은 공분산을 포착하는 가용한 특징들의 하위 집합을 선택합니다 — 이들은 여러 표 전반에 존재하는 신호를 반영하는 특징들입니다. 그런 다음 선택된 특징들의 하위 집합에 PCA를 적용합니다. 이런 의미에서, 우리는 희소 CCA를 서열화 방법이라기보다는 스크리닝 절차로 사용합니다.\n구현은 아래와 같습니다. penaltyx와 penaltyz 매개변수는 희소성 페널티입니다. penaltyx 값이 작을수록 더 적은 수의 미생물이 선택되며, 마찬가지로 penaltyz는 선택되는 대사산물의 수를 조절합니다. 우리는 후속 해석을 용이하게 하기 위해 이를 수동으로 조정합니다 — 우리는 일반적으로 기본 매개변수가 제공하는 것보다 더 높은 희소성을 선호합니다.\nlibrary(\"PMA\")\nccaRes = CCA(t(X), t(metab), penaltyx = 0.15, penaltyz = 0.15, \n             typex = \"standard\", typez = \"standard\")\n\n\n123456789\n\n\nccaRes\n\n\nCall: CCA(x = t(X), z = t(metab), typex = \"standard\", typez = \"standard\", \n    penaltyx = 0.15, penaltyz = 0.15)\n\n\nNum non-zeros u's:  5 \nNum non-zeros v's:  16 \nType of x:  standard \nType of z:  standard \nPenalty for x: L1 bound is  0.15 \nPenalty for z: L1 bound is  0.15 \nCor(Xu,Zv):  0.9904707\n이 매개변수들을 사용하면, 표들 사이의 공변동을 설명하는 능력을 바탕으로 5개의 박테리아와 16개의 대사산물이 선택되었습니다. 또한, 이 특징들은 두 표 사이에서 0.99의 상관관계를 나타냅니다. 우리는 이를 미생물 데이터와 대사체 데이터가 유사한 기저 신호를 반영하며, 이러한 신호들이 선택된 특징들에 의해 잘 근사될 수 있음을 의미하는 것으로 해석합니다. 하지만 상관관계 값에 대해서는 주의해야 합니다. 점수들이 일반적인 이변량 정규 분포 구름과는 거리가 멀기 때문입니다. 또한, 다른 특징 하위 집합들도 데이터를 그만큼 잘 설명할 수 있다는 점에 유의하세요 — 희소 CCA는 특징들 간의 중복성을 최소화했지만, 이들이 어떤 의미에서든 “진정한” 특징임을 보장하지는 않습니다.\n그럼에도 불구하고, 우리는 여전히 이 21개 특징을 사용하여 큰 손실 없이 두 표의 정보를 압축할 수 있습니다. 회복된 대사산물과 OTU를 그것들이 측정된 샘플의 특성과 연관시키기 위해, 우리는 그것들을 일반적인 PCA의 입력으로 사용합니다. 그림 9.32를 생성하는 데 사용한 코드는 생략했습니다. 독자들께서는 책과 함께 제공되는 온라인 자료나 Callahan 등 (2016)에 발표된 워크플로를 참조하시기 바랍니다.\n그림 9.32는 PCA 트리플롯(triplot) 을 보여주는데, 여기에는 서로 다른 유형의 샘플들과 다중 도메인 특징(대사산물 및 OTU)이 표시됩니다. 이를 통해 측정된 샘플들 간의 비교 — 결손(knockout)은 삼각형으로, 야생형(wild type)은 원으로 표시 — 가 가능하며, 서로 다른 특징들의 영향 — 텍스트 레이블이 있는 다이아몬드 — 을 특징짓습니다. 예를 들어, 데이터의 주요 변동은 서로 다른 식단에 해당하는 PD 및 ST 샘플들 사이에서 나타나는 것을 볼 수 있습니다. 또한, 특징들 중 15개의 큰 값은 ST 상태와 연관되어 있고, 5개의 작은 값은 PD 상태를 나타냅니다.\n\n그림 9.32: 여러 데이터 유형(대사산물 및 OTU)으로부터 CCA로 선택된 특징들로 생성된 PCA 트리플롯.\n희소 CCA 스크리닝의 장점은 이제 분명합니다 — 우리는 상대적으로 단순한 플롯을 사용하여 샘플들 전반의 변동 대부분을 표시할 수 있으며, 모든 특징을 표시하는 데 필요했을 수백 개의 추가 점들을 플롯하는 것을 피할 수 있습니다.\n\n\n11.5.6 9.6.6 정전(또는 제약된) 대응 분석 (CCpnA)\n\n\n\nCCA에 대한 명칭 중복: 원래 Braak(1985)에 의해 발명되었고 정전 대응 분석(Canonical Correspondence analysis)이라 불렸으나, 우리는 정전 상관 분석(Canonical Correlation Analysis, CCA)과의 혼동을 피하기 위해 이 방법을 제약된 대응 분석(Constrained Correspondence Analysis)이라 부르고 CCpnA로 약칭하겠습니다. 하지만 ade4나 vegan과 같은 여러 R 패키지들은 그들의 대응 분석 함수로 cca라는 이름을 사용합니다.\n\n\nCCA에 대한 명칭 중복 : 원래 Braak (1985)에 의해 발명되었고 정전 대응 분석(Canonical Correspondence analysis)이라 불렸으나, 우리는 정전 상관 분석(Canonical Correlation Analysis, CCA)과의 혼동을 피하기 위해 이 방법을 제약된 대응 분석(Constrained Correspondence Analysis) 이라 부르고 CCpnA로 약칭하겠습니다. 하지만 ade4 나 vegan 과 같은 여러 R 패키지들은 그들의 대응 분석 함수로 cca라는 이름을 사용합니다.\n제약된 대응 분석이라는 용어는 이 방법이 제약된 회귀(constrained regression)와 유사하다는 사실을 나타냅니다. 이 방법은 잠재 변수들이 ’설명 변수’로 제공된 환경 변수들과 상관관계를 갖도록 강제합니다.\nCCpnA는 샘플의 위치가 종의 시그니처와 환경적 특성 모두의 유사성에 의해 결정되는 바이플롯을 생성합니다. 대조적으로, 주성분 분석이나 대응 분석은 오직 종의 시그니처만을 살펴봅니다. 더 형식적으로 말하자면, 결과로 나오는 CCpnA 방향들이 환경 변수들이 생성하는 공간(span) 내에 놓이도록 보장합니다. 철저한 설명은 Braak (1985; Greenacre 2007)을 참조하십시오.\n이 방법은 phyloseq 패키지의 ordinate 함수를 사용하여 실행할 수 있습니다. 샘플 데이터의 공변량을 사용하기 위해, 어떤 특징들을 고려할지 지정하는 추가 인수를 제공합니다.\n여기서는 4장에서 dada2 를 사용하여 노이즈를 제거했던 데이터를 사용합니다. phyloseq 객체를 생성하는 자세한 방법은 10장에서 다룰 것입니다. 당분간은 서로 다른 분류군에 대한 카운트의 분할표를 포함하는 otu_table 구성 요소를 사용합니다. 우리는 나이와 가족 관계(두 변수 모두 ps1 객체의 sample_data 슬롯에 포함됨)로 분류군 풍부도를 설명하는 제약된 대응 분석을 수행하고자 합니다.\n우리는 가장 풍부한 상위 4개 분류군만을 사용하여 (바이플롯을 읽기 쉽게 만들기 위해) 2차원 플롯을 만들고자 합니다:\nps1=readRDS(\"../data/ps1.rds\")\nps1p=filter_taxa(ps1, function(x) sum(x) &gt; 0, TRUE)\npsCCpnA = ordinate(ps1p, \"CCA\",\n                 formula = ps1p ~ ageBin + family_relationship)\n바이플롯을 위한 위치 정보에 접근하기 위해 vegan 패키지의 scores 함수를 사용할 수 있습니다. 또한 그림 주석을 용이하게 하기 위해 사이트 점수(site scores)를 sample_data 슬롯에 있는 환경 데이터와 결합합니다. 총 23개의 목(orders) 중에서 가장 풍부한 4개만 명시적으로 주석을 답니다 — 이는 바이플롯을 읽기 쉽게 만듭니다.\nevalProp = 100 * psCCpnA$CCA$eig[1:2] / sum(psCCpnA$CA$eig)\nggplot() +\n geom_point(data = sites,aes(x =CCA2, y =CCA1),shape =2,alpha=0.5) +\n geom_point(species,aes(x =CCA2,y =CCA1,col = Order),size=1)+\n geom_text_repel(data = dplyr::filter(species, CCA2 &lt; (-2)),\n                   aes(x = CCA2, y = CCA1, label = otu_id),\n                   size = 2, segment.size = 0.1) +\n facet_grid(. ~ ageBin) +\n guides(col = guide_legend(override.aes = list(size = 2))) +\n labs(x = sprintf(\"Axis2 [%s%% variance]\", round(evalProp[2])),\n      y = sprintf(\"Axis1 [%s%% variance]\", round(evalProp[1]))) +\n scale_color_brewer(palette = \"Set1\") + theme(legend.position=\"bottom\")\n\n그림 9.33: CCpnA에 의해 생성된 생쥐 및 분류군 점수. 사이트(생쥐 샘플)는 삼각형이고, 종은 원으로 각각 표시됩니다. 별도의 패널은 서로 다른 연령 그룹을 나타냅니다.\n질문 9.19\n이 책의 온라인 리소스에서 tax 및 species 객체를 생성하는 추가 코드를 찾아보세요. 그런 다음 나이 대신 집단(litter)을 패싯 변수로 사용하여 그림 9.33의 대응물을 만들어 보세요.\n해결책\n\n그림 9.34: 나이가 아닌 집단 멤버십에 따라 패싯을 나눈, 그림 9.33의 대응물.\n그림 9.33과 9.34는 각각 나이와 집단 멤버십에 따라 사이트를 나누어 주석을 단 점수들의 플롯을 보여줍니다. 패싯이 있는 상황에서 적절한 종횡비를 유지하기 위해 수직축을 첫 번째 정전 성분(canonical component)으로 잡았습니다. 두 번째 CCpnA 방향을 따라 이상치인 개별 박테리아들에 레이블을 달았습니다.\n분명히 첫 번째 CCpnA 방향은 두 주요 연령대의 생쥐를 구분합니다. 바이플롯의 왼쪽과 오른쪽에 있는 원들은 각각 어린 생쥐와 나이 든 생쥐의 특징인 박테리아를 나타냅니다. 두 번째 CCpnA 방향은 가장 나이 많은 그룹의 소수 생쥐들을 분리하며, 또한 두 집단을 부분적으로 구분합니다. 두 번째 CCpnA 방향에서 낮은 값을 가진 이러한 샘플들은 다른 샘플들보다 이상치 박테리아를 더 많이 가지고 있습니다.\n이 CCpnA 분석은 서로 다른 생쥐 마이크로바이옴 커뮤니티 간의 주요 차이가 나이 축을 따라 존재한다는 결론을 뒷받침합니다. 하지만 환경 변수의 영향이 그렇게 강하지 않은 상황에서는 CCA가 그러한 연관성을 탐지하는 데 더 큰 검정력을 가질 수 있습니다. 일반적으로 보충 데이터를 통합하는 것이 바람직할 때마다 이를 적용할 수 있지만, (1) 지도 방식보다 덜 공격적이고, (2) 여러 환경 변수를 한꺼번에 사용할 수 있는 방식으로 적용됩니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>이질적 데이터의 다변량 분석 (Multivariate Analysis for Heterogeneous Data)</span>"
    ]
  },
  {
    "objectID": "09-chap.html#이-장의-요약",
    "href": "09-chap.html#이-장의-요약",
    "title": "11  이질적 데이터의 다변량 분석 (Multivariate Analysis for Heterogeneous Data)",
    "section": "11.6 9.7 이 장의 요약",
    "text": "11.6 9.7 이 장의 요약\n불균질 데이터 많은 연속형 변수와 소수의 범주형 변수가 섞여 있는 경우, 범주형 변수를 PCA에 보조 정보로 추가하여 처리할 수 있습니다. 이는 그룹 내 모든 점의 평균을 맵에 투영함으로써 수행됩니다.\n거리 사용 데이터 객체 간의 관계는 종종 점 간 거리(트리, 이미지, 그래프 또는 기타 복잡한 객체 사이의 거리)로 요약될 수 있습니다.\n서열화(Ordination) 이러한 거리에 대한 유용한 표현은 PCA와 유사한 다차원 척도법(MDS), 즉 PCoA(주좌표 분석)라 불리는 방법을 통해 가능합니다. 이러한 분석의 결과를 잠재 변수를 찾아내는 것으로 생각하면 도움이 될 수 있습니다. 군집화의 경우 잠재 변수가 범주형인 반면, 서열화에서는 시간이나 물까지의 거리와 같은 환경적 그래디언트와 같은 잠재 변수입니다. 이것이 이러한 방법들을 흔히 서열화라고 부르는 이유입니다.\n강건한 버전들 은 점 간 거리가 매우 다를 때 사용될 수 있습니다. NMDS(비계량 다차원 척도법)는 점 간 거리의 순서가 가능한 한 가깝게 유지되도록 하는 좌표를 생성하는 것을 목표로 합니다.\n대응 분석 : 범주형 데이터의 의존성을 설명하는 낮은 차원의 투영을 계산하는 방법입니다. PCA가 분산을 분해하는 것과 거의 같은 방식으로 카이제곱 거리를 분해합니다. 대응 분석은 대개 유의미한 카이제곱 검정에 대한 후속 조치를 취하는 가장 좋은 방법입니다. 카테고리의 서로 다른 수준들 사이에 유의미한 의존성이 있음을 확인했다면, 이를 매핑하고 플롯과 바이플롯을 사용하여 이 맵 상의 근접성을 해석할 수 있습니다.\n거리에 대한 순열 검정 동일한 점들 사이의 두 가지 거리 세트가 주어졌을 때, 만텔(Mantel) 순열 검정을 사용하여 그들이 서로 관련이 있는지 측정할 수 있습니다.\n분산과 공분산의 일반화 동일한 데이터에 대해 두 개 이상의 측정 행렬을 다룰 때, 공분산과 상관관계의 개념을 공동 관성(co-inertia)의 벡터 측정으로 일반화할 수 있습니다.\n정전 상관 은 각 표에서 가능한 한 상관관계가 높은 소수의 선형 변수 결합을 찾는 방법입니다. 변수 수가 많은 행렬에 이 방법을 사용할 때는 0이 아닌 계수의 수를 줄이는 L1 페널티를 사용하는 정규화된 버전을 사용합니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>이질적 데이터의 다변량 분석 (Multivariate Analysis for Heterogeneous Data)</span>"
    ]
  },
  {
    "objectID": "09-chap.html#더-읽을거리",
    "href": "09-chap.html#더-읽을거리",
    "title": "11  이질적 데이터의 다변량 분석 (Multivariate Analysis for Heterogeneous Data)",
    "section": "11.7 9.8 더 읽을거리",
    "text": "11.7 9.8 더 읽을거리\nPCoA 맵과 비선형 임베딩의 해석은 보충 점 방법의 일반화를 사용하여 PCA에서 했던 것과 같은 방식으로 강화될 수 있습니다. Trosset과 Priebe (2008) 또는 Bengio 등 (2004)을 참조하십시오. 우리는 7장에서 하나의 범주형 변수를 PCA에 투영하는 방법을 보았습니다. 대응 분석 프레임워크는 사실 여러 범주형 변수를 임의의 수의 연속형 변수와 혼합할 수 있게 해줍니다. 이는 다중 대응 분석(MCA)이라는 확장을 통해 수행되는데, 이를 통해 수많은 이진 범주형 변수에 대해 동일한 분석을 수행하고 유용한 맵을 얻을 수 있습니다. 여기서의 비결은 연속형 변수들을 먼저 범주형 변수들로 바꾸는 것입니다. R을 사용한 광범위한 예제는 예를 들어 Pagès (2016)의 책을 참조하십시오.\n고유벡터에 의해 정의된 주방향 대신 비선형 주성분 곡선 추정을 가능하게 하는 PCA의 단순한 확장이 Hastie와 Stuetzle (1989)에 의해 제안되었으며, princurve 패키지에서 사용할 수 있습니다.\n1보다 높은 차원에서 고밀도 데이터를 포함하는 곡면 하위 공간을 찾는 것은 현재 매니폴드 임베딩(manifold embedding)이라고 불리며, 라플라시안 고유맵(Laplacian eigenmaps, Belkin and Niyogi 2003), Roweis와 Saul (2000)의 국소 선형 임베딩, 또는 isomap 방법(Tenenbaum, De Silva, and Langford 2000)을 통해 수행될 수 있습니다. 비선형 비지도 학습 방법을 다루는 교과서로는 Hastie, Tibshirani, Friedman (2008, 14장) 또는 Izenman (2008)을 참조하십시오.\n많은 다중 표 상관 계수에 대한 리뷰와 응용 분석은 Josse와 Holmes (2016)에서 찾아볼 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>이질적 데이터의 다변량 분석 (Multivariate Analysis for Heterogeneous Data)</span>"
    ]
  },
  {
    "objectID": "09-chap.html#연습-문제",
    "href": "09-chap.html#연습-문제",
    "title": "11  이질적 데이터의 다변량 분석 (Multivariate Analysis for Heterogeneous Data)",
    "section": "11.8 9.9 연습 문제",
    "text": "11.8 9.9 연습 문제\n연습 문제 9.1\nPhylochip 데이터를 다시 한 번 살펴보겠습니다. 원래의 발현 값을 존재/부재(presence/absence)로 대체합니다. 최소 8개 샘플에서 적어도 8.633의 값을 가진 것들만 남기도록 데이터를 임계값 처리합니다7.\nibd.pres = ifelse(assayIBD[, 1:28] &gt; 8.633, 1, 0)\n이 이진 데이터에 대해 대응 분석을 수행하고 그림 9.15에서 보았던 것과 얻은 플롯을 비교해 보세요.\n7 이 값들은 이전의 임계값 선택과 유사하게 약 3,000개의 분류군을 유지하도록 선택되었습니다.\n해결책\n그림 9.35를 참조하세요.\nIBDca = dudi.coa(ibd.pres, scannf = FALSE, nf = 4)\nfviz_eig(IBDca, geom = \"bar\", bar_width = 0.7) +\n    ylab(\"Percentage of chisquare\") + ggtitle(\"\")\nfviz(IBDca, element = \"col\", axes = c(1, 2), geom = \"point\",\n     habillage = day, palette = \"Dark2\", addEllipses = TRUE, color = day,\n     ellipse.type = \"convex\", alpha = 1, col.row.sup =  \"blue\",\n     select = list(name = NULL, cos2 = NULL, contrib = NULL),\n     repel = TRUE)\n\n\n()\n\n\n\n()\n\n그림 9.35: 이진 데이터에 대한 대응 분석.\n연습 문제 9.2\n색상 연관성 표에 대한 대응 분석:\n여기에 단어 쌍의 쿼리로 인한 Google 히트 수를 조사하여 수집된 데이터 예시가 있습니다. 표 9.4의 수치에는 1000을 곱해야 합니다. 예를 들어, “quiet”와 “blue” 단어의 조합은 2,150,000개의 히트를 반환했습니다.\n표 9.4: 검색 엔진 결과로부터 얻은 공동 발생 용어들의 분할표.\n\n\n\nblack\nblue\ngreen\ngrey\norange\npurple\nwhite\n\n\n\n\n\nquiet\n2770\n2150\n2140\n875\n1220\n821\n2510\n\n\nangry\n2970\n1530\n1740\n752\n1040\n710\n1730\n\n\nclever\n1650\n1270\n1320\n495\n693\n416\n1420\n\n\ndepressed\n1480\n957\n983\n147\n330\n102\n1270\n\n\nhappy\n19300\n8310\n8730\n1920\n4220\n2610\n9150\n\n\nlively\n1840\n1250\n1350\n659\n621\n488\n1480\n\n\nperplexed\n110\n71\n80\n19\n23\n15\n109\n\n\nvirtuous\n179\n80\n102\n20\n25\n17\n165\n\n\n\n이 데이터에 대해 대응 분석을 수행하세요. 2차원 바이플롯을 보았을 때 무엇을 알 수 있나요?\n해결책\n그림 9.36을 참조하세요. 코드는 여기에는 렌더링되지 않았지만, 문서의 소스 파일에 나와 있습니다.\n\n그림 9.36: 대응 분석은 표 9.4와 같은 공동 발생 분할표에 대해 색상과 감정이라는 두 범주형 변수의 대칭적인 그래픽 표현을 가능하게 합니다.\n\n연습 문제 9.3\n플라톤이 그의 다양한 책들을 쓴 날짜는 알려져 있지 않습니다. 우리는 문장 끝맺음을 가져와 그 패턴 빈도를 데이터로 사용합니다.\nplatof = read.table(\"../data/platof.txt\", header = TRUE)\nplatof[1:4, ]\n\n\n      Rep Laws Crit Phil Pol Soph Tim\nuuuuu  42   91    5   24  13   26  18\n-uuuu  60  144    3   27  19   33  30\nu-uuu  64   72    3   20  24   31  46\nuu-uu  72   98    2   25  20   24  14\n\n\nresPlato = dudi.coa(platof, scannf = FALSE, nf = 2)\nfviz_ca_biplot(resPlato, axes=c(2, 1)) + ggtitle(\"\")\nfviz_eig(resPlato, geom = \"bar\", width = 0.6) + ggtitle(\"\")\n그림 9.37: 플라톤 문장 끝맺음의 바이플롯.\n\n그림 9.37의 바이플롯으로부터 플라톤 저작들의 연대순을 추측할 수 있나요?\n힌트: 가장 먼저 쓰인 것(earliest)은 Republica 임이 알려져 있습니다. 가장 마지막 것(latest)은 Laws 입니다.\n플라톤이 생애 초기에 더 자주 사용한 문장 끝맺음은 무엇인가요?\n그림 9.37의 지도에 의해 설명되는 관성 ((^2)-거리)의 백분율은 얼마인가요?\n\n\n\n\n\n\n\n\n\n해결책\n처음 두 축에 의해 설명되는 관성의 백분율을 계산하기 위해, 고윳값들의 누적 합을 2번째 값에서 구합니다:\nnames(resPlato)\n\n\n [1] \"tab\"  \"cw\"   \"lw\"   \"eig\"  \"rank\" \"nf\"   \"c1\"   \"li\"   \"co\"   \"l1\"  \n[11] \"call\" \"N\"   \n\n\nsum(resPlato$eig)\n\n\n[1] 0.132618\n\n\npercentageInertia=round(100*cumsum(resPlato$eig)/sum(resPlato$eig))\npercentageInertia\n\n\n[1]  69  85  92  96  98 100\n\n\npercentageInertia[2]\n\n\n[1] 85\n연습 문제 9.4\n우리는 두 개의 데이터 세트를 살펴볼 것인데, 하나는 다른 하나의 섭동된(perturbed) 버전이며 둘 다 생태학적 데이터에서 흔히 볼 수 있는 그래디언트를 보여줍니다. lakes.RData 객체로 저장된 두 개의 종 카운트 행렬 lakelike와 lakelikeh를 읽어 들이세요. 두 데이터 세트 각각에 대해 대응 분석과 주성분 분석의 출력을 비교해 보세요; 두 개 차원으로 제한하세요. 플롯과 고윳값에서 무엇을 발견했나요?\n해결책\nload(\"../data/lakes.RData\")\nlakelike[ 1:3, 1:8]\n\n\n     plant1 plant2 plant3 plant4 plant5 plant6 plant7 plant8\nloc1      6      4      0      3      0      0      0      0\nloc2      4      5      5      3      4      2      0      0\nloc3      3      4      7      4      5      2      1      1\n\n\nlakelikeh[1:3, 1:8]\n\n\n     plant1 plant2 plant3 plant4 plant5 plant6 plant7 plant8\nloc1      6      4      0      3      0      0      0      0\nloc2      4      5      5      3      4      2      0      0\nloc3      3      4      7      4      5      2      1      1\n\n\ne_coa  = dudi.coa(lakelike,  scannf = FALSE, nf = 2)\ne_pca  = dudi.pca(lakelike,  scannf = FALSE, nf = 2)\neh_coa = dudi.coa(lakelikeh, scannf = FALSE, nf = 2)\neh_pca = dudi.pca(lakelikeh, scannf = FALSE, nf = 2)\n비교 (출력은 표시되지 않음):\nscatter(e_pca)\n\n\n scatter(e_coa)\n\n\n s.label(e_pca$li)\n\n\n s.label(e_coa$li)\n\n\n s.label(eh_pca$co)\n\n\n s.label(eh_pca$li)\n\n\n s.label(eh_coa$li)\n\n\n s.label(eh_coa$co)\n연습 문제 9.5\n우리는 9.5.1절에서 정규화된 Moignard 데이터를 분석했습니다. 이제 원시(raw) 데이터(nbt.3154-S3-raw.csv 파일에 있음)로 분석을 다시 수행하고 그 출력을 정규화된 값을 사용했을 때 얻은 것과 비교해 보세요.\n해결책\nmoignard_raw = as.matrix(read.csv(\"../data/nbt.3154-S3-raw.csv\", row.names = 1))\ndist2r.euclid = dist(moignard_raw)\ndist1r.l1     = dist(moignard_raw, \"manhattan\")\ncells1.cmds = cmdscale(dist1r.l1,     k = 20, eig = TRUE)\ncells2.cmds = cmdscale(dist2r.euclid, k = 20, eig = TRUE)\nsum(cells1.cmds$eig[1:2]) / sum(cells1.cmds$eig)\n\n\n[1] 0.776075\n\n\nsum(cells2.cmds$eig[1:2]) / sum(cells2.cmds$eig)\n\n\n[1] 0.6297133\n연습 문제 9.6\n우리는 커널 방법의 사용을 탐구할 것입니다.\n\nkernlab 을 사용하여 Moignard 데이터에 대해 커널 정의에서의 시그마(sigma) 튜닝 매개변수의 다양한 값을 사용하여 커널화된 거리(kernelized distances)를 계산하세요. 그런 다음 이 커널화된 거리들에 대해 MDS를 수행하세요. 커널 다차원 척도법의 처음 네 성분에 의해 설명되는 가변성에 어떤 차이가 있나요?\n성분들의 대화형 3차원 표현을 만드세요: 보라색 점들을 위한 분기(branch)가 보이는 투영이 있나요?\n\n해결책\n\n커널화된 거리들\n\nlibrary(\"kernlab\")\nlaplacedot1 = laplacedot(sigma = 1/3934)\nrbfdot1     = rbfdot(sigma = (1/3934)^2 )\nKlaplace_cellsn   = kernelMatrix(laplacedot1, blom)\nKGauss_cellsn     = kernelMatrix(rbfdot1, blom)\nKlaplace_rawcells = kernelMatrix(laplacedot1, moignard_raw)\nKGauss_rawcells   = kernelMatrix(rbfdot1, moignard_raw)\n이상치로부터 보호하고 비선형 성분의 발견을 가능하게 하기 위해 커널화된 거리들을 사용합니다.\ndist1kr = 1 - Klaplace_rawcells\ndist2kr = 1 - KGauss_rawcells\ndist1kn = 1 - Klaplace_cellsn\ndist2kn = 1 - KGauss_cellsn\n\ncells1.kcmds = cmdscale(dist1kr, k = 20, eig = TRUE) \ncells2.kcmds = cmdscale(dist2kr, k = 20, eig = TRUE) \n\npercentage = function(x, n = 4) round(100 * sum(x[seq_len(n)]) / sum(x[x&gt;0]))\nkperc1 = percentage(cells1.kcmds$eig)\nkperc2 = percentage(cells2.kcmds$eig)\n\ncellsn1.kcmds = cmdscale(dist1kn, k = 20, eig = TRUE) \ncellsn2.kcmds = cmdscale(dist2kn, k = 20, eig = TRUE)\n\n3D 산점도를 대화형으로 사용하기:\n\ncolc = rowData(Moignard)$cellcol\nlibrary(\"scatterplot3d\")\nscatterplot3d(cellsn2.kcmds$points[, 1:3], color=colc, pch = 20,\n   xlab = \"Axis k1\", ylab = \"Axis k2\", zlab = \"Axis k3\", angle=15)\nscatterplot3d(cellsn2.kcmds$points[, 1:3], color=colc, pch = 20,\n   xlab = \"Axis k1\", ylab = \"Axis k2\", zlab = \"Axis k3\", angle = -70)\n\n\n()\n\n\n\n()\n\n그림 9.38: 커널 다차원 척도법.\n연습 문제 9.7\n세포 데이터의 고해상도 연구.\n9.5.1절에서 생성한 원래의 발현 데이터 blom을 가져오세요. 가장 가변적인 상위 10개 유전자 각각의 발현 강도를 확산 맵(diffusion mapping)으로 만든 3D 플롯 위에 매핑하세요. 어느 차원, 또는 어느 주좌표 (1,2,3,4)가 4SG (빨간색) 점들을 가장 많이 클러스터링하는 것으로 보이나요?\n해결책\nlibrary(\"rgl\")\nplot3d(cellsn2.kcmds$points[, 1:3], col = colc, size = 3,\n       xlab = \"Axis1\", ylab = \"Axis2\", zlab = \"Axis3\")\nplot3d(cellsn2.kcmds$points[, c(1,2,4)], col = colc, size = 3,\n       xlab = \"Axis1\", ylab = \"Axis2\", zlab = \"Axis4\")\n# L1 거리를 대신 사용하기.\nplot3d(cellsn1.kcmds$points[, 1:3], col = colc, size = 3,\n       xlab = \"Axis1\", ylab = \"Axis2\", zlab = \"Axis3\")\nplot3d(cellsn1.kcmds$points[, c(1,2,4)], col = colc, size = 3,\n       xlab = \"Axis1\", ylab = \"Axis2\", zlab = \"Axis4\")\nLPCM 패키지 에 구현된 lpc 함수는 주성분 곡선을 추정합니다. 여기서 우리는 확산 맵의 출력으로부터 선택된 세 개 차원으로 우리 자신을 제한하고 평활화된 곡선들을 생성합니다.\nlibrary(\"LPCM\")\nlibrary(\"diffusionMap\")\ndmap1 = diffuse(dist1n.l1, neigen = 10)\n\n\nPerforming eigendecomposition\nComputing Diffusion Coordinates\nElapsed time: 5.014 seconds\n\n\ncombs = combn(4, 3)\nlpcplots = apply(combs, 2, function(j) lpc(dmap1$X[, j], scale = FALSE))\n평활화된 데이터가 우리에게 무엇을 보여주는지 감을 잡기 위해, rgl 패키지의 plot3d 함수를 사용하여 대화형 그래픽을 살펴봅니다.\nlibrary(\"rgl\")\nfor (i in seq_along(lpcplots))\n  plot(lpcplots[[i]], type = \"l\", lwd = 3,\n  xlab = paste(\"Axis\", combs[1, i]),\n  ylab = paste(\"Axis\", combs[2, i]),\n  zlab = paste(\"Axis\", combs[3, i]))\n평활화된 선과 데이터 포인트들을 함께 플로팅하는 한 가지 방법은 plot3d 함수를 사용하여 선을 추가하는 것입니다.\noutlpce134 = lpc(dmap1$X[,c(1,3,4)], scale=FALSE, h=0.5)\nplot3d(dmap1$X[,c(1,3,4)], col=colc, pch=20, \n       xlab=\"Axis1\", ylab=\"Axis3\", zlab=\"Axis4\")\nplot3d(outlpce134$LPC, type=\"l\", lwd=7, add=TRUE)\n\noutlpce134 = lpc(dmap1$X[,c(1,3,4)], scale=FALSE, h=0.7)\nplot3d(outlpce134$LPC, type=\"l\", lwd=7,\n       xlab=\"Axis1\", ylab=\"Axis3\", zlab=\"Axis4\")\nplot3d(dmap1$X[,c(1,3,4)], col=colc, \n       xlab=\"\", ylab=\"\", zlab=\"\", add=TRUE)\n\n그림 9.39: 축 1, 3, 4에 대한 확산 맵 투영. 하단 그림은 발달 과정에서 세포들이 따르는 평활화된 경로를 보여줍니다.\n\n그림 9.40: 축 1, 3, 4에 대한 확산 맵 투영. 하단 그림은 발달 과정에서 세포들이 따르는 평활화된 경로를 보여줍니다.\n연습 문제 9.8\n여기서 우리는 그림 9.41에서와 같이 세포 발달 궤적을 보여줄 수 있는 더 정교한 거리와 확산 맵을 탐구합니다.\n확산 맵(diffusion map) 방법은 거리의 추정을 국소적인 점들로 제한하며, 종종 국소적인 거리들만이 정확하게 표현되어야 하고 점들이 서로 멀어질수록 동일한 ’기준’으로 측정되지 않는다는 아이디어를 더 추구합니다. 이 방법 역시 거리를 입력으로 사용하지만, 유사성의 지표로서 국소적인 확률적 전이(local probabilistic transitions)를 생성하며, 이들은 친화도 행렬(affinity matrix)로 결합되어 표준 MDS에서와 같이 고윳값과 고유벡터가 계산됩니다.\ndiffusionMap 패키지의 diffuse 함수 출력을, 9.5.1절에서 사용 가능한 세포들 사이에서 계산된 l1 및 l2 거리(dist2n.euclid 및 dist1n.l1 객체) 모두에 대해 비교하세요.\n\n그림 9.41: 3차원 확산 맵 투영으로부터의 출력.\n해결책\nlibrary(\"diffusionMap\")\ndmap2 = diffuse(dist2n.euclid, neigen = 11)\n\n\nPerforming eigendecomposition\nComputing Diffusion Coordinates\nElapsed time: 7.243 seconds\n\n\ndmap1 = diffuse(dist1n.l1, neigen = 11)\n\n\nPerforming eigendecomposition\nComputing Diffusion Coordinates\nElapsed time: 4.849 seconds\n\n\nplot(dmap2)\ndmap 객체에 대한 바닐라(vanilla) 플롯은 색상의 사용을 허용하지 않음에 유의하세요. 이것이 세포 발달을 이해하는 데 필수적이므로, 우리는 수동으로 색상을 추가합니다. 물론, 여기서는 정적인 3D 플롯을 사용하지만 이들은 우리가 코드에서 제공하는 plot3d 예제들로 보완되어야 합니다.\n우리는 맞춤형 래퍼 함수 scp3d를 사용하여 관련 매개변수들을 쉽게 삽입할 수 있도록 합니다:\nlibrary(\"scatterplot3d\")\nscp3d = function(axestop = 1:3, dmapRes = dmap1, color = colc,\n           anglea = 20, pch = 20)\nscatterplot3d(dmapRes$X[, axestop], color = colc,\n    xlab = paste(\"Axis\",axestop[1]), ylab = paste(\"Axis\", axestop[2]),\n    zlab = paste(\"Axis\",axestop[3]), pch = pch, angle = anglea)\n\n\nscp3d()\nscp3d(anglea=310)\nscp3d(anglea=210)\nscp3d(anglea=150)\n데이터를 시각화하는 가장 좋은 방법은 rgl 패키지를 사용하여 회전 가능한 대화형 플롯을 만드는 것입니다.\n# 대화형 플롯\nlibrary(\"rgl\")\nplot3d(dmap1$X[,1:3], col=colc, size=3)\nplot3d(dmap1$X[,2:4], col=colc, size=3)\nBelkin, Mikhail, and Partha Niyogi. 2003. “Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.” Neural Computation 15 (6): 1373–96.\nBengio, Yoshua, Jean-François Paiement, Pascal Vincent, Olivier Delalleau, Nicolas Le Roux, and Marie Ouimet. 2004. “Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering.” Advances in Neural Information Processing Systems 16: 177–84.\nBraak, Cajo ter. 1985. “Correspondence Analysis of Incidence and Abundance Data: Properties in Terms of a Unimodal Respose.” Biometrics 41 (January).\nBrodie, Eoin L, Todd Z DeSantis, Dominique C Joyner, Seung M Baek, Joern T Larsen, Gary L Andersen, Terry C Hazen, et al. 2006. “Application of a High- Density Oligonucleotide Microarray Approach to Study Bacterial Population Dynamics During Uranium Reduction and Reoxidation.” Applied and Environmental Microbiology 72 (9): 6288–98.\nCallahan, Ben J, Kris Sankaran, Julia A Fukuyama, Paul J McMurdie, and Susan P Holmes. 2016. “Bioconductor Workflow for Microbiome Data Analysis: From Raw Reads to Community Analyses.” F1000Research 5.\nChessel, Daniel, Anne Dufour, and Jean Thioulouse. 2004. “The ade4 Package - i: One-Table Methods.” R News 4 (1): 5–10. http://CRAN.R-project.org/doc/Rnews/.\nDiaconis, Persi, Sharad Goel, and Susan Holmes. 2008. “Horseshoes in Multidimensional Scaling and Kernel Methods.” Annals of Applied Statistics 2: 777. https://doi.org/DOI:10.1214/08-AOAS165.\nEkman, Gosta. 1954. “Dimensions of Color Vision.” The Journal of Psychology 38 (2): 467–74.\nGoslee, Sarah C, Dean L Urban, et al. 2007. “The Ecodist Package for Dissimilarity-Based Analysis of Ecological Data.” Journal of Statistical Software 22 (7): 1–19.\nGreenacre, Michael J. 2007. Correspondence Analysis in Practice. Chapman & Hall.\nGuillot, Gilles, and François Rousset. 2013. “Dismantling the Mantel Tests.” Methods in Ecology and Evolution 4 (4): 336–44.\nHastie, Trevor, and Werner Stuetzle. 1989. “Principal Curves.” Journal of the American Statistical Association 84 (406): 502–16.\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2008. The Elements of Statistical Learning. 2^{} ed. Springer.\nHolmes, Susan. 2006. “Multivariate Analysis: The French way.” In Probability and Statistics: Essays in Honor of David a. Freedman , edited by D. Nolan and T. P. Speed. Vol. 56. IMS Lecture Notes–Monograph Series. Beachwood, OH: IMS. http://www.imstat.org/publications/lecnotes.htm.\nHolmes, Susan, Alexander V Alekseyenko, Alden Timme, Tyrrell Nelson, Pankaj Jay Pasricha, and Alfred Spormann. 2011. “Visualization and Statistical Comparisons of Microbial Communities Using r packages on Phylochip Data.” In Pacific Symposium on Biocomputing , 142–53. World Scientific.\nIzenman, Alan Julian. 2008. “Nonlinear Dimensionality Reduction and Manifold Learning.” In Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning , 597–632. New York, NY: Springer New York.\nJosse, Julie, and Susan Holmes. 2016. “Measuring Multivariate Association and Beyond.” Statistics Surveys 10: 132–67.\nKashyap, Purna C, Angela Marcobal, Luke K Ursell, Samuel A Smits, Erica D Sonnenburg, Elizabeth K Costello, Steven K Higginbottom, et al. 2013. “Genetically Dictated Change in Host Mucus Carbohydrate Landscape Exerts a Diet-Dependent Effect on the Gut Microbiota.” PNAS 110 (42): 17059–64.\nKendall, David. 1969. “Incidence Matrices, Interval Graphs and Seriation in Archeology.” Pacific Journal of Mathematics 28 (3): 565–70.\nLeek, Jeffrey T, Robert B Scharpf, Héctor Corrada Bravo, David Simcha, Benjamin Langmead, W Evan Johnson, Donald Geman, Keith Baggerly, and Rafael A Irizarry. 2010. “Tackling the Widespread and Critical Impact of Batch Effects in High-Throughput Data.” Nature Reviews Genetics 11 (10): 733–39.\nMoignard, Victoria, Steven Woodhouse, Laleh Haghverdi, Andrew J Lilly, Yosuke Tanaka, Adam C Wilkinson, Florian Buettner, et al. 2015. “Decoding the Regulatory Network of Early Blood Development from Single-Cell Gene Expression Measurements.” Nature Biotechnology.\nNelson, Tyrell A, Susan Holmes, Alexander Alekseyenko, Masha Shenoy, Todd DeSantis, Cindy Wu, Gary Andersen, et al. 2010. “PhyloChip Microarray Analysis Reveals Altered Gastrointestinal Microbial Communities in a Rat Model of Colonic Hypersensitivity.” Neurogastroenterology & Motility.\nPagès, Jérôme. 2016. Multiple Factor Analysis by Example Using R. CRC Press.\nPerraudeau, Fanny, Davide Risso, Kelly Street, Elizabeth Purdom, and Sandrine Dudoit. 2017. “Bioconductor Workflow for Single-Cell RNA Sequencing: Normalization, Dimensionality Reduction, Clustering, and Lineage Inference.” F1000Research 6.\nPrentice, IC. 1977. “Non-Metric Ordination Methods in Ecology.” The Journal of Ecology , 85–94.\nRhee, Soo-Yon, Matthew J Gonzales, Rami Kantor, Bradley J Betts, Jaideep Ravela, and Robert W Shafer. 2003. “Human Immunodeficiency Virus Reverse Transcriptase and Protease Sequence Database.” Nucleic Acids Research 31 (1): 298–303.\nRoweis, Sam T, and Lawrence K Saul. 2000. “Nonlinear Dimensionality Reduction by Locally Linear Embedding.” Science 290 (5500): 2323–26.\nTenenbaum, Joshua B, Vin De Silva, and John C Langford. 2000. “A Global Geometric Framework for Nonlinear Dimensionality Reduction.” Science 290 (5500): 2319–23.\nTrosset, Michael W, and Carey E Priebe. 2008. “The Out-of-Sample Problem for Classical Multidimensional Scaling.” Computational Statistics & Data Analysis 52 (10): 4635–42.\n페이지는 R 버전 4.5.1 (2025-06-13)을 사용하여 2025-09-01 01:33에 빌드되었습니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>이질적 데이터의 다변량 분석 (Multivariate Analysis for Heterogeneous Data)</span>"
    ]
  },
  {
    "objectID": "10-chap.html",
    "href": "10-chap.html",
    "title": "12  네트워크와 트리 (Networks and Trees)",
    "section": "",
    "text": "12.1 10.2 그래프(Graphs)\n네트워크와 트리는 생물학적 시스템에 대한 지식을 표현하는 데 자주 사용됩니다. 이들은 또한 실험이나 연구로부터 얻은 관측치를 직접 인코딩하는 데 사용될 수도 있습니다. 계통수(Phylogenetic trees)는 종들 사이의 가족 관계 및 유사성 관계를 표현하기 위해 그려졌으며, 심지어 이러한 트리에 기계론적이고 인과적인 해석을 부여한 다윈(Darwin)의 유명한 노트북 스케치 이전부터 존재했습니다. 네트워크에서 노드(nodes)와 에지(edges)의 의미는 다를 수 있으며 명시되어야 합니다. 예를 들어, 네트워크는 그림 10.1에서와 같이 단백질 사이의 관계를 도식화할 수 있는데, 여기서 노드는 유전자나 그들이 인코딩하는 단백질을 나타낼 수 있고, 에지는 직접적인 물리적 상호작용이거나 실험 결과를 나타내는 좀 더 추상적인 “기능적” 또는 “유전적” 상호작용일 수 있습니다. 이 책에서 우리는 그래프(graph) 와 네트워크(network) 라는 용어를 거의 혼용하여 사용합니다. 전자는 수학적 구조를 좀 더 연상시키고, 후자는 생물학적 해석을 좀 더 연상시킵니다.\n그림 10.1: 단백질 사이의 쌍별 관계를 나타내는 작은 단백질-단백질 네트워크.\n우리는 2장에서 상태 전이 시퀀스를 마르코프 체인(Markov chain)으로 모델링할 수 있음을 보았는데, 이는 에지에 가중치가 부여된 유향 그래프(directed graphs)로 표현될 수 있습니다. 노드가 화학적 대사산물이고 에지가 화학 반응을 나타내는 대사 경로(metabolic pathways)가 그 예입니다. 암 유전체학에서는 돌연변이의 계보를 나타내기 위해 돌연변이 이력 트리(mutation history trees)가 사용됩니다.\n전염 네트워크(transmission networks)는 감염병의 역학을 공부하는 데 중요합니다. 실제 네트워크는 매우 클 수 있으므로, 이를 표현하고 시각화하기 위한 특수한 방법들이 필요할 것입니다.\n이 장에서 우리는 다음을 수행할 것입니다:",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>네트워크와 트리 (Networks and Trees)</span>"
    ]
  },
  {
    "objectID": "10-chap.html#그래프graphs",
    "href": "10-chap.html#그래프graphs",
    "title": "12  네트워크와 트리 (Networks and Trees)",
    "section": "",
    "text": "12.1.1 10.2.1 그래프란 무엇이며 어떻게 인코딩할 수 있는가?\n그래프(graph) 는 대개 ((V,E))로 표시되는 두 세트의 조합으로 정의됩니다. 여기서 (V)는 노드(nodes) 또는 정점(vertices) 의 세트이고, (E)는 정점들 사이의 에지(edges) 세트입니다. (E)의 각 원소는 노드 쌍으로 구성됩니다. 즉, (V)의 두 원소입니다. 그래프를 표현하는 직관적인 방식은 출발-도착(from-to) 표현입니다. 정점 세트를 (V=(A,B,C, …))라고 하면, 출발-도착(또는 에지 리스트) 표현은 다음과 같은 형태의 표입니다:\n  from to\n1    A  B\n2    B  C\n3    A  E\n4    C  D\n5    E  F\n출발-도착 표에서 행의 순서는 아무런 역할을 하지 않습니다. 유향 그래프(directed graph) 또는 지향 그래프(oriented graph) 에서 에지는 순서가 있는 쌍입니다. 즉, 위 표의 첫 번째 줄은 A에서 B로 가는 에지가 있음을 나타내지만, B에서 A로 가는 에지도 있는지는 말해주지 않습니다 – 이는 표의 별도 행에 표시되어야 합니다.\n무향 그래프(undirected graph) 에서 에지는 순서가 없는 쌍입니다. 즉, A에서 B로 가는 에지는 B에서 A로 가는 에지와 구분되지 않습니다. 무향 그래프는 노드 사이의 대칭적인 관계를 인코딩하고, 유향 그래프는 비대칭적인 관계를 나타냅니다.\n그래프와 그 시각화를 혼동하지 않는 것이 중요합니다. 그림 10.1에서처럼 그래프를 2차원 영역에 그리는 것이 가능하지만, 이는 선택 사항이며 유일하지 않습니다 – 동일한 그래프를 그리는 방식은 항상 여러 가지가 있습니다. 또한 그러한 시각화에서 에지들이 서로 교차하지 않는다는 보장도 없습니다; 그래프에 따라 그런 일이 발생할 수 있습니다. 그래프는 물리적 공간(2D든 3D든)에 살고 있는 것이 아니라, 말 그대로 노드와 에지의 세트일 뿐입니다.\n출발-도착 표에 대한 대안적인 표현 방식은 인접 행렬(adjacency matrix) 입니다. 이는 그래프의 노드 수만큼의 행(및 열)을 가진 정사각 행렬입니다. 행렬의 (i)번째 행과 (j)번째 열에 0이 아닌 항목이 있으면 이는 (i)번째 정점과 (j)번째 정점 사이에 에지가 있음을 인코딩합니다.\n질문 10.1\n무향 그래프의 경우, 인접 행렬 (A)의 어떤 점이 특별한가요?\n해결책\n인접 행렬은 대칭입니다. 즉, (M = M^T)입니다. 그림 10.2와 10.3에 예시가 나와 있습니다. g1은 아래 코드를 통해 (2열 행렬 edges에 인코딩된) 에지 리스트로부터 생성되었습니다.\nlibrary(\"igraph\")\nedges = matrix(c(1,3, 2,3, 3,4, 4,5, 4,6), byrow = TRUE, ncol = 2)\ng1 = graph_from_edgelist(edges, directed = FALSE)\nvertex_attr(g1, name = \"name\") = 1:6\nplot(g1, vertex.size = 25, edge.width = 5, vertex.color = \"coral\")\n\n그림 10.2: 번호가 매겨진 노드들이 있는 작은 무향 그래프.\n\n그림 10.3: 그림 10.2에 표시된 그래프의 인접 행렬은 0과 1로 구성된 대칭 (n imes n) 행렬이며, 여기서 (n)은 노드의 수입니다.\n질문 10.2\n위의 질문 10.1의 답변에 나온 방법 외에, 에지 리스트 데이터 프레임으로부터 그래프를 생성하는 대안적인 방법을 제시할 수 있나요?\n해결책\nedges = \"1,3\\n2,3\\n3,4\\n4,6\\n4,5\"\ndf = read.csv(textConnection(edges), header = FALSE)\nsg = graph_from_data_frame(df, directed = FALSE)\nsg\n\n\nIGRAPH 1467322 UN-- 6 5 -- \n+ attr: name (v/c)\n+ edges from 1467322 (vertex names):\n[1] 1--3 2--3 3--4 4--6 4--5\n일반적으로는 별도의 파일로부터 에지 리스트 표를 읽기 위해 read.csv 함수를 사용할 것입니다. 여기서는 예제를 더 간결하게 유지하기 위해 문자열 edges를 즉석에서 만들고 textConnection 함수를 사용하여 이를 파일과 동등하게 만들었습니다. 표기법 \"\\n\"은 줄바꿈을 나타냅니다.\n\n12.1.1.1 단순 그래프의 구성 요소들\n\n노드 또는 정점. 이들은 그림 10.2에서 번호가 들어 있는 색칠된 원들입니다.\n에지 또는 연결. 노드들을 잇는 세그먼트이며 방향이 있을 수도 있고 없을 수도 있습니다.\n에지 속성(에지 길이 등). 별도로 지정되지 않으면 에지 길이는 모두 동일하게 대개 1이라고 가정합니다. 예를 들어, 그래프 상의 두 노드 사이의 거리를 계산하려면 최단 경로를 따라 에지들의 길이를 합산합니다.\n에지 및 노드 속성: 선택적으로, 각 에지나 각 노드는 유형, 색상, 가중치, 에지 너비, 노드 크기 등과 같은 추가적인 연속형 또는 범주형 변수들에 매핑될 수 있습니다. 응용 분야와 의도된 계산에 따라 거의 모든 것이 가능합니다.\n\n우리는 또한 에지 길이를 가진 유향 그래프를 네트워크(network) 라고 부릅니다. 네트워크의 인접 행렬은 에지 길이에 대응하는 양수들로 이루어진 (n imes n) 행렬입니다.\n\n\n12.1.1.2 기본 개념들\n노드의 차수(degree) 는 그 노드에 연결된 에지의 수입니다. 유향 그래프에서는 들어오는 에지와 나가는 에지에 대해 진입 차수(in-degree) 와 진출 차수(out-degree) 를 구분합니다. 또한 사이클(cycles)(셀프 루프나 여러 정점을 거치는 루프를 일컫는 공식 용어)을 포함하는 유향 그래프와 그렇지 않은 그래프(순환 및 비순환 그래프라고 함)를 추가로 구분할 수 있습니다.\n큰 그래프의 경우, 정점 차수의 분포를 살펴봄으로써 전체적인 그래프 구조를 요약할 수 있으며, 중심성(centrality) 이나 매개 중심성(betweenness) 과 같은 척도들을 사용하여 그래프에서 특히 흥미로운 영역이나 특정 노드 및 에지들을 식별할 수 있습니다. 이러한 척도들은 network , igraph 와 같은 다양한 패키지들에서 사용할 수 있습니다.\n만약 에지의 수가 노드의 수와 같은 규모(order of magnitude)라면 ( (#Eope O(#V))라고 씀), 우리는 그 그래프가 희소(sparse) 하다고 말합니다. 일부 그래프는 노드가 매우 많습니다. 예를 들어, ppiData 패키지에는 약 20,000개의 에지를 가진 약 2,500개의 단백질에 대한 예측된 단백질 상호작용(ppipred) 그래프가 들어 있습니다1. 그러한 그래프에 대한 완전한 인접 행렬은 600만 개 이상의 메모리 유닛을 필요로 하며, 그중 대부분은 0을 포함합니다. 이는 불필요한 낭비입니다. 동일한 그래프의 에지 리스트 표현은 더 콤팩트합니다: 에지가 있는 곳에만 저장 공간을 사용하며, 우리 예제에서는 20,000개의 메모리 유닛에 해당합니다. 에지 리스트 표현의 한 가지 구체적인 선택은 Matrix 패키지에 구현된 것과 같은 희소 행렬 인코딩입니다.\n1 유전자 및 종 계통수(phylogenies)는 이보다 훨씬 더 클 수도 있습니다.\n반면에 조밀한(dense) 그래프에서는 에지의 수가 잠재적인 에지의 수, 즉 노드 수의 제곱 ( (#Eope O(#V^2))라고 씀)과 같은 규모입니다. 크고 조밀한 그래프의 경우 저장 메모리 공간이 문제가 될 수 있습니다.\n\n\n12.1.1.3 그래프 레이아웃\n우리는 미학적 또는 실무적인 이유로 동일한 그래프가 서로 다른 방식으로 그려지는 여러 예시를 보게 될 것입니다. 이는 그래프 레이아웃 의 선택을 통해 이루어집니다.\n에지가 거리를 나타내는 길이를 가질 때, 그래프의 2D 표현 문제는 9장에서 보았던 다차원 척도법과 동일합니다. 이는 종종 정점 지점들을 가능한 한 멀리 퍼뜨림으로써 유사한 방식으로 해결됩니다. 길이가 없는 에지의 단순한 경우, 알고리듬은 서로 다른 기준을 선택할 수 있습니다. 프루히터만-라잉골드(Fruchterman and Reingold) 방법이 기본적인 선택입니다. 이는 마치 (뉴턴의) 물리적 힘의 영향을 받는 것처럼 유사한 점들이 서로 끌어당기고 밀어내는 물리 기반 모델에 기초합니다.\n태스크\nigraph 패키지를 사용하여 다음을 수행하세요:\n\n12개의 노드와 50개 이상의 에지를 가진 조밀한 무작위 그래프를 만듭니다.\n서로 다른 레이아웃으로 그래프를 그리는 실험을 해보세요: 노드들을 원형으로 배치하거나, 그래프를 가능한 한 대칭적으로 표현하고 노드나 에지가 겹치지 않게 해보세요.\n\n\n\n12.1.1.4 데이터로부터의 그래프\n대개 데이터는 그래프 형태로 도착하지 않습니다. 그래픽이나 네트워크 표현은 종종 다른 데이터 유형으로부터 변환된 결과입니다.\n거리나 유사성으로부터: 그래프는 거리나 유사성 관계를 이진화(binarising)함으로써 이를 단순화할 수 있습니다. 노드들이 비슷하거나 가까우면 연결하고, 그렇지 않으면 연결하지 않습니다. 따라서 입력값은 관심 있는 모든 객체 쌍 (유전자, 단백질, 종, 표현형, …) 사이의 유사성 또는 거리 척도 세트이며, 여기에 임계값(threshold)이 적용됩니다. 척도 세트는 조밀한 행렬로 구현되거나 즉석에서 계산될 수 있습니다.\n이분 그래프 (Bipartite graphs): 일부 데이터는 갈라파고스 제도의 핀치새 종과 섬들(그림 10.4), 또는 전사 인자와 그들이 결합하는 것으로 간주되는 유전자 조절 영역 사이의 관계처럼 두 가지 유형의 객체 사이의 존재 또는 부재 관계로 자연스럽게 나타납니다. 그러한 관계는 직사각형 행렬에서 0/1 값으로 인코딩될 수 있으며, 여기서 행은 한 객체 유형을 나타내고 열은 다른 유형을 나타냅니다. 결과로 나오는 그래프는 두 가지 유형의 노드(예: 핀치새 노드와 섬 노드)를 가지며, 에지는 서로 다른 유형의 노드 사이에만 존재할 수 있습니다 (예: 분류군과 섬 사이에는 존재하지만 분류군 사이나 섬 사이에는 존재하지 않음). 그림 10.4의 에지는 ~에 산다 는 관계를 나타냅니다.\n\n그림 10.4: 이 이분(bipartite) 그래프는 각 분류군을 그것이 관찰된 사이트들과 연결합니다.\n질문 10.3\nfinch.csv 데이터를 불러오고, 이것이 이분 네트워크를 나타냄을 강조하도록 그리는 실험을 해보세요.\n해결책\n다음 코드의 출력은 그림 10.5에 나와 있습니다.\nfinch = readr::read_csv(\"../data/finch.csv\", comment = \"#\", col_types = \"cc\")\nfinch\n\n\n# A tibble: 122 × 2\n   .tail .head             \n   &lt;chr&gt; &lt;chr&gt;             \n 1 C     Large ground finch\n 2 D     Large ground finch\n 3 E     Large ground finch\n 4 F     Large ground finch\n 5 G     Large ground finch\n 6 H     Large ground finch\n 7 I     Large ground finch\n 8 J     Large ground finch\n 9 L     Large ground finch\n10 M     Large ground finch\n# ℹ 112 more rows\n\n\nlibrary(\"network\")\nfinch.nw  = as.network(finch, bipartite = TRUE, directed = FALSE)\nis.island = nchar(network.vertex.names(finch.nw)) == 1\nplot(finch.nw, vertex.cex = 2.5, displaylabels = TRUE, \n     vertex.col = ifelse(is.island, \"forestgreen\", \"gold3\"),\n     label= sub(\" finch\", \"\", network.vertex.names(finch.nw)))\nfinch.nw |&gt; as.matrix() |&gt; t() |&gt; (\\(x) x[, order(colnames(x))])()\n\n\n                          A B C D E F G H I J K L M N O P Q\nLarge ground finch        0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\nMedium ground finch       1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0\nSmall ground finch        1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0\nSharp-beaked ground finch 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1\nCactus ground finch       1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0\nLarge cactus ground finch 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\nLarge tree finch          0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0\nMedium tree finch         0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\nSmall tree finch          0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0\nVegetarian finch          0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0\nWoodpecker finch          0 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0\nMangrove finch            0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\nWarbler finch             1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n그림 10.5: finches 그래프. 이분 그래프의 특성을 더 잘 고려하도록 레이아웃을 개선하는 방법이 많이 있습니다.\n질문 10.4\nggraph 패키지를 사용하여 그래프 g1의 플롯을 만드세요. 레이아웃과 geom_edge_link, geom_node_point, geom_node_text와 같은 제공된 geom들을 선택하세요.\n해결책\n다음 코드의 출력은 그림 10.6에 나와 있습니다.\nlibrary(\"ggraph\")\nggraph(g1, layout = \"nicely\") + \n  geom_edge_link() + \n  geom_node_point(size=6,color=\"#8856a7\") + \n  geom_node_text(label=vertex_attr(g1)$name,  color=\"white\")\n\n그림 10.6: ggraph 예시.\n\n\n12.1.1.5 예시: 4개 상태 마르코프 체인\n2장에서 우리는 마르코프 체인이 어떻게 뉴클레오타이드(시스템의 상태로 간주됨) 간의 전이를 요약할 수 있는지 보았습니다. 이는 종종 그래프로 도식화됩니다. igraph 패키지는 그래프 “장식”을 위한 많은 선택지를 제공합니다:\nlibrary(\"markovchain\")\nstatesNames = c(\"A\", \"C\", \"G\",\"T\")\nT1MC = new(\"markovchain\", states = statesNames, transitionMatrix =\n  matrix(c(0.2,0.1,0.4,0.3,0,1,0,0,0.1,0.2,0.2,0.5,0.1,0.1,0.8,0.0),\n         nrow = 4,byrow = TRUE, dimnames = list(statesNames, statesNames)))\nplot(T1MC, edge.arrow.size = 0.4, vertex.color = \"purple\",\n     edge.arrow.width = 2.2, edge.width = 5, edge.color = \"blue\",\n     edge.curved = TRUE, edge.label.cex = 2.5, vertex.size= 32,\n     vertex.label.cex = 3.5, edge.loop.angle = 3,\n     vertex.label.family = \"sans\", vertex.label.color = \"white\")\n\n그림 10.7: 상태 간의 가능한 전이를 나타내는 화살표가 있는 4개 상태 마르코프 체인.\n마르코프 체인은 동역학 시스템의 단순한 모델이며, 상태는 그래프의 노드로 표현됩니다. 전이 행렬은 상태 간의 유향 에지(화살표)에 대한 가중치를 제공합니다.\n질문 10.5\n이 마르코프 체인은 어떤 상태로 끝날 것이라고 생각하시나요?\n해결책\nC를 제외한 모든 노드는 나가는 에지(outgoing edges)를 가지고 있으며, C는 들어오는 에지와 자기 자신으로 향하는 에지만을 가집니다. 따라서 C는 흡수 상태(absorbing state)입니다. 이 체인의 모든 프로세스는 조만간 C에 도달하여 머물게 됩니다.\n태스크\n\nset.seed 함수의 입력값을 바꾸어 보고 플롯이 변하는지 확인해 보세요.\n이 특정 plot 함수에 대한 도움말에 접속해 보세요.\n그래프를 다시 그리고, 전이 확률(transition probabilities) 레이블은 녹색으로, 정점(vertices)은 갈색으로 표시해 보세요.\n\n연습 문제 10.3에서 어노테이션이 달린 상태 공간 마르코프 체인 그래프의 완전한 예시를 구축하는 방법을 살펴볼 것입니다.\n\n\n\n12.1.2 10.2.2 여러 레이어가 있는 그래프: 에지와 노드의 레이블\n여기에 정점에 어노테이션이 달린, STRING 데이터베이스에서 다운로드한 그래프를 그리는 예시가 있습니다.\ndatf = read.table(\"../data/string_graph.txt\", header = TRUE)\ngrs = graph_from_data_frame(datf[, c(\"node1\", \"node2\")], directed = FALSE)\nE(grs)$weight = 1\nV(grs)$size = centr_degree(grs)$res\nggraph(grs) + \n  geom_edge_arc(color = \"black\",  strength = 0.05, alpha = 0.8)+\n  geom_node_point(size = 2.5, alpha = 0.5, color = \"orange\") + \n  geom_node_label(aes(label=vertex_attr(grs)$name), size = 3, alpha = 0.9, color = \"#8856a7\", repel = TRUE)\n\n그림 10.8: Yu 등 (2012)에서 정렬된 T 세포의 차등 유전자 발현 패턴을 사용하여 발견된 섭동된 케모카인 하위 네트워크. 오른쪽 모서리에 있는 CXCR3, CXCL13, CCL19, CSCR5 및 CCR7 유전자의 클릭(clique)과 유사한 구조에 주목하세요.\n그림 10.8은 (Nacu et al. 2007)에서 GXNA를 사용하여 유방암 전이 연구에서 발견되고 Yu 등 (2012)에 의해 보고된 전체 섭동 케모카인 하위 네트워크를 보여줍니다.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>네트워크와 트리 (Networks and Trees)</span>"
    ]
  },
  {
    "objectID": "10-chap.html#유전자-세트-농축에서-네트워크로",
    "href": "10-chap.html#유전자-세트-농축에서-네트워크로",
    "title": "12  네트워크와 트리 (Networks and Trees)",
    "section": "12.2 10.3 유전자 세트 농축에서 네트워크로",
    "text": "12.2 10.3 유전자 세트 농축에서 네트워크로\n\n\n\n차등 발현될 가능성이 있는 유전자들의 길고 구조화되지 않은 세탁물 리스트(laundry list)는 위협적일 수 있습니다.\n\n\n차등 발현될 가능성이 있는 유전자들의 길고 구조화되지 않은 세탁물 리스트(laundry list)는 위협적일 수 있습니다.\n8장에서 우리는 차등 발현된 유전자 리스트를 찾는 방법들을 공부했습니다. 적은 표본 크기와 낮은 FDR을 유지하려는 노력은 종종 차등 발현을 감지하는 낮은 검정력으로 이어집니다. 따라서 차등 발현된 것으로 자신 있게 선언할 수 있는 긴 유전자 리스트를 얻는 것은 초기에는 승리입니다. 그러나 기저의 생물학을 이해하려면 생물학적 시스템에서 유의미한 행위자들의 단순한 리스트 이상의 것이 필요합니다.\n\n12.2.1 10.3.1 사전 정의된 유전자 세트를 사용하는 방법 (GSEA)\n가장 초기 접근 방식 중 하나는 유의미한 유전자 리스트에서 과다하게 나타나거나(overrepresented) 농축된(enriched) 유전자 속성들을 찾는 것이었습니다. 이러한 유전자 클래스들은 종종 유전자 온톨로지(Gene Ontology, GO) 범주에 기반합니다 (예: 기관 성장에 관여하는 유전자, 또는 섭식 행동에 관여하는 유전자). 유전자 온톨로지(GO)는 유전자와 유전자 산물을 설명하는 세 가지 온톨로지의 모음입니다. 이러한 온톨로지들은 유향 비순환 그래프(DAGS) 구조를 가진 제한된 어휘들입니다. 가장 구체적인 용어들이 그래프의 잎(leaves)이 됩니다. GO 그래프는 노드(여기서는 유전자 온톨로지 용어)와 더 구체적인 용어(자식)에서 덜 구체적인 용어(부모)로 가는 에지들로 구성되며, 흔히 이러한 에지들은 방향성이 있습니다. 노드와 에지는 시각화될 수 있는 여러 속성을 가질 수 있습니다. 실험에서 유의미한 것으로 지정된 특정 유전자 세트에 대해 GO 어노테이션을 사용하는 주된 목적은 이 리스트에서 특정 GO 용어의 농축(enrichment) 을 찾는 것이며, 우리는 아래에서 이 용어에 통계적 의미를 부여할 것입니다. 다른 많은 유용한 중요 유전자 세트 리스트들이 존재합니다.\n태스크\n유용한 유전자 세트 데이터베이스를 찾아보세요.\n예를 들어, MsigDB 분자 시그니처 데이터베이스 (Liberzon et al. 2011)에는 많은 유전자 세트가 포함되어 있으며, 이는 바이오컨덕터 패키지 GSEABase 의 getBroadSets 함수를 사용하여 R 내부에서 대략 다음과 같이 접근할 수 있습니다:\nlibrary(\"GSEABase\")\n## 웹사이트 로그인이 필요합니다.\nfl   =  \"/path/to/msigdb_v5.1.xml\"\ngss  =  getBroadSets(fl) \norganism(gss[[1]])\ntable(sapply(gss, organism))\n\n\n12.2.2 10.3.2 2원 분할표 검정을 이용한 유전자 세트 분석\n표 10.1: 유의미한 세트에서 각 범주의 유전자 수가 동일하더라도, 아래의 시뮬레이션과 2원 분할표(two-way tables)의 검정 이론은 모두 파란색 범주가 농축되었음을 보여줍니다.\n\n\n\n노란색\n파란색\n빨간색\n\n\n\n\n\n유의미함\n25\n25\n25\n\n\n전체 (Universe)\n500\n100\n400\n\n\n\n여기서는 흔히 피셔의 정확 검정(Fisher’s “exact” test) 또는 초기하 검정(hypergeometric test) 이라 불리는 기본적인 접근 방식을 설명하는 것부터 시작해 보겠습니다.\n\n\n\n소위 ‘exact’ 검정이라고 불리는 이유는 비모수적이며 전수 조사(exhaustive enumerations)에 기반하기 때문이지, 답이 확실하기 때문은 아닙니다. 결국 이것은 통계학이니까요.\n\n\n소위 ‘정확(exact)’ 검정이라고 불리는 이유는 비모수적이며 전수 조사(exhaustive enumerations)에 기반하기 때문이지, 답이 확실하기 때문은 아닙니다. 결국 이것은 통계학이니까요.\n잠재적으로 유의미할 수 있는 후보 유전자들의 전체 집합(universe)을 정의하십시오; 이 유니버스의 크기를 (N)이라고 합시다. 우리는 또한 실제로 유의미하게 나타난 유전자들의 기록을 가지고 있는데, 그 수가 (m)개라고 가정해 봅시다.\n우리는 유전자 유니버스에서 식별된 유전자들에 대응하는 총 (N)개의 공이 상자 안에 들어 있는 장난감 모델을 만듭니다. 이 유전자들은 서로 다른 기능적 범주로 나뉘어 있습니다. (N=1,000)개의 유전자가 있고 그중 500개는 노란색, 100개는 파란색, 400개는 빨간색이라고 가정해 봅시다. 그런 다음 (m=75)개의 유전자 하위 집합에 유의미함(significant) 이라는 레이블이 붙습니다. 이 유의미하게 흥미로운 유전자들 중에 노란색 25개, 빨간색 25개, 파란색 25개가 있다고 가정합시다. 파란색 범주가 농축되었거나 과다하게 나타났다고 할 수 있을까요?\n우리는 일부 범주는 매우 수가 많고 다른 범주는 드물다는 사실을 고려하기 위해 이 초기하 2원 분할표 검정을 사용합니다.\n질문 10.6\n20,000번의 시뮬레이션을 통해 몬테카를로 실험을 실행하고, 유의미한 세트에서 어떤 범주도 과다하게 나타나지 않는다는 귀무 가설하에서 파란색이 25개일 때의 유의성 p-값을 계산하세요.\n해결책\n귀무 가설하에서 75개는 다음과 같이 우리의 균등하지 않은 상자들로부터 무작위로 샘플링됩니다:\nuniverse = c(rep(“Yellow”, 500), rep(“Blue”, 100), rep(“Red”, 400)) countblue = replicate(20000, { pick75 = sample(universe, 75, replace = FALSE) sum(pick75 == “Blue”) }) summary(countblue)\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   6.000   7.000   7.496   9.000  20.000 \n그림 10.9의 히스토그램은 귀무 모델하에서 25만큼 큰 값을 갖는 것이 매우 드물 것임을 보여줍니다.\n\n그림 10.9: 20,000번의 시뮬레이션에서도 파란색 카운트가 25에 근접하는 경우가 없음을 볼 수 있습니다. 우리는 그러한 사건이 우연히 일어났을 가능성을 기각하고 파란색이 농축되었다 고 결론 내릴 수 있습니다.\n일반적인 경우, 유전자 유니버스는 (N)개의 공이 들어 있는 항아리이며, 만약 우리가 (m)개의 공을 무작위로 뽑고 파란색 공의 비율이 (k/N)이라면, (k)개 크기의 추출에서 (km/N)개의 파란색 공을 볼 것으로 기대합니다.\n\n12.2.2.1 GOplot 을 이용한 유전자 농축 네트워크 플로팅\n여기서는 유전자 기능 범주와 유의미한 유전자 세트 사이의 연결을 요약하는 매력적인 방법을 보여줍니다.\nlibrary(\"GOplot\")\ndata(\"EC\")\ncirc  =  circle_dat(EC$david, EC$genelist)\nchord =  chord_dat(circ, EC$genes, EC$process)\nGOChord(chord, limit = c(0, 5))\n\n그림 10.10: 이 그래프는 두 가지 정상 상태 조직(뇌와 심장, Nolan 등 (2013) 참조)의 내피 세포에서의 차등 발현 연구에서 GO 용어와 유의미하게 변한 유전자 사이의 대응 관계를 보여줍니다. 정규화 후 차등 발현 분석이 수행되어 유전자 리스트를 얻었습니다. 그 다음 GOplot 패키지를 사용하여 차등 발현된 유전자 세트 (조정된 p-값 &lt; 0.05)에 대한 유전자 어노테이션 농축 분석을 수행했습니다.\n사실, 유전자 온톨로지 그래프는 서로 다른 프로세스의 유전자들이 종종 생산적으로 상호작용하기 때문에 의미 있는 유전자 상호작용 을 반드시 캡처하는 것은 아닙니다. 많은 양의 정보가 사용되지 않은 채로 남아 있는데, 예를 들어 모든 유의미한 유전자는 p-값의 잠재적으로 큰 변동에도 불구하고 보통 동일한 가중치가 부여됩니다.\n\n\n\n12.2.3 10.3.3 유의미한 하위 그래프 및 고득점 모듈\n우리는 유전자 온톨로지 이상의 것을 마음대로 사용할 수 있습니다. 유전자 네트워크의 다양한 데이터베이스가 있으며, 그중에서 우리의 차등 발현 실험에서 얻은 p-값과 같은 유의성 점수들을 투영할 알려진 골격(skeleton) 그래프를 선택할 수 있습니다. 우리는 Ideker 등 (2002)이 처음 제안한 아이디어를 따를 것입니다. 이는 Nacu 등 (2007)에서 더 발전되었습니다. 많은 개선 사항이 포함된 세심한 구현체는 바이오컨덕터 패키지 BioNet (Beisser et al. 2010)으로 제공됩니다. 이러한 방법들은 모두 점수가 매겨진 골격 네트워크에서 특히 섭동된(perturbed) 것으로 보이는 하위 그래프나 모듈을 검색합니다.\n네트워크의 각 유전자 노드에는 t-통계량이나 p-값으로부터 계산된 점수가 할당됩니다. 종종 경로는 상향 조절된 유전자와 하향 조절된 유전자를 모두 포함하는데, Ideker 등 (2002)에서 지적했듯이, 이는 검정 통계량의 절댓값을 취하거나 p-값으로부터 계산된 점수를 통합함으로써 캡처될 수 있습니다2. Beisser 등 (2010)은 6장에서 했던 것처럼 유전자의 p-값을 모델링합니다: p-값이 균등하게 분포할 섭동되지 않은 유전자와, 섭동된 유전자로부터 나온 균등하지 않게 분포된 p-값의 혼합물입니다. 우리는 Pounds와 Morris (2003)를 따라 p-값에 대해 베타 분포를 사용하여 데이터의 신호를 모델링합니다.\n2 작은 p-값이 큰 점수를 주도록 (-p)와 같은 것이 필요할 것입니다.\n노드 점수 함수가 주어지면, 그래프에서 연결된 핫스팟, 즉 높은 결합 점수를 가진 유전자들의 하위 그래프를 검색합니다.\n\n12.2.3.1 하위 그래프 검색 알고리즘 사용하기\n일반적인 그래프의 최대 점수 하위 그래프를 찾는 것은 일반적으로 다루기 힘든(intractable) 것으로 알려져 있으므로 (NP-어렵 문제라고 함), 다양한 근사 알고리즘들이 제안되었습니다. Ideker 등 (2002)은 시뮬레이티드 어닐링(simulated annealing) 사용을 제안했지만, 이는 속도가 느리고 해석하기 어려운 큰 하위 그래프를 생성하는 경향이 있습니다. Nacu 등 (2007)은 시드(seed) 정점에서 시작하여 그 주변을 점진적으로 확장했습니다. Beisser 등 (2010)은 소위 최소 신장 트리(MST)를 사용하여 검색을 시작했는데, 이 그래프는 이 장의 뒷부분에서 공부할 것입니다.\n\n\n\n12.2.4 10.3.4 BioNet 구현 예시\n방법을 설명하기 위해 BioNet 패키지의 데이터를 보여줍니다.\ninteractome 데이터는 9,386개의 노드와 36,504개의 상호작용으로 구성된 네트워크를 포함합니다. 이것이 우리가 작업할 골격 그래프를 구성합니다 (Beisser et al. (2010) 참조).\ndataLym은 관련 p-값과 (t) 통계량을 포함하며, 다음과 같이 액세스하여 분석을 수행할 수 있습니다:\nlibrary(\"BioNet\")\nlibrary(\"DLBCL\")\ndata(\"dataLym\")\ndata(\"interactome\")\ninteractome\n\n\nA graphNEL graph with undirected edges\nNumber of Nodes = 9386 \nNumber of Edges = 36504 \n\n\npval = dataLym$t.pval\nnames(pval)  =  dataLym$label\nsubnet = subNetwork(dataLym$label, interactome)\nsubnet = rmSelfLoops(subnet)\nsubnet\n\n\nA graphNEL graph with undirected edges\nNumber of Nodes = 2559 \nNumber of Edges = 7788 \n\n12.2.4.1 베타-균등 모델 적합시키기 (Fit a Beta-Uniform model)\np-값은 4장에서 공부한 혼합 모델 유형으로 적합됩니다. 여기에는 확률 (_0)을 갖는 귀무 가설의 균등 성분과 대립 가설에 해당하는 p-값에 대한 베타 분포 ( (a x^{a - 1})에 비례)가 포함됩니다 (Pounds and Morris 2003). [f(x|a,π_0)= π_0 + (1-π_0) a x^{a - 1}\\qquad 0 &lt;x ≤ 1; ; 0&lt;a&lt;1] FDR을 0.001로 설정하여 모델을 실행합니다:\n패키지는 실제로 (_0)에 다른 이름을 부여합니다: ()를 사용하고 이를 혼합 매개변수(mixing parameter)라고 부릅니다.\nfb = fitBumModel(pval, plot = FALSE)\nfb\n\n\nBeta-Uniform-Mixture (BUM) model\n\n3583 pvalues fitted\n\nMixture parameter (lambda): 0.482\nshape parameter (a):        0.180\nlog-likelihood:         4471.8\n\n\nscores=scoreNodes(subnet, fb, fdr = 0.001)\n\n그림 10.11: qqplot은 데이터에 대한 베타-균등 혼합 모델 적합의 품질을 보여줍니다. 빨간색 점은 베타 분포의 이론적 분위수를 (x) 좌표로, 관측된 분위수를 (y) 좌표로 가집니다. 파란색 선은 이 모델이 잘 들어맞음을 보여줍니다.\n\n그림 10.12: p-값에 대한 혼합 성분의 히스토그램입니다. 베타 분포는 빨간색, 균등 분포는 파란색으로 표시됩니다. (_0)는 분포가 균등해야 하는 귀무 성분에 할당된 혼합 비율입니다.\n그다음 다음을 사용하여 고득점 하위 그래프에 대한 휴리스틱 검색을 실행합니다:\nhotSub = runFastHeinz(subnet, scores) hotSub\nA graphNEL graph with undirected edges Number of Nodes = 144 Number of Edges = 221\nlogFC=dataLym$diff\nnames(logFC)=dataLym$label\n질문 10.7\n다음 코드를 사용하여 그림 10.13을 만들었습니다:\nplotModule(hotSub, layout = layout.davidson.harel, scores = scores,\n                  diff.expr = logFC)\n\n그림 10.13: ABC와 GCB B세포 림프종 사이의 차등 발현에 대해 최대로 농축된 것으로 찾아진 하위 그래프입니다. 노드는 빨간색과 초록색으로 칠해져 있습니다: 초록색은 ACB에서의 상향 조절을, 빨간색은 GBC에서의 상향 조절을 나타냅니다. 노드의 모양은 점수를 나타냅니다: 직사각형은 음수 점수를, 원은 양수 점수를 나타냅니다.\nigraph.from.graphNEL 함수를 사용하여 모듈 객체를 변환하고 10.2.2절에 표시된 ggraph 방법을 사용하여 플로팅하세요.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>네트워크와 트리 (Networks and Trees)</span>"
    ]
  },
  {
    "objectID": "10-chap.html#계통수-phylogenetic-trees",
    "href": "10-chap.html#계통수-phylogenetic-trees",
    "title": "12  네트워크와 트리 (Networks and Trees)",
    "section": "12.3 10.4 계통수 (Phylogenetic Trees)",
    "text": "12.3 10.4 계통수 (Phylogenetic Trees)\n\n그림 10.14: 수학적 객체로서, 계층적 클러스터링 트리(5장에서 연구됨)는 계통수와 동일합니다. 이들은 팁에 레이블이 있는 뿌리가 있는 이진(rooted binary) 트리입니다.\n생물학에서 그래프의 정말 중요한 용도 중 하나는 계통수를 구축하는 것입니다. 트리는 사이클(cycles) (셀프 루프나 여러 정점을 거치는 루프를 일컫는 공식 용어)이 없는 그래프입니다. 계통수는 보통 팁에 있는 현대의3 분류군(taxa)에 해당하는 잎(leaves)에만 레이블이 있는 뿌리가 있는 이진 트리입니다. 내부 노드는 팁에 있는 현대(contemporaneous) 데이터로부터 추론되어야 하는 조상(ancestral) 서열에 해당합니다. 많은 방법들이 서로 다른 종이나 집단에서 얻은 정렬된 DNA 서열을 사용하여 트리를 추론하거나 추정합니다. 트리의 팁은 보통 OTU(Operational Taxonomic Units)라고 불립니다. 이러한 분석에서 관심 있는 통계적 매개변수(parameter) 는 잎에 OTU 레이블이 있는 뿌리가 있는 이진 트리입니다 (자세한 내용은 Holmes (1999, 2003b)를 참조하세요).\n3 현대적이기 때문에, 트리는 종종 잎들이 모두 뿌리에서 같은 거리에 있도록 표현됩니다.\n\n12.3.0.1 HIV의 예\n\n그림 10.15: 이 계통수는 아프리카에서의 서로 다른 HIV/SIV 균주의 역사를 설명합니다 (Wertheim and Worobey 2009), [그림 출처].\nHIV는 매우 빠르게 진화함으로써 (수개월 내에 여러 돌연변이가 나타날 수 있음) 스스로를 보호하는 바이러스입니다. 따라서 그 진화는 실시간으로 추적될 수 있는 반면, 대형 유기체의 진화는 수백만 년에 걸쳐 일어났습니다. HIV 트리는 약물 내성 탐지 및 이해와 같은 의료 목적으로 구축됩니다. 이들은 개별 유전자에 대해 추정됩니다. 서로 다른 유전자는 진화 역사에서 차이를 보일 수 있으며 따라서 서로 다른 유전자 트리(gene trees) 를 생성할 수 있습니다. 그림 10.15의 계통수는 바이러스가 원숭이에서 인간으로 옮겨간 시기를 보여줍니다 (Wertheim and Worobey 2009).\n\n\n12.3.0.2 계통수의 특별한 요소들\n\n대부분의 계통수는 뿌리가 있는 것으로 표시되며, 뿌리(root)는 나중에 보게 되겠지만 보통 트리의 팁에 외집단(outgroup)을 포함시킴으로써 찾아집니다.\n이 공통 조상으로부터 유래된 형질을 상동(homologous) 이라고 합니다 (집단 유전학자들은 상동이라는 용어 대신 가계에 의한 동일성(identity by descent, IBD)으로 대체하기도 합니다).\n공통 조상에 의해 정의된 트리의 자매들을 분기군(clades) 또는 단계통군(monophyletic groups) 이라고 하며, 이들은 단순한 유사성 이상의 공통점을 가집니다.\n\n\n\n12.3.1 10.4.1 진화를 위한 마르코프 모델\n\n\n\n이것을 분자 시계 가설이라고 합니다. 만약 이 가정을 하지 않는다면 비식별성(non-identifiability) 문제에 직면하게 됩니다 (즉, 관측된 데이터가 주어졌을 때 가능한 많은 돌연변이 역사들 사이의 차이를 말할 수 없습니다).\n\n\n우리는 그림 10.7에서 본 [A, C, G, T] 상태에 대한 마르코프 체인을 사용할 것입니다. 하지만 이제는 상태의 변화, 즉 돌연변이가 무작위 시간에 발생한다고 가정합니다. 이러한 돌연변이 사건 사이의 간격은 지수 분포를 따를 것입니다. 이러한 연속 시간 마르코프 체인은 다음과 같은 속성들을 가집니다:\n\n기억 없음 (No Memory). (P(Y(u+t)=j|Y(t)=i))는 (t) 이전의 시간에 의존하지 않습니다.\n시간 동질성 (Time homogeneity). 확률 (P(Y(h+t)=j|Y(t)=i))는 (t)에 의존하지 않고, 사건 사이의 시간인 (h)와 (i) 및 (j)에 의존합니다.\n선형성 (Linearity). 순간 전이율은 대략 선형 형태입니다.\n\n\n\n\n여기서 o(h)라고 쓴 오차항을 사용합니다. 이것은 리틀 o 오브 h라고 읽으며, 이 오차항이 h보다 훨씬 느리게(즉, 하위 선형으로) 커진다는 것을 의미합니다.\n\n\n여기서 (o(h))라고 쓴 오차항을 사용합니다. 이것은 ’리틀 o 오브 h’라고 읽으며, 이 오차항이 (h)보다 훨씬 느리게 (즉, 하위 선형으로) 커진다는 것을 의미합니다.\n[ \\[\\begin{align} P_{ij}(h)&=q_{ij}h+o(h), \\quad\\text{for }j\\neq i\\\\ P_{ii}(h)&=1-q_i(h)+ o(h), \\qquad\\text{where }q_i=\\sum_{j\\neq i}q_{ij}. \\end{align}\\] ]\n(q_{ij})는 순간 전이율(instantaneous transition rate) 로 알려져 있습니다. 이러한 비율들은 표 10.2와 같은 행렬을 정의합니다.\n표 10.2: 두 가지 전이율 행렬의 예입니다. 왼쪽은 Jukes-Cantor(JC69) 모델이고, 오른쪽은 Kimura(K80) 2-매개변수 모델을 보여줍니다.\n\n\n\n\n\n\n\n(Q = \\begin{array}{lcccc} & A & T & C & G \\ A & -3& & & \\ T & & -3& & \\ C & & & -3& \\ G & & & & -3\\end{array})\n(Q = \\begin{array}{lcccc} & A & T & C & G \\ A & - & & & \\ T & & - & & \\ C & & & - & \\ G & & & & - \\end{array})\n\n\n\n\n\n생성기(generator) 라고 불리는 순간 변화 확률 행렬입니다. 가장 단순한 모델인 Jukes-Cantor 모델에서는 모든 돌연변이가 발생할 가능성이 동일합니다 (표 10.2의 왼쪽 참조). 좀 더 유연한 모델인 Kimura 모델은 표 10.2의 오른쪽에 나와 있습니다.\n질문 10.8\n왜 Kimura 모델이 더 유연하다고 말할까요?\n해결책\nJukes-Cantor 모델은 하나의 매개변수만을 가지며, 모든 전이(transitions)와 트랜스버전(transversions)이 발생할 가능성이 동일하다고 가정합니다. Kimura 모델에는 전이와 트랜스버전(퓨린 내에서 혹은 피리미딘 내에서 발생하는 돌연변이 대 퓨린에서 피리미딘으로 또는 그 반대로 발생하는 돌연변이)에 대해 각각 하나씩 매개변수가 있습니다.\n\n\n\n여기서 용어의 혼란이 있을 수 있습니다! : 이 문맥에서 전이(Transitions)는 퓨린 내(A&lt;-&gt;G) 또는 피리미딘 내(C &lt;-&gt; T)의 돌연변이 변화를 의미합니다. 반면 앞서 마르코프 체인에 대해 이야기했을 때의 전이(transition) 행렬은 모든 상태 변화의 모든 확률을 포함합니다.\n\n\n가장 유연한 모델은 일반화된 시간 가역적 (Generalized Time Reversible, GTR) 모델이라고 하며, 6개의 자유 매개변수를 가집니다. 우리는 알려진 트리로부터 이러한 생성 모델에 따라 시뮬레이션된 데이터의 예를 보여줄 것입니다.\n\n\n12.3.2 10.4.2 데이터 시뮬레이션 및 트리 플로팅\n우리가 이미 계통수를 알고 있고 이 트리를 따른 뉴클레오타이드의 진화를 시뮬레이션하고 싶다고 가정해 봅시다. 먼저 관련 패키지들을 로드하고 ggtree를 사용하여 트리 tree1을 시각화합니다:\nlibrary(\"phangorn\")\nlibrary(\"ggtree\")\nload(file.path(\"../data/tree1.RData\"))\n태스크\nggtree 함수를 사용하여 tree1을 플롯하세요. 트리의 팁(tips)은 초록색 삼각형으로, 조상 노드들은 빨간색 원으로 만드세요.\nggtree(tree1, lwd = 2, color = “darkgreen”, alpha = 0.8, right = TRUE) + geom_tiplab(size = 7, angle = 90, offset = 0.05) + geom_point(aes(shape = isTip, color = isTip), size = 5, alpha = 0.6)\n\n그림 10.16: 이것은 우리가 실제 매개변수로 사용하는 트리입니다. 우리는 뿌리(root)에서 하나씩 뉴클레오타이드를 생성하여 트리를 따라 “떨어뜨립니다”. 에지 길이에 비례하는 확률로 브랜치를 따라 돌연변이가 발생합니다.\n이제 우리의 트리로부터 몇 가지 서열을 생성합니다. 각 서열은 뿌리에서 무작위로 생성된 새로운 뉴클레오타이드 문자로 시작하며, 트리를 따라 내려가면서 돌연변이가 발생할 수 있습니다. 그림 10.17에서 색상들이 균등하게 나타나지 않는 것을 볼 수 있는데, 이는 뿌리에서의 빈도가 균등하지 않게 선택되었기 때문입니다. 아래 코드를 참조하세요.\nseqs6 = simSeq(tree1, l = 60, type = \"DNA\", bf = c(1, 1, 3, 3)/8, rate = 0.1)\nseqs6\n\n\n6 sequences with 60 character and 30 different site patterns.\nThe states are a c g t \n\n\nmat6df = data.frame(as.character(seqs6))\np = ggtree(tree1, lwd = 1.2) + geom_tiplab(aes(x = branch), size = 5, vjust = 2)\ngheatmap(p, mat6df[, 1:60], offset = 0.01, colnames = FALSE)\n\n그림 10.17: 왼쪽의 트리는 Jukes-Cantor 모델에 따라 오른쪽의 서열들을 생성하는 데 사용되었습니다. 뿌리에서 생성된 뉴클레오타이드 빈도는 꽤 균등하지 않았으며, A와 C가 더 드물게 생성되었습니다. 서열이 트리를 따라 스며들면서 돌연변이가 발생하며, 이는 긴 브랜치에서 발생할 가능성이 더 높습니다.\n질문 10.9\n위의 코드로 실험해 보세요. 돌연변이가 더 잘 발생하도록 simSeq 함수의 bf 및 rate 인수를 변경해 보세요. 매우 높은 돌연변이율로 생성된 서열들이 그것들을 생성한 트리를 추론하는 것을 더 쉽게 만들 것이라고 생각하시나요?\n해결책\n매우 높은 돌연변이율은 돌연변이가 스스로를 덮어쓰게 만들어 추론을 더 어렵게 만듭니다. 물론, 브랜치를 해상(resolve)하기 위해 어느 정도의 돌연변이는 발생해야 하므로 적정 수준이 존재합니다. 특정 시간과 일정 횟수의 돌연변이가 발생한 후에는 뿌리에서 무슨 일이 일어났는지 말하기가 매우 어려울 수 있습니다. 자세한 내용은 Mossel (2003)을 참조하세요.\n질문 10.10\n추정 편향: 거리 과소추정.\n1) 만약 우리가 단순한 해밍 거리를 사용하여 두 서열 사이의 변화 횟수만을 센다면, 두 서열 사이에 많은 진화적 변화가 있었을 때 왜 서열 간의 거리를 과소추정하게 될까요?\n2) 진화적 거리가 작을 때 편향이 더 커질까요?\n위에서 본 진화의 표준 마르코프 모델들은 이러한 추정치들을 개선할 수 있게 해줍니다.\n\n\n12.3.3 10.4.3 계통수 추정\n\n“이러한 종류의 문제를 해결할 때 가장 중요한 것은 거꾸로 추론할 수 있는 능력입니다. 이는 매우 유용한 성취이며 매우 쉬운 일이지만, 사람들은 이를 많이 연습하지 않습니다. 일상생활의 업무에서는 앞으로 추론하는 것이 더 유용하므로 다른 쪽은 무시되기 마련입니다. 종합적으로 추론할 수 있는 사람이 50명이라면 분석적으로 추론할 수 있는 사람은 한 명뿐입니다.”\n셜록 홈즈\n실제 트리-매개변수가 알려져 있을 때, 위에서 언급한 진화의 확률적 생성 모델은 서열에서 어떤 패턴을 예상해야 할지 알려줍니다. 이전 장들에서 보았듯이, 통계학이란 데이터로부터 매개변수의 합리적인 추정치로 되돌아가는 것을 의미합니다. 여기서 트리 그 자체와 브랜치 에지 길이, 심지어 진화율조차 매개변수로 간주될 수 있습니다.\n\n그림 10.18: 스타이너 트리(Steiner tree), 내부 점들은 사각형으로 표시됩니다. 외부 점 1, 2, 5, 6을 모두 통과하는 가장 짧은 트리를 만드는 방법은 내부의 (“조상”) 점 3과 4를 만드는 것입니다.\n추정에는 여러 접근 방식이 있습니다: 트리 ’구축’도 예외는 아니며, 주요 방식들은 다음과 같습니다:\n비모수적 추정: 파시모니 트리(parsimony tree) 파시모니(Parsimony)는 데이터를 설명하는 데 필요한 변화의 횟수를 최소화하는 비모수적 방법이며, 그 해결책은 스타이너 트리(Steiner tree) 문제의 해결책과 동일합니다 (그림 10.18 참조).\n모수적 추정: 최대 우도 트리(maximum likelihood tree) 최대 우도나 베이지안 접근법을 사용하여 트리를 추정하려면, 돌연변이율과 브랜치 에지 길이를 통합하는 분자 진화 모델이 필요합니다. ML 추정(예: Phyml, FastML, RaxML)은 모델 가정하에서 트리의 우도를 최대화하기 위해 효율적인 최적화 알고리즘을 사용합니다.\n트리에 대한 베이지안 사후 분포 베이지안 추정인 MrBayes (Ronquist et al. 2012)나 BEAST (Bouckaert et al. 2014)는 모두 계통의 사후 분포를 찾기 위해 MCMC를 사용합니다. 베이지안 방법은 R에 직접 통합되어 있지 않으며, 사용자가 몬테카를로 방법에 의해 생성된 트리 컬렉션을 요약하고 신뢰 진술을 하기 위해 이를 임포트해야 합니다. 간단한 예시는 Chakerian과 Holmes (2012)를 참조하세요.\n준모수적 접근: 거리 기반 방법 Neighbor Joining 및 UPGMA라 불리는 이 방법들은 우리가 5장에서 이미 접했던 계층적 군집화 알고리즘과 매우 유사합니다. 하지만 거리 추정 단계는 표 10.2의 모수적 진화 모델을 사용합니다. 우리가 이 방법을 준모수적(semi-parametric)이라고 부르는 이유 중 ’모수적’인 부분이 바로 이것입니다.\nNeighbor-Joining 알고리즘 자체는 결합된 두 점의 요약으로 스타이너 점을 사용하며, 계층적 군집화에서와 같이 반복적으로 진행됩니다. 이는 매우 빠를 수 있으며 종종 더 많은 시간이 소요되는 추정 절차를 위한 좋은 시작점으로 사용됩니다.\nJukes-Cantor 모델에 기반한 DNA 거리에 대해 nj(neighbor joining)를 사용하여 데이터 seqs6로부터 트리를 추정하는 것으로 시작해 봅시다. ggtree 함수를 사용하여 그림 10.19를 만듭니다:\ntree.nj = nj(dist.ml(seqs6, “JC69”)) ggtree(tree.nj) + geom_tiplab(size = 7)\n\n그림 10.19: Neighbor Joining 알고리즘으로 구축된 트리는 계산 속도가 매우 빠르며, 종종 최대 우도나 파시모니와 같은 더 비싼 추정 절차를 위한 초기 값으로 사용됩니다.\n질문 10.11\nseqs6 데이터가 주어졌을 때 tree1의 최대 우도 점수를 생성하고 이를 neighbor joining 트리의 점수와 비교하세요.\n해결책\nfit = pml(tree1, seqs6, k = 4)\n질문 10.12\n트리를 추론하려는 정렬된 아미노산이 있을 때, 우리는 ( (20 imes 20)) 전이 행렬을 사용합니다. 계통수를 추정하는 방법들은 매우 유사합니다. https://www.hiv.lanl.gov/content/sequence/NEWALIGN/align.html 에서 다운로드한 HIV 아미노산 서열을 가지고 phangorn에서 이를 시도해 보세요.\n트리 추정치의 품질은 분류군당 서열 수와 뿌리까지의 거리에 따라 달라집니다. 우리는 모수적 및 비모수적 붓스트랩을 사용하거나 MCMC를 사용한 베이지안 트리 추정을 수행하여 추정치의 품질을 평가할 수 있습니다. 트리의 표집 분포를 시각화하고 비교하는 방법의 예시는 Chakerian과 Holmes (2012)를 참조하세요.\n\n\n12.3.4 10.4.4 16S rRNA 데이터에의 적용\n5장에서 우리는 16S rRNA 서열의 노이즈를 제거하기 위해 확률적 군집화 방법을 사용하는 법을 보았습니다. 이제 이러한 노이즈가 제거된 서열들을 다시 로드하고 계통수를 구축하기 전에 전처리할 수 있습니다 4.\n4 모든 정보를 유지하고 서로 다른 실험에서 얻은 서열들을 비교할 수 있도록 하기 위해, 우리는 서열 자체를 레이블로 사용합니다 (Callahan, McMurdie, and Holmes 2017).\nlibrary(\"dada2\")\nseqtab = readRDS(file.path(\"../data/seqtab.rds\"))\nseqs = getSequences(seqtab)\nnames(seqs) = seqs\n16S rRNA 유전자와 같이 잘 연구된 마커 유전자 좌위를 사용하는 이점 중 하나는 시퀀싱된 변이들을 분류학적으로 분류할 수 있다는 능력입니다. dada2 는 이 목적을 위해 나이브 베이지안 분류기(naive Bayesian classifier) 방법을 포함하고 있습니다 (Wang et al. 2007). 이 분류기는 시퀀싱된 변이들을 분류된 서열의 훈련 세트와 비교합니다. 여기서는 RDP v16 훈련 세트 (Cole et al. 2009)를 사용합니다 5. 예를 들어, 그러한 분류를 위한 코드는 다음과 같습니다.\n5 dada2 웹사이트의 다운로드 링크를 참조하세요: https://benjjneb.github.io/dada2/training.html\nfastaRef = \"../tmp/rdp_train_set_16.fa.gz\"\ntaxtab = assignTaxonomy(seqtab, refFasta = fastaRef)\nassignTaxonomy 함수는 실행 시간이 꽤 걸리므로 위의 코드는 라이브로 실행되지 않으며, 여기서는 이전에 계산된 결과인 분류학 정보 테이블을 로드합니다:\ntaxtab = readRDS(file.path(“../data/taxtab16.rds”)) dim(taxtab)\n[1] 268   6\n질문 10.13\nR의 파이프 연산자 |&gt;를 사용하여 행 이름을 제외한 분류학 정보의 처음 6행만을 보여주는 코드 한 줄을 작성하세요.\n해결책\nhead(taxtab) |&gt; `rownames&lt;-`(NULL)\n\n\n     Kingdom    Phylum          Class         Order          \n[1,] \"Bacteria\" \"Bacteroidetes\" \"Bacteroidia\" \"Bacteroidales\"\n[2,] \"Bacteria\" \"Bacteroidetes\" \"Bacteroidia\" \"Bacteroidales\"\n[3,] \"Bacteria\" \"Bacteroidetes\" \"Bacteroidia\" \"Bacteroidales\"\n[4,] \"Bacteria\" \"Bacteroidetes\" \"Bacteroidia\" \"Bacteroidales\"\n[5,] \"Bacteria\" \"Bacteroidetes\" \"Bacteroidia\" \"Bacteroidales\"\n[6,] \"Bacteria\" \"Bacteroidetes\" \"Bacteroidia\" \"Bacteroidales\"\n     Family               Genus        \n[1,] \"Porphyromonadaceae\" NA           \n[2,] \"Porphyromonadaceae\" NA           \n[3,] \"Porphyromonadaceae\" NA           \n[4,] \"Porphyromonadaceae\" \"Barnesiella\"\n[5,] \"Bacteroidaceae\"     \"Bacteroides\"\n[6,] \"Porphyromonadaceae\" NA           \n질문 10.14\n분류학적(taxonomic) 정보와 계통학적(phylogenetic) 정보의 차이는 무엇인가요?\nseqs 데이터는 무작위로 생성되었으므로, 우리가 실제로 다루어야 할 실제 데이터보다는 “깨끗”하다는 점에 유의하세요.\n특히 자연적으로 발생하는 가공되지 않은 서열들은 정렬(aligned) 되어야 합니다. 이는 일부 서열에 추가적인 뉴클레오타이드가 있는 경우가 빈번하기 때문인데, 이는 소위 indel 사건의 결과입니다 6. 또한 돌연변이가 발생하여 한 뉴클레오타이드가 다른 것으로 치환되어 나타나기도 합니다.\n6 뉴클레오타이드가 삭제되거나 삽입되며, 어느 것이 일어났는지 구별하기 어려운 경우가 많습니다.\n여기에 정렬된 서열의 처음 몇 글자가 어떻게 보이는지 예시가 있습니다:\nreadLines(file.path(\"../data/mal2.dna.txt\")) |&gt; head(12) |&gt; cat(sep=\"\\n\")\n\n\n    11   1620\nPre1        GTACTTGTTA GGCCTTATAA GAAAAAAGT- TATTAACTTA AGGAATTATA\nPme2        GTATCTGTTA AGCCTTATAA AAAGATAGT- T-TAAATTAA AGGAATTATA\nPma3        GTATTTGTTA AGCCTTATAA GAGAAAAGTA TATTAACTTA AGGA-TTATA\nPfa4        GTATTTGTTA GGCCTTATAA GAAAAAAGT- TATTAACTTA AGGAATTATA\nPbe5        GTATTTGTTA AGCCTTATAA GAAAAA--T- TTTTAATTAA AGGAATTATA\nPlo6        GTATTTGTTA AGCCTTATAA GAAAAAAGT- TACTAACTAA AGGAATTATA\nPfr7        GTACTTGTTA AGCCTTATAA GAAAGAAGT- TATTAACTTA AGGAATTATA\nPkn8        GTACTTGTTA AGCCTTATAA GAAAAGAGT- TATTAACTTA AGGAATTATA\nPcy9        GTACTCGTTA AGCCTTTTAA GAAAAAAGT- TATTAACTTA AGGAATTATA\nPvi10       GTACTTGTTA AGCCTTTTAA GAAAAAAGT- TATTAACTTA AGGAATTATA\n우리는 DECIPHER 패키지 (Wright 2015)를 사용하여 우리 seqs 데이터에 대해 다중 정렬 (multiple-alignment)을 수행할 것입니다:\nlibrary(\"DECIPHER\")\nalignment = AlignSeqs(DNAStringSet(seqs), anchor = NA, verbose = FALSE)\n우리는 phangorn 패키지를 사용하여 (GTR 모델하에서) MLE 트리를 구축할 것이지만, neighbor-joining 트리를 시작점으로 사용할 것입니다.\nphangAlign = phangorn::phyDat(as(alignment, \"matrix\"), type = \"DNA\")\ndm = phangorn::dist.ml(phangAlign)\ntreeNJ = phangorn::NJ(dm)   # Note: tip order != sequence order\nfit = phangorn::pml(treeNJ, data = phangAlign)\nfitGTR = update(fit, k = 4, inv = 0.2)\nfitGTR = phangorn::optim.pml(fitGTR, model = \"GTR\", optInv = TRUE,\n         optGamma = TRUE,  rearrangement = \"stochastic\",\n         control = phangorn::pml.control(trace = 0))",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>네트워크와 트리 (Networks and Trees)</span>"
    ]
  },
  {
    "objectID": "10-chap.html#계통수를-데이터-분석에-결합하기",
    "href": "10-chap.html#계통수를-데이터-분석에-결합하기",
    "title": "12  네트워크와 트리 (Networks and Trees)",
    "section": "12.4 10.5 계통수를 데이터 분석에 결합하기",
    "text": "12.4 10.5 계통수를 데이터 분석에 결합하기\n이제 계통수와 노이즈가 제거된 리드 풍부도를, 리드가 수집된 샘플에 대해 제공된 보완 정보와 결합해야 합니다. 샘플에 대한 정보는 종종 스프레드시트(또는 .csv) 형식으로 제공되며, 때로는 메타(meta) 데이터라고 불리기도 합니다 7. 이러한 데이터 결합 단계는 phyloseq 가 제공하는 특수 컨테이너와 접근자들에 의해 용이해집니다.\n7 우리는 메타 라는 접두사가 여기서 도움이 되지 않으며 잠재적으로 혼동을 줄 수 있다고 생각합니다: 샘플에 대한 데이터는 그저 데이터일 뿐입니다.\n다음 단계 세트는 몇 가지 데이터 정리 및 재조직화 작업 –응용 통계학의 지루하지만 필수적인 부분– 을 포함하며 ps1 객체 생성으로 마무리됩니다.\nsamples = read.csv(“../data/MIMARKS_Data_combined.csv”, header = TRUE) samples\\(SampleID = paste0(gsub(\"00\", \"\", samples\\)host_subject_id), “D”, samples\\(age-21)\n    samples = samples[!duplicated(samples\\)SampleID), ] stopifnot(all(rownames(seqtab) %in% samples\\(SampleID))\n    rownames(samples) = samples\\)SampleID keepCols = c(“collection_date”, “biome”, “target_gene”, “target_subfragment”, “host_common_name”, “host_subject_id”, “age”, “sex”, “body_product”, “tot_mass”, “diet”, “family_relationship”, “genotype”, “SampleID”) samples = samples[rownames(seqtab), keepCols]\n샘플별 서열 특징 테이블, 샘플 (메타)데이터, 서열 분류학 정보, 그리고 계통수가 다음과 같이 하나의 객체로 결합됩니다:\nlibrary(\"phyloseq\")\npso = phyloseq(tax_table(taxtab), \n               sample_data(samples),\n               otu_table(seqtab, taxa_are_rows = FALSE), \n               phy_tree(fitGTR$tree))\n우리는 8장에서 pasilla 데이터를 연구했을 때처럼, 불균질한 데이터 세트들을 데이터 세트의 서로 다른 부분들을 연결하고 일관되게 유지하는 작업을 자동화해 주는 특수한 데이터 클래스로 결합하는 여러 사례들을 접했습니다.\n태스크\n자세한 phyloseq 문서를 여기에서 살펴보세요. 몇 가지 필터링 작업을 시도해 보세요. 예를 들어, 리드 수가 5000개 이상인 샘플들에 대해서만 트리, 분류군 풍부도 테이블, 샘플 및 분류군 정보를 포함하는 데이터 하위 집합을 만들어 보세요.\n이는 한 줄로 수행할 수 있습니다:\nprune_samples(rowSums(otu_table(pso)) &gt; 5000, pso)\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 268 taxa and 10 samples ]\nsample_data() Sample Data:       [ 10 samples by 14 sample variables ]\ntax_table()   Taxonomy Table:    [ 268 taxa by 6 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 268 tips and 266 internal nodes ]\n우리는 또한 데이터 구성 요소들 사이의 링크 무결성을 유지하면서 다른 데이터 변환을 수행할 수 있습니다.\n질문 10.15\n다음 코드 줄들은 무엇을 하나요?\nprevalence = apply(X = otu_table(pso),\n                   MARGIN = ifelse(taxa_are_rows(pso), yes = 1, no = 2),\n                   FUN = function(x) {sum(x &gt; 0)})\nprevdf = data.frame(Prevalence = prevalence,\n                    TotalAbundance = taxa_sums(pso),\n                    tax_table(pso))\ntab = table(prevdf$Phylum)\nkeepPhyla = names(tab)[tab&gt;5]\nprevdf1   = subset(prevdf,   Phylum %in% keepPhyla)\nps2v      = subset_taxa(pso, Phylum %in% keepPhyla)\n특정 박테리아의 풍부도를 플로팅하는 것은 막대 차트를 사용하여 수행할 수 있습니다. ggplot2 표현식들이 phyloseq 패키지의 적절한 한 줄 호출로 고정되어 있습니다. 또한 대화형 Shiny-phyloseq 브라우저 기반 도구도 있습니다 (McMurdie and Holmes 2015). 자세한 내용은 온라인 비네트를 참조하세요.\n\n12.4.1 10.5.1 계층적 다중 검정\n가설 검정은 개별 박테리아 중에서 그 풍부도가 관심 있는 샘플 변수와 연관된 것을 식별할 수 있습니다. 표준적인 접근 방식은 6장에서 이미 방문했던 접근 방식과 매우 유사합니다. 각 분류군에 대해 개별적으로 검정 통계량을 계산한 다음, 거짓 발견율(FDR) 상한을 보장하기 위해 p-값들을 공동으로 조정합니다. 하지만 이 절차는 테스트된 가설들 사이의 구조를 활용하지 않습니다. 예를 들어, 만약 우리가 하나의 Ruminococcus 종이 나이와 강하게 연관되어 있음을 관찰했지만 그 연관성의 생물학적 이유가 속(genus) 수준에 있다면, 우리는 다른 종들도 그러한 연관성을 가질 것으로 예상할 것입니다. 그러한 정보를 통합하기 위해, Benjamini와 Yekutieli (2003) 및 Benjamini와 Bogomolov (2014)는 하위 수준의 분류학적 그룹들은 상위 수준이 연관된 것으로 판명되었을 때만 검정하는 계층적 검정 절차를 제안했습니다. 많은 관련 종들이 약간의 신호를 가지고 있는 경우, 이러한 정보의 풀링(pooling)은 검정력을 높일 수 있습니다.\n우리는 이 방법을 적용하여 미생물 풍부도와 나이 사이의 연관성을 테스트합니다. 우리는 위에서 사용한 pso와 유사하지만 몇 가지 추가적인 변환 및 필터링 단계를 거친 데이터 객체 ps1을 사용합니다. 또한 8장에서 논의했던 DESeq2 패키지에서 사용 가능한 정규화 프로토콜을 적용해야 합니다. 이는 RNA-Seq 데이터에 대해서는 Love, Huber, Anders (2014)를, 16S rRNA 생성 카운트 데이터에 대해서는 McMurdie와 Holmes (2014)를 따릅니다.\n# warning: !expr c(\"DESeqDataSet.se, design = design, ignoreRank.: some variables in design formula are characters, converting to factors\")\nlibrary(\"DESeq2\")\nps1 = readRDS(file.path(\"../data/ps1.rds\"))\nps_dds = phyloseq_to_deseq2(ps1, design = ~ ageBin + family_relationship)\ngeometricmean = function(x)\n   if (all(x == 0)) { 0 } else { exp(mean(log(x[x != 0]))) }\ngeoMeans = apply(counts(ps_dds), 1, geometricmean)\nps_dds = estimateSizeFactors(ps_dds, geoMeans = geoMeans)\nps_dds = estimateDispersions(ps_dds)\nabund = getVarianceStabilizedData(ps_dds)\n우리는 계층적 검정을 수행하기 위해 structSSI 패키지를 사용합니다 (Sankaran and Holmes 2014). 더 편리한 출력을 위해, 먼저 분류군의 이름을 줄입니다:\nrownames(abund) = substr(rownames(abund), 1, 5) |&gt; make.names(unique = TRUE)\n우리가 지금 수행할 계층적 검정 절차는 표준적인 다중 가설 검정과 다른데, 단변량 검정이 모든 분류군뿐만 아니라 각 상위 수준의 분류학적 그룹에 대해서도 수행되기 때문입니다. 이를 위해 헬퍼 함수인 treePValues를 사용할 수 있습니다: 이 함수는 첫 번째 행에 뿌리 노드를 지정하고 부모-자식 관계를 인코딩한 에지 리스트를 기대합니다.\nlibrary(\"structSSI\")\nel = phy_tree(ps1)$edge\nel0 = el\nel0 = el0[rev(seq_len(nrow(el))), ]\nel_names = c(rownames(abund), seq_len(phy_tree(ps1)$Nnode))\nel[, 1] = el_names[el0[, 1]]\nel[, 2] = el_names[el0[, 2]]\nunadj_p = treePValues(el, abund, sample_data(ps1)$ageBin)\n우리는 이제 계층적 검정 절차를 사용하여 FDR 계산을 수행할 수 있습니다. 검정 결과는 여러 수준에서 FDR을 제어하는 것을 보장합니다. 자세한 내용은 (Benjamini and Yekutieli 2003; Benjamini and Bogomolov 2014; Sankaran and Holmes 2014)를 참조하세요.\n태스크\n브라우저 창을 열 대화형 플로팅 명령을 포함한 다음 코드를 시도해 보세요:\nhfdr_res = hFDR.adjust(unadj_p, el, 0.75) summary(hfdr_res) #plot(hfdr_res, height = 5000) # not run: opens in a browser\n\n그림 10.20: 계층적 검정 절차에 의해 결정된, 차등적으로 풍부한 미생물이 많은 하위 트리의 스냅샷입니다. 현재 사용자가 미생물 GCGAG.33과 연관된 노드 위에 마우스를 올리고 있습니다. 이로 인해 조정된 p-값(0.029)이 나타납니다.\n플롯은 새 브라우저에서 열립니다 – 하위 트리의 정적인 스냅샷이 그림 10.20에 표시되어 있습니다. 노드들은 p-값에 따라 파란색에서 주황색으로 음영 처리되며, 이는 가장 강한 연관성에서 가장 약한 연관성까지를 나타냅니다. 회색 노드들은 유망한 하위 트리에 검정력을 집중하기 위해 결코 검정되지 않은 노드들입니다. 전체 트리를 스캔해 보면, 연령 그룹과 분류학적 풍부도 사이의 연관성이 소수의 고립된 분류학적 그룹들에만 존재한다는 것이 명확해집니다. 이러한 그룹들 내에서는 연관성이 꽤 강합니다. 이러한 결과들에 문맥을 제공하기 위해, 우리는 기각된 가설들의 분류학적 정체를 추출할 수 있습니다.\nlibrary(\"dplyr\")\noptions(digits = 3)\ntax = tax_table(ps1)[, c(\"Family\", \"Genus\")] |&gt; data.frame()\ntax$seq = rownames(abund)\nhfdr_res@p.vals$seq = rownames(hfdr_res@p.vals)\nleft_join(tax, hfdr_res@p.vals[,-3]) |\n  arrange(adjp) |&gt; head(9) |&gt; dplyr::select(1,2,4,5)\n\n\n              Family       Genus hypothesisName hypothesisIndex\n1 Porphyromonadaceae        &lt;NA&gt;           &lt;NA&gt;              NA\n2 Porphyromonadaceae        &lt;NA&gt;           &lt;NA&gt;              NA\n3 Porphyromonadaceae        &lt;NA&gt;           &lt;NA&gt;              NA\n4 Porphyromonadaceae Barnesiella           &lt;NA&gt;              NA\n5     Bacteroidaceae Bacteroides           &lt;NA&gt;              NA\n6 Porphyromonadaceae Barnesiella           &lt;NA&gt;              NA\n7      Rikenellaceae   Alistipes           &lt;NA&gt;              NA\n8 Porphyromonadaceae        &lt;NA&gt;           &lt;NA&gt;              NA\n9 Porphyromonadaceae        &lt;NA&gt;           &lt;NA&gt;              NA\n가장 강력하게 연관된 박테리아들은 모두 Lachnospiraceae 과에 속하는 것으로 보입니다.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>네트워크와 트리 (Networks and Trees)</span>"
    ]
  },
  {
    "objectID": "10-chap.html#최소-신장-트리",
    "href": "10-chap.html#최소-신장-트리",
    "title": "12  네트워크와 트리 (Networks and Trees)",
    "section": "12.5 10.6 최소 신장 트리",
    "text": "12.5 10.6 최소 신장 트리\n매우 단순하고 유용한 그래프는 소위 최소 신장 트리(minimum spanning tree, MST) 입니다. 정점 세트가 주어졌을 때, 신장 트리(spanning tree) 는 모든 점들을 적어도 한 번 통과하는 트리입니다. 예시가 그림 10.21에 나와 있습니다. 정점들 사이의 거리가 주어졌을 때, MST는 전체 길이가 최소인 신장 트리입니다 (그림 10.21 참조).\n탐욕 알고리즘(Greedy algorithms)이 MST 계산에 잘 작동하며 R에는 많은 구현체들이 있습니다: ade4 의 mstree, ape 의 mst, vegan 의 spantree, igraph 의 mst.\n\n\n\n\n\n\n\n\n그림 10.21: 동일한 6개 정점 세트에 대한 두 개의 신장 트리. 2D 평면 상의 점들 사이의 유클리드 거리가 사용된다면 파란색 그래프가 최소 신장 트리입니다.\n여기서 우리는 전 세계 환자들로부터 얻은 HIV 균주의 DNA 서열 거리를 가져와 그들의 최소 신장 트리를 구축할 것입니다. 결과는 그림 10.22에 나와 있습니다.\nload(file.path(\"../data/dist2009c.RData\"))\ncountry09 = attr(dist2009c, \"Label\")\nmstree2009 = ape::mst(dist2009c)\ngr09 = graph_from_adjacency_matrix(mstree2009, mode = \"undirected\")\nggraph(gr09, layout=\"fr\") + \n  geom_edge_link(color = \"black\",alpha=0.5) + \n  geom_node_point(aes(color = vertex_attr(gr09)$name), size = 2) + \n  geom_node_text(aes(label = vertex_attr(gr09)$name), color=\"black\",size=2) + \n  theme_void() + \n  guides(color=guide_legend(keyheight=0.1,keywidth=0.1,\n      title=\"Countries\"))\n\n그림 10.22: 2009년에 채취되었고 기원 국가가 알려진 샘플들로부터 얻은 HIV 서열 간의 DNA 거리를 사용하여 계산된 최소 신장 트리. 데이터는 HIVdb 데이터베이스 (Rhee et al. 2003)에 발표되었습니다.\n질문 10.16\n네트워크 플롯을 다시 만드세요. 단, 겹치는 노드 레이블을 최소화하기 위해 geom_node_text를 서로 밀어내는 레이블로 교체하세요.\n해결책\n그림 10.23을 참조하세요. 아마도 더 좋거나 추가적인 접근 방식은, 먼저 매우 가깝고 동일한 국가에서 온 정점들을 군집화하는 것일 것입니다.\nlibrary(\"ggraph\")\nggraph(gr09, layout=\"fr\") + \n  geom_edge_link(color = \"black\",alpha=0.5) + \n  geom_node_point(aes(color = vertex_attr(gr09)$name), size = 2) + \n  geom_node_label(aes(label = vertex_attr(gr09)$name), color=\"black\",size=2,repel=TRUE) + \n  theme_void() + \n  guides(color=guide_legend(keyheight=0.1,keywidth=0.1,\n      title=\"Countries\"))\n\n그림 10.23: 질문 10.16에 대한 해결책.\n알려진 지리적 좌표를 통합하는 그래프 레이아웃 을 사용하는 것이 더 선호될 수도 있습니다. 그렇게 함으로써, 여행자 이동성을 통해 바이러스가 전 세계적으로 얼마나 먼 거리를 도약했는지 볼 수 있을지도 모릅니다. 우리는 대략적인 국가 좌표를 도입하고, 겹침을 줄이기 위해 이를 약간 지터링(jittering) 합니다.\nlibrary(\"rworldmap\")\nmat = match(country09, countriesLow$NAME)\ncoords2009 = data.frame(\n  lat = countriesLow$LAT[mat],\n  lon = countriesLow$LON[mat],\n  country = country09)\nlayoutCoordinates = cbind(\n  x = jitter(coords2009$lon, amount = 15),\n  y = jitter(coords2009$lat, amount = 8))\nlabc = names(table(country09)[which(table(country09) &gt; 1)])\nmatc = match(labc, countriesLow$NAME)\ndfc = data.frame(\n  latc = countriesLow$LAT[matc],\n  lonc = countriesLow$LON[matc],\n  labc)\ndfctrans = dfc\ndfctrans[, 1] = (dfc[,1] + 31) / 93\ndfctrans[, 2] = (dfc[,2] + 105) / 238\nCountries = vertex_attr(gr09)$name \n\nggraph(gr09, layout=layoutCoordinates) + \n  geom_node_point(aes(color=Countries),size = 3, alpha=0.75) + \n  geom_edge_arc(color = \"black\", alpha = 0.5, strength=0.15) + \n  geom_label(data=dfc,aes(x=lonc,y=latc,label=labc,fill=labc),colour=\"white\",alpha=0.8,size=3,show.legend=F) + \n  theme_void()\n\n그림 10.24: HIV 사례들 간의 최소 신장 트리. 사례들의 지리적 위치는 겹침을 줄이기 위해 지터링되었습니다. HIV 균주 사이의 DNA 서열 거리가 무향 최소 신장 트리 알고리즘의 입력으로 사용되었습니다.\n최소 신장 트리 알고리즘의 입력은 거리 행렬이나 에지 길이 속성이 있는 그래프입니다. 그림 10.24는 HIVdb 데이터베이스 (Rhee et al. 2003)를 통해 균주 정보가 공개된 HIV 사례들 사이의 최소 신장 트리입니다. DNA 거리는 Jukes-Cantor 돌연변이 모델을 사용하여 계산되었습니다.\n질문 10.17\n위의 분석은 무향 네트워크 연결을 제공했습니다. 사실 최소 신장 트리의 여러 구현체들 (예를 들어 ade4 의 mstree)은 점들을 통과하는 유향 경로를 제공하며, 이는 질병의 (겉보기) 확산에 대한 의미 있는 정보를 제공할 수 있습니다. 위의 지도의 유향 네트워크 버전을 만드세요.\nMST는 그 정점들에 매핑된 요인들 사이의 차이를 탐지하기 위한 단순한 비모수적 검정의 매우 유용한 구성 요소입니다.\n\n12.5.1 10.6.1 MST 기반 검정: 프리드먼-라프스키 검정\n그래프 기반의 2-표본 검정 8은 프리드먼과 라프스키 (Friedman and Rafsky 1979)에 의해 왈드-울포위츠 런 검정(Wald-Wolfowitz runs test)의 일반화로서 도입되었습니다 (그림 10.25 참조). 우리의 이전 예제들은 국가와 같은 공변량과 연관된 그래프 정점들을 보여줍니다. 여기서 우리는 공변량이 그래프 구조와 유의미하게 연관 되어 있는지 테스트합니다.\n8 두 표본이 동일한 분포에서 추출되었는지 탐구하는 검정.\n프리드먼-라프스키(Friedman-Rafsky) 검정은 최소 신장 트리 상에서 2-표본/다중-표본 분리를 테스트합니다. 이는 단변량 왈드-울포위츠 런 검정의 일반화로 구상되었습니다. 만약 우리가 관심 측정값을 좌표로 하는 남성과 여성을 비교한다면, 그림 10.25에서처럼 두 그룹을 파란색과 빨간색으로 칠합니다. 왈드-울포위츠 검정은 두 그룹이 서로 다른 평균을 가짐을 나타낼 동일한 색상의 긴 ’런(runs)’을 검색합니다.\n\n그림 10.25: 1차원 2-표본 비모수적 왈드-울포위츠 검정에서 런(runs)의 수를 살펴보는 것은 두 그룹이 동일한 분포를 가지는지 여부를 나타낼 수 있습니다.\n한 유형의 연속된 값들(‘런’)을 찾는 대신, 우리는 동일한 유형의 연결된 노드들의 수를 셉니다.\n일단 최소 신장 트리가 구축되면, 정점들에는 요인 변수의 서로 다른 수준들에 따라 ’색상’이 할당됩니다. 우리는 두 노드가 요인 변수의 동일한 수준을 가진 에지들을 순수(pure) 에지라고 부릅니다. 우리는 순수 에지의 수인 (S_O)를 검정 통계량으로 사용합니다. 관측된 값이 그룹들이 동일한 분포를 가질 때 우연히 발생할 수 있었는지 평가하기 위해, 정점 레이블(색상)을 무작위로 순열(permute)시키고 순수 에지가 몇 개인지 다시 셉니다. 이러한 레이블 스와핑(label swapping)은 여러 번 반복되어 (S)에 대한 귀무 분포를 생성합니다.\n\n\n12.5.2 10.6.2 예시: 생쥐 간 박테리아 공유\n여기서는 미생물 함량 분석을 위해 대변을 분석한 생쥐 샘플 컬렉션을 통해 이 아이디어를 설명합니다. 우리는 많은 생쥐와 많은 분류군이 포함된 데이터 세트를 읽어 들이고, 자카드 거리를 계산한 다음 igraph 패키지의 mst 함수를 사용합니다. 아래 코드에 표시된 것처럼 그래프에 관련 공변량들을 주석으로 답니다:\nps1  = readRDS(file.path(\"../data/ps1.rds\"))\nsampledata = data.frame( sample_data(ps1))\nd1 = as.matrix(phyloseq::distance(ps1, method=\"jaccard\"))\ngr = graph_from_adjacency_matrix(d1,  mode = \"undirected\", weighted = TRUE)\nnet = igraph::mst(gr)\nV(net)$id = sampledata[names(V(net)), \"host_subject_id\"]\nV(net)$litter = sampledata[names(V(net)), \"family_relationship\"]\n우리는 결과로 얻은 igraph 생성 최소 신장 트리로부터 ggraph 객체를 만들고, 이를 그림 10.26에서처럼 플롯합니다.\nggraph(net, layout=“fr”)+ geom_edge_arc(color = “darkgray”) + geom_node_point(aes(color = id, shape = litter)) + theme(legend.position=“bottom”)\n\n그림 10.26: 자카드 비유사성에 기초하고 생쥐 ID 및 집단 요인으로 주석이 달린 최소 신장 트리.\n이제 검정을 위한 귀무 분포와 p-값을 계산합니다. 이는 phyloseqGraphTest 패키지에 구현되어 있습니다:\nlibrary(\"phyloseqGraphTest\")\ngt = graph_perm_test(ps1, \"host_subject_id\", distance=\"jaccard\",\n                     type=\"mst\",  nperm=1000)\ngt$pval\n\n\n[1] 0.000999\n우리는 다음을 사용하여 순열에 의해 생성된 귀무 분포의 전체 히스토그램을 살펴볼 수 있습니다:\nplot_permutations(gt)\n\n그림 10.27: 자카드 유사성을 이용한 최소 신장 트리로부터 얻은 네트워크의 순수 에지 수에 대한 순열 히스토그램.\n\n12.5.2.1 골격 그래프의 다양한 선택지\n에지를 정의하는 골격 그래프를 위해 반드시 MST를 사용할 필요는 없습니다. 가장 가까운 이웃들을 연결하여 만든 그래프 (Schilling 1986)나 거리 임계값 처리를 사용한 그래프도 잘 작동합니다.\n바이오컨덕터 패키지 phyloseq 는 make_network 함수를 통해 거리 행렬의 임계값 처리에 기반한 그래프 생성 기능을 가지고 있습니다. 우리는 자카드 비유사성이 임계값보다 작은 샘플들 사이에 에지를 만듦으로써 네트워크를 생성하며, 임계값은 아래에서 max.dist 매개변수를 통해 설정합니다. 또한 ggraph 패키지를 사용하여 정점에 샘플이 어느 생쥐로부터 왔는지, 그리고 그 생쥐가 어느 집단에 있었는지를 나타내는 속성들을 추가할 수 있습니다. 그림 10.28에 표시된 결과 네트워크에서, 샘플들이 생쥐와 집단 모두에 의해 그룹화되어 있음을 볼 수 있습니다.\nnet = make_network(ps1, max.dist = 0.35) sampledata = data.frame(sample_data(ps1)) V(net)\\(id = sampledata[names(V(net)), \"host_subject_id\"]\n    V(net)\\)litter = sampledata[names(V(net)), “family_relationship”]\nggraph(net, layout=“fr”) + geom_edge_link(color = “darkgray”) + geom_node_point(aes(color = id, shape = litter)) + theme(plot.margin = unit(c(0, 5, 2, 0), “cm”))+ theme(legend.position = c(1.4, 0.3),legend.background = element_blank(), legend.margin=margin(0, 3, 0, 0, “cm”))+ guides(color=guide_legend(ncol=2))+ theme_graph(background = “white”)\n\n그림 10.28: 자카드 비유사성 행렬에 임계값을 사용하여 생성된 공동 발생 네트워크. 색상은 샘플이 어느 생쥐로부터 왔는지를 나타내고, 모양은 그 생쥐가 어느 집단에 있었는지를 나타냅니다.\n샘플들 사이에 어떤 그래프를 구축하든 상관없이, 우리는 그래프 노드의 레이블을 순열시킴으로써 귀무 분포를 근사할 수 있음을 기억하세요. 하지만 때로는 알려진 공변량들 사이의 구조를 고려하도록 순열 분포를 조정하는 것이 더 선호될 수도 있습니다.\n\n\n\n12.5.3 10.6.3 중첩 공변량이 있는 프리드먼-라프스키 검정\n위의 검정에서 우리는 다소 나이브한 접근 방식을 취했고 개별 생쥐들(host_subject_id 변수) 사이에 유의미한 차이가 있음을 보여주었습니다. 여기서 우리는 생쥐들 간의 차이를 통제할 때, 집단(family_relationship 변수) 효과가 있는지 알아보기 위해 약간 다른 순열 검정을 수행합니다. 검정의 설정은 유사하며, 단지 순열이 생성되는 방식만이 다릅니다. 우리는 grouping 인수를 사용하여 두 요인의 중첩된 구조를 유지합니다. 우리는 family_relationship 레이블을 순열시키지만 host_subject_id 구조는 손대지 않고 유지합니다.\ngt = graph_perm_test(ps1, “family_relationship”, grouping = “host_subject_id”, distance = “jaccard”, type = “mst”, nperm= 1000) gt$pval\n[1] 0.002\n이 검정은 작은 p-값을 가지며, 우리는 두 표본이 동일한 분포에서 왔다는 귀무 가설을 기각합니다. 그림 10.27의 최소 신장 트리 플롯으로부터, 샘플들이 우리가 우연히 예상하는 것보다 더 많이 집단별로 그룹화됨을 육안으로 확인할 수 있습니다.\nplot_permutations(gt)\n\n그림 10.29: 자카드 유사성을 이용한 최소 신장 트리로부터 얻은 순열 히스토그램.\n질문 10.18\n(k)-최근접 이웃(nearest neighbors) 그래프는 한 샘플이 다른 샘플의 (k)-최근접 이웃 세트에 있을 때마다 에지로 연결됨으로써 얻어집니다. 자카드 거리로 정의된 최근접 이웃을 사용하여 그래프를 정의하고 검정을 다시 수행하세요. 무엇을 결론지으시겠습니까?\n해결책\ngt = graph_perm_test(ps1, “family_relationship”, grouping = “host_subject_id”, distance = “jaccard”, type = “knn”, knn = 1) gt$pval\n[1] 0.004\n그림 10.30은 이 최근접 이웃 그래프에서 에지로 연결된 샘플 쌍들이 동일한 집단에서 왔을 가능성이 훨씬 더 높음을 보여줍니다.\nplot_test_network(gt)\n\n그림 10.30: 자카드 유사성을 이용한 최근접 이웃 그래프로부터 얻은 그래프.\n참고: 쌍대 그래프 (The dual graph)\n위의 예제들에서 우리는 공유된 분류군을 통해 샘플들 사이의 관계를 보여주려 했습니다. 일부 분류군이 여러분이 예상하는 것보다 더 자주 공동 발생하는지 묻는 분류군에 대한 질문을 하는 것도 흥미로울 수 있습니다. 이러한 접근 방식은 마이크로바이옴에서 조립되는 미생물 ’커뮤니티’를 연구하는 데 도움을 줄 수 있습니다. 우리가 위에서 개발한 방법들은 모두 이 사용 사례에 적용될 수 있으며, 정말로 해야 할 일은 데이터를 전치(transpose)하는 것뿐입니다. 마이크로바이옴과 같은 희소 데이터의 경우 자카드를 사용하고, 다른 상황에서 적절할 수 있는 상관관계 네트워크를 구축하지 않는 것이 항상 바람직합니다.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>네트워크와 트리 (Networks and Trees)</span>"
    ]
  },
  {
    "objectID": "10-chap.html#이-장의-요약",
    "href": "10-chap.html#이-장의-요약",
    "title": "12  네트워크와 트리 (Networks and Trees)",
    "section": "12.6 10.7 이 장의 요약",
    "text": "12.6 10.7 이 장의 요약\n\n12.6.0.1 어노테이션이 달린 그래프\n이 장에서 우리는 단순한 배열보다 더 많은 구조를 가진 데이터를 저장하고 플로팅하는 법을 배웠습니다: 그래프는 에지와 노드를 가지며, 이들은 또한 유용하게 표시될 수 있는 추가적인 어노테이션들과 연관될 수 있습니다.\n\n\n12.6.0.2 그래프의 주요 예시와 유용한 R 패키지\n우리는 마르코프 체인 그래프, 계통수 및 최소 신장 트리와 같은 구체적인 예시들로부터 시작했습니다. 우리는 ggraph 및 igraph 패키지를 사용하여 그래프를 시각화하고 특정 그래프 레이아웃 알고리즘을 사용하여 가능한 한 많은 정보를 보여주는 방법을 보았습니다.\n\n\n12.6.0.3 그래프와 통계 데이터의 결합\n그런 다음 우리는 알려진 ‘골격’ 그래프를 차등 발현 분석에 통합하는 문제에 접근했습니다. 이는 네트워크에서 섭동 핫스팟을 정밀하게 가리킬 수 있게 해줍니다. 우리는 뿌리가 있는 이진 트리를 따라 정의된 진화 모델이 어떻게 계통수 추정의 기초가 되는지 보았으며, 이러한 트리들을 R 패키지 structSSI 및 phyloseq 를 사용하여 어떻게 차등 풍부도 분석에 보충 정보로 통합할 수 있는지 살펴보았습니다.\n\n\n12.6.0.4 공동 발생과 다른 변수의 연결\n그래프 및 네트워크 도구들은 또한 공동 발생 데이터로부터 네트워크 생성을 가능하게 하며, 요인 공변량의 효과를 시각화하고 테스트하는 데 사용될 수 있습니다. 우리는 변수와 골격 그래프의 에지 구조 사이의 의존성을 테스트하는 쉬운 방법을 제공하는 프리드먼-라프스키 검정을 보았습니다.\n\n\n12.6.0.5 맥락 및 해석 보조 도구\n이 장에서는 네트워크에서 행위자들의 상호작용을 통합하는 방법들을 예시했으며, 이것을 통계 점수들과 결합하는 것이 얼마나 유용한지 보았습니다. 이는 복잡한 생물학적 시스템의 분석에 종종 생물학적 통찰력을 제공합니다.\n\n\n12.6.0.6 이전 지식 또는 결과\n우리는 그래프가 우리의 이전 지식, 대사 네트워크 정보, 유전자 온톨로지 및 알려진 박테리아의 계통수 정보를 인코딩하는 데 유용할 수 있음을 보았습니다. 연구에서 알려진 모든 정보를 통합하는 것이 유익하며, 이러한 골격 네트워크들을 관찰된 데이터와 결합함으로써 이미 알려진 것들의 맥락에서 실험 결과를 이해하는 능력을 향상시킬 수 있습니다.\n반면에, 그래프 자체가 우리가 예측하고자 하는 결과일 수도 있으며, 우리는 데이터로부터 그래프를 구축하는 방법(계통수, 공동 발생 네트워크 및 최소 신장 트리)을 보았습니다.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>네트워크와 트리 (Networks and Trees)</span>"
    ]
  },
  {
    "objectID": "10-chap.html#더-읽을거리",
    "href": "10-chap.html#더-읽을거리",
    "title": "12  네트워크와 트리 (Networks and Trees)",
    "section": "12.7 10.8 더 읽을거리",
    "text": "12.7 10.8 더 읽을거리\n계통수에서 사용되는 진화 모델의 완전한 발전과 많은 중요한 결과들에 대해서는 Li (1997; Li and Graur 1991)의 책들을 참조하십시오. Felsenstein (2004)의 책은 계통수 추정에 관한 고전적인 텍스트입니다.\nape 패키지의 저자가 쓴 책인 Paradis (2011)는 R에서의 트리 조작에 관한 많은 사용 사례와 세부 사항들을 담고 있습니다. 계통수에 대한 붓스트랩의 리뷰는 Holmes (2003a)에서 찾아볼 수 있습니다.\n우리는 PCoA-MDS의 확장인 DPCoA(Double principal coordinate analysis)를 통해 트리뿐만 아니라 분할표 데이터의 풍부도도 사용할 수 있습니다. 마이크로바이옴 데이터의 경우, 계통수는 분류군 사이의 거리를 제공하며, 이러한 거리들은 첫 번째 PCoA의 기초가 됩니다. 두 번째 PCoA는 가중치가 부여된 샘플 점들의 투영을 가능하게 합니다. 이는 마이크로바이옴 생태학 응용 분야에서 매우 효과적인 것으로 입증되었습니다. 자세한 내용은 Purdom (2010) 또는 Fukuyama 등 (2012)을 참조하십시오.\n그래프는 정점 공변량을 예측하는 데 사용될 수 있습니다. 그래프의 에지를 반응 변수로 간주하여 공변량이나 그래프에 대한 부분적인 지식을 바탕으로 예측을 수행하는 응용 통계 및 기계 학습의 큰 분야가 있습니다. 여기에는 ERGM (Exponential Random Graph Models, Robins et al. (2007)) 및 그래프를 위한 커널 방법 (Schölkopf, Tsuda, and Vert 2004)이 포함됩니다.\n프리드먼-라프스키 검정의 이론적 속성과 더 많은 예시는 Bhattacharya (2015)를 참조하십시오.\n그래프와 네트워크를 다루는 패키지들의 전체 리스트는 다음에서 확인 가능합니다: http://www.bioconductor.org/packages/release/BiocViews.html#___GraphAndNetwork.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>네트워크와 트리 (Networks and Trees)</span>"
    ]
  },
  {
    "objectID": "10-chap.html#연습-문제",
    "href": "10-chap.html#연습-문제",
    "title": "12  네트워크와 트리 (Networks and Trees)",
    "section": "12.8 10.9 연습 문제",
    "text": "12.8 10.9 연습 문제\n연습 문제 10.1\n인접 행렬(adjacency matrix)로부터 시작하여 그래프를 그리는 함수를 만드세요. 예시를 통해 어떻게 작동하는지 보여주세요.\n해결책\nggplotadjacency = function(a) { n = nrow(a) p = ncol(a) fromto = reshape2::melt(a) stopifnot(identical(nrow(fromto), n*p)) fromto\\(value = as.factor(fromto\\)value) cols = c(“white”, “darkblue”) ggplot(data = fromto, aes(x = Var1, y = Var2, fill = value)) + geom_tile(colour = “black”) + coord_fixed(ratio = 1, ylim = c(0.5, n + 0.5), xlim = c(0.5, p + 0.5)) + scale_fill_manual(values = cols) + scale_x_continuous(name = “” , breaks = 1:p, labels = paste(1:p)) + scale_y_reverse( name = “” , breaks = n:1, labels = paste(n:1)) + theme_bw() + theme(axis.text = element_text(size = 14), legend.key = element_rect(fill = “white”), legend.background = element_rect(fill = “white”), panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(color = “white”), axis.ticks.x = element_blank(), axis.ticks.y = element_blank() ) }\n연습 문제 10.2\n유전자 기능 간의 관계는 유전자 온톨로지(Gene Ontology, GO) 그래프라고 불리는 그래프로 계층적으로 조직되어 있습니다. 생물학적 프로세스는 점점 더 미세한 스케일로 조직됩니다. 여러분이 관심 있는 유기체에 대한 GO 정보를 제공하는 데이터베이스 중 하나를 선택하세요. 유전자 리스트를 선택하고 그 리스트에 대한 GO 그래프를 구축하세요.\n힌트: 일부 예시는 패키지들에서 찾을 수 있습니다.\n연습 문제 10.3\n질 마이크로바이옴의 상태 간 전이에 대한 마르코프 체인 그래프 : DiGiulio 등 (2015)에서 저자들은 markovchain 패키지를 사용하여 커뮤니티 상태 유형 (CSTs) 간의 전이율을 나타내기 위해 igraph 플롯을 사용했습니다. 데이터를 로드하고 전이율과 상태 이름을 특수 클래스인 markovchain 객체에 넣은 후, 정점의 공변량으로 조산율(percentage of preterm birth)을 포함하도록 레이아웃을 세심하게 조정하세요 (정점 크기를 이 변수에 비례하게 만드세요). 상태 간의 전이 크기를 화살표의 너비로 포함시키세요.\n해결책\n참고: 아래 코드는 라이브로 실행되지 않으며, 한때 저자들을 위해 실행되었던 버전입니다. 여기서는 독자가 마무리할 수 있도록 시작점으로 제공되며, 현대화 및 개선의 여지가 있습니다.\n\n그림 10.31: 이 그림은 원래 DiGiulio 등 (2015) 연구를 위해 작성되었으며, 노드는 질 마이크로바이옴의 상태를 나타내고 화살표는 서로 다른 확률을 가진 상태 간의 전이를 나타냅니다.\nlibrary(\"markovchain\")\n# 마르코프 체인 객체 만들기\nmcPreg  =  new(\"markovchain\", states = CSTs,\n              transitionMatrix = trans, name=\"PregCST\")\nmcPreg\n# 마르코프 체인의 igraph 설정\nnetMC  =  markovchain:::.getNet(mcPreg, round = TRUE)\n이제 여러 플로팅 매개변수들을 정의하고, 해당 CST와 조산 결과 사이의 연관성에 기초하여 노드 색상을 할당합니다.\nwts = E(netMC)\\(weight/100\n    edgel  =  get.edgelist(netMC)\n    elcat  =  paste(edgel[,1], edgel[,2])\n    elrev  =  paste(edgel[,2], edgel[,1])\n    edge.curved  =  sapply(elcat, function(x) x %in% elrev)\n    samples_def  =  data.frame(sample_data(ps))\n    samples_def  =  samples_def[samples\\)Preterm | samples\\(Term,] # 명확하게 할당된 것들만, 즉 한계적인 것들은 제외\n    premat  =  table(samples_def\\)CST, samples_def$Preterm) rownames(premat) = markovchain::states(mcPreg) colnames(premat) = c(“Term”, “Preterm”) premat premat = premat/rowSums(premat) vert.CSTclrs = CSTColors\ndefault.par = par(no.readonly = TRUE) # 색상 척도 정의 # 마르코프 체인을 위한 플로팅 함수 plotMC = function(object, …) { netMC = markovchain:::.getNet(object, round = TRUE) plot.igraph(x = netMC, …) } # 마르코프 체인 시각화를 위한 컬러 바, 조산 연관성 강도의 그라디언트 color.bar = function(lut, min, max=-min, nticks=11, ticks=seq(min, max, len=nticks), title=NULL) { scale = (length(lut)-1)/(max-min) cur.par = par(no.readonly = TRUE) par(mar = c(0, 4, 1, 4) + 0.1, oma = c(0, 0, 0, 0) + 0.1) par(ps = 10, cex = 0.8) par(tcl=-0.2, cex.axis=0.8, cex.lab = 0.8) plot(c(min,max), c(0,10), type=‘n’, bty=‘n’, xaxt=‘n’, xlab=’‘, yaxt=’n’, ylab=’’, main=title) axis(1, c(0, 0.5, 1)) for (i in 1:(length(lut)-1)) { x = (i-1)/scale + min rect(x,0,x+1/scale,10, col=lut[i], border=NA) } }\npal  =  colorRampPalette(c(\"grey50\", \"maroon\", \"magenta2\"))(101)\nvert.clrs  =  sapply(states(mcPreg), function(x) pal[1+round(100*premat[x,\"Preterm\"])] )\nvert.sz  =  4 + 2*sapply(states(mcPreg),\n              function(x) nrow(unique(sample_data(ps)[sample_data(ps)$CST==x,\"SubjectID\"])))\nvert.sz  =  vert.sz * 0.85\nvert.font.clrs  =  c(\"white\", \"white\", \"white\", \"white\", \"white\")\n\n# E(netMC)로 에지 리스트 확인, 루프 각도는 정점이 아닌 에지 리스트 번호별로 개별적으로 정의해야 함\nedge.loop.angle = c(0, 0, 0, 0, 3.14, 3.14, 0, 0, 0, 0, 3.14, 0, 0, 0, 0, 0)-0.45\nlayout  =  matrix(c(0.6,0.95, 0.43,1, 0.3,0.66, 0.55,0.3, 0.75,0.65), nrow = 5, ncol = 2, byrow = TRUE)\n\n# 조산 연관성에 따른 색상\nlayout(matrix(c(1,1,2,2), 2, 2, byrow = TRUE), heights=c(1,10))\ncolor.bar(pal, min=0, max=1, nticks=6, title=\"조산 비율\")\npar(mar=c(0,1,1,1)+0.1)\nedge.arrow.size=0.8\nedge.arrow.width=1.4\nedge.width = (15*wts + 0.1)*0.6\nedge.labels  =  as.character(E(netMC)$weight/100)\nedge.labels[edge.labels&lt;0.4]  =  NA  # 셀프 루프를 위한 레이블만\nplotMC(mcPreg, edge.arrow.size=edge.arrow.size, edge.arrow.width = edge.arrow.width,\n       edge.width=edge.width, edge.curved=edge.curved,\n       vertex.color=vert.clrs, vertex.size=(vert.sz),\n       vertex.label.font = 2, vertex.label.cex = 1, \n       vertex.label.color = vert.font.clrs, vertex.frame.color = NA, \n       layout=layout, edge.loop.angle = edge.loop.angle)\npar(default.par)\n연습 문제 10.4\n단백질 상호작용 네트워크 : STRING 데이터베이스 (http://www.string-db.org)에 관한 위키백과 문서를 읽어보세요.\nCyclin B1 단백질은 CCNB1 유전자에 의해 인코딩됩니다. 위키백과 (https://en.wikipedia.org/wiki/Cyclin_B1)에서 이에 대해 읽을 수 있습니다.\nSTRING을 사용하여 CCNB1 유전자 주변의 에지 텍스트 파일(ccnb1datsmall.txt 라고 부릅시다)을 생성하세요. 0.9보다 높은 신뢰도로 공동 발현의 증거에 의해 연결된 노드들을 선택하세요. 50개 이하의 상호작용과 그래프에서 CCNB1로부터 두 단계 떨어진 추가 노드들을 수집하세요.\n해결책\n\nhttp://www.string-db.org 에 접속합니다.\n단백질 이름으로 CCNB1을, 유기체로 Homo sapiens를 입력합니다. “Continue!”를 클릭합니다.\nCCNB1 단백질이 있는 옵션(가장 위쪽 것)을 선택합니다.\n“info and Parameters …”로 스크롤을 내립니다. 4a. Active Prediction Methods에서 – “Co-Expression”을 제외한 모든 것을 선택 해제합니다. 4b. Required confidence에서 – “highest confidence (0.900)”을 선택합니다. 4c. For interactors shown에서 – “no more than 50 interactors”를 선택합니다. 4d. For additional (white) nodes에서 – “100”을 선택합니다 (이들은 CCNB1로부터 두 단계 떨어진 노드들입니다). 4e. “Update Parameters”를 클릭합니다. 아래 이미지와 같은 결과가 나와야 합니다.\n그림 아래의 “save”(디스켓 모양)를 클릭합니다. 데이터를 저장할 형식을 선택할 수 있는 새 창이 열립니다.\n“Text Summary (TXT - simple tab delimited flatfile)” 파일로 스크롤을 내려서 해당 문서를 ccnb1datsmall.txt 로 저장합니다.\n\n연습 문제 10.5\nccnb1datsmall.txt 파일을 R로 읽어 들이고 이 장에서 다룬 그래프 시각화 방법 중 하나를 사용하여 그래프 플롯을 만드세요.\n해결책\n\n그림 10.32: 이 네트워크는 STRING 웹사이트에서 공동 발현 수준 () 0.900에 대해 CCNB1 유전자 주변의 2단계 이웃을 설정하여 생성되었습니다.\ndat = read.table(file.path(“../data/ccnb1datsmall.txt”), header = TRUE, comment.char = ““, stringsAsFactors = TRUE) v = levels(unlist(dat[,1:2])) # 정점 이름 n = length(v) # 정점 수 e = matrix(match(as.character(unlist(dat[,1:2])), v), ncol=2) # 에지 리스트 w = dat$coexpression # 에지 가중치\nM은 우리의 공동 발현 네트워크 인접 행렬입니다. STRING 데이터는 단지 단백질 i와 j가 공동 발현되는지만 말해주고 (i,j)와 (j,i)를 구별하지 않으므로, (i,j)의 가중치가 (j,i)와 같다고 간주하여 M을 대칭(무향)으로 만들고 싶습니다. A는 우리의 공동 발현 그래프 인접 행렬이며 공동 발현되는 경우 (A_{ij} = 1)로 만듭니다.\nM = matrix(0, n, n) M[e] = w M = M + t(M) dimnames(M) = list(v, v) A = 1*(M &gt; 0)\n우리는 기본 플로팅 매개변수를 사용하고 에지 벡터 e로부터 시작하여 igraph 패키지를 사용해 그래프를 생성합니다 (대안으로는 인접 행렬 A를 사용하는 것이 있습니다).\n참고: 그래프가 항상 동일하게 보이도록 하기 위해 시드(seed)를 사용합니다. 그래프 레이아웃은 종종 무작위 성분을 포함한 최적화를 수반하므로 그래프 자체는 같더라도 그림이 다르게 보일 수 있습니다.\nlibrary(igraph)\nnet = network(e, directed=FALSE)\npar(mar=rep(0,4))\nplot(net, label=v)\nggraph 를 사용하여 그래프를 만들 수도 있습니다.\n연습 문제 10.6\n연습 문제 10.5에서 생성된 그래프의 인접 행렬을 보여주는 히트맵을 만드세요.\n해결책\n기본값 외에 히트맵을 만드는 데 사용되는 옵션은 색상을 변경하는 것입니다. 추가 매개변수를 실험하고 추가할 수 있습니다.\nbreaks  =  c(0, seq(0.9, 1, length=11))\ncols  =  grey(1-c(0,seq(0.5,1,length=10)))\nccnb1ind  =  which(v == \"CCNB1\")\nvcols  =  rep(\"white\",n)\nvcols[ccnb1ind]  =  \"blue\"\nvcols[which(M[,ccnb1ind]&gt;0 | M[ccnb1ind,])]  =  \"red\"\npar(mar = rep(0, 4))\nheatmap(M, symm = TRUE, ColSideColors = vcols, RowSideColors = vcols,\n        col = cols, breaks = breaks,  frame = TRUE)\nlegend(\"topleft\", c(\"Neighbors(CCNB1)\", \"CCNB1\"),\n       fill = c(\"red\",\"blue\"),\n       bty = \"n\", inset = 0, xpd = TRUE,  border = FALSE)\n\n그림 10.33: 이것은 CCNB1 네트워크의 인접성을 나타냅니다 – 공동 발현 수준 () 0.900의 2단계 이웃, R에서 생성됨 (어두울수록 1에 가까움, 0.9 미만 값은 무시).\n연습 문제 10.7\n시각화는 CCNB1의 2단계 이웃에서 가장 강한 상호작용을 보여줍니다. 플롯된 그래프와 히트맵 이미지 모두 동일한 데이터를 보여줍니다: CCNB1과 유사한 단백질 클러스터가 있고 다른 단백질들도 다른 클러스터가 있는 것 같습니다. CCNB1 클러스터의 많은 단백질들이 서로 동시에 공동 발현됩니다.\n이것이 왜 그럴 수 있을까요? 반대로, CCNB1과 공동 발현되는 단백질과 공동 발현되는 단백질(두 단계 떨어진)은 서로 공동 발현되는 경향이 없습니다. 두 그림 중 (플롯 또는 히트맵) 하나에서 이것을 다른 것보다 더 쉽게 볼 수 있나요?\n연습 문제 10.8\nHIV GAG 데이터 분석에서 ape 와 phangorn 의 사용을 비교하세요. 두 패키지를 사용하여 서열 간의 Jukes Cantor 거리를 계산하고 Hamming 거리와 비교하세요.\nlibrary(\"ape\")\nlibrary(\"phangorn\")\nGAG = read.dna(file.path(\"../data/DNA_GAG_20.txt\"))\n연습 문제 10.9\nBray-Curtis 비유사성을 사용하여 “two-nearest” 이웃 그래프로 프리드먼-라프스키 유형 검정을 수행하세요.\n해결책\ngt = graph_perm_test(ps1, “family_relationship”, distance = “bray”, grouping = “host_subject_id”, type = “knn”, knn = 2) gt$pval\n[1] 0.004\n\n\nplot_test_network(gt)\npermdf = data.frame(perm=gt$perm)\nobs = gt$observed\nymax = max(gt$perm)\nggplot(permdf, aes(x = perm)) + geom_histogram(bins = 20) + \n  geom_segment(aes(x = obs, y = 0, xend = obs, yend = ymax/10), color = \"red\") + \n  geom_point(aes(x = obs, y = ymax/10), color = \"red\") + xlab(\"Number of pure edges\")\n\n\n( ext{})\n\n\n\n( ext{})\n\n그림 10.34: 자카드 유사성을 이용한 최근접 이웃 그래프 (a) 및 순열 히스토그램 (b).\nBeisser, Daniela, Gunnar W Klau, Thomas Dandekar, Tobias Müller, and Marcus T Dittrich. 2010. “BioNet: An R-Package for the Functional Analysis of Biological Networks.” Bioinformatics 26 (8): 1129–30.\nBenjamini, Yoav, and Marina Bogomolov. 2014. “Selective Inference on Multiple Families of Hypotheses.” Journal of the Royal Statistical Society: Series B 76 (1): 297–318.\nBenjamini, Yoav, and Daniel Yekutieli. 2003. “Hierarchical FDR Testing of Trees of Hypotheses.” Technical report, Department of Statistics; Operations Research, Tel Aviv University.\nBhattacharya, Bhaswar B. 2015. “Power of Graph-Based Two-Sample Tests.” arXiv Preprint arXiv:1508.07530.\nBouckaert, Remco, Joseph Heled, Denise Kühnert, Tim Vaughan, Chieh-Hsi Wu, Dong Xie, Marc A Suchard, Andrew Rambaut, and Alexei J Drummond. 2014. “BEAST 2: A Software Platform for Bayesian Evolutionary Analysis.” PLoS Computational Biology 10 (4): e1003537.\nCallahan, Benjamin J, Paul J McMurdie, and Susan P Holmes. 2017. “Exact Sequence Variants Should Replace Operational Taxonomic Units in Marker Gene Data Analysis.” ISME Journal , 1–5.\nChakerian, John, and Susan Holmes. 2012. “Computational Tools for Evaluating Phylogenetic and Hierarchical Clustering Trees.” Journal of Computational and Graphical Statistics 21 (3): 581–99.\nCole, J. R., Q. Wang, E. Cardenas, J. Fish, B. Chai, R. J. Farris, A. S. Kulam-Syed-Mohideen, et al. 2009. “The Ribosomal Database Project: Improved Alignments and New Tools for rRNA Analysis.” Nucleic Acids Research 37 (Supplement 1): D141–45.\nDiGiulio, Daniel B., Benjamin J. Callahan, Paul J. McMurdie, Elizabeth K. Costello, Deirdre J. Lyelle, Anna Robaczewska, Christine L. Sun, et al. 2015. “Temporal and Spatial Variation of the Human Microbiota During Pregnancy.” PNAS.\nFelsenstein, Joseph. 2004. Inferring Phylogenies. Boston: Sinauer.\nFriedman, Jerome H, and Lawrence C Rafsky. 1979. “Multivariate Generalizations of the Wald-Wolfowitz and Smirnov Two-Sample Tests.” The Annals of Statistics , 697–717.\nFukuyama, Julia, Paul J McMurdie, Les Dethlefsen, David A Relman, and Susan Holmes. 2012. “Comparisons of Distance Methods for Combining Covariates and Abundances in Microbiome Studies.” In Pac Symp Biocomput. World Scientific.\nHolmes, Susan. 1999. “Phylogenetic Trees: An Overview.” In Statistics and Genetics , 81–118. IMA 112. New York: Springer.\n———. 2003a. “Bootstrapping Phylogenetic Trees: Theory and Methods.” Statistical Science 18 (2): 241–55.\n———. 2003b. “Statistics for phylogenetic trees.” Theoretical Population Biology 63 (1): 17–32.\nIdeker, Trey, Owen Ozier, Benno Schwikowski, and Andrew F Siegel. 2002. “Discovering Regulatory and Signalling Circuits in Molecular Interaction Networks.” Bioinformatics 18 Suppl 1 (January): S233–40. http://bioinformatics.oxfordjournals.org/cgi/reprint/18/suppl_1/S233.\nLi, Wen-Hsiung. 1997. Molecular Evolution. Sinauer Associates Incorporated.\nLi, Wen-Hsiung, and Dan Graur. 1991. Fundamentals of Molecular Evolution. Vol. 48. Sinauer Associates Sunderland, MA.\nLiberzon, Arthur, Aravind Subramanian, Reid Pinchback, Helga Thorvaldsdóttir, Pablo Tamayo, and Jill P Mesirov. 2011. “Molecular Signatures Database (MSigDB) 3.0.” Bioinformatics 27 (12): 1739–40.\nLove, Michael I, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation of Fold Change and Dispersion for RNA-seq Data with DESeq2.” Gnome Biology 15 (12): 1–21.\nMcMurdie, Paul J, and Susan Holmes. 2014. “Waste Not, Want Not: Why Rarefying Microbiome Data Is Inadmissible.” PLoS Computational Biology 10 (4): e1003531.\n———. 2015. “Shiny-Phyloseq: Web Application for Interactive Microbiome Analysis with Provenance Tracking.” Bioinformatics 31 (2): 282–83.\nMossel, Elchanan. 2003. “On the Impossibility of Reconstructing Ancestral Data and Phylogenies.” Journal of Computational Biology 10 (5): 669–76.\nNacu, Serban, Rebecca Critchley-Thorne, Peter Lee, and Susan Holmes. 2007. “Gene Expression Network Analysis and Applications to Immunology.” Bioinformatics 23 (7, 7): 850–58. https://doi.org/10.1093/bioinformatics/btm019.\nNolan, Daniel J, Michael Ginsberg, Edo Israely, Brisa Palikuqi, Michael G Poulos, Daylon James, Bi-Sen Ding, et al. 2013. “Molecular Signatures of Tissue-Specific Microvascular Endothelial Cell Heterogeneity in Organ Maintenance and Regeneration.” Developmental Cell 26 (2): 204–19.\nO’Neill, Kieran, Nima Aghaeepour, Josef Špidlen, and Ryan Brinkman. 2013. “Flow Cytometry Bioinformatics.” PLoS Computational Biology 9 (12): e1003365.\nOhnishi, Y., W. Huber, A. Tsumura, M. Kang, P. Xenopoulos, K. Kurimoto, A. K. Oles, et al. 2014. “Cell-to-Cell Expression Variability Followed by Signal Reinforcement Progressively Segregates Early Mouse Lineages.” Nature Cell Biology 16 (1): 27–37.\nRosen, Michael J, Benjamin J Callahan, Daniel S Fisher, and Susan P Holmes. 2012. “Denoising PCR-Amplified Metagenome Data.” BMC Bioinformatics 13 (1): 283.\nSchloss, P D, S L Westcott, T Ryabin, J R Hall, M Hartmann, E B Hollister, R A Lesniewski, et al. 2009. “Introducing mothur: Open-Source, Platform- Independent, Community-Supported Software for Describing and Comparing Microbial Communities.” Applied and Environmental Microbiology 75 (23): 7537–41.\nSchloss, P. D., A. M. Schuber, J. P. Zackular, K. D. Iverson, Young V. B., and Petrosino J. F. 2012. “Stabilization of the Murine Gut Microbiome Following Weaning.” Gut Microbes 3 (4): 383–93.\nSankaran, Kris, and Susan Holmes. 2014. “structSSI: Simultaneous and Selective Inference for Grouped or Hierarchically Structured Data.” Journal of Statistical Software 59 (1): 1–21.\nSchilling, Mark F. 1986. “Multivariate Two-Sample Tests Based on Nearest Neighbors.” Journal of the American Statistical Association 81 (395): 799–806.\nSchölkopf, Bernhard, Koji Tsuda, and Jean-Philippe Vert. 2004. Kernel Methods in Computational Biology. MIT press.\nWang, Q., G. M. Garrity, J. M. Tiedje, and J. R. Cole. 2007. “Naive Bayesian Classifier for Rapid Assignment of rRNA Sequences into the New Bacterial Taxonomy.” Applied and Environmental Microbiology 73 (16): 5261.\nWertheim, Joel O, and Michael Worobey. 2009. “Dating the Age of the SIV Lineages That Gave Rise to HIV-1 and HIV-2.” PLoS Computational Biology 5 (5): e1000377.\nWright, Erik S. 2015. “DECIPHER: Harnessing Local Sequence Context to Improve Protein Multiple Sequence Alignment.” BMC Bioinformatics 16 (1): 1.\nYu, Hongxiang, Diana L Simons, Ilana Segall, Valeria Carcamo-Cavazos, Erich J Schwartz, Ning Yan, Neta S Zuckerman, et al. 2012. “PRC2/EED-EZH2 Complex Is up-Regulated in Breast Cancer Lymph Node Metastasis Compared to Primary Tumor and Correlates with Tumor Proliferation in Situ.” PloS One 7 (12): e51239.\n페이지는 R 버전 4.5.1 (2025-06-13)을 사용하여 2025-09-01 01:33에 빌드되었습니다.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>네트워크와 트리 (Networks and Trees)</span>"
    ]
  },
  {
    "objectID": "11-chap.html",
    "href": "11-chap.html",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "",
    "text": "13.1 11.1 이 장의 목표\n이미지는 풍부한 데이터 소스입니다. 이 장에서 우리는 이미지에서 어떻게 정량적 정보를 추출할 수 있는지, 그리고 데이터를 요약하고 이해하기 위해 통계적 방법을 어떻게 사용할 수 있는지 살펴볼 것입니다. 이 장의 목표는 이미지 데이터로 작업을 시작하는 것이 쉽다는 것을 보여주는 것입니다. 기본적인 R 환경을 다룰 수 있다면 이미지 작업을 시작할 준비가 된 것입니다. 그렇긴 하지만, 이 장이 이미지 분석에 대한 일반적인 입문서는 아닙니다. 이 분야는 광범위하며 신호 처리, 정보 이론, 수학, 공학 및 컴퓨터 과학의 많은 영역을 다루고 있으며, 체계적인 개요를 제시하는 훌륭한 책들이 많이 있습니다.\n우리는 주로 일련의 2차원 이미지, 특히 세포 이미지를 공부할 것입니다. 세포의 위치와 모양을 식별하는 방법과 식별된 모양 및 패턴의 특성(예: 크기, 강도, 색상 분포 및 상대적 위치)을 정량적으로 측정하는 방법을 배울 것입니다. 이러한 정보는 후속 분석에 사용될 수 있습니다. 예를 들어 서로 다른 조건(예: 서로 다른 약물의 효과 하에 있거나 분화 및 성장 단계가 다른 경우) 사이의 세포를 비교할 수 있습니다. 또는 이미지의 객체들이 서로 어떻게 연관되어 있는지 측정할 수 있습니다. 예를 들어 객체들이 함께 모이거나 서로 밀어내는 것을 선호하는지, 아니면 이웃한 객체들 사이에 특정 특성이 공유되는 경향이 있는지(세포 간 통신을 나타냄) 등을 측정할 수 있습니다. 유전학의 언어로 말하자면, 이미지를 복잡한 표현형이나 다변량 양적 형질로 사용할 수 있다는 뜻입니다.\n여기서 우리는 2차원 이상의 이미지 분석은 다루지 않을 것입니다. 3D 분할(segmentation) 및 등록(registration)이나 시간적 추적(temporal tracking)은 고려하지 않을 것입니다. 이것들은 정교한 작업이며, 이 장의 범위 내에서 우리가 조립할 수 있는 것보다 전문화된 소프트웨어가 아마도 더 나은 성능을 보일 것입니다.\n고처리량 이미징 데이터와 유전체학의 다른 고처리량 데이터 사이에는 유사점이 있습니다. 예를 들어 염색 효율의 변화, 조명 또는 기타 여러 요인으로 인해 배치 효과(batch effects)가 작용하는 경향이 있습니다. 실험 설계와 분석 선택에서 적절한 예방 조치를 취해야 합니다. 원칙적으로 이미지의 강도 값은 물리적 단위(예: 복사 에너지 또는 형광체 농도)로 보정될 수 있습니다. 하지만 생물학적 이미징 실무에서 이것이 항상 수행되는 것은 아니며, 아마도 필요하지 않을 수도 있습니다. 다소 달성하기 쉽고 분명히 가치 있는 것은 이미지의 공간적 차원에 대한 보정, 즉 픽셀 단위와 미터법 거리 사이의 변환 계수입니다.\n이 장에서 우리는 다음을 수행할 것입니다:",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#이-장의-목표",
    "href": "11-chap.html#이-장의-목표",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "",
    "text": "R에서 이미지를 읽고, 쓰고, 조작하는 방법을 배웁니다.\n이미지에 필터와 변환을 적용하는 방법을 이해합니다.\n이러한 기술을 결합하여 분할(segmentation) 및 특징 추출(feature extraction)을 수행합니다. 세포 분할을 예로 사용할 것입니다.\n공간 분포와 의존성을 분석하기 위해 통계적 방법을 사용하는 방법을 배웁니다.\n공간 점 과정(spatial point process)에 대한 가장 기본적인 분포인 균질 포아송 과정(homogeneous Poisson process)에 대해 알아봅니다.\n여러분의 데이터가 그 기본 가정에 부합하는지, 아니면 덩어리짐(clumping)이나 배제(exclusion)의 증거를 보이는지 인식합니다.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#이미지-로드",
    "href": "11-chap.html#이미지-로드",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.2 11.2 이미지 로드",
    "text": "13.2 11.2 이미지 로드\nR에서 이미지를 처리하는 데 유용한 툴킷은 바이오컨덕터 패키지인 EBImage (Pau et al. 2010)입니다. 기본 기능을 시연하기 위해 간단한 사진을 읽어 들이는 것으로 시작합니다.\nlibrary(\"EBImage\")\nimagefile = system.file(\"images\", \"mosquito.png\", package = \"MSMB\")\nmosq = readImage(imagefile)\nEBImage는 현재 세 가지 이미지 파일 형식인 jpeg, png, tiff를 지원합니다. 위에서 우리는 MSMB 패키지의 샘플 이미지를 로드했습니다. 자신의 데이터로 작업할 때는 그 패키지가 필요하지 않으며, readImage 함수에 파일 이름을 제공하기만 하면 됩니다. 이 장의 뒷부분에서 보게 되겠지만, readImage는 한 번에 여러 이미지를 읽을 수 있으며, 이들은 모두 단일 이미지 데이터 객체로 조립됩니다. 이것이 작동하려면 이미지들이 동일한 차원과 색상 모드를 가져야 합니다.\n질문 11.1\nRBioFormats 패키지(GitHub에서 사용 가능: https://github.com/aoles/RBioFormats)는 더 많은 이미지 파일 형식을 읽고 쓰는 기능을 제공합니다. 얼마나 많은 서로 다른 파일 형식이 지원되나요?\n해결책\nRBioFormats 패키지의 read.image 함수 매뉴얼 페이지(이는 EBImage::readImage와는 다름에 유의)와 The Open Microscopy Environment 웹사이트의 Bio-Formats 프로젝트 온라인 문서(http://www.openmicroscopy.org/site/support/bio-formats5.5/supported-formats.html)를 참조하세요.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#이미지-표시",
    "href": "11-chap.html#이미지-표시",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.3 11.3 이미지 표시",
    "text": "13.3 11.3 이미지 표시\n방금 읽어 들인 이미지를 시각화해 봅시다. 기본 함수는 EBImage::display입니다.\nEBImage::display(mosq)\n위의 명령은 웹 브라우저 창(getOption(\"browser\")에 의해 설정됨)에서 이미지를 엽니다. 마우스나 키보드 단축키를 사용하여 이미지를 확대하거나 축소하고, 팬(pan) 이동을 하거나 여러 이미지 프레임을 순환하며 볼 수 있습니다.\n또는 method = \"raster\" 인수를 사용하여 display를 호출함으로써 R의 내장 플로팅 기능을 사용하여 이미지를 표시할 수도 있습니다. 그러면 이미지는 현재 장치로 출력됩니다. 이런 방식으로 이미지 데이터를 다른 플로팅 기능과 결합하여 예를 들어 텍스트 레이블을 추가할 수 있습니다.\nEBImage::display(mosq)\ntext(x = 85, y = 800, label = \"A mosquito\", adj = 0, col = \"orange\", cex = 1.5)\n\n그림 11.1: 조지아주 디케이터 교외에서 사체로 발견된 모기 (출처: CDC / Janice Haney Carr).\n결과 플롯은 그림 11.1에 나와 있습니다. 평소와 같이 R 장치에 표시된 그래픽은 base R 함수인 dev.print나 dev.copy를 사용하여 저장할 수 있습니다.\n컬러 이미지를 읽고 볼 수도 있습니다(그림 11.2 참조).\nimagefile = system.file(\"images\", \"hiv.png\", package = \"MSMB\")\nhivc = readImage(imagefile)\n\n\nEBImage::display(hivc, method = \"raster\")\n\n그림 11.2: 배양된 림프구에서 싹이 트고 있는 HIV-1 비리온의 주사 전자 현미경 사진 (출처: CDC / C. Goldsmith, P. Feorino, E.L. Palmer, W.R. McManus).\n또한 이미지에 여러 프레임이 있는 경우, all = TRUE 인수를 지정하여 그리드 배열로 한 번에 모두 표시할 수 있습니다(그림 11.3).\nnuc = readImage(system.file(\"images\", \"nuclei.tif\", package = \"EBImage\"))\nEBImage::display(1 - nuc, all = TRUE)\n\n그림 11.3: EBImage 패키지에 포함된 세포 핵 이미지 4개를 타일 형식으로 표시한 모습.\n또는 예를 들어 두 번째 프레임과 같이 단일 프레임만 볼 수도 있습니다.\nEBImage::display(1 - nuc, frame = 2)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#r에서-이미지는-어떻게-저장되나요",
    "href": "11-chap.html#r에서-이미지는-어떻게-저장되나요",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.4 11.4 R에서 이미지는 어떻게 저장되나요?",
    "text": "13.4 11.4 R에서 이미지는 어떻게 저장되나요?\n먼저 이미지 객체의 클래스를 식별하여 무슨 일이 일어나고 있는지 살펴봅시다.\nclass(mosq)\n\n\n[1] \"Image\"\nattr(,\"package\")\n[1] \"EBImage\"\n이 객체가 Image 클래스를 가지고 있음을 알 수 있습니다. 이것은 기본 R 클래스 중 하나가 아니라 EBImage 패키지에 의해 정의된 것입니다. 도움말 브라우저를 통하거나 class?Image를 입력하여 이 클래스에 대해 더 자세히 알아볼 수 있습니다. 이 클래스는 기본 R 클래스인 array 에서 파생되었으므로, R 배열로 할 수 있는 모든 작업을 Image 객체로도 할 수 있습니다. 게다가 몇 가지 추가적인 기능과 동작들을 가지고 있습니다1.\n1 R의 용어로 추가적인 기능은 슬롯(slots)이라고 불리고 동작들은 메서드(methods)라고 불립니다. 메서드는 함수의 일종입니다.\n질문 11.3\nImage 객체의 슬롯이 무엇인지, 그리고 어떤 메서드를 적용할 수 있는지 어떻게 알 수 있나요?\n해결책\n클래스 정의는 간단합니다. showClass(\"Image\")로 접근할 수 있습니다. R 함수 호출을 통해 Image 클래스에 적용 가능한 모든 메서드를 찾는 것은 번거로운 일입니다. 가장 좋은 방법은 클래스의 매뉴얼 페이지를 참조하여 저자가 언급하기로 선택한 메서드를 확인하는 것입니다.\n이미지의 차원은 일반적인 배열과 마찬가지로 dim 메서드를 사용하여 추출할 수 있습니다.\ndim(mosq)\n\n\n[1] 1400  952\nhist 메서드는 배열에 대한 일반적인 hist 함수에 비해 재정의되었습니다2. 이는 서로 다르며 아마도 더 유용한 기본값을 사용합니다(그림 11.4).\n2 객체 지향 용어로는 오버로드(overloaded)라고 합니다.\nhist(mosq)\n\n그림 11.4: mosq의 픽셀 강도 히스토그램. 범위가 0과 1 사이임에 주목하세요.\n데이터 행렬에 R array 로 직접 접근하고 싶다면, 접근자 함수 imageData를 사용할 수 있습니다.\nimageData(mosq)[1:3, 1:6]\n\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n[1,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784\n[2,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784\n[3,] 0.1960784 0.1960784 0.2000000 0.2039216 0.2000000 0.1960784\n단순히 객체의 이름을 입력하면 Image 객체에 대한 유용한 요약 정보가 출력됩니다.\nmosq\n\n\nImage \n  colorMode    : Grayscale \n  storage.mode : double \n  dim          : 1400 952 \n  frames.total : 1 \n  frames.render: 1 \n\nimageData(object)[1:5,1:6]\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n[1,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784\n[2,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784\n[3,] 0.1960784 0.1960784 0.2000000 0.2039216 0.2000000 0.1960784\n[4,] 0.1960784 0.1960784 0.2039216 0.2078431 0.2000000 0.1960784\n[5,] 0.1960784 0.2000000 0.2117647 0.2156863 0.2000000 0.1921569\n이제 컬러 이미지를 살펴봅시다.\nhivc\n\n\nImage \n  colorMode    : Color \n  storage.mode : double \n  dim          : 1400 930 3 \n  frames.total : 3 \n  frames.render: 1 \n\nimageData(object)[1:5,1:6,1]\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    0    0    0    0    0    0\n[2,]    0    0    0    0    0    0\n[3,]    0    0    0    0    0    0\n[4,]    0    0    0    0    0    0\n[5,]    0    0    0    0    0    0\n두 이미지는 colorMode 속성에서 차이가 납니다. mosq는 Grayscale이고 hivc는 Color입니다. 이 속성의 요점은 무엇일까요? 이미지 스택(stacks)을 다룰 때 편리하다는 것이 밝혀졌습니다. 만약 colorMode가 Grayscale이라면, 배열의 세 번째 이상의 모든 차원은 예를 들어 서로 다른 (z)-위치, 시점, 반복 실험 등에 해당하는 별개의 이미지 프레임으로 간주됩니다. 반면에 colorMode가 Color라면, 세 번째 차원은 서로 다른 색상 채널을 갖는 것으로 간주되며, 네 번째 이상의 차원만이 –존재한다면– 여러 이미지 프레임을 위해 사용됩니다. hivc에는 사진의 빨간색, 녹색, 파란색 강도에 해당하는 세 개의 색상 채널이 있습니다. 하지만 이것이 반드시 그럴 필요는 없으며, 색상 채널의 수는 얼마든지 될 수 있습니다.\n질문 11.4\nR이 데이터 nuc를 어떻게 저장하는지 설명하세요.\n해결책\nnuc\n\n\nImage \n  colorMode    : Grayscale \n  storage.mode : double \n  dim          : 510 510 4 \n  frames.total : 4 \n  frames.render: 4 \n\nimageData(object)[1:5,1:6,1]\n           [,1]       [,2]       [,3]       [,4]       [,5]       [,6]\n[1,] 0.06274510 0.07450980 0.07058824 0.08235294 0.10588235 0.09803922\n[2,] 0.06274510 0.05882353 0.07843137 0.09019608 0.09019608 0.10588235\n[3,] 0.06666667 0.06666667 0.08235294 0.07843137 0.09411765 0.09411765\n[4,] 0.06666667 0.06666667 0.07058824 0.08627451 0.08627451 0.09803922\n[5,] 0.05882353 0.06666667 0.07058824 0.08235294 0.09411765 0.10588235\n\n\ndim(imageData(nuc))\n\n\n[1] 510 510   4\n우리는 총 4개의 프레임을 가지고 있음을 알 수 있으며, 이는 4개의 개별 이미지(frames.render)에 해당합니다.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#이미지-파일-쓰기",
    "href": "11-chap.html#이미지-파일-쓰기",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.5 11.5 이미지 파일 쓰기",
    "text": "13.5 11.5 이미지 파일 쓰기\n이전 섹션에서 보았던 배열 표현 그대로 이미지를 디스크에 저장하는 것은 불필요하게 큰 파일 크기를 초래할 수 있습니다. 저장 공간 소비를 줄이기 위해 압축 알고리즘을 사용하는 것이 일반적입니다. 이미지 압축 3에는 크게 두 가지 유형이 있습니다:\n3 유추하자면 영화와 음악도 마찬가지입니다.\n\n무손실 압축(Lossless compression): 압축된 파일로부터 원래의 이미지 데이터를 정확하게 재구성할 수 있습니다. 무손실 압축의 간단한 원리는 다음과 같습니다: (1) 픽셀을 표현하는 데 필요한 것보다 많은 비트를 소비하지 않습니다(예: mosq 이미지의 픽셀은 256단계의 회색조 범위를 가지며 이는 8비트로 표현될 수 있지만, mosq는 이를 64비트 수치 형식으로 저장합니다 4). (2) 패턴을 식별하고(앞서 출력된 mosq와 hivc의 픽셀 값에서 본 것과 같은), 이를 훨씬 짧은 규칙으로 대신 표현합니다.\n손실 압축(Lossy compression): 어차피 인간 관찰자가 알아채지 못할 것 같은 세부 사항들을 버림으로써 무손실 압축에 비해 추가적인 절감을 이뤄냅니다.\n\n4 비록 이것이 메모리 측면에서는 어느 정도 낭비일 수 있지만, R의 나머지 작동 방식과 더 호환되며 현대 컴퓨터 하드웨어에서 제한 요인이 되는 경우는 드뭅니다.\n무손실 압축을 사용하는 저장 형식의 예로는 PNG 5가 있고, 손실 압축의 예로는 JPEG 6 형식이 있습니다. JPEG는 여러분의 휴가 사진에는 좋지만, 과학적 이미지는 무손실 형식으로 저장하는 것이 좋은 습관입니다.\n우리는 PNG 형식의 파일에서 이미지 hivc를 읽었으므로, 이제 이를 JPEG 파일로 써봅시다. 손실 정도는 1(최악)에서 100(최선) 사이의 값을 갖는 quality 매개변수로 지정됩니다.\noutput_file = file.path(tempdir(), \"hivc.jpeg\")\nwriteImage(hivc, output_file, quality = 85)\n유사하게, 우리는 이미지를 TIFF 파일로 쓸 수도 있었고 여러 압축 알고리즘 중에서 선택할 수 있었습니다(writeImage와 writeTiff 함수의 매뉴얼 페이지 참조). RBioFormats 패키지를 사용하면 더 많은 이미지 파일 형식으로 쓸 수 있습니다.\n질문 11.5\nR의 메모리에서 hivc 객체의 크기는 얼마인가요? JPEG 파일의 크기는요? 3가지 색상의 1600만 화소(Megapixel) 이미지가 차지할 것으로 예상되는 RAM 용량은 얼마인가요?\n해결책\nobject.size(hivc) |&gt; format(units = \"Mb\")\n\n\n[1] \"29.8 Mb\"\n\n\n(object.size(hivc) / prod(dim(hivc))) |&gt; format() |&gt; paste(\"per pixel\")\n\n\n[1] \"8 bytes per pixel\"\n\n\nfile.info( output_file )$size\n\n\n[1] 294904\n\n\n16 * 3 * 8\n\n\n[1] 384\n5 https://en.wikipedia.org/wiki/Portable_Network_Graphics 6 https://en.wikipedia.org/wiki/JPEG",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#이미지-조작",
    "href": "11-chap.html#이미지-조작",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.6 11.6 이미지 조작",
    "text": "13.6 11.6 이미지 조작\n이제 이미지가 R에서 숫자 배열로 저장된다는 것을 알았으므로, 이미지를 조작하는 방법이 명확해졌습니다 – 간단한 대수학입니다! 예를 들어, 그림 11.5a에 다시 표시된 원래 이미지를 가져와서 이미지에 -1을 곱함으로써 밝은 영역을 어둡게, 어두운 영역을 밝게 뒤집을 수 있습니다 (그림 11.5b).\nmosqinv = normalize(-mosq)\n질문 11.6\nnormalize 함수는 무엇을 하나요?\n우리는 또한 곱셈을 통해 대비를 조정할 수 있고 (그림 11.5c), 거듭제곱 변환을 통해 감마 인자를 조정할 수 있습니다 (그림 11.5d).\nmosqcont = mosq * 3\nmosqexp = mosq ^ (1/3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n그림 11.5: 원래의 모기 이미지 (a)와 세 가지 서로 다른 이미지 변환: (b) 뺄셈, (c) 곱셈, (d) 거듭제곱 변환.\n더 나아가, 우리는 행렬 연산을 통해 이미지를 자르고(crop), 임계값 처리(threshold)하고, 전치(transpose)할 수 있습니다 (그림 11.6).\nmosqcrop   = mosq[100:438, 112:550]\nmosqthresh = mosq &gt; 0.5\nmosqtransp = transpose(mosq)\n\n\n\n\n\n\n\n\n\n\n\n\n그림 11.6: 세 가지 추가 이미지 변환: (a) 자르기, (b) 임계값 처리, (c) 전치.\n질문 11.7\n임계값 처리의 결과인 mosqthresh는 어떤 데이터 유형인가요?\n해결책\n픽셀이 R의 logical 유형 배열로 표현된 이진 값을 가지는 Image 객체입니다. 콘솔에 객체 이름을 입력하여 확인할 수 있습니다.\nmosqthresh\n\n\nImage \n  colorMode    : Grayscale \n  storage.mode : logical \n  dim          : 1400 952 \n  frames.total : 1 \n  frames.render: 1 \n\nimageData(object)[1:5,1:6]\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n[1,] FALSE FALSE FALSE FALSE FALSE FALSE\n[2,] FALSE FALSE FALSE FALSE FALSE FALSE\n[3,] FALSE FALSE FALSE FALSE FALSE FALSE\n[4,] FALSE FALSE FALSE FALSE FALSE FALSE\n[5,] FALSE FALSE FALSE FALSE FALSE FALSE\n질문 11.8\n위와 같이 transpose 함수를 사용하는 대신, R의 base 함수 t를 사용할 수도 있을까요?\n해결책\n이 경우 t(mosq)와 transpose(mosq)의 값은 동일하지만, transpose는 컬러 및 다중 프레임 이미지에서도 작동하므로 더 선호됩니다.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#공간-변환",
    "href": "11-chap.html#공간-변환",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.7 11.7 공간 변환",
    "text": "13.7 11.7 공간 변환\n우리는 방금 공간 변환의 한 유형인 전치를 보았지만, 그 외에도 많은 변환이 있습니다. 여기에 몇 가지 예시가 있습니다:\nmosqrot   = EBImage::rotate(mosq, angle = 30)\nmosqshift = EBImage::translate(mosq, v = c(100, 170))\nmosqflip  = flip(mosq)\nmosqflop  = flop(mosq)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n그림 11.7: 공간 변환: (a) 회전, (b) 이동, (c) 중앙 수평축에 대한 반전(flip), (d) 중앙 수직축에 대한 반전(flop).\n위 코드에서 rotate 7 함수는 주어진 각도만큼 이미지를 시계 방향으로 회전시키고, translate는 지정된 2차원 벡터만큼 이미지를 이동시킵니다(이미지 영역 밖으로 나가는 픽셀은 잘려나가고, 이미지 영역으로 들어오는 픽셀은 0으로 설정됩니다). flip과 flop 함수는 각각 이미지를 중앙 수평축과 수직축을 기준으로 반사시킵니다. 이러한 연산들의 결과가 그림 11.7에 나와 있습니다.\n7 나중에 첨부할 spatstat 패키지의 동일한 이름의 함수와 혼동을 피하기 위해 여기서는 네임스페이스 수식어 EBImage::를 붙여 호출합니다.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#선형-필터",
    "href": "11-chap.html#선형-필터",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.8 11.8 선형 필터",
    "text": "13.8 11.8 선형 필터\n이제 세포 생물학 분야의 응용으로 넘어가 봅시다. Laufer, Fischer 및 동료들에 의해 연구된 인간 암세포 이미지를 로드합니다 (Laufer et al. 2013). 이들은 그림 11.8에 나와 있습니다.\nimagefiles = system.file(\"images\", c(\"image-DAPI.tif\", \"image-FITC.tif\", \"image-Cy3.tif\"), package = \"MSMB\")\ncells = readImage(imagefiles)\n\n그림 11.8: 인간 대장암 세포(HCT116). 네 개의 이미지는 동일한 세포들을 보여줍니다: 가장 왼쪽 이미지는 세포 DNA의 DAPI 염색에 대응하고, 두 번째는 알파-튜불린에 대한 면역 염색, 세 번째는 액틴에 대응합니다. 이들은 회색조 이미지로 표시됩니다. 가장 오른쪽 이미지는 세 이미지를 RGB 이미지의 색상 채널(빨강: 액틴, 초록: 알파-튜불린, 파랑: DNA)로 겹쳐서 얻은 것입니다.\nImage 객체 cells는 340 () 490 () 3 크기의 3차원 배열이며, 마지막 차원은 3개의 개별 회색조 프레임이 있음을 나타냅니다. 우리의 목표는 이제 이 이미지들에서 개별 세포들을 계산적으로 식별하고 정량적으로 특성화하는 것입니다. 그 자체로는 소박한 목표일 수 있지만, Laufer 등은 2,048 () 2,048 픽셀 크기의 이미지를 690,000개 이상 보유하고 있었습니다. 우리는 그중 작은 영역이 잘려진 3개를 보고 있는 것입니다. 일단 우리가 명시된 목표를 달성하는 방법을 알게 된다면, 우리는 이러한 방대한 이미지 컬렉션에 우리의 능력을 적용할 수 있으며, 그것은 더 이상 소박한 목표가 아닙니다!\n\n13.8.1 11.8.1 막간: 이미지의 강도 스케일\n하지만 실제 작업을 시작하기 전에, 약간 지루한 데이터 변환 문제를 다루어야 합니다. 물론 이는 흔한 일입니다. 이미지의 동적 범위(최솟값과 최댓값)를 조사해 봅시다.\napply(cells, 3, range)\n\n\n      image-DAPI  image-FITC   image-Cy3\n[1,] 0.001586938 0.002899214 0.001663233\n[2,] 0.031204700 0.062485695 0.055710689\n최댓값들이 1보다 훨씬 작은 숫자임을 알 수 있습니다. 그 이유는 readImage 함수가 TIFF 이미지가 각 픽셀을 표현하기 위해 16비트 정수를 사용함을 인식하고, 데이터를 –R의 수치형 변수에서 흔히 그렇듯이– 배정밀도 부동 소수점 수 배열로 반환하기 때문입니다. 이때 정수 값(이론적 범위는 0에서 (2^{16}-1=65535)까지)은 부동 소수점 표현의 가수(mantissa)에 저장되고 지수는 이론적 범위가 구간 ([0,1])에 매핑되도록 선택됩니다. 하지만 이 이미지들을 생성하는 데 사용된 스캐너는 하위 11비트 또는 12비트만을 사용했으며, 이것이 이미지의 최댓값이 작은 이유입니다. 우리는 이 데이터를 다음과 같이 대략 ([0,1]) 범위를 덮도록 재조정할 수 있습니다 8.\n8 normalize 함수는 이미지의 스케일 조정에 대해 더 유연한 인터페이스를 제공합니다.\ncells[,,1]   = 32 * cells[,,1]\ncells[,,2:3] = 16 * cells[,,2:3]\napply(cells, 3, range)\n\n\n     image-DAPI image-FITC  image-Cy3\n[1,] 0.05078202 0.04638743 0.02661173\n[2,] 0.99855039 0.99977111 0.89137102\n이러한 2의 배수 곱셈은 저장된 데이터의 기저 정밀도에는 아무런 영향을 주지 않는다는 점을 기억합시다.\n\n\n13.8.2 11.8.2 평활화를 통한 노이즈 감소\n이제 이미지를 분석할 준비가 되었습니다. 우리의 첫 번째 목표는 개별 세포를 식별하기 위해 이미지를 분할하는 것이므로, 평활화(smoothing)를 통해 이미지에서 국소적인 아티팩트나 노이즈를 제거하는 것으로 시작할 수 있습니다. 직관적인 접근 방식은 각 픽셀 주변에 선택된 크기의 창(window)을 정의하고 그 창 안의 값들을 평균 내는 것입니다. 이 절차를 모든 픽셀에 적용한 후, 새로운 평활화된 이미지가 얻어집니다. 수학적으로 우리는 이를 다음과 같이 표현할 수 있습니다.\n[ f^*(x,y) = {s=-a}^{a}{t=-a}^{a} f(x+s, y+t), ]\n여기서 (f(x,y))는 좌표 (x), (y)에서의 픽셀 값이고, (a)는 각 방향으로 (2a+1)인 창의 크기를 결정합니다. (N=(2a+1)^2)은 평균을 낸 픽셀의 수이며, (f^*)는 새로운 평활화된 이미지입니다.\n더 일반적으로, 이동 평균을 가중 평균으로 대체할 수 있으며, 가중치 함수 (w)를 사용합니다. (w)는 대개 창의 중간 지점((s=t=0))에서 가장 높은 가중치를 갖고 가장자리로 갈수록 감소합니다.\n[ (w * f)(x,y) = {s=-}^{+} {t=-}^{+} w(s,t), f(x+s, y+s) ]\n표기상의 편의를 위해 합산 범위를 (-)에서 ()까지로 두었지만, 실제로는 (w)가 유한한 수의 0이 아닌 값만을 가지므로 합은 유한합니다. 사실 우리는 가중치 함수 (w)를 또 다른 이미지로 생각할 수 있으며, 이 연산을 두 이미지 (f)와 (w)의 컨벌루션(convolution)이라고도 부르며 기호 (*)로 나타냅니다. EBImage에서 2차원 컨벌루션은 filter2 함수에 의해 구현되며, 보조 함수 makeBrush를 사용하여 가중치 함수 (w)를 생성할 수 있습니다.\nw = makeBrush(size = 51, shape = \"gaussian\", sigma = 7)\nnucSmooth = filter2(getFrame(cells, 1), w)\n\n그림 11.9: nucSmooth, 이미지 객체 cells의 DNA 채널을 평활화한 버전(원래 버전은 그림 11.8의 가장 왼쪽 패널에 나와 있습니다).\n질문 11.9\n가중치 행렬 w는 어떻게 생겼나요?\n해결책\n그림 11.10을 참조하세요.\nlibrary(\"tibble\")\nlibrary(\"ggplot2\")\ntibble(w = w[(nrow(w)+1)/2, ]) |&gt;\n  ggplot(aes(y = w, x = seq(along = w))) + geom_point()\n\n그림 11.10: 가중치 행렬 w의 중간 행, w[26, ].\n사실 filter2 함수는 식 11.2에 명시된 합산을 직접 수행하지 않습니다. 대신, 수학적으로 동일하면서도 계산적으로 더 효율적인 방식인 고속 푸리에 변환(Fast Fourier Transformation)을 사용합니다.\n식 11.2의 컨벌루션은 임의의 두 이미지 (f_1, f_2)와 숫자 (c_1, c_2)에 대해 (w(c_1f_1+c_2f_2)= c_1wf_1 + c_2w*f_2)가 성립한다는 의미에서 선형(linear) 연산입니다. 선형 필터의 근간이 되는 아름답고 강력한 이론이 있습니다 (Vetterli, Kovačević, and Goyal 2014).\n계속 진행하기 위해, 이제 그림 11.9에서 시연을 위해 보여주었던 것보다 작은 평활화 대역폭을 사용합니다. DNA 채널에는 1픽셀, 액틴과 튜불린에는 3픽셀의 sigma를 사용합시다.\ncellsSmooth = Image(dim = dim(cells))\nsigma = c(1, 3, 3)\nfor(i in seq_along(sigma))\n  cellsSmooth[,,i] = filter2( cells[,,i],\n         filter = makeBrush(size = 51, shape = \"gaussian\",\n                            sigma = sigma[i]) )\n평활화된 이미지는 픽셀 노이즈가 감소하면서도 여전히 필요한 해상도를 유지합니다.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#적응형-임계값-처리",
    "href": "11-chap.html#적응형-임계값-처리",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.9 11.9 적응형 임계값 처리",
    "text": "13.9 11.9 적응형 임계값 처리\n적응형 임계값 처리(adaptive thresholding)의 아이디어는, 그림 11.6b에서 했던 것과 같은 단순한 임계값 처리와 비교할 때, 이미지의 서로 다른 영역에서 임계값이 달라질 수 있도록 허용하는 것입니다. 이런 방식으로, 예를 들어 불균일한 조명이나 근처의 밝은 물체에서 새어 나오는 신호로 인해 발생하는 기저 배경 신호의 공간적 의존성을 예상할 수 있습니다. 사실 우리는 이미 그림 11.3의 오른쪽 하단 이미지에서 불균일한 배경의 예시를 보았습니다.\n우리의 대장암 이미지(그림 11.8)에는 그러한 아티팩트가 없지만, 시연을 위해 중간이 가장 높고 측면으로 갈수록 떨어지는 2차원 벨 함수 illuminationGradient를 이미지에 곱하여 불균일한 조명을 시뮬레이션해 봅시다 (그림 11.11).\npy = seq(-1, +1, length.out = dim(cellsSmooth)[1])\npx = seq(-1, +1, length.out = dim(cellsSmooth)[2])\nilluminationGradient = Image(outer(py, px, function(x, y) exp(-(x^2 + y^2))))\nnucBadlyIlluminated = cellsSmooth[,,1] * illuminationGradient\n우리는 이제 우리가 탐지하고자 하는 핵보다는 크지만 조명 아티팩트의 길이 스케일보다는 작은 21픽셀 크기의 평활화 창 disc를 정의합니다. 이를 사용하여 localBackground 이미지(그림 11.11 (c))와 임계값 처리된 이미지 nucBadThresh를 계산합니다.\ndisc = makeBrush(21, \"disc\")\ndisc = disc / sum(disc)\nlocalBackground = filter2(nucBadlyIlluminated, disc)\noffset = 0.02\nnucBadThresh = (nucBadlyIlluminated - localBackground &gt; offset)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n그림 11.11: a: illuminationGradient, 중앙에서 최댓값을 갖고 측면으로 갈수록 떨어지는 함수이며, 이미지에서 때때로 보이는 불균일한 조명을 시뮬레이션합니다. (b) nucBadlyIlluminated, cellsSmooth의 DNA 채널에 illuminationGradient를 곱한 결과 이미지. (c) localBackground, 탐지할 객체보다 큰 대역폭을 가진 선형 필터를 적용한 결과. (d) nucBadThresh, 적응형 임계값 처리 결과. 신호 강도의 감소에도 불구하고 이미지 주변부의 핵들이 상당히 잘 식별되었습니다.\n이것이 효과가 있을 수 있음을 확인했으므로, 다음 단계들을 위해 실제(인위적으로 성능이 저하되지 않은) 이미지에 대해 동일한 작업을 다시 수행해 봅시다.\nnucThresh = (cellsSmooth[,,1] - filter2(cellsSmooth[,,1], disc) &gt; offset)\n각 픽셀의 강도를 국소 이웃에서 결정된 배경과 비교함으로써, 우리는 객체들이 이미지에 상대적으로 드문드문 분포되어 있어 이웃의 신호 분포가 배경에 의해 지배된다고 가정합니다. 우리 이미지의 핵들에 대해서는 이 가정이 타당하지만, 다른 상황에서는 다른 가정을 세워야 할 수도 있습니다. 우리가 여기서 수행한 적응형 임계값 처리는 선형 필터인 filter2를 사용하므로 (가중) 국소 평균화에 해당합니다. 비록 계산 비용은 더 많이 들지만, 중앙값(median)이나 낮은 분위수와 같은 다른 분포 요약 수치들이 더 선호되는 경향이 있습니다. 국소 중앙값 필터링을 위해 EBimage는 medianFilter 함수를 제공합니다.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#이진-이미지에-대한-형태학적-연산",
    "href": "11-chap.html#이진-이미지에-대한-형태학적-연산",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.10 11.10 이진 이미지에 대한 형태학적 연산",
    "text": "13.10 11.10 이진 이미지에 대한 형태학적 연산\n임계값 처리된 이미지 nucThresh(그림 11.12a에 표시됨)는 아직 만족스럽지 않습니다. 핵의 경계가 약간 울퉁불퉁하고 단일 픽셀 수준의 노이즈가 있습니다. 이러한 방해 요소들을 제거하는 효과적이고 간단한 방법이 일련의 형태학적 연산(morphological operations)에 의해 제공됩니다 (Serra 1983).\n이진 이미지(예를 들어 배경과 전경 픽셀을 나타내는 0과 1의 값을 가짐)와 이른바 구조화 요소(structuring element)라고도 불리는 이진 마스크(mask) 9가 주어졌을 때, 이러한 연산들은 다음과 같이 작동합니다.\n9 마스크의 예로는 주어진 반지름을 가진 원, 또는 더 정확하게는 중심 픽셀로부터 특정 거리 내에 있는 픽셀들의 집합이 있습니다.\n\nerode(침식): 모든 전경 픽셀에 대해 그 주변에 마스크를 놓고, 만약 마스크 아래의 픽셀 중 하나라도 배경에 해당하면 해당 전경 픽셀들을 모두 배경으로 설정합니다.\ndilate(팽창): 모든 배경 픽셀에 대해 그 주변에 마스크를 놓고, 만약 마스크 아래의 픽셀 중 하나라도 전경에 해당하면 해당 배경 픽셀들을 모두 전경으로 설정합니다.\nopen(열기): erode를 수행한 후 이어서 dilate를 수행합니다.\n\n우리는 이러한 연산들을 필터로 생각할 수도 있지만, 11.8절의 선형 필터와 대조적으로 이들은 이진 이미지에서만 작동하며 선형성이 없습니다.\n우리의 이미지에 형태학적 열기 연산을 적용해 봅시다.\nnucOpened = EBImage::opening(nucThresh, kern = makeBrush(5, shape = \"disc\"))\n이 결과는 미묘하며, 그림 11.12에서 차이점을 찾으려면 이미지를 확대해 보아야 할 것입니다. 하지만 이 연산은 이진 이미지에서 우리 응용 목적상 바람직하지 않은 일부 픽셀 수준의 특징들을 평활화하는 데 성공합니다.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#이진-이미지를-객체로-분할",
    "href": "11-chap.html#이진-이미지를-객체로-분할",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.11 11.11 이진 이미지를 객체로 분할",
    "text": "13.11 11.11 이진 이미지를 객체로 분할\n이진 이미지 nucOpened는 이미지를 전경과 배경 픽셀로 분할한 것이지, 개별 핵으로 분할한 것은 아닙니다. 우리는 한 단계 더 나아가 연결된 픽셀 세트로 정의된 개별 객체들을 추출할 수 있습니다. EBImage에는 이 목적을 위한 편리한 함수인 bwlabel이 있습니다.\nnucSeed = bwlabel(nucOpened)\ntable(nucSeed)\n\n\nnucSeed\n     0      1      2      3      4      5      6      7      8      9     10 \n155408    511    330    120    468    222    121    125    159    116    520 \n    11     12     13     14     15     16     17     18     19     20     21 \n   115    184    179    116    183    187    303    226    164    309    194 \n    22     23     24     25     26     27     28     29     30     31     32 \n   148    345    287    203    379    371    208    222    320    443    409 \n    33     34     35     36     37     38     39     40     41     42     43 \n   493    256    169    225    376    214    228    341    269    119    315 \n이 함수는 0이 배경을 나타내고 1부터 43까지의 숫자가 식별된 서로 다른 객체들을 인덱싱하는 정수 값 이미지 nucSeed를 반환합니다.\n질문 11.10\n위 테이블의 숫자들은 무엇을 의미하나요?\n해결책\n이들은 각 객체의 면적(픽셀 단위)에 해당합니다. 우리는 우리가 예상하는 것과 비교하여 너무 크거나 작은 객체들을 제거하는 데 이 정보를 사용할 수 있습니다.\n그러한 이미지들을 시각화하기 위해, (회색조) 정수 이미지를 각 객체마다 임의로 선택된 서로 다른 색상을 사용하여 컬러 이미지로 변환해 주는 colorLabels 함수가 편리합니다.\nEBImage::display(colorLabels(nucSeed))\n이것은 그림 11.12의 중간 패널에 나와 있습니다. 결과는 이미 고무적이지만, 두 가지 유형의 오류를 발견할 수 있습니다:\n\n일부 인접한 객체들이 제대로 분리되지 않았습니다.\n일부 객체들이 구멍을 포함하고 있습니다.\n\n사실 우리는 11.9절에서 평활화 창 크기와 offset 매개변수를 조절하여 이러한 오류의 발생 빈도를 바꿀 수 있습니다: 오프셋을 높이면 두 인접한 객체가 닿아서 bwlabel에 의해 하나의 객체로 간주될 확률이 줄어들지만, 반면에 구멍이 더 많이 그리고 더 크게 생기게 됩니다. 반대로 낮추는 것도 마찬가지입니다.\n분할(Segmentation)은 광범위한 문헌, 소프트웨어 도구 (Schindelin et al. 2012; Chaumont et al. 2012; Carpenter et al. 2006; Held et al. 2010), 그리고 이미지 분석 및 기계 학습 커뮤니티의 실무 경험이 축적된 풍부하고 다양한 연구 및 공학 분야입니다. 주어진 작업에 적합한 접근 방식이 무엇인지는 데이터와 기저의 질문에 크게 좌우되며, 모든 경우에 최선인 유일한 방법은 없습니다. 일반적으로 분석을 평가하기 위한 “실측 자료(ground truth)”나 “표준(gold standards)”을 얻는 것조차 어려우며 – 적은 수의 선택된 이미지에 대한 수동 주석 처리에 의존하는 것도 드문 일이 아닙니다. 비록 당혹스러울 정도로 많은 선택지들이 있지만, 시작하는 것은 쉽습니다. 우리는 단순한 해결책으로 시작하는 것을 두려워할 필요가 없으며, 이를 점진적으로 개선해 나갈 수 있습니다. 식별될 객체들의 예상되는 모양, 크기 및 객체 간의 관계에 대한 사전 지식을 더 많이 포함할 수 있는 방법들을 통해 대개 개선을 얻을 수 있습니다.\n고처리량 이미지의 통계 분석을 위해, 우리는 너무 많은 매개변수나 가정에 의존하지 않으면서 아마도 최적은 아닐지라도 신속하고 충분히 좋은 결과를 내는 단순한 방법에 만족하기로 선택할 수도 있습니다 (Rajaram et al. 2012). 이러한 정신으로 우리가 가진 것을 가지고 계속 진행해 봅시다. 우리는 모든 핵 염색 영역을 확실히 커버하되 핵 사이의 일부 영역도 커버하는 관대한 전경 마스크를 생성합니다. 이를 위해 단순히 덜 엄격한 두 번째 적 적응형 임계값 처리를 적용합니다.\nnucMask = cellsSmooth[,,1] - filter2(cellsSmooth[,,1], disc) &gt; 0\n그리고 전경 픽셀로 둘러싸인 구멍들을 채워주는 또 다른 형태학적 연산인 fillHull을 적용합니다.\nnucMask = fillHull(nucMask)\nnucSeed를 개선하기 위해, 이제 우리는 nucMask에 의해 정의된 마스크를 채울 때까지 식별된 객체들을 _전파(propagate)_시킬 수 있습니다. 마스크가 연결된 부분에서의 핵 사이의 경계는 다음 섹션에서 설명할 보로노이 테셀레이션(Voronoi tessellation)에 의해 그려질 수 있으며, 이는 propagate 함수에 구현되어 있습니다.\nnuclei = propagate(cellsSmooth[,,1], nucSeed, mask = nucMask)\n그 결과는 그림 11.12의 가장 오른쪽 패널에 표시되어 있습니다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n그림 11.12: 핵 분할의 서로 다른 단계들. (a-e): nucThresh, nucOpened, nucSeed, nucMask, nuclei.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#보로노이-테셀레이션",
    "href": "11-chap.html#보로노이-테셀레이션",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.12 11.12 보로노이 테셀레이션",
    "text": "13.12 11.12 보로노이 테셀레이션\n보로노이 테셀레이션(Voronoi tessellation)은 일련의 시드(seed) 점들(또는 영역들)이 있고 시드들 사이에 놓인 공간을 각 공간 상의 점이 가장 가까운 시드에 할당되도록 분할하고 싶을 때 유용합니다. 이것은 직관적이고 강력한 아이디어이므로, 이 섹션에서 이에 대해 잠시 곁가지로 다뤄보겠습니다. 기본적인 예시를 고려해 봅시다. 우리는 nuclei 이미지를 시드로 사용합니다. propagate 함수를 호출하려면 다른 이미지도 지정해야 합니다: 지금은 단순히 모든 값이 0인 이미지(zeros)를 제공하고, lambda 매개변수를 큰 양수값으로 설정합니다(이 선택들에 대해서는 나중에 다시 다룰 것입니다).\nzeros        = Image(dim = dim(nuclei))\nvoronoiExamp = propagate(seeds = nuclei, x = zeros, lambda = 100)\nvoronoiPaint = paintObjects(voronoiExamp, 1 - nucOpened)\n\n그림 11.13: 핵(검은색 영역으로 표시됨)을 시드로 사용한 보로노이 분할 예시이며 회색 선으로 표시됩니다.\n질문 11.11\n테셀레이션에서 분할 요소를 어떻게 선택하나요?\n해결책\n위의 propagate 호출 결과인 voronoiExamp는 단순히 값이 서로 다른 분할들을 나타내는 정수 이미지입니다.\nhead(table(voronoiExamp))\n\n\nvoronoiExamp\n   1    2    3    4    5    6 \n5645 4735  370 5964 3333 1377 \n\n\nind = which(voronoiExamp == 13, arr.ind = TRUE)\nhead(ind, 3)\n\n\n     row col\n[1,] 112 100\n[2,] 113 100\n[3,] 114 100\n그 결과는 그림 11.13에 나와 있습니다. 이것은 흥미로워 보이지만, 아마도 그림 11.12의 nuclei 이미지만큼 유용하지는 않을 것입니다. 우리는 보로노이 테셀레이션의 기본 정의에서 두 가지 일반화가 가능하다는 점에 주목합니다:\n\n기본적으로 우리가 분할하는 공간은 전체 직사각형 이미지 영역이지만 – 사실 우리는 어떤 임의의 하위 공간으로도 자신을 제한할 수 있습니다. 이는 각 점으로부터 다음 시드까지의 최단 거리를 단순한 평면이 아니라, (가로지를 수 없는) 호수와 강이 산재한 지형에서 모든 경로가 육지 위에 남아 있어야 하는 조건 하에서 찾는 것과 비슷합니다. propagate는 mask 매개변수를 통해 이러한 일반화를 가능하게 합니다.\n기본적으로 우리는 공간을 평평하다고 생각하지만 – 사실 공간은 언덕과 협곡을 가질 수 있으며, 따라서 지형 상의 두 점 사이의 거리는 그들의 (x)- 및 (y)-위치뿐만 아니라 그 사이에 놓인 (z)-방향의 오르막과 내리막인 “고도”에도 의존합니다. 여러분은 propagate의 x 인수를 통해 그러한 지형을 지정할 수 있습니다.\n\n수학적으로 말하자면, 우리는 단순한 기본 사례(위에 유클리드 메트릭이 있는 평평한 직사각형 또는 이미지) 대신, 특수한 형태와 메트릭을 가진 리만 다양체(Riemann manifold) 위에서 보로노이 분할을 수행하는 것입니다. 이미지의 열과 행 좌표에 대해 (x)와 (y) 표기법을 사용하고 고도에 대해 (z)를 사용합시다. 좌표 ((x, y, z))와 ((x+x, y+y, z+z))로 정의되는 인접한 두 점 사이의 거리 (s)는 일반적인 2D 이미지 상의 유클리드 메트릭인\n[ s^2 = x^2 + y^2 ]\n이 아니라 대신 다음과 같이 얻어집니다.\n[ s^2 = , ]\n여기서 매개변수 ()는 ()인 실수입니다. 이를 이해하기 위해 몇 가지 중요한 경우를 살펴봅시다:\n[\n\\[\\begin{aligned} \\lambda=1:&  \\quad \\text{d}s^2 = \\text{d}x^2 + \\text{d}y^2 + \\text{d}z^2\\\\\n\\lambda=0:& \\quad \\text{d}s^2 = 2\\, \\text{d}z^2\\\\\n\\lambda\\to\\infty:&  \\quad \\text{d}s^2 = 2 \\left( \\text{d}x^2 + \\text{d}y^2 \\right)\\\\\n\\end{aligned} \\tag{11.5}\\]\\]\n()인 경우 메트릭은 등방성 유클리드 메트릭이 됩니다. 즉, (z)-방향의 이동이 (x)- 또는 (y)-방향과 똑같이 “비싸거나” “멀게” 느껴집니다. 극한의 경우인 ()에서는 오직 (z)-이동만이 중요하며, 옆으로의 이동((x)- 또는 (y)-방향)은 거리에 기여하지 않습니다. 다른 극한의 경우인 ()에서는 오직 옆으로의 이동만이 중요하며, (z)-방향의 이동은 “공짜”입니다. 더 멀리 떨어진 점들 사이의 거리는 그들 사이의 최단 경로를 따라 (s)를 합산하여 얻어집니다. 매개변수 ()는 옆으로의 이동((x)와 (y)축을 따라)과 수직 이동 사이의 상대적 가중치를 제어하는 편리한 수단 역할을 합니다. 직관적으로 여러분이 그러한 지형의 등산객이라고 상상한다면, ()를 선택함으로써 여러분은 산을 돌아가는 대신 산을 넘기 위해 얼마나 많은 오르내림을 감수할지 지정할 수 있습니다. 우리가 이 섹션의 시작 부분에서 propagate를 호출할 때 lambda = 100을 사용했을 때, 이 값은 사실상 무한대였으므로 식 11.5의 세 번째 경계 사례에 해당했습니다.\n세포 분할을 목적으로 이러한 아이디어들은 Thouis Jones 등 (Jones, Carpenter, and Golland 2005; Carpenter et al. 2006)에 의해 제시되었으며, 그들은 또한 propagate에서 사용되는 효율적인 알고리즘을 작성했습니다.\n태스크\n서로 다른 ()들을 사용했을 때의 효과를 시험해 보세요.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#세포체-분할",
    "href": "11-chap.html#세포체-분할",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.13 11.13 세포체 분할",
    "text": "13.13 11.13 세포체 분할\n\n그림 11.14: 로그를 취한 후 cellsSmooth의 액틴 채널 히스토그램.\n\n그림 11.15: 그림 11.14를 확대한 모습.\n이미지에서 세포질 영역의 마스크를 결정하기 위해, 이번에는 데이터에 혼합 모델을 적합시켜서 찾은 전역 임계값을 사용하는 다른 방식의 임계값 처리를 살펴봅시다. 히스토그램은 액틴 이미지의 픽셀 강도 분포를 보여줍니다. 우리는 로그 스케일에서 데이터를 살펴보며, 그림 11.15에서는 데이터의 대부분이 놓인 영역을 확대합니다.\nhist(log(cellsSmooth[,,3]) ) hist(log(cellsSmooth[,,3]), xlim = -c(3.6, 3.1), breaks = 300)\n수많은 이미지에 대한 이러한 히스토그램들을 살펴보면서, 우리는 분할 목적으로 다음과 같은 모델을 세울 수 있습니다: Image cells의 세포질 채널 신호는 두 분포의 혼합물이며, 로그-정규(log-Normal) 배경과, 명시되지는 않았지만 다소 평평하고 대부분 겹치지 않는 또 다른 분포를 가진 전경으로 구성됩니다 10. 더욱이 픽셀의 대다수는 배경에서 옵니다. 그러면 우리는 (패키지 genefilter에 구현된) half range mode와, 모드(mode)의 왼쪽에 놓인 값들의 제곱평균제곱근(root mean square)으로부터 로그-정규 성분의 위치와 너비 매개변수에 대한 강건한 추정치를 찾을 수 있습니다.\n10 이것은 우리가 4장에서 보았던 혼합 모델 아이디어의 응용입니다.\nlibrary(\"genefilter\")\nbgPars = function(x) {\n  x    = log(x)\n  loc  = half.range.mode( x )\n  left = (x - loc)[ x &lt; loc ]\n  wid  = sqrt( mean(left^2) )\n  c(loc = loc, wid = wid, thr = loc + 6*wid)\n}\ncellBg = apply(cellsSmooth, MARGIN = 3, FUN = bgPars)\ncellBg\n\n\n           [,1]        [,2]        [,3]\nloc -2.90176965 -2.94427499 -3.52191681\nwid  0.00635322  0.01121337  0.01528207\nthr -2.86365033 -2.87699477 -3.43022437\n이 함수는 위치 loc에 6배의 너비 wid를 더한 값을 임계값으로 정의합니다 11.\n11 여기서 숫자 6의 선택은 임의적입니다(ad hoc). 우리는 두 혼합 성분의 가중치를 추정하고 혼합 모델에 따른 사후 확률에 기초하여 각 픽셀을 전경 또는 배경으로 할당함으로써 임계값 선택을 더 객관적으로 만들 수 있습니다. 더 고급 분할 방법들은 이것이 실제로는 분류 문제라는 사실을 사용하며, 전경과 배경 영역을 분리하기 위해 추가적인 특징들과 더 복잡한 분류기들을 포함합니다 (예: (Berg et al. 2019)).\nhist(log(cellsSmooth[,,3]), xlim = -c(3.6, 3.1), breaks = 300) abline(v = cellBg[c(“loc”, “thr”), 3], col = c(“brown”, “red”))\n\n그림 11.16: 그림 11.15와 같지만, loc와 thr이 수직선으로 표시되었습니다.\n우리는 이제 액틴 또는 튜불린 이미지에서 임계값 위에 있거나, 이미 이미지 nuclei에서 핵으로 분류된 모든 픽셀들의 합집합으로 cytoplasmMask를 정의할 수 있습니다.\ncytoplasmMask = (cellsSmooth[,,2] &gt; exp(cellBg[“thr”, 2])) | nuclei | (cellsSmooth[,,3] &gt; exp(cellBg[“thr”, 3]))\n그 결과는 그림 11.17의 왼쪽 패널에 나와 있습니다. 세포체(cellular bodies)를 정의하기 위해, 이제 우리는 11.12절의 보로노이 테셀레이션 기반 전파 알고리즘을 사용하여 이 마스크 내에서 핵 분할을 간단히 확장할 수 있습니다. 이 방법은 각 핵마다 정확히 하나의 세포체가 있도록 보장하며, 세포의 모양이 콤팩트하게 유지되는 것과 이미지의 액틴 및 ()-튜불린 강도 신호를 따르는 것 사이에서 타협이 이루어지도록 세포체의 윤곽을 그립니다. propagate 알고리즘의 용어로 말하자면, 세포 모양은 거리 메트릭 11.4의 (x)와 (y) 성분에 의해 콤팩트하게 유지되고, 액틴 신호는 (z) 성분을 위해 사용됩니다. ()가 이들 사이의 상충 관계를 제어합니다.\ncellbodies = propagate(x = cellsSmooth[,,3], seeds = nuclei,\n                       lambda = 1.0e-2, mask = cytoplasmMask)\ncolorLabel 플롯에 대한 대안적인 표현으로서, 우리는 또한 paintObjects 함수를 사용하여 원래 이미지 위에 핵과 세포체의 분할을 표시할 수 있습니다. 아래에서 계산된 이미지 nucSegOnNuc, nucSegOnAll, cellSegOnAll은 그림 11.17의 중간부터 오른쪽 패널에 나와 있습니다.\ncellsColor = EBImage::rgbImage(red   = cells[,,3],\n                               green = cells[,,2],\n                               blue  = cells[,,1])\nnucSegOnNuc  = paintObjects(nuclei, tgt = EBImage::toRGB(cells[,,1]), col = \"#ffff00\")\nnucSegOnAll  = paintObjects(nuclei,     tgt = cellsColor,    col = \"#ffff00\")\ncellSegOnAll = paintObjects(cellbodies, tgt = nucSegOnNuc,   col = \"#ff0080\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n그림 11.17: 세포체 분할 단계들. (a-d): cytoplasmMask, cellbodies (파랑: DAPI, 빨강: 액틴, 초록: 알파-튜불린), nucSegOnNuc, nucSegOnAll, cellSegOnAll.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#특징-추출",
    "href": "11-chap.html#특징-추출",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.14 11.14 특징 추출",
    "text": "13.14 11.14 특징 추출\n이제 원래의 이미지 데이터 cells와 함께 분할 결과인 nuclei 및 cellbodies를 확보했으므로, 각 세포에 대해 다양한 서술자(descriptors) 또는 특징(features)을 계산할 수 있습니다. 우리는 이미 11.11절의 시작 부분에서 기본 R 함수 table을 사용하여 객체의 총 수와 크기를 결정하는 방법을 보았습니다. 이제 이를 더 발전시켜서, 분할된 핵에서의 DAPI 신호(cells[,,1])의 평균 강도, 분할된 핵에서의 평균 액틴 강도(cells[,,3]), 그리고 세포체에서의 평균 액틴 강도를 계산해 봅시다.\nmeanNucInt       = tapply(cells[,,1], nuclei, mean)\nmeanActIntInNuc  = tapply(cells[,,3], nuclei, mean)\nmeanActIntInCell = tapply(cells[,,3], cellbodies, mean)\n우리는 특징들을 쌍별 산점도로 시각화할 수 있습니다 (그림 11.18). 각 특징이 독립적인 정보를 담고 있긴 하지만, 서로 상관관계가 있음을 볼 수 있습니다.\nlibrary(\"GGally\")\nggpairs(tibble(meanNucInt, meanActIntInNuc, meanActIntInCell))\n\n그림 11.18: 세포별 강도 서술자의 쌍별 산점도.\n조금만 더 노력하면, 우리는 더 정교한 요약 통계량들도 계산할 수 있습니다 – 예를 들어, 세포체 면적에 대한 핵 면적의 비율, 또는 세포 구조의 다소 추상적인 척도로서 각 세포체에서의 서로 다른 형광 신호들의 엔트로피, 상호 정보량(mutual information) 및 상관관계 등입니다. 그러한 척도들은 예를 들어 약물에 의해 유도된 미묘한 세포 구조의 변화를 탐지하는 데 사용될 수 있습니다.\n위의 tapply 표현식처럼 기본 R 관용구를 사용하여 이러한 계산들을 수행하는 것은 쉽고 직관적이지만, EBImage 패키지는 문헌에서 흔히 사용되어 온 방대한 특징 컬렉션을 효율적으로 계산해 주는 computeFeatures 함수도 제공합니다 (선구적인 참고 문헌은 Boland와 Murphy. (2001)입니다). 이 함수에 대한 자세한 내용은 매뉴얼 페이지에 설명되어 있으며, 예시 응용 사례는 HD2013SGI 비네트에 정리되어 있습니다. 아래에서는 핵 분할(nuclei)을 사용하여 DAPI 채널로부터, 그리고 세포체 분할(cytoplasmRegions)을 사용하여 액틴과 튜불린 채널로부터 각 세포의 강도, 모양 및 텍스처에 대한 특징들을 계산합니다.\nF1 = computeFeatures(nuclei,     cells[,,1], xname = \"nuc\",  refnames = \"nuc\")\nF2 = computeFeatures(cellbodies, cells[,,2], xname = \"cell\", refnames = \"tub\")\nF3 = computeFeatures(cellbodies, cells[,,3], xname = \"cell\", refnames = \"act\")\ndim(F1)\n\n\n[1] 43 89\nF1은 43개 행(각 세포마다 하나씩)과 89개 열(계산된 특징마다 하나씩)을 가진 행렬입니다.\nF1[1:3, 1:5]\n\n\n  nuc.0.m.cx nuc.0.m.cy nuc.0.m.majoraxis nuc.0.m.eccentricity nuc.0.m.theta\n1   119.5523   17.46895          44.86819            0.8372059     -1.314789\n2   143.4511   15.83709          26.15009            0.6627672     -1.213444\n3   336.5401   11.48175          18.97424            0.8564444      1.470913\n열 이름은 특징의 유형뿐만 아니라, 그것이 계산된 색상 채널과 분할 마스크를 인코딩합니다. 우리는 이제 5장, 7장, 9장에서 보았던 다변량 분석 방법들을 다음과 같은 수많은 작업에 사용할 수 있습니다.\n\n세포 하위 모집단 탐지 (군집화)\n세포를 사전 정의된 세포 유형이나 표현형으로 분류 (분류)\n서로 다른 생물학적 조건에 해당하는 이미지들 사이에서 하위 모집단이나 세포 유형의 절대적 또는 상대적 빈도가 다른지 확인\n\n이러한 “일반적인” 기계 학습 작업 외에도, 우리는 세포의 공간적 위치도 알고 있으며, 이하에서는 우리의 분석에서 이를 활용하는 몇 가지 방법들을 탐구해 볼 것입니다.\n태스크\n탐색적 다변량 방법인 PCA, 히트맵을 사용하여 행렬 F1, F2, F3를 시각화하세요. “이상치” 세포들의 특별한 점은 무엇인가요?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#공간-통계-점-과정",
    "href": "11-chap.html#공간-통계-점-과정",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.15 11.15 공간 통계: 점 과정",
    "text": "13.15 11.15 공간 통계: 점 과정\n이전 섹션들에서 우리는 이미지로부터 세포의 위치와 다양한 모양 및 형태학적 특징들을 추출하는 방법들을 보았습니다. 이제는 위치의 공간적 분포를 탐구해 볼 것입니다. 흥미로운 작업 데이터를 확보하기 위해, 데이터셋을 바꾸어 유방암 림프절 생검 데이터를 살펴보겠습니다.\n\n13.15.1 11.15.1 사례 연구: 면역 세포와 암세포의 상호작용\n림프절은 림프(lymph)라 불리는 체액을 위한 면역학적 필터 역할을 합니다. 항원은 혈액 순환계로 돌아가기 전에 림프절에서 림프로부터 걸러집니다. 림프절은 몸 전체에서 발견되며, 주로 T 세포, B 세포, 수지상 세포(dendritic cells) 및 대식세포(macrophages)로 구성됩니다. 림프절은 우리 조직 대부분의 액체를 배출합니다. 유방의 림프관은 보통 겨드랑이 밑의 나머지 림프절을 거치기 전에 하나의 림프절로 먼저 배출됩니다. 그 첫 번째 림프절을 감시(sentinel) 림프절이라고 부릅니다. 비장과 유사한 방식으로, 항원을 포획하는 대식세포와 수지상 세포는 이러한 이물질들을 T 세포와 B 세포에 제시함으로써 면역 반응을 개시합니다.\nT 림프구는 대개 기능적 및 표현형적으로 다른 두 가지 주요 하위 집합으로 나뉩니다.\n\nCD4+ T 세포, 또는 보조 T 세포(T helper cells): 이들은 면역 조절의 적절한 조정자입니다. 보조 T 세포의 주요 기능은 감염과 싸우기 위해 다른 백혈구들을 활성화하는 특수 인자들을 분비함으로써 면역 반응을 증강하거나 강화하는 것입니다.\nCD8+ T 세포, 또는 독성/억제 T 세포(T killer/suppressor cells): 이 세포들은 특정 종양 세포, 바이러스에 감염된 세포, 그리고 때때로 기생충을 직접 죽이는 데 중요합니다. CD8+ T 세포는 면역 반응의 하향 조절(down-regulation)에도 중요합니다.\n\n두 유형의 T 세포 모두 몸 전체에서 발견될 수 있습니다. 이들은 종종 활성화가 일어나는 장소로서 이차 림프 기관(림프절과 비장)에 의존합니다.\n수지상 세포 또는 CD1a 세포는 항원을 처리하여 펩타이드를 T 세포에 제시하는 항원 제시 세포입니다.\n\n그림 11.19: 비대해진 림프절의 생검 결과, 캡슐은 온전하고 정맥동(sinuses)은 폐쇄된 것이 밝혀졌습니다 (왼쪽 상단 패널, 헤마톡실린 및 에오신 염색, 원래 배율 () 100). 침윤물은 소림프구, 대식세포 및 형질세포의 혼합물로 구성되었습니다 (오른쪽 상단 패널, 헤마톡실린 및 에오신, 원래 배율 () 400). 침윤물은 CD3 양성 T 세포(CD4 및 CD8 양성 세포 모두 포함)와 CD20 양성 B 세포의 혼합물이었습니다. 수많은 대식세포 또한 CD4 양성임이 확인되었습니다. (출처: Hurley et al., Diagnostic Pathology (2008) 3:13)\n\n그림 11.20: 염색된 림프절; 이 이미지는 brcalymphnode의 공간 데이터의 기초가 됩니다.\n우리는 Setiadi 등 (2010)의 데이터를 살펴볼 것입니다. 그림 11.20에 표시된 이미지를 분할 방법인 GemIdent (Holmes, Kapelner, and Lee 2009)를 사용하여 분할한 후, 저자들은 이미지 내의 모든 세포의 좌표와 유형을 얻었습니다. 우리는 이러한 유형의 데이터를 마크된 점 과정(marked point process)이라고 부르며, 이는 3개의 열을 가진 단순한 표로 간주될 수 있습니다.\nlibrary(\"readr\")\nlibrary(\"dplyr\")\ncellclasses = c(\"T_cells\", \"Tumor\", \"DCs\", \"other_cells\")\nbrcalymphnode = lapply(cellclasses, function(k) {\n    read_csv(file.path(\"..\", \"data\", sprintf(\"99_4525D-%s.txt\", k)))\n    |&gt; transmute(x = globalX, y = globalY, class = k)\n}) |&gt; bind_rows() |&gt; mutate(class = factor(class))\n\nbrcalymphnode\n\n\n# A tibble: 209,462 × 3\n       x     y class  \n   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  \n 1  6355 10382 T_cells\n 2  6356 10850 T_cells\n 3  6357 11070 T_cells\n 4  6357 11082 T_cells\n 5  6358 10600 T_cells\n 6  6361 10301 T_cells\n 7  6369 10309 T_cells\n 8  6374 10395 T_cells\n 9  6377 10448 T_cells\n10  6379 10279 T_cells\n# ℹ 209,452 more rows\n\n\ntable(brcalymphnode$class)\n\n\n        DCs other_cells     T_cells       Tumor \n        878       77081      103681       27822 \n우리는 10만 개 이상의 T 세포, 약 2만 8천 개의 종양 세포, 그리고 수백 개의 수지상 세포가 있음을 알 수 있습니다. 세포들의 (x)- 및 (y)-위치를 플롯해 봅시다 (그림 11.21).\nggplot(filter(brcalymphnode, class %in% c(\"T_cells\", \"Tumor\")),\n   aes(x = x, y = y, col = class)) + geom_point(shape = \".\") +\n   facet_grid( . ~ class) + guides(col = \"none\")\n\n그림 11.21: brcalymphnode의 T 세포와 종양 세포의 (x) 및 (y) 위치 산점도. 위치는 그림 11.20의 고해상도 버전으로부터 분할 알고리즘에 의해 얻어졌습니다. T 세포 플롯의 일부 직사각형 영역들이 의심스러울 정도로 비어 있는데, 이는 전체 합성 이미지 내의 해당 이미지 타일들이 유실되었거나 분석되지 않았기 때문일 수 있습니다.\n질문 11.12\n그림 11.20과 11.21을 비교해 보세요. 왜 (y)축이 서로 반전되어 있나요?\n해결책\n그림 11.20은 이미지 데이터의 관례를 따라 이미지의 왼쪽 상단 모서리를 원점으로 하는 반면, 그림 11.21은 하단 왼쪽을 원점으로 하는 데카르트 좌표계의 관례를 따릅니다.\nspatstat 패키지의 기능을 사용하려면, brcalymphnode의 데이터를 ppp 클래스의 객체로 변환하는 것이 편리합니다. 우리는 동명의 함수를 호출하여 이 작업을 수행합니다.\nlibrary(\"spatstat\")\n\n\nln = with(brcalymphnode, ppp(x = x, y = y, marks = class, \n                             xrange = range(x), yrange = range(y)))\nln\n\n\nMarked planar point pattern: 209462 points\nMultitype, with levels = DCs, other_cells, T_cells, Tumor \nwindow: rectangle = [3839, 17276] x [6713, 23006] units\nppp 객체들은 공간 점 과정(spatial point process) 의 실현물들을 캡처하도록 설계되었습니다. 즉, 수학적 공간 내에 위치한 일련의 고립된 점들의 집합입니다. 우리의 경우 위에서 보듯이, 공간은 (x)- 및 (y)-좌표의 범위를 포함하는 2차원 직사각형입니다. 또한 점들은 특정 속성들로 마크(marked)될 수 있습니다. ln에서 마크는 단순히 factor 변수인 class입니다. 더 일반적으로 마크는 여러 속성, 시간 또는 정량적 데이터가 될 수도 있습니다. 마크된 점 과정과 이미지 사이에는 유사점이 있지만, 점 과정의 경우 점들이 공간 내 어디에나 놓일 수 있는 반면, 이미지에서는 픽셀들이 규칙적이고 직사각형인 방식으로 공간을 덮는다는 점이 다릅니다.\n\n\n13.15.2 11.15.2 볼록 껍질 (Convex hull)\n위에서 우리는 (암묵적으로) 점 과정이 직사각형 내에 놓이도록 제한했습니다. 사실, 데이터 생성 과정은 조직 절편의 모양에 의해 더 제한됩니다. 우리는 점들의 볼록 껍질(convex hull)로부터 더 타이트한 영역을 계산하고 이를 근사할 수 있습니다 12.\n12 str(cvxhull)을 사용하여 이 S3 객체의 내부 구조를 살펴볼 수 있습니다.\ncvxhull = convexhull.xy(cbind(ln\\(x, ln\\)y)) ggplot(as_tibble(cvxhull$bdry[[1]]), aes(x = x, y = y)) + geom_polygon(fill = NA, col = “black”) + geom_point() + coord_fixed()\n\n그림 11.22: ln의 점들의 볼록 껍질을 설명하는 다각형.\n그림 11.22에서 다각형을 볼 수 있으며, 이제 이 다각형과 함께 ppp를 다시 호출합니다.\nln = with(brcalymphnode, ppp(x = x, y = y, marks = class, \n                             poly = cvxhull$bdry[[1]]))\nln\n\n\nMarked planar point pattern: 209462 points\nMultitype, with levels = DCs, other_cells, T_cells, Tumor \nwindow: polygonal boundary\nenclosing rectangle: [3839, 17276] x [6713, 23006] units\n\n\n13.15.3 11.15.3 점 과정의 공간을 정의하는 다른 방법들\n점 과정이 고려되는 공간을 정의하기 위해 반드시 볼록 껍질을 사용할 필요는 없습니다. 대안으로, 이전 지식에 기초하여 공간을 정의하는 이미지 마스크를 ppp에 제공할 수 있습니다. 또는 샘플링된 점들에 대해 밀도 추정을 사용하여, 산발적인 이상치들을 무시하고 충분히 높은 점 밀도를 가진 영역만을 식별할 수도 있습니다. 이러한 선택들은 공간 점 과정을 고려할 때 분석가의 몫입니다.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#차-효과-강도",
    "href": "11-chap.html#차-효과-강도",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.16 11.16 1차 효과: 강도",
    "text": "13.16 11.16 1차 효과: 강도\n공간 통계의 가장 기본적인 질문 중 하나는 인접한 점들이 “군집(clustering)”을 이루고 있는지, 즉 점들이 “우연히” 예상되는 것보다 서로 더 가까운지, 아니면 반대로 서로를 밀어내는 것처럼 보이는지 하는 것입니다. 이러한 유형의 질문을 던질 수 있는 많은 예시가 있습니다. 예를 들어:\n\n도시 내의 범죄 패턴,\n국가 내의 질병 패턴,\n한 지역의 토양 측정값.\n\n대개 그러한 패턴이 존재하는 이유를 찾는 것은 어렵지 않습니다: 좋은 동네와 나쁜 동네, 생활 방식이나 환경적 노출의 국소적 변동, 토양의 공통된 지질학적 역사 등이 있습니다. 때로는 관찰된 사건들이 서로를 끌어당기거나 밀어내는 메커니즘이 존재할 수도 있습니다 – 동네의 소위 “깨진 유리창”이나, 많은 세포 유형들이 다른 세포들 근처에 붙어 있으려는 경향 등이 있습니다.\n세포 예시는 공간적 군집화(또는 안티-군집화)가 객체의 속성(또는 공간 점 과정의 용어로 마크)에 따라 달라질 수 있음을 강조합니다. 또한 답변이 고려되는 길이 스케일에 따라 달라질 수 있다는 점도 강조합니다. 비록 세포들이 서로를 끌어당기더라도, 세포는 유한한 크기를 가지며 동일한 공간을 차지할 수 없습니다. 따라서 세포들 사이에는 최소한의 거리가 존재할 것이며, 그 스케일에서 세포들은 본질적으로 서로를 밀어내지만, 더 먼 거리에서는 서로를 끌어당깁니다.\n이러한 질문들을 더 정량적으로 다루기 위해, 우리는 “우연히” 무엇을 기대할지에 대한 확률 모델을 정의해야 합니다. 어떤 점 (p=(x,y)) 주변의 면적이 (a)인 원 안에 놓인 점들의 수를 세어 봅시다. 이를 (N(p, a))라고 부릅시다 13. (N)의 평균과 공분산은 1차 및 2차 속성을 제공합니다. 1차 속성은 과정의 강도(intensity)입니다.\n13 평소와 같이, 우리는 무작위 변수에 대해 대문자 (N(p, a)) 표기법을 사용하고, 그 실현물 또는 샘플에 대해 소문자 (n(p, a))를 사용합니다.\n[ (p) = _{a} . ]\n여기서 우리는 국소적 강도 ((p))를 정의하기 위해 무한소 미적분학을 사용했습니다. 시계열에서와 마찬가지로, 정상 과정(stationary process)이란 영역 전체에서 균일성을 갖는 과정입니다. 즉, ((p) = )입니다. 그러면 영역 (A)에서의 강도는 면적에 비례합니다: (E[N(, A)] = A). 나중에 우리는 공간적 공분산과 같은 더 높은 차수의 통계량들도 살펴볼 것입니다.\n[ (p_1, p_2) = _{a } . ]\n만약 과정이 정상적이라면, 이것은 오직 두 점의 상대적 위치(두 점 사이의 벡터)에만 의존할 것입니다. 만약 이것이 오직 거리에만 의존한다면, 즉 벡터의 방향이 아니라 길이에만 의존한다면, 이를 2차 등방성(second order isotropic)이라고 부릅니다.\n\n그림 11.23: 바닥에 떨어지는 빗방울은 포아송 과정으로 모델링됩니다. 특정 지점에 떨어지는 빗방울의 수는 오직 비율 ()(와 그 지점의 크기)에만 의존하며, 다른 지점에서 일어나는 일에는 의존하지 않습니다.\n\n13.16.1 11.16.1 포아송 과정 (Poisson Process)\n가장 단순한 공간 과정은 포아송 과정입니다. 우리는 이를 우리의 데이터와 비교하기 위한 귀무 모델로 사용할 것입니다. 이는 강도 ()를 가진 정상 과정이며, 겹치지 않는 공간 영역에서의 사건 발생들 사이에는 더 이상의 의존성이 없습니다. 더욱이, 면적 (A)인 영역 내의 점들의 수는 비율 (A)를 가진 포아송 분포를 따릅니다.\n\n\n13.16.2 11.16.2 강도 추정하기\n강도를 추정하려면, 영역을 ((p))의 잠재적인 국소적 변동을 볼 수 있을 만큼 작으면서도 충분한 수의 점들을 포함할 수 있을 만큼 큰 하위 영역들로 나눕니다. 이는 2D 밀도 추정과 유사하며, 딱딱한 영역 경계 대신 매끄러운 커널 함수 (K)를 사용할 수 있습니다.\n[ (p) = _i e(p_i) K(p-p_i). ]\n커널 함수는 평활화 매개변수인 ()에 의존합니다. ()가 클수록 각 (p)에 대한 국소적 추정치를 계산하는 영역이 커집니다. (e(p))는 에지 보정 인자(edge correction factor)로, 커널의 서포트(“평활화 창”)가 점 과정이 정의된 공간 밖으로 나갈 때 발생하는 추정 편향을 고려합니다. spatstat 패키지의 ppp 객체에 대해 정의된 density 함수는 식 11.8을 구현합니다.\nd = density(subset(ln, marks == “Tumor”), edge=TRUE, diggle=TRUE) plot(d)\n\n그림 11.24: ppp에서 Tumor로 마크된 세포들에 대한 강도 추정. 추정치의 서포트는 우리가 이전에 지정했던 다각형(그림 11.22)입니다.\n플롯은 그림 11.24에 나와 있습니다.\n질문 11.13\n에지 보정 없이 추정하면 어떻게 보이나요?\n해결책\nd0 = density(subset(ln, marks == “Tumor”), edge = FALSE) plot(d0)\n\n그림 11.25: 그림 11.24와 같지만 에지 보정이 없는 모습.\n이제 추정된 강도는 공간의 가장자리로 갈수록 작아지는데, 이는 에지 편향을 반영합니다 (그림 11.25).\ndensity는 우리에게 점 과정의 강도 에 대한 추정치를 제공합니다. 이와 관련이 있지만 다른 작업은 특정 세포 클래스에 속할 (조건부) 확률 을 추정하는 것입니다. relrisk 함수는 특정 사건 유형의 공간적으로 변화하는 위험에 대한 비모수적 추정치를 계산합니다. 우리는 특정 공간적 위치에 존재하는 세포가 종양 세포일 확률에 관심이 있습니다 (그림 11.26).\nrr = relrisk(ln, sigma = 250)\n\n\nplot(rr)\n\n그림 11.26: 세포가 존재한다는 조건 하에 각 세포 클래스의 공간적으로 변화하는 확률 추정치.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#차-효과-공간적-의존성",
    "href": "11-chap.html#차-효과-공간적-의존성",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.17 11.17 2차 효과: 공간적 의존성",
    "text": "13.17 11.17 2차 효과: 공간적 의존성\n우리의 공간 과정에서 무작위로 한 점을 골랐을 때, 가장 가까운 이웃까지의 거리 (W)는 얼마일까요? 균질 포아송 과정의 경우, 이 거리의 누적 분포 함수는 다음과 같습니다.\n[ G(w) = P(Ww) = 1-e{-w2}. ]\n(G)를 플롯하는 것은 균질 포아송 과정으로부터의 이탈을 알아차리는 한 가지 방법을 제공합니다. 에지 효과도 고려하는 (G)의 추정량 (A. J. Baddeley 1998; Ripley 1988)은 spatstat 패키지의 Gest 함수에 의해 제공됩니다.\ngln = Gest(ln) gln\nFunction value object (class 'fv')\nfor the function r -&gt; G(r)\n.....................................................................\n        Math.label      Description                                  \nr       r               distance argument r                          \ntheo    G[pois](r)      theoretical Poisson G(r)                     \nhan     hat(G)[han](r)  Hanisch estimate of G(r)                     \nrs      hat(G)[bord](r) border corrected estimate of G(r)            \nkm      hat(G)[km](r)   Kaplan-Meier estimate of G(r)                \nhazard  hat(h)[km](r)   Kaplan-Meier estimate of hazard function h(r)\ntheohaz h[pois](r)      theoretical Poisson hazard function h(r)     \n.....................................................................\nDefault plot formula:  .~r\nwhere \".\" stands for 'km', 'rs', 'han', 'theo'\nRecommended range of argument r: [0, 20.998]\nAvailable range of argument r: [0, 52.443]\n\n\nlibrary(\"RColorBrewer\")\nplot(gln, xlim = c(0, 10), lty = 1, col = brewer.pal(4, \"Set1\"))\n\n그림 11.27: 세 가지 서로 다른 에지 효과 보정을 사용한 (G)의 추정치들(여기서는 본질적으로 서로 겹쳐져 있음)과 균질 포아송 과정에 대한 이론적 분포.\n객체 gln에 대해 출력된 요약 정보는 계산된 추정치들에 대한 개요를 제공하며, 추가 설명은 Gest 매뉴얼 페이지에 있습니다. 그림 11.27에서 우리는 경험적 분포 함수와 우리가 적절히 선택한 강도를 가진 균질 포아송 과정인 귀무 모델의 분포 함수가 약 4.5 단위 지점에서 교차하는 것을 볼 수 있습니다. 이 값보다 짧은 세포 간 거리는 귀무 모델에서보다 덜 발생할 가능성이 높으며, 특히 약 2 이하의 거리는 거의 존재하지 않습니다. 이는 물론 우리의 세포들이 유한한 크기를 가지고 있어 동일한 공간을 겹쳐서 차지할 수 없다는 사실을 반영합니다. 포아송 과정과 비교했을 때 매우 큰 거리를 피하려는 경향이 있는 것으로 보이는데, 이는 아마도 세포들이 군집을 이루려는 경향을 나타내는 것으로 보입니다.\n\n13.17.1 11.17.1 Ripley의 (K) 함수\n균질 공간 포아송 과정에서, 우리가 임의의 점을 무작위로 고르고 거리 (r) 이내에 있는 점들의 수를 센다면, 이 숫자가 원의 면적인 (r^2)에 따라 증가할 것으로 기대합니다. 주어진 데이터 세트에 대해, 우리는 모든 점들에 대해 평균을 낸 관측된 이웃의 수를 이 기대값과 비교할 수 있습니다.\n정상 점 과정(stationary point process)의 (K) 함수(Ripley의 (K)-함수 또는 축소된 2차 모멘트 함수라고도 함)는 (K(r))이 주어진 임의의 점으로부터 거리 (r) 이내에 있는 (추가적인) 점들의 기대값이 되도록 정의됩니다. ()는 과정의 강도, 즉 단위 면적당 기대되는 점의 수임을 기억합시다. (K) 함수는 과정의 2차 모멘트 속성입니다.\n(K)의 정의는 비균질 점 과정으로 일반화될 수 있으며 (A. Baddeley, Moller, and Waagepetersen 2000), 다음과 같이 쓰여집니다.\n[ K_{}(r)= {i,j} \\𝟙{d(p_i, p_j) r} { (x_i) (x_j) }, ]\n여기서 (d(p_i, p_j))는 점 (p_i)와 (p_j) 사이의 거리이며, (e(p_i, p_j, r))은 에지 보정 인자입니다 14. 추정과 시각화를 위해 (K)(그리고 유사하게 (K_{}))의 변환인 소위 (L) 함수를 고려하는 것이 유용합니다.\n14 자세한 내용은 Kinhom 매뉴얼 페이지를 참조하세요.\n[ L(r)=. ]\n균질 공간 포아송 과정의 경우, 이론적 값은 (L(r) = r)입니다. 이를 데이터 세트에 대한 (L) 추정치와 비교함으로써, 점 간 의존성과 공간적 군집화에 대해 배울 수 있습니다. 식 11.11의 제곱근은 추정량의 분산을 안정화하는 효과가 있어, (K)와 비교할 때 (L)은 데이터 분석과 시뮬레이션에 더 적합합니다. spatstat 패키지의 Linhom 함수를 사용한 계산은 우리 데이터에 대해 몇 분 정도 걸립니다 (그림 11.28).\nLln = Linhom(subset(ln, marks == \"T_cells\"))\n\n\nLln\n\n\nFunction value object (class 'fv')\n\n\nfor the function r -&gt; L[inhom](r)\n\n\n................................................................................\n           Math.label                \nr          r                         \ntheo       L[pois](r)                \nborder     {hat(L)[inhom]^{bord}}(r) \nbord.modif {hat(L)[inhom]^{bordm}}(r)\n           Description                                      \nr          distance argument r                              \ntheo       theoretical Poisson L[inhom](r)                  \nborder     border-corrected estimate of L[inhom](r)         \nbord.modif modified border-corrected estimate of L[inhom](r)\n................................................................................\nDefault plot formula:  .~.x\nwhere \".\" stands for 'bord.modif', 'border', 'theo'\nRecommended range of argument r: [0, 694.7]\nAvailable range of argument r: [0, 694.7]\n\n\nplot(Lln, lty = 1, col = brewer.pal(3, \"Set1\"))\n\n그림 11.28: T 세포 패턴의 (L_{})(식 11.10 및 11.11) 추정치.\n우리는 이제 다른 세포 유형들에 대해서도, 그리고 다른 종양들과 건강한 림프절들에 대해서도 (L) 함수를 살펴보는 작업을 계속할 수 있습니다. 이것이 Setiadi와 동료들이 그들의 보고서에서 수행한 작업이며 (Setiadi et al. 2010), 건강한 림프절과 유방암 림프절 사이의 T 세포 및 B 세포 공간적 그룹화 패턴을 비교함으로써, B 세포가 일부 종양에서 림프절의 소포 외(extrafollicular) 영역에서의 정상적인 위치를 잃는 것처럼 보인다는 것을 확인했습니다.\n\n13.17.1.1 쌍 상관 함수 (The pair correlation function)\n는 기준 점으로부터의 거리의 함수로서 점 밀도가 어떻게 변하는지 설명합니다. 이는 공간적 군집화를 살펴보는 데 있어 물리학에서 영감을 얻은 관점을 제공합니다. 정상 점 과정의 경우, 다음과 같이 정의됩니다.\n[ g(r)=(r). ]\n정상 포아송 과정의 경우, 쌍 상관 함수는 항상 1과 같습니다. (g(r) &lt; 1)인 값은 점들 사이의 억제(inhibition)를 시사하고, 1보다 큰 값은 군집화를 시사합니다.\nspatstat 패키지는 비균질 과정에 대해서도 (g)의 추정치 계산을 가능하게 하며, 아래와 같이 pcf를 호출하면 식 11.12의 정의가 (K_{}). 추정치에 적용됩니다.\npcfln = pcf(Kinhom(subset(ln, marks == “T_cells”)))\nplot(pcfln, lty = 1)\nplot(pcfln, lty = 1, xlim = c(0, 10))\n\n\n\n\n\n\n\n\n그림 11.29: T 세포 패턴의 쌍 상관 함수(식 11.12) 추정치.\n그림 11.29에서 보듯이, T 세포들은 군집을 이루고 있지만, 아주 짧은 거리에서는 기피(avoidance)의 증거도 보입니다.\n질문 11.14\n그림 11.29의 하단 패널에 있는 쌍 상관 함수 플롯의 샘플링 해상도가 낮습니다. 어떻게 이를 높일 수 있을까요?\n해결책\n답은 Kinhom 함수의 r 인수에 있습니다. 그림 11.30을 참조하세요.\npcfln2 = pcf(Kinhom(subset(ln, marks == “T_cells”), r = seq(0, 10, by = 0.2))) plot(pcfln2, lty = 1)\n\n그림 11.30: 질문 11.14에 대한 답변: 그림 11.29의 하단 패널과 같지만, 더 조밀한 샘플링을 사용한 모습.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#이-장의-요약",
    "href": "11-chap.html#이-장의-요약",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.18 11.18 이 장의 요약",
    "text": "13.18 11.18 이 장의 요약\n우리는 R에서 이미지 데이터로 작업하는 법을 배웠습니다. 이미지는 기본적으로 배열일 뿐이며, 우리는 이를 조작하기 위해 익숙한 관용구들을 사용할 수 있습니다. 우리는 이미지로부터 정량적 특징들을 추출할 수 있으며, 그 후의 많은 분석적 질문들은 다른 고처리량 데이터의 경우와 다르지 않습니다: 특징들을 평균이나 분산과 같은 통계량으로 요약하고, 조건들 사이의 차이에 대해 가설 검정을 수행하고, 분산 분석을 수행하며, 차원 축소, 군집화 및 분류를 적용합니다.\n종종 우리는 전체 이미지가 아니라 이미지에 표시된 개별 객체들에 대해 그러한 정량적 특징들을 계산하고 싶어 하며, 그때 우리는 관심 있는 객체들의 경계를 획정하기 위해 먼저 이미지를 분할해야 합니다. 우리는 핵과 세포 이미지에 대해 이를 수행하는 방법을 보았습니다.\n객체들의 위치와 이러한 위치들이 서로 어떻게 연관되어 있는지에 관심이 있을 때, 우리는 공간 통계 영역으로 들어갑니다. 우리는 spatstat 패키지의 일부 기능들을 탐구했고, 점 과정 클래스를 접했으며, Ripley의 (K) 함수와 같이 점 패턴을 위해 사용되는 몇 가지 특수한 진단 통계량들을 배웠습니다.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#더-읽을거리",
    "href": "11-chap.html#더-읽을거리",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.19 11.19 더 읽을거리",
    "text": "13.19 11.19 더 읽을거리\n\n이미지 분석에 관한 방대한 문헌이 있습니다. 이를 탐색할 때, 이 분야가 두 가지 힘에 의해 주도된다는 점을 깨닫는 것이 도움이 됩니다: 구체적인 응용 도메인(우리는 고처리량 세포 기반 어세이 분석을 보았습니다)과 가용한 컴퓨터 하드웨어입니다. 1970년대에 개발된 일부 알고리즘과 개념들은 여전히 유효하며, 다른 것들은 더 체계적이고 아마도 계산 집약적인 방법들에 의해 대체되었습니다. 많은 알고리즘은 데이터의 성격과 제기된 과학적 질문에 대해 특정한 가정들을 내포하고 있는데, 이는 한 응용 분야에서는 괜찮을 수 있지만 다른 분야에서는 새롭게 검토되어야 할 필요가 있습니다. 고전적인 입문서는 The Image Processing Handbook (Russ and Neal 2015)이며, 현재 7판까지 나와 있습니다.\n공간 점 패턴 분석에 대해서는, Diggle (2013; Ripley 1988; Cressie 1991; Chiu et al. 2013)을 참조하십시오.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "11-chap.html#연습-문제",
    "href": "11-chap.html#연습-문제",
    "title": "13  이미지 분석 (Image Analysis)",
    "section": "13.20 11.20 연습 문제",
    "text": "13.20 11.20 연습 문제\n연습 문제 11.1\n여러분의 개인 사진 보관함에서 몇 가지 이미지를 R로 로드하고 11.6절의 조작법들을 적용해 보세요.\n연습 문제 11.2\n그림 11.17에서처럼 cellbodies 이미지를 표시하되, 사용자가 propagate 함수의 lambda 매개변수(11.12, 11.13절)의 효과를 탐색할 수 있는 대화형 요소를 포함한 shiny 앱을 작성해 보세요.\n연습 문제 11.3\n다음과 같은 2차원 경험적 자기 상관 함수(autocorrelation function)를 고려해 보세요.\n[ a(v_x, v_y) = _{(x,y)I} B(x, y),B(x+v_x, , y+v_y), ]\n여기서 (B)는 이미지, 즉 픽셀 세트 (I) 위의 함수이고, 튜플 ((x,y))는 모든 픽셀 좌표를 순회하며, (v=(v_x, v_y))는 오프셋 벡터입니다. 위너-킨친 정리(Wiener–Khinchin theorem)를 사용하면, 고속 푸리에 변환을 통해 이 함수를 효율적으로 계산할 수 있습니다.\nautocorr2d = function(x) {\n  y = fft(x/sum(x))\n  abs(gsignal::fftshift(fft(y * Conj(y), inverse = TRUE), MARGIN = 1:2))\n}\n아래에서 우리는 행렬을 ggplot2를 사용하여 (기본 R의 image와 유사하게) 히트맵으로 보여주는 작은 헬퍼 함수를 사용할 것입니다.\nmatrix_as_heatmap = function(m)\n  ggplot(reshape2::melt(m), aes(x = Var1, y = Var2, fill = value)) +\n    geom_tile() + coord_fixed() +\n    scale_fill_continuous(type = \"viridis\") +\n    scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))\n이제 autocorr2d를 세 가지 색상 채널 각각에 별도로 적용해 봅시다. 그 결과는 그림 11.31에 나와 있습니다.\nnm = dimnames(cells)[[3]] ac = lapply(nm, function(i) autocorr2d(cells[,, i])) |&gt; setNames(sub(“^image-”, ““, nm))\nfor (w in names(ac))\n  print(matrix_as_heatmap(ac[[w]]) + ggtitle(w))\n\ncy = dim(cells)[1] / 2\ncx = dim(cells)[2] / 2\nr  = round(sqrt((col(cells[,,1]) - cx)^2 + (row(cells[,,1]) - cy)^2))\n\nmatrix_as_heatmap(r) + ggtitle(\"radius r\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n그림 11.31: 히트맵으로 표시된 cells 이미지의 세 색상 채널의 자기 상관 함수. 중앙의 피크들은 짧은 거리에서의 신호 상관관계에 해당합니다. 또한 방사 좌표 r도 표시되어 있습니다.\n이미지들은 등방성(isotropic)이어야 하므로(즉, 선호되는 방향이 없어야 하므로), 우리는 각도 좌표에 대해 평균을 낼 수 있습니다. 그 결과는 그림 11.32에 나와 있습니다. 우리는 서로 다른 색상 채널의 신호들이 서로 다른 길이 스케일을 가짐을 알 수 있습니다.\naggregate_by_radius = function(x, r)\n  tibble(x = as.vector(x),\n         r = as.vector(r)) |&gt;\n  group_by(r) |&gt;\n  summarize(value = mean(x))\n\nlapply(names(ac), function(w) \n  cbind(channel = w, \n        aggregate_by_radius(ac[[w]], r))\n  ) |&gt; \n  bind_rows() |&gt; \n  dplyr::filter(r &lt;= 50) |&gt; \n  ggplot(aes(x = r, y = value, col = channel)) + geom_line() + \n    scale_color_manual(values = c(`Cy3` = \"red\", `FITC` = \"green\", `DAPI` = \"blue\"))\n서로 다른 채널들 사이의 상호 상관관계(cross-correlation)도 계산할 수 있도록 autocorr2d 함수를 확장해 보세요.\n\n위에서 구현한 autocorr2d에서 sum 정규화의 동기는 무엇인가요?\n다른 계산을 수행하기 전에 x의 평균을 빼는 것이 타당할까요?\n이 함수와 기본 R의 var 및 sd 함수를 통해 계산된 일반적인 경험적 분산 또는 상관관계 사이의 관계는 무엇인가요?\n그림 11.32와 같은 플롯들이 고처리량 스크리닝 환경(즉, 수천 또는 수백만 개의 이미지를 분석해야 할 때)에서 품질 지표를 구축하는 데 어떻게 사용될 수 있을까요?\nautocorr2d의 3차원 또는 (n)차원 확장은 어떤 모습일까요? 그것이 어디에 유용할까요?\n\n\n그림 11.32: 반지름별로 집계된 cells 이미지의 세 색상 채널의 자기 상관 함수.\n연습 문제 11.4\n“이미지 데이터 작업(Working with Image Data)” 워크숍(https://github.com/wolfganghuber/WorkingWithImageData)을 살펴보세요. 이 워크숍은 이 장과 동일한 내용 중 일부를 다른 이미지들로 다루고 있으며, 분할과 광학 흐름(optical flow)에 대한 추가적인 예시들도 포함하고 있습니다.\n연습 문제 11.5\n9장의 우크라이나 도시들에 대해 보로노이 테셀레이션을 계산하고 표시해 보세요. 2D 평면에서 유클리드 거리를 사용한 MDS 좌표를 사용하거나, 대권 거리(great circle distance, 하버사인 공식)를 사용한 위도와 경도를 사용하세요.\n연습 문제 11.6\n광시트 현미경(light sheet microscopy) 15으로부터 3D 이미지 데이터를 다운로드하고, 이를 EBImage Image 객체로 로드하여 데이터를 탐색해 보세요.\n15 예: http://www.digital-embryo.org",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>이미지 분석 (Image Analysis)</span>"
    ]
  },
  {
    "objectID": "12-chap.html",
    "href": "12-chap.html",
    "title": "14  지도 학습 (Supervised Learning)",
    "section": "",
    "text": "14.1 12.1 이 장의 목표\n지도 학습(Supervised Learning) 환경에서는 우리가 얼마나 잘하고 있는지를 판단할 수 있는 잣대, 바로 반응 변수(response) 그 자체가 존재합니다.\n생물학 및 생물의학 응용 분야에서 자주 묻는 질문은 관심 있는 속성(예: 질병 유형, 세포 유형, 환자의 예후)을 다른 하나 이상의 속성, 즉 예측 변수(predictors)를 통해 “예측”할 수 있는지 여부입니다. 종종 우리는 예측해야 할 속성은 알 수 없지만(미래에 있거나 측정하기 어려운 경우), 예측 변수는 알고 있는 상황에 직면합니다. 중요한 점은 관심 있는 속성 또한 알고 있는 학습 데이터(training data) 집합으로부터 예측 규칙을 학습 한다는 것입니다. 규칙을 확보하고 나면, 이를 새로운 데이터에 적용하여 알 수 없는 결과를 실제로 예측하거나, 기저에 있는 생물학적 원리를 더 잘 이해하기 위해 규칙 자체를 분석할 수 있습니다.\n우리가 무엇을 찾고 있는지, 또는 결과가 “맞는지” 어떻게 결정해야 할지 모르는 비지도 학습(Unsupervised Learning)이나 5장, 7장, 9장에서 본 내용과 비교할 때, 지도 학습은 훨씬 더 확고한 기반 위에 있습니다. 목표가 명확하게 명시되어 있고, 우리가 얼마나 잘하고 있는지 측정할 수 있는 간단한 기준이 있기 때문입니다.\n지도 학습 1의 핵심 이슈는 과적합(overfitting)과 일반화 가능성(generalizability)입니다. 훈련 데이터에 대해 100% 정확도를 가지지만 새로운 데이터에 대해서는 형편없는 성능을 보이는 규칙을 만들어 훈련 데이터를 단순히 “암기”한 것일까요? 아니면 우리의 규칙이 연구 중인 시스템의 적절한 패턴을 실제로 포착하여 아직 보지 못한 새로운 데이터에도 적용될 수 있을까요? (그림 12.1)\n1 때때로 통계적 학습(statistical learning)이라는 용어가 거의 같은 의미로 사용됩니다.\n그림 12.1: 과적합(overfitting)의 예: 두 회귀선이 ((x, y))-평면의 데이터(검은 점)에 적합되었습니다. 우리는 이러한 선을 (x) 값이 주어졌을 때 (y) 값을 예측하는 규칙으로 생각할 수 있습니다. 두 선 모두 부드럽지만, 적합(fit)은 대역폭(bandwidth)이라고 불리는 것에서 차이가 나며, 이는 직관적으로 경직성(stiffness)으로 해석될 수 있습니다. 파란색 선은 데이터의 사소한 움직임까지 지나치게 따라가려는 것처럼 보이는 반면, 주황색 선은 전반적인 추세는 포착하지만 세부적인 것은 덜합니다. 파란색 선을 설명하는 데 필요한 유효 매개변수(effective parameters)의 수는 주황색 선보다 훨씬 높습니다. 또한, 추가 데이터를 얻게 된다면 파란색 선은 주황색 선보다 새로운 데이터를 모델링하는 데 더 나쁜 성능을 보일 가능성이 높습니다. 이 장의 뒷부분에서 이러한 개념들 – 훈련 오차(training error)와 테스트 세트 오차(test set error) – 을 공식화할 것입니다. 여기서는 선 적합(line fitting)으로 예시를 들었지만, 이 개념은 예측 모델에 더 일반적으로 적용됩니다.\n이 장에서는 다음을 수행합니다:",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>지도 학습 (Supervised Learning)</span>"
    ]
  },
  {
    "objectID": "12-chap.html#이-장의-목표",
    "href": "12-chap.html#이-장의-목표",
    "title": "14  지도 학습 (Supervised Learning)",
    "section": "",
    "text": "지도 학습 방법의 사용 동기가 되는 예시적인 응용 사례를 살펴봅니다.\n판별 분석(discriminant analysis)이 무엇을 하는지 배웁니다.\n성능 척도(measures of performance)를 정의합니다.\n차원의 저주(curse of dimensionality)를 접하고 과적합(overfitting)이 무엇인지 알아봅니다.\n정규화(regularization) – 특히 벌점화(penalization) – 에 대해 알아보고 일반화 가능성(generalizability)과 모델 복잡성(model complexity)의 개념을 이해합니다.\n교차 검증(cross-validation)을 사용하여 알고리즘의 매개변수를 조정하는 방법을 살펴봅니다.\n방법론 해킹(method hacking)에 대해 논의합니다.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>지도 학습 (Supervised Learning)</span>"
    ]
  },
  {
    "objectID": "12-chap.html#데이터란-무엇인가",
    "href": "12-chap.html#데이터란-무엇인가",
    "title": "14  지도 학습 (Supervised Learning)",
    "section": "14.2 12.2 데이터란 무엇인가?",
    "text": "14.2 12.2 데이터란 무엇인가?\n지도 학습과 비지도 학습 모두에 대한 기본적인 데이터 구조는 (적어도 개념적으로는) 데이터프레임입니다. 여기서 각 행은 객체(object)에 해당하고 열은 객체의 다른 특징(feature, 주로 숫자 값)입니다2. 비지도 학습에서는 특징 값을 기반으로 객체 간의 (비)유사성 관계를 찾는 것(예: 군집화 또는 정렬)을 목표로 하는 반면, 지도 학습에서는 다른 특징들로부터 특징 중 하나의 값을 예측하는 수학적 함수(또는 계산 알고리즘)를 찾는 것을 목표로 합니다. 많은 구현체들은 결측치(missing values)가 없어야 작동하지만, 일부 방법은 약간의 결측 데이터가 있어도 작동하도록 만들 수 있습니다.\n2 이것은 단순화된 설명입니다. 기계 학습(Machine Learning)은 거대한 분야이며, 이 단순한 개념적 그림에 대한 많은 일반화가 이루어졌습니다. 이미 관련 특징을 구축하는 것 자체가 하나의 예술입니다. 11장에서 세포 이미지의 예를 보았고, 더 일반적으로 이미지, 소리, 영화, 자유 텍스트 등에서 특징을 추출할 수 있는 많은 가능성이 있습니다 … 또한, 특징이 전혀 필요 없는 커널 방법(kernel methods)이라는 기계 학습 방법의 변형도 있습니다. 대신 커널 방법은 객체 간의 거리 또는 유사성 척도를 사용합니다. 예를 들어, 두 자연어텍스트 객체를 나타낼 관련 수치 특징을 찾는 것보다 두 객체 간의 유사성 척도를 정의하는 것이 더 쉬울 수 있습니다. 커널 방법은 이 책의 범위를 벗어납니다.\n예측을 목표로 다른 모든 특징들 중에서 선택한 특징을 목표(objective) 또는 반응(response)이라고 합니다. 때로는 선택이 자연스럽지만, 때로는 역할을 뒤집는 것이 유익할 수 있습니다. 특히 생물학적 이해를 위해 예측 함수를 분석하거나 상관관계와 인과관계를 구분하는 데 관심이 있다면 더욱 그렇습니다.\n지도 학습의 프레임워크는 연속형 및 범주형 반응 변수를 모두 포괄합니다. 연속형인 경우 회귀(regression)라고 하고, 범주형인 경우 분류(classification)라고 합니다. 이 구분은 사소한 것이 아닙니다. 손실 함수 (12.5절) 선택과 그에 따른 알고리즘 선택에 꽤 광범위한 영향을 미치기 때문입니다 (Friedman 1997).\n모든 지도 학습 작업에서 고려해야 할 첫 번째 질문은 객체의 수가 예측 변수의 수와 어떻게 비교되는지입니다. 객체가 많을수록 좋으며, 지도 학습에서의 많은 힘든 작업은 유한한(그리고 일반적으로 너무 작은) 훈련 세트가 가진 한계를 극복하는 것과 관련이 있습니다.\n\n그림 12.2: 지도 학습에서 우리는 변수에 두 가지 다른 역할을 부여합니다. 설명 변수를 (X), 반응 변수(들)를 (Y)라고 라벨링했습니다. 또한 두 가지 다른 관측값 세트가 있습니다: 훈련 세트 (X_)과 (Y_), 그리고 테스트 세트 (X_v)와 (Y_v). (아래첨자는 두 세트의 다른 이름인 “learning”과 “validation”을 나타냅니다.)\n일\n이 책에서 범주형 반응 변수를 가진 지도 학습 사례를 접한 예시를 드시오.\n\n14.2.1 12.2.1 동기 부여 예시\n\n14.2.1.1 당뇨병 유형 예측\ndiabetes 데이터셋(Reaven and Miller 1979)은 세 가지 다른 당뇨병 환자 그룹과 그들에게서 측정된 5가지 임상 변수를 제공합니다.\ndata(\"diabetes\", package = \"rrcov\")\nhead(diabetes)\n\n\n    rw fpg glucose insulin sspg  group\n1 0.81  80     356     124   55 normal\n2 0.95  97     289     117   76 normal\n3 0.94 105     319     143  105 normal\n4 1.04  90     356     199  108 normal\n5 1.00  90     323     240  143 normal\n6 0.76  86     381     157  165 normal\n일변량 분포(더 정확하게는 그것들의 밀도 추정치)는 그림 12.3에 나와 있습니다.\nlibrary(\"reshape2\")\nggplot(melt(diabetes, id.vars = \"group\"), aes(x = value, col = group)) +\n geom_density() + facet_wrap( ~variable, ncol = 1, scales = \"free\") +\n theme(legend.position = \"bottom\")\n\n그림 12.3: 1차원 분포에서 이미 개별 변수 중 일부가 환자가 어떤 그룹에 속할 가능성이 높은지 잠재적으로 예측할 수 있음을 알 수 있습니다. 우리의 목표는 변수들을 결합하여 이러한 1차원 예측 모델보다 개선하는 것입니다.\n변수들은 데이터셋의 매뉴얼 페이지와 논문(Reaven and Miller 1979)에 설명되어 있습니다:\n\nrw: 상대 체중 (relative weight)\nfpg: 공복 혈장 포도당 (fasting plasma glucose)\nglucose: 3시간 경구 포도당 내성 검사(OGTT)에 대한 혈장 포도당 곡선 아래 면적\ninsulin: OGTT에 대한 혈장 인슐린 곡선 아래 면적\nsspg: 정상 상태 혈장 포도당 반응 (steady state plasma glucose response)\ngroup: 정상(normal), 화학적 당뇨병(chemical diabetes), 명백한 당뇨병(overt diabetes)\n\n\n\n14.2.1.2 세포 표현형 예측\nNeumann 등(2010)은 라이브 셀 이미징(live-cell imaging)을 사용하여 인간 암세포를 관찰했습니다. 세포의 히스톤이 녹색 형광 단백질(GFP)로 태그되도록 유전적으로 조작되었습니다. 게놈 전체 RNAi 라이브러리를 세포에 적용했고, 각 siRNA 교란에 대해 며칠 동안 수백 개의 세포 영상을 기록하여 각 유전자의 고갈이 세포 주기, 핵 형태 및 세포 증식에 어떤 영향을 미치는지 확인했습니다. 그들의 논문은 각 세포 핵의 시각적 외관을 정량화하고 정상적인 유사 분열 상태 또는 비정상적인 핵을 예측할 수 있게 해주는 자동화된 이미지 분류 알고리즘의 사용을 보고합니다. 이 알고리즘은 인간 전문가가 주석을 단 약 3000개의 세포 데이터로 훈련되었습니다. 그런 다음 거의 20억 개의 핵 이미지에 적용되었습니다(그림 12.4). 자동화된 이미지 분류를 사용하여 확장성(20억 개의 이미지를 수동으로 주석 처리하는 데는 오랜 시간이 걸릴 것입니다)과 객관성을 제공했습니다.\n\n그림 12.4: 데이터는 영상에서 나온 (2^9)개 핵의 이미지였습니다. 이미지는 핵을 식별하기 위해 분할되었고, 각 핵에 대해 크기, 모양, 밝기 및 픽셀 강도의 결합 분포에 대한 다소 추상적인 정량적 요약에 해당하는 수치 특징이 계산되었습니다. 특징으로부터 세포는 16가지 다른 핵 형태 클래스로 분류되었으며, 막대 그래프의 행으로 표시됩니다. 각 클래스에 대한 대표 이미지는 가운데 열에 흑백으로 표시됩니다. 클래스 빈도는 막대의 길이로 표시되며 매우 불균형합니다.\n\n\n14.2.1.3 배아 세포 상태 예측\n우리는 3장, 5장, 7장에서 이미 본 마우스 배아 데이터(Ohnishi et al. 2014)를 다시 살펴볼 것입니다. 12.3.2절과 12.6.3절에서 유전자 발현 측정값으로부터 세포 상태와 유전자형을 예측해 보겠습니다.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>지도 학습 (Supervised Learning)</span>"
    ]
  },
  {
    "objectID": "12-chap.html#선형-판별-linear-discrimination",
    "href": "12-chap.html#선형-판별-linear-discrimination",
    "title": "14  지도 학습 (Supervised Learning)",
    "section": "14.3 12.3 선형 판별 (Linear discrimination)",
    "text": "14.3 12.3 선형 판별 (Linear discrimination)\n가장 간단한 판별 문제 중 하나부터 시작해 보겠습니다3. 두 개의 연속형 특징으로 설명되는(따라서 객체는 2D 평면의 점으로 생각할 수 있는) 객체가 있고, 이들은 세 그룹으로 나뉩니다. 우리의 목표는 2D 공간의 선인 클래스 경계를 정의하는 것입니다.\n3 틀림없이 가장 간단한 문제는 단일 연속형 특징, 두 개의 클래스, 그리고 두 그룹을 구별하기 위한 단일 임계값을 찾는 작업입니다 – 그림 6.2와 같이.\n\n14.3.1 12.3.1 당뇨병 데이터\ndiabetes 데이터의 sspg와 glucose 변수로부터 group을 예측할 수 있는지 알아봅시다. 먼저 데이터를 시각화하는 것이 항상 좋은 아이디어입니다(그림 12.5).\nggdb = ggplot(mapping = aes(x = sspg, y = glucose)) +\n  geom_point(aes(colour = group), data = diabetes)\nggdb\n\n그림 12.5: diabetes 데이터의 두 변수에 대한 산점도. 각 점은 샘플이며, 색상은 group 변수에 인코딩된 당뇨병 유형을 나타냅니다.\n선형 판별 분석(Linear Discriminant Analysis, LDA)이라는 방법으로 시작하겠습니다. 이 방법은 분류의 초석이며, 더 복잡한(그리고 때로는 더 강력한) 알고리즘 중 많은 것들이 실제로는 LDA의 일반화일 뿐입니다.\nlibrary(\"MASS\")\ndiabetes_lda = lda(group ~ sspg + glucose, data = diabetes)\ndiabetes_lda\n\n\nCall:\nlda(group ~ sspg + glucose, data = diabetes)\n\nPrior probabilities of groups:\n   normal  chemical     overt \n0.5241379 0.2482759 0.2275862 \n\nGroup means:\n             sspg   glucose\nnormal   114.0000  349.9737\nchemical 208.9722  493.9444\novert    318.8788 1043.7576\n\nCoefficients of linear discriminants:\n                LD1         LD2\nsspg    0.005036943 -0.01539281\nglucose 0.005461400  0.00449050\n\nProportion of trace:\n   LD1    LD2 \n0.9683 0.0317 \n\n\nghat = predict(diabetes_lda)$class\ntable(ghat, diabetes$group)\n\n\n          \nghat       normal chemical overt\n  normal       69       12     1\n  chemical      7       24     6\n  overt         0        0    26\n\n\nmean(ghat != diabetes$group)\n\n\n[1] 0.1793103\n질문 12.1\n위 출력의 다른 부분들은 무엇을 의미합니까?\n이제 LDA 결과를 시각화해 보겠습니다. 각 세 그룹에 대한 예측 영역을 그릴 것입니다. 이를 위해 점 그리드를 만들고 각각에 대해 예측 규칙을 사용합니다. 그런 다음 LDA의 메커니즘을 좀 더 깊이 파고들어 클래스 중심(diabetes_lda$means)과 적합된 공분산 행렬(diabetes_lda$scaling)에 해당하는 타원을 그릴 것입니다. 이 시각화를 조립하려면 약간의 코드를 작성해야 합니다.\nmake1Dgrid = function(x) {\n  rg = grDevices::extendrange(x)\n  seq(from = rg[1], to = rg[2], length.out = 100)\n}\n데이터 범위를 커버하는 (100 ) 그리드인 예측용 점들을 설정합니다.\ndiabetes_grid = with(diabetes,\n  expand.grid(sspg = make1Dgrid(sspg),\n              glucose = make1Dgrid(glucose)))\n예측을 수행합니다.\ndiabetes_grid$ghat =\n  predict(diabetes_lda, newdata = diabetes_grid)$class\n그룹 중심입니다.\ncenters = diabetes_lda$means\n타원을 계산합니다. 단위원(360개의 변을 가진 다각형으로 근사)에서 시작하여 LDA 출력의 해당 아핀 변환(affine transformation)을 적용합니다.\nunitcircle = exp(1i * seq(0, 2*pi, length.out = 360)) |&gt;\n          (\\(z) cbind(Re(z), Im(z)))() \nellipse = unitcircle %*% solve(diabetes_lda$scaling) |&gt; as_tibble()\n세 개의 타원 모두, 각 그룹 중심에 하나씩입니다.\nlibrary(\"dplyr\")\nellipses = lapply(rownames(centers), function(gr) {\n  mutate(ellipse, \n     sspg    = sspg    + centers[gr, \"sspg\"],\n     glucose = glucose + centers[gr, \"glucose\"],\n     group   = gr)\n}) |&gt; bind_rows()\n이제 그릴 준비가 되었습니다 (그림 12.6).\nggdb + geom_raster(aes(fill = ghat),\n            data = diabetes_grid, alpha = 0.25, interpolate = TRUE) +\n    geom_point(data = as_tibble(centers), pch = \"+\", size = 8) +\n    geom_path(aes(colour = group), data = ellipses) +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0))\n\n그림 12.6: 그림 12.5와 동일하지만, LDA 모델의 분류 영역이 표시되어 있습니다. 세 개의 타원은 클래스 중심과 LDA 모델의 공분산 행렬을 나타냅니다. 공분산 행렬은 단 하나뿐이며 세 클래스 모두에 대해 동일하다는 점에 유의하십시오. 따라서 타원의 크기와 방향도 세 클래스 모두에 대해 동일하며 중심만 다릅니다. 이들은 동일한 클래스 멤버십 확률의 등고선을 나타냅니다.\n질문 12.2\nchemical 과 overt 사이의 예측 영역 경계가 그룹 중심 사이의 선과 수직이 아닌 이유는 무엇입니까?\n해결책\n타원이 원이라면 경계는 수직일 것입니다. 일반적으로 경계는 동일 클래스 확률의 등고선에 접하며, 등고선의 타원 형태로 인해 경계는 일반적으로 중심 사이의 선과 수직이 아닙니다.\n질문 12.3\n모든 군집 중심에서 멀리 떨어진 2D 평면 영역에서의 예측에 대해 얼마나 확신할 수 있습니까?\n해결책\n어떤 군집 중심에서도 멀리 떨어진 예측은 비판적으로 평가해야 합니다. 이는 LDA 모델이 좋지 않을 수 있거나 예측을 뒷받침할 훈련 데이터가 근처에 없을 수 있는 영역으로의 외삽(extrapolation)에 해당하기 때문입니다. 가장 가까운 중심까지의 거리를 특정 지점에 대한 예측 신뢰도의 척도로 사용할 수 있습니다. 그러나 리샘플링 및 교차 검증 기반 방법이 더 일반적이고 보통 더 신뢰할 수 있는 척도를 제공한다는 것을 보게 될 것입니다.\n질문 12.4\nnormal 과 chemical 사이의 예측 영역 경계가 중심 사이의 중간이 아니라 normal 에 유리하게 이동된 이유는 무엇입니까? 힌트: lda의 prior 인수를 살펴보세요. 균일 사전 확률(uniform prior)로 다시 시도해 보세요.\n해결책\n다음 코드 청크의 결과는 그림 12.7에 나와 있습니다. 접미사 _up은 “uniform prior”의 줄임말입니다.\ndiabetes_up = lda(group ~ sspg + glucose, data = diabetes,\n  prior = (\\(n) rep(1/n, n)) (nlevels(diabetes$group)))\n\ndiabetes_grid$ghat_up =\n  predict(diabetes_up, newdata = diabetes_grid)$class\n\nstopifnot(all.equal(diabetes_up$means, diabetes_lda$means))\n\nellipse_up  = unitcircle %*% solve(diabetes_up$scaling) |&gt; as_tibble()\nellipses_up = lapply(rownames(centers), function(gr) {\n  mutate(ellipse_up, \n     sspg    = sspg    + centers[gr, \"sspg\"],\n     glucose = glucose + centers[gr, \"glucose\"],\n     group   = gr)\n}) |&gt; bind_rows()\n\nggdb + geom_raster(aes(fill = ghat_up),\n            data = diabetes_grid, alpha = 0.4, interpolate = TRUE) +\n    geom_point(data = data.frame(centers), pch = \"+\", size = 8) +\n    geom_path(aes(colour = group), data = ellipses_up) +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0))\n\n그림 12.7: 그림 12.6과 동일하지만, 균일 클래스 사전 확률을 사용했습니다.\nstopifnot 라인은 클래스 중심이 사전 확률과 독립적이기 때문에 동일함을 확인해 줍니다. 결합 공분산은 그렇지 않습니다.\n질문 12.5\n그림 12.6과 12.7은 타원을 통한 적합된 LDA 모델과 영역 색칠을 통한 예측 영역을 모두 보여줍니다. 이 시각화의 어떤 부분이 모든 종류의 분류 방법에 대해 일반적이며, 어떤 부분이 방법 특정적입니까?\n해결책\n예측 영역은 “블랙박스” 방법을 포함한 모든 분류 방법에 대해 표시될 수 있습니다. 그림 12.6과 12.7의 군집 중심과 타원은 방법 특정적입니다.\n질문 12.6\nglucose와 sspg만 사용하는 대신 5개의 변수를 모두 사용하면 예측 정확도에 어떤 차이가 있습니까?\n해결책\ndiabetes_lda5 = lda(group ~ rw + fpg + glucose + sspg + insulin, data = diabetes)\ndiabetes_lda5\n\n\nCall:\nlda(group ~ rw + fpg + glucose + sspg + insulin, data = diabetes)\n\nPrior probabilities of groups:\n   normal  chemical     overt \n0.5241379 0.2482759 0.2275862 \n\nGroup means:\n                rw       fpg   glucose     sspg  insulin\nnormal   0.9372368  91.18421  349.9737 114.0000 172.6447\nchemical 1.0558333  99.30556  493.9444 208.9722 288.0000\novert    0.9839394 217.66667 1043.7576 318.8788 106.0000\n\nCoefficients of linear discriminants:\n                  LD1          LD2\nrw       1.3624356881 -3.784142444\nfpg     -0.0336487883  0.036633317\nglucose  0.0125763942 -0.007092017\nsspg     0.0042431866  0.001134070\ninsulin -0.0001022245 -0.006173424\n\nProportion of trace:\n   LD1    LD2 \n0.8812 0.1188 \n\n\nghat5 = predict(diabetes_lda5)$class\ntable(ghat5, diabetes$group)\n\n\n          \nghat5      normal chemical overt\n  normal       73        5     1\n  chemical      3       31     5\n  overt         0        0    27\n\n\nmean(ghat5 != diabetes$group)\n\n\n[1] 0.09655172\nQuestion 12.7\n점 그리드로부터 분류하여 예측 영역을 근사하는 대신, 선형 판별 계수로부터 구분선을 명시적으로 계산하세요.\nSolution\n(Hastie, Tibshirani, and Friedman 2008)의 4.3절, 식 (4.10)을 참조하십시오.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>지도 학습 (Supervised Learning)</span>"
    ]
  },
  {
    "objectID": "13-chap.html",
    "href": "13-chap.html",
    "title": "15  실험 설계 (Design of Experiments)",
    "section": "",
    "text": "15.1 13.1 이 장의 목표\n영화 감독이 촬영된 장면이 너무 많아지기 전에 조명이나 촬영 문제를 수정하기 위해 매일 촬영분(daily takes)을 확인하는 것과 마찬가지로, 실험의 모든 실행이 완료될 때까지 기다리지 않고 데이터를 살펴보는 것이 좋습니다. 중간 데이터 분석과 시각화는 예상치 못한 변동 원인을 추적하고 프로토콜을 조정할 수 있게 해줍니다. 실험의 순차적 설계(Mead 1990)에 대해 많은 것이 알려져 있지만, 좀 더 실용적인 환경에서도 변동 원인이 발생할 때 이를 인지하고 조정하는 것이 중요합니다.\n우리는 이제 많은 다양한 생물학적 데이터 세트와 데이터 유형, 그리고 이를 분석하는 방법들을 보았습니다. 이 책을 마무리하며 우리가 배운 몇 가지 일반적인 교훈을 요약해 보겠습니다. 세 가지 훌륭한 조언은 다음과 같습니다:\n1 1938년 제1회 인도 통계 회의 대통령 연설. Sankhya 4, 14-17.\n이 장에서는 다음을 수행합니다:",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>실험 설계 (Design of Experiments)</span>"
    ]
  },
  {
    "objectID": "13-chap.html#이-장의-목표",
    "href": "13-chap.html#이-장의-목표",
    "title": "15  실험 설계 (Design of Experiments)",
    "section": "",
    "text": "실험의 유형과 각 유형에 대해 우리가 가질 수 있는 통제 수준에 대한 간단한 분류를 개발합니다.\n가변성의 서로 다른 유형인 오차(error), 노이즈(noise), 편향(bias)을 구별하는 방법을 요약합니다.\n우리가 걱정해야 할 것들인 혼동(confounding), 의존성(dependencies), 배치 효과(batch effects)에 대해 논의합니다. 유명한 질문인 반복 실험은 몇 번이나 필요한가? 에 대해 답해 봅니다.\n평균-분산 관계(mean-variance relationships)와 이것이 데이터 변환 여부 및 방법에 대해 어떤 정보를 주는지 핵심 아이디어를 요약합니다.\n계산 기법과 도구는 작업을 완료하는 데 필수적입니다. 효율적인 워크플로우 설계, 데이터 표현 및 계산에 대해 논의합니다.\n분석 워크플로우에서의 데이터 요약 단계와 통계적 충분성(sufficiency) 문제를 인식하려고 노력합니다. 그래야 “상류(upstream)”의 어떤 단계에서 중요한 정보를 버려 하류에서 문제를 겪는 일을 방지할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>실험 설계 (Design of Experiments)</span>"
    ]
  },
  {
    "objectID": "13-chap.html#실험의-유형",
    "href": "13-chap.html#실험의-유형",
    "title": "15  실험 설계 (Design of Experiments)",
    "section": "15.2 13.2 실험의 유형",
    "text": "15.2 13.2 실험의 유형\n\n15.2.0.1 “충분히 좋은” 것의 예술.\n우리의 자원은 유한하고, 장비는 완벽하지 않으며, 현실 세계는 복잡하다는 사실을 다루기 위해 실험 설계가 필요합니다. 우리는 그럼에도 불구하고 가능한 최선의 결과를 얻고 싶어 합니다. 이는 필연적으로 어려운 결정과 상충 관계(trade-off)를 초래합니다. 실험 설계(Experimental design)는 이러한 결정을 합리화하는 것을 목표로 합니다. 우리의 실험적 개입과 측정 장비는 정밀도와 정확도가 제한적입니다. 종종 우리는 처음에 이러한 한계를 알지 못하며, 이를 추정하기 위해 예비 데이터(preliminary data)를 수집해야 합니다. 우리는 관심 있는 현상을 직접 관찰하기보다 간접적으로만 관찰할 수 있을지도 모릅니다. 우리의 처리 조건은 피하기 어렵고 바람직하지 않은 부작용을 가질 수 있으며, 우리의 측정값에는 방해 신호나 “배경 노이즈”가 섞여 있을 수 있습니다. 표본 크기는 실무적이고 경제적인 이유로 제한됩니다. 비현실적인 이상향을 처방하는 것은 의미가 없습니다. 우리는 실용적이고 실행 가능한 선택을 해야 합니다. (Bacher and Kendziorski 2016)의 인용구는 이를 명확하게 설명합니다: “일반적으로 말해서, 잘 설계된 실험이란 충분한 검정력(power)을 가지고 있으며, 측정값에 체계적으로 영향을 미칠 수 있는 기술적 아티팩트와 생물학적 특징들이 균형을 이루거나 무작위화되거나 다른 방식으로 제어되어 연구 중인 효과에 대한 여러 가지 설명 가능성을 최소화하는 것입니다.”\n우선, 실험의 주요 유형들에 대해 논의해 봅시다. 각 유형은 서로 다른 접근 방식을 필요로 하기 때문입니다.\n통제된 실험(controlled experiment)에서 우리는 모든 관련 변수에 대해 통제권을 가집니다: 연구 중인 (모델) 시스템, 환경 조건, 실험적 리드아웃 등입니다. 예를 들어, 정의된 배지, 온도 및 대기 상태의 실험실 조건에서 자라는 잘 특성화된 세포주를 가지고 정밀한 양의 약물을 투여한 후, 72시간 후에 특정 경로 리포터의 활성을 측정할 수 있습니다.\n연구(study)에서 우리는 통제권이 적습니다: 측정된 결과에 영향을 줄 수 있는 중요한 조건들이 연구자의 통제 하에 있지 않으며, 대개 윤리적 우려나 물류상의 제약 때문입니다. 예를 들어, 생태학적 현장 연구에서 이는 날씨, 영양 자원의 가용성 또는 포식자의 활동일 수 있습니다. 관찰 연구(observational study)에서는 심지어 관심 변수조차 연구자에 의해 통제되지 않습니다. 예를 들어, 임상 시험에서 이는 개별 피험자의 그룹 할당일 수 있습니다. 혼동(13.4.1절)의 가능성이 많기 때문에 관찰 연구의 해석은 어려울 수 있습니다. 여기서 “상관관계는 인과관계가 아니다”라는 오래된 격언이 적용됩니다.\n무작위 통제 시험(randomized controlled trial)에서 우리는 여전히 결과에 영향을 미치는 많은 요인들에 대한 통제권 결여 문제를 다루어야 하지만, 관심 변수의 할당(예: 임상 시험에서의 치료 유형)을 통제하므로, 표본 크기가 충분히 크다면 모든 성가신 효과들이 상쇄되고 관찰된 효과가 진정으로 개입에 인과적으로 할당될 수 있을 것으로 기대할 수 있습니다. 이러한 시험은 대개 전향적(prospective) 2입니다. 즉, 피험자를 그룹에 할당할 당시에는 결과를 알지 못합니다.\n2 반의어는 후향적(retrospective)입니다. 관찰 연구는 전향적일 수도 있고 후향적일 수도 있습니다.\n메타 분석(meta-analysis)은 여러 이전 실험이나 연구에 대한 관찰 연구입니다. 메타 분석의 한 가지 동기는 유효 표본 크기를 늘려 검정력을 높이는 것입니다. 또 다른 동기는 연구자의 편향이나 다른 편향으로 고통받거나, 검정력이 부족하거나, 혹은 다른 방식으로 결함이 있거나 무작위적일 수 있는 개별 실험이나 연구의 한계를 극복하는 것입니다. 여러 연구의 결과를 합침으로써 이러한 “연구 수준”의 문제들이 상쇄되기를 기대하는 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>실험 설계 (Design of Experiments)</span>"
    ]
  },
  {
    "objectID": "13-chap.html#오차-분할-편향과-노이즈",
    "href": "13-chap.html#오차-분할-편향과-노이즈",
    "title": "15  실험 설계 (Design of Experiments)",
    "section": "15.3 13.3 오차 분할: 편향과 노이즈",
    "text": "15.3 13.3 오차 분할: 편향과 노이즈\n\n\n\n통계학자들은 측정값이 참값에서 벗어난 모든 편차를 오차(error)라는 용어로 부릅니다. 이는 일상적인 단어 사용과는 다릅니다. 통계학에서 오차는 피할 수 없는 삶의 한 측면입니다. 그것은 “나쁜” 것이 아니라 소중히 여기고, 계산하고, 길들이고, 통제해야 하는 것입니다.\n\n\n통계학자들은 측정값이 참값에서 벗어난 모든 편차를 오차(error)라는 용어로 부릅니다. 이는 일상적인 단어 사용과는 다릅니다. 통계학에서 오차는 피할 수 없는 삶의 한 측면입니다. 그것은 “나쁜” 것이 아니라 소중히 여기고, 계산하고, 길들이고, 통제해야 하는 것입니다.\n우리는 크게 두 가지 유형의 오차를 구분합니다. 첫 번째는 우리가 노이즈(noise)라고 부르는 것으로, 충분한 반복 실험을 수행하기만 하면 “상쇄”됩니다. 두 번째는 편향(bias)이라고 부르는 것으로, 그대로 남아 있으며 반복 실험이 많아질수록 오히려 더 분명해집니다. 그림 12.18의 과녁을 떠올려 보세요: 아래쪽 패널에는 노이즈는 많지만 편향은 없으며, 점 구름의 중심은 정확한 위치에 있습니다. 위쪽 패널에는 노이즈는 훨씬 적지만 편향이 있습니다. 아무리 많은 반복 실험을 해도 점들의 중심이 잘못된 위치에 있다는 사실을 해결할 수는 없습니다.\n편향은 노이즈보다 다루기 더 어렵습니다. 노이즈는 반복 실험을 살펴보는 것만으로도 쉽게 인식되고 더 많은 반복 실험을 분석함에 따라 상쇄됩니다. 편향의 경우, 그것이 있다는 것을 인식하는 것조차 어려울 수 있으며, 대개 어떤 정량적 모델을 사용하여 그것을 측정하고 조정할 방법을 찾아야 합니다.\n질문 13.1\n고처리량 데이터에서 편향을 모델링했던 이전 장의 예시 두 가지를 드세요.\n해결책\n예를 들어, 8장에서 우리는 감마-포아송 분포로 샘플링 노이즈를 모델링했고, 라이브러리 크기 인자로 시퀀싱 깊이 편향을 추정하여 차등 발현 검정 시 이를 고려했습니다. 또한 우리는 일반화 선형 모델에 차단 요인(blocking factor)을 도입하여 두 가지 서로 다른 프로토콜(단일말단, 쌍말단)에 의해 발생한 샘플링 편향을 모델링했습니다.\n\n15.3.1 13.3.1 오차 모델: 노이즈는 보는 사람의 관점에 달려 있다\nDNA 폴리머가 관여하는 대부분의 생화학적 또는 물리적 프로세스의 효율성은 그들의 서열 내용에 의존합니다. 예를 들어, 긴 호모폴리머(homopolymer) 구간, 팔린드롬(palindromes), 전체적 또는 국소적 GC 함량의 발생은 PCR의 효율이나 나노포어를 통해 폴리머가 당겨지는 역학을 변화시킬 수 있습니다. 이러한 효과의 크기와 특성을 모델링하는 것은 어렵습니다. 이들은 농도, 온도, 사용된 효소 등과 같은 요인들에 미묘하게 의존합니다. 그렇다면, RNA-Seq 데이터를 볼 때 GC 함량을 노이즈로 취급해야 할까요, 아니면 편향으로 취급해야 할까요?\n질문 13.2\nDESeq2 방법은 이 문제를 어떻게 다루나요?\n해결책\nDESeq2는 두 가지 옵션을 모두 제공합니다. 샘플별 샘플링 편향을 모델링하기 위해 크기 인자가 사용된다면, 이러한 효과는 명시적으로 모델링되지 않습니다.\n참고: 이 경우 각 유전자에 대해 그러한 편향이 모든 샘플에서 동일하게 작용하여 차등 발현 분석 목적상 상쇄될 것이라는 가정이 깔려 있습니다. 그러한 효과가 샘플별로 다른 한, 그것은 노이즈로 취급됩니다. 그러나 비네트에 설명된 대로, DESeq2는 행렬에 대해 샘플 및 유전자 의존적인 정규화 인자를 지정할 수 있게 해주며, 이는 그러한 편향들에 대한 명시적인 추정치를 포함하기 위한 용도입니다.\n여기서 명사 _샘플(sample)_은 관례상 카운트 행렬의 한 열, 예를 들어 한 생물학적 조건의 한 반복 실험에 해당하는 하나의 시퀀싱 라이브러리를 의미함을 기억하세요. 동일한 용어(여기서는 동사 형태인 샘플링(sampling))는 “분포로부터 얻은 데이터 표본”과 같이 좀 더 일반적인 통계적 의미로도 사용됩니다. 이러한 모호함을 피할 쉬운 방법은 없으므로, 우리는 단지 이를 인지하고 있어야 합니다.\n공식적인 오차 모델은 가변성을 노이즈와 편향으로 분해하는 데 도움을 줄 수 있습니다. 여러분이 접해 보았을 표준적인 분해는 ANOVA(분산 분석)라고 불립니다. 이러한 유형의 모델에서 가변성은 제곱합으로 측정되고 그 기원에 따라 할당됩니다. 예를 들어, 12장에서 선형 판별 분석(LDA)으로 지도 분류를 수행할 때, 우리는 전체 제곱합 (C)를 다음과 같이 계산했습니다.\n[ C_{} = C_{} + C_{}. ]\n하지만 대개 그러한 분해를 하는 방법은 여러 가지가 있습니다: 한 단계에서 그룹 내 변동(노이즈)으로 간주되었던 효과가 올바른 (하위)그룹이 할당되면 그룹 간 효과로 간주될 수 있습니다.\n\n\n\n아마도 이것은 “정밀 의료”의 비전과 유사할 것입니다: 치료 실패나 불필요한 치료를 포함하는 그룹 내 변동을, 모든 그룹이 정확히 필요한 것을 얻는 그룹 간 변동으로 변환하는 더 나은 환자 계층화 말이죠.\n\n\n아마도 이것은 “정밀 의료”의 비전과 유사할 것입니다: 치료 실패나 불필요한 치료를 포함하는 그룹 내 변동을, 모든 그룹이 정확히 필요한 것을 얻는 그룹 간 변동으로 변환하는 더 나은 환자 계층화 말이죠.\n\n15.3.1.1 결정론 대 우연.\n\n그림 13.1: 세심하게 제작된 동전 던지기 기계는 결정론적인 동전 던지기 결과를 제공하도록 만들어질 수 있습니다.\n모두가 동전 던지기의 결과를 무작위라고 생각하며, 따라서 노이즈의 완벽한 예라고 봅니다. 하지만 우리가 동전 던지기의 초기 조건을 꼼꼼하게 기록하고 역학 방정식을 푼다면, 어느 쪽이 나올 확률이 더 높은지 예측할 수 있습니다 (Diaconis, Holmes, and Montgomery 2007).\n따라서 어떤 효과나 프로세스가 무작위인지 결정론적인지 묻기보다는, 우리가 그것을 결정론적으로(편향으로) 모델링하고 싶은지, 아니면 세부 사항을 무시하고 확률적으로 처리하여 확률 모델링(노이즈)을 사용할 것인지 말하는 것이 더 유익합니다. 이런 의미에서 확률 모델은 우리의 무지를 정량화하고 불확실성을 길들이는 방법입니다.\n\n\n15.3.1.2 잠재 요인.\n때때로 우리는 편향을 일으키는 요인들을 명시적으로 알고 있습니다. 예를 들어 실험의 서로 다른 단계에서 서로 다른 시약 배치가 사용된 경우입니다. 우리는 이를 배치 효과(batch effects)라고 부릅니다 (Jeffrey T. Leek et al. 2010). 다른 때에는 그러한 요인들이 작용할 것으로 예상하지만 그것들에 대한 명시적인 기록이 없을 수 있습니다. 우리는 이를 잠재 요인(latent factors)이라고 부릅니다. 우리는 그것들을 노이즈에 추가되는 것으로 취급할 수 있으며, 4장에서 혼합 모델을 사용하여 그렇게 하는 방법을 보았습니다. 하지만 이것만으로는 충분하지 않을 수 있습니다: 고차원 데이터에서 잠재 요인에 의해 발생하는 노이즈는 상관관계를 갖는 경향이 있으며, 이는 잘못된 추론으로 이어질 수 있습니다 (Jeffrey T. Leek et al. 2010). 좋은 소식은 이러한 동일한 상관관계가 데이터로부터 잠재 요인을 추정하고, 이를 편향으로 모델링하여 노이즈를 줄이는 데 활용될 수 있다는 것입니다 (Jeffrey T. Leek and Storey 2007; Stegle et al. 2010).\n\n\n\n15.3.2 13.3.2 생물학적 반복 대 기술적 반복\n질문 13.3\n체중 감량 약물이 효과가 있는지 테스트하고 싶다고 가정해 봅시다. 다음 중 어떤 연구 설계를 사용하시겠습니까?\n\n한 사람의 체중을 밀리그램 단위의 정밀 저울로 20번 반복해서 잽니다. 그는 다이어트를 따르고, 4주 후에 다시 20번 반복해서 잽니다.\n10명의 사람이 각자 자신의 집 저울로 한 번씩 재고 그 숫자를 보고합니다. 4주 후에 다시 재서 보고합니다.\n\n첫 번째 옵션이 오래된 장비를 사용하는 단 10명보다 정밀한 장비로 20번의 반복을 하므로 반드시 더 나을까요?\n해결책\n우리가 여기서 가진 것은 기술적 반복(technical replicates) 대 생물학적 반복(biological replicates)의 차이에 대한 (노골적인) 예시입니다. 반복 실험의 횟수보다 어떤 유형의 변동이 그것들에 영향을 미치도록 허용되었는지가 더 중요합니다. 첫 번째 설계의 20번의 반복은 우리가 이미 충분히 정밀하게 알고 있는 것을 다시 측정하는 데 낭비되고 있습니다. 반면 훨씬 더 중요한 질문인 –그 효과가 다른 사람들에게 어떻게 일반화되는가– 는 비록 실제로는 더 많은 사람이 필요하겠지만 두 번째 설계에서 다루어지기 시작합니다.\n참고: 추론이나 일반화는 우리가 해당 모집단의 대표적이고 무작위화된 표본을 우리 연구에 가지고 있을 때만 그 모집단에 대해 이루어질 수 있습니다. 첫 번째 사례에서 체중 감량이 일어난다면, 오직 그 당시의 그 사람에 대해서만 추론할 수 있을 것입니다.\n생물학적 실험에서도 유사한 질문들이 발생합니다. 예를 들어, 동일한 세포주에서 5번의 반복 실험을 하시겠습니까, 아니면 3개의 서로 다른 세포주에서 각각 한 번씩 하시겠습니까?\n질문 13.4\n1000 Genomes 프로젝트에서 사용된 시퀀싱 기술로 신뢰할 수 있는 변이 호출(variant calling)을 하려면 게놈당 약 (30)의 커버리지가 필요합니다. 하지만 생성된 데이터의 평균 깊이는 1,092명에 대해 5.1이었습니다 (1000 Genomes Project Consortium 2012). 왜 그러한 연구 설계가 선택되었을까요?\n해결책\n프로젝트의 목표는 흔한 유전 변이를 찾는 것, 즉 모집단에서 유병률이 예를 들어 1% 이상인 변이를 찾는 것이었습니다. 개별 사람들의 고신뢰도 유전자형을 판독하는 것이 목표가 아니었습니다. 따라서 적은 수의 개인을 고커버리지로 샘플링하는 것보다(예: 182명을 30x로), 많은 개인을 저커버리지로 샘플링하는 것이(예: 1092명을 5x로) 더 비용 효율적이었습니다. 이런 방식으로 흔한 변이들은 1000명 중 여러 명에게 존재할 것이므로 여전히 (&gt;=30)의 커버리지로 발견될 것이며((1092 % = 55)), 더 많은 변이들이 발견되고 모집단 빈도에 대한 더 정확한 추정치를 얻을 수 있었습니다.\n기술적 대 생물학적 반복이라는 용어는 어느 정도 가치가 있지만, 종종 너무 거칠습니다. 관찰된 효과는 실험실, 한 실험실 내의 다른 작업자, 기술, 동일 기술의 다른 기계, 프로토콜의 다른 변체, 계통, 새끼(litters), 성별, 개별 동물 등 여러 수준에서 일반화될 수도 있고 그렇지 않을 수도 있습니다. 복제 수준의 이름을 더 명시적으로 부르는 것이 좋습니다.\n\n\n15.3.3 13.3.3 단위 대 폴드 변화\n물리학에서의 측정값은 대개 미터, 킬로그램, 초와 같은 SI3 단위의 배수로 보고됩니다. 호주의 한 실험실에서 한 기구로 측정한 미터 단위의 길이는 일년 후 캐나다의 한 실험실에서 다른 기구로 측정한 길이나, 먼 은하계의 외계인 과학자가 측정한 길과 직접적으로 비교 가능합니다. 생물학에서 이만큼 표준화된 측정을 하는 것은 드물거나 실용적이지 않습니다. 여기서의 상황은 사람의 신체 부위(피트, 인치 등)가 길이 측정에 사용되고, 이러한 신체 부위의 크기가 마을이나 나라마다, 하물며 은하계마다 다른 상황과 더 비슷합니다.\n3 국제 단위계 (프랑스어: Système International d’Unités)\n생물학자들은 종종 측정값을 어떤 국소적이고 다소 임의적인 참조값의 배수(즉, 그것에 대한 폴드 변화)로 보고합니다. 이것의 문제는 폴드 변화와 비율이 분수라는 것입니다. 분모가 무작위 변수이므로(실험실마다, 아마도 실험마다 변하기 때문에), 이는 높은 불안정성과 실험 간의 매우 불균등한 분산을 초래할 수 있습니다. 이 장의 뒷부분에 나오는 변환과 충분성에 관한 섹션을 참조하십시오. 겉보기에 절대적인 수치가 존재하는 경우에도(예: RNA-Seq 실험의 TPKM 값), 실험 특이적인 샘플링 편향 때문에 이들은 보편적인 단위로 변환되지 않으며, 종종 정밀도에 대한 지표가 부족합니다.\n\n\n15.3.4 13.3.4 규칙적인 노이즈와 치명적인 노이즈\n규칙적인 노이즈는 독립적인 정규 분포, 포아송, 또는 감마-포아송이나 라플라스와 같은 혼합물과 같은 단순한 확률 모델로 모델링될 수 있습니다. 우리는 그러한 노이즈를 데이터 분석 시 고려하고 이례적으로 크거나 작은 값의 확률을 계산하기 위해 상대적으로 직관적인 방법들을 사용할 수 있습니다. 현실 세계에서는 이것이 이야기의 일부일 뿐입니다: 측정값이 범위를 완전히 벗어날 수도 있고(샘플 뒤바뀜, 오염 또는 소프트웨어 버그), 모든 것이 한꺼번에 잘못될 수도 있습니다(마이크로타이터 플레이트 전체가 잘못되어 그로부터 측정된 모든 데이터에 영향을 주는 경우). 그러한 사건들은 모델링하거나 보정하기 어렵습니다 – 그것들에 대처할 수 있는 최선의 기회는 데이터 품질 평가, 이상치 탐지 및 기록된 제거입니다.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>실험 설계 (Design of Experiments)</span>"
    ]
  },
  {
    "objectID": "13-chap.html#실험-설계의-기본-원칙",
    "href": "13-chap.html#실험-설계의-기본-원칙",
    "title": "15  실험 설계 (Design of Experiments)",
    "section": "15.4 13.4 실험 설계의 기본 원칙",
    "text": "15.4 13.4 실험 설계의 기본 원칙\n\n15.4.1 13.4.1 혼동 (Confounding)\n\n그림 13.2: 질병 상태와 건강 상태 샘플 간의 (가상의) 바이오마커 비교. 왼쪽 패널에 표시된 정보만 주어진다면, 우리는 이 바이오마커가 질병을 탐지하는 데 좋은 성능을 보인다고 결론 내릴 수 있습니다. 만약 추가로 오른쪽 패널에 표시된 것과 같이 데이터가 두 개의 별도 배치(예: 다른 실험실, 다른 기계, 다른 시점)에서 획득되었다는 정보를 듣게 된다면 결론은 달라질 것입니다.\n질문 13.5\n그림 13.2에 표시된 데이터를 고려해 보세요. 바이오마커 수준에서 관찰된 차이가 질병 대 건강 때문인지, 아니면 배치 때문인지 어떻게 결정할 수 있을까요?\n해결책\n이 데이터로부터는 아는 것이 불가능합니다: 두 변수가 혼동되어 있기 때문입니다.\n혼동(Confounding)은 생물학적 변수와 기술적 변수 사이에서만 일어나는 것이 아니며, 더 미묘할 수도 있습니다. 예를 들어, 바이오마커가 질병과 직접적인 관련이 없을 수도 있습니다 – 그것은 단지 질병을 유발하는(동시에 다른 것도 유발하는) 생활 방식의 마커이거나, 질병에 의해(동시에 다른 많은 것들에 의해서도) 유발되는 염증의 마커일 뿐일 수도 있습니다.\n\n\n15.4.2 13.4.2 효과 크기와 반복 실험\n[](imgs/Avicenna.png “그림 13.3: 혼동은 서기 1020년경 페르시아의 의사-과학자 아부 알리 알 후세인 이븐 시나(아비센나)가 나열한 실험 설계의 일곱 가지 규칙 중 하나가”한 번에 질병의 한 가지 가능한 원인만을 연구하는 것”이었던 이유입니다 [@Stigler:sevenpillars].”)\n그림 13.3: 혼동은 서기 1020년경 페르시아의 의사-과학자 아부 알리 알 후세인 이븐 시나(아비센나)가 나열한 실험 설계의 일곱 가지 규칙 중 하나가 “한 번에 질병의 한 가지 가능한 원인만을 연구하는 것”이었던 이유입니다 (Stigler 2016).\n효과 크기(Effect size)는 그림 13.4에서 빨간색 화살표로 표시된 것과 같이 그룹 중심 사이의 차이입니다. 각 그룹의 표본 크기가 클수록 각 그룹의 위치와 효과 크기를 알 수 있는 정밀도가 높아지며, 따라서 차이를 감지할 수 있는 검정력이 높아집니다 (그림 13.5). 반면에, 건강 상태와 질병 상태 사이의 개별 샘플을 구별하기 위한 진단으로서의 바이오마커의 성능은 그룹 내 분포(및 두 상태의 상대적 유병률)에 달려 있으며, 반복 실험에 의해 개선되지 않습니다.\n\n그림 13.4: 빨간색 화살표는 두 그룹의 중심 사이의 차이로 측정된 효과 크기를 보여줍니다. 여기서는 중앙값으로 중심을 위치시켰으며, 때때로 평균이 사용되기도 합니다.\n\n그림 13.5: 왼쪽의 박스플롯은 크기 6인 표본으로 생성되었습니다. 오른쪽의 표본 크기는 60입니다. 측정값은 두 경우 모두 동일한 기저 오차 분포를 가집니다.\n\n\n15.4.3 13.4.3 영리한 조합: Hotelling의 가중치 예시\n가용 자원으로부터 최선의 데이터를 얻으려면 상쇄와 대칭을 활용하는 것이 중요한 측면입니다. 여기에 Hotelling이 어떻게 개선된 계량 체계를 고안했는지에 대한 유명한 예시가 있습니다. 8개의 알 수 없는 무게 (= (_1, …,_8)) 세트가 주어졌다고 가정해 봅시다. 다음 코드에서는 R의 난수 생성기를 사용하여 그러한 실제 무게 세트를 시뮬레이션합니다.\ntheta = round((2 * sample(8, 8) + rnorm(8)), 1)\ntheta\n\n\n[1] 10.7 13.4 16.4  3.9  8.5 16.0  1.2  4.4\n방법 1 : 8번의 계량을 사용하는 나이브한 방법. 약사의 저울(그림 13.6)을 사용하여 각 무게 (_i)를 개별적으로 재고, 오차가 표준 편차 0.1인 정규 분포를 따른다고 가정해 봅시다. 우리는 다음과 같이 오차 벡터 errors1과 그 제곱합을 계산합니다:\nX = theta + rnorm(length(theta), 0, 0.1)\nX\n\n\n[1] 10.513279 13.268145 16.507673  3.881881  8.395974 16.073952  1.131341\n[8]  4.289040\n\n\nerrors1 = X - theta\nerrors1\n\n\n[1] -0.18672051 -0.13185519  0.10767279 -0.01811869 -0.10402607  0.07395242\n[7] -0.06865871 -0.11095993\n\n\nsum(errors1^2)\n\n\n[1] 0.09748857\n방법 2 : 역시 8번의 계량을 사용하는 Hotelling의 방법. 이 방법은 아다마르(Hadamard) 행렬에 기초하며, 여기서 계산합니다.\nlibrary(\"survey\")\nh8 = hadamard(6)\ncoef8 = 2*h8 - 1\ncoef8\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    1    1    1    1    1    1    1    1\n[2,]    1   -1    1   -1    1   -1    1   -1\n[3,]    1    1   -1   -1    1    1   -1   -1\n[4,]    1   -1   -1    1    1   -1   -1    1\n[5,]    1    1    1    1   -1   -1   -1   -1\n[6,]    1   -1    1   -1   -1    1   -1    1\n[7,]    1    1   -1   -1   -1   -1    1    1\n[8,]    1   -1   -1    1   -1    1    1   -1\n우리는 coef8을 새로운 계량 체계의 계수로 사용합니다: 행렬의 첫 번째 열은 모든 무게를 저울의 한쪽에 올려놓고 재라고 말해줍니다. 결과값을 Y[1]이라고 부릅시다. 두 번째 열은 무게 1, 3, 5, 7을 저울의 한쪽에 놓고 무게 2, 4, 6, 8을 다른 쪽에 놓으라고 말해줍니다. 그런 다음 차이를 측정하고 그 결과를 Y[2]라고 부릅니다. coef8의 모든 8개 열에 대해 이와 같이 수행합니다. 우리는 필요한 계산을 아래와 같이 행렬 곱셈 형태로 표현할 수 있습니다.\nY = theta  %*% coef8 + rnorm(length(theta), 0, 0.1)\n첫 번째 방법에서와 마찬가지로, 8개의 무게 측정값 각각은 표준 편차 0.1인 정규 오차를 가집니다.\n질문 13.6\n\ncoef8이 –전체적인 인자를 제외하고– 직교 행렬((C^t C = ) 어떤 ()에 대해)임을 확인하세요.\n만약 우리가 theta에 coef8과 coef8의 전치 행렬을 곱하고 8로 나누면, 다시 theta를 얻게 됨을 확인하세요.\n\n해결책\ncoef8 %*% t(coef8)\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    8    0    0    0    0    0    0    0\n[2,]    0    8    0    0    0    0    0    0\n[3,]    0    0    8    0    0    0    0    0\n[4,]    0    0    0    8    0    0    0    0\n[5,]    0    0    0    0    8    0    0    0\n[6,]    0    0    0    0    0    8    0    0\n[7,]    0    0    0    0    0    0    8    0\n[8,]    0    0    0    0    0    0    0    8\n\n\ntheta %*% coef8 %*% t(coef8) / ncol(coef8)\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,] 10.7 13.4 16.4  3.9  8.5   16  1.2  4.4\n우리는 coef8의 직교성을 사용하여 이 결과들을 결합해 theta를 추정합니다.\nthetahat = Y %*% t(coef8) / ncol(coef8)\n우리는 실제 ()를 알고 있으므로 오차와 그 제곱합을 계산할 수 있습니다.\nerrors2 = as.vector(thetahat) - theta\nerrors2\n\n\n[1] -0.005213746  0.025216488  0.003201562  0.033880188 -0.029459127\n[6] -0.043173774  0.083202870 -0.025818188\n\n\nsum(errors2^2)\n\n\n[1] 0.01214228\n우리는 여기서의 제곱합이 첫 번째 절차의 그것보다 상당히 작다는 것을 알 수 있습니다. 단지 운이 좋았던 것일까요?\n질문 13.7\n\n위 실험을 B = 10000번 반복하되, 매번 다른 theta를 사용하고 두 체계의 오차 제곱합의 표집 분포를 살펴보세요.\n두 분산 사이의 관계가 무엇이라고 생각하시나요?\n\n해결책\nB  = 10000\ntc = t(coef8) / ncol(coef8)\nsse = replicate(B, {\n  theta = round((2 * sample(8, 8)) + rnorm(8), 1)\n  X = theta + rnorm(length(theta), 0, 0.1)\n  err1 = sum((X - theta)^2)\n  Y = coef8 %*% theta + rnorm(length(theta), 0, 0.1)\n  thetahat = tc %*% Y\n  err2 = sum((thetahat - theta)^2)\n  c(err1, err2)\n})\nrowMeans(sse)\n\n\n[1] 0.079591221 0.009954419\n\n\nggplot(tibble(lr = log2(sse[1, ] / sse[2, ])), aes(x = lr)) +\n  geom_histogram(bins = 50) +\n  geom_vline(xintercept = log2(8), col = \"orange\") +\n  xlab(\"log2 ratio of SSE, Method 1 vs 2\")\n\n그림 13.7: 두 방법의 오차 제곱합 비율의 로그(밑 2). 수직 주황색 선은 8에 해당합니다.\n두 번째 체계는 첫 번째보다 8배 더 효율적(efficient)인데, 측정에 의해 생성된 오차의 제곱합이 8배 더 낮기 때문입니다 (그림 13.7).\n이 예제는 여러 수량을 확정해야 할 때, 한 실험에서 측정값들을 결합하고 유사한 그룹들 사이를 비교함으로써 정확도를 높이고 비용을 줄일 수 있는 기회가 있음을 보여줍니다.\n최적의 설계는 한 번에 하나의 요인만을 변화시킬 수 있다는 이븐 시나의 규칙은 20세기에 R.A. Fisher에 의해 대체되었습니다. 그는 대비(contrasts)가 세심하게 설계되기만 한다면, 요인들을 조합하여 수정하더라도 여전히 결론에 도달할 수 있으며 –때때로 계량 예시에서처럼 훨씬 더 나은 결론을 얻을 수 있다는 것을 깨달았습니다.\n\n그림 13.8: 왼쪽은 각각 크기 6인 두 표본이 비교되고 있습니다. 오른쪽은 동일한 데이터가 표시되어 있지만, 데이터 수집 시간에 따라 색상이 입혀져 있습니다. 우리는 데이터가 이러한 시간에 따라 블록으로 나뉘는 경향이 있음을 주목합니다. 이 때문에 그룹 간의 비교가 희석됩니다. 이러한 효과는 시간 내에서 비교함으로써, 즉 세 개의 그룹으로 블록화함으로써 완화될 수 있습니다. 질문 13.8—13.10에서 시연된 쌍체 분석(Paired analysis)은 블록화의 특수한 사례입니다.\n\n\n15.4.4 13.4.4 블록화와 쌍체(Pairing)\n다윈은 옥수수의 성장이 토양의 구성과 화분 내의 습도에 영향을 받는다고 의심했습니다. 이러한 이유로, 그가 타가 수분된 씨앗에서 자란 식물과 자가 수분된 씨앗에서 자란 식물을 비교하고 싶었을 때, 그는 15개의 화분 각각에 각 유형의 묘목을 하나씩 심었습니다. 다윈의 Zea Mays 실험에서 각 화분은 하나의 블록이며, 각 블록 내에서 관심 요인(수분 방법), 즉 처리(treatment)만이 다릅니다 (그림 13.9).\n\n그림 13.9: 쌍체 실험은 블록화의 가장 단순한 사례입니다.\n사실 R.A. Fisher는 다윈의 실험을 비판했는데, 다윈이 타가 수분된 식물들을 화분의 한쪽 면에만 체계적으로 배치했기 때문입니다. 이는 예를 들어 화분의 한쪽 면이 햇빛을 더 많이 받는 경우 “측면” 효과와 교배 효과의 혼동을 일으킬 수 있었습니다. 예를 들어 동전을 던져서 화분의 측면을 무작위화하는 것이 더 좋았을 것입니다.\n블록화할 수 있는 것은 블록화하고, 할 수 없는 것은 무작위화하라.\n(George Box, 1978)\n\n15.4.4.1 쌍체 설계와 비쌍체 설계의 비교\n여러 가능한 설계를 비교할 때, 우리는 1장에서 보았던 것과 유사한 검정력 시뮬레이션(power simulations)을 수행합니다. 각 그룹의 표본 크기가 15이고 효과 크기가 0.2라고 가정해 봅시다. 또한 측정값의 표준 편차에 대한 가정도 필요한데, 여기서는 두 그룹 모두 동일하게 sd=0.25라고 가정하고 데이터를 시뮬레이션합니다:\nn = 15\neffect = 0.2\npots   = rnorm(n, 0, 1)\nnoiseh = rnorm(n, 0, 0.25)\nnoisea = rnorm(n, 0, 0.25)\nhybrid = pots + effect + noiseh\nautoz  = pots + noisea\n질문 13.8\n단순 (t)-검정과 쌍체 (t)-검정을 모두 수행하세요. 이 경우 어느 것이 더 강력한가요?\n해결책\nt.test(hybrid, autoz, paired = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  hybrid and autoz\nt = 0.77183, df = 26.012, p-value = 0.4472\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.3145706  0.6928591\nsample estimates:\nmean of x mean of y \n0.5073519 0.3182076 \n\n\nt.test(hybrid, autoz, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  hybrid and autoz\nt = 1.8783, df = 14, p-value = 0.08133\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.02683705  0.40512561\nsample estimates:\nmean difference \n      0.1891443 \n아마 여기서 시뮬레이션된 데이터로만 운이 좋았던 걸까요?\n질문 13.9\n일반적으로 어느 방법이 더 강력한지 확인하세요. 위 계산을 (1000)번 반복하고, 위양성률 ()를 사용하여 이 1000번의 시험에 대한 평균 기각 확률을 계산하세요.\n해결책\nB     = 1000\nalpha = 0.05\nwhat  = c(FALSE, TRUE)\npvs = replicate(B, {\n  pots   = rnorm(n, 0, 1)\n  noiseh = rnorm(n, 0, 0.25)\n  noisea = rnorm(n, 0, 0.25)\n  hybrid = pots + effect + noiseh\n  autoz  = pots + noisea\n  vapply(what,\n    function(paired)\n      t.test(hybrid, autoz, paired = paired)$p.value,\n    double(1)) |&gt; setNames(paste(what))\n})\nrowMeans(pvs &lt;= alpha)\n\n\nFALSE  TRUE \n0.000 0.532 \n우리는 두 방법을 사용하여 얻은 p-값들을 비교할 수 있습니다 (그림 13.10).\ntidyr::pivot_longer(as.data.frame(t(pvs)), cols = everything(), names_to = \"paired\") |&gt;\n  ggplot(aes(x = value, fill = paired)) +\n  geom_histogram(binwidth = 0.01, boundary = 0, alpha = 1/3)\n\n그림 13.10: 일반적인 비쌍체 (t)-검정과 쌍체 (t)-검정의 p-값 분포를 비교한 검정력 계산 결과.\n질문 13.10\n\n효과 크기, 표본 크기, 화분 효과의 크기(표준 편차로 측정됨), 노이즈 표준 편차 및 표본 크기의 서로 다른 값에 대해 두 유형의 검정력을 비교하는 함수를 작성하세요.\n여러분의 함수를 사용하여 (n=15)일 때 어느 표준 편차(화분 또는 노이즈)가 쌍체에 의한 개선에 가장 큰 영향을 미치는지 찾아보세요.\n두 표준 편차가 모두 0.5일 때 80%의 검정력을 달성하려면 (n)이 얼마나 커야 할까요?\n\n해결책\npowercomparison = function(effect = 0.2, n = 15, alpha = 0.05,\n                sdnoise, sdpots, B = 1000) {\n  what = c(FALSE, TRUE)\n  pvs = replicate(B, {\n    pots   = rnorm(n, 0, sdpots)\n    noiseh = rnorm(n, 0, sdnoise)\n    noisea = rnorm(n, 0, sdnoise)\n    hybrid = pots + effect + noiseh\n    autoz  = pots + noisea\n    vapply(what,\n      function(paired)\n        t.test(hybrid, autoz, paired = paired)$p.value,\n      double(1)) |&gt; setNames(paste(what))\n  })\n  rowMeans(pvs &lt;= alpha)\n}\n화분 효과가 노이즈 표준 편차에 비해 작을 때는 쌍체화가 거의 차이를 만들지 못함을 보여주는 몇 가지 시뮬레이션입니다. 만약 화분 효과가 크다면, 쌍체화는 큰 차이를 만듭니다.\npowercomparison(sdpots = 0.5,  sdnoise = 0.25)\n\n\nFALSE  TRUE \n0.034 0.533 \n\n\npowercomparison(sdpots = 0.25, sdnoise = 0.25)\n\n\nFALSE  TRUE \n0.242 0.524 \n\n\npowercomparison(sdpots = 0.1,  sdnoise = 0.25)\n\n\nFALSE  TRUE \n0.510 0.534 \n각 유형의 식물 100개와 두 표준 편차 모두 0.5일 때, 쌍체 검정의 검정력은 약 80%입니다.\npowercomparison(sdpots = 0.5, sdnoise = 0.5, n = 100)\n\n\nFALSE  TRUE \n0.513 0.796 \n질문 13.11\n쌍체 설계(Paired designs)는 관측치의 자연스러운 쌍체 관계를 고려합니다 – 예를 들어 쌍둥이 연구, 또는 치료 전후의 환자 연구 등이 있습니다. 쌍체 관계가 없을 때는 무엇을 할 수 있을까요?\n해결책\n매칭 설계(Matched designs)는 나이, 성별, 배경 건강 상태 등을 일치시킴으로써 피험자들 사이의 유사성을 최대한으로 확보한 쌍을 만들려고 시도합니다. 하나는 치료를 받고, 다른 하나는 대조군 역할을 합니다.\n균형 설계(balanced design)는 모든 서로 다른 요인 조합이 동일한 수의 관측 반복을 가지는 실험 설계입니다. 각 요인의 효과를 식별할 수 있습니다. 만약 성가신 요인(nuisance factors)이 있다면, 이들이 관심 요인들과 균형을 이루도록 하는 것이 좋습니다. 때때로 이것은 물류상이나 경제적인 이유로 불편하거나 비실용적일 수 있지만 – 그러한 경우 분석가는 살얼음판 위를 걷는 것과 같으므로 주의해서 진행해야 합니다.\n\n\n15.4.4.2 무작위화 (Randomization)\n종종 우리는 어떤 성가신 요인이 중요할지 알지 못하거나, 미리 계획할 수 없습니다. 그러한 경우 무작위화는 실용적인 전략입니다: 적어도 표본 크기가 충분히 큰 한계에서는 모든 성가신 요인의 효과가 상쇄될 것으로 기대할 수 있습니다.\n무작위화는 무의식적인 편향을 줄이는 데에도 도움을 줄 수 있습니다. 예를 들어, 한 그룹의 샘플을 얻기가 극도로 어렵다면, 우리는 다른 그룹의 샘플을 다룰 때보다 그 샘플들을 다룰 때 더욱 세심하게 주의를 기울이고 싶은 유혹을 느낄 수 있습니다. 불행히도 이는 측정 결과에 편향을 일으켜 비교를 무효화할 수 있습니다. 단순한 무작위화를 개선하려고 할 때 발생하는 몇 가지 함정에 대한 광범위한 논의는 Senn (2004)을 참조하십시오.\n\n\n\n15.4.5 13.4.5 반복 실험은 몇 번이나 필요한가?\n\n\n\n저전력의 “나도(me-too)” 연구를 주의하세요.\n\n\n저전력의 “나도(me-too)” 연구를 주의하세요.\n1.4.1절에서 우리는 대안 가설을 알고 있을 때 80%의 진양성률을 달성하기 위해 얼마나 많은 뉴클레오타이드가 필요한지 계산하는 시뮬레이션 실험을 보여주었습니다. 이제 13.2절의 실험 대 연구에 관한 논의를 떠올려 봅시다. 세포주 실험의 경우, 우리는 단 한 번의 반복 실험만으로도 이미 올바른 결과를 얻을 수도 있습니다. 대개 확신을 위해 두세 번을 할 것입니다. 반면에 환자들에 대한 두 가지 대안 약물의 효과를 비교하는 연구의 경우, 우리의 직관은 결과에 대해 확신을 가질 수 있을 때까지 수십 명(혹은 그 이상)의 환자가 필요할 것임을 알려줍니다. 필요한 반복 실험의 횟수는 문맥에 따라 크게 달라집니다. 그것은 통제되지 않은 가변성의 양과 효과 크기에 달려 있습니다. 실용적인 접근 방식은 이전에 성공했던(또는 실패했던) 유사한 실험이나 연구를 확인하고 시뮬레이션, 서브샘플링 또는 붓스트랩을 사용하여 계획된 연구의 검정력을 추정하는 것입니다.\n\n15.4.5.1 검정력은 표본 크기, 효과 크기 및 가변성에 달려 있습니다.\n\n그림 13.11: 검정력 계산에서 보이지 않는 큰 문제(elephant in the room)는 효과 크기입니다. 특히 옴익스(’omics) 연구에서 수천 개의 유전자(또는 다른 특징)에서 차이를 스크리닝할 때, 어떤 효과 크기를 기대해야 할지 정확히 알기는 어렵습니다. 하지만 그럼에도 불구하고, 검정력 계산은 자릿수 계산이나 이 섹션에서 쌍체 대 비쌍체 검정에 대해 보여준 것과 같은 정성적 비교에 유용합니다. 출처: Wikimedia CH.\npwr 패키지는 표준 검정력 계산(power calculations)을 수행하는 함수들을 제공합니다. 이러한 계산에는 항상 표본 크기, 효과 크기, 유의 수준(위양성률) 및 검정력 자체(가설을 기각해야 할 때 기각할 확률, 즉 진양성률)의 네 가지 수치가 관여합니다. pwr.2p.test, pwr.chisq.test, pwr.f2.test 함수는 각각 두 비율의 검정, 카이제곱 검정 및 일반 선형 검정에 대한 계산을 제공합니다.\n여기에 (n=15)일 때의 2표본 (t)-검정에 대한 검정력 계산 예시가 있습니다. 이 함수는 여러 인수를 필요로 합니다:\nlibrary(\"pwr\")\nstr(pwr.t.test)\n\n\nfunction (n = NULL, d = NULL, sig.level = 0.05, power = NULL, type = c(\"two.sample\", \n    \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))  \n만약 검정력과 효과 크기 값을 넣어 함수를 호출하면 필요한 표본 크기를 반환하고, 표본 크기와 효과 크기를 지정하면 검정력을 반환합니다.\npwr.t.test(n = 15, d = 0.4, sig.level = 0.05, type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 15\n              d = 0.4\n      sig.level = 0.05\n          power = 0.1848496\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\npwr.t.test(n = 15, d = 0.4, sig.level = 0.05, type = \"paired\")\n\n\n     Paired t test power calculation \n\n              n = 15\n              d = 0.4\n      sig.level = 0.05\n          power = 0.3031649\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n주어진 효과 크기를 감지하기 위해 어떤 표본 크기가 필요할지 알고 싶다면:\npwr.t.test(d = 0.4, sig.level = 0.05, type = \"two.sample\", power=0.8)\n\n\n     Two-sample t test power calculation \n\n              n = 99.08032\n              d = 0.4\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\npwr.t.test(d = 0.4, sig.level = 0.05, type = \"paired\", power=0.8)\n\n\n     Paired t test power calculation \n\n              n = 51.00945\n              d = 0.4\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n우리는 쌍체 검정을 사용하지 않을 때 동일한 검정력을 얻기 위해 약 두 배 더 많은 관측치가 필요함을 알 수 있습니다.\n\n\n15.4.5.2 유효 표본 크기\n독립적인 관측치들의 표본은 같은 수의 종속적인 관측치들보다 더 많은 정보를 줍니다. 여러분이 집마다 문을 두드려 질문을 함으로써 여론 조사를 하고 싶다고 가정해 봅시다. 첫 번째 시나리오에서는 전국적으로 무작위인 (n)개 장소에서 (n)명의 사람을 뽑습니다. 두 번째 시나리오에서는 여행 시간을 절약하기 위해 무작위인 (n/3)개 장소를 고르고 각 장소에서 옆집에 사는 세 사람씩 인터뷰합니다. 두 경우 모두 설문에 응한 사람 수는 (n)명이지만, 같은 동네에 사는 사람들은 같은 의견을 가질 가능성이 더 높다고 가정한다면, 두 번째 시나리오의 데이터는 (양(+)의) 상관관계를 갖습니다. 이를 탐구하기 위해 시뮬레이션을 해봅시다.\ndoPoll = function(n = 100, numPeoplePolled = 12) {\n  opinion = sort(rnorm(n))\n  i1 = sample(n, numPeoplePolled)\n  i2 = sample(seq(3, n, by = 3), numPeoplePolled / 3)\n  i2 = c(i2, i2 - 1, i2 - 2)\n  c(independent = mean(opinion[i1]), correlated = mean(opinion[i2]))\n}\nresponses = replicate(5000, doPoll())\n\ntidyr::pivot_longer(as.data.frame(t(responses)), \n        cols = everything(), names_to = \"design\") |&gt;\nggplot(aes(x = value, col = design)) + geom_density() +\n  geom_vline(xintercept = 0) + xlab(\"여론 조사 결과\")\n\n그림 13.12: 두 가지 샘플링 방법을 사용한 여론 조사 결과의 밀도 추정치. 상관관계가 있는 방법이 더 큰 퍼짐을 가집니다. 참값은 수직선으로 표시됩니다.\n그 나라에는 100명의 사람이 있고, 첫 번째 접근 방식(i1)에서는 그중 12명을 무작위로 샘플링합니다. 두 번째 접근 방식에서는 4명의 사람과 각자의 이웃 두 명씩을 샘플링합니다(i2). 우리의 “의견”은 이 경우 실수이며, 모집단에서 평균 0, 표준 편차 1인 정규 분포를 따릅니다. 우리는 doPoll 함수의 첫 번째 줄에서 집들을 가장 부정적인 의견부터 가장 긍정적인 의견 순으로 정렬함으로써 나라의 공간-사회적 구조를 모델링합니다. 출력 결과는 그림 13.12에 나와 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>실험 설계 (Design of Experiments)</span>"
    ]
  },
  {
    "objectID": "13-chap.html#평균-분산-관계와-분산-안정화-변환",
    "href": "13-chap.html#평균-분산-관계와-분산-안정화-변환",
    "title": "15  실험 설계 (Design of Experiments)",
    "section": "15.5 13.5 평균-분산 관계와 분산 안정화 변환",
    "text": "15.5 13.5 평균-분산 관계와 분산 안정화 변환\n4장과 8장에서 우리는 정량적 측정값의 공간을 압축하거나 늘려서 측정값의 분산이 전반적으로 더 비슷해지도록 만드는 데이터 변환의 예시들을 보았습니다. 따라서 반복 측정값 사이의 분산은 더 이상 평균값에 크게 의존하지 않게 됩니다.\n변환 전 우리 데이터의 평균-분산 관계는 원칙적으로 어떤 함수든 될 수 있지만, 많은 경우 다음과 같은 원형적인 관계들이 적어도 대략적으로 발견됩니다:\n\n상수(constant): 분산이 평균과 독립적임, (v(m)=c).\n포아송(Poisson): 분산이 평균에 비례함, (v(m)=am).\n이차(quadratic): 표준 편차가 평균에 비례함, 따라서 분산은 이차적으로 증가함, (v(m)=bm^2).\n\n여기서 (v(m))은 평균 (m)의 함수로서 분산 (v)의 추세를 설명하는 함수입니다. 실수 (a, b, c)는 평균 외에 분산에 영향을 미치는 요인들을 매개변수화합니다.\n질문 13.12\n이러한 유형의 평균-분산 관계를 보여주는 생물학적 어세이나 측정 기술의 예시를 드세요.\n실제 데이터는 이들의 조합에 의해 영향을 받을 수도 있습니다. 예를 들어, DNA 마이크로어레이의 경우 형광 강도는 신호와 거의 독립적인 배경 노이즈와, 표준 편차가 신호에 비례하는 곱셈 노이즈(multiplicative noise)의 조합에 영향을 받습니다 (Rocke and Durbin 2001). 따라서 평균-분산 관계는 (v(m)=bm^2+c)입니다. 밝은 점(큰 (m))의 경우 곱셈 노이즈((bm^2))가 지배적인 반면, 희미한 점의 경우 배경 노이즈 (c)가 지배적입니다.\n질문 13.13\n분산 안정화 변환을 적용하는 요점이 무엇인가요?\n해결책\n변환된 스케일에서 데이터를 분석하면 다음과 같은 경향이 있습니다:\n\n시각화가 개선됩니다. 플롯의 물리적 공간이 데이터 범위 전체에서 더 “공평하게” 사용되기 때문입니다. 히트맵의 경우 색상 공간에도 유사한 논리가 적용됩니다.\nPCA와 같은 서열화 방법이나 상관관계 기반의 군집화 결과가 개선됩니다. 결과가 소수의 매우 높게 발현된 유전자의 신호에 의해 좌우되지 않고, 동적 범위 전체의 많은 유전자로부터 더 균일하게 영향을 받기 때문입니다.\n동일하게 분포된(따라서 등분산적인) 노이즈를 가정하는 통계 모델로부터의 추정 및 추론이 개선됩니다.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>실험 설계 (Design of Experiments)</span>"
    ]
  },
  {
    "objectID": "13-chap.html#데이터-품질-평가-및-품질-관리",
    "href": "13-chap.html#데이터-품질-평가-및-품질-관리",
    "title": "15  실험 설계 (Design of Experiments)",
    "section": "15.6 13.6 데이터 품질 평가 및 품질 관리",
    "text": "15.6 13.6 데이터 품질 평가 및 품질 관리\n우리는 데이터 품질 평가(Quality Assessment, QA) –품질을 측정하고 모니터링하기 위해 취해진 단계들– 와 품질 관리(Quality Control, QC) –나쁜 데이터를 제거하는 것– 을 구분합니다. 이러한 활동들은 가공되지 않은 데이터를 조립하는 것부터 변환, 요약, 모델 적합, 가설 검정 또는 “히트” 스크리닝, 해석에 이르기까지 분석의 모든 단계에 스며들어 있습니다. QA 관련 질문들은 다음과 같습니다:\n\n변수들의 주변 분포(marginal distributions)가 어떻게 보이나요 (히스토그램, ECDF 플롯)?\n그들의 결합 분포(joint distributions)가 어떻게 보이나요 (산점도, 페어즈 플롯)?\n반복 실험들이 얼마나 잘 일치하나요 (서로 다른 생물학적 조건과 비교했을 때)? 여러 조건들 사이의 차이의 크기가 타당한가요?\n배치 효과의 증거가 있나요? 이들은 단계적(categorical)이거나 점진적(continuous)인 성격일 수 있습니다. 예: 실험 시약, 프로토콜 또는 환경 요인의 변화 때문. 그러한 효과와 관련된 요인들은 명시적으로 알려져 있을 수도 있고, 알려지지 않은 잠재적인 것일 수도 있으며, 종종 그 중간 어디쯤일 수도 있습니다 (예: 측정 장치가 시간이 지남에 따라 천천히 성능이 저하되고, 우리는 시간을 기록했지만 정확히 어느 시점에 얼마나 나빠졌는지는 모르는 경우).\n\n마지막 두 가지 질문 세트에 대해서는 히트맵, 주성분 분석 플롯 및 다른 서열화 플롯(7장과 9장에서 보았던 것들)이 유용합니다.\n[](imgs/1896_Ford_Quadricycle.jpeg “그림 13.13: 헨리 포드의 (아마도 가짜일 수 있는) 인용구:”내가 사람들에게 무엇을 원하는지 물었다면, 그들은 더 빠른 말이라고 대답했을 것이다.”는 사양 준수(adherence to specifications)가 아닌, 목적 적합성(fitness for purpose)으로서의 품질에 대한 관점을 표현합니다. (출처: Ford)“)\n그림 13.13: 헨리 포드의 (아마도 가짜일 수 있는) 인용구: “내가 사람들에게 무엇을 원하는지 물었다면, 그들은 더 빠른 말이라고 대답했을 것이다.”는 사양 준수(adherence to specifications)가 아닌, 목적 적합성(fitness for purpose)으로서의 품질에 대한 관점을 표현합니다. (출처: Ford)\n품질(Quality)을 정의하는 것은 쉽지 않으며, 이 단어는 많은 의미로 사용됩니다. 우리에게 가장 적절한 것은 목적 적합성(fitness for purpose) 4이며, 이는 규범적 사양에 기초한 품질의 다른 정의들과 대조됩니다. 예를 들어, RNA-Seq 데이터를 이용한 차등 발현 분석에서 우리의 목적은 두 생물학적 조건 사이의 차등 발현 유전자를 검출하는 것일 수 있습니다. 우리는 리드 수, 리드 길이, 베이스 콜링 품질, 정렬된 리드의 비율과 같은 사양들을 확인할 수 있지만, 궁극적으로 이러한 측정값들만으로는 우리의 목적과 거의 관련이 없습니다. 더 적절한 것은 예상대로 작동하지 않는 샘플들(예: 샘플 뒤바뀜이나 분해 때문)이나, 제대로 측정되지 않은 유전자들을 식별하는 것입니다. 8.10.3절에서 이에 대한 예시를 보았습니다. 유용한 플롯으로는 그림 8.6과 같은 서열화 플롯과 그림 8.7과 같은 히트맵이 있습니다. 품질 지표(quality metric)는 품질을 측정하기 위해 사용하는 모든 수치이며, 명시적인 품질 지표를 갖는 것은 QA/QC를 자동화하는 데 도움이 됩니다.\n4 http://en.wikipedia.org/wiki/Quality_%28business%29",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>실험 설계 (Design of Experiments)</span>"
    ]
  },
  {
    "objectID": "13-chap.html#종단적-데이터-longitudinal-data",
    "href": "13-chap.html#종단적-데이터-longitudinal-data",
    "title": "15  실험 설계 (Design of Experiments)",
    "section": "15.7 13.7 종단적 데이터 (Longitudinal data)",
    "text": "15.7 13.7 종단적 데이터 (Longitudinal data)\n종단적 데이터(Longitudinal data) 5는 시간을 공변량으로 가집니다. 첫 번째 질문은 우리가 소수의 시점을 보고 있는지 –예를 들어 약물 노출 후 48시간, 72시간, 96시간에 측정된 세포주의 반응– 아니면 길고 조밀하게 샘플링된 시계열을 보고 있는지 –예를 들어 전기생리학의 패치 클램프 데이터나 생세포 현미경 검사 영상– 하는 것입니다.\n5 관련이 있지만 다른 개념으로 시간(time)이 결과 변수인 생존 데이터(survival data)가 있습니다.\n첫 번째 경우, 시간은 대개 또 다른 이산적인 실험 요인으로 생각하는 것이 가장 좋습니다. 아마도 실험자가 어느 시점이 가장 유용한 결과를 줄지 확신하지 못했기 때문에 여러 시점이 선택되었을 것입니다. 그러면 가장 좋은 시점을 식별하고 그 시점에 집중할 수 있습니다. 데이터에 따라 다른 시점들은 어느 정도 반복 실험으로서 검증 용도로 쓰일 수 있습니다. 실험을 설계할 때, 우리는 더 많은 일이 일어날 것으로 예상되는 기간(예: 섭동 직후)을 더 조밀하게 커버하려고 노력할 것입니다.\n스크리닝 문맥에서, 우리는 특정 시점이나 형태에 상관없이 효과가 전혀 있는지 여부를 (F)-검정 같은 것을 사용하여 물을 수 있습니다. 그런 다음 서로 다른 시점에서의 측정값들 사이의 의존성을 고려하고 그에 따라 귀무 분포를 결정하기만 하면 됩니다.\n시계열 데이터가 있는 두 번째 경우, 우리는 데이터에 동적 모델(dynamical models)을 적합시키고 싶을 수 있습니다. 시간 (t)에서의 우리 시스템의 _상태_를 (X(t))라고 쓸 수 있으며, 다음과 같은 조건에 따라 많은 선택지를 가집니다:\n\n(X)가 연속형인지 이산형인지,\n(X)의 동역학(dynamics) 6이 결정론적인지 확률적인지,\n동역학이 매끄러운지(smooth) 그리고/또는 도약(jumpy)이 있는지,\n우리가 (X)를 직접 관찰하는지, 아니면 노이즈가 섞여 있거나 축소된 버전인 (Y = g(X)+) 7만을 관찰하는지.\n\n6 시간 (t)에서의 (X(t))가 주어졌을 때 (X(t+t))의 값, 즉 시간적 진화\n7 여기서 (g)는 벡터 값을 가진 (X)의 변수 중 일부를 버리는 등 정보를 잃는 함수를 나타내고, ()은 노이즈 항입니다.\n우리는 다음과 같은 많은 모델링 도구들을 가지고 있습니다:\n\n마르코프 모델(Markov Models): 이산 상태 공간; 동역학은 확률적이며 상태 사이를 도약함으로써 발생합니다.\n상미분 또는 편미분 방정식: 연속 상태 공간; 동역학은 결정론적이고 매끄러우며, 물리학이나 화학에 뿌리를 둔 제1원리로부터 도출되었을 수도 있는 미분 방정식으로 설명됩니다.\n마스터 방정식, 포커-플랑크 방정식(Fokker-Planck equation): 동역학은 확률적이며 공간과 시간에서의 (X)의 확률 분포에 대한 (편)미분 방정식으로 설명됩니다.\n부분 결정론적 확률 과정(Piece-wise deterministic stochastic processes): 위의 것들의 조합으로, 과정의 표본은 매끄러운 결정론적 움직임뿐만 아니라 가끔씩 발생하는 도약을 포함합니다.\n\n만약 우리가 (X)를 직접 관찰하지 못하고 노이즈가 섞여 있거나 요약된 버전인 (Y)만을 본다면, 마르코프 모델의 경우 은닉 마르코프 모델(Hidden Markov Models) (Durbin et al. 1998)의 형식주의를 통해 그러한 모델을 적합시키는 것이 상대적으로 간단해집니다. 다른 유형의 과정들에 대해서도 유사한 접근 방식이 가능하지만, 이들은 기술적으로 더 까다롭고 전문 문헌을 참조해야 합니다.\n모델 중심적인 관점이 아닌 데이터 중심적인 관점을 취한다면, 시계열 데이터를 분석하는 방법들은 다음과 같습니다:\n\n비모수적 평활화(smoothing) 후 원형적인 형태들로의 군집화 또는 분류\n변화점 탐지(Change point detection)\n자기회귀 모델(Autoregressive models)\n푸리에 및 웨이블릿 분해\n\n자세한 내용을 다루는 것은 이 책의 범위를 벗어나며, 수많은 선택지가 있습니다 8. 많은 방법들이 경제학이나 신호 처리 분야에서 유래했으므로, 해당 분야의 문헌을 훑어보는 것이 가치가 있습니다.\n8 한 가지 시작점은 CRAN 태스크 뷰 https://cran.r-project.org/web/views/TimeSeries.html입니다.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>실험 설계 (Design of Experiments)</span>"
    ]
  },
  {
    "objectID": "13-chap.html#데이터-통합-알고-있는혹은-알-수-있는-모든-것을-활용하라",
    "href": "13-chap.html#데이터-통합-알고-있는혹은-알-수-있는-모든-것을-활용하라",
    "title": "15  실험 설계 (Design of Experiments)",
    "section": "15.8 13.8 데이터 통합: 알고 있는(혹은 알 수 있는) 모든 것을 활용하라",
    "text": "15.8 13.8 데이터 통합: 알고 있는(혹은 알 수 있는) 모든 것을 활용하라\n\n\n\n자신이 멍청한 척하지 마세요.\n\n\n자신이 멍청한 척하지 마세요.\n손에 쥔 데이터를 이전 지식에 대한 언급 없이 분석하려는, 겉보기에 “편향되지 않은” 접근 방식에는 매력이 있습니다. 통계적 방법들이 종종 일반적이고 자기 완비적으로 개발되어 왔다는 사실이 이러한 경향을 강화합니다. 예를 들어, 응용 분야에서 행과 열이 무엇을 의미하는지나 다른 관련 데이터가 있을 수 있는지에 대한 구체적인 언급 없이 일반적인 행렬에 대해 작동하도록 개발된 경우입니다.\n일반적인 접근 방식은 시작하기에 좋은 방법이며, 분석이 간단하고 검정력이 매우 높은 경우에는 그러한 접근 방식이 효과적일 수 있습니다. 하지만 종종 그것은 낭비입니다. RNA-Seq 실험의 차등 발현 사례를 떠올려 보십시오. 6장과 8장에서 보았듯이, 우리는 신호 강도 9나 다른 어떤 것과도 상관없이 기록된 각 유전자에 대해 가설 검정을 수행할 수 있고, 그런 다음 모든 검정을 똑같이(즉, 교환 가능한 것으로) 취급하는 다중 검정 방법을 실행할 수 있습니다. 하지만 이는 비효율적입니다: 검정력이 더 낮거나 참일 사전 확률 (_0)가 더 높은 가설들을 필터링하거나 가중치를 낮춤으로써 우리의 검출력을 개선할 수 있습니다.\n9 즉, 평균 리드 카운트.\n유사하게, 개별 p-값의 해석에서도 우리가 알고 있는 다른 모든 것을 무시하고 어떤 일이 있어도 5% 컷오프를 맹목적으로 고수할 필요가 없습니다. 오히려 검정의 검정력과 (_0)에 대한 사전 지식이 우리의 해석을 가이드하도록 할 수 있습니다 (Altman and Krzywinski 2017).\n잘못된 객관성의 다른 잠재적 예시는 다음과 같습니다:\n\n고차원 회귀나 분류에서의 벌점화(penalization) 또는 특징 선택. 모든 특징을 동일하게 취급하는 체계(예: 모든 특징을 평균 0, 분산 1로 표준화함)를 사용하기는 쉽습니다. 하지만 때때로 우리는 특징의 일부 클래스가 다른 것들보다 더 정보가 많을 가능성이 높다는 것을 알고 있습니다 (Wiel et al. 2016). 또한 “다른” 데이터를 표현하기 위해 그래프나 네트워크를 사용할 수 있고, 고차원 모델링 시 페널티를 구조화하기 위해 그룹 또는 그래프 라쏘(group or graph lasso) (Jacob, Obozinski, and Vert 2009)와 같은 접근 방식을 사용할 수 있습니다.\n관심 대상(샘플, 유전자 또는 서열)의 비지도 군집화 및 후속적인 과다 표현 어노테이션 검색. 우리는 군집화 알고리즘에 그것들이 측정된 서로 다른 불확실성뿐만 아니라 서로 다른 빈도들을 통합함으로써 더 나은 결과를 얻을 수 있습니다. 우리는 확률과 유사성을 사용하여 군집의 구성원들이 무작위로 뽑은 두 대상보다 더 유사한지 확인할 수 있습니다 (Callahan et al. 2016).\n\n분석에 착수할 때, 단일 방법을 적용하고 곧바로 결과를 얻어 끝나는 경우는 드물다는 것을 예상하는 것이 중요합니다. 우리는 다른 관련 데이터 세트들을 파헤쳐야 하고, 우리의 결과에 대한 확인(또는 반증)을 찾아야 하며, 추가적인 해석을 얻어야 합니다. 한 가지 예가 유전자 세트 농축 분석(gene set enrichment analysis)입니다: 우리의 데이터를 분석하고 관심 비교군과 관련이 있어 보이는 유전자 리스트를 찾은 후, 관여하는 광범위한 생물학적 프로세스를 탐구하기 위해 분자 시그니처 데이터베이스 (Liberzon et al. 2011)의 것과 같은 다른 유전자 리스트들과 겹쳐볼 것입니다. 또는 맥락을 찾기 위해 우리 연구의 상류 또는 하류의 조절 수준 10을 살펴보는 데이터 세트들을 불러올 수도 있습니다.\n10 게놈, 염색질 상태, 전사, mRNA 수명 주기, 번역, 단백질 수명 주기, 위치 및 상호작용; 대사산물, (…)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>실험 설계 (Design of Experiments)</span>"
    ]
  },
  {
    "objectID": "13-chap.html#도구-갈고닦기-재현-가능한-연구",
    "href": "13-chap.html#도구-갈고닦기-재현-가능한-연구",
    "title": "15  실험 설계 (Design of Experiments)",
    "section": "15.9 13.9 도구 갈고닦기: 재현 가능한 연구",
    "text": "15.9 13.9 도구 갈고닦기: 재현 가능한 연구\n분석 프로젝트는 종종 몇 가지 초기 아이디어를 시도하고 파일럿 데이터의 품질을 탐색하기 위한 단순한 스크립트로 시작됩니다. 그런 다음 더 많은 아이디어가 추가되고, 더 많은 데이터가 들어오고, 다른 데이터 세트가 통합되며, 더 많은 사람들이 참여하게 됩니다. 결국 논문을 써야 하고, 그림들을 ‘제대로’ 그려야 하며, 분석 내용은 과학적 기록과 그 무결성을 증명하기 위해 저장되어야 합니다. 여기에 그러한 과정에 도움이 될 수 있는 몇 가지 원칙들이 있습니다 11.\n11 데이터 관리, 프로그래밍, 동료와의 협업, 프로젝트 조직, 작업 추적 및 원고 작성을 포함하여 연구자를 위한 좋은 컴퓨팅 실무에 대한 훌륭하고 매우 읽기 쉬운 개요가 Wilson 등 (2017)에 의해 제시되었습니다.\n통합 개발 환경을 사용하세요. RStudio는 훌륭한 선택입니다. Emacs나 Eclipse와 같은 다른 플랫폼들도 있습니다.\n리터레이트 프로그래밍(literate programming) 도구인 Rmarkdown이나 Jupyter를 사용하세요. 이는 소스 코드의 주석이나 별도의 README 파일에 설명과 사용 지침을 묻어두는 것보다 (자신과 다른 사람들에게) 더 읽기 쉽습니다. 게다가 이러한 문서에 그림과 표를 직접 포함시킬 수 있습니다. 이러한 문서들은 여러분의 논문 보충 자료를 위한 좋은 시작점이 됩니다. 더욱이, 여러분의 협력자들에게 분석 내용을 보고하는 데에도 훌륭합니다.\n데이터 형식과 소프트웨어의 재설계(re-engineering)를 예상하세요. 처음에 데이터를 표현하고 분석 워크플로우를 구조화한 방식이 프로젝트가 진화함에 따라 계속 지원할 수 있는 경우는 드뭅니다. 여러분이 어색한 데이터 조작이나 반복적인 단계들을 많이 하고 있다는 것을 알아차리는 즉시, 과감하게 12 기존의 방식을 버리고 다시 설계하는 것을 두려워하지 마세요. 이는 보람 있는 시간 투자입니다. 거의 항상 버그를 찾아내는 데에도 도움이 됩니다.\n12 전문가들도 그렇게 합니다: “Google의 대부분의 소프트웨어는 몇 년마다 다시 작성됩니다.” (Henderson 2017)\n기존 도구들을 재사용하세요. 바퀴를 새로 발명하지 마세요. 여러분의 시간은 실제로 새로운 것들에 쓰이는 것이 더 가치가 있습니다. 직접 만든 “휴리스틱”이나 임시적인 “지름길”을 사용하기 전에, 이와 같은 것이 이전에 수행된 적이 있는지 몇 분 동안 조사해 보세요. 대개는 이미 존재하며, 때로는 깔끔하고 확장 가능하며 이미 검증된 해결책이 있습니다.\n버전 관리 를 사용하세요. 배우는 데 시간이 걸리지만, 이 시간은 보람 있게 쓰일 것입니다. 장기적으로 볼 때 버전 번호, 스위치 등을 사용하여 코드를 관리하려는 여러분의 모든 자생적인 시도들보다 무한히 더 나을 것입니다. 더욱이 이는 코드에 대한 협력 작업을 위한 가장 건전한 옵션이며, 서버가 여러분의 개인용 컴퓨터와 별개라면 코드베이스에 대한 추가적인 백업을 제공합니다.\n함수 를 사용하세요. 코드 구간을 복사-붙여넣기하거나 반복적으로 source하는 대신 함수를 사용하세요.\nR 패키지 시스템을 사용하세요. 곧 여러분의 서로 다른 스크립트들 사이에서 공유하고 싶은 반복적인 함수나 변수 정의들을 발견하게 될 것입니다. 처음에는 관리하기 위해 R 함수 source를 사용하는 것도 괜찮지만, 다른 사람들(혹은 자신)에게 어떤 기능을 어떻게 사용하는지 설명하는 이메일이나 코드 주석을 쓰기 시작할 때쯤이면 –늦어도– 그것들을 여러분만의 패키지로 옮기는 것이 좋습니다. 기존 코드를 R 패키지로 조립하는 것은 어렵지 않으며, 문서화, 코드 사용 예시 제시, 코드 테스트, 버전 관리 및 타인에게 제공하는 표준화된 방법들을 제공해 줍니다. 그리고 곧 네임스페이스를 사용하는 이점을 깨닫게 될 것입니다.\n가공되지 않은 데이터 파일의 위치를 중앙화하고 중간 데이터의 유도를 자동화하세요. 입력 데이터를 전문적으로 백업되는 중앙 파일 서버에 저장하세요. 파일들을 읽기 전용으로 표시하세요. 가공되지 않은 파일들로부터 파생된 데이터(예: 정규화, 요약, 변환 등)를 계산하기 위한 명확하고 선형적인 워크플로우를 가지고, 이들을 별도의 디렉토리에 저장하세요. 이 워크플로우가 여러 번 실행되어야 할 것임을 예상하고 13 버전을 관리하세요. BiocFileCache 패키지를 사용하여 이러한 파일들을 개인용 컴퓨터에 미러링하세요 14.\n13 항상 최종 데이터 프리즈(freeze) 직전의 정말 마지막 순간보다 한 번 더…\n14 좀 더 기초적인 대안으로 유틸리티가 있습니다. 일부 조직에서 제공하는 인기 있는 솔루션은 ownCloud에 기반합니다. Dropbox, Google Drive 등과 같은 상업적 옵션들도 있습니다.\n15 컴퓨터 과학에서 데이터 웨어하우스(data warehouse)라는 용어가 이러한 개념으로 사용되기도 합니다.\n요리 레시피 관점에서 생각하고 자동화하려고 노력하세요. 서로 다른 여러 데이터 유형을 한데 모으는 하류 분석 아이디어를 개발할 때, 데이터 유형 특이적인 형식에서 기계 학습이나 일반적인 통계 방법에 적합한 표현으로의 변환을 매번 즉석에서 새로 하고 싶지는 않을 것입니다. 서로 다른 재료들을 조립하고 그것들을 쉽게 소비 가능한 15 행렬, 데이터 프레임 또는 바이오컨덕터 SummarizedExperiment 로 요리해 주는 레시피 스크립트를 준비하세요.\n모든 분석 내용의 인덱스가 포함된 하이퍼링크 웹페이지를 유지하세요. 이는 협력자들에게 도움이 되며(특히 페이지와 분석 내용이 웹 브라우저를 통해 접근 가능하다면), 또한 여러분의 논문 방법론 부분의 좋은 시작점이 됩니다. 시간순이나 논리적 순서, 또는 두 가지의 조합으로 구조화하세요.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>실험 설계 (Design of Experiments)</span>"
    ]
  },
  {
    "objectID": "13-chap.html#데이터-표현",
    "href": "13-chap.html#데이터-표현",
    "title": "15  실험 설계 (Design of Experiments)",
    "section": "15.10 13.10 데이터 표현",
    "text": "15.10 13.10 데이터 표현\n분석이나 시각화를 위해 데이터를 준비하는 과정에는 데이터가 분석 알고리즘이나 그래픽 루틴에 적합한 형태와 형식을 갖출 때까지 많은 셔플링(shuffling)이 수반되곤 합니다. 3장에서 보았듯이, ggplot2는 측정 기록당 한 행을 가진 데이터 프레임 객체를 선호합니다. 이러한 선택의 배경은 Hadley Wickham의 타이디 데이터(tidy data) 에 관한 논문 (Wickham 2014)에 잘 설명되어 있습니다.\n\n15.10.1 13.10.1 넓은 테이블 형식 대 긴 테이블 형식\nHiiragi 데이터를 떠올려 봅시다 (공간상 4개의 유전자만 선택하고, xwdf의 처음 5개 열만 출력합니다):\nlibrary(\"magrittr\")\ndata(\"x\", package = \"Hiiragi2013\")\nxwdf = tibble(\n  probe  = c(\"1420085_at\", \"1418863_at\", \"1425463_at\", \"1416967_at\"),\n  symbol = c(      \"Fgf4\",      \"Gata4\",      \"Gata6\",       \"Sox2\"))\nxwdf %&lt;&gt;% bind_cols(as_tibble(Biobase::exprs(x)[xwdf$probe, ]))\ndim(xwdf)\n\n\n[1]   4 103\n\n\nxwdf[, 1:5]\n\n\n# A tibble: 4 × 5\n  probe      symbol `1 E3.25` `2 E3.25` `3 E3.25`\n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 1420085_at Fgf4        3.03      9.29      2.94\n2 1418863_at Gata4       4.84      5.53      4.42\n3 1425463_at Gata6       5.50      6.16      4.58\n4 1416967_at Sox2        1.73      9.70      4.16\n이 데이터 프레임의 각 행은 선택된 유전자 중 하나에 해당합니다. 처음 두 열은 Affymetrix 프로브 식별자와 유전자 심볼을 포함합니다. 나머지 101개 열은 각 샘플에 대해 측정된 발현 값을 보고합니다. 샘플 식별자는 샘플을 채취한 시점에 대한 정보와 함께 열 이름에 연결된 문자열로 기록되어 있습니다. 이것이 넓은 형식(wide format)의 데이터 테이블 예시입니다. 이제 tidyr 패키지의 pivot_longer 함수를 호출하고 그 출력을 살펴봅시다.\nlibrary(\"tidyr\")\nxldf = pivot_longer(xwdf, cols = !all_of(c(\"probe\", \"symbol\")),\n                          names_to = \"sample\")\ndim(xldf)\n\n\n[1] 404   4\n\n\nhead(xldf)\n\n\n# A tibble: 6 × 4\n  probe      symbol sample  value\n  &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;\n1 1420085_at Fgf4   1 E3.25  3.03\n2 1420085_at Fgf4   2 E3.25  9.29\n3 1420085_at Fgf4   3 E3.25  2.94\n4 1420085_at Fgf4   4 E3.25  9.72\n5 1420085_at Fgf4   5 E3.25  8.92\n6 1420085_at Fgf4   6 E3.25 11.3 \nxldf에서 각 행은 value 열에 저장된 정확히 하나의 측정값에 해당합니다. 그리고 해당 측정값과 관련된 공변량들을 저장하는 추가적인 열들인 probe, symbol, sample이 있습니다. 이것이 긴 형식(long format)의 사례입니다.\nxwdf에서 일부 열은 모든 샘플의 데이터와 관련이 있는 반면(즉, probe와 symbol), 다른 열들(발현 측정값이 있는 열들)은 샘플 특이적인 정보를 포함하고 있습니다. 우리는 데이터 프레임을 해석할 때 이를 어떻게든 “알고” 있어야 합니다. 이것이 바로 Hadley Wickham이 지저분한 데이터(untidy data) 라고 부르는 것입니다 16. 이와 대조적으로, 깔끔한(tidy) 데이터 프레임 xldf에서는 각 행이 정확히 하나의 관측치를 형성하며, 그 값은 value라는 이름의 열에 있고, 그 관측치와 관련된 다른 모든 정보는 같은 행의 다른 열에 있습니다. Ensembl 유전자 식별자나 염색체 위치와 같은 추가 열을 더하고 싶다면 간단히 추가하면 됩니다. 유사하게, 더 많은 유전자나 추가 샘플의 데이터를 더하고 싶다면 간단히 xldf에 해당 행들을 추가하면 됩니다. 어느 쪽이든, 우리는 기존의 코드를 깨뜨리지 않을 것이라고 가정할 수 있습니다. 이는 열을 추가하는 것이 데이터 열(측정값이 있는 열)과 공변량 열 사이의 구분을 확신할 수 없게 하여 기존 코드를 무효화할 수 있는 xwdf와는 대조적입니다.\n16 안나 카레니나 원리를 떠올려 보세요: 데이터가 지저분해지는 방법은 수없이 많습니다.\n또한, 프로브 식별자, 유전자 심볼 또는 샘플, 그리고 사실상 다른 모든 공변량에 의한 하위 집합 추출(subsetting)이 간단하며 항상 동일한 dplyr::filter 구문을 사용할 수 있습니다. 이와 대조적으로 xwdf에서는 샘플의 하위 집합 추출은 열 추출에 해당하고, 유전자의 하위 집합 추출은 행 추출에 해당한다는 것을 기억해야 합니다.\nHiiragi 데이터는 xwdf 외에도 또 다른 자연스러운 넓은 형식 표현을 가집니다: 유전자당 하나의 행과 서로 다른 샘플들에 대한 열 대신, 샘플당 하나의 행과 서로 다른 유전자들에 대한 열을 가질 수도 있습니다. 이 두 가지 넓은 표현 모두 유용할 수 있습니다. 예를 들어, ggplot2를 사용하여 두 샘플 사이의 모든 유전자 발현 값에 대한 산점도를 그리거나, 두 유전자 사이의 모든 샘플에 대한 산점도를 그리려면 두 넓은 형식 중 하나를 사용해야 합니다.\n긴 형식에서 넓은 형식(둘 중 어느 것이든)으로 변환하려면, 위에서 이미 사용했던 pivot_longer 함수의 보완재인 tidyr 패키지의 pivot_wider 함수를 사용할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>실험 설계 (Design of Experiments)</span>"
    ]
  },
  {
    "objectID": "13-chap.html#타이디-데이터-현명하게-사용하기",
    "href": "13-chap.html#타이디-데이터-현명하게-사용하기",
    "title": "15  실험 설계 (Design of Experiments)",
    "section": "15.11 13.11 타이디 데이터 – 현명하게 사용하기",
    "text": "15.11 13.11 타이디 데이터 – 현명하게 사용하기\n타이디 데이터(tidy data) (Wickham 2014)에서는,\n\n각 변수는 하나의 열을 형성합니다.\n각 관측치는 하나의 행을 형성합니다.\n각 유형의 관측 단위는 하나의 테이블을 형성합니다.\n\ntidyverse의 성공은 그 기저의 아이디어의 힘과 구현의 품질을 증명합니다. 이 책의 많은 코드가 이러한 아이디어를 채택했고 tidyverse를 사용하고 있습니다.\n그럼에도 불구하고, 긴 형식의 데이터 프레임이 만병통치약은 아닙니다. 다음 사항들을 염두에 두세요:\n효율성과 무결성. 비록 프로브-유전자 심볼 관계는 4개뿐이지만, 우리는 이를 xldf의 행에 404번 반복해서 저장하고 있습니다. 이 사례에서 추가적인 저장 비용은 무시할 수 있습니다. 다른 사례에서는 더 상당할 수도 있습니다. 더 중요한 것은 정보의 확산입니다: 우리가 xldf와 같은 객체를 받고 그것이 사용하는 모든 프로브-유전자 심볼 관계를 알고 싶을 때, 우리는 데이터 프레임 내의 수많은 복사본으로부터 이 정보를 다시 모아야 합니다. 우리는 추가적인 확인 없이 중복된 복사본들이 서로 일치한다고 확신할 수 없으며, 정보를 업데이트하고 싶다면 여러 곳을 수정해야 합니다. 이는 xldf와 같은 객체를 장기적인 데이터 저장용으로 사용하지 않고, 분석의 비교적 늦은 단계에서 기본 데이터 객체를 포함하는 더 정규화된 17 데이터 컨테이너로부터 조립하여 사용하는 워크플로우 설계를 옹호하는 논거가 됩니다.\n17 데이터 정규화(Data normalization)는 데이터베이스를 조직하여 중복을 줄이고 무결성을 개선하는 과정입니다. 예: https://en.wikipedia.org/wiki/Database_normalization.\n계약과 표준화의 부재. 우리가 xldf와 같은 객체에 대해 작동할 것으로 기대되는 함수를 작성할 때, 우리는 probe 열이 실제로 유효한 프로브 식별자를 포함하고 있는지, 심지어 그러한 열이 존재하는지조차 보장받지 못합니다. tidyverse에는 ” xldf와 같은 객체”가 의미하는 바를 프로그래밍 방식으로 표현하는 직접적인 방법조차 없습니다. 객체 지향(OO) 프로그래밍, 그리고 R에서의 그 화신인 S4가 이러한 문제들을 해결합니다. 예를 들어, 위에서 언급한 확인 작업들은 적절하게 정의된 클래스에 대한 validObject 메서드에 의해 수행될 수 있으며, 클래스 정의는 ” xldf와 같은 객체”라는 개념을 공식화할 것입니다. 이러한 이슈들을 다루는 것이 SummarizedExperiment 클래스와 같은 바이오컨덕터의 데이터 구조들의 객체 지향 설계 배경입니다. OO 데이터 표현의 다른 잠재적으로 유용한 특징들은 다음과 같습니다:\n\n구현으로부터 인터페이스의 추상화 및 캡슐화: 사용자는 정의된 채널을 통해서만 데이터에 접근하고 데이터가 “내부적으로” 어떻게 저장되는지 볼 필요가 없습니다 – 이는 사용자 수준의 코드를 깨뜨리지 않고 내부를 변경하고 최적화할 수 있음을 의미합니다.\n다형성(Polymorphism): 서로 다른 클래스의 객체들에 대해 plot이나 filter와 같이 동일한 이름을 가진 서로 다른 함수들을 가질 수 있으며, R이 여러분을 위해 어느 것을 호출할지 결정해 줍니다.\n상속(Inheritance): 단순한 것으로부터 더 복잡한 데이터 표현을 구축할 수 있습니다.\n성찰(Reflection) 및 자기 문서화: 객체에 프로그래밍 방식의 쿼리를 보내 자기 자신에 대한 정보를 물어볼 수 있습니다.\n\n이 모든 것들은 비록 인프라와 “관료주의”에 대한 초기 투자가 더 많이 필요하긴 하지만, 구성 요소들의 구현 세부 사항보다는 큰 그림의 기능에 집중하는 고수준 코드를 작성하는 것을 더 쉽게 만들어 줍니다.\n데이터 출처 및 메타데이터. xldf와 같은 객체에는 데이터 출처에 대한 정보, 예를 들어 누가 실험을 수행했는지, 어디에 발표되었는지, 어디서 데이터를 다운로드했는지 또는 우리가 보고 있는 데이터의 버전이 무엇인지(데이터 버그는 존재하니까요…)를 추가할 명백한 장소가 없습니다. 또한 단위나 어세이 유형과 같은 열에 대한 설명도 없습니다. 다시 말하지만, 바이오컨덕터의 데이터 클래스들이 이러한 요구를 해결하려고 시도합니다.\n\n그림 13.14: 순차적인 데이터 분석 워크플로우는 샐 수 있습니다. 만약 한 단계에서 다음 단계로 충분한 정보가 전달되지 않는다면, 절차는 결국 최적화되지 못하고 검정력을 잃게 될 수 있습니다.\n행렬 형태의 데이터. 생물학의 많은 데이터 세트는 자연스러운 행렬 구조를 가집니다. 다수의 특성(예: 유전자; 관례상 행렬의 행)이 여러 샘플(관례상 행렬의 열)에서 분석되기 때문입니다. 행렬을 xldf와 같은 긴 형태로 풀어헤치는 것은 일부 연산(예: PCA, SVD, 특성 또는 샘플의 군집화)을 더 번거롭게 만듭니다.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>실험 설계 (Design of Experiments)</span>"
    ]
  },
  {
    "objectID": "13-chap.html#새는-파이프라인과-통계적-충분성",
    "href": "13-chap.html#새는-파이프라인과-통계적-충분성",
    "title": "15  실험 설계 (Design of Experiments)",
    "section": "15.12 13.12 새는 파이프라인과 통계적 충분성",
    "text": "15.12 13.12 새는 파이프라인과 통계적 충분성\n고처리량 생물학에서의 데이터 분석 파이프라인은 종종 데이터를 순차적으로 요약하고 압축하는 ’깔때기’처럼 작동합니다. 고처리량 시퀀싱에서 우리는 플로우 셀의 현미경 이미지로 시작하여, 베이스 콜링(base calling)을 수행하여 시퀀싱 리드를 도출하고, 이를 참조 서열에 정렬한 다음, 정렬된 리드만을 각 위치별로 세고, 위치들을 유전자(또는 다른 종류의 영역)별로 요약하고, 이 수치들을 라이브러리 크기에 따라 “정규화”하여 라이브러리 간에 비교 가능하게 만드는 식입니다. 각 단계에서 우리는 정보를 잃게 되지만, 당면한 과제에 대해 여전히 충분한 정보를 가지고 있는지 확인하는 것이 중요합니다 18. 이 문제는 우리가 파이프라인을 각기 다른 개발자들이 만든 일련의 구성 요소들로 구축할 때 특히 심각해집니다.\n18 예를 들어, 8장에서 보았던 RNA-Seq 차등 발현 분석을 위해서는 “정규화된” 버전이 아닌 실제 리드 카운트가 필요했습니다. 어떤 분석에는 유전자 수준의 요약으로 충분할 수 있지만, 다른 분석에서는 엑손이나 이소형 수준을 살펴보고 싶을 것입니다.\n통계학자들은 특정 요약 정보가 데이터의 모든 관련 정보를 재구성할 수 있게 해주는지에 대한 개념을 가지고 있습니다: 충분성(sufficiency). 시행 횟수 (n)이 알려진 베르누이 무작위 실험에서, 성공 횟수는 성공 확률 (p)를 추정하기 위한 충분 통계량입니다.\n질문 13.14\n13장에서 보았던 것과 같은 4개 상태(A, C, G, T) 마르코프 체인에서, 전이 확률을 추정하기 위한 충분 통계량은 무엇인가요?\nEM 알고리즘을 사용했을 때와 유사한 반복적인 접근 방식은 때때로 정보 손실을 피하는 데 도움이 될 수 있습니다. 예를 들어, 질량 분석 데이터를 분석할 때, 첫 번째 실행은 각 샘플에 대해 개별적으로 피크를 추측합니다. 이러한 예비적인 스펙트럼 포착 후, 다른 반복 단계를 통해 다른 샘플들로부터 힘을 빌려 이전에 간과되었던(노이즈처럼 보였던) 스펙트럼을 포착할 수 있게 됩니다.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>실험 설계 (Design of Experiments)</span>"
    ]
  },
  {
    "objectID": "13-chap.html#효율적인-계산",
    "href": "13-chap.html#효율적인-계산",
    "title": "15  실험 설계 (Design of Experiments)",
    "section": "15.13 13.13 효율적인 계산",
    "text": "15.13 13.13 효율적인 계산\n데이터 획득 기술의 빠른 발전은 점점 더 거대한 데이터 세트로 이어지며, 이를 다루는 것은 하나의 도전 과제입니다. 빅데이터와 확장성을 위해 설계된 소프트웨어 기술로 곧장 뛰어들고 싶은 유혹이 생깁니다. 하지만 대개는 먼저 한 걸음 물러나서 생각하는 것이 더 도움이 됩니다. 소프트웨어 엔지니어들은 조기 최적화(premature optimization) 의 위험성을 알고 있습니다. 혹은 John Tukey의 말을 빌리자면 19: “올바른 문제에 대한 느리고 서툰 해결책이 잘못된 문제에 대한 빠르고 확장 가능한 해결책보다 훨씬 더 가치 있다.” 때로는 확장성과 성능을 추구하기 전에 데이터의 하위 집합에서 올바른 해결책이 무엇인지 먼저 알아내는 것이 좋은 전략입니다.\n19 http://stats.stackexchange.com/a/744\n또한 CPU 시간보다는 여러분 자신의 시간의 가치를 염두에 두는 것이 좋습니다. 긴 계산 시간을 대가로 코드 개발 시간을 절약할 수 있다면, 그것은 가치 있는 상충 관계일 수 있습니다.\n이 모든 것을 고려한 후, 성능에 대해 이야기해 봅시다. R은 속도가 느리고 메모리를 낭비한다는 평판이 있으며, 그러한 인식이 다른 플랫폼을 선택하는 동기로 언급되기도 합니다. 어떤 경우에는 이것이 정당화됩니다: 아무도 단일 리드 정렬기(short read aligner)나 자율 주행차의 조종 로직을 R로 작성하라고 권하지 않을 것입니다. 그러나 통계 분석의 경우, 다음 중 하나 이상의 개념을 사용하여 매우 효율적인 코드를 작성하는 것이 가능합니다:\n벡터화(Vectorization). 동일한 결과를 계산하는 다음의 대안적인 선택지들을 고려해 보세요.\na = runif(1e6)\nb = runif(length(a))\nsystem.time({\n  z1 = numeric(length(a))\n  for (i in seq(along = a))\n    z1[i] = a[i]^2 * b[i]\n})\n\n\n   user  system elapsed \n  0.076   0.001   0.076 \n\n\nsystem.time({\n  z2 = a^2 * b\n})\n\n\n   user  system elapsed \n  0.003   0.000   0.003 \n\n\nidentical(z1, z2)\n\n\n[1] TRUE\n벡터화된 버전(z2)은 명시적으로 인덱싱된 버전(z1)보다 여러 배 빠르며 읽기도 더 쉽습니다. 때로는 인덱스를 사용하여 공식화된 알고리즘을 번역하는 것이 조금 더 어려울 때가 있습니다 – 예를 들어 if 조건문이 있거나, 인덱스 i에 대한 계산이 인덱스 i-1의 결과를 포함하는 경우입니다. ifelse를 사용한 벡터화된 조건문, dplyr 패키지의 lead나 lag와 같은 함수를 사용한 벡터 이동, 그리고 일반적으로 (행 단위가 아닌) 데이터 프레임 전체에 대한 계산을 표현하도록 설계된 dplyr 의 인프라가 도움이 될 수 있습니다.\n병렬화(Parallelization). R로 계산을 병렬화하는 것은 쉽습니다. 특히 R이 명시적인 입력과 출력을 가지고 부작용(side effects)이 없는 함수로 계산을 표현하는 것이 자연스러운 함수형 언어이기 때문입니다. 병렬 계산을 지원하는 R 패키지와 기능의 지형은 빠르게 변하고 있습니다. CRAN 태스크 뷰 “고성능 및 병렬 컴퓨팅(High-Performance and Parallel Computing)”과 BiocParallel 패키지가 좋은 시작점입니다.\n메모리 부족 데이터 및 청킹(chunking). 일부 데이터 세트는 랜덤 액세스 메모리(RAM)에 한꺼번에 올리고 조작하기에는 너무 큽니다. 청킹(Chunking)은 데이터를 관리 가능한 부분(“청크”)으로 나누고, 보조 저장 장치로부터 각 부분을 순차적으로 로드하여 계산하고 결과를 저장한 후, 다음 부분을 로드하기 전에 RAM에서 제거하는 것을 의미합니다. R은 또한 관계형 데이터베이스 관리 시스템(DBI 패키지)이나 HDF5(rhdf5 패키지)에 저장된 대규모 데이터 세트로 작업하기 위한 인프라를 제공합니다. 바이오컨덕터 프로젝트는 SummarizedExperiment 클래스를 제공하는데, 이는 이 클래스의 사용자에게 투명한 방식으로 대규모 데이터 행렬을 RAM이나 HDF5 백엔드에 저장할 수 있습니다.\n저수준 언어의 적절한 사용. Rcpp 패키지는 코드의 일부를 C++로 작성하고 이를 R 코드 내에 매끄럽게 포함시키는 것을 쉽게 만들어 줍니다. R 클래스인 numeric 벡터를 감싸는 C++ 클래스 NumericVector와 같이 편리한 래퍼들이 많이 제공됩니다.\nlibrary(\"Rcpp\")\ncppFunction(\" \n  NumericVector myfun(NumericVector x, NumericVector y) {\n    int n = x.size();\n    NumericVector out(n);\n    for(int i = 0; i &lt; n; ++i) {\n      out[i] = pow(x[i], 2) * y[i];\n    }\n    return out;\n  }\")\nz3 = myfun(a, b)\nidentical(z1, z3)\n\n\n[1] TRUE\n실제로 위의 코드에는 y의 길이를 확인하는 절차도 포함되어야 할 것입니다. 여기서 우리는 C++ 코드를 R 문자 벡터로 Rcpp에 제공했는데, 이는 짧은 삽입 구문에 편리합니다. 더 큰 함수의 경우, C++ 코드를 별도의 파일에 저장할 수 있습니다. 물론 아이디어는 모든 코드를 C++로 작성하는 것이 아니라, 가장 시간이 많이 걸리는 부분만을 작성하는 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>실험 설계 (Design of Experiments)</span>"
    ]
  },
  {
    "objectID": "13-chap.html#이-장의-요약",
    "href": "13-chap.html#이-장의-요약",
    "title": "15  실험 설계 (Design of Experiments)",
    "section": "15.14 13.14 이 장의 요약",
    "text": "15.14 13.14 이 장의 요약\n이 마지막 장에서 우리는 책 전반에 걸쳐 나타났던 몇 가지 개념과 아이디어들을 수집하고, 일반화하고, 분류하려고 노력했습니다. 이들은 여러분이 정보가 풍부한 실험이나 연구를 설계하고 이를 효과적으로 분석하는 데 도움을 줄 수 있습니다. 이러한 아이디어 중 일부는 직관적이고 자연스럽습니다. 13.4.3절의 Hotelling의 가중치 예시와 같이 덜 직관적인 아이디어들도 있습니다. 이는 공식적인 수학적 추론을 필요로 합니다. 분석적인 계산을 할 수 없는 경우라도, 서로 다른 비자명한 설계 선택지들을 벤치마킹하기 위해 시뮬레이션을 하거나 기존의 유사한 데이터를 가지고 계산해 볼 수 있을 것입니다.\n또한 어떤 아이디어들은 훈련과 선견지명을 필요로 합니다: 예를 들어, “데일리(dailies)”는 시간과 주의를 끄는 다른 많은 우려사항들과 경쟁하는 실험 캠페인의 열기 속에서 쉽게 잊히거나 합리화될 수 있습니다. 개별적인 상황에서 주방을 깔끔하게 유지하거나 건강하게 먹는 것을 건너뛰고 넘어갈 수도 있겠지만 – 일반적인 접근 방식으로는 권장되지 않습니다.\n우리는 계산 실무의 중요성을 강조했습니다. 책 전체에 걸쳐 수많은 코드가 섞여 있고 거의 모든 데이터 시각화가 “라이브”로 이루어지는 과정을 통해, 우리는 계산 분석을 설정하는 많은 예시들을 보았습니다. 그럼에도 불구하고, 여러분 자신의 데이터로 직접 분석을 실행하는 것은 책의 계산 과정을 따라가는 것과는 매우 다릅니다 – 마치 요리책을 읽는 것이 연회를 준비하거나 심지어 단 한 가지 요리를 만드는 것과 매우 다른 것처럼 말이죠. 여러분을 더욱 무장시키기 위해, 13.15절에 언급된 리소스들을 강력히 추천합니다. 그리고 여러분의 즐거운 요리를 기원합니다!",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>실험 설계 (Design of Experiments)</span>"
    ]
  },
  {
    "objectID": "13-chap.html#더-읽을거리",
    "href": "13-chap.html#더-읽을거리",
    "title": "15  실험 설계 (Design of Experiments)",
    "section": "15.15 13.15 더 읽을거리",
    "text": "15.15 13.15 더 읽을거리\n\n이 장은 _실험 설계_에 대한 실용적이고 간략한 소개만을 제시했습니다. 혼동을 피하고 검정력을 최적화하기 위해 실험을 설정하는 것에 대한 상세한 조언을 제공하는 수많은 교재들이 있습니다 (Wu and Hamada 2011; Box, Hunter, and Hunter 1978; Glass 2007).\n우리는 더 정교한 절차들의 겉핥기조차 하지 못했습니다. 예를 들어 결정을 내릴 수 있게 되면 바로 중단할 수 있는 일련의 실험들을 설정할 가능성이 있다면, 순차적 설계(sequential design) 를 공부해야 할 것입니다 (Lai 2001). “좋은” 시작점을 선택하고 연속적인 결과를 사용하여 다음 지점들을 선택함으로써 복잡한 반응 표면(response surfaces)을 탐색하는 것은 매우 효과적일 수 있습니다. Box, Draper 등 (1987)은 매우 소중한 리소스입니다.\nGentleman 등 (2004)은 바이오컨덕터의 데이터 구조와 소프트웨어 설계 이면의 아이디어를 설명하며, Huber 등 (2015)은 바이오컨덕터가 사용자 및 개발자를 위해 협력적 소프트웨어 개발을 어떻게 지원하는지에 대한 업데이트를 제공합니다.\nGit 및 GitHub. Jenny Bryan의 웹사이트 Happy Git and GitHub for the useR은 R과 함께 버전 관리를 사용하는 것에 대한 훌륭한 입문서입니다.\nWickham (2014)은 타이디 데이터 의 원칙을 설명합니다.\n충분히 좋은 실무(Good enough practices). Wilson 등 (2017)은 과학적 계산에서 성공하기 위한 실용적이고 현명한 권장 사항들을 제시합니다.\n매뉴얼 Writing R Extensions은 R 패키지 제작 을 위한 궁극적인 참고서입니다. 이는 바이오컨덕터의 패키지 가이드라인과 함께 읽는 것이 좋습니다.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>실험 설계 (Design of Experiments)</span>"
    ]
  },
  {
    "objectID": "13-chap.html#연습-문제",
    "href": "13-chap.html#연습-문제",
    "title": "15  실험 설계 (Design of Experiments)",
    "section": "15.16 13.16 연습 문제",
    "text": "15.16 13.16 연습 문제\n연습 문제 13.1\n여러분의 측정값이 대칭 라플라스 분포(4장에서 정의된 정규 분포의 무한 혼합)를 따르는 노이즈에 영향을 받을 것임을 알고 있을 때, 얼마나 많은 피험자가 필요한지 결정하기 위한 시뮬레이션 실험을 설정하세요. 서로 다른 가능한 노이즈 수준과 효과 크기가 포함된 테이블을 설정해야 할 것입니다.\n연습 문제 13.2\n바이오컨덕터 패키지 PROPER를 사용하여 RNA-Seq 실험을 위한 샘플 수를 결정하고, 그 결과를 RNASeqPower 바이오컨덕터 패키지의 결과와 비교해 보세요.\n연습 문제 13.3\nR의 model.matrix 함수를 확인해 보세요. 매뉴얼 페이지를 읽고 거기 제공된 예제들을 탐색해 보세요.\n연습 문제 13.4\n여러분의 최근 데이터 분석 중 하나로 돌아가서 이를 R 패키지로 조립해 보세요.\n해결책\n\n하나 이상의 반복적인 작업(예: 플롯)을 함수로 모으고 매뉴얼 페이지를 작성하세요 (roxygen2를 사용할 수 있습니다).\ndata 또는 inst/extdata 디렉토리 아래에 데이터 세트를 추가하세요.\n이미 해당 형식이 아니라면, 여러분의 분석 스크립트를 Rmarkdown으로 변환하세요.\n모든 오류와 경고가 사라질 때까지 R CMD build와 R CMD check를 실행하세요.\n\n간단한 소개가 여기에 있습니다: https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch. 더 자세한 내용은 모든 R 설치 시 함께 제공되는 매뉴얼 Writing R Extensions에 있습니다.\n연습 문제 13.5\nGitHub 계정을 열고 여러분의 패키지를 업로드하세요. 힌트: Jenny Bryan의 Happy Git and GitHub for the useR 사이트의 지침을 따르세요.\n연습 문제 13.6\nrenjin 프로젝트와 renjin 패키지를 확인해 보세요. renjin으로 컴파일된 코드와 순수 R 코드, 그리고 위에서와 같이 Rcpp를 사용하여 C/C++로 번역된 코드를 비교해 보세요.\n해결책\n다음 Gist를 참조하세요: https://gist.github.com/wolfganghuber/909e14e45af6888eec384b82682b3766.\n1000 Genomes Project Consortium. 2012. “An Integrated Map of Genetic Variation from 1,092 Human Genomes.” Nature 491 (7422): 56–65.\nAltman, Naomi, and Martin Krzywinski. 2017. “Points of Significance: Interpreting p Values.” Nature Methods 14 (3): 213–14. https://doi.org/10.1038/nmeth.4210.\nBacher, Rhonda, and Christina Kendziorski. 2016. “Design and Computational Analysis of Single-Cell RNA-Sequencing Experiments.” Genome Biology 17 (1): 1.\nBox, George EP, Norman Richard Draper, et al. 1987. Empirical Model-Building and Response Surfaces. Vol. 424. Wiley New York.\nBox, George EP, William G Hunter, and J Stuart Hunter. 1978. Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building. John Wiley & Sons.\nCallahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy J Johnson, and Susan P Holmes. 2016. “DADA2: High Resolution Sample Inference from Amplicon Data.” Nature Methods , 1–4.\nDiaconis, Persi, Susan Holmes, and Richard Montgomery. 2007. “Dynamical Bias in the Coin Toss.” SIAM Review 49 (2): 211–35.\nDurbin, Richard, Sean Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis. Cambridge University Press.\nFisher, Ronald Aylmer. 1935. The Design of Experiments. Oliver & Boyd.\nGentleman, Robert C, Vincent J Carey, Douglas M Bates, Ben Bolstad, Marcel Dettling, Sandrine Dudoit, Byron Ellis, et al. 2004. “Bioconductor: Open Software Development for Computational Biology and Bioinformatics.” Genome Biology 5 (10): R80. https://doi.org/10.1186/gb-2004-5-10-r80.\nGlass, David J. 2007. Experimental Design for Biologists. Cold Spring Harbor Laboratory Press.\nHenderson, Fergus. 2017. “Software Engineering at Google.” ArXiv e-Prints. https://arxiv.org/abs/1702.01715.\nHotelling, Harold. 1944. “Some Improvements in Weighing and Other Experimental Techniques.” The Annals of Mathematical Statistics 15 (3): 297–306.\nHuber, Wolfgang, Vincent J Carey, Robert Gentleman, Simon Anders, Marc Carlson, Benilton S Carvalho, Hector Corrada Bravo, et al. 2015. “Orchestrating High-Throughput Genomic Analysis with Bioconductor.” Nature Methods 12 (2): 115–21.\nJacob, Laurent, Guillaume Obozinski, and Jean-Philippe Vert. 2009. “Group Lasso with Overlap and Graph Lasso.” In Proceedings of the 26th Annual International Conference on Machine Learning , 433–40. ACM.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>실험 설계 (Design of Experiments)</span>"
    ]
  },
  {
    "objectID": "14-chap.html",
    "href": "14-chap.html",
    "title": "16  부록: 통계 개념 및 방법 목록 (Appendix: Statistical Concepts and Methods)",
    "section": "",
    "text": "색인을 보완하기 위해, 통계 개념 및 절차와 이들이 다루어진 장의 목록을 제공합니다.\n\n\n\n방법\n장\n\n\n\n\n분산분석\n8 고처리량 카운트 데이터 & 일반화 선형 모델\n\n\n베이지안 통계\n2 통계 모델링\n\n\n부트스트랩\n4 혼합 모델, 5 군집화\n\n\n카이제곱 검정\n2 통계 모델링\n\n\n군집화\n4 혼합 모델, 5 군집화\n\n\n대응분석\n9 이질적 데이터를 위한 다변량 방법\n\n\n데이터 변환\n4 혼합 모델, 8 고처리량 카운트 데이터 & 일반화 선형 모델\n\n\n확산 맵 모델\n9 이질적 데이터를 위한 다변량 방법\n\n\n거리\n4 혼합 모델, 9 이질적 데이터를 위한 다변량 방법\n\n\n탐색적 데이터 분석 (EDA)\n3 데이터 시각화\n\n\n피셔의 정확 검정\n10 네트워크와 트리\n\n\n거짓 발견율 (FDR)\n6 검정\n\n\n적합도\n2 통계 모델링\n\n\n일반화 선형 모델\n8 고처리량 카운트 데이터 & 일반화 선형 모델\n\n\n초기하 검정\n10 네트워크와 트리\n\n\n가설 검정\n1 이산 데이터를 위한 생성 모델, 6 검정\n\n\n마르코프 체인\n2 통계 모델링, 10 네트워크와 트리\n\n\n최대 우도\n2 통계 모델링\n\n\n다차원 척도법 (MDS)\n9 이질적 데이터를 위한 다변량 방법\n\n\n다중 가설 검정\n6 검정\n\n\n다변량 회귀\n8 고처리량 카운트 데이터 & 일반화 선형 모델, 9 이질적 데이터를 위한 다변량 방법, 12 지도 학습\n\n\n서열화 및 기울기 탐지\n9 이질적 데이터를 위한 다변량 방법\n\n\nP-값\n1 이산 데이터를 위한 생성 모델\n\n\n순열 검정\n10 네트워크와 트리\n\n\n계통발생학\n10 네트워크와 트리\n\n\n검정력 계산\n13 고처리량 실험 설계 및 분석\n\n\n주성분 분석 (PCA)\n7 다변량 분석\n\n\n주좌표 분석 (PCoA)\n9 이질적 데이터를 위한 다변량 방법\n\n\n회귀\n7 다변량 분석,\n\n\n강건한 방법\n12 지도 학습\n\n\n공간 통계\n11 이미지 데이터\n\n\n\n페이지는 R 버전 4.5.1 (2025-06-13)을 사용하여 2025-09-01 01:33에 빌드되었습니다.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>부록: 통계 개념 및 방법 목록 (Appendix: Statistical Concepts and Methods)</span>"
    ]
  },
  {
    "objectID": "15-chap.html",
    "href": "15-chap.html",
    "title": "17  헌사 (Dedication)",
    "section": "",
    "text": "Sonia, Sara, Agnès, Johnny, Camille을 위하여\n\\(…\\) 그리고 나에게 생명과학을 사랑하게 만든 “소녀들”을 위하여.\nAlexander와 Daniel을 위하여.\n이 작업은 통계 계산을 위한 R 언어와 환경, 포괄적 R 아카이브 네트워크(CRAN), 그리고 Bioconductor 프로젝트 없이는 상상할 수 없었을 것입니다. 이러한 프로젝트에 기여한 모든 분들께 감사드립니다. 오늘날 사실상 모든 통계 알고리즘, 데이터 처리 및 시각화를 위한 모든 상상 가능한 인터페이스, 그리고 컴퓨터 과학과 수학 전반의 많은 방법들이 이러한 프로젝트를 통해 쉽게 접근할 수 있습니다.\nJJ Allaire와 RStudio(2022년부터: Posit) 팀이 이 책을 집필할 때 우리가 크게 즐겼던 강력한 개발 환경과 많은 유용한 R 패키지를 제공해 주신 것에 감사드립니다. 여기에는 knitr와 pandoc을 기반으로 하는 quarto 출판 시스템이 포함됩니다.\n게놈 규모 데이터를 위한 R 기반 방법의 상호 운용성, 확장성 및 사용성을 촉진하고, 광범위한 생물학적 데이터 및 주석 리소스를 R에서 쉽게 작업할 수 있도록 만들며, 협력적이고 분산된 개발을 조율하는 놀라운 개발자 커뮤니티로 구동되는 Bioconductor 프로젝트에 특별히 감사드립니다. 이 모든 측면은 이 책에서 볼 수 있는 복잡한 생물학적 데이터 분석 워크플로우에 필수적입니다.\n우리가 함께 작업했고 이 책의 여러 장에서 중심 무대를 차지하는 패키지 개발자들에게 감사드립니다. Simon Anders, Ben Callahan, Michael Love, Joey McMurdie, Andrzej Oleś를 포함합니다. Trevor Martin은 2012년 스탠포드의 Stats 366 수업의 학생이었으며 2013년, 2014년, 2015년에 Susan과 함께 수업을 공동으로 가르쳤습니다. 유전학 대학원생으로서 그는 많은 예제에 생명을 불어넣었고 우리가 여기서 제시하는 자료의 초기 버전 개발에 참여했습니다. 그의 도움과 관점에 감사드립니다. 연습 문제와 질문 개발을 도운 Stats 366의 조교들로는 Austen Head, Haben Michael, Julia Fukuyama, Lan Huong Nguyen, Christof Seiler, Nikolaos Ignatiadis가 있습니다. 흥미로운 퀴즈와 실습 자료를 만들려는 그들의 열정은 엄청난 힘을 가지고 있지만 압도적일 수도 있는 계산 환경 내에서 도전적인 새로운 개념에 접근하는 힘든 여정을 다양한 배경의 학생들이 헤쳐나가도록 돕는 데 기여했습니다.\n수년 동안 많은 학생들이 귀중한 피드백을 제공해 주었으며, 우리는 이 과정을 계속 발전시키려는 동기를 부여한 그들의 많은 질문과 의아해하는 표정에 감사드립니다. 특히 Jessica Grembi, Kris Sankaran, Varun Gupta, Chao Jiang으로부터 광범위한 피드백을 받았습니다.\nMike Smith는 현재 여러분이 보고 계신 온라인 HTML 버전의 인프라를 구축했습니다. 여기에는 우리가 책을 지속적으로 업데이트하고 몇 분 내에 온라인에서 변경 사항을 볼 수 있게 해주는 지속적 통합 프레임워크와, 책의 인쇄 버전 및 HTML 버전의 초기 연도에 사용된 R 패키지 msmbstyle이 포함됩니다. Helena Lucia Crowell이 책의 소스를 Sweave에서 quarto로 포팅하는 훌륭한 작업을 해준 것에 감사드립니다. Sviatoslav Kharuk는 추가 개선을 했습니다. 그들의 도움과 대응에 매우 감사드립니다.\n인쇄판과 온라인 버전 모두의 표지 디자인을 해준 Lorraine Garchery에게 감사드립니다.\n책을 문법적으로 정확하고, 미적으로 매력적이며, 교육적으로 일관되게 만드는 데 지속적으로 노력해 준 Cambridge University Press의 David Tranah와 Diana Gillooly에게 감사드립니다. 개선의 여지는 여전히 많이 남아 있으며, 그 책임은 우리에게 있습니다.\n우리를 격려하고 예비 장에 대한 피드백을 제공해 준 가족과 후원자들에게 감사드립니다: David Relman, Alfred Spormann, Catherine Blish, Don Knuth, Persi Diaconis, Gretchen과 Barry Mazur, ….\nSusan Holmes, Stanford\nWolfgang Huber, Heidelberg\n다음 독자들은 오타와 명확화 기회를 지적하여 온라인 버전 개선에 기여했습니다: Eva-Maria Geissen, Nick Cox, Constantin Ahlmann-Eltze, Tümay Capraz, Irilenia Nobeli, Asger Hobolth, Iulian Ichim, …\n페이지는 R 버전 4.5.1 (2025-06-13)을 사용하여 2025-09-01 01:33에 빌드되었습니다.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>헌사 (Dedication)</span>"
    ]
  },
  {
    "objectID": "16-chap.html",
    "href": "16-chap.html",
    "title": "18  참고문헌 (References)",
    "section": "",
    "text": "1000 Genomes Project Consortium. 2012. “An Integrated Map of Genetic Variation from 1,092 Human Genomes.” Nature 491 (7422): 56–65.\nAbbott, Edwin A. 1884. Flatland: A Romance of Many Dimensions. OUP Oxford.\nAgresti, Alan. 2007. An Introduction to Categorical Data Analysis. John Wiley.\nAltman, Naomi, and Martin Krzywinski. 2017. “Points of Significance: Interpreting p Values.” Nature Methods 14 (3): 213–14. https://doi.org/10.1038/nmeth.4210.\nAmbroise, Christophe, and Geoffrey J. McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” PNAS 99 (10): 6562–66.\nAnders, Simon, and Wolfgang Huber. 2010. “Differential Expression Analysis for Sequence Count Data.” Genome Biology 11: R106. http://genomebiology.com/2010/11/10/R106.\nAnders, Simon, Alejandro Reyes, and Wolfgang Huber. 2012. “Detecting differential usage of exons from RNA-Seq data.” Genome Research 22 (10): 2008–17.\nAnscombe, Francis J. 1948. “The Transformation of Poisson, Binomial and Negative-Binomial Data.” Biometrika , 246–54.\nAure, Miriam Ragle, Valeria Vitelli, Sandra Jernström, Surendra Kumar, Marit Krohn, Eldri U Due, Tonje Husby Haukaas, et al. 2017. “Integrative Clustering Reveals a Novel Split in the Luminal A Subtype of Breast Cancer with Impact on Outcome.” Breast Cancer Research 19 (1): 44.\nBacher, Rhonda, and Christina Kendziorski. 2016. “Design and Computational Analysis of Single-Cell RNA-Sequencing Experiments.” Genome Biology 17 (1): 1.\nBaddeley, Adrain, Jesper Moller, and Rasmus Waagepetersen. 2000. “Non- and Semiparametric Estimation of Interaction in Inhomogeneous Point Patterns.” Statistica Neerlandica 54: 329–50.\nBaddeley, Adrian J. 1998. “Spatial Sampling and Censoring.” In Stochastic Geometry: Likelihood and Computation , edited by O. E. Barndorff-Nielsen, W. S. Kendall, and M. N. M. van Lieshout, 37–78. Chapman; Hall.\nBeisser, Daniela, Gunnar W Klau, Thomas Dandekar, Tobias Müller, and Marcus T Dittrich. 2010. “BioNet: An R-Package for the Functional Analysis of Biological Networks.” Bioinformatics 26 (8): 1129–30.\nBelkin, Mikhail, and Partha Niyogi. 2003. “Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.” Neural Computation 15 (6): 1373–96.\nBellman, Richard Ernest. 1961. Adaptive Control Processes: A Guided Tour. Princeton University Press.\nBendall, Sean C, Garry P Nolan, Mario Roederer, and Pratip K Chattopadhyay. 2012. “A Deep Profiler’s Guide to Cytometry.” Trends in Immunology 33 (7): 323–32.\nBengio, Yoshua, Jean-François Paiement, Pascal Vincent, Olivier Delalleau, Nicolas Le Roux, and Marie Ouimet. 2004. “Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering.” Advances in Neural Information Processing Systems 16: 177–84.\nBenjamini, Yoav, and Marina Bogomolov. 2014. “Selective Inference on Multiple Families of Hypotheses.” Journal of the Royal Statistical Society: Series B 76 (1): 297–318.\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society B 57: 289–300.\nBenjamini, Yoav, and Daniel Yekutieli. 2003. “Hierarchical FDR Testing of Trees of Hypotheses.” Technical report, Department of Statistics; Operations Research, Tel Aviv University.\nBerg, Stuart, Dominik Kutra, Thorben Kroeger, Christoph N Straehle, Bernhard X Kausler, Carsten Haubold, Martin Schiegg, et al. 2019. “Ilastik: Interactive Machine Learning for (Bio)image Analysis.” Nature Methods 16 (12): 1226–32.\nBhattacharya, Bhaswar B. 2015. “Power of Graph-Based Two-Sample Tests.” arXiv Preprint arXiv:1508.07530.\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\nBoland, Michael V., and Robert F. Murphy. 2001. “A neural network classifier capable of recognizing the patterns of all major subcellular structures in fluorescence microscope images of HeLa cells.” Bioinformatics 17 (12): 1213–23.\nBouckaert, Remco, Joseph Heled, Denise Kühnert, Tim Vaughan, Chieh-Hsi Wu, Dong Xie, Marc A Suchard, Andrew Rambaut, and Alexei J Drummond. 2014. “BEAST 2: A Software Platform for Bayesian Evolutionary Analysis.” PLoS Computational Biology 10 (4): e1003537.\nBox, George EP, Norman Richard Draper, et al. 1987. Empirical Model-Building and Response Surfaces. Vol. 424. Wiley New York.\nBox, George EP, William G Hunter, and J Stuart Hunter. 1978. Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building. John Wiley & Sons.\nBraak, Cajo ter. 1985. “Correspondence Analysis of Incidence and Abundance Data: Properties in Terms of a Unimodal Respose.” Biometrics 41 (January).\nBrodie, Eoin L, Todd Z DeSantis, Dominique C Joyner, Seung M Baek, Joern T Larsen, Gary L Andersen, Terry C Hazen, et al. 2006. “Application of a High- Density Oligonucleotide Microarray Approach to Study Bacterial Population Dynamics During Uranium Reduction and Reoxidation.” Applied and Environmental Microbiology 72 (9): 6288–98.\nBronštein, Il’ja N., and Konstantin A Semendjajew. 1979. Taschenbuch Der Mathematik. B.G. Teubner Verlagsgesellschaft, Leipzig; Verlag Nauka, Moscow.\nBrooks, Angela N, Li Yang, Michael O Duff, Kasper D Hansen, Jung W Park, Sandrine Dudoit, Steven E Brenner, and Brenton R Graveley. 2011. “Conservation of an RNA Regulatory Map Between Drosophila and Mammals.” Genome Research , 193–202. https://doi.org/10.1101/gr.108662.110.\nBulmer, Michael George. 2003. Francis Galton: Pioneer of Heredity and Biometry. JHU Press.\nCallahan, Ben J, Kris Sankaran, Julia A Fukuyama, Paul J McMurdie, and Susan P Holmes. 2016. “Bioconductor Workflow for Microbiome Data Analysis: From Raw Reads to Community Analyses.” F1000Research 5.\nCallahan, Benjamin J, Paul J McMurdie, and Susan P Holmes. 2017. “Exact Sequence Variants Should Replace Operational Taxonomic Units in Marker Gene Data Analysis.” ISME Journal , 1–5.\nCallahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy J Johnson, and Susan P Holmes. 2016. “DADA2: High Resolution Sample Inference from Amplicon Data.” Nature Methods , 1–4.\nCannings, Chris, and Anthony WF Edwards. 1968. “Natural Selection and the de Finetti Diagram.” Annals of Human Genetics 31 (4): 421–28.\nCaporaso, J. G., J. Kuczynski, J. Stombaugh, K. Bittinger, F. D. Bushman, E. K. Costello, N. Fierer, et al. 2010. “QIIME Allows Analysis of High-Throughput Community Sequencing Data.” Nature Methods 7 (5): 335–36.\nCarpenter, Anne E, Thouis R Jones, Michael R Lamprecht, Colin Clarke, In Han Kang, Ola Friman, David A Guertin, Joo Han Chang, Robert A Lindquist, and Jason Moffat. 2006. “CellProfiler: Image Analysis Software for Identifying and Quantifying Cell Phenotypes.” Genome Biology 7: R100.\nCarr, Daniel B, Richard J Littlefield, WL Nicholson, and JS Littlefield. 1987. “Scatterplot Matrix Techniques for Large N.” Journal of the American Statistical Association 82 (398): 424–36.\nChakerian, John, and Susan Holmes. 2012. “Computational Tools for Evaluating Phylogenetic and Hierarchical Clustering Trees.” Journal of Computational and Graphical Statistics 21 (3): 581–99.\nChaumont, Fabrice de, Stéphane Dallongeville, Nicolas Chenouard, Nicolas Hervé, Sorin Pop, Thomas Provoost, Vannary Meas-Yedid, et al. 2012. “Icy: an open bioimage informatics platform for extended reproducible research.” Nature Methods 9: 690–96.\nChen, Min, Yang Xie, and Michael Story. 2011. “An Exponential-Gamma Convolution Model for Background Correction of Illumina BeadArray Data.” Communications in Statistics-Theory and Methods 40 (17): 3055–69.\nChessel, Daniel, Anne Dufour, and Jean Thioulouse. 2004. “The ade4 Package - i: One-Table Methods.” R News 4 (1): 5–10. http://CRAN.R-project.org/doc/Rnews/.\nChiu, Sung Nok, Dietrich Stoyan, Wilfrid S. Kendall, and Joseph Mecke. 2013. Stochastic Geometry and Its Applications. Springer.\nClemmensen, Line, Trevor Hastie, Daniela Witten, and Bjarne Ersbøll. 2011. “Sparse Discriminant Analysis.” Technometrics 53: 406–13.\nCleveland, William S. 1988. The Collected Works of John w. Tukey: Graphics 1965-1985. Vol. 5. CRC Press.\nCleveland, William S., Marylyn E. McGill, and Robert McGill. 1988. “The Shape Parameter of a Two-Variable Graph.” Journal of the American Statistical Association 83: 289–300.\nCole, J. R., Q. Wang, E. Cardenas, J. Fish, B. Chai, R. J. Farris, A. S. Kulam-Syed-Mohideen, et al. 2009. “The Ribosomal Database Project: Improved Alignments and New Tools for rRNA Analysis.” Nucleic Acids Research 37 (Supplement 1): D141–45.\nCook, R. Dennis. 1977. “Detection of Influential Observation in Linear Regression.” Technometrics.\nCressie, Noel A. 1991. Statistics for Spatial Data. John Wiley; Sons.\nDiaconis, Persi, and David Freedman. 1980. “Finite Exchangeable Sequences.” The Annals of Probability , 745–64.\nDiaconis, Persi, Sharad Goel, and Susan Holmes. 2008. “Horseshoes in Multidimensional Scaling and Kernel Methods.” Annals of Applied Statistics 2: 777. https://doi.org/DOI:10.1214/08-AOAS165.\nDiaconis, Persi, and Susan Holmes. 1994. “Gray Codes for Randomization Procedures.” Statistics and Computing 4 (4): 287–302.\nDiaconis, Persi, Susan Holmes, and Richard Montgomery. 2007. “Dynamical Bias in the Coin Toss.” SIAM Review 49 (2): 211–35.\nDiday, Edwin, and M Paula Brito. 1989. “Symbolic Cluster Analysis.” In Conceptual and Numerical Analysis of Data , 45–84. Springer.\nDiggle, Peter J. 2013. Statistical Analysis of Spatial and Spatio-Temporal Point Patterns. Chapman; Hall/CRCs.\nDiGiulio, Daniel B., Benjamin J. Callahan, Paul J. McMurdie, Elizabeth K. Costello, Deirdre J. Lyelle, Anna Robaczewska, Christine L. Sun, et al. 2015. “Temporal and Spatial Variation of the Human Microbiota During Pregnancy.” PNAS.\nDundar, Murat, Ferit Akova, Halid Z. Yerebakan, and Bartek Rajwa. 2014. “A Non-Parametric Bayesian Model for Joint Cell Clustering and Cluster Matching: Identification of Anomalous Sample Phenotypes with Random Effects.” BMC Bioinformatics 15 (1): 1–15. https://doi.org/10.1186/1471-2105-15-314.\nDurbin, Richard, Sean Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis. Cambridge University Press.\nEfron, Bradley. 2010. Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction. Cambridge University Press.\nEfron, Bradley, and Robert J Tibshirani. 1994. An Introduction to the Bootstrap. CRC press.\nEfron, B., and R. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman & Hall/CRC.\nEkman, Gosta. 1954. “Dimensions of Color Vision.” The Journal of Psychology 38 (2): 467–74.\nElson, D, and E Chargaff. 1952. “On the Desoxyribonucleic Acid Content of Sea Urchin Gametes.” Experientia 8 (4): 143–45.\nFelsenstein, Joseph. 2004. Inferring Phylogenies. Boston: Sinauer.\nFinetti, Bruno de. 1926. “Considerazioni Matematiche Sull’ereditarieta Mendeliana.” Metron 6: 3–41.\nFisher, Ronald Aylmer. 1935. The Design of Experiments. Oliver & Boyd.\nFlury, Bernard. 1997. A First Course in Multivariate Statistics. Springer.\nFreedman, David A. 1991. “Statistical Models and Shoe Leather.” Sociological Methodology 21 (2): 291–313.\nFreedman, David, Robert Pisani, and Roger Purves. 1997. Statistics. New York, NY: WW Norton.\nFriedman, Jerome H. 1997. “On Bias, Variance, 0/1—Loss, and the Curse-of- Dimensionality.” Data Mining and Knowledge Discovery 1: 55–77.\nFriedman, Jerome H, and Lawrence C Rafsky. 1979. “Multivariate Generalizations of the Wald-Wolfowitz and Smirnov Two-Sample Tests.” The Annals of Statistics , 697–717.\nFukuyama, Julia, Paul J McMurdie, Les Dethlefsen, David A Relman, and Susan Holmes. 2012. “Comparisons of Distance Methods for Combining Covariates and Abundances in Microbiome Studies.” In Pac Symp Biocomput. World Scientific.\nGentleman, Robert C, Vincent J Carey, Douglas M Bates, Ben Bolstad, Marcel Dettling, Sandrine Dudoit, Byron Ellis, et al. 2004. “Bioconductor: Open Software Development for Computational Biology and Bioinformatics.” Genome Biology 5 (10): R80. https://doi.org/10.1186/gb-2004-5-10-r80.\nGlass, David J. 2007. Experimental Design for Biologists. Cold Spring Harbor Laboratory Press.\nGoslee, Sarah C, Dean L Urban, et al. 2007. “The Ecodist Package for Dissimilarity-Based Analysis of Ecological Data.” Journal of Statistical Software 22 (7): 1–19.\nGrantham, Richard, Christian Gautier, Manolo Gouy, M Jacobzone, and R Mercier. 1981. “Codon Catalog Usage Is a Genome Strategy Modulated for Gene Expressivity.” Nucleic Acids Research 9 (1): 213–13.\nGreenacre, Michael J. 2007. Correspondence Analysis in Practice. Chapman & Hall.\nGrolemund, Garrett, and Hadley Wickham. 2017. R for Data Science. O’Reilly.\nGrün, Bettina, Theresa Scharl, and Friedrich Leisch. 2012. “Modelling Time Course Gene Expression Data with Finite Mixtures of Linear Additive Models.” Bioinformatics 28 (2): 222–28. https://doi.org/10.1093/bioinformatics/btr653.\nGuillot, Gilles, and François Rousset. 2013. “Dismantling the Mantel Tests.” Methods in Ecology and Evolution 4 (4): 336–44.\nHallett, Robin M, Anna Dvorkin-Gheva, Anita Bane, and John A Hassell. 2012. “A Gene Signature for Predicting Outcome in Patients with Basal-Like Breast Cancer.” Scientific Reports 2.\nHastie, Trevor, and Werner Stuetzle. 1989. “Principal Curves.” Journal of the American Statistical Association 84 (406): 502–16.\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2008. The Elements of Statistical Learning. 2^{} ed. Springer.\nHead, Megan L, Luke Holman, Rob Lanfear, Andrew T Kahn, and Michael D Jennions. 2015. “The Extent and Consequences of p-Hacking in Science.” PLoS Biology 13 (3): e1002106.\nHeld, M., M. H. A. Schmitz, B. Fischer, T. Walter, B. Neumann, M. H. Olma, M. Peter, J. Ellenberg, and D. W. Gerlich. 2010. “CellCognition: Time-Resolved Phenotype Annotation in High-Throughput Live Cell Imaging.” Nature Methods 7: 747.\nHelmholtz, H. von. 1867. Handbuch Der Physiologischen Optik. Leipzig: Leopold Voss.\nHenderson, Fergus. 2017. “Software Engineering at Google.” ArXiv e-Prints. https://arxiv.org/abs/1702.01715.\nHoeting, Jennifer A, David Madigan, Adrian E Raftery, and Chris T Volinsky. 1999. “Bayesian Model Averaging: A Tutorial.” Statistical Science , 382–401.\nHolmes - Junca, Susan. 1985. “Outils Informatiques Pour l’évaluation de La Pertinence d’un résultat En Analyse Des Données.” PhD thesis, Université Montpellier II, France.\nHolmes, Susan. 1999. “Phylogenetic Trees: An Overview.” In Statistics and Genetics , 81–118. IMA 112. New York: Springer.\n———. 2003a. “Bootstrapping Phylogenetic Trees: Theory and Methods.” Statistical Science 18 (2): 241–55.\n———. 2003b. “Statistics for phylogenetic trees.” Theoretical Population Biology 63 (1): 17–32.\n———. 2006. “Multivariate Analysis: The French way.” In Probability and Statistics: Essays in Honor of David a. Freedman , edited by D. Nolan and T. P. Speed. Vol. 56. IMS Lecture Notes–Monograph Series. Beachwood, OH: IMS. http://www.imstat.org/publications/lecnotes.htm.\n———. 2018. “Statistical Proof? The Problem of Irreproducibility.” Bulletin of the AMS 55 (1): 31–55.\nHolmes, Susan, Alexander V Alekseyenko, Alden Timme, Tyrrell Nelson, Pankaj Jay Pasricha, and Alfred Spormann. 2011. “Visualization and Statistical Comparisons of Microbial Communities Using r Packages on Phylochip Data.” In Pacific Symposium on Biocomputing , 142–53. World Scientific.\nHolmes, Susan, Michael He, Tong Xu, and Peter P Lee. 2005. “Memory t Cells Have Gene Expression Patterns Intermediate Between Naive and Effector.” PNAS 102 (15): 5519–23.\nHolmes, Susan, Adam Kapelner, and Peter P Lee. 2009. “An Interactive Java Statistical Image Segmentation System: GemIdent.” Journal of Statistical Software 30 (10).\nHornik, Kurt. 2005. “A CLUE for CLUster Ensembles.” Journal of Statistical Software 14 (12).\nHotelling, Harold. 1933. “Analysis of a Complex of Statistical Variables into Principal Components.” Journal of Educational Psychology 24 (6): 417–41.\n———. 1944. “Some Improvements in Weighing and Other Experimental Techniques.” The Annals of Mathematical Statistics 15 (3): 297–306.\nHuber, Peter J. 1964. “Robust Estimation of a Location Parameter.” The Annals of Mathematical Statistics 35: 73–101.\nHuber, Wolfgang, Vincent J Carey, Robert Gentleman, Simon Anders, Marc Carlson, Benilton S Carvalho, Hector Corrada Bravo, et al. 2015. “Orchestrating High-Throughput Genomic Analysis with Bioconductor.” Nature Methods 12 (2): 115–21.\nHulett, Henry R, William A Bonner, Janet Barrett, and Leonard A Herzenberg. 1969. “Cell Sorting: Automated Separation of Mammalian Cells as a Function of Intracellular Fluorescence.” Science 166 (3906): 747–49.\nIdeker, Trey, Owen Ozier, Benno Schwikowski, and Andrew F Siegel. 2002. “Discovering Regulatory and Signalling Circuits in Molecular Interaction Networks.” Bioinformatics 18 Suppl 1 (January): S233–40. http://bioinformatics.oxfordjournals.org/cgi/reprint/18/suppl\\_1/S233.\nIgnatiadis, Nikolaos, and Wolfgang Huber. 2021. “Covariate Powered Cross- Weighted Multiple Testing.” Journal of the Royal Statistical Society: Series B 83: 720–51. https://doi.org/10.1111/rssb.12411.\nIgnatiadis, Nikolaos, Bernd Klaus, Judith Zaugg, and Wolfgang Huber. 2016. “Data-Driven Hypothesis Weighting Increases Detection Power in Genome-Scale Multiple Testing.” Nature Methods 13: 577–80.\nIhaka, Ross. 2003. “Color for Presentation Graphics.” In Proceedings of the 3rd International Workshop on Distributed Statistical Computing , edited by Kurt Hornik and Friedrich Leisch. Vienna, Austria: http://www.r-project.org/conferences/DSC-2003/Proceedings/; ISSN 1609-395X.\nIhaka, Ross, and Robert Gentleman. 1996. “R: A Language for Data Analysis and Graphics.” Journal of Computational and Graphical Statistics 5 (3): 299–314.\nIrizarry, R. A., B. Hobbs, F. Collin, Y. D. Beazer-Barclay, K. J. Antonellis, U. Scherf, and T. P. Speed. 2003. “Exploration, Normalization, and Summaries of High Density Oligonucleotide Array Probe Level Data.” Biostatistics 4 (2): 249–64.\nIrizarry, Rafael A, Hao Wu, and Andrew P Feinberg. 2009. “A Species- Generalized Probabilistic Model-Based Definition of CpG Islands.” Mammalian Genome 20 (9-10): 674–80.\nIzenman, Alan Julian. 2008. “Nonlinear Dimensionality Reduction and Manifold Learning.” In Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning , 597–632. New York, NY: Springer New York.\nJacob, Laurent, Guillaume Obozinski, and Jean-Philippe Vert. 2009. “Group Lasso with Overlap and Graph Lasso.” In Proceedings of the 26th Annual International Conference on Machine Learning , 433–40. ACM.\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Springer.\nJolicoeur, Pierre, and James E Mosimann. 1960. “Size and Shape Variation in the Painted Turtle. A Principal Component Analysis.” Growth 24: 339–54.\nJolliffe, Ian. 2002. Principal Component Analysis. Wiley Online Library.\nJones, T., A. Carpenter, and P. Golland. 2005. “Voronoi-Based Segmentation of Cells on Image Manifolds.” Computer Vision for Biomedical Image Applications , 535.\nJosse, Julie, and Susan Holmes. 2016. “Measuring Multivariate Association and Beyond.” Statistics Surveys 10: 132–67.\nKahneman, Daniel. 2011. Thinking, Fast and Slow. Macmillan.\nKashyap, Purna C, Angela Marcobal, Luke K Ursell, Samuel A Smits, Erica D Sonnenburg, Elizabeth K Costello, Steven K Higginbottom, et al. 2013. “Genetically Dictated Change in Host Mucus Carbohydrate Landscape Exerts a Diet-Dependent Effect on the Gut Microbiota.” PNAS 110 (42): 17059–64.\nKaufman, Leonard, and Peter J Rousseeuw. 2009. Finding Groups in Data: An Introduction to Cluster Analysis. Vol. 344. John Wiley & Sons.\nKendall, David. 1969. “Incidence Matrices, Interval Graphs and Seriation in Archeology.” Pacific Journal of Mathematics 28 (3): 565–70.\nKéry, Marc, and J Andrew Royle. 2015. Applied Hierarchical Modeling in Ecology: Analysis of Distribution, Abundance and Species Richness in r and BUGS: Volume 1: Prelude and Static Models. Academic Press.\nKorthauer, K., P. K. Kimes, C. Duvallet, A. Reyes, A. Subramanian, M. Teng, C. Shukla, E. J. Alm, and S. C. Hicks. 2019. “A practical guide to methods controlling false discoveries in computational biology.” Genome Biology 20 (1): 118.\nKozich, James J, Sarah L Westcott, Nielson T Baxter, Sarah K Highlander, and Patrick D Schloss. 2013. “Development of a Dual-Index Sequencing Strategy and Curation Pipeline for Analyzing Amplicon Sequence Data on the MiSeq Illumina Sequencing Platform.” Applied and Environmental Microbiology 79 (17): 5112–20.\nKristiansson, Erik, Michael Thorsen, Markus J Tamás, and Olle Nerman. 2009. “Evolutionary Forces Act on Promoter Length: Identification of Enriched Cis- Regulatory Elements.” Molecular Biology and Evolution 26 (6): 1299–1307.\nKuan, Pei Fen, Dongjun Chung, Guangjin Pan, James A Thomson, Ron Stewart, and Sündüz Keleş. 2011. “A Statistical Framework for the Analysis of ChIP-Seq Data.” Journal of the American Statistical Association 106 (495): 891–903.\nLai, Tze Leung. 2001. Sequential Analysis. Wiley Online Library.\nLange, Kenneth. 2016. MM Optimization Algorithms. SIAM.\nLaufer, Christina, Bernd Fischer, Maximilian Billmann, Wolfgang Huber, and Michael Boutros. 2013. “Mapping genetic interactions in human cancer cells with RNAi and multiparametric phenotyping.” Nature Methods 10: 427–31.\nLawrence, Michael S., Petar Stojanov, Paz Polak, Gregory V. Kryukov, Kristian Cibulskis, Andrey Sivachenko, Scott L. Carter, et al. 2013. “Mutational Heterogeneity in Cancer and the Search for New Cancer-Associated Genes.” Nature 499 (7457): 214–18. https://doi.org/10.1038/nature12213.\nLeek, Jeffrey T, Robert B Scharpf, Héctor Corrada Bravo, David Simcha, Benjamin Langmead, W Evan Johnson, Donald Geman, Keith Baggerly, and Rafael A Irizarry. 2010. “Tackling the Widespread and Critical Impact of Batch Effects in High-Throughput Data.” Nature Reviews Genetics 11 (10): 733–39.\nLeek, Jeffrey T., and John D. Storey. 2007. “Capturing heterogeneity in gene expression studies by surrogate variable analysis.” PLoS Genetics 3 (9): 1724–35.\nLi, Wen-Hsiung. 1997. Molecular Evolution. Sinauer Associates Incorporated.\nLi, Wen-Hsiung, and Dan Graur. 1991. Fundamentals of Molecular Evolution. Vol. 48. Sinauer Associates Sunderland, MA.\nLiberzon, Arthur, Aravind Subramanian, Reid Pinchback, Helga Thorvaldsdóttir, Pablo Tamayo, and Jill P Mesirov. 2011. “Molecular Signatures Database (MSigDB) 3.0.” Bioinformatics 27 (12): 1739–40.\nLove, Michael I., Simon Anders, Vladislav Kim, and Wolfgang Huber. 2015. “RNA- Seq Workflow: Gene-Level Exploratory Analysis and Differential Expression.” F1000Research 4 (1070). https://doi.org/10.12688/f1000research.7035.1.\nLove, Michael I, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation of Fold Change and Dispersion for RNA-seq Data with DESeq2.” Gnome Biology 15 (12): 1–21.\nMandal, Rakesh, Sophie St-Hilaire, John G Kie, and DeWayne Derryberry. 2009. “Spatial Trends of Breast and Prostate Cancers in the United States Between 2000 and 2005.” International Journal of Health Geographics 8 (1): 53.\nMardia, Kanti, John T Kent, and John M Bibby. 1979. Multiariate Analysis. New York: Academic Press.\nMarin, Jean-Michel, and Christian Robert. 2007. Bayesian Core: A Practical Approach to Computational Bayesian Statistics. Springer Science & Business Media.\nMcCormick Jr, William T, Paul J Schweitzer, and Thomas W White. 1972. “Problem Decomposition and Data Reorganization by a Clustering Technique.” Operations Research 20 (5): 993–1009.\nMcElreath, Richard. 2015. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Chapman; Hall/CRC.\nMcLachlan, Geoffrey, and Thriyambakam Krishnan. 2007. The EM Algorithm and Extensions. Vol. 382. John Wiley & Sons.\nMcLachlan, Geoffrey, and David Peel. 2004. Finite Mixture Models. John Wiley & Sons.\nMcMurdie, Paul J, and Susan Holmes. 2014. “Waste Not, Want Not: Why Rarefying Microbiome Data Is Inadmissible.” PLoS Computational Biology 10 (4): e1003531.\n———. 2015. “Shiny-Phyloseq: Web Application for Interactive Microbiome Analysis with Provenance Tracking.” Bioinformatics 31 (2): 282–83.\nMead, Roger. 1990. The Design of Experiments: Statistical Principles for Practical Applications. Cambridge University Press.\nMoignard, Victoria, Steven Woodhouse, Laleh Haghverdi, Andrew J Lilly, Yosuke Tanaka, Adam C Wilkinson, Florian Buettner, et al. 2015. “Decoding the Regulatory Network of Early Blood Development from Single-Cell Gene Expression Measurements.” Nature Biotechnology.\nMollon, John. 1995. “Seeing Colour.” In Colour: Art and Science , edited by T. Lamb and J. Bourriau. Cambridge Unversity Press.\nMood, Alexander M. 1946. “On Hotelling’s Weighing Problem.” The Annals of Mathematical Statistics , 432–46.\nMossel, Elchanan. 2003. “On the Impossibility of Reconstructing Ancestral Data and Phylogenies.” Journal of Computational Biology 10 (5): 669–76.\nMourant, AE, Ada Kopec, and K Domaniewska-Sobczak. 1976. “The Distribution of the Human Blood Groups 2nd Edition.” Oxford University Press London.\nMüllner, Daniel. 2013. “Fastcluster: Fast Hierarchical, Agglomerative Clustering Routines for r and Python.” Journal of Statistical Software 53 (9): 1–18.\nNacu, Serban, Rebecca Critchley-Thorne, Peter Lee, and Susan Holmes. 2007. “Gene Expression Network Analysis and Applications to Immunology.” Bioinformatics 23 (7, 7): 850–58. https://doi.org/10.1093/bioinformatics/btm019.\nNelson, Tyrell A, Susan Holmes, Alexander Alekseyenko, Masha Shenoy, Todd DeSantis, Cindy Wu, Gary Andersen, et al. 2010. “PhyloChip Microarray Analysis Reveals Altered Gastrointestinal Microbial Communities in a Rat Model of Colonic Hypersensitivity.” Neurogastroenterology & Motility.\nNeumann, B., T. Walter, J. K. Heriche, J. Bulkescher, H. Erfle, C. Conrad, P. Rogers, et al. 2010. “Phenotypic profiling of the human genome by time-lapse microscopy reveals cell division genes.” Nature 464 (7289): 721–27.\nNeyman, Jerzy, and Egon S Pearson. 1936. Sufficient Statistics and Uniformly Most Powerful Tests of Statistical Hypotheses. University California Press.\nNolan, Daniel J, Michael Ginsberg, Edo Israely, Brisa Palikuqi, Michael G Poulos, Daylon James, Bi-Sen Ding, et al. 2013. “Molecular Signatures of Tissue-Specific Microvascular Endothelial Cell Heterogeneity in Organ Maintenance and Regeneration.” Developmental Cell 26 (2): 204–19.\nO’Neill, Kieran, Nima Aghaeepour, Josef Špidlen, and Ryan Brinkman. 2013. “Flow Cytometry Bioinformatics.” PLoS Computational Biology 9 (12): e1003365.\nOhnishi, Y., W. Huber, A. Tsumura, M. Kang, P. Xenopoulos, K. Kurimoto, A. K. Oles, et al. 2014. “Cell-to-Cell Expression Variability Followed by Signal Reinforcement Progressively Segregates Early Mouse Lineages.” Nature Cell Biology 16 (1): 27–37.\nOzsolak, Fatih, and Patrice M Milos. 2011. “RNA sequencing: advances, challenges and opportunities.” Nature Reviews Genetics 12: 87–98.\nPagès, Jérôme. 2016. Multiple Factor Analysis by Example Using R. CRC Press.\nParadis, Emmanuel. 2011. Analysis of Phylogenetics and Evolution with r. Springer Science & Business Media.\nPau, Grégoire, Florian Fuchs, Oleg Sklyar, Michael Boutros, and Wolfgang Huber. 2010. “EBImage R Package for Image Processing with Applications to Cellular Phenotypes.” Bioinformatics 26 (7): 979–81.\nPearson, Karl. 1901. “LIII. On Lines and Planes of Closest Fit to Systems of Points in Space.” The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 2 (11): 559–72.\nPerraudeau, Fanny, Davide Risso, Kelly Street, Elizabeth Purdom, and Sandrine Dudoit. 2017. “Bioconductor Workflow for Single-Cell RNA Sequencing: Normalization, Dimensionality Reduction, Clustering, and Lineage Inference.” F1000Research 6.\nPerrière, Guy, and Jean Thioulouse. 2002. “Use and Misuse of Correspondence Analysis in Codon Usage Studies.” Nucleic Acids Research 30 (20): 4548–55.\nPounds, Stan, and Stephan W Morris. 2003. “Estimating the Occurrence of False Positives and False Negatives in Microarray Studies by Approximating and Partitioning the Empirical Distribution of p-Values.” Bioinformatics 19 (10): 1236–42.\nPrentice, IC. 1977. “Non-Metric Ordination Methods in Ecology.” The Journal of Ecology , 85–94.\nPurdom, Elizabeth. 2010. “Analysis of a Data Matrix and a Graph: Metagenomic Data and the Phylogenetic Tree.” Annals of Applied Statistics , July.\nPurdom, Elizabeth, and Susan P Holmes. 2005. “Error Distribution for Gene Expression Data.” Statistical Applications in Genetics and Molecular Biology 4 (1).\nRajaram, S., B. Pavie, L. F. Wu, and S. J. Altschuler. 2012. “PhenoRipper: software for rapidly profiling microscopy images.” Nature Methods 9: 635–37.\nReaven, GM, and RG Miller. 1979. “An Attempt to Define the Nature of Chemical Diabetes Using a Multidimensional Analysis.” Diabetologia 16 (1): 17–24.\nReyes, Alejandro, Simon Anders, Robert J. Weatheritt, Toby J. Gibson, Lars M. Steinmetz, and Wolfgang Huber. 2013. “Drift and Conservation of Differential Exon Usage Across Tissues in Primate Species.” Proceedings of the National Academy of Sciences 110 (38): 15377–82. https://doi.org/10.1073/pnas.1307202110.\nReyes, Alejandro, and Wolfgang Huber. 2017. “Alternative Start and Termination Sites of Transcription Drive Most Transcript Isoform Differences Across Human Tissues.” Nucleic Acids Research 46 (2): 582–92. https://doi.org/10.1093/nar/gkx1165.\nRhee, Soo-Yon, Matthew J Gonzales, Rami Kantor, Bradley J Betts, Jaideep Ravela, and Robert W Shafer. 2003. “Human Immunodeficiency Virus Reverse Transcriptase and Protease Sequence Database.” Nucleic Acids Research 31 (1): 298–303.\nRice, John. 2006. Mathematical Statistics and Data Analysis. Cengage Learning.\nRipley, B. D. 1988. Statistical Inference for Spatial Processes. Cambridge University Press.\nRobert, Christian, and George Casella. 2009. Introducing Monte Carlo Methods with R. Springer Science & Business Media.\nRobins, Garry, Tom Snijders, Peng Wang, Mark Handcock, and Philippa Pattison. 2007. “Recent Developments in Exponential Random Graph (p*) Models for Social Networks.” Social Networks 29 (2): 192–215.\nRobinson, M. D., D. J. McCarthy, and G. K. Smyth. 2009. “edgeR: A Bioconductor Package for Differential Expression Analysis of Digital Gene Expression Data.” Bioinformatics 26 (1): 139–40. https://doi.org/10.1093/bioinformatics/btp616.\nRocke, David M, and Blythe Durbin. 2001. “A Model for Measurement Error for Gene Expression Arrays.” Journal of Computational Biology 8 (6): 557–69.\nRonquist, Fredrik, Maxim Teslenko, Paul van der Mark, Daniel L Ayres, Aaron Darling, Sebastian Höhna, Bret Larget, Liang Liu, Marc A Suchard, and John P Huelsenbeck. 2012. “MrBayes 3.2: Efficient Bayesian Phylogenetic Inference and Model Choice Across a Large Model Space.” Systematic Biology 61 (3): 539–42.\nRosen, Michael J, Benjamin J Callahan, Daniel S Fisher, and Susan P Holmes. 2012. “Denoising PCR-Amplified Metagenome Data.” BMC Bioinformatics 13 (1): 283.\nRousseeuw, Peter J. 1987. “Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis.” Journal of Computational and Applied Mathematics 20: 53–65.\nRousseeuw, Peter J., and Annick M. Leroy. 1987. Robust Regression and Outlier Detection. Wiley. https://doi.org/10.1002/0471725382.\nRoweis, Sam T, and Lawrence K Saul. 2000. “Nonlinear Dimensionality Reduction by Locally Linear Embedding.” Science 290 (5500): 2323–26.\nRuss, John C., and F. Brent Neal. 2015. The Image Processing Handbook. 7th ed. CRC Press;\nSankaran, Kris, and Susan Holmes. 2014. “structSSI: Simultaneous and Selective Inference for Grouped or Hierarchically Structured Data.” Journal of Statistical Software 59 (1): 1–21.\nSchilling, Mark F. 1986. “Multivariate Two-Sample Tests Based on Nearest Neighbors.” Journal of the American Statistical Association 81 (395): 799–806.\nSchindelin, Johannes, Ignacio Arganda-Carreras, Erwin Frise, Verena Kaynig, Mark Longair, Tobias Pietzsch, Stephan Preibisch, et al. 2012. “Fiji: an open- source platform for biological-image analysis.” Nature Methods 9: 676–82.\nSchloss, P D, S L Westcott, T Ryabin, J R Hall, M Hartmann, E B Hollister, R A Lesniewski, et al. 2009. “Introducing mothur: Open-Source, Platform- Independent, Community-Supported Software for Describing and Comparing Microbial Communities.” Applied and Environmental Microbiology 75 (23): 7537–41.\nSchloss, P. D., A. M. Schuber, J. P. Zackular, K. D. Iverson, Young V. B., and Petrosino J. F. 2012. “Stabilization of the Murine Gut Microbiome Following Weaning.” Gut Microbes 3 (4): 383–93.\nSchölkopf, Bernhard, Koji Tsuda, and Jean-Philippe Vert. 2004. Kernel Methods in Computational Biology. MIT press.\nSchweder, T., and E. Spjøtvoll. 1982. “Plots of P-values to Evaluate Many Tests Simultaneously.” Biometrika 69: 493–502. https://doi.org/10.1093/biomet/69.3.493.\nSenn, Stephen. 2004. “Controversies Concerning Randomization and Additivity in Clinical Trials.” Statistics in Medicine 23: 3729–53.\nSerra, Jean. 1983. Image Analysis and Mathematical Morphology. Academic Press.\nSetiadi, A Francesca, Nelson C Ray, Holbrook E Kohrt, Adam Kapelner, Valeria Carcamo-Cavazos, Edina B Levic, Sina Yadegarynia, et al. 2010. “Quantitative, Architectural Analysis of Immune Cell Subsets in Tumor-Draining Lymph Nodes from Breast Cancer Patients and Healthy Lymph Nodes.” PLoS One 5 (8): e12420.\nShalizi, Cosma. 2017. Advanced Data Analysis from an Elementary Point of View. Cambridge University Press. https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf.\nSlonim, Noam, Gurinder Singh Atwal, Gašper Tkačik, and William Bialek. 2005. “Information-Based Clustering.” PNAS 102 (51): 18297–302.\nStegle, O., L. Parts, R. Durbin, and J. Winn. 2010. “A Bayesian framework to account for complex non-genetic factors in gene expression levels greatly increases power in eQTL studies.” PLoS Computational Biology 6 (5): e1000770.\nSteijger, T., J. F. Abril, P. G. Engstrom, F. Kokocinski, T. J. Hubbard, R. Guigo, J. Harrow, et al. 2013. “Assessment of transcript reconstruction methods for RNA-seq.” Nature Methods 10 (12): 1177–84.\nStigler, Stephen M. 2016. The Seven Pillars of Statistical Wisdom. Harvard University Press.\nStorey, John D. 2003. “The Positive False Discovery Rate: A Bayesian Interpretation and the q-Value.” The Annals of Statistics 31 (6). https://doi.org/10.1214/aos/1074290335.\nStrang, Gilbert. 2009. Introduction to Linear Algebra. Fourth. Wellesley- Cambridge Press.\nTenenbaum, Joshua B, Vin De Silva, and John C Langford. 2000. “A Global Geometric Framework for Nonlinear Dimensionality Reduction.” Science 290 (5500): 2319–23.\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society. Series B (Methodological) , 267–88.\nTibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. “Estimating the Number of Clusters in a Data Set via the Gap Statistic.” JRSSB 63 (2): 411–23.\nTrosset, Michael W, and Carey E Priebe. 2008. “The Out-of-Sample Problem for Classical Multidimensional Scaling.” Computational Statistics & Data Analysis 52 (10): 4635–42.\nTseng, George C, and Wing H Wong. 2005. “Tight Clustering: A Resampling-Based Approach for Identifying Stable and Tight Patterns in Data.” Biometrics 61 (1): 10–16.\nTukey, John W. 1977. “Exploratory Data Analysis.” Massachusetts: Addison- Wesley.\nTversky, Amos, and Daniel Kahneman. 1974. “Heuristics and Biases: Judgement Under Uncertainty.” Science 185: 1124–30.\n———. 1975. “Judgment Under Uncertainty: Heuristics and Biases.” In Utility, Probability, and Human Decision Making , 141–62. Springer.\nVerhulst, Pierre-François. 1845. “Recherches mathématiques Sur La Loi d’accroissement de La Population.” Nouveaux Mémoires de l’Académie Royale Des Sciences Et Belles-Lettres de Bruxelles 18: 1–42.\nVetterli, Martin, Jelena Kovačević, and Vivek Goyal. 2014. Foundations of Signal Processing. Cambridge University Press.\nWang, Q., G. M. Garrity, J. M. Tiedje, and J. R. Cole. 2007. “Naive Bayesian Classifier for Rapid Assignment of rRNA Sequences into the New Bacterial Taxonomy.” Applied and Environmental Microbiology 73 (16): 5261.\nWasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA’s Statement on p-Values: Context, Process, and Purpose.” The American Statistician.\nWertheim, Joel O, and Michael Worobey. 2009. “Dating the Age of the SIV Lineages That Gave Rise to HIV-1 and HIV-2.” PLoS Computational Biology 5 (5): e1000377.\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28.\n———. 2014. “Tidy Data.” Journal of Statistical Software 59 (10).\n———. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer New York. http://had.co.nz/ggplot2/book.\nWiel, Mark A, Tonje G Lien, Wina Verlaat, Wessel N Wieringen, and Saskia M Wilting. 2016. “Better Prediction by Use of Co-Data: Adaptive Group- Regularized Ridge Regression.” Statistics in Medicine 35 (3): 368–81.\nWilkinson, Leland. 1999. “Dot Plots.” The American Statistician 53 (3): 276.\n———. 2005. The Grammar of Graphics. Springer.\nWills, Quin F, Kenneth J Livak, Alex J Tipping, Tariq Enver, Andrew J Goldson, Darren W Sexton, and Chris Holmes. 2013. “Single-Cell Gene Expression Analysis Reveals Genetic Associations Masked in Whole-Tissue Experiments.” Nature Biotechnology 31 (8): 748–52.\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” Edited by Francis Ouellette. PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510.\nWitten, Daniela M, and Robert Tibshirani. 2011. “Penalized Classification Using Fisher’s Linear Discriminant.” JRSSB 73 (5): 753–72.\nWitten, Daniela M, Robert Tibshirani, and Trevor Hastie. 2009. “A Penalized Matrix Decomposition, with Applications to Sparse Principal Components and Canonical Correlation Analysis.” Biostatistics , kxp008.\nWright, Erik S. 2015. “DECIPHER: Harnessing Local Sequence Context to Improve Protein Multiple Sequence Alignment.” BMC Bioinformatics 16 (1): 1.\nWu, CF Jeff, and Michael S Hamada. 2011. Experiments: Planning, Analysis, and Optimization. Vol. 552. John Wiley & Sons.\nYu, Hongxiang, Diana L Simons, Ilana Segall, Valeria Carcamo-Cavazos, Erich J Schwartz, Ning Yan, Neta S Zuckerman, et al. 2012. “PRC2/EED-EZH2 Complex Is up-Regulated in Breast Cancer Lymph Node Metastasis Compared to Primary Tumor and Correlates with Tumor Proliferation in Situ.” PloS One 7 (12): e51239.\nZeileis, Achim, Christian Kleiber, and Simon Jackman. 2008. “Regression Models for Count Data in R.” Journal of Statistical Software 27 (8). http://www.jstatsoft.org/v27/i08/.\nZeller, Georg, Julien Tap, Anita Y Voigt, Shinichi Sunagawa, Jens Roat Kultima, Paul I Costea, Aurélien Amiot, et al. 2014. “Potential of Fecal Microbiota for Early-Stage Detection of Colorectal Cancer.” Molecular Systems Biology 10 (11): 766. https://doi.org/10.15252/msb.20145645.\nZou, Hui, Trevor Hastie, and Robert Tibshirani. 2006. “Sparse Principal Component Analysis.” Journal of Computational and Graphical Statistics 15 (2): 265–86.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>참고문헌 (References)</span>"
    ]
  }
]