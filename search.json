[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modern Statistics for Modern Biology",
    "section": "",
    "text": "1 Home\nPlease navigate between and within the chapters of this book via the dropdown menu (“Chapters”) at the top.\nHow to cite this book: Modern Statistics for Modern Biology, by Susan Holmes and Wolfgang Huber, Cambridge University Press (2019). ISBN: 9781108705295.\nLicense: CC BY-NC-SA\nIntroduction\nGenerative Models for Discrete Data\nStatistical Modeling\nData visualization\nMixture Models\nClustering\nTesting\nMultivariate Analysis\nHigh-Throughput Count Data & Generalized Linear Models\nMultivariate methods for heterogeneous data\nNetworks and Trees\nImage data\nSupervised Learning\nDesign of High Throughput Experiments and their Analyses\nStatistical Concordance\nAcknowledgements\nReferences\nNo matching items",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#note-to-readers",
    "href": "index.html#note-to-readers",
    "title": "Modern Statistics for Modern Biology",
    "section": "1.1 Note to readers",
    "text": "1.1 Note to readers\nThe print version of the book (see below) was published in 2019, but we are continually updating the online version. So if you spot any inaccuracies, mistakes, typos, please let us know and we will do our best to improve the book accordingly.\nA particular issue is the code examples. R and its many packages on CRAN and Bioconductor are a highly dynamic environment. We finalized the print version in 2018 on R 3.5 and Bioconductor 3.7. We have been constantly updating the code to changes in R or the packages. What you see here has been built using R version 4.5.1 (2025-06-13) using the most recent release versions of all packages on 2025-09-01. It is possible that we have overlooked unintended changes whose detection is not easy to automate, e.g., in plot outputs or in results from certain computations. We think that this risk is a prize worth paying to enable readers to work in an up-to-date compute environment, but apologize to our readers for possible confusions.\nIf you spot anything, please alert us via email to wolfgang.huber [at] embl.org.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#code-and-data",
    "href": "index.html#code-and-data",
    "title": "Modern Statistics for Modern Biology",
    "section": "1.2 Code and data",
    "text": "1.2 Code and data\n\n1.2.1 Package Installation\nYou can install all the packages required to run every code example in the book via the following command:\nsource(\"https://www.huber.embl.de/msmb/install_packages.R\")__\n\n\n1.2.2 Data\n\nData files (zipped folder)\nlast modified: 2022-12-15 16:05:03\nmd5 hash: 4aefffbcd826d9645b9e0e5b12274f07 -\n\n\n\n1.2.3 Code\n\nR code (zipped folder)\n\n\n\n1.2.4 Download an electronic copy of the whole book\n\nHTML tree for the book (zipped)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "index.html#physical-copy",
    "href": "index.html#physical-copy",
    "title": "Modern Statistics for Modern Biology",
    "section": "1.3 Physical Copy",
    "text": "1.3 Physical Copy\nIf you would like a hard copy of Modern Statistics for Modern Biology, the book can be purchased from Cambridge University Press\n\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "00-chap.html",
    "href": "00-chap.html",
    "title": "2  The challenge: heterogeneity",
    "section": "",
    "text": "2.1 What’s in this book?\nThe two instances of modern in the title of this book reflect the two major recent revolutions in biological data analyses:\nThe aim of this book is to enable scientists working in biological research to quickly learn many of the important ideas and methods that they need to make the best of their experiments and of other available data. The book takes a hands-on approach. The narrative is driven by classes of questions, or by certain data types. Methods and theory are introduced on a need-to-know basis. We don’t try to systematically deduce from first principles. The book will often throw readers into the pool and hope they can swim in spite of so many missing details.\nBy no means this book will replace systematic training in underlying theory: probability, linear algebra, calculus, computer science, databases, multivariate statistics. Such training takes many semesters of coursework. Perhaps the book will whet your appetite to engage more deeply with one of these fields.\nAny biological system or organism is composed of tens of thousands of components, which can be in different states and interact in multiple ways. Modern biology aims to understand such systems by acquiring comprehensive –and this means high-dimensional– data in their temporal and spatial context, with multiple covariates and interactions. Dealing with this complexity will be our primary challenge. This includes real, biological complexity as well as the practical complexities and heterogeneities of the data we are able to acquire with our always imperfect instruments.\nBiological data come in all sorts of shapes: nucleic acid and protein sequences, rectangular tables of counts, multiple tables, continuous variables, batch factors, phenotypic images, spatial coordinates. Besides data measured in lab experiments, there are clinical data, environmental observations, measurements in space a d time, networks and lineage trees, and heaps of previously accumulated knowledge in biological databases as free text or controlled vocabularies, \\(…\\)\nIt is such heterogeneity that motivates our choice of R and Bioconductor as the computational platform for this book – some more on this below.\nFigure 1: The hypothesis testing paradigm recommended by R.A. Fisher starts with the formulation of a null hypothesis and the design of an experiment before the collection of any data. We could think in a similarly schematic way about model fitting: just replace Hypothesis H0 by Parametric Model and by Fit Parameters.\nFigure 1 outlines a sequential view of statistical data analysis. Motivated by the groundbreaking work on significance and hypothesis testing in the 1930s by Fisher (1935) and Neyman and Pearson (1936), it is well amenable to mathematical formalism, especially the part where we compute the distribution of test statistics under a hypothesis (null or alternative), or where we need to set up distributional assumptions and can search for analytical approximations.\nFigure 2: J.W. Tukey recommended starting any analysis with the data and wrote: “No catalogue of techniques can convey a willingness to look for what can be seen, whether or not anticipated.” (Holmes - Junca 1985).\nReal scientific discovery rarely works in the caricature manner of Figure 1. Tukey (1977) emphasized two separate approaches. The first he termed exploratory data analysis (EDA). EDA uses the data themselves to decide how to conduct the statistical analysis. EDA is built on data visualization, and then complemented by confirmatory data analyses (CDA): hypothesis-driven inferential methods, which ideally should be robust and not rely on complex assumptions. Tukey recommended an iterative approach schematized in Figure 2 that enable us to see the data at different resolutions and from different perspectives. This enables successive refinement of our understanding of the data and the underlying natural phenomena.\nBiology in the late 1990s raised the large-\\(p\\), small-\\(n\\) problem: consider a gene expression dataset for \\(n=200\\) patient-derived tissue samples on \\(p=20000\\) genes. If we want to construct a regression or classification model that “predicts” a clinical variable, for instance the disease type or outcome, from the \\(20000\\) genes, or features, we immediately run into problems, since the potential number of model parameters could be orders of magnitudes larger than the number of measurements. The problems derive from non-identifiability of the parameters or overfitting. At least, this is the case for common models, say, an ordinary multivariate linear model. Statisticians realized that they could remedy the situation by requiring sparsity through the use of regularization techniques (Hastie, Tibshirani, and Friedman 2008), i.e., by requiring that many of the potential parameters are either zero or at least close to it.\nA generalization of the sparsity principle is attained by invoking one of the most powerful recent ideas in high-dimensional statistics, which goes under the name of empirical Bayes : we don’t try to learn all the parameters from scratch, but rather use the fact that groups of them will be similar, or even the same. There are several important book long treatments (Efron 2010) of the subject of large scale inference so essential in modern estimation and hypotheses testing. In the 2010s, computer scientists also discovered ways of engineering “deep neural networks” that can provide sometimes amazing predictive qualities without worrying too much about parameter identifiability or uniqueness.\nWe’ll use the roulette icon to identify when we are using a Monte Carlo approximation method. This name for the method is used because it uses randomness, similar to the randomness of casino games. Ironically, for many casino games the probabilities of winning is not known analytically, and casinos use their own empirical data to evaluate the odds of winning.\nSimulations play an essential role in this book, as many of the results we need escape the reach of standard analytic approaches. In other words, simulations liberate us from being able to only consider methods that are tractable by “paper and pencil mathematics”, and from worrying about the appropriateness of simplifying assumptions or approximations.\nIn this book, we try to cover a wide spectrum of these developments and their applications to current biological research. We cover many different types of data that modern biologists have to deal with, including RNA-Seq, flow- cytometry, taxa abundances, imaging data and single cell measurements. We assume no prior formal training in statistics. However, you’ll need some familiarity with R and willingness to engage in mathematical and analytical thinking.\nWe will put notes and extra information under the devil icon, this is the devil who looks after the details.\nAs outlined in the following, the chapters in the book build upon each other, but they are reasonably self-contained, so they can also be studied separately. Each chapter starts with a motivations and goals section. Questions in the text will help you check whether you are following along. The text contains complete R code examples throughout. You don’t need to scrape R code from the HTML or manually copy it from the book. Please use the R files (extension .R) on this website. Each chapter concludes with a summary of the main points and a set of exercises.\nGenerative models are our basic building blocks. In order to draw conclusions about complicated data it tends to be useful to have simple models for the data generated under this or that situation. We do this through the use of probability theory and generative models, which we introduce in 1 Generative Models for Discrete Data. We will use examples from immunology and DNA analysis to describe useful generative models for biological data: binomial, multinomial and Poisson random variables.\nOnce we know how data would look like under a certain model, we can start working our way backwards: given some data, what model is most likely able to explain it? This bottom up approach is the core of statistical thinking, and we explain it in 2 Statistical Modeling.\nWe saw the primary role of graphics in Tukey’s scheme (Figure 2), and so we’ll learn how to visualize our data in 3 Data visualization. We’ll use the grammar of graphics and ggplot2.\nReal biological data often have more complex distributional properties than what we could cover in 1 Generative Models for Discrete Data. We’ll use mixtures , which we explore in 4 Mixture Models; these enable us to build realistic models for heterogeneous biological data and provide solid foundations for choosing appropriate variance stabilizing transformations.\nThe large, matrix-like datasets in biology naturally lend themselves to clustering : once we define a distance measure between matrix rows (the features), we can cluster and group the genes by similarity of their expression patterns, and similarly, for the columns (the patient samples). We’ll cover clustering in 5 Clustering. Since clustering only relies on distances, we can even apply it to data that are not matrix-shaped, as long as there are objects and distances defined between them.\nFurther following the path of EDA, we cover the most fundamental unsupervised analysis method for simple matrices – principal component analysis – in 7 Multivariate Analysis. We turn to more heterogeneous data that combine multiple data types in 9 Multivariate methods for heterogeneous data. There, we’ll see nonlinear unsupervised methods for counts from single cell data. We’ll also address how to use generalizations of the multivariate approaches covered in 7 Multivariate Analysis to combinations of categorical variables and multiple assays recorded on the same observational units.\nThe basic hypothesis testing workflow outlined in Figure 1 is explained in 6 Testing. We use the opportunity to apply it to one of the most common queries to \\(np\\)-datasets: which of the genes (features) are associated with a certain property of the samples, say, disease type or outcome? However, conventional significance thresholds would lead to lots of spurious associations: with a false positive rate of \\(\\) we expect \\(p\\) false positives if none of the \\(p=20000\\) features has a true association. Therefore we also need to deal with multiple testing.\nOne of the most fruitful ideas in statistics is that of variance decomposition, or analysis of variance (ANOVA). We’ll explore this, in the framework of linear models and generalized linear models, in 8 High- Throughput Count Data & Generalized Linear Models. Since we’ll draw our example data from an RNA-Seq experiment, this gives us also an opportunity to discuss models for such count data, and concepts of robustness.\nNothing in biology makes sense except in the light of evolution1, and evolutionary relationships are usefully encoded in phylogenetic trees. We’ll explore networks and trees in 10 Networks and Trees.\n1 Theodosius Dobzhansky, https://en.wikipedia.org/wiki/Nothing_in_Biology_Makes_Sense_Except_in_the_Light_of_Evolution\nA rich source of data in biology is images, and in 11 Image data we reinforce our willingness to do EDA on all sorts of heterogeneous data types by exploring feature extraction from images and spatial statistics.\nIn 12 Supervised Learning, we look at supervised learning: train an algorithm to distinguish between different classes of objects, given a multivariate set of features for each object. We’ll start simple with low- dimensional feature vectors and linear methods, and then step forward to some of the issues of classification in high-dimensional settings. Here, we focus on ‘’classical’’ supervised learning, where (at least conceptually) a training set of ground truth classifications is available to the algorithm all at once, simultaneously, to learn from. We do not (yet…) cover reinforcement learning, a more flexible framework in which one or several agents learn by interacting with a so-called environment by successively performing actions and getting feedback on them; so there are notions of time and state in that framework.\nWe wrap up in 13 Design of High Throughput Experiments and their Analyses with considerations on good practices in the design of experiments and of data analyses. For this we’ll use and reflect what we have learned in the course of the preceding chapters.\nFigure 3: Analyzing data is not a one step process. Each step involves visualizing and decomposing some of the complexity in the data. Tukey’s iterative data structuration can be conceptualized as \\(Total=V_1+V_2+V_3\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The challenge: heterogeneity</span>"
    ]
  },
  {
    "objectID": "00-chap.html#whats-in-this-book",
    "href": "00-chap.html#whats-in-this-book",
    "title": "2  The challenge: heterogeneity",
    "section": "",
    "text": "We will put notes and extra information under the devil icon, this is the devil who looks after the details.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The challenge: heterogeneity</span>"
    ]
  },
  {
    "objectID": "00-chap.html#computational-tools-for-modern-biologists",
    "href": "00-chap.html#computational-tools-for-modern-biologists",
    "title": "2  The challenge: heterogeneity",
    "section": "2.2 Computational tools for modern biologists",
    "text": "2.2 Computational tools for modern biologists\nAs we’ll see over and over again, the analysis approaches, tools and choices to be made are manifold. Our work can only be validated by keeping careful records in a reproducible script format. R and Bioconductor provide such a platform.\nAlthough we are tackling many different types of data, questions and statistical methods hands-on, we maintain a consistent computational approach by keeping all the computation under one roof: the R programming language and statistical environment, enhanced by the biological data infrastructure and specialized method packages from the Bioconductor project. The reader will have to start by acquiring some familiarity with R before using the book. There are many good books and online resources. One of them is by Grolemund and Wickham (2017), online at http://r4ds.had.co.nz.\nR code is a major component of this book. It is how we make the textual explanations explicit. Essentially every data visualization in the book is produced with code that is shown, and the reader should be able to replicate all of these figures, and any other result shown.\nNot all code is shown in the HTML pages, especially in cases where the code is lengthy and not directly conducive to the training aims. But all code is available in the .R files that we provide on this webpage.\nEven if you have a some familiarity with R, don’t worry if you don’t immediately understand every line of code in the book. Although we have tried to keep the code explicit and give tips and hints at potentially challenging places, there will be instances where\n\nthere is a function invoked that you have not seen before and that does something mysterious,\nthere is a complicated R expression that you don’t understand (perhaps involving apply-functions or data manipulations from the dplyr package).\n\nDon’t panic. For the mysterious function, have a look at its manual page. Open up RStudio and use the object explorer to look at the variables that go into the expression, and those that come out. Split up the expression to look at intermediate values.\nIn Chapters 1 Generative Models for Discrete Data and 2 Statistical Modeling, we use base R functionality for light doses of plotting and data manipulation. As we successively need more sophisticated operations, we introduce the ggplot2 way of making graphics in 3 Data visualization. Besides the powerful grammar of graphics concepts that enable us to produce sophisticated plots using only a limited set of instructions, this implies using the dplyr way of data manipulation.\n\n2.2.0.1 Why R and Bioconductor?\nThere are many reasons why we have chosen to present all analyses on the R (Ihaka and Gentleman 1996) and Bioconductor (Huber et al. 2015) platforms:\nDownload R and Rstudio to follow the code in the book.\nCutting edge solutions. The availability of over \\(10,000\\) packages ensures that almost all statistical methods are available, including the most recent developments. Moreover, there are implementations of or interfaces to many methods from computer science, mathematics, machine learning, data management, visualization and internet technologies. This puts thousands of person-years of work by experts at your finger tips.\nOpen source and community-owned. R and Bioconductor have been built collaboratively by a large community of developers. They are constantly tried and tested by thousands of users.\nData input and wrangling. Bioconductor packages support the reading of many of the data types and formats produced by measurement instruments used in modern biology, as well as the needed technology-specific “preprocessing” routines. The community is actively keeping these up-to-date with the rapid developments on the instrument market.\nSimulation. There are random number generators for every known statistical distribution and powerful numeric routines for linear algebra, optimization, etc.\nVisualization and presentation. R can make attractive, publication-quality graphics. We’ve dedicated 3 Data visualization to this and practice data visualization extensively throughout the book.\nEasy to use interactive development environment. RStudio is easy and fun to use and helps with all aspects of programming in R. It is an essential piece in following the iterative approach to data analysis schematized in Figure 2.\nReproducibility. As an equivalent to the laboratory notebook that is standard good practice in labwork, we advocate the use of a computational diary written in the R markdown or quarto formats. We use the quarto system to convert these files into easy-to- read and shareable HTML or PDF documents. These can even become full-fledged scientific articles or supplements. Together with a version control system, this approach also helps with tracking changes.\nCollaborative environment. Quarto enables the creation of websites containing code, text, figures and tables with a minimum of work.\nRich data structures. The Bioconductor project has defined specialized data containers to represent complex biological datasets. These help to keep your data consistent, safe and easy to use.\nInteroperability and distributed development. Bioconductor in particular contains packages from diverse authors that cover a wide range of functionalities but still interoperate because of the common data containers.\nDocumentation. Many R packages come with excellent documentation in their function manual pages and vignettes. The vignettes are usually the best starting point into a package, as they give you a high-level narrative on what the package does, whereas the manual pages give detailed information on input, output and inner workings of each function. There are online tutorials, fora and mailing lists for many aspects of working with R and Bioconductor.\nHigh-level language. R is an interpreted high-level language. Its roots in LISP and its functional programming features mean that code is data and can be computed on, which enables efficient programming and is fun. These features facilitate constructing powerful domain specific languages2. R is not a fixed language – throughout its history, it has been actively evolving and is constantly improving.\n2 Examples include R’s formula interface, the grammar of graphics in ggplot2 , the data manipulation functionality of dplyr and R markdown.\nEfron, Bradley. 2010. Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction. Cambridge University Press.\nFisher, Ronald Aylmer. 1935. The Design of Experiments. Oliver & Boyd.\nGrolemund, Garrett, and Hadley Wickham. 2017. R for Data Science. O’Reilly.\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2008. The Elements of Statistical Learning. 2^{} ed. Springer.\nHolmes - Junca, Susan. 1985. “Outils Informatiques Pour l’évaluation de La Pertinence d’un résultat En Analyse Des Données.” PhD thesis, Université Montpellier II, France.\nHuber, Wolfgang, Vincent J Carey, Robert Gentleman, Simon Anders, Marc Carlson, Benilton S Carvalho, Hector Corrada Bravo, et al. 2015. “Orchestrating High-Throughput Genomic Analysis with Bioconductor.” Nature Methods 12 (2): 115–21.\nIhaka, Ross, and Robert Gentleman. 1996. “R: A Language for Data Analysis and Graphics.” Journal of Computational and Graphical Statistics 5 (3): 299–314.\nNeyman, Jerzy, and Egon S Pearson. 1936. Sufficient Statistics and Uniformly Most Powerful Tests of Statistical Hypotheses. University California Press.\nTukey, John W. 1977. “Exploratory Data Analysis.” Massachusetts: Addison- Wesley.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The challenge: heterogeneity</span>"
    ]
  },
  {
    "objectID": "01-chap.html",
    "href": "01-chap.html",
    "title": "3  1.1 Goals for this chapter",
    "section": "",
    "text": "3.1 1.2 A real example\nIn molecular biology, many situations involve counting events: how many codons use a certain spelling, how many reads of DNA match a reference, how many CG digrams are observed in a DNA sequence. These counts give us discrete variables, as opposed to quantities such as mass and intensity that are measured on continuous scales.\nIf we know the rules that the mechanisms under study follow, even if the outcomes are random, we can generate the probabilities of any events we are interested in by computations and standard probability laws. This is a top- down approach based on deduction and our knowledge of how to manipulate probabilities. In Chapter 2, you will see how to combine this with data-driven (bottom-up) statistical modeling.\nIn this chapter we will:\nLet’s dive into an example where we have a probability model for the data generating process. Our model says that mutations along the genome of HIV (Human Immunodeficiency Virus) occur with a rate of \\(5 ^{-4}\\) per nucleotide per replication cycle. The rate is the same at each nucleotide position, and mutations at one position happen independently of what happens at other positions1. The genome size of HIV is about \\(10^4=10,000\\) nucleotides, thus, after one cycle, the total number of mutations will follow a Poisson distribution2 with rate \\(5 ^{-4} ^4 = 5\\). What does that tell us?\n1 In practice, and strictly speaking, complete and utter independence will rarely hold in reality, if you look close enough. Thus, what modellers usually mean with such assertions is that any possible correlations or dependencies are so weak and rare that ignoring them is a good enough approximation.\n2 We will give more details later about this type of probability distribution\nThis probability model predicts that the number of mutations over one replication cycle will be close to 5, and that the variability of this estimate is \\(\\) (the standard error). We now have baseline reference values for both the number of mutations we expect to see in a typical HIV strain and its variability.\nIn fact, we can deduce even more detailed information. If we want to know how often 3 mutations could occur under the Poisson(5) model, we can use an R function to generate the probability of seeing \\(x=3\\) events, taking the value of the rate parameter of the Poisson distribution, called lambda (\\(\\)), to be \\(5\\).\nGreek letters such as \\(\\) and \\(\\) often denote important parameters that characterize the probability distributions we use.\nThis says the chance of seeing exactly three events is around 0.14, or about 1 in 7.\nIf we want to generate the probabilities of all values from 0 to 12, we do not need to write a loop. We can simply set the first argument to be the vector of these 13 values, using R’s sequence operator, the colon “:”. We can see the probabilities by plotting them (Figure 1.1). As with this figure, most figures in the margins of this book are created by the code shown in the text.\nNote how the output from R is formatted: the first line begins with the first item in the vector, hence the [1], and the second line begins with the 9th item, hence the [9]. This helps you keep track of elements in long vectors. The term vector is R parlance for an ordered list of elements of the same type (in this case, numbers).\nFigure 1.1: Probabilities of seeing 0,1,2,…,12 mutations, as modeled by the Poisson(5) distribution. The plot shows that we will often see 4 or 5 mutations but rarely as many as 12. The distribution continues to higher numbers (\\(13,…\\)), but the probabilities will be successively smaller, and here we don’t visualize them.\nMathematical theory tells us that the Poisson probability of seeing the value \\(x\\) is given by the formula \\(e^{-} ^x / x!\\). In this book, we’ll discuss theory from time to time, but give preference to displaying concrete numeric examples and visualizations like Figure 1.1.\nThe Poisson distribution is a good model for rare events such as mutations. Other useful probability models for discrete events are the Bernoulli, binomial and multinomial distributions. We will explore these models in this chapter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "01-chap.html#a-real-example",
    "href": "01-chap.html#a-real-example",
    "title": "3  1.1 Goals for this chapter",
    "section": "",
    "text": "dpois(x = 3, lambda = 5)__\n\n\n[1] 0.1403739\n\n\n\n\n\nNote how the output from R is formatted: the first line begins with the first item in the vector, hence the [1], and the second line begins with the 9th item, hence the [9]. This helps you keep track of elements in long vectors. The term vector is R parlance for an ordered list of elements of the same type (in this case, numbers).\n\n\n\n0:12 __\n\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12\n\n\ndpois(x = 0:12, lambda = 5)__\n\n\n [1] 0.0067 0.0337 0.0842 0.1404 0.1755 0.1755 0.1462 0.1044 0.0653 0.0363\n[11] 0.0181 0.0082 0.0034\n\n\nbarplot(dpois(0:12, 5), names.arg = 0:12, col = \"red\")__",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "01-chap.html#using-discrete-probability-models",
    "href": "01-chap.html#using-discrete-probability-models",
    "title": "3  1.1 Goals for this chapter",
    "section": "3.2 1.3 Using discrete probability models",
    "text": "3.2 1.3 Using discrete probability models\n\n\n\nThink of a categorical variable as having different alternative values. These are the levels, similar to the different alternatives at a gene locus: alleles.\n\n\nThink of a categorical variable as having different alternative values. These are the levels, similar to the different alternatives at a gene locus: alleles.\nA point mutation can either occur or not; it is a binary event. The two possible outcomes (yes, no) are called the levels of the categorical variable.\nNot all events are binary. For example, the genotypes in a diploid organism can take three levels (AA, Aa, aa).\nSometimes the number of levels in a categorical variable is very large; examples include the number of different types of bacteria in a biological sample (hundreds or thousands) and the number of codons formed of 3 nucleotides (64 levels).\nWhen we measure a categorical variable on a sample, we often want to tally the frequencies of the different levels in a vector of counts. R has a special encoding for categorical variables and calls them factors 3. Here we capture the different blood genotypes for 19 subjects in a vector which we tabulate.\n3 R makes sure that the factor variable will accept no other, “illegal” values, and this is useful for keeping your calculations safe.\n\n\n\nc() is one of the most basic functions. It collates elements of the same type into a vector. In the code shown here, the elements of genotype are character strings.\n\n\nc() is one of the most basic functions. It collates elements of the same type into a vector. In the code shown here, the elements of genotype are character strings.\ngenotype = c(\"AA\",\"AO\",\"BB\",\"AO\",\"OO\",\"AO\",\"AA\",\"BO\",\"BO\",\n             \"AO\",\"BB\",\"AO\",\"BO\",\"AB\",\"OO\",\"AB\",\"BB\",\"AO\",\"AO\")\ntable(genotype)__\n\n\ngenotype\nAA AB AO BB BO OO \n 2  2  7  3  3  2 \nOn creating a factor , R automatically detects the levels. You can access the levels with the levels function.\n\n\n\nIt is not obvious from the output of the table function that the input was a factor; however if there had been another level with no instances, the table would also have contained that level, with a zero count.\n\n\nIt is not obvious from the output of the table function that the input was a factor; however if there had been another level with no instances, the table would also have contained that level, with a zero count.\ngenotypeF = factor(genotype)\nlevels(genotypeF)__\n\n\n[1] \"AA\" \"AB\" \"AO\" \"BB\" \"BO\" \"OO\"\n\n\ntable(genotypeF)__\n\n\ngenotypeF\nAA AB AO BB BO OO \n 2  2  7  3  3  2 \n__\nQuestion 1.1\nWhat if you want to create a factor that has some levels not yet in your data?\n__\nSolution\n__\nLook at the manual page of the factor function.\nIf the order in which the data are observed doesn’t matter, we call the random variable exchangeable. In that case, all the information available in the factor is summarized by the counts of the factor levels. We then say that the vector of frequencies is sufficient to capture all the relevant information in the data, thus providing an effective way of compressing the data.\n\n3.2.1 1.3.1 Bernoulli trials\n\nFigure 1.2: Two possible events with unequal probabilities. We model this by a Bernoulli distribution with probability parameter \\(p=2/3\\).\nTossing a coin has two possible outcomes. This simple experiment, called a Bernoulli trial, is modeled using a so-called Bernoulli random variable. Understanding this building block will take you surprisingly far. We can use it to build more complex models.\nLet’s try a few experiments to see what some of these random variables look like. We use special R functions tailored to generate outcomes for each type of distribution. They all start with the letter r, followed by a specification of the model, here rbinom, where binom is the abbreviation used for binomial.\nSuppose we want to simulate a sequence of 15 fair coin tosses. To get the outcome of 15 Bernoulli trials with a probability of success equal to 0.5 (a fair coin), we write\nrbinom(15, prob = 0.5, size = 1)__\n\n\n [1] 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0\nWe use the rbinom function with a specific set of parameters 4: the first parameter is the number of trials we want to observe; here we chose 15. We designate by prob the probability of success. By size=1 we declare that each individual trial consists of just one single coin toss.\n4 For R functions, parameters are also called argument s.\n__\nQuestion 1.2\nRepeat this function call a number of times. Why isn’t the answer always the same?\nSuccess and failure can have unequal probabilities in a Bernoulli trial, as long as the probabilities sum to one5. To simulate twelve trials of throwing a ball into the two boxes as shown in Figure 1.2, with probability of falling in the right-hand box \\(\\) and in the left-hand box \\(\\), we write\n5 We call such events complementary.\nrbinom(12, prob = 2/3, size = 1)__\n\n\n [1] 1 1 1 0 0 0 1 0 1 0 1 0\nThe 1 indicates success, meaning that the ball fell in the right-hand box, 0 means the ball fell in the left-hand box.\n\n\n3.2.2 1.3.2 Binomial success counts\nIf we only care how many balls go in the right-hand box, then the order of the throws doesn’t matter6, and we can get this number by just taking the sum of the cells in the output vector. Therefore, instead of the binary vector we saw above, we only need to report a single number. In R, we can do this using one call to the rbinom function with the parameter size set to 12.\nTwo outcomes and a size of 1 or more makes it a binomial trial. If the size is 1, then this is the special case of the Bernoulli trial.\n6 The exchangeability property.\nrbinom(1, prob = 2/3, size = 12)__\n\n\n[1] 9\nThis output tells us how many of the twelve balls fell into the right-hand box (the outcome that has probability 2/3). We use a random two-box model when we have only two possible outcomes such as heads or tails, success or failure, CpG or non-CpG, M or F, Y = pyrimidine or R = purine, diseased or healthy, true or false. We only need the probability of “success” \\(p\\), because “failure” (the complementary event) will occur with probability \\(1-p\\). When looking at the result of several such trials, if they are exchangeable7, we record only the number of successes. Therefore, SSSSSFSSSSFFFSF is summarized as (#Successes=10, #Failures=5), or as \\(x=10\\), \\(n=15\\).\n7 One situation in which trials are exchangeable is if they are independent of each other.\nThe number of successes in 15 Bernoulli trials with a probability of success of 0.3 is called a binomial random variable or a random variable that follows the \\(B(15,0.3)\\) distribution. To generate samples, we use a call to the rbinom function with the number of trials set to 15:\n\n\n\nWhat does set.seed do here?\n\n\nWhat does set.seed do here?\nset.seed(235569515)\nrbinom(1, prob = 0.3, size = 15)__\n\n\n[1] 5\n__\nQuestion 1.3\nRepeat this function call ten times. What seems to be the most common outcome?\n__\nSolution\n__\nThe most frequent value is 4. In fact, the theoretical proportion of times that we expect 4 to appear is the value of the probability that \\(X=4\\) if \\(X\\) follows \\(B(15, 0.3)\\).\nThe complete probability mass distribution is available by typing:\nWe use the function round to keep the number of printed decimal digits down to 2.\nprobabilities = dbinom(0:15, prob = 0.3, size = 15)\nround(probabilities, 2)__\n\n\n [1] 0.00 0.03 0.09 0.17 0.22 0.21 0.15 0.08 0.03 0.01 0.00 0.00 0.00 0.00 0.00\n[16] 0.00\nWe can produce a bar plot of this distribution, shown in Figure 1.3.\nbarplot(probabilities, names.arg = 0:15, col = \"red\")__\n\nFigure 1.3: Theoretical distribution of \\(B(15,0.3)\\) . The highest bar is at \\(x=4\\). We have chosen to represent theoretical values in red throughout.\nThe number of trials is the number we input in R as the size parameter and is often written \\(n\\), while the probability of success is \\(p\\). Mathematical theory tells us that for \\(X\\) distributed as a binomial distribution with parameters \\((n,p)\\) written \\(X B(n,p)\\), the probability of seeing \\(X=k\\) successes is\n\n\n\nInstead of \\frac{n!}{(n-k)!k!} we can use the special notation {n \\choose k} as a shortcut.\n\n\nInstead of \\(\\) we can use the special notation \\({n k}\\) as a shortcut.\n\\[ ]\n__\nQuestion 1.4\nWhat is the output of the formula for \\(k=3\\), \\(p=2/3\\), \\(n=4\\)?\n\n\n3.2.3 1.3.3 Poisson distributions\n\nFigure 1.4: Simeon Poisson, after whom the Poisson distribution is named (this is why it always has a capital letter, except in our R code).\nWhen the probability of success \\(p\\) is small and the number of trials \\(n\\) large, the binomial distribution \\(B(n, p)\\) can be faithfully approximated by a simpler distribution, the Poisson distribution with rate parameter \\(=np\\). We already used this fact, and this distribution, in the HIV example (Figure 1.1).\n__\nQuestion 1.5\nWhat is the probability mass distribution of observing 0:12 mutations in a genome of \\(n = 10^4\\) nucleotides, when the probability is \\(p = 5 ^{-4}\\) per nucleotide? Is it similar when modeled by the binomial \\(B(n,p)\\) distribution and by the Poisson\\((=np)\\) distribution?\n__\nSolution\n__\nNote that, unlike the binomial distribution, the Poisson no longer depends on two separate parameters \\(n\\) and \\(p\\), but only on their product \\(np\\). As in the case of the binomial distribution, we also have a mathematical formula for computing Poisson probabilities:\n\\[ P(X=k)= . \\]\nFor instance, let’s take \\(\\) and compute \\(P(X=3)\\):\n5^3 * exp(-5) / factorial(3)__\n\n\n[1] 0.1403739\nwhich we can compare with what we computed above using dpois.\n__\nTask\nSimulate a mutation process along 10,000 positions with a mutation rate of \\(5^{-4}\\) and count the number of mutations. Repeat this many times and plot the distribution with the barplot function (see Figure 1.5).\nrbinom(1, prob = 5e-4, size = 10000)__\n\n\n[1] 6\n\n\nsimulations = rbinom(n = 300000, prob = 5e-4, size = 10000)\nbarplot(table(simulations), col = \"lavender\")__\n\nFigure 1.5: Simulated distribution of B(10000, \\(10^{-4}\\)) for 300000 simulations.\nNow we are ready to use probability calculations in a case study.\n\n\n3.2.4 1.3.4 A generative model for epitope detection\nWhen testing certain pharmaceutical compounds, it is important to detect proteins that provoke an allergic reaction. The molecular sites that are responsible for such reactions are called epitopes. The technical definition of an epitope is:\n\nA specific portion of a macromolecular antigen to which an antibody binds. In the case of a protein antigen recognized by a T-cell, the epitope or determinant is the peptide portion or site that binds to a Major Histocompatibility Complex (MHC) molecule for recognition by the T cell receptor (TCR).\n\nAnd in case you’re not so familiar with immunology: an antibody (as schematized in Figure 1.6) is a type of protein made by certain white blood cells in response to a foreign substance in the body, which is called the antigen.\n\nFigure 1.6: A diagram of an antibody showing several immunoglobulin domains in color.\nAn antibody binds (with more or less specificity) to its antigen. The purpose of the binding is to help destroy the antigen. Antibodies can work in several ways, depending on the nature of the antigen. Some antibodies destroy antigens directly. Others help recruit white blood cells to destroy the antigen. An epitope, also known as antigenic determinant, is the part of an antigen that is recognized by the immune system, specifically by antibodies, B cells or T cells.\n\n3.2.4.1 ELISA error model with known parameters\nELISA8 assays are used to detect specific epitopes at different positions along a protein. Suppose the following facts hold for an ELISA array we are using:\n8 E nzyme-L inked I mmunoS orbent A ssay (Wikipedia link ELISA).\n\nThe baseline noise level per position, or more precisely the false positive rate, is 1%. This is the probability of declaring a hit – we think we have an epitope – when there is none. We write this \\(P(|)\\)9.\nThe protein is tested at 100 different positions, supposed to be independent.\nWe are going to examine a collection of 50 patient samples.\n\n9 The vertical bar in expressions such as \\(X|Y\\) means “\\(X\\) happens conditional on \\(Y\\) being the case”.\n\n\n3.2.4.2 One patient’s data\nThe data for one patient’s assay look like this:\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nwhere the 1 signifies a hit (and thus the potential for an allergic reaction), and the zeros signify no reaction at that position.\n__\nTask\nVerify by simulation that the sum of 50 independent Bernoulli variables with \\(p=0.01\\) is –to good enough approximation– the same as a Poisson(\\(0.5\\)) random variable.\n\n\n3.2.4.3 Results from the 50 assays\nWe’re going to study the data for all 50 patients tallied at each of the 100 positions. If there are no allergic reactions, the false positive rate means that for one patient, each individual position has a probability of 1 in 100 of being a 1. So, after tallying 50 patients, we expect at any given position the sum of the 50 observed \\((0,1)\\) variables to have a Poisson distribution with parameter 0.5. A typical result may look like Figure 1.7. Now suppose we see actual data as shown in Figure 1.8, loaded as R object e100 from the data file e100.RData.\n\nFigure 1.7: Plot of typical data from our generative model for the background, i.,e., for the false positive hits: 100 positions along the protein, at each position the count is drawn from a Poisson(0.5) random variable.\nload(\"../data/e100.RData\")\nbarplot(e100, ylim = c(0, 7), width = 0.7, xlim = c(-0.5, 100.5),\n  names.arg = seq(along = e100), col = \"darkolivegreen\")__\n\nFigure 1.8: Output of the ELISA array results for 50 patients in the 100 positions.\nThe spike in Figure 1.8 is striking. What are the chances of seeing a value as large as 7, if no epitope is present?\nIf we look for the probability of seeing a number as big as 7 (or larger) when considering one Poisson(\\(0.5\\)) random variable, the answer can be calculated in closed form as\n\\[ P(X)= _{k=7}^P(X=k). \\]\nThis is, of course, the same as \\(1-P(X)\\). The probability \\(P(X)\\) is the so-called cumulative distribution function at 6, and R has the function ppois for computing it, which we can use in either of the following two ways:10\n10 Besides the convenience of not having to do the subtraction from one, the second of these computations also tends to be more accurate when the probability is small. This has to do with limitations of floating point arithmetic.\n1 - ppois(6, 0.5)__\n\n\n[1] 1.00238e-06\n\n\nppois(6, 0.5, lower.tail = FALSE)__\n\n\n[1] 1.00238e-06\n__\nTask\nCheck the manual page of ppois for the meaning of the lower.tail argument.\nWe denote this number by \\(\\), the Greek letter epsilon11. We have shown that the probability of seeing a count as large as \\(7\\), assuming no epitope reactions, is:\n11 Mathematicians often call small numbers (and children) \\(\\)s.\n\\[ =P(X)=1-P(X)^{-6}. \\]\n\n\n3.2.4.4 Extreme value analysis for the Poisson distribution\nStop! The above calculation is not the correct computation in this case.\n__\nQuestion 1.6\nCan you spot the flaw in our reasoning if we want to compute the probability that we observe these data if there is no epitope?\n__\nSolution\n__\nWe looked at all 100 positions, looked for the largest value and found that it was 7. Due to this selection, a value as large as 7 is more likely to occur than if we only looked at one position.\nSo instead of asking what the chances are of seeing a Poisson(0.5) as large as 7, we should ask ourselves, what are the chances that the maximum of 100 Poisson(0.5) trials is as large as 7? We use extreme value analysis here12. We order the data values \\(x_1,x_2,… ,x_{100}\\) and rename them \\(x_{(1)},x_{(2)},x_{(3)},… ,x_{(100)}\\), so that \\(x_{(1)}\\) denotes the smallest and \\(x_{(100)}\\) the largest of the counts over the 100 positions. Together, \\(x_{(1)},… x_{(100)}\\) are called the rank statistic of this sample of 100 values.\n12 Meaning that we’re interested in the behavior of the very large or very small values of a random distribution, for instance the maximum or the minimum.\n13 The notation with the \\(\\) is just a compact way to write the product of a series of terms, analogous to the \\(\\) for sums.\nThe maximum value being as large as 7 is the complementary event of having all 100 counts be smaller than or equal to 6. Two complementary events have probabilities that sum to 1. Because the positions are supposed to be independent, we can now do the computation13:\n\\[ ]\nBecause we suppose each of these 100 events are independent, we can use our result from above:\n\\[ _{i=1}^{100} P(x_i )= (P(x_i ))^{100}= (1-)^{100}. \\]\n\n\n3.2.4.5 Actually computing the numbers\nWe could just let R compute the value of this number, \\((1-)^{100}\\). For those interested in how such calculations can be shortcut through approximation, we give some details. These can be skipped on a first reading.\nWe recall from above that \\(^{-6}\\) is much smaller than 1. To compute the value of \\((1-)^{100}\\) approximately, we can use the binomial theorem and drop all “higher order” terms of \\(\\), i.e., all terms with \\(^2, ^3, …\\), because they are negligibly small compared to the remaining (“leading”) terms.\n\\[ (1-)^n = _{k=0}^n {nk} , 1^{n-k} , (-)^k = 1-n+{n} ^2 - {n} ^3 + … -n - 10^{-4} \\]\nAnother, equivalent, route goes by using the approximation \\(e^{-} -\\), which is the same as \\((1-) -\\). Hence\n\\[ (1-)^{100} = e{((1-){100})} = e^{ 100 (1-)} e^{-100 } e{-10{-4}} - 10^{-4}. \\]\nThus the correct probability of seeing a number of hits as large or larger than 7 in the 100 positions, if there is no epitope, is about 100 times the probability we wrongly calculated previously.\nBoth computed probabilities \\(10^{-6}\\) and \\(10^{-4}\\) are smaller than standard significance thresholds (say, \\(0.05, 0.01\\) or \\(0.001\\)). The decision to reject the null of no epitope would have been the same. However if one has to stand up in court and defend the p-value to 8 significant digits as in some forensic court cases:14 that is another matter. The adjusted p-value that takes into account the multiplicity of the test is the one that should be reported, and we will return to this important issue in Chapter 6.\n14 This occurred in the examination of the forensic evidence in the OJ Simpson case.\n\n\n3.2.4.6 Computing probabilities by simulation\nIn the case we just saw, the theoretical probability calculation was quite simple and we could figure out the result by an explicit calculation. In practice, things tend to be more complicated, and we are better to compute our probabilities using the Monte Carlo method: a computer simulation based on our generative model that finds the probabilities of the events we’re interested in. Below, we generate 100,000 instances of picking the maximum from 100 Poisson distributed numbers.\nmaxes = replicate(100000, {\n  max(rpois(100, 0.5))\n})\ntable(maxes)__\n\n\nmaxes\n    1     2     3     4     5     6     7     9 \n    7 23028 60840 14364  1604   141    15     1 \nIn 16 of 100000 trials, the maximum was 7 or larger. This gives the following approximation for \\(P(X_{})\\)15:\n15 In R, the expression maxes &gt;= 7 evaluates into a logical vector of the same length as maxes, but with values of TRUE and FALSE. If we apply the function mean to it, that vector is converted into 0s and 1s, and the result of the computation is the fraction of 1s, which is the same as the fraction of TRUEs.\nmean( maxes &gt;= 7 )__\n\n\n[1] 0.00016\nwhich more or less agrees with our theoretical calculation. We already see one of the potential limitations of Monte Carlo simulations: the “granularity” of the simulation result is determined by the inverse of the number of simulations (100000) and so will be around 10^{-5}. Any estimated probability cannot be more precise than this granularity, and indeed the precision of our estimate will be a few multiples of that. Everything we have done up to now is only possible because we know the false positive rate per position, we know the number of patients assayed and the length of the protein, we suppose we have identically distributed independent draws from the model, and there are no unknown parameters. This is an example of probability or generative modeling : all the parameters are known and the mathematical theory allows us to work by deduction in a top-down fashion.\nWe postulated the Poisson distribution for the noise, pretending we knew all the parameters and were able to conclude through mathematical deduction.\nIf instead we are in the more realistic situation of knowing the number of patients and the length of the proteins, but don’t know the distribution of the data, then we have to use statistical modeling. This approach will be developed in Chapter 2. We will see that if we have only the data to start with, we first need to fit a reasonable distribution to describe it. However, before we get to this harder problem, let’s extend our knowledge of discrete distributions to more than binary, success-or-failure outcomes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "01-chap.html#multinomial-distributions-the-case-of-dna",
    "href": "01-chap.html#multinomial-distributions-the-case-of-dna",
    "title": "3  1.1 Goals for this chapter",
    "section": "3.3 1.4 Multinomial distributions: the case of DNA",
    "text": "3.3 1.4 Multinomial distributions: the case of DNA\n\n3.3.0.1 More than two outcomes.\nWhen modeling four possible outcomes, as for instance the boxes in Figure 1.9 or when studying counts of the four nucleotides [A,C,G] and [T], we need to extend the [binomial] model.\n\nFigure 1.9: The boxes represent four outcomes or levels of a discrete categorical variable. The box on the right represents the more likely outcome.\nRecall that when using the binomial, we can consider unequal probabilities for the two outcomes by assigning a probability \\(p=P(1)=p_1\\) to the outcome 1 and \\(1-p=p(0)=p_0\\) to the outcome 0. When there are more than two possible outcomes, say [A,C,G] and [T], we can think of throwing balls into boxes of differing sizes corresponding to different probabilities, and we can label these probabilities \\(p_A,p_C,p_G,p_T\\). Just as in the binomial case the sum of the probabilities of all possible outcomes is 1, \\(p_A+p_C+p_G+p_T=1\\).\n\n\n\nYou are secretly meeting a continuous distribution here, the uniform distribution: runif.\n\n\nYou are secretly meeting a continuous distribution here, the uniform distribution: runif.\n__\nTask\nExperiment with the random number generator that generates all possible numbers between \\(0\\) and \\(1\\) through the function called runif. Use it to generate a random variable with 4 levels (A, C, G, T) where \\(p_{}=, p_{}=, p_{}=, p_{}=\\).\nMathematical formulation. Multinomial distributions are the most important model for tallying counts and R uses a general formula to compute the probability of a multinomial vector of counts \\((x_1,…,x_m)\\) for outcomes of \\(n\\) draws from \\(m\\) boxes with probabilities \\(p_1,…,p_m\\):\nThe first term reads: the joint probability of observing count \\(x_1\\) in box 1 and \\(x_2\\) in 2 and … \\(x_m\\) in box m, given that box 1 has probability \\(p_1\\), box 2 has probability \\(p_2\\), … and box \\(m\\) has probability \\(p_m\\).\n\\[\\[\\begin{align} P(x_1,x_2,...,x_m) &=\\frac{n!}{\\prod_{i=1}^m x_i!}\n\\prod_{i=1}^m p_i^{x_i}\\\\\\ &={{n}\\choose{x_1,x_2,...,x_m}} \\;\np_1^{x_1}\\,p_2^{x_2}\\cdots p_m^{x_m}. \\end{align}\\]\\]\nThe term in brackets is called the multinomial coefficient and is an abbreviation for \\[{nx_1,x_2,…,x_m}=.\\] So this is a generalization of the binomial coefficient – for \\(m=2\\) it is the same as the binomial coefficient.\n__\nQuestion 1.7\nSuppose we have four boxes that are equally likely. Using the formula, what is the probability of observing 4 in the first box, 2 in the second box, and none in the two other boxes?\n__\nSolution\n__\n\\[ P(4,2,0,0)= =. \\]\ndmultinom(c(4, 2, 0, 0), prob = rep(1/4, 4))__\n\n\n[1] 0.003662109\nWe often run simulation experiments to check whether the data we see are consistent with the simplest possible four-box model where each box has the same probability 1/4. In some sense it is the strawman (nothing interesting is happening). We’ll see more examples of this in Chapter 2. Here we use a few R commands to generate such vectors of counts. First suppose we have 8 characters of four different, equally likely types:\npvec = rep(1/4, 4)\nt(rmultinom(1, prob = pvec, size = 8))__\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    1    3\n__\nQuestion 1.8\nTry the code without using the t() function; what does t stand for?\n__\nQuestion 1.9\nHow do you interpret the difference between rmultinom(n = 8, prob = pvec, size = 1) and rmultinom(n = 1, prob = pvec, size = 8)? Hint: remember what we did in Sections 1.3.1 and 1.3.2.\n\n\n3.3.1 1.4.1 Simulating for power\nLet’s see an example of using Monte Carlo for the multinomial in a way which is related to a problem scientists often have to solve when planning their experiments: how big a sample size do I need?\n\nAsk a statistician about sample size, they will always tell you they need more data. The larger the sample size, the more sensitive the results. However lab work is expensive, so there is a tricky cost-benefit tradeoff to be considered. This is such an important problem that we have dedicated a whole chapter to it at the end of the book (Chapter 13).\nThe term power has a special meaning in statistics. It is the probability of detecting something if it is there, also called the true positive rate.\nConventionally, experimentalists aim for a power of 80% (or more) when planning experiments. This means that if the same experiment is run many times, about 20% of the time it will fail to yield significant results even though it should.\nLet’s call \\(H_0\\) the null hypothesis that the DNA data we have collected comes from a fair process, where each of the 4 nucleotides is equally likely \\((p_A,p_C,p_G,p_T)=(0.25,0.25,0.25,0.25)\\). Null here just means: the baseline, where nothing interesting is going on. It’s the strawman that we are trying to disprove (or “reject”, in the lingo of statisticians), so the null hypothesis should be such that deviations from it are interesting16.\n16 If you know a little biology, you will know that DNA of living organisms rarely follows that null hypothesis – so disproving it may not be all that interesting. Here we proceed with this null hypothesis because it allows us to illustrate the calculations, but it can also serve us as a reminder that the choice of a good null hypothesis (one whose rejection is interesting) requires scientific input.\nAs you saw by running the R commands for 8 characters and 4 equally likely outcomes, represented by equal-sized boxes, we do not always get 2 in each box. It is impossible to say, from looking at just 8 characters, whether the nucleotides come from a fair process or not.\nLet’s determine if, by looking at a sequence of length \\(n=20\\), we can detect whether the original distribution of nucleotides is fair or whether it comes from some other (“alternative”) process.\nWe generate 1000 simulations from the null hypothesis using the rmultinom function. We display only the first 11 columns to save space.\nobsunder0 = rmultinom(1000, prob = pvec, size = 20)\ndim(obsunder0)__\n\n\n[1]    4 1000\n\n\nobsunder0[, 1:11]__\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]\n[1,]    6    5    6    8    4    6    2    7    5     4     4\n[2,]    6    6    3    7    3    3    8    4    3     3     5\n[3,]    3    3    6    2    8    3    5    7    4     7     6\n[4,]    5    6    5    3    5    8    5    2    8     6     5\n\n\n\nNotice that the top of every column there is an index of the form [,1][,2]… These are the column indices. The rows are labeled [1,][2,]…. The object obsunder0 is not a simple vector as those we have seen before, but an array of numbers in a matrix.\n\n\nNotice that the top of every column there is an index of the form [,1][,2]... These are the column indices. The rows are labeled [1,][2,].... The object obsunder0 is not a simple vector as those we have seen before, but an array of numbers in a matrix.\nEach column in the matrix obsunder0 is a simulated instance. You can see that the numbers in the boxes vary a lot: some are as big as 8, whereas the expected value is 5=20/4.\n\n3.3.1.1 Creating a test\nRemember: we know these values come from a fair process. Clearly, knowing the process’ expected values isn’t enough. We also need a measure of variability that will enable us to describe how much variability is expected and how much is too much. We use as our measure the following statistic, which is computed as the sum of the squares of the differences between the observed values and the expected values relative to the expected values. Thus, for each instance,\nThis measure weights each of the square residuals relative to their expected values.\n\\[ {}=+ + + =_i \\]\nHow much do the first three columns of the generated data differ from what we expect? We get:\nexpected0 = pvec * 20\nsum((obsunder0[, 1] - expected0)^2 / expected0)__\n\n\n[1] 1.2\n\n\nsum((obsunder0[, 2] - expected0)^2 / expected0)__\n\n\n[1] 1.2\n\n\nsum((obsunder0[, 3] - expected0)^2 / expected0)__\n\n\n[1] 1.2\nThe values of the measure can differ- you can look at a few more than 3 columns, we’re going to see how to study all 1,000 of them. To avoid repetitive typing, we encapsulate the formula for stat, Equation 1.1, in a function:\nstat = function(obsvd, exptd = 20 * pvec) {\n  sum((obsvd - exptd)^2 / exptd)\n}\nstat(obsunder0[, 1])__\n\n\n[1] 1.2\nTo get a more complete picture of this variation, we compute the measure for all 1000 instances and store these values in a vector we call S0: it contains values generated under \\(H_0\\). We can consider the histogram of the S0 values shown in Figure 1.10 an estimate of our null distribution.\nS0 = apply(obsunder0, 2, stat)\nsummary(S0)__\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   1.200   2.800   3.126   4.400  17.600 \n\n\nhist(S0, breaks = 25, col = \"lavender\", main = \"\")__\n\nFigure 1.10: The histogram of simulated values S0 of the statistic stat under the null (fair) distribution provides an approximation of the sampling distribution of the statistic stat.\n\n\n\nThe apply function is shorthand for a loop over the rows or columns of an array. Here the second argument, 2, indicates looping over the columns.\n\n\nThe apply function is shorthand for a loop over the rows or columns of an array. Here the second argument, 2, indicates looping over the columns.\nThe summary function shows us that S0 takes on a spread of different values. From the simulated data we can approximate, for instance, the 95% quantile (a value that separates the smaller 95% of the values from the 5% largest values).\nq95 = quantile(S0, probs = 0.95)\nq95 __\n\n\n95% \n7.6 \nSo we see that 5% of the S0 values are larger than 7.6. We’ll propose this as our critical value for testing data and will reject the hypothesis that the data come from a fair process, with equally likely nucleotides, if the weighted sum of squares stat is larger than 7.6.\n\n\n3.3.1.2 Determining our test’s power\nWe must compute the probability that our test—based on the weighted sum-of- square differences—will detect that the data in fact do not come from the null hypothesis. We compute the probability of rejecting by simulation. We generate 1000 simulated instances from an alternative process, parameterized by pvecA.\n\npvecA = c(3/8, 1/4, 1/4, 1/8)\nobserved = rmultinom(1000, prob = pvecA, size = 20)\ndim(observed)__\n\n\n[1]    4 1000\n\n\nobserved[, 1:7]__\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n[1,]   10    4    8    8    4    7    7\n[2,]    3   10    5    6    6    7    2\n[3,]    5    3    5    6    4    2    6\n[4,]    2    3    2    0    6    4    5\n\n\napply(observed, 1, mean)__\n\n\n[1] 7.469 4.974 5.085 2.472\n\n\nexpectedA = pvecA * 20\nexpectedA __\n\n\n[1] 7.5 5.0 5.0 2.5\nAs with the simulation from the null hypothesis, the observed values vary considerably. The question is: how often (out of 1000 instances) will our test detect that the data depart from the null?\nThe test doesn’t reject the first observation, (10, 3, 5, 2), because the value of the statistic is within the 95th percentile.\nstat(observed[, 1])__\n\n\n[1] 7.6\n\n\nS1 = apply(observed, 2, stat)\nq95 __\n\n\n95% \n7.6 \n\n\nsum(S1 &gt; q95)__\n\n\n[1] 199\n\n\npower = mean(S1 &gt; q95)\npower __\n\n\n[1] 0.199\n\n\n\nWe read the vertical line as given or conditional on.\n\n\nWe read the vertical line as given or conditional on.\nRun across 1000 simulations, the test identified 199 as coming from an alternative distribution. We have thus computed that the probability \\(P(H_0 ;|; H_A)\\) is 0.199.\nWith a sequence length of \\(n = 20\\) we have a power of about 20% to detect the difference between the fair generating process and our alternative.\n__\nTask\nIn practice, as we mentioned, an acceptable value of power is \\(0.8\\) or more. Repeat the simulation experiments and suggest a new sequence length \\(n\\) that will ensure that the power is acceptable.\n\n\n3.3.1.3 Classical statistics for classical data\nWe didn’t need to simulate the data using Monte Carlo to compute the 95th percentiles; there is an adequate theory to help us with the computations.\nOur statistic stat actually has a well-known distribution called the chi- square distribution (with 3 degrees of freedom) and written \\({}^2_3\\).\n\nFigure 1.11: We have studied how a probability model has a distribution, which we call \\(F\\). \\(F\\) often depends on parameters, which are–by convention–denoted by Greek letters, such as \\(\\). The observed data are generated via the brown arrow and are represented by Latin letters, such as \\(x\\). The vertical bar in the probability computation stands for supposing that or conditional on\nWe will see in Chapter 2 how to compare distributions using Q-Q plots (see Figure 2.8). We could have used a more standard test instead of running a hand-made simulation. However, the procedure we’ve learned extends to many situations in which the chi-square distribution doesn’t apply. For instance, when some of the boxes have extremely low probabilities and their counts are mostly zero.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "01-chap.html#summary-of-this-chapter",
    "href": "01-chap.html#summary-of-this-chapter",
    "title": "3  1.1 Goals for this chapter",
    "section": "3.4 1.5 Summary of this chapter",
    "text": "3.4 1.5 Summary of this chapter\nWe have used mathematical formulæ and R to compute probabilities of various discrete events , using a few basic distributions:\nThe Bernoulli distribution was our most basic building block – it is used to represent a single binary trial such as a coin flip. We can code the outcomes as 0 and 1. We call \\(p\\) the probability of success (the 1 outcome).\nThe binomial distribution is used for the number of 1s in \\(n\\) binary trials, and we can compute the probabilities of seeing \\(k\\) successes using the R function dbinom. We also saw how to simulate a binomial eperiment with \\(n\\) trials using the function rbinom.\nThe Poisson distribution is most appropriate for cases when \\(p\\) is small (the 1s are rare). It has only one parameter \\(\\), and the Poisson distribution for \\(=np\\) is approximately the same as the binomial distribution for \\((n,p)\\) if \\(p\\) is small. We used the Poisson distribution to model the number of randomly occurring false positives in an assay that tested for epitopes along a sequence, presuming that the per- position false positive rate \\(p\\) was small. We saw how such a parametric model enabled us to compute the probabilities of extreme events, as long as we knew all the parameters.\nThe multinomial distribution is used for discrete events that have more than two possible outcomes or levels. The power example showed us how to use Monte Carlo simulations to decide how much data we need to collect if we want to test whether a multinomial model with equal probabilities is consistent with the data. We used probability distributions and probabilistic models to evaluate hypotheses about how our data were generated, by making assumptions about the generative models. We term the probability of seeing the data, given a hypothesis, a p-value. This is not the same as the probability that the hypothesis is true!\n\n\n\nP(H_0\\;|\\;\\text{data}) is not the same as a p-value P(\\text{data}\\;|\\;H_0).\n\n\n\\(P(H_0;|;)\\) is not the same as a p-value \\(P(;|;H_0)\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "01-chap.html#further-reading",
    "href": "01-chap.html#further-reading",
    "title": "3  1.1 Goals for this chapter",
    "section": "3.5 1.6 Further reading",
    "text": "3.5 1.6 Further reading\n\nThe elementary book by Freedman, Pisani, and Purves (1997) provides the best introduction to probability through the type of box models we mention here.\nThe book by Durbin et al. (1998) covers many useful probability distributions and provides in its appendices a more complete view of the theoretical background in probability theory and its applications to sequences in biology.\nMonte Carlo methods are used extensively in modern statistics. Robert and Casella (2009) provides an introduction to these methods using R.\nChapter 6 will cover the subject of hypothesis testing. We also suggest Rice (2006) for more advanced material useful for the type of more advanced probability distributions, beta, gamma, exponentials we often use in data analyses.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "01-chap.html#exercises",
    "href": "01-chap.html#exercises",
    "title": "3  1.1 Goals for this chapter",
    "section": "3.6 1.7 Exercises",
    "text": "3.6 1.7 Exercises\n__\nExercise 1.1\nR can generate numbers from all known distributions. We now know how to generate random discrete data using the specialized R functions tailored for each type of distribution. We use the functions that start with an r as in rXXXX, where XXXX could be pois, binom, multinom. If we need a theoretical computation of a probability under one of these models, we use the functions dXXXX, such as dbinom, which computes the probabilities of events in the discrete binomial distribution, and dnorm, which computes the probability density function for the continuous normal distribution. When computing tail probabilities such as \\(P(X&gt;a)\\) it is convenient to use the cumulative distribution functions, which are called pXXXX. Find two other discrete distributions that could replace the XXXX above.\n__\nExercise 1.2\nIn this chapter we have concentrated on discrete random variables, where the probabilities are concentrated on a countable set of values. How would you calculate the probability mass at the value \\(X=2\\) for a binomial \\(B(10, 0.3)\\) with dbinom? Use dbinom to compute the cumulative distribution at the value 2, corresponding to \\(P(X)\\), and check your answer with another R function.\n__\nSolution\n__\ndbinom(2, size = 10, prob = 0.3)__\n\n\n[1] 0.2334744\n\n\npbinom(2, size = 10, prob = 0.3)__\n\n\n[1] 0.3827828\n\n\nsum(dbinom(0:2, size = 10, prob = 0.3)) __\n\n\n[1] 0.3827828\n__\nExercise 1.3\nWhenever we note that we keep needing a certain sequence of commands, it’s good to put them into a function. The function body contains the instructions that we want to do over and over again, the function arguments take those things that we may want to vary. Write a function to compute the probability of having a maximum as big as m when looking across n Poisson variables with rate lambda.\n__\nSolution\n__\npoismax = function(lambda, n, m) {\n  epsilon = 1 - ppois(m - 1, lambda)\n  1 - exp( -n * epsilon)\n}\npoismax(lambda = 0.5, n = 100, m = 7)__\n\n\n[1] 0.0001002329\n\n\npoismax(lambda = mean(e100), n = 100, m = 7)__\n\n\n[1] 0.0001870183\n__\nExercise 1.4\nRewrite the function to have default values for its arguments (i.e., values that are used by it if the argument is not specified in a call to the function).\n__\nSolution\n__\npoismax = function(lambda, n = 100, m = 7) {\n  1 - exp( -n * (1 - ppois(m - 1, lambda)))\n}\npoismax(0.5)__\n\n\n[1] 0.0001002329\n\n\npoismax(0.5, m = 9)__\n\n\n[1] 3.43549e-07\n__\nExercise 1.5\nIn the epitope example, use a simulation to find the probability of having a maximum of 9 or larger in 100 trials. How many simulations do you need if you would like to prove that “the probability is smaller than 0.000001”?\n__\nExercise 1.6\nUse ?Distributions in R to get a list of available distributions17. Make plots of the probability mass or density functions for various distributions (using the functions named dXXXX), and list five distributions that are not discrete.\n17 These are just the ones that come with a basic R installation. There are more in additional packages, see the CRAN task view: Probability Distributions.\n__\nExercise 1.7\nGenerate 100 instances of a Poisson(3) random variable. What is the mean? What is the variance as computed by the R function var?\n__\nExercise 1.8\nC. elegans genome nucleotide frequency: Is the mitochondrial sequence of C. elegans consistent with a model of equally likely nucleotides?\n\nExplore the nucleotide frequencies of chromosome M by using a dedicated function in the Biostrings package from Bioconductor.\nTest whether the C. elegans data is consistent with the uniform model (all nucleotide frequencies the same) using a simulation. Hint: This is our opportunity to use Bioconductor for the first time. Since Bioconductor’s package management is more tightly controlled than CRAN’s, we need to use a special install function (from the BiocManager package) to install Bioconductor packages.\n\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(c(\"Biostrings\", \"BSgenome.Celegans.UCSC.ce2\"))__\nAfter that, we can load the genome sequence package as we load any other R packages.\n__\nSolution\n__\nlibrary(\"BSgenome.Celegans.UCSC.ce2\")\nCelegans __\n\n\n| BSgenome object for Worm\n| - organism: Caenorhabditis elegans\n| - provider: UCSC\n| - genome: ce2\n| - release date: Mar. 2004\n| - 7 sequence(s):\n|     chrI   chrII  chrIII chrIV  chrV   chrX   chrM                       \n| \n| Tips: call 'seqnames()' on the object to get all the sequence names, call\n| 'seqinfo()' to get the full sequence info, use the '$' or '[[' operator to\n| access a given sequence, see '?BSgenome' for more information.\n\n\nseqnames(Celegans)__\n\n\n[1] \"chrI\"   \"chrII\"  \"chrIII\" \"chrIV\"  \"chrV\"   \"chrX\"   \"chrM\"  \n\n\nCelegans$chrM __\n\n\n13794-letter DNAString object\nseq: CAGTAAATAGTTTAATAAAAATATAGCATTTGGGTT...TATTTATAGATATATACTTTGTATATATCTATATTA\n\n\nclass(Celegans$chrM)__\n\n\n[1] \"DNAString\"\nattr(,\"package\")\n[1] \"Biostrings\"\n\n\nlength(Celegans$chrM)__\n\n\n[1] 13794\n\n\nlibrary(\"Biostrings\")\nlfM = letterFrequency(Celegans$chrM, letters=c(\"A\", \"C\", \"G\", \"T\"))\nlfM __\n\n\n   A    C    G    T \n4335 1225 2055 6179 \n\n\nsum(lfM)__\n\n\n[1] 13794\n\n\nlfM / sum(lfM)__\n\n\n         A          C          G          T \n0.31426707 0.08880673 0.14897782 0.44794838 \nCreate a random (each letter with equal probability) sequence of the same length as the C. elegans chromosome M:\nt(rmultinom(1, length(Celegans$chrM), p = rep(1/4, 4)))__\n\n\n     [,1] [,2] [,3] [,4]\n[1,] 3409 3486 3476 3423\nThe expected frequencies are just\nlength(Celegans$chrM) / 4 __\n\n\n[1] 3448.5\nWe’re going to compute a statistic that measures how close two multinomial outputs are to each other. We’ll take the average squared difference between observed (o) and expected (e) counts, scaled by e. We will call the function oestat.\noestat = function(o, e) {\n  sum((o-e)^2 / e)\n}\noe = oestat(o = lfM, e = length(Celegans$chrM) / 4)\noe __\n\n\n[1] 4386.634\nIs this larger than what randomness could explain? We already saw above a set of typical counts we could expect under the null model. But we need a whole set (distribution) of values. We compute these using the replicate function that evaluates a function many times. We run the following:\nB = 10000\nn = length(Celegans$chrM)\nexpected = rep(n / 4, 4)\noenull = replicate(B,\n  oestat(e = expected, o = rmultinom(1, n, p = rep(1/4, 4))))__\nDurbin, Richard, Sean Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis. Cambridge University Press.\nFreedman, David, Robert Pisani, and Roger Purves. 1997. Statistics. New York, NY: WW Norton.\nRice, John. 2006. Mathematical Statistics and Data Analysis. Cengage Learning.\nRobert, Christian, and George Casella. 2009. Introducing Monte Carlo Methods with R. Springer Science & Business Media.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "02-chap.html",
    "href": "02-chap.html",
    "title": "4  2.1 Goals for this chapter",
    "section": "",
    "text": "4.0.0.1 Parameters are the key.\nIn the previous chapter, the knowledge of both the generative model and the values of the parameters provided us with probabilities we could use for decision making – for instance, whether we had really found an epitope. In many real situations, neither the generative model nor the parameters are known, and we will need to estimate them using the data we have collected. Statistical modeling works from the data upwards to a model that might plausibly explain the data1. This upward-reasoning step is called statistical inference. This chapter will show us some of the distributions and estimation mechanisms that serve as building blocks for inference. Although the examples in this chapter are all parametric (i.e., the statistical models only have a small number of unknown parameters), the principles we discuss will generalize.\n1 Even if we have found a model that perfectly explains all our current data, it could always be that reality is more complex. A new set of data lets us conclude that another model is needed, and may include the current model as a special case or approximation.\nIn a statistical setting, we start with the data \\(X\\) and use them to estimate the parameters. These estimates are denoted by Greek letters with what we call hats on them, as in \\(\\).\nIn this chapter we will:\nExamples of parameters : the single parameter \\(\\) defines a Poisson distribution. The letter \\(\\) is often used for the mean of the normal. More generally, we use the Greek letter \\(\\) to designate a generic tuple of parameters necessary to specify a probability model. For instance, in the case of the binomial distribution, \\(=(n,p)\\) comprises two numbers, a positive integer and a real number between 0 and 1.\nWe saw in Chapter 1 that the knowledge of all the parameter values in the epitope example enabled us to use our probability model and test a null hypothesis based on the data we had at hand. We will see different approaches to statistical modeling through some real examples and computer simulations, but let’s start by making a distinction between two situations depending on how much information is available.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "02-chap.html#the-difference-between-statistical-and-probabilistic-models",
    "href": "02-chap.html#the-difference-between-statistical-and-probabilistic-models",
    "title": "4  2.1 Goals for this chapter",
    "section": "4.1 2.2 The difference between statistical and probabilistic models",
    "text": "4.1 2.2 The difference between statistical and probabilistic models\nA probabilistic analysis is possible when we know a good generative model for the randomness in the data, and we know its parameters’ actual values.\n\nFigure 2.1: The probabilistic model we obtained in Chapter 1. The data are represented as \\(x\\) in green. If we know the true value of \\(\\), then we can compute the probability of observing \\(x\\) for all possible instances of \\(x\\), in particular, for an \\(x\\) that we observed.\nIn the epitope example, knowing that false positives occur as Bernoulli(0.01) per position, the number of patient samples assayed and the length of the protein, meant that there were no unknown parameters.\nIn such a case, we can use mathematical deduction to compute the probability of an event as schematized in Figure 2.1. In the epitope examples, we used the Poisson probability as our null model with the given parameter \\(\\). We were able to conclude through mathematical deduction that the chances of seeing a maximum value of 7 or larger was around \\(10^{-4}\\) and thus that in fact the observed data were highly unlikely under that model (or “null hypothesis”).\nNow suppose that we know the number of patients and the length of the proteins (these are given by the experimental design) but not the distribution itself and the false positive rate. Once we observe data, we need to go up from the data to estimate both a probability model \\(F\\) (Poisson, normal, binomial) and eventually the missing parameter(s) for that model. This is the type of statistical inference we will explain in this chapter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "02-chap.html#a-simple-example-of-statistical-modeling",
    "href": "02-chap.html#a-simple-example-of-statistical-modeling",
    "title": "4  2.1 Goals for this chapter",
    "section": "4.2 2.3 A simple example of statistical modeling",
    "text": "4.2 2.3 A simple example of statistical modeling\n\n4.2.0.1 Start with the data\nThere are two parts to the modeling procedure. First we need a reasonable probability distribution to model the data generation process. As we saw in Chapter 1, discrete count data may be modeled by simple probability distributions such as binomial, multinomial or Poisson distributions. The normal distribution, or bell shaped curve, is often a good model for continuous measurements. Distributions can also be more complicated mixtures of these elementary ones (more on this in Chapter 4).\nLet’s revisit the epitope example from the previous chapter, starting without the tricky outlier.\nload(\"../data/e100.RData\")\ne99 = e100[-which.max(e100)]__\n\n\n4.2.0.2 Goodness-of-fit : visual evaluation\nOur first step is to find a fit from candidate distributions; this requires consulting graphical and quantitative goodness-of-fit plots. For discrete data, we can plot a barplot of frequencies (for continuous data, we would look at the histogram) as in Figure 2.2.\nbarplot(table(e99), space = 0.8, col = \"chartreuse4\")__\n\nFigure 2.2: The observed distribution of the epitope data without the outlier.\nHowever, it is hard to decide which theoretical distribution fits the data best without using a comparison. One visual goodness-of-fit diagram is known as the rootogram (Cleveland 1988); it hangs the bars with the observed counts from the theoretical red points. If the counts correspond exactly to their theoretical values, the bottom of the boxes will align exactly with the horizontal axis.\nlibrary(\"vcd\")\ngf1 = goodfit( e99, \"poisson\")\nrootogram(gf1, xlab = \"\", rect_gp = gpar(fill = \"chartreuse4\"))__\n\nFigure 2.3: Rootogram showing the square root of the theoretical values as red dots and the square root of the observed frequencies as drop down rectangles. (We’ll see a bit below how the goodfit function decided which \\(\\) to use.)\n__\nQuestion 2.1\nTo calibrate what such a plot looks like with a known Poisson variable, use rpois with \\(\\) = 0.05 to generate 100 Poisson distributed numbers and draw their rootogram.\n__\nSolution\n__\nsimp = rpois(100, lambda = 0.05)\ngf2 = goodfit(simp, \"poisson\")\nrootogram(gf2, xlab = \"\")__\nWe see that the rootogram for e99 seems to fit the Poisson model reasonably well. But remember, to make this happen we removed the outlier. The Poisson is completely determined by one parameter, often called the Poisson mean \\(\\). In most cases where we can guess the data follows a Poisson distribution, we will need to estimate the Poisson parameter from the data.\n\n\n\nThe parameter is called the Poisson mean because it is the mean of the theoretical distribution and, as it turns out, is estimated by the sample mean. This overloading of the word is confusing to everyone.\n\n\nThe parameter is called the Poisson mean because it is the mean of the theoretical distribution and , as it turns out, is estimated by the sample mean. This overloading of the word is confusing to everyone.\nThe most common way of estimating \\(\\) is to choose the value \\(\\) that makes the observed data the most likely. This is called the maximum likelihood estimator (Rice 2006, chap. 8, Section 5), often abbreviated MLE. We will illustrate this rather paradoxical idea in the next section.\nAlthough we above took out the extreme observation before taking a guess at the probability distribution, we are going to return to the data with it for the rest of our analysis. In practice we would not know whether there is an outlier, and which data point(s) it is / they are. The effect of leaving it in is to make our estimate of the mean higher. In turns this would make it more likely that we’d observe a value of 7 under the null model, resulting in a larger p-value. So, if the resulting p-value is small even with the outlier included, we are assured that our analysis is up to something real. We call such a tactic being conservative : we err on the side of caution, of not detecting something.\n\n\n4.2.0.3 Estimating the parameter of the Poisson distribution\nWhat value for the Poisson mean makes the data the most probable? In a first step, we tally the outcomes.\ntable(e100)__\n\n\ne100\n 0  1  2  7 \n58 34  7  1 \nThen we are going to try out different values for the Poisson mean and see which one gives the best fit to our data. If the mean \\(\\) of the Poisson distribution were 3, the counts would look something like this:\ntable(rpois(100, 3))__\n\n\n 0  1  2  3  4  5  6  7 \n 4 12 23 24 14 16  4  3 \nwhich has many more 2’s and 3’s than we see in our data. So we see that \\(\\) is unlikely to have produced our data, as the counts do not match up so well.\n__\nQuestion 2.2\nRepeat this simulation with different values of \\(\\). Can you find one that gives counts close to the observed ones just by trial and error?\nSo we could try out many possible values and proceed by brute force. However, we’ll do something more elegant and use a little mathematics to see which value maximizes the probability of observing our data. Let’s calculate the probability of seeing the data if the value of the Poisson parameter is \\(m\\). Since we suppose the data derive from independent draws, this probability is simply the product of individual probabilities:\n\\[\\[\\begin{equation*} \\begin{aligned} P(58 \\times 0, 34 \\times 1, 7 \\times 2,\n\\text{one }7 \\;|\\; \\text{data are Poisson}(m)) = P(0)^{58}\\times\nP(1)^{34}\\times P(2)^{7}\\times P(7)^{1}.\\end{aligned} \\end{equation*}\\]\\]\nFor \\(m=3\\) we can compute this2.\n2 Note how we here use R’s vectorization: the call to dpois returns four values, corresponding to the four different numbers. We then take these to the powers of 58, 34, 7 and 1, respectively, using the ^ operator, resulting again in four values. Finally, we collapse them into one number, the product, with the prod function.\nprod(dpois(c(0, 1, 2, 7), lambda = 3) ^ (c(58, 34, 7, 1)))__\n\n\n[1] 1.392143e-110\n__\nQuestion 2.3\nCompute the probability as above for \\(m=0,1,2\\). Does \\(m\\) have to be integer? Try computing the probability for \\(m=0.4\\) for example.\n__\nSolution\n__\nprod(dpois(c(0, 1, 2, 7), lambda = 0.4) ^ (c(58, 34, 7, 1)))__\n\n\n[1] 8.5483e-46\nThis probability is the likelihood function of \\(\\), given the data, and we write it\nHere \\(L\\) stands for likelihood and \\(f(k)=e^{-} ,^k,/,k!\\), the Poisson probability we saw earlier.\n\\[ L(,,x=(k_1,k_2,k_3,…))=_{i=1}^{100}f(k_i) \\]\nInstead of working with multiplications of a hundred small numbers, it is convenient3 to take the logarithm. Since the logarithm is strictly increasing, if there is a point where the logarithm achieves its maximum within an interval it will also be the maximum for the probability.\n3 That’s usually true both for pencil and paper and for computer calculations.\n4 Here we again use R’s vector syntax that allows us to write the computation without an explicit loop over the data points. Compared to the code above, here we call dpois on each of the 100 data points, rather than tabulating data with the table function before calling dpois only on the distinct values. This is a simple example for alternative solutions whose results are equivalent, but may differ in how easy it is to read the code or how long it takes to execute.\nLet’s start with a computational illustration. We compute the likelihood for many different values of the Poisson parameter. To do this we need to write a small function that computes the probability of the data for different values4.\nloglikelihood  =  function(lambda, data = e100) {\n  sum(log(dpois(data, lambda)))\n}__\nNow we can compute the likelihood for a whole series of lambda values from 0.05 to 0.95 (Figure 2.4).\nlambdas = seq(0.05, 0.95, length = 100)\nloglik = vapply(lambdas, loglikelihood, numeric(1))\nplot(lambdas, loglik, type = \"l\", col = \"red\", ylab = \"\", lwd = 2,\n     xlab = expression(lambda))\nm0 = mean(e100)\nabline(v = m0, col = \"blue\", lwd = 2)\nabline(h = loglikelihood(m0), col = \"purple\", lwd = 2)\nm0 __\n\n\n[1] 0.55\n\nFigure 2.4: The red curve is the log-likelihood function. The vertical line shows the value of m (the mean) and the horizontal line the log-likelihood of m. It looks like m maximizes the likelihood.\n__\nQuestion 2.4\nWhat does the vapply function do in the above code? Hint: check its manual page.\n__\nSolution\n__\nvapply takes its first argument, the vector lambdas in this case, and iteratively applies the function loglikelihood (its second argument) to each of the vector elements. As a result, it returns a vector of the results. The function also needs a third argument, numeric(1) in this case, that specifies what type of value each individual call to loglikelihood is supposed to return: a single number. (In general, it could happen that the function sometimes returns something else, say, a character string, or two numbers; in that case it would not be possible to assemble the overall results into a coherent vector, and vapply would complain.)\nIn fact there is a shortcut: the function goodfit.\ngf  =  goodfit(e100, \"poisson\")\nnames(gf)__\n\n\n[1] \"observed\" \"count\"    \"fitted\"   \"type\"     \"method\"   \"df\"       \"par\"     \n\n\ngf$par __\n\n\n$lambda\n[1] 0.55\nThe output of goodfit is a composite object called a list. One of its components is called par and contains the value(s) of the fitted parameter(s) for the distribution studied. In this case it’s only one number, the estimate of \\(\\).\n__\nQuestion 2.5\nWhat are the other components of the output from the goodfit function?\n__\nTask\nCompare the value of m to the value that we used previously for \\(\\), 0.5. Redo the modeling that we did in Chapter 1 with m instead of 0.5.\n\n\n4.2.1 2.3.1 Classical statistics for classical data\nHere is a formal proof of our computational finding that the sample mean maximizes the (log-)likelihood.\n\\[ \\[\\begin{align} \\log L(\\lambda, x) &= \\sum_{i=1}^{100} - \\lambda +\nk_i\\log\\lambda - \\log(k_i!) \\\\\\ &= -100\\lambda +\n\\log\\lambda\\left(\\sum_{i=1}^{100}k_i\\right) + \\text{const.} \\end{align}\\] \\]\nWe use the catch-all “const.” for terms that do not depend on \\(\\) (although they do depend on \\(x\\), i.e., on the \\(k_i\\)). To find the \\(\\) that maximizes this, we compute the derivative in \\(\\) and set it to zero.\n\\[ \\[\\begin{align} \\frac{d}{d\\lambda}\\log L &= -100 + \\frac{1}{\\lambda}\n\\sum_{i=1}^{100}k_i \\stackrel{?}{=}0 \\\\\\ \\lambda &= \\frac{1}{100}\n\\sum_{i=1}^{100}k_i = \\bar{k} \\end{align}\\] \\]\nYou have just seen the first steps of a statistical approach , starting `from the ground up’ (from the data) to infer the model parameter(s): this is statistical estimation of a parameter from data. Another important component will be choosing which family of distributions we use to model our data; that part is done by evaluating the goodness of fit. We will encounter this later.\nIn the classical statistical testing framework, we consider one single model, that we call the null model , for the data. The null model formulates an “uninteresting” baseline, such as that all observations come from the same random distribution regardless of which group or treatment they are from. We then test whether there is something more interesting going on by computing the probability that the data are compatible with that model. Often, this is the best we can do, since we do not know in sufficient detail what the “interesting”, non-null or alternative model should be. In other situations, we have two competing models that we can compare, as we will see later.\n__\nQuestion 2.6\nWhat is the value of modeling with a known distribution? For instance, why is it interesting to know a variable has a Poisson distribution ?\n__\nSolution\n__\nModels are concise but expressive representations of the data generating process. For the Poisson for instance, knowing one number allows us to know everything about the distribution, including, as we saw earlier, the probabilities of extreme or rare events.\nAnother useful direction is regression. We may be interested in knowing how our count-based response variable (e.g., the result of counting sequencing reads) depends on a continuous covariate, say, temperature or nutrient concentration. You may already have encountered linear regression, where our model is that the response variable \\(y\\) depends on the covariate \\(x\\) via the equation \\(y = ax+b + e\\), with parameters \\(a\\) and \\(b\\) (that we need to estimate), and with residuals \\(e\\) whose probability model is a normal distribution (whose variance we usually also need to estimate). For count data the same type of regression model is possible, although the probability distribution for the residuals then needs to be non-normal. In that case we use the generalized linear models framework. We will see examples when studying RNA-Seq in Chapter 8 and another type of next generation sequencing data, 16S rRNA data, in Chapter 9.\nKnowing that our probability model involves a Poisson, binomial, multinomial distribution or another parametric family will enable us to have quick answers to questions about the parameters of the model and compute quantities such as p-values and confidence intervals.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "02-chap.html#binomial-distributions-and-maximum-likelihood",
    "href": "02-chap.html#binomial-distributions-and-maximum-likelihood",
    "title": "4  2.1 Goals for this chapter",
    "section": "4.3 2.4 Binomial distributions and maximum likelihood",
    "text": "4.3 2.4 Binomial distributions and maximum likelihood\nIn a binomial distribution there are two parameters: the number of trials \\(n\\), which is typically known, and the probability \\(p\\) of seeing a 1 in a trial. This probability is often unknown.\n\n4.3.1 2.4.1 An example\nSuppose we take a sample of \\(n=120\\) males and test them for red-green colorblindness. We can code the data as 0 if the subject is not colorblind and 1 if he is. We summarize the data by the table:\ntable(cb)__\n\n\ncb\n  0   1 \n110  10 \n__\nQuestion 2.7\nWhich value of \\(p\\) is the most likely given these data?\n__\nSolution\n__\n\\(=\\).\nmean(cb)__\n\n\n[1] 0.08333333\n\n\n\nHowever, be careful: sometimes, maximum likelihood estimates are harder to guess and to compute, as well as being much less intuitive (see Exercise imp- models-mlmax).\n\n\nHowever, be careful: sometimes, maximum likelihood estimates are harder to guess and to compute, as well as being much less intuitive (see Exercise 2.2).\nIn this special case, your intuition may give you the estimate \\(=\\), which turns out to be the maximum likelihood estimate. We put a hat over the letter to remind us that this is not (necessarily) the underlying true value, but an estimate we make from the data.\nAs before in the case of the Poisson, if we compute the likelihood for many possible \\(p\\), we can plot it and see where its maximum falls (Figure 2.5).\nprobs  =  seq(0, 0.3, by = 0.005)\nlikelihood = dbinom(sum(cb), prob = probs, size = length(cb))\nplot(probs, likelihood, pch = 16, xlab = \"probability of success\",\n       ylab = \"likelihood\", cex=0.6)\nprobs[which.max(likelihood)]__\n\n\n[1] 0.085\n\nFigure 2.5: Plot of the likelihood as a function of the probabilities. The likelihood is a function on \\([0, 1]\\). Here we have zoomed into the range of \\([0, 0.3]\\), as the likelihood is practically zero for larger values of \\(p\\).\nNote: 0.085 is not exactly the value we expected \\(()\\), and that is because the set of values that we tried (in probs) did not include the exact value of \\(\\), so we obtained the next best one. We could use numeric optimisation methods to overcome that.\n\n\n4.3.2 2.4.2 Likelihood for the binomial distribution\n\n\n\nOne can come up with different criteria than maximum likelihood, which lead to other estimators. They all carry hats. We’ll see other examples in sec- mixtures.\n\n\nOne can come up with different criteria than maximum likelihood, which lead to other estimators. They all carry hats. We’ll see other examples in Chapter 4.\nThe likelihood and the probability are the same mathematical function, only interpreted in different ways – in one case, the function tells us how probable it is to see a particular set of values of the data, given the parameters; in the other case, we consider the data as given, and ask for the parameter value(s) that likely generated these data. Suppose \\(n=300\\), and we observe \\(y=40\\) successes. Then, for the binomial distribution:\n\\[ f(p,|,n,y) = f(y,|,n,p)={n y} , p^y , (1-p)^{(n-y)}. \\]\nAgain, it is more convenient to work with the logarithm of the likelihood,\n\\[ f(p |y) = + y(p) + (n-y)(1-p). \\]\nHere’s a function we can use to calculate it5,\n5 In practice, we would try to avoid explicitly computing choose(n, y), since it can be a very large number that tests the limits of our computer’s floating point arithmetic (for n=300 and y=40, it is around 9.8e+49). One could approximate the term using Stirling’s formula, or indeed ignore it, as it is only an additive offset independent of \\(p\\) that does not impact the maximization.\nloglikelihood = function(p, n = 300, y = 40) {\n  log(choose(n, y)) + y * log(p) + (n - y) * log(1 - p)\n}__\nwhich we plot for the range of \\(p\\) from 0 to 1 (Figure 2.6).\np_seq = seq(0, 1, by = 0.001)\nplot(p_seq, loglikelihood(p_seq), xlab = \"p\", ylab = \"log f(p|y)\", type = \"l\")__\n\nFigure 2.6: Plot of the log likelihood function for \\(n=300\\) and \\(y=40\\).\nThe maximum lies at 40/300 = 0.1333… , consistent with intuition, but we see that other values of \\(p\\) are almost equally likely, as the function is quite flat around the maximum. We will see in a later section how Bayesian methods enable us to work with a range of values for \\(p\\) instead of just picking a single maximum.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "02-chap.html#more-boxesmultinomial-data",
    "href": "02-chap.html#more-boxesmultinomial-data",
    "title": "4  2.1 Goals for this chapter",
    "section": "4.4 2.5 More boxes:multinomial data",
    "text": "4.4 2.5 More boxes:multinomial data\n\n4.4.1 2.5.1 DNA count modeling: base pairs\nThere are four basic molecules of DNA: A - adenine, C - cytosine, G - guanine, T - thymine. The nucleotides are classified into 2 groups: purines (A and G) and pyrimidines (C and T). The binomial would work as a model for the purine/pyrimidine groupings but not if we want to use A, C, G, T; for that we need the multinomial model from Section 1.4. Let’s look at noticeable patterns that occur in these frequencies.\n\n\n4.4.2 2.5.2 Nucleotide bias\nThis section combines estimation and testing by simulation in a real example. Data from one strand of DNA for the genes of Staphylococcus aureus bacterium are available in a fasta file staphsequence.ffn.txt, which we can read with a function from the Bioconductor package Biostrings.\nlibrary(\"Biostrings\")\nstaph = readDNAStringSet(\"../data/staphsequence.ffn.txt\", \"fasta\")__\nLet’s look at the first gene:\nstaph[1]__\n\n\nDNAStringSet object of length 1:\n    width seq                                               names               \n[1]  1362 ATGTCGGAAAAAGAAATTTGGGA...AAAAAGAAATAAGAAATGTATAA lcl|NC_002952.2_c...\n\n\nletterFrequency(staph[[1]], letters = \"ACGT\", OR = 0)__\n\n\n  A   C   G   T \n522 219 229 392 \n__\nQuestion 2.8\nWhy did we use double square brackets in the second line?\n__\nSolution\n__\nThe double square brackets [[i]] extract the sequence of the i-th gene as a DNAString , as opposed to the pair of single brackets [i], which return a DNAStringSet with just a single DNAString in it. If you look at the length of staph[1], it is 1, whereas staph[[1]] has length 1362.\n__\nQuestion 2.9\nFollowing a similar procedure as in Exercise 1.8, test whether the nucleotides are equally distributed across the four nucleotides for this first gene.\nDue to their different physical properties, evolutionary selection can act on the nucleotide frequencies. So we can ask whether, say, the first ten genes from these data come from the same multinomial. We do not have a prior reference, we just want to decide whether the nucleotides occur in the same proportions in the first 10 genes. If not, this would provide us with evidence for varying selective pressure on these ten genes.\nletterFrq = vapply(staph, letterFrequency, FUN.VALUE = numeric(4),\n         letters = \"ACGT\", OR = 0)\ncolnames(letterFrq) = paste0(\"gene\", seq(along = staph))\ntab10 = letterFrq[, 1:10]\ncomputeProportions = function(x) { x/sum(x) }\nprop10 = apply(tab10, 2, computeProportions)\nround(prop10, digits = 2)__\n\n\n  gene1 gene2 gene3 gene4 gene5 gene6 gene7 gene8 gene9 gene10\nA  0.38  0.36  0.35  0.37  0.35  0.33  0.33  0.34  0.38   0.27\nC  0.16  0.16  0.13  0.15  0.15  0.15  0.16  0.16  0.14   0.16\nG  0.17  0.17  0.23  0.19  0.22  0.22  0.20  0.21  0.20   0.20\nT  0.29  0.31  0.30  0.29  0.27  0.30  0.30  0.29  0.28   0.36\n\n\np0 = rowMeans(prop10)\np0 __\n\n\n        A         C         G         T \n0.3470531 0.1518313 0.2011442 0.2999714 \nSo let’s suppose p0 is the vector of multinomial probabilities for all the ten genes and use a Monte Carlo simulation to test whether the departures between the observed letter frequencies and expected values under this supposition are within a plausible range.\nWe compute the expected counts by taking the outer product of the vector of probabilities p0 with the sums of nucleotide counts from each of the 10 columns, cs.\ncs = colSums(tab10)\ncs __\n\n\n gene1  gene2  gene3  gene4  gene5  gene6  gene7  gene8  gene9 gene10 \n  1362   1134    246   1113   1932   2661    831   1515   1287    696 \n\n\nexpectedtab10 = outer(p0, cs, FUN = \"*\")\nround(expectedtab10)__\n\n\n  gene1 gene2 gene3 gene4 gene5 gene6 gene7 gene8 gene9 gene10\nA   473   394    85   386   671   924   288   526   447    242\nC   207   172    37   169   293   404   126   230   195    106\nG   274   228    49   224   389   535   167   305   259    140\nT   409   340    74   334   580   798   249   454   386    209\nWe can now create a random table with the correct column sums using the rmultinom function. This table is generated according to the null hypothesis that the true proportions are given by p0.\nrandomtab10 = sapply(cs, function(s) { rmultinom(1, s, p0) } )\nall(colSums(randomtab10) == cs)__\n\n\n[1] TRUE\nNow we repeat this B = 1000 times. For each table we compute our test statistic from Section 1.4.1 in Chapter 1 (the function stat) and store the results in the vector simulstat. Together, these values constitute our null distribution, as they were generated under the null hypothesis that p0 is the vector of multinomial proportions for each of the 10 genes.\nstat = function(obsvd, exptd) {\n   sum((obsvd - exptd)^2 / exptd)\n}\nB = 1000\nsimulstat = replicate(B, {\n  randomtab10 = sapply(cs, function(s) { rmultinom(1, s, p0) })\n  stat(randomtab10, expectedtab10)\n})\nS1 = stat(tab10, expectedtab10)\nsum(simulstat &gt;= S1)__\n\n\n[1] 0\n\n\nhist(simulstat, col = \"lavender\", breaks = seq(0, 75, length.out=50))\nabline(v = S1, col = \"red\")\nabline(v = quantile(simulstat, probs = c(0.95, 0.99)),\n       col = c(\"darkgreen\", \"blue\"), lty = 2)__\n\nFigure 2.7: Histogram of simulstat. The value of S1 is marked by the vertical red line, those of the 0.95 and 0.99 quantiles (see next section) by the dotted lines.\nThe histogram is shown in Figure 2.7. We see that the probability of seeing a value as large as S1=70.1 is very small under the null model. It happened 0 times in our 1000 simulations that a value as big as S1 occurred. Thus the ten genes do not seem to come from the same multinomial model.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "02-chap.html#the-2-distribution",
    "href": "02-chap.html#the-2-distribution",
    "title": "4  2.1 Goals for this chapter",
    "section": "4.5 2.6 The \\(^2\\) distribution",
    "text": "4.5 2.6 The \\(^2\\) distribution\nIn fact, we could have used statistical theory to come to the same conclusion without running these simulations. The theoretical distribution of the simulstat statistic is called the \\(^2\\) (chi-squared) distribution6 with parameter 30 (\\(=10(4-1)\\)). We can use this for computing the probability of having a value as large as S1 \\(=\\) 70.1. As we just saw above, small probabilities are difficult to compute by Monte Carlo: the granularity of the computation is \\(1/B\\), so we cannot estimate any probabilities smaller than that, and in fact the uncertainty of the estimate is larger. So if any theory is applicable, that tends to be useful. We can check how well theory and simulation match up in our case using another visual goodness-of-fit tool: the quantile-quantile (QQ) plot. When comparing two distributions, whether from two different samples or from one sample versus a theoretical model, just looking at histograms is not informative enough. We use a method based on the quantiles of each of the distributions.\n6 Strictly speaking, the distribution of simulstat is approximately described by a \\(^2\\) distribution; the approximation is particularly good if the counts in the table are large.\n\n4.5.1 2.6.1 Intermezzo: quantiles and the quantile-quantile plot\nIn the previous chapter, we ordered the 100 sample values \\(x_{(1)},x_{(2)},…,x_{(100)}\\). Say we want the 22nd percentile. We can take any value between the 22nd and the 23rd value, i.e., any value that fulfills \\(x_{(22)} c_{0.22} &lt; x_{(23)}\\) is acceptable as a 0.22 quantile (\\(c_{0.22}\\)). In other words, \\(c_{0.22}\\) is defined by\n\\[ = 0.22. \\]\nIn Section 3.6.7, we’ll introduce the empirical cumulative distribution function (ECDF) \\(\\), and we’ll see that our definition of \\(c_{0.22}\\) can also be written as \\(n(c{0.22}) = 0.22\\). In Figure 2.7, our histogram of the distribution of simulstat, the quantiles \\(c_{0.95}\\) and \\(c_{0.99}\\) are also shown.\n__\nQuestion 2.10\n\nCompare the simulstat values and 1000 randomly generated \\(^2_{30}\\) random numbers by displaying them in histograms with 50 bins each.\nCompute the quantiles of the simulstat values and compare them to those of the \\(_{30}^2\\) distribution. Hint:\n\nqs = ppoints(100)\nquantile(simulstat, qs)\nquantile(qchisq(qs, df = 30), qs)__\n\n\n\nA name collision occurs here. Statisticians call the summary statistic we just computed as simulstat (sum of squares of weighted differences), the chi-squared or \\chi^2 statistic. The theoretical distribution \\chi^2_\\nu is a distribution in its own right, with a parameter \\nu called the degrees of freedom. When reading about the chi-squared or \\chi^2, you will need to pay attention to the context to see which meaning is appropriate.\n\n\nA name collision occurs here. Statisticians call the summary statistic we just computed as simulstat (sum of squares of weighted differences), the chi- squared or \\(^2\\) statistic. The theoretical distribution \\(^2_\\) is a distribution in its own right, with a parameter \\(\\) called the degrees of freedom. When reading about the chi-squared or \\(^2\\), you will need to pay attention to the context to see which meaning is appropriate.\n__\nQuestion 2.11\nDo you know another name for the 0.5 quantile?\n__\nSolution\n__\nThe median.\n__\nQuestion 2.12\nIn the above definition, we were a little vague on how the quantile is defined in general, i.e., not just for 0.22. How is the quantile computed for any number between 0 and 1, including ones that are not multiples of \\(1/n\\)?\n__\nSolution\n__\nCheck the manual page of the quantile function and its argument named type.\nNow that we have an idea what quantiles are, we can do the quantile-quantile plot. We plot the quantiles of the simulstat values, which we simulated under the null hypothesis, against the theoretical null distribution \\(^2_{30}\\) (Figure 2.8):\nqqplot(qchisq(ppoints(B), df = 30), simulstat, main = \"\",\n  xlab = expression(chi[nu==30]^2), asp = 1, cex = 0.5, pch = 16)\nabline(a = 0, b = 1, col = \"red\")__\n\nFigure 2.8: Our simulated statistic’s distribution compared to \\({30}^2\\) using a quantile-quantile (QQ) plot, which shows the theoretical quantiles for the \\(^2{30}\\) distribution on the horizontal axis and the sampled ones on the vertical axis.\nHaving convinced ourselves that simulstat is well described by a \\(^2_{30}\\) distribution, we can use that to compute our p-value, i.e., the probability that under the null hypothesis (counts are distributed as multinomial with probabilities \\(p_{} = 0.35\\), \\(p_{} = 0.15\\), \\(p_{} = 0.2\\), \\(p_{} = 0.3\\)) we observe a value as high as S1=70.1:\n1 - pchisq(S1, df = 30)__\n\n\n[1] 4.74342e-05\nWith such a small p-value, the null hypothesis seems improbable. Note how this computation did not require the 1000 simulations and was faster.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "02-chap.html#chargaffs-rule",
    "href": "02-chap.html#chargaffs-rule",
    "title": "4  2.1 Goals for this chapter",
    "section": "4.6 2.7 Chargaff’s Rule",
    "text": "4.6 2.7 Chargaff’s Rule\nThe most important pattern in the nucleotide frequencies was discovered by Chargaff (Elson and Chargaff 1952).\n\nLong before DNA sequencing was available, using the weight of the molecules, he asked whether the nucleotides occurred at equal frequencies. He called this the tetranucleotide hypothesis. We would translate that into asking whether \\(p_{} = p_{} = p_{} = p_{}\\).\nUnfortunately, Chargaff only published the percentages of the mass present in different organisms for each of the nucleotides, not the measurements themselves.\nload(\"../data/ChargaffTable.RData\")\nChargaffTable __\n\n\n                  A    T    C    G\nHuman-Thymus   30.9 29.4 19.9 19.8\nMycobac.Tuber  15.1 14.6 34.9 35.4\nChicken-Eryth. 28.8 29.2 20.5 21.5\nSheep-liver    29.3 29.3 20.5 20.7\nSea Urchin     32.8 32.1 17.7 17.3\nWheat          27.3 27.1 22.7 22.8\nYeast          31.3 32.9 18.7 17.1\nE.coli         24.7 23.6 26.0 25.7\n\nFigure 2.9: Barplots for the different rows in ChargaffTable. Can you spot the pattern?\n__\nQuestion 2.13\n\nDo these data seem to come from equally likely multinomial categories?\nCan you suggest an alternative pattern? puted from simulations u\nCan you do a quantitative analysis of the pattern, perhaps inspired by the simulations above?\n\n__\nSolution\n__\nChargaff saw the answer to this question and postulated a pattern called base pairing , which ensured a perfect match of the amount of adenine (A) in the DNA of an organism to the amount of thymine (T). Similarly, whatever the amount of guanine (G), the amount of cytosine (C) would be the same. This is now called Chargaff’s rule. On the other hand, the amount of C/G in an organism could be quite different from that of A/T, with no obvious pattern across organisms. Based on Chargaff’s rule, we might define a statistic\n\\[ (p_{} - p_{})^2 + (p_{} - p_{})^2, \\]\nsummed over all rows of the table. We are going to look at a comparison between the data and what would occur if the nucleotides were ‘exchangeable’, in the sense that the probabilities observed in each row were in no particular order, so that there were no special relationship between the proportions of As and Ts, or between those of Cs and Gs.\nstatChf = function(x){\n  sum((x[, \"C\"] - x[, \"G\"])^2 + (x[, \"A\"] - x[, \"T\"])^2)\n}\nchfstat = statChf(ChargaffTable)\npermstat = replicate(100000, {\n     permuted = t(apply(ChargaffTable, 1, sample))\n     colnames(permuted) = colnames(ChargaffTable)\n     statChf(permuted)\n})\npChf = mean(permstat &lt;= chfstat)\npChf __\n\n\n[1] 0.00014\n\n\nhist(permstat, breaks = 100, main = \"\", col = \"lavender\")\nabline(v = chfstat, lwd = 2, col = \"red\")__\n\nFigure 2.10: Histogram of our statistic statChf computed from simulations using per-row permutations of the columns. The value it yields for the observed data is shown by the red line.\nThe histogram in Figure 2.10 shows that it is quite rare to have a value as small as the observed 11.1, where the red line is drawn. The probability of observing a value as small or smaller is pChf=1.4^{-4}. Thus the data strongly support Chargaff’s insight.\n__\nQuestion 2.14\nWhen computing pChf, we only looked at the values in the null distribution smaller than the observed value. Why did we do this in a one-sided way here?\n\n4.6.1 2.7.1 Two categorical variables\nUp to now, we have visited cases where the data are taken from a sample that can be classified into different boxes: the binomial for Yes/No binary boxes and the multinomial distribution for categorical variables such as A, C, G, T or different genotypes such aa, aA, AA. However it might be that we measure two (or more) categorical variables on a set of subjects, for instance eye color and hair color. We can then cross-tabulate the counts for every combination of eye and hair color. We obtain a table of counts called a contingency table. This concept is very useful for many biological data types.\nHairEyeColor[,, \"Female\"]__\n\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    36    9     5     2\n  Brown    66   34    29    14\n  Red      16    7     7     7\n  Blond     4   64     5     8\n__\nQuestion 2.15\nExplore the HairEyeColor object in R. What data type, shape and dimensions does it have?\n__\nSolution\n__\nIt is a numeric array with three dimensions:\nstr(HairEyeColor)__\n\n\n 'table' num [1:4, 1:4, 1:2] 32 53 10 3 11 50 10 30 10 25 ...\n - attr(*, \"dimnames\")=List of 3\n  ..$ Hair: chr [1:4] \"Black\" \"Brown\" \"Red\" \"Blond\"\n  ..$ Eye : chr [1:4] \"Brown\" \"Blue\" \"Hazel\" \"Green\"\n  ..$ Sex : chr [1:2] \"Male\" \"Female\"\n\n\n## ?HairEyeColor __\n\n4.6.1.1 Color blindness and sex\nDeuteranopia is a form of red-green color blindness due to the fact that medium wavelength sensitive cones (green) are missing. A deuteranope can only distinguish 2 to 3 different hues, whereas somebody with normal vision sees 7 different hues. A survey for this type of color blindness in human subjects produced a two-way table crossing color blindness and sex.\nload(\"../data/Deuteranopia.RData\")\nDeuteranopia __\n\n\n          Men Women\nDeute      19     2\nNonDeute 1981  1998\nHow do we test whether there is a relationship between sex and the occurrence of color blindness? We postulate the null model with two independent binomials: one for sex and one for color blindness. Under this model we can estimate all the cells’ multinomial probabilities, and we can compare the observed counts to the expected ones. This is done through the chisq.test function in R.\nchisq.test(Deuteranopia)__\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  Deuteranopia\nX-squared = 12.255, df = 1, p-value = 0.0004641\nThe small p value tells us that we should expect to see such a table with only a very small probability under the null model – i.e., if the fractions of deuteranopic color blind among women and men were the same.\nWe’ll see another test for this type of data called Fisher’s exact test (also known as the hypergeometric test) in Section 10.3.2. This test is widely used for testing the over-representations of certain types of genes in a list of significantly expressed ones.\n\n\n\n4.6.2 2.7.2 A special multinomial: Hardy-Weinberg equilibrium\nHere we highlight the use of a multinomial with three possible levels created by combining two alleles M and N. Suppose that the overall frequency of allele M in the population is \\(p\\), so that of N is \\(q = 1-p\\). The Hardy- Weinberg model looks at the relationship between \\(p\\) and \\(q\\) if there is independence of the frequency of both alleles in a genotype, the so-called Hardy-Weinberg equilibrium (HWE). This would be the case if there is random mating in a large population with equal distribution of the alleles among sexes. The probabilities of the three genotypes are then as follows:\n\\[ p_{}=p2,p_{}=q2,p_{}=2pq \\]\nWe only observe the frequencies \\((n_{},,n_{},,n_{})\\) for the genotypes MM, MN, NN and the total number \\(S=n_{}+ n_{}+n_{}\\). We can write the likelihood, i.e., the probability of the observed data when the probabilities of the categories are given by Equation 2.5, using the multinomial formula\n\\[ P(n_{},,n_{},,n_{};|;p) = {S n_{},n_{},n_{}} (p2){n_{}} ,, (2pq)^{n_{}} ,, (q2){n_{}}, \\]\nand the log-likelihood under HWE\n\\[ L(p)=n_{}(p^2)+n_{} (2pq)+n_{}(q^2). \\]\nThe value of \\(p\\) that maximizes the log-likelihood is\n\\[ p = . \\]\nSee (Rice 2006, chap. 8, Section 5) for the proof. Given the data \\((n_{},,n_{},,n_{})\\), the log-likelihood \\(L\\) is a function of only one parameter, \\(p\\). Figure 2.11 shows this log-likelihood function for different values of \\(p\\) for the 216th row of the Mourant data7, computed in the following code.\n7 This is genotype frequency data of blood group alleles from Mourant, Kopec, and Domaniewska-Sobczak (1976) available through the R package HardyWeinberg.\nlibrary(\"HardyWeinberg\")\ndata(\"Mourant\")\nMourant[214:216,]__\n\n\n    Population    Country Total  MM  MN  NN\n214    Oceania Micronesia   962 228 436 298\n215    Oceania Micronesia   678  36 229 413\n216    Oceania     Tahiti   580 188 296  96\n\n\nnMM = Mourant$MM[216]\nnMN = Mourant$MN[216]\nnNN = Mourant$NN[216]\nloglik = function(p, q = 1 - p) {\n  2 * nMM * log(p) + nMN * log(2*p*q) + 2 * nNN * log(q)\n}\nxv = seq(0.01, 0.99, by = 0.01)\nyv = loglik(xv)\nplot(x = xv, y = yv, type = \"l\", lwd = 2,\n     xlab = \"p\", ylab = \"log-likelihood\")\nimax = which.max(yv)\nabline(v = xv[imax], h = yv[imax], lwd = 1.5, col = \"blue\")\nabline(h = yv[imax], lwd = 1.5, col = \"purple\")__\n\nFigure 2.11: Plot of the log-likelihood for the Tahiti data.\nThe maximum likelihood estimate for the probabilities in the multinomial is also obtained by using the observed frequencies as in the binomial case, however the estimates have to account for the relationships between the three probabilities. We can compute \\({}\\), \\({}\\) and \\(_{}\\) using the af function from the HardyWeinberg package.\nphat  =  af(c(nMM, nMN, nNN))\nphat __\n\n\n        A \n0.5793103 \n\n\npMM   =  phat^2\nqhat  =  1 - phat __\nThe expected values under Hardy-Weinberg equilibrium are then\npHW = c(MM = phat^2, MN = 2*phat*qhat, NN = qhat^2)\nsum(c(nMM, nMN, nNN)) * pHW __\n\n\n    MM.A     MN.A     NN.A \n194.6483 282.7034 102.6483 \nwhich we can compare to the observed values above. We can see that they are quite close to the observed values. We could further test whether the observed values allow us to reject the Hardy-Weinberg model, either by doing a simulation or a \\(^2\\) test as above. A visual evaluation of the goodness-of-fit of Hardy-Weinberg was designed by de Finetti (Finetti 1926; Cannings and Edwards 1968). It places every sample at a point whose coordinates are given by the proportions of each of the different alleles.\n\n4.6.2.1 Visual comparison to the Hardy-Weinberg equilibrium\nWe use the HWTernaryPlot function to display the data and compare it to Hardy-Weinberg equilibrium graphically.\npops = c(1, 69, 128, 148, 192)\ngenotypeFrequencies = as.matrix(Mourant[, c(\"MM\", \"MN\", \"NN\")])\nHWTernaryPlot(genotypeFrequencies[pops, ],\n        markerlab = Mourant$Country[pops],\n        alpha = 0.0001, curvecols = c(\"red\", rep(\"purple\", 4)),\n        mcex = 0.75, vertex.cex = 1)__\n\nFigure 2.12: This de Finetti plot shows the points as barycenters of the three genotypes using the frequencies as weights on each of the corners of the triangle. The Hardy-Weinberg model is the red curve, the acceptance region is between the two purple lines. We see that the US is the furthest from being in HW equilibrium.\n__\nQuestion 2.16\nMake the ternary plot as in the code above, then add the other data points to it, what do you notice? You could back up your discussion using the HWChisq function.\n__\nSolution\n__\nHWTernaryPlot(genotypeFrequencies[-pops, ], \n              newframe = FALSE, alpha = 0.0001, cex = 0.5)__\n__\nQuestion 2.17\nDivide all total frequencies by 50, keeping the same proportions for each of the genotypes, and recreate the ternary plot.\n\nWhat happens to the points ?\nWhat happens to the confidence regions and why?\n\n__\nSolution\n__\nnewgf = round(genotypeFrequencies / 50)\nHWTernaryPlot(newgf[pops, ],\n              markerlab = Mourant$Country[pops],\n              curvecols = c(\"red\", rep(\"purple\", 4)),\n              alpha = 0.0001, mcex = 0.75, vertex.cex = 1)__\n\n\n\n4.6.3 2.7.3 Concatenating several multinomials: sequence motifs and logos\nThe Kozak Motif is a sequence that occurs close to the start codon ATG of a coding region. The start codon itself always has a fixed spelling but in positions 5 to the left of it, there is a nucleotide pattern in which the letters are quite far from being equally likely.\nWe summarize this by giving the position weight matrix (PWM) or position-specific scoring matrix (PSSM), which provides the multinomial probabilities at every position. This is encoded graphically by the sequence logo (Figure 2.13).\nlibrary(\"seqLogo\")\nload(\"../data/kozak.RData\")\nkozak __\n\n\n  [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\nA 0.33 0.25  0.4 0.15 0.20    1    0    0 0.05\nC 0.12 0.25  0.1 0.40 0.40    0    0    0 0.05\nG 0.33 0.25  0.4 0.20 0.25    0    0    1 0.90\nT 0.22 0.25  0.1 0.25 0.15    0    1    0 0.00\n\n\npwm = makePWM(kozak)\nseqLogo(pwm, ic.scale = FALSE)__\n\nFigure 2.13: Here is a diagram called a sequence logo for the position dependent multinomial used to model the Kozak motif. It codifies the amount of variation in each of the positions on a log scale. The large letters represent positions where there is no uncertainty about which nucleotide occurs.\nOver the last sections, we’ve seen how the different “boxes” in the multinomial distributions we have encountered very rarely have equal probabilities. In other words, the parameters \\(p_1, p_2, …\\) are often different, depending on what is being modeled. Examples of multinomials with unequal frequencies include the twenty different amino acids, blood types and hair color.\nIf there are multiple categorical variables, we have seen that they are rarely independent (sex and colorblindness, hair and eye color, …). We will see later in Chapter 9 that we can explore the patterns in these dependencies by using multivariate decompositions of the contingency tables. Here, we’ll look at an important special case of dependencies between categorical variables: those that occur along a sequence (or “chain”) of categorical variables, e.g., over time or along a biopolymer.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "02-chap.html#modeling-sequential-dependencies-markov-chains",
    "href": "02-chap.html#modeling-sequential-dependencies-markov-chains",
    "title": "4  2.1 Goals for this chapter",
    "section": "4.7 2.8 Modeling sequential dependencies: Markov chains",
    "text": "4.7 2.8 Modeling sequential dependencies: Markov chains\nIf we want to predict tomorrow’s weather, a reasonably good guess is that it will most likely be the same as today’s weather, in addition we may state the probabilities for various kinds of possible changes^[The same reasoning can also be applied in reverse: we could “predict” yesterday’s weather from today’s.. This method for weather forecasting is an example for the Markov assumption: the prediction for tomorrow only depends on the state of things today, but not on yesterday or three weeks ago (all information we could potentially use is already contained in today’s weather). The weather example also highlights that such an assumption need not necessarily be exactly true, but it should be a good enough assumption. It is fairly straightforward to extend this assumption to dependencies on the previous \\(k\\) days, where \\(k\\) is a finite and hopefully not too large number. The essence of the Markov assumption is that the process has a finite “memory”, so that predictions only need to look back for a finite amount of time.\nInstead of temporal sequences, we can also apply this to biological sequences. In DNA, we may see specific succession of patterns so that pairs of nucleotides, called digrams, say, [CG, CA, CC] and [CT] are not equally frequent. For instance, in parts of the genome we see more frequent instances of [CA] than we would expect under independence:\n\\[ P() P() , P(). \\]\nWe model this dependency in the sequence as a Markov chain :\n\\[ P() = P() = P() = P(…) = P() , P(), \\]\nwhere N stands for any nucleotide, and \\(P()\\) stands for “the probability of \\(\\), given that the preceding base is a \\(\\)”. Figure 2.14 shows a schematic representation of such transitions on a graph.\n\nFigure 2.14: Visualisation of a 4-state Markov chain. The probability of each possible digram (e.,g., CA) is given by the weight of the edge between the corresponding nodes. So for instance, the probability of CA is given by the edge C$ o$ A. We’ll see in Chapter 11 how to use R packages to draw these type of network graphs.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "02-chap.html#bayesian-thinking",
    "href": "02-chap.html#bayesian-thinking",
    "title": "4  2.1 Goals for this chapter",
    "section": "4.8 2.9 Bayesian Thinking",
    "text": "4.8 2.9 Bayesian Thinking\n\nFigure 2.15: Turtles all the way down. Bayesian modeling of the uncertainty of the parameter of a distribution is done by using a random variable whose distribution may depend on parameters whose uncertainty can be modeled as a random variable; these are called hierarchical models.\nUp to now we have followed a classical approach, where the parameters of our models and the distributions they use, i.e., the probabilities of the possible different outcomes, represent long term frequencies. The parameters are—at least conceptually—definite, knowable and fixed. We may not know them, so we estimate them from the data at hand. However, such an approach does not take into account any information that we might already have, and that might inform us on the parameters or make certain parameter values or their combinations more likely than others—even before we see any of the current set of data. For that we need a different approach, in which we use probabilistic models (i.e., distributions) to express our prior knowledge8 about the parameters, and use the current data to update such knowledge, for instance by shifting those distributions or making them more narrow. Such an approach is provided by the Bayesian paradigm (Figure 2.15).\n8 Some like to say “our belief(s)”.\nThe Bayesian paradigm is a practical approach where prior and posterior distributions to model our knowledge before and after collecting some data and making an observation. It can be iterated ad infinitum: the posterior after one round of data generation can be used as the prior for the next round. Thus, it is also particularly useful for integrating or combining information from different sources.\nThe same idea can also be applied to hypothesis testing, where we want to use data to decide whether we believe that a certain statement—which we might call the hypothesis \\(H\\)—is true. Here, our “parameter” is the probability that \\(H\\) is true, and we can formalize our prior knowledge in the form of a prior probability, written \\(P(H)\\)9. After we see the data, we have the posterior probability. We write it as \\(P(H,|,D)\\), the probability of \\(H\\) given that we saw \\(D\\). This may be higher or lower than \\(P(H)\\), depending on what the data \\(D\\) were.\n9 For a so-called frequentist, such a probability does not exist. Their viewpoint is that, although the truth is unknown, in reality the hypothesis is either true or false; there is no meaning in calling it, say, “70% true”.\n\n4.8.1 2.9.1 Example: haplotype frequencies\nTo keep the mathematical formalism to a minimum, we start with an example from forensics, using combined signatures (haplotypes) from the Y chromosome.\nA haplotype is a collection of alleles (DNA sequence variants) that are spatially adjacent on a chromosome, are usually inherited together (recombination tends not to disconnect them), and thus are genetically linked. In this case we are looking at linked variants on the Y chromosome.\nFirst we’ll look at the motivation behind haplotype frequency analyses, then we’ll revisit the idea of likelihood. After this, we’ll explain how we can think of unknown parameters as being random numbers themselves, modeling their uncertainty with a prior distribution. Then we will see how to incorporate new data observed into the probability distributions and compute posterior confidence statements about the parameters.\n\nFigure 2.16: A short tandem repeat (STR) in DNA occurs when a pattern of two or more nucleotides is repeated, and the repeated sequences are directly adjacent to each other. An STR is also known as a microsatellite. The pattern can range in length from 2 to 13 nucleotides, and the number of repeats is highly variable across individuals. STR numbers can be used as genetic signatures.\n\nFigure 2.17: Location of short tandem repeats (STR) on the human Y chromosome. Source: https://strbase.nist.gov/ystrpos1.htm\n\nFigure 2.18: Y STR haplotype lookup from a database used by the FBI.\nWe’re interested in the frequencies of particular Y-haplotypes that consist of a set of different short tandem repeats (STR). The combination of STR numbers at the specific locations used for DNA forensics are labeled by the number of repeats at the specific positions. Here is a short excerpt of such an STR haplotype table:\nhaplo6 = read.table(\"../data/haplotype6.txt\", header = TRUE)\nhaplo6 __\n\n\n  Individual DYS19 DXYS156Y DYS389m DYS389n DYS389p\n1         H1    14       12       4      12       3\n2         H3    15       13       4      13       3\n3         H4    15       11       5      11       3\n4         H5    17       13       4      11       3\n5         H7    13       12       5      12       3\n6         H8    16       11       5      12       3\nThe table says that the haplotype H1 has 14 repeats at position DYS19, 12 repeats at position DXYS156Y, etc. Suppose we want to find the underlying proportion \\(p\\) of a particular haplotype in a population of interest, by haplotyping \\(n=300\\) men; and suppose we found H1 in \\(y=40\\) of them. We are going to use the binomial distribution \\(B(n,p)\\) to model this, with \\(p\\) unknown.\nThe haplotypes created through the use of these Y-STR profiles are shared between men in the same patriarchal lineages. Thus, it is possible that two different men share the same profile.\n\n\n4.8.2 2.9.2 Simulation study of the Bayesian paradigm for the binomial\ndistribution\nInstead of assuming that our parameter \\(p\\) has one single value (e.g., the maximum likelihood estimate 40/300), the Bayesian approach allows us to see it as a draw from a statistical distribution. The distribution expresses our belief about the possible values of the parameter \\(p\\). In principle, we can use any distribution that we like whose possible values are permissible for \\(p\\). As here we are looking at a parameter that expresses a proportion or a probability, and which takes its values between 0 and 1, it is convenient to use the Beta distribution. Its density formula is written\n\\[ f_{,}(x) = (,)=. \\]\nWe can see in Figure 2.19 how this function depends on two parameters \\(\\) and \\(\\), which makes it a very flexible family of distributions (it can “fit” a lot different situations). And it has a nice mathematical property: if we start with a prior belief on \\(p\\) that is Beta-shaped, observe a dataset of \\(n\\) binomial trials, then update our belief, the posterior distribution on \\(p\\) will also have a Beta distribution, albeit with updated parameters. This is a mathematical fact. We will not prove it here, however we demonstrate it by simulation.\n\nFigure 2.19: Beta distributions with \\(,20,50\\) and \\(,60,150\\). We can use these as a prior for probability of success in a binomial experiment. These three distributions have the same mean (\\(\\)), but different concentrations around the mean.\n\n\n4.8.3 2.9.3 The distribution of \\(Y\\)\nFor a given choice of \\(p\\), we know what the distribution of \\(Y\\) is, by virtue of Equation 2.3. But what is the distribution of \\(Y\\) if \\(p\\) itself also varies according to some distribution? We call this the marginal distribution of \\(Y\\). Let’s simulate that. First we generate a random sample rp of 100000 \\(p\\)s. For each of them, we then generate a random sample of \\(Y\\), shown in Figure 2.20. In the code below, for the sake of demonstration we use the parameters 50 and 350 for the prior. Such a prior is already quite informative (“peaked”) and may, e.g., reflect beliefs we have based on previous studies. In Question 2.20 you have the opportunity to try out a “softer” (less informative) prior. We again use vapply to apply a function, the unnamed (anonymous) function of x, across all elements of rp to obtain as a result another vector y of the same length.\nrp = rbeta(100000, 50, 350)\ny = vapply(rp, \n           function(x) rbinom(1, prob = x, size = 300), \n           integer(1))\nhist(y, breaks = 50, col = \"orange\", main = \"\", xlab = \"\")__\n\nFigure 2.20: Marginal Distribution of \\(Y\\).\n__\nQuestion 2.18\nVerify that we could have gotten the same result as in the above code chunk by using R’s vectorisation capabilities and writing rbinom(length(rp), rp, size = 300).\n__\nSolution\n__\nset.seed(0xbebe)\ny1 = vapply(rp, \n            function(x) rbinom(1, prob = x, size = 300), \n            integer(1))\nset.seed(0xbebe)\ny2 = rbinom(length(rp), rp, size = 300)\nstopifnot(identical(y1, y2))__\n\n\n4.8.4 2.9.4 Histogram of all the \\(p\\)s such that \\(Y=40\\): the posterior\ndistribution\nSo now let’s compute the posterior distribution of \\(p\\) by conditioning on those outcomes where \\(Y\\) was 40. We compare it to the theoretical posterior, densPostTheory, of which more below. The results are shown in Figure 2.21.\npPostEmp = rp[ y == 40 ]\nhist(pPostEmp, breaks = 40, col = \"chartreuse4\", main = \"\",\n  probability = TRUE, xlab = \"posterior p\")\n\np_seq = seq(0, 1, by = 0.001)\ndensPostTheory = dbeta(p_seq, 50 + 40, 350 + 260)\nlines(p_seq, densPostTheory, type = \"l\", lwd = 3)__\n\nFigure 2.21: Only choosing the values of the distribution with \\(Y=40\\) gives the posterior distribution of \\(p\\). The histogram (green) shows the simulated values for the posterior distribution, the line the density of a Beta distribution with the theoretical parameters.\nWe can also check the means of both distributions computed above and see that they are close to 4 significant digits.\nmean(pPostEmp)__\n\n\n[1] 0.128726\n\n\ndp = p_seq[2] - p_seq[1]\nsum(p_seq * densPostTheory * dp)__\n\n\n[1] 0.1285714\nTo approximate the mean of the theoretical density densPostTheory, we have above literally computed the integral\n\\[ _0^1 p , f(p) , dp \\]\nusing numerical integration, i.e., the sum over the integral. This is not always convenient (or feasible), in particular if our model involves not just a single, scalar parameter \\(p\\), but has many parameters, so that we are dealing with a high-dimensional parameter vector and a high-dimensional integral. If the integral cannot be computed analytically, we can use Monte Carlo integration. You already saw a very simple instance of Monte Carlo integration in the code above, where we sampled the posterior with pPostEmp and performed integration to compute the posterior mean by calling R’s mean function. In this case, an alternative Monte Carlo algorithm is to generate posterior samples using the rbeta function directly with the right parameters.\npPostMC = rbeta(n = 100000, 90, 610)\nmean(pPostMC)__\n\n\n[1] 0.1285718\nWe can check the concordance between the Monte Carlo samples pPostMC and pPostEmp, generated in slightly different ways, using a quantile-quantile plot (QQ-plot , Figure 2.22).\nqqplot(pPostMC, pPostEmp, type = \"l\", asp = 1)\nabline(a = 0, b = 1, col = \"blue\")__\n\nFigure 2.22: Quantile-quantile (QQ) plot of our Monte Carlo sample pPostMC from the theoretical distribution and our simulation sample pPostEmp. We could also similarly compare either of these two distributions to the theoretical distribution function pbeta(., 90, 610). If the curve lies on the line \\(y=x\\), this indicates a good agreement. There are some random differences at the tails.\n__\nQuestion 2.19\nWhat is the difference between the simulation that results in pPostEmp and the Monte Carlo simulation that leads to pPostMC?\n\n\n4.8.5 2.9.5 The posterior distribution is also a Beta\nNow we have seen that the posterior distribution is also a Beta. In our case its parameters \\(\\) and \\(\\) were obtained by summing the prior parameters \\(\\), \\(\\) with the observed successes \\(y=40\\) and the observed failures \\(n-y=260\\), thus obtaining the posterior\n\\[ (90,, 610)=(+y,+(n-y)). \\]\nWe can use it to give the best10 estimate we can for \\(p\\) with its uncertainty given by the posterior distribution.\n10 We could take the value that maximizes the posterior distribution as our best estimate, this is called the MAP estimate, and in this case it would be \\(=\\).\n\n\n4.8.6 2.9.6 Suppose we had a second series of data\nAfter seeing our previous data, we now have a new prior, \\((90, 610)\\). Suppose we collect a new set of data with \\(n=150\\) observations and \\(y=25\\) successes, thus 125 failures. Now what would we take to be our best guess at \\(p\\)?\nUsing the same reasoning as before, the new posterior will be \\((90+25=115,, 610+125=735)\\). The mean of this distribution is \\(=\\), thus one estimate of \\(p\\) would be 0.135. The maximum a posteriori (MAP) estimate would be the mode of \\((115, 735)\\), ie \\(\\). Let’s check this numerically.\ndensPost2 = dbeta(p_seq, 115, 735)\nmcPost2   = rbeta(1e6, 115, 735)\nsum(p_seq * densPost2 * dp)   # mean, by numeric integration __\n\n\n[1] 0.1352941\n\n\nmean(mcPost2)                 # mean by MC __\n\n\n[1] 0.1352655\n\n\np_seq[which.max(densPost2)]   # MAP estimate __\n\n\n[1] 0.134\n__\nQuestion 2.20\nRedo all the computations replacing our original prior with a softer prior (less peaked), meaning that we use less prior information. For instance, try Beta(1,1), which is the uniform distribution. How much does this change the final result?\nAs a general rule, the prior rarely changes the posterior distribution substantially except if it is very peaked. This would be the case if, at the outset, we were already rather sure of what to expect. Another case when the prior has an influence is if there is very little data.\nThe best situation to be in is to have enough data to swamp the prior so that its choice doesn’t have much impact on the final result.\n\n\n4.8.7 2.9.7 Confidence Statements for the proportion parameter\nNow it is time to conclude about what the proportion \\(p\\) actually is, given the data. One summary is a posterior credibility interval, which is a Bayesian analog of the confidence interval. We can take the 2.5 and 97.5-th percentiles of the posterior distribution: \\(P(q_{2.5%} p q_{97.5%})=0.95\\).\nquantile(mcPost2, c(0.025, 0.975))__\n\n\n     2.5%     97.5% \n0.1131080 0.1590221 \n\nFigure 2.23: An example from Love, Huber, and Anders (2014) shows plots of the likelihoods (solid lines, scaled to integrate to 1) and the posteriors (dashed lines) for the green and purple genes and of the prior (solid black line): due to the higher dispersion of the purple gene, its likelihood is wider and less peaked (indicating less information), and the prior has more influence on its posterior than for the green gene. The stronger curvature of the green posterior at its maximum translates to a smaller reported standard error for the MAP logarithmic fold change (LFC) estimate (horizontal error bar).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "02-chap.html#example-occurrence-of-a-nucleotide-pattern-in-a-genome",
    "href": "02-chap.html#example-occurrence-of-a-nucleotide-pattern-in-a-genome",
    "title": "4  2.1 Goals for this chapter",
    "section": "4.9 2.10 Example: occurrence of a nucleotide pattern in a genome",
    "text": "4.9 2.10 Example: occurrence of a nucleotide pattern in a genome\nThe examples we have seen up to now have concentrated on distributions of discrete counts and categorical data. Let’s look at an example of distributions of distances, which are quasi-continuous. This case study of the distributions of the distances between instances of a specific motif in genome sequences will also allow us to explore specific genomic sequence manipulations in Bioconductor.\nThe Biostrings package provides tools for working with sequence data. The essential data structures, or classes as they are known in R, are DNAString and DNAStringSet. These enable us to work with one or multiple DNA sequences efficiently .\nThe Biostrings package also contains additional classes for representing amino acid sequences, and more general, biology-inspired sequences.\nlibrary(\"Biostrings\")__\n__\nQuestion 2.21\nExplore some of the useful data and functions provided in the Biostrings package by exploring the tutorial vignette.\n__\nSolution\n__\nThe first line prints genetic code information, the second one returns IUPAC nucleotide ambiguity codes. The third line lists all the vignettes available in the Biostrings package, the fourth display one particular vignette.\nGENETIC_CODE\nIUPAC_CODE_MAP\nvignette(package = \"Biostrings\")\nvignette(\"BiostringsQuickOverview\", package = \"Biostrings\")__\nThis last command will open a list in your browser window from which you can access the documentation11. The BSgenome package provides access to many genomes, and you can access the names of the data packages that contain the whole genome sequences by typing\n11 Vignettes are manuals for the packages complete with examples and case studies.\nlibrary(\"BSgenome\")\nag = available.genomes()\nlength(ag)__\n\n\n[1] 113\n\n\nag[1:2]__\n\n\n[1] \"BSgenome.Alyrata.JGI.v1\"              \n[2] \"BSgenome.Amellifera.BeeBase.assembly4\"\nWe are going to explore the occurrence of the AGGAGGT motif12 in the genome of E.coli. We use the genome sequence of one particular strain, Escherichia coli str. K12 substr.DH10B13, whose NCBI accession number is NC_010473.\n12 This is the Shine-Dalgarno motif which helps initiate protein synthesis in bacteria.\n13 It is known as the laboratory workhorse, often used in experiments.\nlibrary(\"BSgenome.Ecoli.NCBI.20080805\")\nEcoli\nshineDalgarno = \"AGGAGGT\"\necoli = Ecoli$NC_010473 __\nWe can count the pattern’s occurrence in windows of width 50000 using the countPattern function.\nwindow = 50000\nstarts = seq(1, length(ecoli) - window, by = window)\nends   = starts + window - 1\nnumMatches = vapply(seq_along(starts), function(i) {\n  countPattern(shineDalgarno, ecoli[starts[i]:ends[i]],\n               max.mismatch = 0)\n  }, numeric(1))\ntable(numMatches)__\n\n\nnumMatches\n 0  1  2  3  4 \n48 32  8  3  2 \n__\nQuestion 2.22\nWhat distribution might this table fit ?\n__\nSolution\n__\nThe Poisson is a good candidate, as a quantitative and graphical evaluation (see Figure 2.24) for these data shows.\nlibrary(\"vcd\")\ngf = goodfit(numMatches, \"poisson\")\nsummary(gf)__\n\n\n     Goodness-of-fit test for poisson distribution\n\n                      X^2 df  P(&gt; X^2)\nLikelihood Ratio 4.134932  3 0.2472577\n\n\ndistplot(numMatches, type = \"poisson\")__\n\nFigure 2.24: Evaluation of a Poisson model for motif counts along the sequence Ecoli$NC_010473.\nWe can inspect the matches using the matchPattern function.\nsdMatches = matchPattern(shineDalgarno, ecoli, max.mismatch = 0)__\nYou can type sdMatches in the R command line to obtain a summary of this object. It contains the locations of all 65 pattern matches, represented as a set of so-called views on the original sequence. Now what are the distances between them?\nbetweenmotifs = gaps(sdMatches)__\nSo these are in fact the 66 complementary regions. Now let’s find a model for the distribution of the gap sizes between motifs. If the motifs occur at random locations, we expect the gap lengths to follow an exponential distribution14. The code below (whose output is shown in Figure 2.25) assesses this assumption. If the exponential distribution is a good fit, the points should lie roughly on a straight line. The exponential distribution has one parameter, the rate, and the line with slope corresponding to an estimate from the data is also shown.\n14 How could we guess that the exponential is the right fit here? Whenever we have independent, random Bernoulli occurrences along a sequence, the gap lengths are exponential. You may be familiar with radioactive decay, where the waiting times between emissions are also exponentially distributed. It is a good idea if you are not familiar with this distribution to look up more details in the Wikipedia.\nlibrary(\"Renext\")\nexpplot(width(betweenmotifs), rate = 1/mean(width(betweenmotifs)),\n        labels = \"fit\")__\n\nFigure 2.25: Evaluation of fit to the exponential distribution (black line) of the gaps between the motifs.\n__\nQuestion 2.23\nThere appears to be a slight deviation from the fitted line in Figure 2.25 at the right tail of the distribution, i.e., for the largest values. What could be the reason?\n\n4.9.1 2.10.1 Modeling in the case of dependencies\nAs we saw in Section 2.8, nucleotide sequences are often dependent: the probability of seing a certain nucleotide at a given position tends to depend on the surrounding sequence. Here we are going to put into practice dependency modeling using a Markov chain. We are going to look at regions of chromosome 8 of the human genome and try to discover differences between regions called CpG15 islands and the rest.\n15 CpG stands for 5’-C-phosphate-G-3’; this means that a C is connected to a G through a phosphate along the strand (this is unrelated to C-G base-pairing of Section 2.7). The cytosines in the CpG dinucleotide can be methylated, changing the levels of gene expression. This type of gene regulation is part of epigenetics. Some more information is on Wikipedia: CpG site and epigenetics.\nWe use data (generated by Irizarry, Wu, and Feinberg (2009)) that tell us where the start and end points of the islands are in the genome and look at the frequencies of nucleotides and of the digrams ‘CG’, ‘CT’, ‘CA’, ‘CC’. So we can ask whether there are dependencies between the nucleotide occurrences and if so, how to model them.\nlibrary(\"BSgenome.Hsapiens.UCSC.hg19\")\nchr8  =  Hsapiens$chr8\nCpGtab = read.table(\"../data/model-based-cpg-islands-hg19.txt\",\n                    header = TRUE)\nnrow(CpGtab)__\n\n\n[1] 65699\n\n\nhead(CpGtab)__\n\n\n    chr  start    end length CpGcount GCcontent pctGC obsExp\n1 chr10  93098  93818    721       32       403 0.559  0.572\n2 chr10  94002  94165    164       12        97 0.591  0.841\n3 chr10  94527  95302    776       65       538 0.693  0.702\n4 chr10 119652 120193    542       53       369 0.681  0.866\n5 chr10 122133 122621    489       51       339 0.693  0.880\n6 chr10 180265 180720    456       32       256 0.561  0.893\n\n\nirCpG = with(dplyr::filter(CpGtab, chr == \"chr8\"),\n         IRanges(start = start, end = end))__\n\n\n\nWe use the :: operator to call the filter function specifically from the dplyr package—and not from any other packages that may happen to be loaded and defining functions of the same name. This precaution is particularly advisable in the case of the filter function, since this name is used by quite a few other packages. You can think of the normal (without ::) way of calling R functions like calling people by their first (given) names; whereas the fully qualified version with :: corresponds to calling someone by their full name. At least within the reach of the CRAN and Bioconductor repositories, such fully qualified names are guaranteed to be unique.\n\n\nWe use the :: operator to call the filter function specifically from the dplyr package—and not from any other packages that may happen to be loaded and defining functions of the same name. This precaution is particularly advisable in the case of the filter function, since this name is used by quite a few other packages. You can think of the normal (without ::) way of calling R functions like calling people by their first (given) names; whereas the fully qualified version with :: corresponds to calling someone by their full name. At least within the reach of the CRAN and Bioconductor repositories, such fully qualified names are guaranteed to be unique.\nIn the line above, we subset (filter) the data frame CpGtab to only chromosome 8, and then we create an IRanges object whose start and end positions are defined by the equally named columns of the data frame. In the IRanges function call (which constructs the object from its arguments), the first start is the argument name of the function, the second start refers to the column in the data frame obtained as an output from filter; and similarly for end. IRanges is a general container for mathematical intervals. We create the biological context16 with the next line.\n16 The “I” in IRanges stands for “interval”; the “G” in GRanges for “genomic”.\ngrCpG = GRanges(ranges = irCpG, seqnames = \"chr8\", strand = \"+\")\ngenome(grCpG) = \"hg19\"__\nNow let’s visualize; see the output in Figure 2.26.\nlibrary(\"Gviz\")\nideo = IdeogramTrack(genome = \"hg19\", chromosome = \"chr8\")\nplotTracks(\n  list(GenomeAxisTrack(),\n    AnnotationTrack(grCpG, name = \"CpG\"), ideo),\n    from = 2200000, to = 5800000,\n    shape = \"box\", fill = \"#006400\", stacking = \"dense\")__\n\nFigure 2.26: Gviz plot of CpG locations in a selected region of chromosome 8.\nWe now define so-called views on the chromosome sequence that correspond to the CpG islands, irCpG, and to the regions in between (gaps(irCpG)). The resulting objects CGIview and NonCGIview only contain the coordinates, not the sequences themselves (these stay in the big object Hsapiens$chr8), so they are fairly lightweight in terms of storage.\nCGIview    = Views(unmasked(Hsapiens$chr8), irCpG)\nNonCGIview = Views(unmasked(Hsapiens$chr8), gaps(irCpG))__\nWe compute transition counts in CpG islands and non-islands using the data.\nseqCGI      = as(CGIview, \"DNAStringSet\")\nseqNonCGI   = as(NonCGIview, \"DNAStringSet\")\ndinucCpG    = sapply(seqCGI, dinucleotideFrequency)\ndinucNonCpG = sapply(seqNonCGI, dinucleotideFrequency)\ndinucNonCpG[, 1]__\n\n\n AA  AC  AG  AT  CA  CC  CG  CT  GA  GC  GG  GT  TA  TC  TG  TT \n389 351 400 436 498 560 112 603 359 336 403 336 330 527 519 485 \n\n\nNonICounts = rowSums(dinucNonCpG)\nIslCounts  = rowSums(dinucCpG)__\nFor a four state Markov chain as we have, we define the transition matrix as a matrix where the rows are the from state and the columns are the to state.\nTI  = matrix( IslCounts, ncol = 4, byrow = TRUE)\nTnI = matrix(NonICounts, ncol = 4, byrow = TRUE)\ndimnames(TI) = dimnames(TnI) =\n  list(c(\"A\", \"C\", \"G\", \"T\"), c(\"A\", \"C\", \"G\", \"T\"))__\nWe use the counts of numbers of transitions of each type to compute frequencies and put them into two matrices.\n\n\n\nThe transition probabilities are probabilities so the rows need to sum to 1.\n\n\nThe transition probabilities are probabilities so the rows need to sum to 1.\nMI = TI /rowSums(TI)\nMI __\n\n\n           A         C         G         T\nA 0.20457773 0.2652333 0.3897678 0.1404212\nC 0.20128250 0.3442381 0.2371595 0.2173200\nG 0.18657245 0.3145299 0.3450223 0.1538754\nT 0.09802105 0.3352314 0.3598984 0.2068492\n\n\nMN = TnI / rowSums(TnI)\nMN __\n\n\n          A         C          G         T\nA 0.3351380 0.1680007 0.23080886 0.2660524\nC 0.3641054 0.2464366 0.04177094 0.3476871\nG 0.2976696 0.2029017 0.24655406 0.2528746\nT 0.2265813 0.1972407 0.24117528 0.3350027\n__\nQuestion 2.24\nAre the transitions different in the different rows? This would mean that, for instance, \\(P(,|,) P(,|,)\\).\n__\nSolution\n__\nThe transitions are different. For instance, the transitions from C to A and T to A for in the islands (MI) transition matrix seem very different (0.201 versus 0.098).\n__\nQuestion 2.25\nAre the relative frequencies of the different nucleotides different in CpG islands compared to elsewhere?\n__\nSolution\n__\nfreqIsl = alphabetFrequency(seqCGI, baseOnly = TRUE, collapse = TRUE)[1:4]\nfreqIsl / sum(freqIsl)__\n\n\n        A         C         G         T \n0.1781693 0.3201109 0.3206298 0.1810901 \n\n\nfreqNon = alphabetFrequency(seqNonCGI, baseOnly = TRUE, collapse = TRUE)[1:4]\nfreqNon / sum(freqNon)__\n\n\n        A         C         G         T \n0.3008292 0.1993832 0.1993737 0.3004139 \nThis shows an inverse pattern: in the CpG islands, C and G have frequencies around 0.32, whereas in the non-CpG islands, we have A and T that have frequencies around 0.30.\n__\nQuestion 2.26\nHow can we use these differences to decide whether a given sequence comes from a CpG island?\n__\nSolution\n__\nUse a \\(^2\\) statistic to compare the frequencies between the observed and freqIsl and freqNon frequencies. For shorter sequences, this may not be sensitive enough, and a more sensitive approach is given below.\nGiven a sequence for which we do not know whether it is in a CpG island or not, we can ask what is the probability it belongs to a CpG island compared to somewhere else. We compute a score based on what is called the odds ratio. Let’s do an example: suppose our sequence \\(x\\) is ACGTTATACTACG, and we want to decide whether it comes from a CpG island or not.\nIf we model the sequence as a first order Markov chain we can write, supposing that the sequence comes from a CpG island:\n\\[ \\[\\begin{align} P_{\\text{i}}(x = \\mathtt{ACGTTATACTACG}) = \\;\n&P_{\\text{i}}(\\mathtt{A}) \\, P_{\\text{i}}(\\mathtt{AC})\\,\nP_{\\text{i}}(\\mathtt{CG})\\, P_{\\text{i}}(\\mathtt{GT})\\,\nP_{\\text{i}}(\\mathtt{TT}) \\times \\\\\\ &P_{\\text{i}}(\\mathtt{TA})\\,\nP_{\\text{i}}(\\mathtt{AT})\\, P_{\\text{i}}(\\mathtt{TA})\\,\nP_{\\text{i}}(\\mathtt{AC})\\, P_{\\text{i}}(\\mathtt{CG}). \\end{align}\\] \\]\nWe are going to compare this probability to the probability for non-islands. As we saw above, these probabilities tend to be quite different. We will take their ratio and see if it is larger or smaller than 1. These probabilities are going to be products of many small terms and become very small. We can work around this by taking logarithms.\n\\[ \\[\\begin{align} \\log&\\frac{P(x\\,|\\, \\text{island})}{P(x\\,|\\,\\text{non-\nisland})}=\\\\\\ \\log&\\left( \\frac{P_{\\text{i}}(\\mathtt{A})\\,\nP_{\\text{i}}(\\mathtt{A}\\rightarrow \\mathtt{C})\\,\nP_{\\text{i}}(\\mathtt{C}\\rightarrow \\mathtt{G})\\,\nP_{\\text{i}}(\\mathtt{G}\\rightarrow \\mathtt{T})\\,\nP_{\\text{i}}(\\mathtt{T}\\rightarrow \\mathtt{T})\\,\nP_{\\text{i}}(\\mathtt{T}\\rightarrow \\mathtt{A})} {P_{\\text{n}}(\\mathtt{A})\\,\nP_{\\text{n}}(\\mathtt{A}\\rightarrow \\mathtt{C})\\,\nP_{\\text{n}}(\\mathtt{C}\\rightarrow \\mathtt{G})\\,\nP_{\\text{n}}(\\mathtt{G}\\rightarrow \\mathtt{T})\\, P_{\\text{n}}(\n\\mathtt{T}\\rightarrow \\mathtt{T})\\, P_{\\text{n}}( \\mathtt{T}\\rightarrow\n\\mathtt{A})} \\right. \\times\\\\\\ &\\left.\n\\frac{P_{\\text{i}}(\\mathtt{A}\\rightarrow \\mathtt{T})\\,\nP_{\\text{i}}(\\mathtt{T}\\rightarrow \\mathtt{A})\\,\nP_{\\text{i}}(\\mathtt{A}\\rightarrow \\mathtt{C})\\,\nP_{\\text{i}}(\\mathtt{C}\\rightarrow \\mathtt{G})}\n{P_{\\text{n}}(\\mathtt{A}\\rightarrow \\mathtt{T})\\,\nP_{\\text{n}}(\\mathtt{T}\\rightarrow \\mathtt{A})\\,\nP_{\\text{n}}(\\mathtt{A}\\rightarrow \\mathtt{C})\\,\nP_{\\text{n}}(\\mathtt{C}\\rightarrow \\mathtt{G})} \\right) \\end{align}\\] \\]\nThis is the log-likelihood ratio score. To speed up the calculation, we compute the log-ratios \\((P_{}()/P_{}()),…, (P_{}( )/P_{}())\\) once and for all and then sum up the relevant ones to obtain our score.\n\n\n\nWorked out examples and many useful details can be found in Durbin et al. (1998).\n\n\nWorked out examples and many useful details can be found in Durbin et al. (1998).\nalpha = log((freqIsl/sum(freqIsl)) / (freqNon/sum(freqNon)))\nbeta  = log(MI / MN)__\n\n\nx = \"ACGTTATACTACG\"\nscorefun = function(x) {\n  s = unlist(strsplit(x, \"\"))\n  score = alpha[s[1]]\n  if (length(s) &gt;= 2)\n    for (j in 2:length(s))\n      score = score + beta[s[j-1], s[j]]\n  score\n}\nscorefun(x)__\n\n\n         A \n-0.2824623 \nIn the code below, we pick sequences of length len = 100 out of the 2855 sequences in the seqCGI object, and then out of the 2854 sequences in the seqNonCGI object (each of them is a DNAStringSet). In the first three lines of the generateRandomScores function, we drop sequences that contain any letters other than A, C, T, G; such as “.” (a character used for undefined nucleotides). Among the remaining sequences, we sample with probabilities proportional to their length minus len and then pick subsequences of length len out of them. The start points of the subsequences are sampled uniformly, with the constraint that the subsequences have to fit in.\ngenerateRandomScores = function(s, len = 100, B = 1000) {\n  alphFreq = alphabetFrequency(s)\n  isGoodSeq = rowSums(alphFreq[, 5:ncol(alphFreq)]) == 0\n  s = s[isGoodSeq]\n  slen = sapply(s, length)\n  prob = pmax(slen - len, 0)\n  prob = prob / sum(prob)\n  idx  = sample(length(s), B, replace = TRUE, prob = prob)\n  ssmp = s[idx]\n  start = sapply(ssmp, function(x) sample(length(x) - len, 1))\n  scores = sapply(seq_len(B), function(i)\n    scorefun(as.character(ssmp[[i]][start[i]+(1:len)]))\n  )\n  scores / len\n}\nscoresCGI    = generateRandomScores(seqCGI)\nscoresNonCGI = generateRandomScores(seqNonCGI)__\n\n\nrgs = range(c(scoresCGI, scoresNonCGI))\nbr = seq(rgs[1], rgs[2], length.out = 50)\nh1 = hist(scoresCGI,    breaks = br, plot = FALSE)\nh2 = hist(scoresNonCGI, breaks = br, plot = FALSE)\nplot(h1, col = rgb(0, 0, 1, 1/4), xlim = c(-0.5, 0.5), ylim=c(0,120))\nplot(h2, col = rgb(1, 0, 0, 1/4), add = TRUE)__\n\nFigure 2.27: Island and non-island scores as generated by the function generateRandomScores. This is the first instance of a mixture we encounter. We will revisit them in Chapter 4.\nWe can consider these our training data : from data for which we know the types, we can see whether our score is useful for discriminating – see Figure 2.27.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "02-chap.html#summary-of-this-chapter",
    "href": "02-chap.html#summary-of-this-chapter",
    "title": "4  2.1 Goals for this chapter",
    "section": "4.10 2.11 Summary of this chapter",
    "text": "4.10 2.11 Summary of this chapter\nIn this chapter we experienced the basic yoga of statistics: how to go from the data back to the possible generating distributions and how to estimate the parameters that define these distributions.\nStatistical models We showed some specific statistical models for experiments with categorical outcomes (binomial and multinomial).\nGoodness of fit We used different visualizations and showed how to run simulation experiments to test whether our data could be fit by a fair four box multinomial model. We encountered the chi-square statistic and saw how to compare simulation and theory using a qq-plot.\nEstimation We explained maximum likelihood and Bayesian estimation procedures. These approaches were illustrated on examples involving nucleotide pattern discovery and haplotype estimations.\nPrior and posterior distributions When assessing data of a type that has been been previously studied, such as haplotypes, it can be beneficial to compute the posterior distribution of the data. This enables us to incorporate uncertainty in the decision making, by way of a simple computation. The choice of the prior has little effect on the result as long as there is sufficient data.\nCpG islands and Markov chains We saw how dependencies along DNA sequences can be modeled by Markov chain transitions. We used this to build scores based on likelihood ratios that enable us to see whether long DNA sequences come from CpG islands or not. When we made the histogram of scores, we saw in Figure 2.27 a noticeable feature: it seemed to be made of two pieces. This bimodality was our first encounter with mixtures, they are the subject of Chapter 4.\nThis is the first instance of building a model on some training data: sequences which we knew were in CpG islands, that we could use later to classify new data. We will develop a much more complete way of doing this in Chapter 12.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "02-chap.html#further-reading",
    "href": "02-chap.html#further-reading",
    "title": "4  2.1 Goals for this chapter",
    "section": "4.11 2.12 Further reading",
    "text": "4.11 2.12 Further reading\nOne of the best introductory statistics books available is Freedman, Pisani, and Purves (1997). It uses box models to explain the important concepts. If you have never taken a statistics class, or you feel you need a refresher, we highly recommend it. Many introductory statistics classes do not cover statistics for discrete data in any depth. The subject is an important part of what we need for biological applications. A book-long introduction to these types of analyses can be found in (Agresti 2007).\nHere we gave examples of simple unstructured multinomials. However, sometimes the categories (or boxes) of a multinomial have specific structure. For instance, the 64 possible codons code for 20 amino acids and the stop codons (61+3). So we can see the amino acids themselves as a multinomial with 20 degrees of freedom. Within each amino acid there are multinomials with differing numbers of categories (Proline has four: CCA, CCG, CCC, CCT, see Exercise 2.3). Some multivariate methods have been specifically designed to decompose the variability between codon usage within the differently abundant amino-acids (Grantham et al. 1981; Perrière and Thioulouse 2002), and this enables discovery of latent gene transfer and translational selection. We will cover the specific methods used in those papers when we delve into the multivariate exploration of categorical data in Chapter 9.\nThere are many examples of successful uses of the Bayesian paradigm to quantify uncertainties. In recent years the computation of the posterior distribution has been revolutionized by special types of Monte Carlo that use either a Markov chain or random walk or Hamiltonian dynamics. These methods provide approximations that converge to the correct posterior distribution after quite a few iterations. For examples and much more see (Robert and Casella 2009; Marin and Robert 2007; McElreath 2015).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "02-chap.html#exercises",
    "href": "02-chap.html#exercises",
    "title": "4  2.1 Goals for this chapter",
    "section": "4.12 2.13 Exercises",
    "text": "4.12 2.13 Exercises\n__\nExercise 2.1\nGenerate 1,000 random 0/1 variables that model mutations occurring along a 1,000 long gene sequence. These occur independently at a rate of \\(10^{-4}\\) each. Then sum the 1,000 positions to count how many mutations in sequences of length 1,000.\nFind the correct distribution for these mutation sums using a goodness of fit test and make a plot to visualize the quality of the fit.\n__\nExercise 2.2\nMake a function that generates \\(n\\) random uniform numbers between \\(0\\) and \\(7\\) and returns their maximum. Execute the function for \\(n=25\\). Repeat this procedure \\(B=100\\) times. Plot the distribution of these maxima.\nWhat is the maximum likelihood estimate of the maximum of a sample of size 25 (call it \\(\\))?\nCan you find a theoretical justification and the true maximum \\(\\)?\n__\nExercise 2.3\nA sequence of three nucleotides (a codon) taken in a coding region of a gene can be transcribed into one of 20 possible amino acids. There are \\(4^3=64\\) possible codon sequences, but only 20 amino acids. We say the genetic code is redundant: there are several ways to spell each amino acid.\nThe multiplicity (the number of codons that code for the same amino acid) varies from 2 to 6. The different codon-spellings of each amino acid do not occur with equal probabilities. Let’s look at the data for the standard laboratory strain of tuberculosis (H37Rv):\nmtb = read.table(\"../data/M_tuberculosis.txt\", header = TRUE)\nhead(mtb, n = 4)__\n\n\n  AmAcid Codon Number PerThous\n1    Gly   GGG  25874    19.25\n2    Gly   GGA  13306     9.90\n3    Gly   GGT  25320    18.84\n4    Gly   GGC  68310    50.82\nThe codons for the amino acid proline are of the form \\(CC*\\), and they occur with the following frequencies in Mycobacterium turberculosis:\npro  =  mtb[ mtb$AmAcid == \"Pro\", \"Number\"]\npro/sum(pro)__\n\n\n[1] 0.54302025 0.10532985 0.05859765 0.29305225\n\nExplore the data mtb using table to tabulate the AmAcid and Codon variables.\nHow was the PerThous variable created?\nWrite an R function that you can apply to the table to find which of the amino acids shows the strongest codon bias , i.e., the strongest departure from uniform distribution among its possible spellings.\n\n\\(*\\) stands for any of the 4 letters, using the computer notation for a regular expression.\n__\nExercise 2.4\nDisplay GC content in a running window along the sequence of Staphylococcus Aureus. Read in a fasta file sequence from a file.\nstaph = readDNAStringSet(\"../data/staphsequence.ffn.txt\", \"fasta\")__\n\nLook at the complete staph object and then display the first three sequences in the set.\nFind the GC content along the sequence in sliding windows of width 100.\nDisplay the result of b).\nHow could we visualize the overall trends of these proportions along the sequence?\n\n__\nSolution\n__\n\nThe data is displayed using:\n\nstaph[1:3, ]__\n\n\nDNAStringSet object of length 3:\n    width seq                                               names               \n[1]  1362 ATGTCGGAAAAAGAAATTTGGGA...AAAAAGAAATAAGAAATGTATAA lcl|NC_002952.2_c...\n[2]  1134 ATGATGGAATTCACTATTAAAAG...TTTTACCAATCAGAACTTACTAA lcl|NC_002952.2_c...\n[3]   246 GTGATTATTTTGGTTCAAGAAGT...TCATTCATCAAGGTGAACAATGA lcl|NC_002952.2_c...\n\n\nstaph __\n\n\nDNAStringSet object of length 2650:\n       width seq                                            names               \n   [1]  1362 ATGTCGGAAAAAGAAATTTGGG...AAAGAAATAAGAAATGTATAA lcl|NC_002952.2_c...\n   [2]  1134 ATGATGGAATTCACTATTAAAA...TTACCAATCAGAACTTACTAA lcl|NC_002952.2_c...\n   [3]   246 GTGATTATTTTGGTTCAAGAAG...ATTCATCAAGGTGAACAATGA lcl|NC_002952.2_c...\n   [4]  1113 ATGAAGTTAAATACACTCCAAT...CAAGGTGAAATTATAAAGTAA lcl|NC_002952.2_c...\n   [5]  1932 GTGACTGCATTGTCAGATGTAA...TATGCAAACTTAGACTTCTAA lcl|NC_002952.2_c...\n   ...   ... ...\n[2646]   720 ATGACTGTAGAATGGTTAGCAG...ACTCCTTTACTTGAAAAATAA lcl|NC_002952.2_c...\n[2647]  1878 GTGGTTCAAGAATATGATGTAA...CTCCAAAGGGTGAGTGACTAA lcl|NC_002952.2_c...\n[2648]  1380 ATGGATTTAGATACAATTACGA...CAATTCTGCTTAGGTAAATAG lcl|NC_002952.2_c...\n[2649]   348 TTGGAAAAAGCTTACCGAATTA...TTTAATAAAAAGATTAAGTAA lcl|NC_002952.2_c...\n[2650]   138 ATGGTAAAACGTACTTATCAAC...CGTAAAGTTTTATCTGCATAA lcl|NC_002952.2_c...\n\nWe can compute the frequencies using the function letterFrequency.\n\nletterFrequency(staph[[1]], letters = \"ACGT\", OR = 0)__\n\n\n  A   C   G   T \n522 219 229 392 \n\n\nGCstaph = data.frame(\n  ID = names(staph),\n  GC = rowSums(alphabetFrequency(staph)[, 2:3] / width(staph)) * 100\n)__\n\nPlotting can be done as follows, here exemplarily for sequence 364 (Figure 2.28):\n\nwindow = 100\ngc = rowSums( letterFrequencyInSlidingView(staph[[364]], window,\n      c(\"G\",\"C\")))/window\nplot(x = seq(along = gc), y = gc, type = \"l\")__\n\nFigure 2.28: GC content along sequence 364 of the Staphylococcus Aureus genome.\n\nWe can look at the overall trends by smoothing the data using the function lowess along a window.\n\nplot(x = seq(along = gc), y = gc, type = \"l\")\nlines(lowess(x = seq(along = gc), y = gc, f = 0.2), col = 2)__\n\nFigure 2.29: Similar to Figure 2.28, with smoothing.\nWe will see later an appropriate way of deciding whether the window has an abnormally high GC content by using the idea that as we move along the sequences, we are always in one of several possible states. However, we don’t directly observe the state, just the sequence. Such models are called hidden (state) Markov models , or HMM for short (see Wikipedia). The Markov in the name of these models is for how they model dependencies between neighboring positions, the hidden part indicates that the state is not directly observed, that is, hidden.\n__\nExercise 2.5\nRedo a figure similar to Figure 2.19, but include two other distributions: the uniform (which is Beta(1,1)) and Beta(\\(,\\)). What do you notice?\n__\nSolution\n__\ndfbetas = data.frame(\n  p = rep(p_seq, 5),\n  dbeta = c(dbeta(p_seq, 0.5, 0.5), \n            dbeta(p_seq,   1,   1), \n            dbeta(p_seq,  10,  30),\n            dbeta(p_seq,  20,  60), \n            dbeta(p_seq,  50, 150)),\n  pars = rep(c(\"Beta(0.5,0.5)\", \"U(0,1)=Beta(1,1)\", \n               \"Beta(10,30)\", \"Beta(20,60)\", \n               \"Beta(50,150)\"), each = length(p_seq)))\nggplot(dfbetas) +\n  geom_line(aes(x = p, y = dbeta, colour = pars)) +\n  theme(legend.title = element_blank()) +\n  geom_vline(aes(xintercept = 0.25), colour = \"#990000\", linetype = \"dashed\")__\n\nFigure 2.30: Beta densities for different parameter choices.\nWhereas the Beta distributions with parameters larger than one are unimodal, the Beta(0.5,0.5) distribution is bimodal and the Beta(1,1) is flat and has no mode.\n__\nExercise 2.6\nChoose your own prior for the parameters of the Beta distribution. You can do this by sketching it here: https://jhubiostatistics.shinyapps.io/drawyourprior. Once you have set up a prior, re-analyse the data from Section 2.9.1, where we saw \\(Y = 40\\) successes out of \\(n=300\\) trials. Compare your posterior distribution to the one we obtained in that section using a QQ-plot.\nAgresti, Alan. 2007. An Introduction to Categorical Data Analysis. John Wiley.\nCannings, Chris, and Anthony WF Edwards. 1968. “Natural Selection and the de Finetti Diagram.” Annals of Human Genetics 31 (4): 421–28.\nCleveland, William S. 1988. The Collected Works of John w. Tukey: Graphics 1965-1985. Vol. 5. CRC Press.\nDurbin, Richard, Sean Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis. Cambridge University Press.\nElson, D, and E Chargaff. 1952. “On the Desoxyribonucleic Acid Content of Sea Urchin Gametes.” Experientia 8 (4): 143–45.\nFinetti, Bruno de. 1926. “Considerazioni Matematiche Sull’ereditarieta Mendeliana.” Metron 6: 3–41.\nFreedman, David, Robert Pisani, and Roger Purves. 1997. Statistics. New York, NY: WW Norton.\nGrantham, Richard, Christian Gautier, Manolo Gouy, M Jacobzone, and R Mercier. 1981. “Codon Catalog Usage Is a Genome Strategy Modulated for Gene Expressivity.” Nucleic Acids Research 9 (1): 213–13.\nIrizarry, Rafael A, Hao Wu, and Andrew P Feinberg. 2009. “A Species- Generalized Probabilistic Model-Based Definition of CpG Islands.” Mammalian Genome 20 (9-10): 674–80.\nLove, Michael I, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation of Fold Change and Dispersion for RNA-seq Data with DESeq2.” Gnome Biology 15 (12): 1–21.\nMarin, Jean-Michel, and Christian Robert. 2007. Bayesian Core: A Practical Approach to Computational Bayesian Statistics. Springer Science & Business Media.\nMcElreath, Richard. 2015. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Chapman; Hall/CRC.\nMourant, AE, Ada Kopec, and K Domaniewska-Sobczak. 1976. “The Distribution of the Human Blood Groups 2nd Edition.” Oxford University Press London.\nPerrière, Guy, and Jean Thioulouse. 2002. “Use and Misuse of Correspondence Analysis in Codon Usage Studies.” Nucleic Acids Research 30 (20): 4548–55.\nRice, John. 2006. Mathematical Statistics and Data Analysis. Cengage Learning.\nRobert, Christian, and George Casella. 2009. Introducing Monte Carlo Methods with R. Springer Science & Business Media.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "03-chap.html",
    "href": "03-chap.html",
    "title": "5  3.1 Goals for this chapter",
    "section": "",
    "text": "5.1 3.2 Base R plotting\nThere are (at least) two types of data visualization. The first enables a scientist to explore data and make discoveries about the complex processes at work. The other type of visualization provides informative, clear and visually attractive illustrations of her results that she can show to others and eventually include in a publication.\nBoth of these types of visualizations can be made with R. In fact, R offers multiple graphics systems. This is because R is extensible, and because progress in R graphics over the years has proceeded largely not by replacing the old functions, but by adding packages. Each of the different graphics systems has its advantages and limitations. In this chapter we will get to know two of them. First, we have a cursory look at the base R plotting functions1. Subsequently we will switch to ggplot2.\n1 They live in the graphics package, which ships with every basic R installation.\nFigure 3.1: The ZUSE Plotter Z64 (presented in 1961). Source: https://en.wikipedia.org/wiki/Plotter.\nBase R graphics came historically first: simple, procedural, conceptually motivated by drawing on a canvas. There are specialized functions for different types of plots. These are easy to call – but when you want to combine them to build up more complex plots, or exchange one for another, this quickly gets messy, or even impossible. The user plots (the word harks back to some of the first graphics devices – see Figure 3.1) directly onto a (conceptual) canvas. She explicitly needs to deal with decisions such as how much space to allocate to margins, axes labels, titles, legends, subpanels; once something is “plotted” it cannot be moved or erased.\nThere is a more high-level approach: in the grammar of graphics , graphics are built up from modular logical pieces, so that we can easily try different visualization types for our data in an intuitive and easily deciphered way, like we can switch in and out parts of a sentence in human language. There is no concept of a canvas or a plotter, rather, the user gives ggplot2 a high-level description of the plot she wants, in the form of an R object, and the rendering engine takes a wholistic view on the scene to lay out the graphics and render them on the output device.\nIn this chapter, we will:\nThe most basic function is plot. In the code below, the output of which is shown in Figure 3.2, it is used to plot data from an enzyme-linked immunosorbent assay (ELISA) assay. The assay was used to quantify the activity of the enzyme deoxyribonuclease (DNase), which degrades DNA. The data are assembled in the R object DNase, which conveniently comes with base R. The object DNase is a dataframe whose columns are Run, the assay run; conc, the protein concentration that was used; and density, the measured optical density.\nFigure 3.2: Plot of concentration vs. density for an ELISA assay of DNase.\nThis basic plot can be customized, for example by changing the plot symbol and axis labels using the parameters xlab, ylab and pch (plot character), as shown in Figure 3.3. Information about the variables is stored in the object DNase, and we can access it with the attr function.\nFigure 3.3: Same data as in Figure 3.2 but with better axis labels and a different plot symbol.\n__\nQuestion 3.1\nAnnotating dataframe columns with “metadata” such as longer descriptions, physical units, provenance information, etc., seems like a useful feature. Is this way of storing such information, as in the DNase object, standardized or common across the R ecosystem? Are there other standardized or common ways for doing this?\n__\nSolution\n__\nThere is no good or widely used infrastructure in regular R data.frame s for this, nor in the tidyverse (data_frame , tibble). But have a look at the DataFrame class in the Bioconductor package S4Vectors. Among other things it is used to annotate the rows and columns of a SummarizedExperiment.\nBesides scatterplots, we can also use built-in functions to create histograms and boxplots (Figure 3.4).\nFigure 3.4: (a) Histogram of the density from the ELISA assay, and (b) boxplots of these values stratified by the assay run. The boxes are ordered along the axis in lexicographical order because the runs were stored as text strings. We could use R’s type conversion functions to achieve numerical ordering.\nBoxplots are convenient for showing multiple distributions next to each other in a compact space. We will see more about plotting multiple univariate distributions in Section 3.6.\nThe base R plotting functions are great for quick interactive exploration of data; but we run soon into their limitations if we want to create more sophisticated displays. We are going to use a visualization framework called the grammar of graphics, implemented in the package ggplot2 , that enables step by step construction of high quality graphics in a logical and elegant manner. First let us introduce and load an example dataset.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "03-chap.html#base-r-plotting",
    "href": "03-chap.html#base-r-plotting",
    "title": "5  3.1 Goals for this chapter",
    "section": "",
    "text": "head(DNase)__\n\n\n  Run       conc density\n1   1 0.04882812   0.017\n2   1 0.04882812   0.018\n3   1 0.19531250   0.121\n4   1 0.19531250   0.124\n5   1 0.39062500   0.206\n6   1 0.39062500   0.215\n\n\nplot(DNase$conc, DNase$density)__\n\n\n\nplot(DNase$conc, DNase$density,\n  ylab = attr(DNase, \"labels\")$y,\n  xlab = paste(attr(DNase, \"labels\")$x, attr(DNase, \"units\")$x),\n  pch = 3,\n  col = \"blue\")__\n\n\n\n\n\n\n\n\n\n\nhist(DNase$density, breaks=25, main = \"\")\nboxplot(density ~ Run, data = DNase)__",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "03-chap.html#an-example-dataset",
    "href": "03-chap.html#an-example-dataset",
    "title": "5  3.1 Goals for this chapter",
    "section": "5.2 3.3 An example dataset",
    "text": "5.2 3.3 An example dataset\n\nFigure 3.5: Single-section immunofluorescence image of the E3.5 mouse blastocyst stained for Serpinh1, a marker of primitive endoderm (blue), Gata6 (red) and Nanog (green).\nTo properly testdrive the ggplot2 functionality, we are going to need a dataset that is big enough and has some complexity so that it can be sliced and viewed from many different angles. We’ll use a gene expression microarray dataset that reports the transcriptomes of around 100 individual cells from mouse embryos at different time points in early development. The mammalian embryo starts out as a single cell, the fertilized egg. Through synchronized waves of cell divisions, the egg multiplies into a clump of cells that at first show no discernible differences between them. At some point, though, cells choose different lineages. By further and further specification, the different cell types and tissues arise that are needed for a full organism. The aim of the experiment, explained by Ohnishi et al. (2014), was to investigate the gene expression changes associated with the first symmetry breaking event in the embryo. We’ll further explain the data as we go. More details can be found in the paper and in the documentation of the Bioconductor data package Hiiragi2013. We first load the data:\n\nIt is unfortunate that the data object has the rather generic name x, rather than a more descriptive name. To avoid name collisions, perhaps the most pragmatic solution would be to run code such as following: esHiiragi = x; rm(list=\"x\").\nlibrary(\"Hiiragi2013\")__\n\n\nIn chunk 'loadHiiragi': Warning: replacing previous import 'boot::logit' by 'gtools::logit' whenloading 'Hiiragi2013'\n\n\nIn chunk 'loadHiiragi': Warning: replacing previous import 'boot::inv.logit' by 'gtools::inv.logit'when loading 'Hiiragi2013'\n\n\ndata(\"x\")\ndim(Biobase::exprs(x))__\n\n\n[1] 45101   101\nYou can print out a more detailed summary of the ExpressionSet object x by just typing x at the R prompt. The 101 columns of the data matrix (accessed above through the exprs function from the Biobase package) correspond to the samples (each of these is a single cell), the 45101 rows correspond to the genes probed by the array, an Affymetrix mouse4302 array. The data were normalized using the RMA method (Irizarry et al. 2003). The raw data are also available in the package (in the data object a) and at EMBL-EBI’s ArrayExpress database under the accession code E-MTAB-1681.\nLet’s have a look at what information is available about the samples2.\n2 The notation #CAB2D6 is a hexadecimal representation of the RGB coordinates of a color; more on this in Section 3.10.2.\nhead(pData(x), n = 2)__\n\n\n        File.name Embryonic.day Total.number.of.cells lineage genotype\n1 E3.25  1_C32_IN         E3.25                    32               WT\n2 E3.25  2_C32_IN         E3.25                    32               WT\n          ScanDate sampleGroup sampleColour\n1 E3.25 2011-03-16       E3.25      #CAB2D6\n2 E3.25 2011-03-16       E3.25      #CAB2D6\nThe information provided is a mix of information about the cells (i.e., age, size and genotype of the embryo from which they were obtained) and technical information (scan date, raw data file name). By convention, time in the development of the mouse embryo is measured in days, and reported as, for instance, E3.5. Moreover, in the paper the authors divided the cells into 8 biological groups (sampleGroup), based on age, genotype and lineage, and they defined a color scheme to represent these groups (sampleColour3). Using the following code (see below for explanations), we define a small dataframe groups that contains summary information for each group: the number of cells and the preferred color.\n3 This identifier in the dataset uses the British spelling. Everywhere else in this book, we use the US spelling (color). The ggplot2 package generally accepts both spellings.\nlibrary(\"dplyr\")\ngroups = group_by(pData(x), sampleGroup) |&gt;\n  summarise(n = n(), color = unique(sampleColour))\ngroups __\n\n\n# A tibble: 8 × 3\n  sampleGroup         n color  \n  &lt;chr&gt;           &lt;int&gt; &lt;chr&gt;  \n1 E3.25              36 #CAB2D6\n2 E3.25 (FGF4-KO)    17 #FDBF6F\n3 E3.5 (EPI)         11 #A6CEE3\n4 E3.5 (FGF4-KO)      8 #FF7F00\n5 E3.5 (PE)          11 #B2DF8A\n6 E4.5 (EPI)          4 #1F78B4\n7 E4.5 (FGF4-KO)     10 #E31A1C\n8 E4.5 (PE)           4 #33A02C\nThe cells in the groups whose name contains FGF4-KO are from embryos in which the FGF4 gene, an important regulator of cell differentiation, was knocked out. Starting from E3.5, the wildtype cells (without the FGF4 knock- out) undergo the first symmetry breaking event and differentiate into different cell lineages, called pluripotent epiblast (EPI) and primitive endoderm (PE).\nSince the code chunk above is the first instance that we encounter the pipe operator |&gt; and the functions group_by and summarise from the dplyr package, let’s unpack the code. First, the pipe |&gt;4. Generally, the pipe is useful for making nested function calls easier to read for humans. The following two lines of code are equivalent to R.\n4 |&gt; is the pipe operator that has been coming with base R since version 4.1, released in 2021. The package magrittr has provided the %&gt;% operator, which has similar although not identical semantics, already since a long time before, as well as several other piping related operators, such as %&lt;&gt;% and %T&gt;%. As much of the book was written before 2021, magrittr ’s %&gt;% operator is used in many places. We are occasionally updating from %&gt;% to |&gt; during book maintenance, as in the code shown here.\nf(x) |&gt; g(y) |&gt; h()\nh(g(f(x), y))__\nIt says: “Evaluate f(x), then pass the result to function g as the first argument, while y is passed to g as the second argument. Then pass the output of g to the function h.” You could repeat this ad infinitum. Especially if the arguments x and y are complex expressions themselves, or if there is quite a chain of functions involved, the first version tends to be easier to read.\nThe group_by function simply “marks” the dataframe with a note that all subsequent operations should not be applied to the whole dataframe at once, but to blocks defined by the sampleGroup factor. Finally, summarise computes summary statistics; this could be, e.g., the mean, sum; in this case, we just compute the number of rows in each block, n(), and the prevalent color.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "03-chap.html#ggplot2",
    "href": "03-chap.html#ggplot2",
    "title": "5  3.1 Goals for this chapter",
    "section": "5.3 3.4 ggplot2",
    "text": "5.3 3.4 ggplot2\nggplot2 is a package by Hadley Wickham (Wickham 2016) that implements the idea of grammar of graphics – a concept created by Leland Wilkinson in his eponymous book (Wilkinson 2005). We will explore some of its functionality in this chapter, and you will see many examples of how it can be used in the rest of this book. Comprehensive documentation for the package can be found on its website. The online documentation includes example use cases for each of the graphic types that are introduced in this chapter (and many more) and is an invaluable resource when creating figures.\nLet’s start by loading the package and redoing the simple plot of Figure 3.2.\nlibrary(\"ggplot2\")\nggplot(DNase, aes(x = conc, y = density)) + geom_point()__\n\nFigure 3.6: Our first ggplot2 figure, similar to the base graphics Figure 3.2.\nWe just wrote our first “sentence” using the grammar of graphics. Let’s deconstruct this sentence. First, we specified the dataframe that contains the data, DNase. The aes (this stands for aesthetic) argument states which variables we want to see mapped to the \\(x\\)- and \\(y\\)-axes, respectively. Finally, we stated that we want the plot to use points (as opposed to, say, lines or bars), by adding the result of calling the function geom_point.\nNow let’s turn to the mouse single cell data and plot the number of samples for each of the 8 groups using the ggplot function. The result is shown in Figure 3.7.\nggplot(groups, aes(x = sampleGroup, y = n)) +\n  geom_bar(stat = \"identity\")__\n\nFigure 3.7: A barplot, produced with the ggplot function from the table of group sizes in the mouse single cell data.\nWith geom_bar we now told ggplot that we want each data item (each row of groups) to be represented by a bar. Bars are one example of geometric object (geom in the ggplot2 package’s parlance) that ggplot knows about. We have already seen another such object in Figure 3.6: points, indicated by the geom_point function. We will encounter many other geoms later. We used the aes to indicate that we want the groups shown along the \\(x\\)-axis and the sizes along the \\(y\\)-axis. Finally, we provided the argument stat = \"identity\" (in other words, do nothing) to the geom_bar function, since otherwise it would try to compute a histogram of the data (the default value of stat is \"count\"). stat is short for statistic , which is what we call any function of data. Besides the identity and count statistic, there are others, such as smoothing, averaging, binning, or other operations that reduce the data in some way.\nThese concepts –data, geometrical objects, statistics– are some of the ingredients of the grammar of graphics, just as nouns, verbs and adverbs are ingredients of an English sentence.\n__\nTask\nFlip the \\(x\\)- and \\(y\\)-aesthetics to produce a horizontal barplot.\nThe plot in Figure 3.7 is not bad, but there are several potential improvements. We can use color for the bars to help us quickly see which bar corresponds to which group. This is particularly useful if we use the same color scheme in several plots. To this end, let’s define a named vector groupColor that contains our desired colors for each possible value of sampleGroup.5\n5 The information is completely equivalent to that in the sampleGroup and color columns of the dataframe groups; we are just adapting to the fact that ggplot2 expects this information in the form of a named vector.\ngroupColor = setNames(groups$color, groups$sampleGroup)__\nAnother thing that we need to fix in Figure 3.7 is the readability of the bar labels. Right now they are running into each other — a common problem when you have descriptive names.\nggplot(groups, aes(x = sampleGroup, y = n, fill = sampleGroup)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = groupColor, name = \"Groups\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))__\n\nFigure 3.8: Similar to Figure 3.7, but with colored bars and better bar labels.\nThis is now already a longer and more complex sentence. Let’s dissect it. We added an argument, fill to the aes function that states that we want the bars to be colored (filled) based on sampleGroup (which in this case co- incidentally is also the value of the x argument, but that need not always be so). Furthermore we added a call to the scale_fill_manual function, which takes as its input a color map – i.e., the mapping from the possible values of a variable to the associated colors – as a named vector. We also gave this color map a title (note that in more complex plots, there can be several different color maps involved). Had we omitted the call to scale_fill_manual, ggplot2 would have used its choice of default colors. We also added a call to theme stating that we want the \\(x\\)-axis labels rotated by 90 degrees and right-aligned (hjust; the default would be to center).\n\n5.3.1 3.4.1 Data flow\n\nThe function ggplot expects your data in a dataframe. If they are in a matrix, in separate vectors, or other types of objects, you will have to convert them. The packages dplyr and broom , among others, offer facilities to this end. We will discuss this more in Section 13.10, and you will see examples of such conversions throughout the book.\nThis includes the base R data.frame as well as the tibble (and synonymous data_frame) classes from the tibble package in the tidyverse.\nThe result of a call to the ggplot is a ggplot object. Let’s recall a piece of code from above:\ngg = ggplot(DNase, aes(x = conc, y = density)) + geom_point()__\nWe have now assigned the output of ggplot to the object gg, instead of sending it directly to the console, where it was “printed” and produced Figure 3.6. The situation is completely analogous to what you’re used to from working with the R console: when you enter an expression such as 1+1 and hit “Enter”, the result is printed. When the expression is an assignment, such as s = 1+1, the side effect takes place (the name \"s\" is bound to an object in memory that represents the value of 1+1), but nothing is printed. Similarly, when an expression is evaluated as part of a script called with source, it is not printed. Thus, the above code also does not create any graphic output, since no print method is invoked. To print gg, type its name (in an interactive session) or call print on it:\ngg\nprint(gg)__\n\n\n5.3.2 3.4.2 Saving figures\nggplot2 has a built-in plot saving function called ggsave:\nggplot2::ggsave(\"DNAse-histogram-demo.pdf\", plot = gg)__\nThere are two major ways of storing plots: vector graphics and raster (pixel) graphics. In vector graphics, the plot is stored as a series of geometrical primitives such as points, lines, curves, shapes and typographic characters. The preferred format in R for saving plots into a vector graphics format is PDF. In raster graphics, the plot is stored in a dot matrix data structure. The main limitation of raster formats is their limited resolution, which depends on the number of pixels available. In R, the most commonly used device for raster graphics output is png. Generally, it’s preferable to save your plots in a vector graphics format, since it is always possible later to convert a vector graphics file into a raster format of any desired resolution, while the reverse is quite difficult. And you don’t want the figures in your talks or papers look poor because of pixelization artefacts!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "03-chap.html#the-grammar-of-graphics",
    "href": "03-chap.html#the-grammar-of-graphics",
    "title": "5  3.1 Goals for this chapter",
    "section": "5.4 3.5 The grammar of graphics",
    "text": "5.4 3.5 The grammar of graphics\nThe components of ggplot2 ’s grammar of graphics are\n\none or more datasets,\none or more geometric objects that serve as the visual representations of the data, – for instance, points, lines, rectangles, contours,\ndescriptions of how the variables in the data are mapped to visual properties (aesthetics) of the geometric objects, and an associated scale (e. g., linear, logarithmic, rank),\none or more coordinate systems,\nstatistical summarization rules,\na facet specification, i.e. the use of multiple similar subplots to look at subsets of the same data,\noptional parameters that affect the layout and rendering, such text size, font and alignment, legend positions.\n\nIn the examples above, Figures 3.7 and 3.8, the dataset was groupsize, the variables were the numeric values as well as the names of groupsize, which we mapped to the aesthetics \\(y\\)-axis and \\(x\\)-axis respectively, the scale was linear on the \\(y\\) and rank-based on the \\(x\\)-axis (the bars are ordered alphanumerically and each has the same width), and the geometric object was the rectangular bar.\nItems 4–7 in the above list are optional. If you don’t specify them, then the Cartesian is used as the coordinate system, the statistical summary is the trivial one (i.e., the identity), and no facets or subplots are made (we’ll see examples later on, in Section 3.8). The first three items are required: a valid ggplot2 “sentence” needs to contain at least one of each of them.\nIn fact, ggplot2 ’s implementation of the grammar of graphics allows you to use the same type of component multiple times, in what are called layers (Wickham 2010). For example, the code below uses three types of geometric objects in the same plot, for the same data: points, a line and a confidence band.\ndftx = data.frame(t(Biobase::exprs(x)), pData(x))\nggplot( dftx, aes( x = X1426642_at, y = X1418765_at )) +\n  geom_point( shape = 1 ) +\n  geom_smooth( method = \"loess\" )__\n\nFigure 3.9: A scatterplot with three layers that show different statistics of the same data: points (geom_point), a smooth regression line and a confidence band (the latter two from geom_smooth).\nHere we had to assemble a copy of the expression data (Biobase::exprs(x)) and the sample annotation data (pData(x)) all together into the dataframe dftx – since this is the data format that ggplot2 functions most easily take as input (more on this in Section 13.10).\nWe can further enhance the plot by using colors – since each of the points in Figure 3.9 corresponds to one sample, it makes sense to use the sampleColour information in the object x.\nggplot(dftx, aes(x = X1426642_at, y = X1418765_at))  +\n  geom_point(aes(color = sampleGroup), shape = 19) +\n  scale_color_manual(values = groupColor, guide = \"none\") +\n  geom_smooth(method = \"loess\")__\n\nFigure 3.10: As Figure 3.9, but in addition with points colored by the time point and cell lineage (as defined in Figure 3.8). We can now see that the expression values of the gene Timd2 (targeted by the probe 1418765_at, along the y-axis) are consistently high in the early time points, whereas its expression goes down in the EPI samples at days 3.5 and 4.5. In the FGF4-KO, this decrease is delayed - at E3.5, its expression is still high. Conversely, the gene Fn1 (1426642_at, x-axis) is off in the early timepoints and then goes up at days 3.5 and 4.5. The PE samples (green) show a high degree of cell-to- cell variability.\n__\nQuestion 3.2\nIn the code above we defined the color aesthetics (aes) only for the geom_point layer, while we defined the x and y aesthetics for all layers. What happens if we set the color aesthetics for all layers, i.e., move it into the argument list of ggplot?\n__\nQuestion 3.3\nIs it always meaningful to visualize scatterplot data together with a regression line as in Figures 3.9 and 3.10?\nAs an aside, if we want to find out which genes are targeted by these probe identifiers, and what they might do, we can call:\n\nNote that here were need to use the original feature identifiers (e.,g., “1426642_at”, without the leading “X”). This is the notation used by the microarray manufacturer, by the Bioconductor annotation packages, and also inside the object x. The leading “X” that we used above when working with dftx was inserted during the creation of dftx by the constructor function data.frame, since its argument check.names is set to TRUE by default. Alternatively, we could have kept the original identifier notation by setting check.names = FALSE, but then our code (e.g., the calls to aes()) would need to use backticks around the identifiers to make sure R interprets them correctly.\nNote the use of the :: operator to call the select function by its fully qualified name, including the package. We already encountered this in Chapter 2.\nlibrary(\"mouse4302.db\")__\n\n\nAnnotationDbi::select(mouse4302.db,\n   keys = c(\"1426642_at\", \"1418765_at\"), keytype = \"PROBEID\",\n   columns = c(\"SYMBOL\", \"GENENAME\"))__\n\n\n     PROBEID SYMBOL                                            GENENAME\n1 1426642_at    Fn1                                       fibronectin 1\n2 1418765_at  Timd2 T cell immunoglobulin and mucin domain containing 2\nOften when using ggplot you will only need to specify the data, aesthetics and a geometric object. Most geometric objects implicitly call a suitable default statistical summary of the data. For example, if you are using geom_smooth, then ggplot2 uses stat = \"smooth\" by default and displays a line; if you use geom_histogram, the data are binned and the result is displayed in barplot format. Here’s an example:\ndfx = as.data.frame(Biobase::exprs(x))\nggplot(dfx, aes(x = `20 E3.25`)) + geom_histogram(binwidth = 0.2)__\n\nFigure 3.11: Histogram of probe intensities for one particular sample, cell number 20, which was from day E3.25.\n__\nQuestion 3.4\nWhat is the difference between the objects dfx and dftx? Why did we need to create them both?\nLet’s come back to the barplot example from above.\npb = ggplot(groups, aes(x = sampleGroup, y = n))__\nThis creates a plot object pb. If we try to display it, it creates an empty plot, because we haven’t specified what geometric object we want to use. All that we have in our pb object so far are the data and the aesthetics (Figure 3.12).\nclass(pb)__\n\n\n[1] \"gg\"     \"ggplot\"\n\n\npb __\n\nFigure 3.12: pb: without a geometric object (a geom), the plot area remains empty. With the default style parameters, the tick labels on the \\(x\\)-axis are not legible.\nNow we can simply add on the other components of our plot through using the + operator (Figure 3.13):\npb = pb + geom_bar(stat = \"identity\")\npb = pb + aes(fill = sampleGroup)\npb = pb + theme(axis.text.x = element_text(angle = 90, hjust = 1))\npb = pb + scale_fill_manual(values = groupColor, name = \"Groups\")\npb __\n\nFigure 3.13: The graphics object bp in its full glory.\nThis step-wise buildup –taking a graphics object already produced in some way and then further refining it– can be more convenient and easy to manage than, say, providing all the instructions upfront to the single function call that creates the graphic.\nWe can quickly try out different visualization ideas without having to rebuild our plots each time from scratch, but rather store the partially finished object and then modify it in different ways. For example we can switch our plot to polar coordinates to create an alternative visualization of the barplot.\npb.polar = pb + coord_polar() +\n  theme(axis.text.x = element_text(angle = 0, hjust = 1),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank()) +\n  xlab(\"\") + ylab(\"\")\npb.polar __\n\nFigure 3.14: A barplot in a polar coordinate system.\nNote above that we can override previously set theme parameters by simply setting them to a new value – no need to go back to recreating pb, where we originally set them.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "03-chap.html#visualizing-data-in-1d",
    "href": "03-chap.html#visualizing-data-in-1d",
    "title": "5  3.1 Goals for this chapter",
    "section": "5.5 3.6 Visualizing data in 1D",
    "text": "5.5 3.6 Visualizing data in 1D\nA common task in biological data analysis is the comparison between several samples of univariate measurements. In this section we’ll explore some possibilities for visualizing and comparing such samples. As an example, we’ll use the intensities of a set of four genes: Fgf4, Gata4, Gata6 and Sox26. On the microarray, they are represented by\n6 You can read more about these genes in (Ohnishi et al. 2014).\nselectedProbes = c( Fgf4 = \"1420085_at\", Gata4 = \"1418863_at\",\n                   Gata6 = \"1425463_at\",  Sox2 = \"1416967_at\")__\nTo extract data from this representation and convert them into a dataframe, we use the function melt from the reshape2 package7.\n7 We’ll talk more about the concepts and mechanics of different data representations in Section 13.10.\nlibrary(\"reshape2\")\ngenes = melt(Biobase::exprs(x)[selectedProbes, ],\n             varnames = c(\"probe\", \"sample\"))__\nFor good measure, we also add a column that provides the gene symbol along with the probe identifiers.\ngenes$gene =\n  names(selectedProbes)[match(genes$probe, selectedProbes)]\nhead(genes)__\n\n\n       probe  sample    value  gene\n1 1420085_at 1 E3.25 3.027715  Fgf4\n2 1418863_at 1 E3.25 4.843137 Gata4\n3 1425463_at 1 E3.25 5.500618 Gata6\n4 1416967_at 1 E3.25 1.731217  Sox2\n5 1420085_at 2 E3.25 9.293016  Fgf4\n6 1418863_at 2 E3.25 5.530016 Gata4\n\n5.5.1 3.6.1 Barplots\nA popular way to display data such as in our dataframe genes is through barplots (Figure 3.15).\nggplot(genes, aes(x = gene, y = value)) +\n  stat_summary(fun = mean, geom = \"bar\")__\n\nFigure 3.15: Barplots showing the means of the distributions of expression measurements from four probes.\nIn Figure 3.15, each bar represents the mean of the values for that gene. Such plots are commonly used in the biological sciences, as well as in the popular media. However, summarizing the data into only a single number, the mean, loses much of the information, and given the amount of space they take, barplot are a poor way to visualize data8.\n8 In fact, if the mean is not an appropriate summary, such as for highly skewed or multimodal distributions, or for datasets with large outliers, this kind of visualization can be outright misleading.\nSometimes we want to add error bars, and one way to achieve this in ggplot2 is as follows.\nlibrary(\"Hmisc\")\nggplot(genes, aes( x = gene, y = value, fill = gene)) +\n  stat_summary(fun = mean, geom = \"bar\") +\n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\",\n               width = 0.25)__\n\nFigure 3.16: Barplots with error bars indicating standard error of the mean.\nHere, we see again the principle of layered graphics: we use two summary functions, mean and mean_cl_normal, and two associated geometric objects, bar and errorbar. The function mean_cl_normal is from the Hmisc package and computes the standard error (or c onfidence l imits) of the mean; it’s a simple function, and we could also compute it ourselves using base R expressions if we wished to do so. We have also colored the bars to make the plot more visually pleasing.\n\n\n5.5.2 3.6.2 Boxplots\nBoxplots take up a similar amount of space as barplots, but are much more informative.\np = ggplot(genes, aes( x = gene, y = value, fill = gene))\np + geom_boxplot()__\n\nFigure 3.17: Boxplots.\nIn Figure 3.17 we see that two of the genes (Gata4, Gata6) have relatively concentrated distributions, with only a few data points venturing out in the direction of higher values. For Fgf4, we see that the distribution is right- skewed: the median, indicated by the horizontal black bar within the box is closer to the lower (or left) side of the box. Conversely, for Sox2 the distribution is left-skewed.\n\n\n5.5.3 3.6.3 Dot plots and beeswarm plots\nIf the number of data points is not too large, it is possible to show the data points directly, and it is good practice to do so, compared to the summaries we saw above. However, plotting the data directly will often lead to overlapping points, which can be visually unpleasant, or even obscure the data. We can try to lay out the points so that they are as near possible to their proper locations without overlap (Wilkinson 1999).\np + geom_dotplot(binaxis = \"y\", binwidth = 1/6,\n       stackdir = \"center\", stackratio = 0.75,\n       aes(color = gene))\nlibrary(\"ggbeeswarm\")\np + geom_beeswarm(aes(color = gene))__\n[](03-chap_files/figure-html/fig-graphics-oneddot-1.png “Figure 3.18 (a):”)\n\n\n\n[](03-chap_files/figure-html/fig-graphics-oneddot-2.png “Figure 3.18 (b):”)\n\n\n\nFigure 3.18: (a) Dot plots, made using geom_dotplot from ggplot2. (b) Beeswarm plots, made using geom_beeswarm from ggbeeswarm.\nThe plot in panel (a) of Figure 3.18. The \\(y\\)-coordinates of the points are discretized into bins (above we chose a bin size of 1/6), and then they are stacked next to each other.\nAn alternative is provided by the package ggbeeswarm , which provides geom_beeswarm. The plot is shown in panel (b) of Figure 3.18. The layout algorithm aims to avoid overlaps between the points. If a point were to overlap an existing point, it is shifted along the \\(x\\)-axis by a small amount sufficient to avoid overlap. Some tweaking of the layout parameters is usually needed for each new dataset to make a dot plot or a beeswarm plot look good.\n\n\n5.5.4 3.6.4 Density plots\nYet another way to represent the same data is by density plots. Here, we try to estimate the underlying data-generating density by smoothing out the data points (Figure 3.19).\nggplot(genes, aes( x = value, color = gene)) + geom_density()__\n\nFigure 3.19: Density plots.\nAs you can see from Figures 3.17—3.19, boxplots work fairly well for unimodal data, but they can be misleading if the data distribution is multimodal, and they also do not always give a fair impression of long-tailed distributions. By showing the data points, or their densities, directly, Figures 3.18—3.19) give a better impression of such features. For instance, we can see that the data for Fgf4 and Sox2 are bimodal, while Gata4 and Gata6 have most of their values concentrated around a baseline, but a certain fraction of cells have elevated expression levels, across a wide range of values.\nDensity estimation has, however, a number of complications, in particular, the need for choosing a smoothing window. A window size that is small enough to capture peaks in the dense regions of the data may lead to instable (“wiggly”) estimates elsewhere. On the other hand, if the window is made bigger, pronounced features of the density, such as sharp peaks, may be smoothed out. Moreover, the density lines do not convey the information on how much data was used to estimate them, and plots like Figure 3.19 can be especially problematic if the sample sizes for the curves differ.\n\n\n5.5.5 3.6.5 Violin plots\nDensity plots such as Figure 3.19 can look crowded if there are several densities, and one idea is to arrange the densities in a layout inspired by boxplots: the violin plot (Figure 3.20). Here, the density estimate is used to draw a symmetric shape, which sometimes visually reminds of a violin.\np + geom_violin()__\n\nFigure 3.20: Violin plots.\n\n\n5.5.6 3.6.6 Ridgeline plots\nAnother play on density plots are ridgeline plots (Figure 3.21):\nlibrary(\"ggridges\")\nggplot(genes, aes(x = value, y = gene, fill = gene)) + \n  geom_density_ridges()__\n\nFigure 3.21: Ridgeline plots.\nThis type of display is perhaps most appropriate if the number of densities shown goes into the dozens.\ntop42 = order(rowMeans(Biobase::exprs(x)), decreasing = TRUE)[1:42]\ng42 = melt(Biobase::exprs(x)[rev(top42), ], varnames = c(\"probe\", \"sample\"))\nggplot(g42, aes(x = value, y = probe)) __\n\nFigure 3.22: Like Figure 3.21, with more genes.\n\n\n5.5.7 3.6.7 ECDF plots\nThe mathematically most convenient way to describe the distribution of a one- dimensional random variable \\(X\\) is its cumulative distribution function (CDF), i.e., the function defined by\n\\[ F(x) = P(Xx), \\]\nwhere \\(x\\) takes all values along the real axis. The density of \\(X\\) is then the derivative of \\(F\\), if it exists9. The finite sample version of the probability 3.1 is called the empirical cumulative distribution function (ECDF),\n9 By its definition, \\(F\\) tends to 0 for small \\(x\\) (\\(x-\\)) and to 1 for large \\(x\\) (\\(x+\\)).\n\\[ F_{n}(x) = = _{i=1}^n 𝟙({xx_i}), \\]\nwhere \\(x_1,…,x_n\\) denote a sample of \\(n\\) draws from \\(X\\) and \\(𝟙\\) is the indicator function, i.e., the function that takes the value 1 if the expression in its argument is true and 0 otherwise. If this sounds abstract, we can get a perhaps more intuitive understanding from the following example (Figure 3.23):\nsimdata = rnorm(70)\ntibble(index = seq(along = simdata),\n          sx = sort(simdata)) %&gt;%\nggplot(aes(x = sx, y = index)) + geom_step()__\n\nFigure 3.23: Sorted values of simdata versus their index. This is the empirical cumulative distribution function of simdata.\nPlotting the sorted values against their ranks gives the essential features of the ECDF (Figure 3.23). In practice, we do not need to do the sorting and the other steps in the above code manually and will rather use the stat_ecdf() geometric object. The ECDFs of our data are shown in Figure 3.24.\nggplot(genes, aes( x = value, color = gene)) + stat_ecdf()__\n\nFigure 3.24: Empirical cumulative distribution functions (ECDF).\nThe ECDF has several nice properties:\n\nIt is lossless: the ECDF \\(F_{n}(x)\\) contains all the information contained in the original sample \\(x_1,…,x_n\\), except for the order of the values, which is assumed to be unimportant.\nAs \\(n\\) grows, the ECDF \\(F_{n}(x)\\) converges to the true CDF \\(F(x)\\). Even for limited sample sizes \\(n\\), the difference between the two functions tends to be small. Note that this is not the case for the empirical density! Without smoothing, the empirical density of a finite sample is a sum of Dirac delta functions, which is difficult to visualize and quite different from any underlying smooth, true density. With smoothing, the difference can be less pronounced, but is difficult to control, as we discussed above.\n\n\nFigure 3.25: Part of Figure 1 from Lawrence et al. (2013). Each dot corresponds to a tumour-normal pair, with vertical position indicating the total frequency of somatic mutations in the exome. The resulting curves are, in essence, ECDF plots, and conceptually this plot is similar to Figure 3.24, just that the graphs are rotated by 90 degrees (i.e., the roles of \\(x\\)- and \\(y\\)-axis are exchanged) and the curves for the individual tumor types are horizontally displaced to keep them better apart.\n__\nTask\ntibbles. In the above code we saw the tibble for the first time. Have a look at the vignette of the tibble package for what it does.\n\n\n5.5.8 3.6.8 The effect of transformations on densities\nIt is tempting to look at histograms or density plots and inspect them for evidence of bimodality (or multimodality) as an indication of some underlying biological phenomenon. Before doing so, it is important to remember that the number of modes of a density depends on scale transformations of the data, via the chain rule. For instance, let’s look at the data from one of the arrays in the Hiiragi dataset (Figure 3.26):\nggplot(dfx, aes(x = `64 E4.5 (EPI)`)) + geom_histogram(bins = 100)\nggplot(dfx, aes(x = 2 ^ `64 E4.5 (EPI)`)) + \n  geom_histogram(binwidth = 20) + xlim(0, 1500)__\n[](03-chap_files/figure-html/fig-graphics-onedtrsf-1.png “Figure 3.26 (a):”)\n\n\n\n[](03-chap_files/figure-html/fig-graphics-onedtrsf-2.png “Figure 3.26 (b):”)\n\n\n\nFigure 3.26: Histograms of the same data, with and without logarithm transform. (a) The data are shown on the scale on which they are stored in the data object x, which resulted from logarithm (base 2) transformation of the microarray fluorescence intensities (Irizarry et al. 2003); (b) after re-exponentiating them back to the fluorescence scale. For better use of space, we capped the \\(x\\)-axis range at 1500.\n__\nQuestion 3.5\n(Advanced:) Consider a random variable \\(X\\) and a non-linear 1:1 transformation \\(f: x y\\) that defines the transformed random variable \\(Y = f(X)\\). Suppose the density function of \\(Y\\) is \\(p(y)\\). What is the density of \\(X\\)? How is the mode (or: the modes) of \\(X\\) related to the mode(s) of \\(Y\\)?\nHint: note that a mode of a function \\(p\\) is a root of its derivative \\(p’=dp/dx\\). Is it generally true that if \\(x_0\\) is a mode of \\(X\\), then \\(y_0=f(x_0)\\) is a mode of \\(Y\\)?\n__\nSolution\n__\nAccording to the chain rule, \\(p(y), dy = p(f(x)),f’(x),dx\\), so the density of \\(X\\) is \\((x) = p(f(x)),f’(x)\\). The modes of \\(\\) are roots of its derivative \\(d/dx\\), i.e., they obey \\(p’(f(x)),f’^2(x) + p(f(x))f”(x) = 0\\). The second term in the sum vanishes if \\(f\\) is affine linear (\\(f”\\)), but in general there is no simple relationship between the roots of the two densities, and therefore between the modes of \\(X\\) and \\(Y\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "03-chap.html#visualizing-data-in-2d-scatterplots",
    "href": "03-chap.html#visualizing-data-in-2d-scatterplots",
    "title": "5  3.1 Goals for this chapter",
    "section": "5.6 3.7 Visualizing data in 2D: scatterplots",
    "text": "5.6 3.7 Visualizing data in 2D: scatterplots\nScatterplots are useful for visualizing treatment–response comparisons (as in Figure 3.3), associations between variables (as in Figure 3.10), or paired data (e.g., a disease biomarker in several patients before and after treatment). We use the two dimensions of our plotting paper, or screen, to represent the two variables. Let’s take a look at differential expression between a wildtype and an FGF4-KO sample.\nscp = ggplot(dfx, aes(x = `59 E4.5 (PE)` ,\n                      y = `92 E4.5 (FGF4-KO)`))\nscp + geom_point()__\n\nFigure 3.27: Scatterplot of 45101 expression measurements for two of the samples.\nThe labels 59 E4.5 (PE) and 92 E4.5 (FGF4-KO) refer to column names (sample names) in the dataframe dfx, which we created above. Since they contain special characters (spaces, parentheses, hyphen) and start with numerals, we need to enclose them with the downward sloping quotes to make them syntactically digestible for R. The plot is shown in Figure 3.27. We get a dense point cloud that we can try and interpret on the outskirts of the cloud, but we really have no idea visually how the data are distributed within the denser regions of the plot.\nOne easy way to ameliorate the overplotting is to adjust the transparency (alpha value) of the points by modifying the alpha parameter of geom_point (Figure 3.28).\nscp  + geom_point(alpha = 0.1)__\n\nFigure 3.28: As Figure 3.27, but with semi-transparent points to resolve some of the overplotting.\nThis is already better than Figure 3.27, but in the more dense regions even the semi-transparent points quickly overplot to a featureless black mass, while the more isolated, outlying points are getting faint. An alternative is a contour plot of the 2D density, which has the added benefit of not rendering all of the points on the plot, as in Figure 3.29.\nHowever, we see in Figure 3.29 that the point cloud at the bottom right (which contains a relatively small number of points) is no longer represented. We can somewhat overcome this by tweaking the bandwidth and binning parameters of geom_density2d (Figure 3.30, left panel).\nscp + geom_density2d()__\n\nFigure 3.29: As Figure 3.27, but rendered as a contour plot of the 2D density estimate.\nscp + geom_density2d(h = 0.5, bins = 60)\nlibrary(\"RColorBrewer\")\ncolorscale = scale_fill_gradientn(\n    colors = rev(brewer.pal(9, \"YlGnBu\")),\n    values = c(0, exp(seq(-5, 0, length.out = 100))))\n\nscp + stat_density2d(h = 0.5, bins = 60,\n          aes( fill = after_stat(level)), geom = \"polygon\") +\n  colorscale + coord_fixed()__\n[](03-chap_files/figure-html/fig-graphics-twodsp4-1.png “Figure 3.30 (a):”)\n\n\n\n[](03-chap_files/figure-html/fig-graphics-twodsp4-2.png “Figure 3.30 (b):”)\n\n\n\nFigure 3.30: 2D density plots. (a) As Figure 3.29, but with smaller smoothing bandwidth and tighter binning for the contour lines. (b) With color filling.\nWe can fill in each space between the contour lines with the relative density of points by explicitly calling the function stat_density2d (for which geom_density2d is a wrapper) and using the geometric object polygon , as in the right panel of Figure 3.30.\nWe used the function brewer.pal from the package RColorBrewer to define the color scale, and we added a call to coord_fixed to fix the aspect ratio of the plot, to make sure that the mapping of data range to \\(x\\)- and \\(y\\)-coordinates is the same for the two variables. Both of these issues merit a deeper look, and we’ll talk more about plot shapes in Section 3.7.1 and about colors in Section 3.9.\nThe density based plotting methods in Figure 3.30 are more visually appealing and interpretable than the overplotted point clouds of Figures 3.27 and 3.28, though we have to be careful in using them as we lose much of the information on the outlier points in the sparser regions of the plot. One possibility is using geom_point to add such points back in.\nBut arguably the best alternative, which avoids the limitations of smoothing, is hexagonal binning (Carr et al. 1987).\nscp + geom_hex() + coord_fixed()\nscp + geom_hex(binwidth = c(0.2, 0.2)) + colorscale +\n  coord_fixed()__\n[](03-chap_files/figure-html/fig-graphics-twodsp6-1.png “Figure 3.31 (a):”)\n\n\n\n[](03-chap_files/figure-html/fig-graphics-twodsp6-2.png “Figure 3.31 (b):”)\n\n\n\nFigure 3.31: Hexagonal binning. (a) Default parameters. (b) Finer bin sizes and customized color scale.\n\n5.6.1 3.7.1 Plot shapes\nChoosing the proper shape for your plot is important to make sure the information is conveyed well. By default, the shape parameter, that is, the ratio between the height of the graph and its width, is chosen by ggplot2 based on the available space in the current plotting device. The width and height of the device are specified when it is opened in R, either explicitly by you or through default parameters10. Moreover, the graph dimensions also depend on the presence or absence of additional decorations, like the color scale bars in Figure 3.31.\n10 See for example the manual pages of the pdf and png functions.\nThere are two simple rules that you can apply for scatterplots:\n\nIf the variables on the two axes are measured in the same units, then make sure that the same mapping of data space to physical space is used – i.e., use coord_fixed. In the scatterplots above, both axes are the logarithm to base 2 of expression level measurements; that is, a change by one unit has the same meaning on both axes (a doubling of the expression level). Another case is principal component analysis (PCA), where the \\(x\\)-axis typically represents component 1, and the \\(y\\)-axis component 2. Since the axes arise from an orthonormal rotation of input data space, we want to make sure their scales match. Since the variance of the data is (by definition) smaller along the second component than along the first component (or at most, equal), well-made PCA plots usually have a width that’s larger than the height.\nIf the variables on the two axes are measured in different units, then we can still relate them to each other by comparing their dimensions. The default in many plotting routines in R, including ggplot2 , is to look at the range of the data and map it to the available plotting region. However, in particular when the data more or less follow a line, looking at the typical slope of the line can be useful. This is called banking (William S. Cleveland, McGill, and McGill 1988).\n\nTo illustrate banking, let’s use the classic sunspot data from Cleveland’s paper.\nlibrary(\"ggthemes\")\nsunsp = tibble(year   = time(sunspot.year),\n               number = as.numeric(sunspot.year))\nsp = ggplot(sunsp, aes(x = year, y = number)) + geom_line()\nsp\nratio = with(sunsp, bank_slopes(year, number))\nsp + coord_fixed(ratio = ratio)__\n[](03-chap_files/figure-html/fig-graphics-banking-1.png “Figure 3.32 (a):”)\n\n\n\n[](03-chap_files/figure-html/fig-graphics-banking-2.png “Figure 3.32 (b):”)\n\n\n\nFigure 3.32: The sunspot data. In (a), the plot shape is roughly quadratic, a frequent default choice. In (b), a technique called banking was used to choose the plot shape. (Note: the placement of the tick labels is not great in this plot and would benefit from customization.)\nThe resulting plot is shown in the upper panel of Figure 3.32. We can clearly see long-term fluctuations in the amplitude of sunspot activity cycles, with particularly low maximum activities in the early 1700s, early 1800s, and around the turn of the 20\\(^\\) century. But now lets try out banking.\nHow does the algorithm work? It aims to make the slopes in the curve be around one. In particular, bank_slopes computes the median absolute slope, and then with the call to coord_fixed we set the aspect ratio of the plot such that this quantity becomes 1. The result is shown in the lower panel of Figure 3.32. Quite counter-intuitively, even though the plot takes much smaller space, we see more on it! In particular, we can see the saw-tooth shape of the sunspot cycles, with sharp rises and more slow declines.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "03-chap.html#visualizing-more-than-two-dimensions",
    "href": "03-chap.html#visualizing-more-than-two-dimensions",
    "title": "5  3.1 Goals for this chapter",
    "section": "5.7 3.8 Visualizing more than two dimensions",
    "text": "5.7 3.8 Visualizing more than two dimensions\nSometimes we want to show the relationships between more than two variables. Obvious choices for including additional dimensions are the plot symbol shapes and colors. The geom_point geometric object offers the following aesthetics (beyond x and y):\n\nfill\ncolor\nshape\nsize\nalpha\n\nThey are explored in the manual page of the geom_point function. fill and color refer to the fill and outline color of an object, and alpha to its transparency level. Above, in Figures 3.28 and following, we have used color or transparency to reflect point density and avoid the obscuring effects of overplotting. We can also use these properties to show other dimensions of the data. In principle, we could use all five aesthetics listed above simultaneously to show up to seven-dimensional data; however, such a plot would be hard to decipher. Usually we are better off to limit ourselves to choosing only one or two of these aesthetics and varying them to show one or two additional dimensions in the data.\n\n5.7.1 3.8.1 Faceting\n\n\n\nSometimes this is also called trellis or lattice graphics, in an allusion to how these arrays of plots look like. The first major R package to implement faceting was lattice. In this book, we’ll use the faceting functionalities provided through ggplot2.\n\n\nSometimes this is also called trellis or lattice graphics, in an allusion to how these arrays of plots look like. The first major R package to implement faceting was lattice. In this book, we’ll use the faceting functionalities provided through ggplot2.\nAnother way to show additional dimensions of the data is to show multiple plots that result from repeatedly subsetting (or “slicing”) our data based on one (or more) of the variables, so that we can visualize each part separately. This is called faceting and it enables us to visualize data in up to four or five dimensions. So we can, for instance, investigate whether the observed patterns among the other variables are the same or different across the range of the faceting variable. Let’s look at an example:\nlibrary(\"magrittr\")\ndftx$lineage %&lt;&gt;% sub(\"^$\", \"no\", .)\ndftx$lineage %&lt;&gt;% factor(levels = c(\"no\", \"EPI\", \"PE\", \"FGF4-KO\"))\n\nggplot(dftx, aes(x = X1426642_at, y = X1418765_at)) +\n  geom_point() + facet_grid( . ~ lineage )__\n\nFigure 3.33: An example of faceting : the same data as in Figure 3.9, but now split by the categorical variable lineage.\nThe result is shown in Figure 3.33. We have used R’s formula language to specify the variable by which we want to split the data, and that the separate panels should be in different columns: facet_grid( . ~ lineage ). In fact, we can specify two faceting variables, as follows; the result is shown in Figure 3.34.\nggplot(dftx,\n  aes(x = X1426642_at, y = X1418765_at)) + geom_point() +\n   facet_grid( Embryonic.day ~ lineage )__\n\nFigure 3.34: Faceting : the same data as in Figure 3.9, split by the categorical variables Embryonic.day (rows) and lineage (columns).\nAnother useful function is facet_wrap: if the faceting variable has too many levels for all the plots to fit in one row or one column, then this function can be used to wrap them into a specified number of columns or rows. So far we have seen faceting by categorical variables, but we can also use it with continuous variables by discretizing them into levels. The function cut is useful for this purpose.\nggplot(mutate(dftx, Tdgf1 = cut(X1450989_at, breaks = 4)),\n   aes(x = X1426642_at, y = X1418765_at)) + geom_point() +\n   facet_wrap( ~ Tdgf1, ncol = 2 )__\n\nFigure 3.35: Faceting : the same data as in Figure 3.9, split by the continuous variable X1450989_at and arranged by facet_wrap.\nWe see in Figure 3.35 that the number of points in the four panels is different. This is because cut splits into bins of equal length, not equal number of points. If we want the latter, then we can use quantile in conjunction with cut, or cut on the ranks of the variable’s values.\n\n5.7.1.1 Axes scales\nIn Figures 3.33—3.35, the axes scales are the same for all panels. Alternatively, we could let them vary by setting the scales argument of the facet_grid and facet_wrap functions. This argument lets you control whether \\(x\\)-axis and \\(y\\)-axis in each panel have the same scale, or whether either of them or both adapt to each panel’s data range. There is a trade-off: adaptive axes scales might let us see more detail, on the other hand, the panels are then less comparable across the groupings.\n\n\n5.7.1.2 Implicit faceting\nYou can also facet your plots (without explicit calls to facet_grid and facet_wrap) by specifying the aesthetics. A very simple version of implicit faceting is using a factor as your \\(x\\)-axis, such as in Figures 3.15—3.18.\n\n\n\n5.7.2 3.8.2 Interactive graphics\nThe plots generated thus far have been static images. You can add an enormous amount of information and expressivity by making your plots interactive. We do not try here to to convey interactive visualizations in any depth, but we provide pointers to a few important resources. This is a dynamic space, so readers should explore the R ecosystem for recent developments.\n\n5.7.2.1 shiny\nRstudio’s shiny is a web application framework for R. It makes it easy to create interactive displays with sliders, selectors and other control elements that allow changing all aspects of the plot(s) shown – since the interactive elements call back directly into the R code that produces the plot(s). See the shiny gallery for some great examples.\nAs a graphics engine for shiny -based interactive visualizations you can use ggplot2 , and indeed, base R graphics or any other graphics package. What may be a little awkward here is that the language used for describing the interactive options is separated from the production of the graphics via ggplot2 and the grammar of graphics. The ggvis package aims to overcome this limitation:\n\n\n5.7.2.2 ggvis\nggvis 11 is an attempt at extending the good features of ggplot2 into the realm of interactive graphics. In contrast to ggplot2 , which produces graphics into R’s traditional graphics devices (PDF, PNG, etc.), ggvis builds upon a JavaScript infrastructure called Vega, and its plots are intended to be viewed in an HTML browser. Like ggplot2 , the ggvis package is inspired by grammar of graphics concepts, but uses distinct syntax. It leverages shiny ’s infrastructure to connect to R to perform the computations needed for the interactivity. As its author put it12, “The goal is to combine the best of R (e.g., every modeling function you can imagine) and the best of the web (everyone has a web browser). Data manipulation and transformation are done in R, and the graphics are rendered in a web browser, using Vega.”\n11 At the time of writing (summer 2017), it is not clear whether the initial momentum of ggvis development will be maintained, and its current functionality and maturity do no yet match ggplot2.\n12 https://ggvis.rstudio.com\nAs a consequence of the interactivity in shiny and ggvis , there needs to be an R interpreter running with the underlying data and code to respond to the user’s actions while she views the graphic. This R interpreter can be on the local machine or on a server; in both cases, the viewing application is a web browser, and the interaction with R goes through web protocols (http or https). That is, of course, different from a graphic stored in a self- contained file, which is produced once by R and can then be viewed in a PDF or HTML viewer without any connection to a running instance of R.\n\n\n5.7.2.3 plotly\nA great web-based tool for interactive graphic generation is plotly. You can view some examples of interactive graphics online at https://plot.ly/r. To create your own interactive plots in R, you can use code such as\nlibrary(\"plotly\")\nplot_ly(economics, x = ~ date, y = ~ unemploy / pop)__\nAs with shiny and ggvis , the graphics are viewed in an HTML browser; however, no running R session is required. The graphics are self-contained HTML documents whose “logic” is coded in JavaScript, or more precisely, in the D3.js system.\n\nFigure 3.36: rgl rendering of the volcano data, the topographic information for Maunga Whau (Mt Eden), one of about 50 volcanos in the Auckland volcanic field.\n\n\n5.7.2.4 rgl, webgl\nFor visualizing 3D objects (say, a geometrical structure), there is the package rgl. It produces interactive viewer windows (either in specialized graphics device on your screen or through a web browser) in which you can rotate the scene, zoom and in out, etc. A screenshot of the scene produced by the code below is shown in Figure 3.36; such screenshots can be produced using the function snapshot3d.\ndata(\"volcano\")\nvolcanoData = list(\n  x = 10 * seq_len(nrow(volcano)),\n  y = 10 * seq_len(ncol(volcano)),\n  z = volcano,\n  col = terrain.colors(500)[cut(volcano, breaks = 500)]\n)\nlibrary(\"rgl\")\nwith(volcanoData, persp3d(x, y, z, color = col))__\nIn the code above, the base R function cut computes a mapping from the value range of the volcano data to the integers between 1 and 50013, which we use to index the color scale, terrain.colors(500). For more information, consult the package’s excellent vignette.\n13 More precisely, it returns a factor with as many levels, which we let R autoconvert to integers.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "03-chap.html#color",
    "href": "03-chap.html#color",
    "title": "5  3.1 Goals for this chapter",
    "section": "5.8 3.9 Color",
    "text": "5.8 3.9 Color\nAn important consideration when making plots is the coloring that we use in them. Most R users are likely familiar with the built-in R color scheme, used by base R graphics, as shown in Figure 3.37.\npie(rep(1, 8), col=1:8)__\n\nFigure 3.37: The first eight colors in the base R color palette.\nThe origins of this color palette date back to 1980s hardware, where graphics cards handled colors by letting each pixel either use or not use each of the three basic color channels of a cathode-ray tube: red, green and blue (RGB). This led to \\(2^3=8\\) combinations at the eight corners of an RGB color cube14.\n14 Thus, the \\(8^\\) color in Figure 3.37 should be white, instead it is grey.\nThe first eight colors for a categorical variable in ggplot2 are shown in Figure 3.38:\nggplot(tibble(u = factor(1:8), v = 1), \n       aes(x = \"\",  y = v, fill = u)) +\n  geom_bar(stat = \"identity\", width = 1) + \n  coord_polar(\"y\", start = 0) + theme_void()__\n\nFigure 3.38: The first eight colors in the ggplot2 color palette.\nThese defaults are appropriate for simple use cases, but often we will want to make our own choices. In Section 3.7 we already saw the function scale_fill_gradientn, with which we created the smooth-looking color gradient used in Figures 3.30 and 3.31 by interpolating a set of color steps provided by the function brewer.pal in the RColorBrewer package. This package defines a set of purpose-designed color palettes. We can see all of them at a glance with the function display.brewer.all (Figure 3.39).\ndisplay.brewer.all()__\n\nFigure 3.39: RColorBrewer palettes.\nWe can get information about the available color palettes from brewer.pal.info.\nhead(brewer.pal.info)__\n\n\n     maxcolors category colorblind\nBrBG        11      div       TRUE\nPiYG        11      div       TRUE\nPRGn        11      div       TRUE\nPuOr        11      div       TRUE\nRdBu        11      div       TRUE\nRdGy        11      div      FALSE\n\n\ntable(brewer.pal.info$category)__\n\n\n div qual  seq \n   9    8   18 \nThe palettes are divided into three categories:\n\nqualitative: for categorical properties that have no intrinsic ordering. The Paired palette supports up to 6 categories that each fall into two subcategories - such as before and after , with and without treatment, etc.\nsequential: for quantitative properties that go from low to high\ndiverging: for quantitative properties for which there is a natural midpoint or neutral value, and whose value can deviate both up- and down; we’ll see an example in Figure 3.41.\n\nTo obtain the colors from a particular palette we use the function brewer.pal. Its first argument is the number of colors we want (which can be less than the available maximum number in brewer.pal.info).\nbrewer.pal(4, \"RdYlGn\")__\n\n\n[1] \"#D7191C\" \"#FDAE61\" \"#A6D96A\" \"#1A9641\"\nIf we want more than the available number of preset colors (for example so we can plot a heatmap with continuous colors) we can interpolate using the colorRampPalette function15.\n15 colorRampPalette returns a function of one parameter, an integer. In the code shown, we call that function with the argument 100.\nmypalette  = colorRampPalette(\n    c(\"darkorange3\", \"white\",\"darkblue\")\n  )(100)\nhead(mypalette)__\n\n\n[1] \"#CD6600\" \"#CE6905\" \"#CF6C0A\" \"#D06F0F\" \"#D17214\" \"#D27519\"\n\n\nimage(matrix(1:100, nrow = 100, ncol = 10), col = mypalette,\n        xaxt = \"n\", yaxt = \"n\", useRaster = TRUE)__\n\nFigure 3.40: A quasi-continuous color palette derived by interpolating between the colors darkorange3, white and darkblue.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "03-chap.html#heatmaps",
    "href": "03-chap.html#heatmaps",
    "title": "5  3.1 Goals for this chapter",
    "section": "5.9 3.10 Heatmaps",
    "text": "5.9 3.10 Heatmaps\nHeatmaps are a powerful way of visualizing large, matrix-like datasets and providing a quick overview of the patterns that might be in the data. There are a number of heatmap drawing functions in R; one that is convenient and produces a good-looking output is the function pheatmap from the eponymous package16. In the code below, we first select the top 500 most variable genes in the dataset x and define a function rowCenter that centers each gene (row) by subtracting the mean across columns. By default, pheatmap uses the RdYlBu color palette from RcolorBrewer in conjuction with the colorRampPalette function to interpolate the 11 colors into a smooth-looking palette (Figure 3.41).\n16 A very versatile and modular alternative is the ComplexHeatmap package.\nlibrary(\"pheatmap\")\ntopGenes = order(rowVars(Biobase::exprs(x)), decreasing = TRUE)[1:500]\nrowCenter = function(x) { x - rowMeans(x) }\npheatmap(rowCenter(dfx[topGenes, ]), \n  show_rownames = FALSE, \n  show_colnames = FALSE, \n  breaks = seq(-5, +5, length = 101),\n  annotation_col = pData(x)[, c(\"sampleGroup\", \"Embryonic.day\", \"ScanDate\", \"genotype\") ],\n  annotation_colors = list(\n    sampleGroup = groupColor,\n    genotype = c(`FGF4-KO` = \"chocolate1\", `WT` = \"azure2\"),\n    Embryonic.day = setNames(brewer.pal(9, \"Blues\")[c(3, 6, 9)], c(\"E3.25\", \"E3.5\", \"E4.5\")),\n    ScanDate = setNames(brewer.pal(nlevels(x$ScanDate), \"YlGn\"), levels(x$ScanDate))\n  )\n)__\n\nFigure 3.41: A heatmap of relative expression values, i.e., logarithmic fold change compared to the average expression of that gene (row) across all samples (columns). The color scale uses a diverging palette whose midpoint is at 0.\nLet’s take a minute to deconstruct this rather massive-looking call to pheatmap. The options show_rownames and show_colnames control whether the row and column names are printed at the sides of the matrix. Because our matrix is large in relation to the available plotting space, the labels would not be readable, so we might as well suppress them. The annotation_col argument takes a data frame that carries additional information about the samples. The information is shown in the colored bars on top of the heatmap. There is also a similar annotation_row argument, which we haven’t used here, to annotate the rows (genes) with colored bars at the side. The argument annotation_colors is a list of named vectors by which we can override the default choice of colors for the annotation bars. The pheatmap function has many further options, and if you want to use it for your own data visualizations, it’s worth studying them.\n\n5.9.1 3.10.1 Dendrogram ordering\nIn Figure 3.41, the trees at the left and the top represent the result of a hierarchical clustering algorithm and are also called dendrograms. The ordering of the rows and columns is based on the dendrograms. It has an enormous effect on the visual impact of the heatmap. However, it can be difficult to decide which of the apparent patterns are real and which are consequences of arbitrary tree layout decisions17. Let’s keep in mind that:\n17 We will learn about clustering and methods for evaluating cluster significance in Chapter 5.\n\nOrdering the rows and columns by cluster dendrogram (as in Figure 3.41) is an arbitrary choice, and you could just as well make others.\nEven if you settle on dendrogram ordering, there is an essentially arbitrary choice at each internal branch, as each branch could be flipped without changing the topology of the tree (see also Figure 5.21).\n\n__\nQuestion 3.6\nHow does the pheatmap function deal with the decision of how to pick which branches of the subtree go left and right?\n__\nSolution\n__\nThis is described in the manual page of the hclust function in the stats package, which, by default, is used by pheatmap.\n__\nQuestion 3.7\nWhat other ordering methods can you think of?\n__\nSolution\n__\nAmong the methods proposed is the travelling salesman problem (McCormick Jr, Schweitzer, and White 1972) or projection on the first principal component (for instance, see the examples in the manual page of pheatmap).\n__\nQuestion 3.8\nCheck the argument clustering_callback of the pheatmap function.\n\n\n5.9.2 3.10.2 Color spaces\nColor perception in humans (Helmholtz 1867) is three-dimensional18. There are different ways of parameterizing this space. Above we already encountered the RGB color model, which uses three values in [0,1], for instance at the beginning of Section 3.4, where we printed out the contents of groupColor:\n18 Physically, there is an infinite number of wave-lengths of light and an infinite number of ways of mixing them, so for other species, or robots, color space can have fewer or more dimensions.\ngroupColor[1]__\n\n\n    E3.25 \n\"#CAB2D6\" \nHere, CA is the hexadecimal representation for the strength of the red color channel, B2 of the green and D6 of the blue color channel. In decimal, these numbers are 202, 178 and 214. The range of these values goes from to 0 to 255, so by dividing by this maximum value, an RGB triplet can also be thought of as a point in the three-dimensional unit cube.\n\n\n\n\n\n\n\n\nFigure 3.42: Circles in HCL colorspace. (a) Luminance \\(L\\) fixed at \\(75\\), while the angular coordinate \\(H\\) (hue) varies from 0 to 360 and the radial coordinate \\(C\\) takes the values \\(0, 10, …, 60\\). (b) Constant chroma \\(C\\) at \\(50\\), \\(H\\) as above, and the radial coordinate is luminance \\(L\\) at values \\(10, 20, …, 90\\).\nThe function hcl uses a different coordinate system. Again this consists of three coordinates: hue \\(H\\), an angle in the range \\([0, 360]\\), chroma \\(C\\), a positive number, and luminance \\(L\\), a value in \\([0, 100]\\). The upper bound for \\(C\\) depends on on hue and luminance.\nThe hcl function corresponds to polar coordinates in the CIE-LUV19 and is designed for area fills. By keeping chroma and luminance coordinates constant and only varying hue, it is easy to produce color palettes that are harmonious and avoid irradiation illusions that make light colored areas look bigger than dark ones. Our attention also tends to get drawn to loud colors, and fixing the value of chroma makes the colors equally attractive to our eyes.\n19 CIE: Commission Internationale de l’Éclairage – see e.g. Wikipedia for more on this.\nThere are many ways of choosing colors from a color wheel. Triads are three colors chosen equally spaced around the color wheel; for example, \\(H=0,120,240\\) gives red, green, and blue. Tetrads are four equally spaced colors around the color wheel, and some graphic artists describe the effect as “dynamic”. Warm colors are a set of equally spaced colors close to yellow, cool colors a set of equally spaced colors close to blue. Analogous color sets contain colors from a small segment of the color wheel, for example, yellow, orange and red, or green, cyan and blue. Complementary colors are colors diametrically opposite each other on the color wheel. A tetrad is two pairs of complementaries. Split complementaries are three colors consisting of a pair of complementaries, with one partner split equally to each side, for example, \\(H=60,,240-30,,240+30\\). This is useful to emphasize the difference between a pair of similar categories and a third different one. A more thorough discussion is provided in the references (Mollon 1995; Ihaka 2003).\n\n5.9.2.1 Lines versus areas\nFor lines and points, we want a strong contrast to the background, so on a white background, we want them to be relatively dark (low luminance \\(L\\)). For area fills, lighter, more pastel-type colors with low to moderate chromatic content are usually more pleasant.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "03-chap.html#data-transformations",
    "href": "03-chap.html#data-transformations",
    "title": "5  3.1 Goals for this chapter",
    "section": "5.10 3.11 Data transformations",
    "text": "5.10 3.11 Data transformations\nPlots in which most points are huddled up in one area, with much of the available space sparsely populated, are difficult to read. If the histogram of the marginal distribution of a variable has a sharp peak and then long tails to one or both sides, transforming the data can be helpful. These considerations apply both to x and y aesthetics, and to color scales. In the plots of this chapter that involved the microarray data, we used the logarithmic transformation20 – not only in scatterplots like Figure 3.27 for the \\(x\\) and \\(y\\)-coordinates, but also in Figure 3.41 for the color scale that represents the expression fold changes. The logarithm transformation is attractive because it has a definitive meaning - a move up or down by the same amount on a log-transformed scale corresponds to the same multiplicative change on the original scale: \\((ax)=a+x\\).\n20 We used it implicitly since the data in the ExpressionSet object x already come log-transformed.\nSometimes the logarithm however is not good enough, for instance when the data include zero or negative values, or when even on the logarithmic scale the data distribution is highly uneven. From the upper panel of Figure 3.43, it is easy to take away the impression that the distribution of M depends on A, with higher variances for lower A. However, this is entirely a visual artefact, as the lower panel confirms: the distribution of M is independent of A, and the apparent trend we saw in the upper panel was caused by the higher point density at smaller A.\ngg = ggplot(tibble(A = Biobase::exprs(x)[, 1], M = rnorm(length(A))),\n            aes(y = M))\ngg + geom_point(aes(x = A), size = 0.2)\ngg + geom_point(aes(x = rank(A)), size = 0.2)__\n\n\n\n\n\n\n\n\nFigure 3.43: The effect of rank transformation on the visual perception of dependency.\n__\nQuestion 3.9\nCan the visual artefact be avoided by using a density- or binning-based plotting method, as in Figure 3.31?\n__\nQuestion 3.10\nCan the rank transformation also be applied when choosing color scales e.g. for heatmaps? What does histogram equalization in image processing do?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "03-chap.html#mathematical-symbols-and-other-fonts",
    "href": "03-chap.html#mathematical-symbols-and-other-fonts",
    "title": "5  3.1 Goals for this chapter",
    "section": "5.11 3.12 Mathematical symbols and other fonts",
    "text": "5.11 3.12 Mathematical symbols and other fonts\nWe can use mathematical notation in plot labels, using a notation that is a mix of R syntax and LaTeX-like notation (see help(\"plotmath\") for details):\n\nFigure 3.44: As Figure 3.24, with font “Bauhaus 93”.\nvolume = function(rho, nu)\n            pi^(nu/2) * rho^nu / gamma(nu/2+1)\n\nggplot(tibble(nu    = 1:15,\n  Omega = volume(1, nu)), aes(x = nu, y = Omega)) +\ngeom_line() +\nxlab(expression(nu)) + ylab(expression(Omega)) +\ngeom_text(label =\n\"Omega(rho,nu)==frac(pi^frac(nu,2)~rho^nu, Gamma(frac(nu,2)+1))\",\n  parse = TRUE, x = 6, y = 1.5)__\n\nFigure 3.45: Volume \\(\\) of the \\(\\)-dimensional sphere with radius \\(\\), for \\(,…,15\\).\nThe result is shown in Figure 3.45. It’s also easy to switch to other fonts, for instance the serif font Times (Figure 3.46).\nggplot(genes, aes( x = value, color = gene)) + stat_ecdf() +\n  theme(text = element_text(family = \"Times\"))__\n\nFigure 3.46: As Figure 3.24, with a different font.\nIn fact, the set of fonts that can be used with a standard R installation are limited, but luckily there is the package extrafont , which facilitates using fonts other than R’s standard set of PostScript fonts. There’s some extra work needed before we can use it, since fonts external to R first need to be made known to it. They could come shipped with your operating system, with a word processor or with another graphics application. The set of fonts available and their physical location is therefore not standardized, but will depend on your operating system and further configurations. In the first session after attaching the extrafont package, you will need to run the function font_import to import fonts and make them known to the package. Then in each session in which you want to use them, you need to call the loadfonts function to register them with one or more of R’s graphics devices. Finally you can use the fonttable function to list the available fonts. You will need to refer to the documentation of the extrafonts package to see how to make this work on your machine.\n__\nTask\nUse the package extrafont to produce a version of Figure 3.46 with the font “Bauhaus 93” (or another one available on your system).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "03-chap.html#genomic-data",
    "href": "03-chap.html#genomic-data",
    "title": "5  3.1 Goals for this chapter",
    "section": "5.12 3.13 Genomic data",
    "text": "5.12 3.13 Genomic data\n\nFigure 3.47: Screenshot from Ensembl genome browser, showing gene annotation of a genomic region as well as a read pile-up visualization of an RNA-Seq experiment.\nTo visualize genomic data, in addition to the general principles we have discussed in this chapter, there are some specific considerations. The data are usually associated with genomic coordinates. In fact genomic coordinates offer a great organizing principle for the integration of genomic data. You will probably have seen genome browser displays such as in Figure 3.47. Here we will briefly show how to produce such plots programmatically, using your data as well as public annotation. It will be a short glimpse, and we refer to resources such as Bioconductor for a fuller picture.\nThe main challenge of genomic data visualization is the size of genomes. We need visualizations at multiple scales, from whole genome down to the nucleotide level. It should be easy to zoom in and and out, and we may need different visualization strategies for the different size scales. It can be convenient to visualize biological molecules (genomes, genes, transcripts, proteins) in a linear manner, although their embedding in the 3D physical world can matter (a great deal).\nLet’s start with some fun examples, an ideogram plot of human chromosome 1 (Figure 3.48) and a plot of the genome-wide distribution of RNA editing sites (Figure 3.49).\nlibrary(\"ggbio\")\ndata(\"hg19IdeogramCyto\", package = \"biovizBase\")\nplotIdeogram(hg19IdeogramCyto, subchr = \"chr1\")__\n\nFigure 3.48: Chromosome 1 of the human genome: ideogram plot.\nThe darned_hg19_subset500 lists a selection of 500 RNA editing sites in the human genome. It was obtained from the Database of RNA editing in Flies, Mice and Humans (DARNED, https://darned.ucc.ie/). The result is shown in Figure 3.49.\nlibrary(\"GenomicRanges\")\ndata(\"darned_hg19_subset500\", package = \"biovizBase\")\nautoplot(darned_hg19_subset500, layout = \"karyogram\",\n         aes(color = exReg, fill = exReg))__\n\nFigure 3.49: Karyogram with RNA editing sites. exReg indicates whether a site is in the coding region (C), 3’- or 5’-UTR.\n__\nQuestion 3.11\nHow do you fix the ordering of the chromosomes in Figure 3.49 and get rid of the warning about chromosome lengths?\n__\nSolution\n__\nThe information on chromosome lengths in the hg19 assembly of the human genome is (for instance) stored in the ideoCyto dataset. We use the function keepSeqlevels to reorder the chromosomes. See Figure 3.50.\ndata(\"ideoCyto\", package = \"biovizBase\")\ndn = darned_hg19_subset500\nseqlengths(dn) = seqlengths(ideoCyto$hg19)[names(seqlengths(dn))]\ndn = keepSeqlevels(dn, paste0(\"chr\", c(1:22, \"X\")))\nautoplot(dn, layout = \"karyogram\", aes(color = exReg, fill = exReg))__\n\nFigure 3.50: Improved version of Figure 3.49.\n__\nQuestion 3.12\nWhat type of object is darned_hg19_subset500?\n__\nSolution\n__\ndarned_hg19_subset500[1:2,]__\n\n\nGRanges object with 2 ranges and 10 metadata columns:\n      seqnames    ranges strand |       inchr       inrna         snp\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt; &lt;character&gt; &lt;character&gt;\n  [1]     chr5  86618225      - |           A           I        &lt;NA&gt;\n  [2]     chr7  99792382      - |           A           I        &lt;NA&gt;\n             gene      seqReg       exReg      source      ests      esta\n      &lt;character&gt; &lt;character&gt; &lt;character&gt; &lt;character&gt; &lt;integer&gt; &lt;integer&gt;\n  [1]        &lt;NA&gt;           O        &lt;NA&gt;    amygdala         0         0\n  [2]        &lt;NA&gt;           O        &lt;NA&gt;        &lt;NA&gt;         0         0\n           author\n      &lt;character&gt;\n  [1]    15342557\n  [2]    15342557\n  -------\n  seqinfo: 23 sequences from an unspecified genome; no seqlengths\nIt is a GRanges object, that is, a specialized class from the Bioconductor project for storing data that are associated with genomic coordinates. The first three columns are obligatory: seqnames, the name of the containing biopolymer (in our case, these are names of human chromosomes), ranges, the genomic coordinates of the intervals (in this case, the intervals all have length 1, as they each refer to a single nucleotide), and the DNA strand from which the RNA is transcribed. You can find out more on how to use this class and its associated infrastructure in the documentation, e.g., the vignette of the GenomicRanges package. Learning it is worth the effort if you want to work with genome-associated datasets, as it allows for convenient, efficient and safe manipulation of these data and provides many powerful utilities.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "03-chap.html#summary-of-this-chapter",
    "href": "03-chap.html#summary-of-this-chapter",
    "title": "5  3.1 Goals for this chapter",
    "section": "5.13 3.14 Summary of this chapter",
    "text": "5.13 3.14 Summary of this chapter\nVisualizing data, either “raw” or along the various steps of processing, summarization and inference, is one of the most important activities in applied statistics, and indeed, in science. It sometimes gets short shrift in textbooks since there is not much deductive theory. However, there is a large number of good (and bad) practices, and once you pay attention to it, you will quickly see whether a certain graphic is effective in conveying its message, or what choices you could make to create powerful and aesthetically attrative data visualizations. Among the important options are the plot type (what is called a geom in ggplot2), proportions (incl. aspect ratios) and colors. The grammar of graphics is a powerful set of concepts to reason about graphics and to communicate our intentions for a data visualization to a computer.\nAvoid laziness —just using the software’s defaults without thinking about the options—, and avoid getting carried away —adding lots of visual candy that just clutters up the plot but has no real message. Creating your own visualizations is in many ways like good writing. It is extremely important to get your message across (in talks, in papers), but there is no simple recipe for it. Look carefully at lots of visualizations made by others, experiment with making your own visualizations to learn the ropes, and then decide what is your style.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "03-chap.html#further-reading",
    "href": "03-chap.html#further-reading",
    "title": "5  3.1 Goals for this chapter",
    "section": "5.14 3.15 Further reading",
    "text": "5.14 3.15 Further reading\nThe most useful resources about ggplot2 are the second edition of Wickham (2016) and the ggplot2 website. There are many ggplot2 code snippets online, which you will find through search engines after some practice. But be critical and check your sources: sometimes, code examples you find online refer to outdated versions of the package, and sometimes they are of poor quality.\nThe foundation of the system is based on Wilkinson (2005) and the ideas by Tukey (1977; William S. Cleveland 1988).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "03-chap.html#exercises",
    "href": "03-chap.html#exercises",
    "title": "5  3.1 Goals for this chapter",
    "section": "5.15 3.16 Exercises",
    "text": "5.15 3.16 Exercises\n__\nExercise 3.1\nUse themes to change the visual appearance of your plots. Start by running the following example:\nggcars = ggplot(mtcars, aes(x = hp, y = mpg)) + geom_point()\nggcars\nggcars + theme_bw()\nggcars + theme_minimal()__\nWhat other themes are there? (Hint: have a look at the section for themes in the online documentation of ggplot2.) Are these all the themes? (Hint: have a look at the ggthemes package.) Make a version of the above plot in the style of the Economist magazine. Add a smoothing line\n__\nExercise 3.2\nWhat are admissible color names in base R? Have a look at the manual page of the colors function. Ryun the examples and the demo.\nUse an internet search engine to search for “R color names” and explore some of the resources that come up, e.g., cheat sheets with all the colors diplayed.\nProduce a version of the heatmap in Figure 3.41 that uses a color palette from the R package beyonce.\n__\nExercise 3.3\nCreate plots in the style of the xkcd webcomic. Have a look at\n\na thread on Stackoverflow titled How can we make xkcd style graphs?\nthe R source code of this book for the code that produces the chapter’s opening figure,\nthe vignette of the xkcd package.\n\n\n__\nExercise 3.4\nCheck out the shiny tutorials on the RStudio website. Write a shiny app that displays one of the plots from this chapter, but with interactive elements to control, e.g., which genes are displayed (Figures 3.33—3.35).\n__\nExercise 3.5\nWhat options are there for serializing a graphic, i.e., for storing a graphic in a file that you can save for later use, or load up in another software? How can you serialize interactive graphics?\n__\nExercise 3.6\nImportant and well done graphics do not have to be complex. For instance, check out Figure 9 (page 82) of the public assessment report by the European Medicines Agency on Comirnaty21 and XKCD 2400.\n21 You can also get this from https://www.ema.europa.eu/en/medicines/human/EPAR/comirnaty.\nCarr, Daniel B, Richard J Littlefield, WL Nicholson, and JS Littlefield. 1987. “Scatterplot Matrix Techniques for Large N.” Journal of the American Statistical Association 82 (398): 424–36.\nCleveland, William S. 1988. The Collected Works of John w. Tukey: Graphics 1965-1985. Vol. 5. CRC Press.\nCleveland, William S., Marylyn E. McGill, and Robert McGill. 1988. “The Shape Parameter of a Two-Variable Graph.” Journal of the American Statistical Association 83: 289–300.\nHelmholtz, H. von. 1867. Handbuch Der Physiologischen Optik. Leipzig: Leopold Voss.\nIhaka, Ross. 2003. “Color for Presentation Graphics.” In Proceedings of the 3rd International Workshop on Distributed Statistical Computing , edited by Kurt Hornik and Friedrich Leisch. Vienna, Austria: http://www.r-project.org/conferences/DSC-2003/Proceedings/; ISSN 1609-395X.\nIrizarry, R. A., B. Hobbs, F. Collin, Y. D. Beazer-Barclay, K. J. Antonellis, U. Scherf, and T. P. Speed. 2003. “Exploration, Normalization, and Summaries of High Density Oligonucleotide Array Probe Level Data.” Biostatistics 4 (2): 249–64.\nLawrence, Michael S., Petar Stojanov, Paz Polak, Gregory V. Kryukov, Kristian Cibulskis, Andrey Sivachenko, Scott L. Carter, et al. 2013. “Mutational Heterogeneity in Cancer and the Search for New Cancer-Associated Genes.” Nature 499 (7457): 214–18. https://doi.org/10.1038/nature12213.\nMcCormick Jr, William T, Paul J Schweitzer, and Thomas W White. 1972. “Problem Decomposition and Data Reorganization by a Clustering Technique.” Operations Research 20 (5): 993–1009.\nMollon, John. 1995. “Seeing Colour.” In Colour: Art and Science , edited by T. Lamb and J. Bourriau. Cambridge Unversity Press.\nOhnishi, Y., W. Huber, A. Tsumura, M. Kang, P. Xenopoulos, K. Kurimoto, A. K. Oles, et al. 2014. “Cell-to-Cell Expression Variability Followed by Signal Reinforcement Progressively Segregates Early Mouse Lineages.” Nature Cell Biology 16 (1): 27–37.\nTukey, John W. 1977. “Exploratory Data Analysis.” Massachusetts: Addison- Wesley.\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28.\n———. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer New York. http://had.co.nz/ggplot2/book.\nWilkinson, Leland. 1999. “Dot Plots.” The American Statistician 53 (3): 276.\n———. 2005. The Grammar of Graphics. Springer.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "04-chap.html",
    "href": "04-chap.html",
    "title": "6  4.1 Goals for this chapter",
    "section": "",
    "text": "6.1 4.2 Finite mixtures\nOne of the main challenges of biological data analysis is dealing with heterogeneity. The quantities we are interested in often do not show a simple, unimodal “textbook distribution”. For example, in the last part of Chapter 2 we saw how the histogram of sequence scores in Figure 2.27 had two separate modes, one for CpG- islands and one for non-islands. We can see the data as a simple mixture of a few (in this case: two) components. We call these finite mixtures. Other mixtures can involve almost as many components as we have observations. These we call infinite mixtures 1.\n1 We will see that—as for so many modeling choices–the right complexity of the mixture is in the eye of the beholder and often depends on the amount of data and the resolution and smoothness we want to attain.\nIn Chapter 1 we saw how a simple generative model with a Poisson distribution led us to make useful inference in the detection of an epitope. Unfortunately, a satisfactory fit to real data with such a simple model is often out of reach. However, simple models such as the normal or Poisson distributions can serve as building blocks for more realistic models using the mixing framework that we cover in this chapter. Mixtures occur naturally for flow cytometry data, biometric measurements, RNA-Seq, ChIP-Seq, microbiome and many other types of data collected using modern biotechnologies. In this chapter we will learn from simple examples how to build more realistic models of distributions using mixtures.\nIn this chapter we will:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "04-chap.html#finite-mixtures",
    "href": "04-chap.html#finite-mixtures",
    "title": "6  4.1 Goals for this chapter",
    "section": "",
    "text": "6.1.1 4.2.1 Simple examples and computer experiments\nHere is a first example of a mixture model with two equal-sized components. The generating process has two steps:\nFlip a fair coin.\nIf it comes up heads: Generate a random number from a normal distribution with mean 1 and variance 0.25.\nIf it comes up tails: Generate a random number from a normal distribution with mean 3 and variance 0.25. The histogram shown in Figure 4.1 was produced by repeating these two steps 10000 times using the following code.\ncoinflips = (runif(10000) &gt; 0.5)\ntable(coinflips)__\n\n\ncoinflips\nFALSE  TRUE \n 5003  4997 \n\n\noneFlip = function(fl, mean1 = 1, mean2 = 3, sd1 = 0.5, sd2 = 0.5) {\n  if (fl) {\n   rnorm(1, mean1, sd1)\n  } else {\n   rnorm(1, mean2, sd2)\n  }\n}\nfairmix = vapply(coinflips, oneFlip, numeric(1))\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nggplot(tibble(value = fairmix), aes(x = value)) +\n     geom_histogram(fill = \"purple\", binwidth = 0.1)__\n\nFigure 4.1: Histogram of 10,000 random draws from a fair mixture of two normals. The left hand part of the histogram is dominated by numbers generated from (A), on the right from (B).\n__\nQuestion 4.1\nHow can you use R’s vectorized syntax to remove the vapply-loop and generate the fairmix vector more efficiently?\n__\nSolution\n__\nmeans = c(1, 3)\nsds   = c(0.5, 0.5)\nvalues = rnorm(length(coinflips),\n               mean = ifelse(coinflips, means[1], means[2]),\n               sd   = ifelse(coinflips, sds[1],   sds[2]))__\n__\nQuestion 4.2\nUsing your improved code, use one million coin flips and make a histogram with 200 bins. What do you notice?\n__\nSolution\n__\nfair = tibble(\n  coinflips = (runif(1e6) &gt; 0.5),\n  values = rnorm(length(coinflips),\n                 mean = ifelse(coinflips, means[1], means[2]),\n                 sd   = ifelse(coinflips, sds[1],   sds[2])))\nggplot(fair, aes(x = values)) +\n     geom_histogram(fill = \"purple\", bins = 200)__\n\nFigure 4.2: Similar to Figure 4.1, but with one million observations.\nFigure 4.2 illustrates that as we increase the number of bins and the number of observations per bin, the histogram approaches a smooth curve. This smooth limiting curve is called the density function of the random variable fair$values.\nThe density function for a normal \\(N(,)\\) random variable can be written explicitly. We usually call it\n\\[ (x)=e{-()2}. \\]\n__\nQuestion 4.3\n\nPlot a histogram of those values of fair$values for which coinflips is TRUE. Hint: use y = ..density.. in the call to aes (which means that the vertical axis is to show the proportion of counts) and set the binwidth to 0.01.\nOverlay the line corresponding to \\((z)\\).\n\n__\nSolution\n__\nggplot(dplyr::filter(fair, coinflips), aes(x = values)) +\n  geom_histogram(aes(y = after_stat(density)), fill = \"purple\", binwidth = 0.01) +\n  stat_function(fun = dnorm, color = \"red\",\n                args = list(mean = means[1], sd = sds[1]))__\n\nFigure 4.3: Histogram of half a million draws from the normal distribution \\(N(,2=0.52)\\). The curve is the theoretical density \\((x)\\) calculated using the function dnorm.\nIn fact we can write the mathematical formula for the density of all of fair$values (the limiting curve that the histograms tend to look like) as a sum of the two densities.\n\\[ f(x)=_1(x)+_2(x), \\]\nwhere \\(_1\\) is the density of the normal \\(N(_1=1,^2=0.25)\\) and \\(_2\\) is the density of the normal \\(N(_2=3,^2=0.25)\\). Figure 4.4 was generated by the following code.\nfairtheory = tibble(\n  x = seq(-1, 5, length.out = 1000),\n  f = 0.5 * dnorm(x, mean = means[1], sd = sds[1]) +\n      0.5 * dnorm(x, mean = means[2], sd = sds[2]))\nggplot(fairtheory, aes(x = x, y = f)) +\n  geom_line(color = \"red\", linewidth = 1.5) + ylab(\"mixture density\")__\n\nFigure 4.4: The theoretical density of the mixture.\nIn this case, the mixture model is extremely visible as the two component distributions have little overlap. Figure 4.4 shows two distinct peaks: we call this a bimodal distribution. In practice, in many cases the separation between the mixture components is not so obvious—but nevertheless relevant.\n\nFigure 4.5: A mixture of two normals that is harder to recognize.\n__\nQuestion 4.4\nFigure 4.5 is a histogram of a fair mixture of two normals with the same variances. Can you guess the two mean parameters of the component distributions? Hint: you can use trial and error, and simulate various mixtures to see if you can make a matching histogram. Looking at the R code for the chapter will show you exactly how the data were generated.\n__\nSolution\n__\nThe following code colors in red the points that were generated from the heads coin flips and in blue those from the tails. Its output, in Figure 4.6, shows the two underlying distributions.\nhead(mystery, 3)__\n\n\n# A tibble: 3 × 2\n  coinflips values\n  &lt;lgl&gt;      &lt;dbl&gt;\n1 FALSE       2.40\n2 FALSE       1.66\n3 TRUE        1.22\n\n\nbr = with(mystery, seq(min(values), max(values), length.out = 30))\nggplot(mystery, aes(x = values)) +\n  geom_histogram(data = dplyr::filter(mystery, coinflips),\n     fill = \"red\", alpha = 0.2, breaks = br) +\n  geom_histogram(data = dplyr::filter(mystery, !coinflips),\n     fill = \"darkblue\", alpha = 0.2, breaks = br) __\n\nFigure 4.6: The mixture from Figure 4.5, but with the two components colored in red and blue.\nIn Figure 4.6, the bars from the two component distributions are plotted on top of each other. A different way of showing the components is Figure 4.7, produced by the code below.\nggplot(mystery, aes(x = values, fill = coinflips)) +\n  geom_histogram(data = dplyr::filter(mystery, coinflips),\n     fill = \"red\", alpha = 0.2, breaks = br) +\n  geom_histogram(data = dplyr::filter(mystery, !coinflips),\n     fill = \"darkblue\", alpha = 0.2, breaks = br) +\n  geom_histogram(fill = \"purple\", breaks = br, alpha = 0.2)__\n\nFigure 4.7: As Figure 4.6, with stacked bars for the two mixture components.\n__\nQuestion 4.5\nWhy do the bars in Figure 4.7, but not those in Figure 4.6 have the same heights as in Figure 4.5?\n__\nSolution\n__\nIn Figures 4.7 and 4.5, each count occupies a different piece of vertical space in a bin. In Figure 4.6, in the overlapping (darker) region, some of the counts falling within the same bin are overplotted.\nIn Figures 4.6 and 4.7, we were able to use the coinflips column in the data to disentangle the components. In real data, this information is missing.\n\n\n\nA book-long treatment on the subject of finite mixtures (McLachlan and Peel 2004).\n\n\nA book-long treatment on the subject of finite mixtures (McLachlan and Peel 2004).\n\n\n6.1.2 4.2.2 Discovering the hidden class labels\nWe use a method called the expectation-maximization (EM) algorithm to infer the value of the hidden groupings. The EM algorithm is a popular iterative procedure that alternates between pretending we know one part of the solution to compute the other part, and pretending the other part is known and computing the first part, and so on, until convergence. More concretely, it alternates between\n\npretending we know the probabilities with which each observation belongs to the different mixture components and, from this, estimating the parameters of the components, and\npretending we know the parameters of the mixture components and estimating the probabilities with which each observation belongs to the components.\n\nLet’s take a simple example. We measure a variable \\(x\\) on a series of objects that we think come from two groups, although we do not know the group labels. We start by augmenting 2 the data with the unobserved (latent) group label, which we will call \\(U\\).\n2 Adding another variable which was not measured, called a hidden or latent variable.\nWe are interested in finding the values of \\(U\\) and the unknown parameters \\(\\) of the underlying distribution of the groups. We will use the maximum likelihood approach introduced in Chapter 2 to estimate the parameters that make the data \\(x\\) the most likely. We can write the probability densities\n\\[ p(x,u,|,) = p(x,|,u,),p(u,|,). \\]\n\n6.1.2.1 Mixture of normals\nFor instance, we could generalize our previous mixture model with two normal distributions Equation 4.1 by allowing non-equal mixture fractions,\n\\[ f(x)=_1(x)+(1-)_2(x), \\]\nfor \\(\\). Similarly as above, \\(_k\\) is the density of the normal \\(N(_k,_k^2)\\) for \\(k=1\\) and \\(k=2\\), respectively. Then, the parameter vector \\(\\) is a five-tuple of the two means, the two standard deviations, and the mixture parameter \\(\\). In other words, \\(=(_1,_2,_1,_2,)\\). Here is an example of data generated according to such a model. The labels are denoted by \\(u\\).\nmus = c(-0.5, 1.5)\nlambda = 0.5\nu = sample(2, size = 100, replace = TRUE, prob = c(lambda, 1-lambda))\nx = rnorm(length(u), mean = mus[u])\ndux = tibble(u, x)\nhead(dux)__\n\n\n# A tibble: 6 × 2\n      u     x\n  &lt;int&gt; &lt;dbl&gt;\n1     2 0.303\n2     2 2.65 \n3     1 0.484\n4     2 3.04 \n5     2 1.10 \n6     2 1.96 \nIf we know the labels \\(u\\), we can estimate the parameters using separate MLEs for each group. The overall likelihood is\n\\[ p(x, u ,|, ) = ( _{\\{i:,u_i=1\\}} 1(x_i) ) ( {\\{i:,u_i=2\\}} _2(x_i) ). \\]\nIts maximization can be split into three independent pieces: we find \\(_1\\) and \\(_1\\) by maximizing the first bracketed expression on the right hand side of Equation 4.4, \\(_2\\) and \\(_2\\) by maximizing the second, and \\(\\) from the empirical frequencies of the labels:\ngroup_by(dux, u) |&gt; summarize(mu = mean(x), sigma = sd(x))__\n\n\n# A tibble: 2 × 3\n      u     mu sigma\n  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     1 -0.558  1.05\n2     2  1.41   1.04\n\n\ntable(dux$u) / nrow(dux)__\n\n\n   1    2 \n0.55 0.45 \n__\nQuestion 4.6\nSuppose we knew the mixing proportion \\(=\\), but not the \\(u_i\\), so that the density is \\(_1(x)+_2(x)\\). Try writing out the (log) likelihood. What prevents us from solving for the MLE explicitly here?\n__\nSolution\n__\nSee, e.g., the chapter on Mixture Models in Shalizi (2017) for the computation of the likelihood of a finite mixture of normals. “If we try to estimate the mixture model, then, we’re doing weighted maximum likelihood, with weights given by the posterior label probabilities. These, to repeat, depend on the parameters we are trying to estimate, so there seems to be a vicious circle.”\nIn many cases we neither know the \\(u\\) labels nor the mixture proportions. What we can do is start with an initial guess for the labels and pretend to know them, estimate the parameters as above, and then go through an iterative cycle, updating at each step our current best guess of the labels and the parameters until our estimates no longer substantially change (i.e., until they seem to have converged) and the likelihood has reached an optimum.\nIn fact we can do something more elaborate and replace the “hard” labels \\(u\\) for each observation (it is either in group 1 or 2) by membership weights that sum up to 1. The mixture model 4.3 suggests that we interpret\n\\[ w(x, 1) = \\]\nas the probability that an observation with value \\(x\\) was generated by the first mixture component, and analogously \\(w(x, 2) = 1 - w(x, 1)\\) for the second component. In other words, while \\(\\) is the prior probability that an observation that we have not yet seen comes from mixture component 1, \\(w(x,1)\\) is the corresponding posterior probability once we have observed its value \\(x\\). This suggests the following iterative algorithm:\nE step : Assuming that \\(\\) (i.e., the means, standard deviations and \\(\\)) is known, evaluate the membership weights 4.5.\nM step : Given the membership weights of each observation \\(x_i\\), determine a new, improved maximum likelihood estimate of \\(\\).\nIterate until \\(\\) and the likelihood have converged. At this point, please check out Exercise 4.1 for a worked out demonstration with code. In fact, this algorithm is far more general than our particular application example here. A very readable exposition is presented by (Bishop 2006), here some of the highlights:\nOur goal is to maximize the marginal likelihood of a probabilistic model that involves an observed variable \\(x\\), an unobserved variable \\(u\\) and some parameter \\(\\). In our simple example, \\(u\\) is a categorical variable with two possible values, and \\(x\\) a real number. In general, both \\(x\\) and \\(u\\) can be a tuple of multiple individual variables (i.e., they can be multivariate) of any variable type. The marginal likelihood is computed by taking the expectation (i.e., a weighted average) across all possible values of \\(u\\):\n\\[ L_(; x) = _U p(x, u,|,) , U. \\]\nIn our specific example, the integration amounts to (probabilistically) averaging over all possible membership configurations, and thus, to the weighted sum taking into account the membership weights,\n\\[ L_(; x) = {i=1}^n {u=1}^2 p(x_i, u,|,) , w(x_i, u,|,). \\]\nDirectly maximizing this quantity is intractable.\nSomething we do have a grip on, given the data and our current parameter estimate \\(_t\\), is the conditional distribution of the latent variable, \\(p(u,|,x, _t)\\). We can use it to find the expectation of the complete data log likelihood \\(p(x, u,|,)\\) evaluated for some general parameter value \\(\\). This expectation is often denoted as\n\\[ Q(, _t) = _U p(x, u,|,) , p(u,|, x, _t) , U. \\]\nIn the M step, we determine the revised parameter estimate \\(_{t+1}\\) by maximizing this function,\n\\[ {t+1} = , Q(, _t). \\]\nThe E step consists of computing \\(p(u,|, x, _t)\\), the essential ingredient of \\(Q\\).\nThese two steps (E and M) are iterated until the improvements are small; this is a numerical indication that we are close to a flattening of the likelihood and have reached a maximum. The iteration trajectory, but hopefully not the point that we arrive at, will depend on the starting point. This is analogous to a hike to the top of a mountain, which can start at different places and may take different routes, but always leads to the top of the hill—as long as there is only one hilltop and not several. Thus, as a precaution, it is good practice to repeat such a procedure several times from different starting points and make sure that we always get the same answer.\n__\nQuestion 4.7\nSeveral R packages provide EM implementations, including mclust , EMcluster and EMMIXskew. Choose one and run the EM function several times with different starting values. Then use the function normalmixEM from the mixtools package to compare the outputs.\n__\nSolution\n__\nHere we show the output from mixtools.\nlibrary(\"mixtools\")\ny = c(rnorm(100, mean = -0.2, sd = 0.5),\n      rnorm( 50, mean =  0.5, sd =   1))\ngm = normalmixEM(y, k = 2, \n                    lambda = c(0.5, 0.5),\n                    mu = c(-0.01, 0.01), \n                    sigma = c(3, 3))__\n\n\nnumber of iterations= 134 \n\n\nwith(gm, c(lambda, mu, sigma, loglik))__\n\n\n[1]    0.9263    0.0737   -0.1859    1.9216    0.6134    0.4746 -170.5662\nThe EM algorithm is very instructive:\n\nWe saw a first example of soft averaging, where we don’t decide whether an observation belongs to one group or another, but allow it to participate in several groups by using membership probabilities as weights, and thus obtain more nuanced estimates (Slonim et al. 2005).\nIt shows us how we can tackle a difficult problem with too many unknowns by alternating between solving simpler problems.\nWe were able to consider a data generating model with hidden variables, and still estimate its parameters. We could do so even without explicitly commiting on the values of the hidden variables: we took a (weighted) average over them, in the Expecation step of Equation 4.8, embodied by the membership probabilities. This basic idea is so powerful that it is the starting point for many more advanced algorithms in machine learning (Bishop 2006).\nIt can be extended to the more general case of model averaging (Hoeting et al. 1999). It can be sometimes beneficial to consider several models simultaneously if we are unsure which one is relevant for our data. We can combine them together into a weighted model. The weights are provided by the likelihoods of the models.\n\n\n\n\n6.1.3 4.2.3 Models for zero inflated data\nEcological and molecular data often come in the form of counts. For instance, this may be the number of individuals from each of several species at each of several locations. Such data can often be seen as a mixture of two scenarios: if the species is not present, the count is necessarily zero, but if the species is present, the number of individuals we observe varies, with a random sampling distribution; this distribution may also include zeros. We model this as a mixture:\n\\[ f_{}(x) = , (x) + (1-) , f_{}(x), \\]\nwhere \\(\\) is Dirac’s delta function, which represents a probability distribution that has all its mass at 0. The zeros from the first mixture component are called “structural”: in our example, they occur because certain species do not live in certain habitats. The second component, \\(f_{}\\) may also include zeros and other small numbers, simply due to random sampling. The R packages pscl (Zeileis, Kleiber, and Jackman 2008) and zicounts provide many examples and functions for working with zero inflated counts.\n\n6.1.3.1 Example: ChIP-Seq data\nLet’s consider the example of ChIP-Seq data. These data are sequences of pieces of DNA that are obtained from chromatin immunoprecipitation (ChIP). This technology enables the mapping of the locations along genomic DNA of transcription factors, nucleosomes, histone modifications, chromatin remodeling enzymes, chaperones, polymerases and other protein. It was the main technology used by the Encyclopedia of DNA Elements (ENCODE) Project. Here we use an example (Kuan et al. 2011) from the mosaicsExample package, which shows data measured on chromosome 22 from ChIP-Seq of antibodies for the STAT1 protein and the H3K4me3 histone modification applied to the GM12878 cell line. Here we do not show the code to construct the binTFBS object; it is shown in the source code file for this chapter and follows the vignette of the mosaics package.\nbinTFBS __\n\n\nSummary: bin-level data (class: BinData)\n----------------------------------------\n- # of chromosomes in the data: 1\n- total effective tag counts: 462479\n  (sum of ChIP tag counts of all bins)\n- control sample is incorporated\n- mappability score is NOT incorporated\n- GC content score is NOT incorporated\n- uni-reads are assumed\n----------------------------------------\nFrom this object, we can create the histogram of per-bin counts.\nbincts = print(binTFBS)\nggplot(bincts, aes(x = tagCount)) +\n  geom_histogram(binwidth = 1, fill = \"forestgreen\")__\n\nFigure 4.8: The number of binding sites found in 200nt windows along chromosome 22 in a ChIP-Seq dataset.\nWe see in Figure 4.8 that there are many zeros, although from this plot it is not immediately obvious whether the number of zeros is really extraordinary, given the frequencies of the other small numbers (\\(1,2,…\\)).\n__\nQuestion 4.8\n\nRedo the histogram of the counts using a logarithm base 10 scale on the \\(y\\)-axis.\nEstimate \\(_0\\), the proportion of bins with zero counts.\n\n__\nSolution\n__\nggplot(bincts, aes(x = tagCount)) + scale_y_log10() +\n   geom_histogram(binwidth = 1, fill = \"forestgreen\")__\n\nFigure 4.9: As Figure 4.8, but using a logarithm base 10 scale on the \\(y\\)-axis. The fraction of zeros seems elevated compared to that of ones, twos, …\n\n\n\n6.1.4 4.2.4 More than two components\nSo far we have looked at mixtures of two components. We can extend our description to cases where there may be more. For instance, when weighing N=7,000 nucleotides obtained from mixtures of deoxyribonucleotide monophosphates (each type has a different weight, measured with the same standard deviation sd=3), we might observe the histogram (shown in Figure 4.10) generated by the following code.\nmasses = c(A =  331, C =  307, G =  347, T =  322)\nprobs  = c(A = 0.12, C = 0.38, G = 0.36, T = 0.14)\nN  = 7000\nsd = 3\nnuclt   = sample(length(probs), N, replace = TRUE, prob = probs)\nquadwts = rnorm(length(nuclt),\n                mean = masses[nuclt],\n                sd   = sd)\nggplot(tibble(quadwts = quadwts), aes(x = quadwts)) +\n  geom_histogram(bins = 100, fill = \"purple\")__\n\nFigure 4.10: Simulation of 7,000 nucleotide mass measurements.\n__\nQuestion 4.9\nRepeat this simulation experiment with \\(N=1000\\) nucleotide measurements. What do you notice in the histogram?\n__\nQuestion 4.10\nWhat happens when \\(N=7000\\) but the standard deviation is 10?\n__\nQuestion 4.11\nPlot the theoretical density curve for the distribution simulated in Figure 4.10.\nIn this case, as we have enough measurements with good enough precision, we are able to distinguish the four nucleotides and decompose the distribution shown in Figure 4.10. With fewer data and/or more noisy measurements, the four modes and the distribution component might be less clear.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "04-chap.html#empirical-distributions-and-the-nonparametric-bootstrap",
    "href": "04-chap.html#empirical-distributions-and-the-nonparametric-bootstrap",
    "title": "6  4.1 Goals for this chapter",
    "section": "6.2 4.3 Empirical distributions and the nonparametric bootstrap",
    "text": "6.2 4.3 Empirical distributions and the nonparametric bootstrap\nIn this section, we will consider an extreme case of mixture models, where we model our sample of \\(n\\) data points as a mixture of \\(n\\) point masses. We could use almost any set of data here; to be concrete, we use Darwin’s Zea Mays data3 in which he compared the heights of 15 pairs of Zea Mays plants (15 self-hybridized versus 15 crossed). The data are available in the HistData package, and we plot the distribution of the 15 differences in height:\n3 They were collected by Darwin who asked his cousin, Francis Galton to analyse them. R.A. Fisher re-analysed the same data using a paired t-test (Bulmer 2003). We will get back to this example in Chapter 13.\nlibrary(\"HistData\")\nZeaMays$diff __\n\n\n [1]  6.125 -8.375  1.000  2.000  0.750  2.875  3.500  5.125  1.750  3.625\n[11]  7.000  3.000  9.375  7.500 -6.000\n\n\nggplot(ZeaMays, aes(x = diff, ymax = 1/15, ymin = 0)) +\n  geom_linerange(linewidth = 1, col = \"forestgreen\") + ylim(0, 0.1)__\n\nFigure 4.11: The observed sample can be seen as a mixture of point masses at each of the values (real point masses would be bars without any width whatsoever).\nIn Section 3.6.7 we saw that the empirical cumulative distribution function (ECDF) for a sample of size \\(n\\) is\n\\[ n(x)= {i=1}^n {}_{x x_i}, \\]\nand we saw ECDF plots in Figure 3.24. We can also write the density of our sample as\n\\[ n(x) ={i=1}^n _{x_i}(x) \\]\nIn general, the density of a probability distribution is the derivative (if it exists) of the distribution function. We have applied this principle here: the density of the distribution defined by Equation 4.11 is Equation 4.12. We could do this since one can consider the function \\(a\\) the “derivative” of the step function \\({}{x a}\\): it is completely flat almost everywhere, except at the one point \\(a\\) where there is the step, and where its value is “infinite” . Equation 4.12 highlights that our data sample can be considered a mixture of \\(n\\) point masses at the observed values \\(x_1,x_2,…, x_{n}\\), such as in Figure 4.11.\nThere is a bit of advanced mathematics (beyond standard calculus) required for this to make sense, which is however beyond the scope of our treatment here.\n\nFigure 4.12: The value of a statistic \\(\\) is estimated from data (the grey matrices) generated from an underlying distribution \\(F\\). Different samples from \\(F\\) lead to different data, and so to different values of the estimate \\(\\): this is called sampling variability. The distribution of all the \\(\\)’s is the sampling distribution.\nStatistics of our sample, such as the mean, minimum or median, can now be written as a function of the ECDF, for instance, \\({x} = {x_i}(x),x\\). As another instance, if \\(n\\) is an odd number, the median is \\(x{()}\\), the value right in the middle of the ordered list.\nThe true sampling distribution of a statistic \\(\\) is often hard to know as it requires many different data samples from which to compute the statistic; this is shown in Figure 4.12.\nThe bootstrap principle approximates the true sampling distribution of \\(\\) by creating new samples drawn from the empirical distribution built from the original sample (Figure 4.13). We reuse the data (by considering it a mixture distribution of \\(\\)s) to create new “datasets” by taking samples and looking at the sampling distribution of the statistics \\(^*\\) computed on them. This is called the nonparametric bootstrap resampling approach, see Bradley Efron and Tibshirani (1994) for a complete reference. It is very versatile and powerful method that can be applied to basically any statistic, no matter how complicated it is. We will see example applications of this method, in particular to clustering, in Chapter 5.\n\nFigure 4.13: The bootstrap simulates the sampling variability by drawing samples not from the underlying true distribution \\(F\\) (as in Figure 4.12), but from the empirical distribution function \\(_n\\).\nUsing these ideas, let’s try to estimate the sampling distribution of the median of the Zea Mays differences we saw in Figure 4.11. We use similar simulations to those in the previous sections: Draw \\(B=1000\\) samples of size 15 from the 15 values (each being a component in the 15-component mixture). Then compute the 1000 medians of each of these samples of 15 values and look at their distribution: this is the bootstrap sampling distribution of the median.\nB = 1000\nmeds = replicate(B, {\n  i = sample(15, 15, replace = TRUE)\n  median(ZeaMays$diff[i])\n})\nggplot(tibble(medians = meds), aes(x = medians)) +\n  geom_histogram(bins = 30, fill = \"purple\")__\n\nFigure 4.14: The bootstrap sampling distribution of the median of the Zea Mays differences.\n__\nQuestion 4.12\nEstimate a 99% confidence interval for the median based on these simulations. What can you conclude from looking at the overlap between this interval and 0?\n__\nQuestion 4.13\nUse the bootstrap package to redo the same analysis using the function bootstrap for both median and mean. What differences do you notice between the sampling distribution of the mean and the median?\n__\nSolution\n__\nlibrary(\"bootstrap\")\nbootstrap(ZeaMays$diff, B, mean)\nbootstrap(ZeaMays$diff, B, median)__\n\n6.2.0.1 Why nonparametric?\nIn theoretical statistics, nonparametric methods are those that have infinitely many degrees of freedom or numbers of unknown parameters.\n\nIn practice, we do not wait for infinity; when the number of parameters becomes as large or larger than the amount of data available, we say the method is nonparametric. The bootstrap uses a mixture with \\(n\\) components, so with a sample of size \\(n\\), it qualifies as a nonparametric method.\nDespite their name, nonparametric methods are not methods that do not use parameters: all statistical methods estimate unknown quantities.\n__\nQuestion 4.14\nIf the sample is composed of \\(n=3\\) different values, how many different bootstrap resamples are possible? Answer the same question with \\(n=15\\).\n__\nSolution\n__\nThe set of all bootstrap resamples is equivalent to the set of all vectors of \\(n\\) integers whose sum is \\(n\\). Denote by \\( = (k_1,k_2,…,k_n)\\) the number of times the observations \\(x_1,x_2,…, x_n\\) occur in a bootstrap sample. We can think of each \\(k_i\\) as a box (as in the multinomial distribution), and there are \\(n\\) boxes in which to drop \\(n\\) balls. We can count the number of configurations by counting the number of ways of separating \\(n\\) balls into the boxes, i.e. by writing down \\(n\\) times an o (for the balls) and \\(n-1\\) times a separator | between them. So we have \\(2n-1\\) positions to fill, for which we must choose either o (a ball) or | (a separator). For \\(n=3\\), a possible placement would be oo||o, which corresponds to \\( = (2,0,1)\\). In general, this number is \\({2n-1} \\), and thus the answers for \\(n=3\\) and \\(15\\) are:\nc(N3 = choose(5, 3), N15 = choose(29, 15))__\n\n\n      N3      N15 \n      10 77558760 \n__\nQuestion 4.15\nWhat are the two types of error that can occur when using the bootstrap as it is implemented in the bootstrap package? Which parameter can you modify to improve one of them?\n__\nSolution\n__\nMonte Carlo simulations of subsets of the data by resampling randomly approximate the exhaustive bootstrap (Diaconis and Holmes 1994). Increasing the size of nboot argument in the bootstrap function reduces the Monte Carlo error, however, the exhaustive bootstrap is still not exact: we are still using an approximate distribution function, that of the data instead of the true distribution. If the sample size is small or the original sample biased, the approximation can still be quite poor, no matter how large we choose nboot.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "04-chap.html#infinite-mixtures",
    "href": "04-chap.html#infinite-mixtures",
    "title": "6  4.1 Goals for this chapter",
    "section": "6.3 4.4 Infinite mixtures",
    "text": "6.3 4.4 Infinite mixtures\nSometimes mixtures can be useful even if we don’t aim to assign a label to each observation or, to put it differently, if we allow as many `labels’ as there are observations. If the number of mixture components is as big as (or bigger than) the number of observations, we say we have an infinite mixture. Let’s look at some examples.\n\n6.3.1 4.4.1 Infinite mixture of normals\n\nFigure 4.15: Laplace knew already that the probability density \\[f_X(y)=(-),&gt;0\\] has the median as its location parameter \\(\\) and the median absolute deviation (MAD) as its scale parameter \\(\\).\nConsider the following two-level data generating scheme:\nLevel 1 Create a sample of Ws from an exponential distribution.\nw = rexp(10000, rate = 1)__\nLevel 2 The \\(w\\)s serve as the variances of normal variables with mean \\(\\) generated using rnorm.\nmu  = 0.3\nlps = rnorm(length(w), mean = mu, sd = sqrt(w))\nggplot(data.frame(lps), aes(x = lps)) +\n  geom_histogram(fill = \"purple\", binwidth = 0.1)__\n\nFigure 4.16: Data sampled from a Laplace distribution.\nThis turns out to be a rather useful distribution. It has well-understood properties and is named after Laplace, who proved that the median is a good estimator of its location parameter \\(\\) and that the median absolute deviation can be used to estimate its scale parameter \\(\\). From the formula in the caption of Figure 4.15 we see that the \\(L_1\\) distance (absolute value of the difference) holds a similar position in the Laplace density as the \\(L_2\\) (square of the difference) does for the normal density.\nConversely, in Bayesian regression4, having a Laplace distribution as a prior on the coefficients amounts to an \\(L_1\\) penalty, called the lasso (Tibshirani 1996), while a normal distribution as a prior leads to an \\(L_2\\) penalty, called ridge regression.\n4 Don’t worry if you are not familiar with this, in that case just skip this sentence.\n__\nQuestion 4.16\nWrite a random variable whose distribution is the symmetric Laplace as a function of normal and exponential random variables.\n__\nSolution\n__\nWe can write the hierarchical model with variances generated as exponential variables, \\(W\\), as:\n\\[ X = Z, W Exp(1), Z N(0,1). \\]\n\n6.3.1.1 Asymmetric Laplace\nIn the Laplace distribution, the variances of the normal components depend on \\(W\\), while the means are unaffected. A useful extension adds another parameter \\(\\) that controls the locations or centers of the components. We generate data alps from a hierarchical model with \\(W\\) an exponential variable; the output shown in Figure 4.17 is a histogram of normal \\(N(+w,w)\\) random numbers, where the \\(w\\)’s themselves were randomly generated from an exponential distribution with mean \\(1\\) as shown in the code:\nmu = 0.3; sigma = 0.4; theta = -1\nw  = rexp(10000, 1)\nalps = rnorm(length(w), theta + mu * w, sigma * sqrt(w))\nggplot(tibble(alps), aes(x = alps)) +\n  geom_histogram(fill = \"purple\", binwidth = 0.1)__\n\nFigure 4.17: Histogram of data generated from an asymmetric Laplace distribution – a scale mixture of many normals whose means and variances are dependent. We write \\(X AL(, , )\\).\nSuch hierarchical mixture distributions, where every instance of the data has its own mean and variance, are useful models in many biological settings. Examples are shown in Figure 4.18.\n\n\nThe lengths of the promoters shorter than 2000bp from Saccharomyces cerevisiae as studied by Kristiansson et al. (2009).\n\n\n\nThe log-ratios of microarray gene expression measurements for 20,000 genes (Purdom and Holmes 2005).\n\nFigure 4.18: Histogram of real data. Both distributions can be modeled by asymmetric Laplace distributions.\n__\nQuestion 4.17\nLooking at the log-ratio of gene expression values from a microarray, one gets a distribution as shown on the right of Figure 4.18. How would one explain that the data have a histogram of this form?\nThe Laplace distribution is an example of where the consideration of the generative process indicates how the variance and mean are linked. The expectation value and variance of an asymmetric Laplace distribution \\(AL(, , )\\) are\n\\[ E(X)=+(X)=2+2. \\]\nNote the variance is dependent on the mean, unless \\(= 0\\) (the case of the symmetric Laplace Distribution). This is the feature of the distribution that makes it useful. Having a mean-variance dependence is very common for physical measurements, be they microarray fluorescence intensities, peak heights from a mass spectrometer, or reads counts from high-throughput sequencing, as we’ll see in the next section.\n\n\n\n6.3.2 4.4.2 Infinite mixtures of Poisson variables.\n\nFigure 4.19: How to count the fish in a lake? MC Escher.\nA similar two-level hierarchical model is often also needed to model real- world count data. At the lower level, simple Poisson and binomial distributions serve as the building blocks, but their parameters may depend on some underlying (latent) process. In ecology, for instance, we might be interested in variations of fish species in all the lakes in a region. We sample the fish species in each lake to estimate their true abundances, and that could be modeled by a Poisson. But the true abundances will vary from lake to lake. And if we want to see whether, for instance, changes in climate or altitude play a role, we need to disentangle such systematic effects from random lake-to-lake variation. The different Poisson rate parameters \\(\\) can be modeled as coming from a distribution of rates. Such a hierarchical model also enables us to add supplementary steps in the hierarchy, for instance we could be interested in many different types of fish, model altitude and other environmental factors separately, etc.\nFurther examples of sampling schemes that are well modeled by mixtures of Poisson variables include applications of high-throughput sequencing, such as RNA-Seq, which we will cover in detail in Chapter 8, or 16S rRNA-Seq data used in microbial ecology.\n\n\n6.3.3 4.4.3 Gamma distribution: two parameters (shape and scale)\nNow we are getting to know a new distribution that we haven’t seen before. The gamma distribution is an extension of the (one-parameter) exponential distribution, but it has two parameters, which makes it more flexible. It is often useful as a building block for the upper level of a hierarchical model. The gamma distribution is positive-valued and continuous. While the density of the exponential has its maximum at zero and then simply decreases towards 0 as the value goes to infinity, the density of the gamma distribution has its maximum at some finite value. Let’s explore it by simulation examples. The histograms in Figure 4.20 were generated by the following lines of code:\nggplot(tibble(x = rgamma(10000, shape = 2, rate = 1/3)),\n   aes(x = x)) + geom_histogram(bins = 100, fill= \"purple\")\nggplot(tibble(x = rgamma(10000, shape = 10, rate = 3/2)),\n   aes(x = x)) + geom_histogram(bins = 100, fill= \"purple\")__\n\n\ngamma\\((2,)\\)\n\n\n\ngamma\\((10,)\\)\n\nFigure 4.20: Histograms of random samples of gamma distributions. The gamma is a flexible two parameter distribution: see Wikipedia.\n\n6.3.3.1 Gamma–Poisson mixture: a hierarchical model\n\nGenerate a set of parameters: \\(_1,_2,…\\) from a gamma distribution.\nUse these to generate a set of Poisson(\\(_i\\)) random variables, one for each \\(_1\\).\n\nlambda = rgamma(10000, shape = 10, rate = 3/2)\ngp = rpois(length(lambda), lambda = lambda)\nggplot(tibble(x = gp), aes(x = x)) +\n  geom_histogram(bins = 100, fill= \"purple\")__\n\nFigure 4.21: Histogram of gp, generated via a gamma-Poisson hierachical model.\nThe resulting values are said to come from a gamma–Poisson mixture. Figure 4.21 shows the histogram of gp.\n__\nQuestion 4.18\n\nAre the values generated from such a gamma–Poisson mixture continuous or discrete ?\nWhat is another name for this distribution? Hint: Try the different distributions provided by the goodfit function from the vcd package.\n\n__\nSolution\n__\nlibrary(\"vcd\")\nofit = goodfit(gp, \"nbinomial\")\nplot(ofit, xlab = \"\")\nofit$par __\n\n\n$size\n[1] 9.911829\n\n$prob\n[1] 0.5963857\n\nFigure 4.22: Goodness of fit plot. The rootogram shows the theoretical probabilities of the gamma-Poisson distribution (a.k.a. negative binomial) as red dots and the square roots of the observed frequencies as the height of the rectangular bars. The bars all end close to the horizontal axis, which indicates a good fit to the negative binomial distribution.\nIn R, and in some other places, the gamma-Poisson distribution travels under the alias name of negative binomial distribution. The two names are synonyms; the second one alludes to the fact that Equation 4.15 bears some formal similarities to the probabilities of a binomial distribution. The first name, gamma–Poisson distribution, is more indicative of its generating mechanism, and that’s what we will use in the rest of the book. It is a discrete distribution, that means that it takes values only on the natural numbers (in contrast to the gamma distribution, which covers the whole positive real axis). Its probability distribution is\n\\[ (K=k)=(]\nwhich depends on the two parameters \\(a^+\\) and \\(p\\). Equivalently, the two parameters can be expressed by the mean \\(=pa/(1-p)\\) and a parameter called the dispersion \\(/a\\). The variance of the distribution depends on these parameters, and is \\(+^2\\).\n\nFigure 4.23: Visualization of the hierarchical model that generates the gamma- Poisson distribution. The top panel shows the density of a gamma distribution with mean 50 (vertical black line) and variance 30. Assume that in one particular experimental replicate, the value 60 is realized. This is our latent variable. The observable outcome is distributed according to the Poisson distribution with that rate parameter, shown in the middle panel. In one particular experiment the outcome may be, say, 55, indicated by the dashed green line. Overall, if we repeat these two subsequent random process many times, the outcomes will be distributed as shown in the bottom panel – the gamma-Poisson distribution.\n__\nQuestion 4.19\nIf you are more interested in analytical derivations than illustrative simulations, try writing out the mathematical derivation of the gamma-Poisson probability distribution.\n__\nSolution\n__\nRecall that the final distribution is the result of a two step process:\n\nGenerate a \\((a,b)\\) distributed number, call it \\(x\\), from the density\n\n\\[ f_(x, a, b)=,x{a-1},e{-b x}, \\]\nwhere \\(\\) is the so-called \\(\\)-function, \\((a)=_0x{a-1},e^{-x},x\\) (not to be confused with the gamma distribution, even though there is this incidental relation).\n\nGenerate a number \\(k\\) from the Poisson distribution with rate \\(x\\). The probability distribution is\n\n\\[ f_{}(k, =x)= \\]\nIf \\(x\\) only took on a finite set of values, we could solve the problem simply by summing over all the possible cases, each weighted by their probability according to \\(f_\\). But \\(x\\) is continuous, so we have to write the sum out as an integral instead of a discrete sum. We call the distribution of \\(K\\) the marginal. Its probability mass function is\n\\[ ]\nCollect terms and move terms independent of \\(x\\) outside the integral\n\\[ P(K=k)= _{x=0}^{} x{k+a-1}e{-(b+1)x} dx \\]\nBecause we know the gamma density sums to one: \\(_0^ x{k+a-1}e{-(b+1)x} dx = \\)\n\\[ ]\nwhere in the last line we used that \\((v+1)=v!\\). This is the same as Equation (4.15), the gamma-Poisson with size parameter \\(a\\) and probability \\(p=\\).\n\n\n\n6.3.4 4.4.4 Variance stabilizing transformations\nA key issue we need to control when we analyse experimental data is how much variability there is between repeated measurements of the same underlying true value, i.e., between replicates. This will determine whether and how well we can see any true differences, i.e., between different conditions. Data that arise through the type of hierarchical models we have studied in this chapter often turn out to have very heterogeneous variances, and this can be a challenge. We will see how in such cases variance-stabilizing transformations (Anscombe 1948) can help. Let’s start with a series of Poisson variables with rates from 10 to 100:\nNote how we construct the dataframe (or, more precisely, the tibble) simdat: the output of the lapply loop is a list of tibble s, one for each value of lam. With the pipe operator |&gt; we send it to the function bind_rows (from the dplyr package). The result is a dataframe of all the list elements neatly stacked on top of each other.\nsimdat = lapply(seq(10, 100, by = 10), function(lam)\n    tibble(n = rpois(200, lambda = lam),\n           `sqrt(n)` = sqrt(n),\n       lambda = lam)) |&gt;\n  bind_rows() |&gt;\n  tidyr::pivot_longer(cols = !lambda)\nggplot(simdat, aes(x = factor(lambda), y = value)) + xlab(expression(lambda)) +\n  geom_violin() + facet_grid(rows = vars(name), scales = \"free\")__\n\nFigure 4.24: Poisson distributed measurement data, for eight different choices of the mean lambda. In the upper panel, the \\(y\\)-axis is proportional to the data; in the lower panel, it is on a square-root scale. Note how the distribution widths change in the first case, but less so in the second.\nThe data that we see in the upper panel of Figure 4.24 are an example of what is called heteroscedasticity : the standard deviations (or, equivalently, the variance) of our data is different in different regions of our data space. In particular, it increases along the \\(x\\)-axis, with the mean. For the Poisson distribution, we indeed know that the standard deviation is the square root of the mean; for other types of data, there may be other dependencies. This can be a problem if we want to apply subsequent analysis techniques (for instance, regression, or a statistical test) that are based on assuming that the variances are the same. In Figure 4.24, the numbers of replicates for each value of lambda are quite large. In practice, this is not always the case. Moreover, the data are usually not explicitly stratified by a known mean as in our simulation, so the heteroskedasticity may be harder to see, even though it is there. However, as we see in the lower panel of Figure 4.24, if we simply apply the square root transformation, then the transformed variables will have approximately the same variance. This works even if we do not know the underlying mean for each observation, the square root transformation does not need this information. We can verify this more quantitatively, as in the following code, which shows the standard deviations of the sampled values n and sqrt(n) for the difference choices of lambda.\nThe standard deviation of the square root transformed values is consistently around 0.5, so we would use the transformation 2*sqrt(n) to achieve unit variance.\nsummarise(group_by(simdat, name, lambda), sd(value)) |&gt; tidyr::pivot_wider(values_from = `sd(value)`)__\n\n\n# A tibble: 10 × 3\n   lambda     n `sqrt(n)`\n    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1     10  2.95     0.478\n 2     20  4.19     0.470\n 3     30  5.62     0.521\n 4     40  5.99     0.473\n 5     50  7.69     0.546\n 6     60  7.59     0.492\n 7     70  8.69     0.520\n 8     80  8.99     0.505\n 9     90  9.44     0.498\n10    100  9.84     0.495\nAnother example, now using the gamma-Poisson distribution, is shown in Figure 4.25. We generate gamma-Poisson variables u5 and plot the 95% confidence intervals around the mean.\n5 To catch a greater range of values for the mean value mu, without creating too dense a sequence, we use a geometric series: \\(_{i+1} = 2_i\\).\nmuvalues = 2^seq(0, 10, by = 1)\nsimgp = lapply(muvalues, function(mu) {\n  u = rnbinom(n = 1e4, mu = mu, size = 4)\n  tibble(mean = mean(u), sd = sd(u),\n         lower = quantile(u, 0.025),\n         upper = quantile(u, 0.975),\n         mu = mu)\n  } ) |&gt; bind_rows()\nhead(as.data.frame(simgp), 2)__\n\n\n    mean       sd lower upper mu\n1 0.9965 1.106440     0     4  1\n2 2.0233 1.748503     0     6  2\n\n\nggplot(simgp, aes(x = mu, y = mean, ymin = lower, ymax = upper)) +\n  geom_point() + geom_errorbar()__\n\nFigure 4.25: gamma-Poisson distributed measurement data, for a range of \\(\\) from 1 to 1024.\n__\nQuestion 4.20\nHow can we find a transformation for these data that stabilizes the variance, similar to the square root function for the Poisson distributed data?\n__\nSolution\n__\nIf we divide the values that correspond to mu[1] (and which are centered around simgp$mean[1]) by their standard deviation simgp$sd[1], the values that correspond to mu[2] (and which are centered around simgp$mean[2]) by their standard deviation simgp$sd[2], and so on, then the resulting values will have, by construction, a standard deviation (and thus variance) of 1. And rather than defining 11 separate transformations, we can achieve our goal by defining one single piecewise linear and continuous function that has the appropriate slopes at the appropriate values.\nsimgp = mutate(simgp,\n  slopes = 1 / sd,\n  trsf   = cumsum(slopes * mean))\nggplot(simgp, aes(x = mean, y = trsf)) +\n  geom_point() + geom_line() + xlab(\"\")__\n\nFigure 4.26: Piecewise linear function that stabilizes the variance of the data in Figure 4.25.\nWe see in Figure 4.26 that this function has some resemblance to a square root function in particular at its lower end. At the upper end, it seems to look more like a logarithm. The more mathematically inclined will see that an elegant extension of these numerical calculations can be done through a little calculus known as the delta method , as follows.\nCall our transformation function \\(g\\), and assume it’s differentiable (that’s not a very strong assumption: pretty much any function that we might consider reasonable here is differentiable). Also call our random variables \\(X_i\\), with means \\(_i\\) and variances \\(v_i\\), and we assume that \\(v_i\\) and \\(_i\\) are related by a functional relationship \\(v_i = v(_i)\\). Then, for values of \\(X_i\\) in the neighborhood of its mean \\(_i\\),\n\\[ g(X_i) = g(_i) + g’(_i) (X_i-_i) + … \\]\nwhere the dots stand for higher order terms that we can neglect. The variances of the transformed values are then\n\\[ \\[\\begin{align} \\text{Var}(g(X_i)) &\\simeq g'(\\mu_i)^2\\\\\\ \\text{Var}(X_i) &=\ng'(\\mu_i)^2 \\, v(\\mu_i), \\end{align}\\] \\]\nwhere we have used the rules \\((X-c)=(X)\\) and \\((cX)=c^2,(X)\\) that hold whenever \\(c\\) is a constant number. Requiring that this be constant leads to the differential equation\n\\[ g’(x) = . \\]\nFor a given mean-variance relationship \\(v()\\), we can solve this for the function \\(g\\). Let’s check this for some simple cases:\n\nif \\(v()=\\) (Poisson), we recover \\(g(x)=\\), the square root transformation.\nIf \\(v()=,^2\\), solving the differential equation 4.19 gives \\(g(x)=(x)\\). This explains why the logarithm transformation is so popular in many data analysis applications: it acts as a variance stabilizing transformation whenever the data have a constant coefficient of variation, that is, when the standard deviation is proportional to the mean.\n\n__\nQuestion 4.21\nWhat is the variance-stabilizing transformation associated with \\(v() = + ,^2\\)?\n__\nSolution\n__\nTo solve the differential equation 4.19 with this function \\(v()\\), we need to compute the integral\n\\[ . \\]\nA closed form expression can be looked up in a reference table such as (Bronštein and Semendjajew 1979). These authors provide the general solution\n\\[ = (2+2ax+b) + , \\]\ninto which we can plug in our special case \\(a=\\), \\(b=1\\), \\(c=0\\), to obtain the variance-stabilizing transformation\n\\[ \\[\\begin{align} g_\\alpha(x) &= \\frac{1}{2\\sqrt{\\alpha}}\n\\ln\\left(2\\sqrt{\\alpha x (\\alpha x+1)} + 2\\alpha x + 1\\right) \\\\\\ &=\n\\frac{1}{2\\sqrt{\\alpha}} {\\displaystyle \\operatorname {arcosh}} (2\\alpha\nx+1).\\\\\\ \\end{align}\\] \\]\nFor the second line in Equation 4.22, we used the identity \\({ }(z) = (z+)\\). In the limit of \\(\\), we can use the linear approximation \\((1+)=+O(^2)\\) to see that \\(g_0(x)=\\). Note that if \\(g_\\) is a variance-stabilizing transformation, then so is \\(ug_+v\\) for any pair of numbers \\(u\\) and \\(v\\), and we have used this freedom to insert an extra factor \\(\\) for reasons that become apparent in the following. You can verify that the function \\(g_\\) from Equation 4.22 fulfills condition 4.19 by computing its derivative, which is an elementary calculation. We can plot it:\nf = function(x, a) \n  ifelse (a==0, \n    sqrt(x), \n    log(2*sqrt(a) * sqrt(x*(a*x+1)) + 2*a*x+1) / (2*sqrt(a)))\nx  = seq(0, 24, by = 0.1)\ndf = lapply(c(0, 0.05*2^(0:5)), function(a) \n  tibble(x = x, a = a, y = f(x, a))) %&gt;% bind_rows()\nggplot(df, aes(x = x, y = y, col = factor(a))) + \n  geom_line() + labs(col = expression(alpha))__\n\nFigure 4.27: Graph of the function Equation 4.22 for different choices of \\(\\).\nand empirically verify the equivalence of two terms in Equation 4.22:\nf2 = function(x, a) ifelse (a==0, sqrt(x), acosh(2*a*x + 1) / (2*sqrt(a)))  \nwith(df, max(abs(f2(x,a) - y)))__\n\n\n[1] 8.881784e-16\nAs we see in Figure 4.27, for small values of \\(x\\), \\(g_(x) \\) (independently of \\(\\)), whereas for large values (\\(x\\)) and \\(&gt;0\\), it behaves like a logarithm:\n\\[ \\[\\begin{align} &\\frac{1}{2\\sqrt{\\alpha}}\\ln\\left(2\\sqrt{\\alpha\\left(\\alpha\nx^2+x\\right)}+2\\alpha x+1\\right)\\\\\\ \\approx&\\frac{1}{2\\sqrt{\\alpha}}\n\\ln\\left(2\\sqrt{\\alpha^2x^2}+2\\alpha x\\right)\\\\\\\n=&\\frac{1}{2\\sqrt{\\alpha}}\\ln\\left(4\\alpha x\\right)\\\\\\\n=&\\frac{1}{2\\sqrt{\\alpha}}\\ln x+\\text{const.} \\end{align}\\]=&(4x)\\\n=&x+ \\end{align} \\]\nWe can verify this empirically by, say,\n  a = c(0.2, 0.5, 1)\n  f(1e6, a) __\n\n\n[1] 15.196731 10.259171  7.600903\n\n\n  1/(2*sqrt(a)) * (log(1e6) + log(4*a))__\n\n\n[1] 15.196728 10.259170  7.600902",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "04-chap.html#summary-of-this-chapter",
    "href": "04-chap.html#summary-of-this-chapter",
    "title": "6  4.1 Goals for this chapter",
    "section": "6.4 4.5 Summary of this chapter",
    "text": "6.4 4.5 Summary of this chapter\nWe have given motivating examples and ways of using mixtures to model biological data. We saw how the EM algorithm is an interesting example of fitting a difficult-to-estimate probabilistic model to data by iterating between partial, simpler problems.\n\n6.4.0.1 Finite mixture models\nWe have seen how to model mixtures of two or more normal distributions with different means and variances. We have seen how to decompose a given sample of data from such a mixture, even without knowing the latent variable, using the EM algorithm. The EM approach requires that we know the parametric form of the distributions and the number of components. In Chapter 5, we will see how we can find groupings in data even without relying on such information – this is then called clustering. We can keep in mind that there is a strong conceptual relationship between clustering and mixture modeling.\n\n\n6.4.0.2 Common infinite mixture models\nInfinite mixture models are good for constructing new distributions (such as the gamma-Poisson or the Laplace) out of more basic ones (such as binomial, normal, Poisson). Common examples are\n\nmixtures of normals (often with a hierarchical model on the means and the variances);\nbeta-binomial mixtures – where the probability \\(p\\) in the binomial is generated according to a \\((a, b)\\) distribution;\ngamma-Poisson for read counts (see Chapter 8);\ngamma-exponential for PCR.\n\n\n\n6.4.0.3 Applications\nMixture models are useful whenever there are several layers of experimental variability. For instance, at the lowest layer, our measurement precision may be limited by basic physical detection limits, and these may be modeled by a Poisson distribution in the case of a counting-based assay, or a normal distribution in the case of the continuous measurement. On top of there may be one (or more) layers of instrument-to-instrument variation, variation in the reagents, operator variaton etc.\nMixture models reflect that there is often heterogeneous amounts of variability (variances) in the data. In such cases, suitable data transformations, i.e., variance stabilizing transformations, are necessary before subsequent visualization or analysis. We’ll study in depth an example for RNA-Seq in Chapter 8, and this also proves useful in the normalization of next generation reads in microbial ecology (McMurdie and Holmes 2014).\nAnother important application of mixture modeling is the two-component model in multiple testing – we will come back to this in Chapter 6.\n\n\n6.4.0.4 The ECDF and bootstrapping\nWe saw that by using the observed sample as a mixture we could generate many simulated samples that inform us about the sampling distribution of an estimate. This method is called the bootstrap and we will return to it several times, as it provides a way of evaluating estimates even when a closed form expression is not available (we say it is non-parametric).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "04-chap.html#further-reading",
    "href": "04-chap.html#further-reading",
    "title": "6  4.1 Goals for this chapter",
    "section": "6.5 4.6 Further reading",
    "text": "6.5 4.6 Further reading\nA useful book-long treatment of finite mixture models is by McLachlan and Peel (2004); for the EM algorithm, see also the book by McLachlan and Krishnan (2007). A recent book that presents all EM type algorithms within the Majorize-Minimization (MM) framework is by Lange (2016).\nThere are in fact mathematical reasons why many natural phenomena can be seen as mixtures: this occurs when the observed events are exchangeable (the order in which they occur doesn’t matter). The theory underlying this is quite mathematical, a good way to start is to look at the Wikipedia entry and the paper by Diaconis and Freedman (1980).\nIn particular, we use mixtures for high-throughput data. You will see examples in Chapters 8 and 11.\nThe bootstrap can be used in many situations and is a very useful tool to know about, a friendly treatment is given in (B. Efron and Tibshirani 1993).\nA historically interesting paper is the original article on variance stabilization by Anscombe (1948), who proposed ways of making variance stabilizing transformations for Poisson and gamma-Poisson random variables. Variance stabilization is explained using the delta method in many standard texts in theoretical statistics, e.g., those by Rice (2006, chap. 6) and Kéry and Royle (2015, 35).\nKéry and Royle (2015) provide a nice exploration of using R to build hierarchical models for abundance estimation in niche and spatial ecology.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "04-chap.html#exercises",
    "href": "04-chap.html#exercises",
    "title": "6  4.1 Goals for this chapter",
    "section": "6.6 4.7 Exercises",
    "text": "6.6 4.7 Exercises\n__\nExercise 4.1\nThe EM algorithm step by step. As an example dataset, we use the values in the file Myst.rds. As always, it is a good idea to first visualize the data. The histogram is shown in Figure 4.28. We are going to model these data as a mixture of two normal distributions with unknown means and standard deviations, and unknown mixture fraction. We’ll call the two components A and B.\nmx = readRDS(\"../data/Myst.rds\")$yvar\nstr(mx)__\n\n\n num [1:1800] 0.3038 0.0596 -0.0204 0.1849 0.2842 ...\n\n\nggplot(tibble(mx), aes(x = mx)) + geom_histogram(binwidth = 0.025)__\nWe start by randomly assigning the membership weights for each of the values in mx for each of the components\nwA = runif(length(mx))\nwB = 1 - wA __\nWe also need to set up some housekeeping variables: iter counts over the iterations of the EM algorithm; loglik stores the current log-likelihood; delta stores the change in the log-likelihood from the previous iteration to the current one. We also define the parameters tolerance, miniter and maxiter of the algorithm.\niter      = 0\nloglik    = -Inf\ndelta     = +Inf\ntolerance = 1e-12\nminiter   = 50\nmaxiter   = 1000 __\nStudy the code below and answer the following questions:\n\nWhich lines correspond to the E-step, which to the M-step?\nWhat is the role of tolerance, miniter and maxiter?\nCompare the result of what we are doing here to the output of the normalmixEM function from the mixtools package.\n\nwhile((delta &gt; tolerance) && (iter &lt;= maxiter) || (iter &lt; miniter)) {\n  lambda = mean(wA)\n  muA = weighted.mean(mx, wA)\n  muB = weighted.mean(mx, wB)\n  sdA = sqrt(weighted.mean((mx - muA)^2, wA))\n  sdB = sqrt(weighted.mean((mx - muB)^2, wB))\n\n  pA   =    lambda    * dnorm(mx, mean = muA, sd = sdA)\n  pB   = (1 - lambda) * dnorm(mx, mean = muB, sd = sdB)\n  ptot = pA + pB\n  wA   = pA / ptot\n  wB   = pB / ptot\n\n  loglikOld = loglik\n  loglik = sum(log(pA + pB))\n  delta = abs(loglikOld - loglik)\n  iter = iter + 1\n}\niter __\n\n\n[1] 447\n\n\nc(lambda, muA, muB, sdA, sdB)__\n\n\n[1]  0.4756 -0.1694  0.1473  0.0983  0.1498\n\nFigure 4.28: Histogram of mx, our example data for the EM algorithm.\n__\nSolution\n__\nThe first five lines in the while loop implement the Maximization step. Given the current values of wA and wB, we estimate the parameters of the mixture model using the maximum-likelihood estimators: the mixture fraction lambda by the mean of wA, and the parameters of the two normal distribution components (muA, sdA) and (muB, sdB) by the sample means and the sample standard deviations. To take into account the membership weights, we use the weighted mean (function weighted.mean) and standard deviation.\nNext comes the Expectation step. For each of the elements in the data vector mx, we compute the probability densities pA and pB for the generative distribution models A and B, using the normal density function dnorm, weighted by the mixture fractions lambda and (1-lambda), respectively. From this, we compute the updated membership weights wA and wB, according to Equation 4.5.\nGiven the membership weights and the parameters, the logarithmic likelihood loglik is easily computed, and the while loop iterates these steps.\nThe termination criterion for the loop is based on delta, the change in the likelihood. The loop can end if this becomes smaller than tolerance. This is a simple way of checking whether the algorithm has converged. The additional conditions on iter make sure that at least miniter iterations are run, and that the loop always stops after maxiter iterations. The latter is to make sure that the loop terminates in finite time no matter what. (“Professional” implementations of such iterative algorithms typically work a bit harder to decide what is the best time to stop.)\nFinally, let’s compare our estimates to those from the function normalmixEM from the mixtools package.\ngm = mixtools::normalmixEM(mx, k = 2)__\n\n\nnumber of iterations= 215 \n\n\nwith(gm, c(lambda[1], mu, sigma))__\n\n\n[1]  0.4757 -0.1694  0.1473  0.0983  0.1498\n__\nExercise 4.2\nWhy do we often consider the logarithm of the likelihood rather than the likelihood? E.g., in the EM code above, why did we work with the probabilities on the logarithmic scale?\n__\nSolution\n__\nLikelihoods often (whenever the data points are sampled independently) take the form of a product. This is, for instance, the case in Equation 4.4. Calculating the derivative, for likelihood optimisation, would then require application of the product rule. On the logarithmic scale, the product turns into a sum, and the derivative of a sum is simply the sum of the derivatives of the individual summands.\nAn additional reason comes from the way computers implement arithmetic. They commonly use a floating point representation of numbers with a finite number of bits. E.g., the IEEE 754-2008 standard uses 64 bits for a double- precision number: 1 bit for the sign, 52 for the mantissa (also called significand), 11 for the exponent. Multiplication between such numbers implies addition of the exponents, but the range of the exponent is only \\(0\\) to \\(2^{11}-1=2047\\). Even likelihoods that involve only a few hundred data points can lead to arithmetic overflow or other problems with precision. On the logarithmic scale, where the product is a sum, the workload tends to be better distributed between mantissa and exponent, and log-likelihoods even with millions of data points to be handled with reasonable precision.\nSee also Gregory Gundersen’s post on the Log-Sum-Exp Trick for normalizing vectors of log probabilities.\n__\nExercise 4.3\nCompare the theoretical values of the gamma-Poisson distribution with parameters given by the estimates in ofit$par in Section 4.4.3 to the data used for the estimation using a QQ-plot.\n__\nExercise 4.4\nMixture modeling examples for regression. The flexmix package (Grün, Scharl, and Leisch 2012) enables us to cluster and fit regressions to the data at the same time. The standard M-step FLXMRglm of flexmix is an interface to R’s generalized linear modeling facilities (the glm function). Load the package and an example dataset.\nlibrary(\"flexmix\")\ndata(\"NPreg\")__\n\nFirst, plot the data and try to guess how the points were generated.\nFit a two component mixture model using the commands\n\nm1 = flexmix(yn ~ x + I(x^2), data = NPreg, k = 2)__\n\nLook at the estimated parameters of the mixture components and make a truth table that cross-classifies true classes versus cluster memberships. What does the summary of the object m1 show us?\nPlot the data again, this time coloring each point according to its estimated class.\n\n__\nSolution\n__\nggplot(NPreg, aes(x = x, y = yn)) + geom_point()__\n\nFigure 4.29: The points seem to come from two different generative processes, one is linear; the other quadratic.\nThe components are:\nmodeltools::parameters(m1, component = 1)__\n\n\n                      Comp.1\ncoef.(Intercept) -0.20998685\ncoef.x            4.81807854\ncoef.I(x^2)       0.03613061\nsigma             3.47665584\n\n\nmodeltools::parameters(m1, component = 2)__\n\n\n                     Comp.2\ncoef.(Intercept) 14.7167886\ncoef.x            9.8468507\ncoef.I(x^2)      -0.9683734\nsigma             3.4795657\nThe parameter estimates of both components are close to the true values. A cross-tabulation of true classes and cluster memberships can be obtained by\ntable(NPreg$class, modeltools::clusters(m1))__\n\n\n   \n     1  2\n  1 95  5\n  2  5 95\nFor our example data, the ratios of both components are approximately 0.7, indicating the overlap of the classes at the cross-section of line and parabola.\nsummary(m1)__\nThe summary shows the estimated prior probabilities \\(k\\), the number of observations assigned to the two clusters, the number of observations where \\(p{nk}&gt;\\) (with a default of \\(^{-4}\\)), and the ratio of the latter two numbers. For well- separated components, a large proportion of observations with non-vanishing posteriors \\(p_{nk}\\) should be assigned to their cluster, giving a ratio close to 1.\nNPreg = mutate(NPreg, gr = factor(class))\nggplot(NPreg, aes(x = x, y = yn, group = gr)) +\n   geom_point(aes(colour = gr, shape = gr)) +\n   scale_colour_hue(l = 40, c = 180)__\n\nFigure 4.30: Regression example using flexmix with the points colored according to their estimated class. You can see that at the intersection we have an `identifiability’ problem: we cannot distinguish points that belong to the straight line from ones that belong to the parabole.\n__\nExercise 4.5\nOther hierarchical noise models:\nFind two papers that explore the use of other infinite mixtures for modeling molecular biology technological variation.\n__\nSolution\n__\nThe paper by Chen, Xie, and Story (2011) explores an exponential-Poisson model for modeling background noise in bead arrays. Wills et al. (2013) compares several Poisson mixture models.\nAnscombe, Francis J. 1948. “The Transformation of Poisson, Binomial and Negative-Binomial Data.” Biometrika , 246–54.\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\nBronštein, Il’ja N., and Konstantin A Semendjajew. 1979. Taschenbuch Der Mathematik. B.G. Teubner Verlagsgesellschaft, Leipzig; Verlag Nauka, Moscow.\nBulmer, Michael George. 2003. Francis Galton: Pioneer of Heredity and Biometry. JHU Press.\nChen, Min, Yang Xie, and Michael Story. 2011. “An Exponential-Gamma Convolution Model for Background Correction of Illumina BeadArray Data.” Communications in Statistics-Theory and Methods 40 (17): 3055–69.\nDiaconis, Persi, and David Freedman. 1980. “Finite Exchangeable Sequences.” The Annals of Probability , 745–64.\nDiaconis, Persi, and Susan Holmes. 1994. “Gray Codes for Randomization Procedures.” Statistics and Computing 4 (4): 287–302.\nEfron, Bradley, and Robert J Tibshirani. 1994. An Introduction to the Bootstrap. CRC press.\nEfron, B., and R. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman & Hall/CRC.\nGrün, Bettina, Theresa Scharl, and Friedrich Leisch. 2012. “Modelling Time Course Gene Expression Data with Finite Mixtures of Linear Additive Models.” Bioinformatics 28 (2): 222–28. https://doi.org/10.1093/bioinformatics/btr653.\nHoeting, Jennifer A, David Madigan, Adrian E Raftery, and Chris T Volinsky. 1999. “Bayesian Model Averaging: A Tutorial.” Statistical Science , 382–401.\nKéry, Marc, and J Andrew Royle. 2015. Applied Hierarchical Modeling in Ecology: Analysis of Distribution, Abundance and Species Richness in r and BUGS: Volume 1: Prelude and Static Models. Academic Press.\nKristiansson, Erik, Michael Thorsen, Markus J Tamás, and Olle Nerman. 2009. “Evolutionary Forces Act on Promoter Length: Identification of Enriched Cis- Regulatory Elements.” Molecular Biology and Evolution 26 (6): 1299–1307.\nKuan, Pei Fen, Dongjun Chung, Guangjin Pan, James A Thomson, Ron Stewart, and Sündüz Keleş. 2011. “A Statistical Framework for the Analysis of ChIP-Seq Data.” Journal of the American Statistical Association 106 (495): 891–903.\nLange, Kenneth. 2016. MM Optimization Algorithms. SIAM.\nMcLachlan, Geoffrey, and Thriyambakam Krishnan. 2007. The EM Algorithm and Extensions. Vol. 382. John Wiley & Sons.\nMcLachlan, Geoffrey, and David Peel. 2004. Finite Mixture Models. John Wiley & Sons.\nMcMurdie, Paul J, and Susan Holmes. 2014. “Waste Not, Want Not: Why Rarefying Microbiome Data Is Inadmissible.” PLoS Computational Biology 10 (4): e1003531.\nPurdom, Elizabeth, and Susan P Holmes. 2005. “Error Distribution for Gene Expression Data.” Statistical Applications in Genetics and Molecular Biology 4 (1).\nRice, John. 2006. Mathematical Statistics and Data Analysis. Cengage Learning.\nShalizi, Cosma. 2017. Advanced Data Analysis from an Elementary Point of View. Cambridge University Press. https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf.\nSlonim, Noam, Gurinder Singh Atwal, Gašper Tkačik, and William Bialek. 2005. “Information-Based Clustering.” PNAS 102 (51): 18297–302.\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society. Series B (Methodological) , 267–88.\nWills, Quin F, Kenneth J Livak, Alex J Tipping, Tariq Enver, Andrew J Goldson, Darren W Sexton, and Chris Holmes. 2013. “Single-Cell Gene Expression Analysis Reveals Genetic Associations Masked in Whole-Tissue Experiments.” Nature Biotechnology 31 (8): 748–52.\nZeileis, Achim, Christian Kleiber, and Simon Jackman. 2008. “Regression Models for Count Data in R.” Journal of Statistical Software 27 (8). http://www.jstatsoft.org/v27/i08/.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "05-chap.html",
    "href": "05-chap.html",
    "title": "7  5.1 Goals for this chapter",
    "section": "",
    "text": "7.1 5.2 What are the data and why do we cluster them?\nFinding categories of cells, illnesses, organisms and then naming them is a core activity in the natural sciences. In Chapter 4 we’ve seen that some data can be modeled as mixtures from different groups or populations with a clear parametric generative model. We saw how in those examples we could use the EM algorithm to disentangle the components. We are going to extend the idea of unraveling of different groups to cases where the clusters do not necessarily have nice elliptic1 shapes.\n1 Mixture modeling with multivariate normal distributions implies elliptic cluster boundaries.\nClustering takes data (continuous or quasi-continuous) and adds to them a new categorical group variable that can often simplify decision making; even if this sometimes comes at a cost of ignoring intermediate states. For instance, medical decisions are simplified by replacing possibly complex, high-dimensional diagnostic measurements by simple groupings: a full report of numbers associated with fasting glucose, glycated hemoglobin and plasma glucose two hours after intake is replaced by assigning the patient to a diabetes mellitus “group”.\nIn this chapter, we will study how to find meaningful clusters or groups in both low-dimensional and high-dimensional nonparametric settings. However, there is a caveat: clustering algorithms are designed to find clusters, so they will find clusters, even where there are none2. So, cluster validation is an essential component of our process, especially if there is no prior domain knowledge that supports the existence of clusters.\n2 This is reminescent of humans: we like to see patterns—even in randomness.\nIn this chapter we will\nFigure 5.1: John Snow’s map of cholera cases: small barcharts at each house indicate a clustering of diagnosed cases.\nDavid Freedman has a wonderful detailed account of all the steps that led to this discovery (Freedman 1991).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "05-chap.html#what-are-the-data-and-why-do-we-cluster-them",
    "href": "05-chap.html#what-are-the-data-and-why-do-we-cluster-them",
    "title": "7  5.1 Goals for this chapter",
    "section": "",
    "text": "7.1.1 5.2.1 Clustering can sometimes lead to discoveries.\nJohn Snow made a map of cholera cases and identified clusters of cases. He then collected additional information about the situation of the pumps. The proximity of dense clusters of cases to the Broadstreet pump pointed to the water as a possible culprit. He collected separate sources of information that enabled him to infer the source of the cholera outbreak.\nNow, let’s look at another map of London, shown in Figure 5.2. The red dots designate locations that were bombed during World War II. Many theories were put forward during the war by the analytical teams. They attempted to find a rational explanation for the bombing patterns (proximity to utility plants, arsenals, \\(…\\)). In fact, after the war it was revealed that the bombings were randomly distributed without any attempt at hitting particular targets.\n\nFigure 5.2: Here is a map of the location of the bombs that were dropped on London on September, 7th, 1940 as depicted by the website of the British National Archives http://bombsight.org.\nClustering is a useful technique for understanding complex multivariate data; it is an unsupervised 3. Exploratory techniques show groupings that can be important in interpreting the data.\n3 Thus named because all variables have the same status, we are not trying to predict or learn the value of one variable (the supervisory response) based on the information from explanatory variables.\nFor instance, clustering has enabled researchers to enhance their understanding of cancer biology. Tumors that appeared to be the same based on their anatomical location and histopathology fell into multiple clusters based on their molecular signatures, such as gene expression data (Hallett et al. 2012). Eventually, such clusterings might lead to the definition of new, more relevant disease types. Relevance is evidenced, e.g., by the fact that they are associated with different patient outcomes. What we aim to do in this chapter is understand how pictures like Figure 5.3 are constructed and how to interpret them.\n\nFigure 5.3: The breast cancer samples (shown from The Cancer Genome Atlas (TCGA) and the Molecular Taxonomy of Breast Cancer International Consortium (METABRIC)) can be split into groups using their miRNA expression (Aure et al. 2017). The authors show in the lower plots that the survival times in different % groups were different. Thus these clusters were biologically and clinically relevant. The promise of such analyses is that the groups can be used to provide more specific, optimized treatments.\nIn Chapter 4, we have already studied one technique, the EM algorithm, for uncovering groups. The techniques we explore in this chapter are more general and can be applied to more complex data. Many of them are based on distances between pairs of observations (this can be all versus all, or sometimes only all versus some), and they make no explicit assumptions about the generative mechanism of the data involving particular families of distributions, such as normal, gamma-Poisson, etc. There is a proliferation of clustering algorithms in the literature and in the scientific software landscape; this can be intimidating. In fact it is linked to the diversity of the types of data and the objectives pursued in different domains.\n__\nTask\nLook up the BiocViews Clustering or the Cluster view on CRAN and count the number of packages providing clustering tools.\n\nFigure 5.4: We decompose the choices made in a clustering algorithm according to the steps taken: starting from an observations-by-features rectangular table \\(X\\), we choose an observations-to-observations distance measure and compute the distance matrix, here schematized by the triangle. The distances are used to construct the clusters. On the left, we schematize agglomerative methods, that build a hierarchical clustering tree; on the right, partitioning methods that separate the data into subsets. Both types of methods require a choice to be made: the number \\(k\\) of clusters. For partitionning approaches such as \\(k\\)-means this choice has to be made at the outset; for hierarchical clustering this can be deferred to the end of the analysis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "05-chap.html#how-do-we-measure-similarity",
    "href": "05-chap.html#how-do-we-measure-similarity",
    "title": "7  5.1 Goals for this chapter",
    "section": "7.2 5.3 How do we measure similarity?",
    "text": "7.2 5.3 How do we measure similarity?\n\n\n\nOf a feather: how the distances are measured and similarities between observations defined has a strong impact on the clustering result.\n\n\nOf a feather : how the distances are measured and similarities between observations defined has a strong impact on the clustering result.\nOur first step is to decide what we mean by similar. There are multiple ways of comparing birds: for instance, a distance using size and weight will give a different clustering than one using diet or habitat. Once we have chosen the relevant features, we have to decide how we combine differences between the multiple features into a single number. Here is a selection of choices, some of them are illustrated in Figure 5.5.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.5: Equal-distance contour plots according to four different distances: points on any one curve are all the same distance from the center point.\nEuclidean The Euclidean distance between two points \\(A=(a_1,…,a_p)\\) and \\(B= (b_1,…,b_p)\\) in a \\(p\\)-dimensional space (for the \\(p\\) features) is the square root of the sum of squares of the differences in all \\(p\\) coordinate directions:\n\\[ d(A,B)=. \\]\nManhattan The Manhattan, City Block, Taxicab or \\(L_1\\) distance takes the sum of the absolute differences in all coordinates.\n\\[ d(A,B)=|a_1-b_1|+|a_2-b_2|+… +|a_p-b_p|. \\]\nMaximum The maximum of the absolute differences between coordinates is also called the \\(L_\\) distance:\n\\[ d_(A,B)= _{i}|a_i-b_i|. \\]\nWeighted Euclidean distance is a generalization of the ordinary Euclidean distance, by giving different directions in feature space different weights. We have already encountered one example of a weighted Euclidean distance in Chapter 2, the \\(^2\\) distance. It is used to compare rows in contingency tables, and the weight of each feature is the inverse of the expected value. The Mahalanobis distance is another weighted Euclidean distance that takes into account the fact that different features may have a different dynamic range, and that some features may be positively or negatively correlated with each other. The weights in this case are derived from the covariance matrix of the features. See also Question 5.1.\nMinkowski Allowing the exponent to be \\(m\\) instead of \\(2\\), as in the Euclidean distance, gives the Minkowski distance\n\\[ d(A,B) = ( (a_1-b_1)m+(a_2-b_2)m+… +(a_p-b_p)^m )^. \\]\nEdit, Hamming This distance is the simplest way to compare character sequences. It simply counts the number of differences between two character strings. This could be applied to nucleotide or amino acid sequences – although in that case, the different character substitutions are usually associated with different contributions to the distance (to account for physical or evolutionary similarity), and deletions and insertions may also be allowed.\nBinary When the two vectors have binary bits as coordinates, we can think of the non-zero elements as ‘on’ and the zero elements as ‘off’. The binary distance is the proportion of features having only one bit on amongst those features that have at least one bit on.\nJaccard Distance Occurrence of traits or features in ecological or mutation data can be translated into presence and absence and encoded as 1’s and 0’s. In such situations, co-occurence is often more informative than co- absence. For instance, when comparing mutation patterns in HIV, the co- existence in two different strains of a mutation tends to be a more important observation than its co-absence. For this reason, biologists use the Jaccard index. Let’s call our two observation vectors \\(S\\) and \\(T\\), \\(f_{11}\\) the number of times a feature co-occurs in \\(S\\) and \\(T\\), \\(f_{10}\\) (and \\(f_{01}\\)) the number of times a feature occurs in \\(S\\) but not in \\(T\\) (and vice versa), and \\(f_{00}\\) the number of times a feature is co-absent. The Jaccard index is\n\\[ J(S,T) = , \\]\n(i.e., it ignores \\(f_{00}\\)), and the Jaccard dissimilarity is\n\\[ d_J(S,T) = 1-J(S,T) = . \\]\nCorrelation based distance\n\\[ d(A,B)=. \\]\n\nFigure 5.6: An example for the use of Mahalanobis distances to measure the distance of a new data point (red) from two cluster centers.\n__\nQuestion 5.1\nWhich of the two cluster centers in Figure 5.6 is the red point closest to?\n__\nSolution\n__\nA naïve answer would use the Euclidean metric and decide that the point is closer to the left cluster. However, as we see that the features have different ranges and correlations, and that these even differ between the two clusters, it makes sense to use cluster-specific Mahalanobis distances. The figure shows contour lines for both clusters. These were obtained from a density estimate; the Mahalanobis distance approximates these contours with ellipses. The distance between the red point and each of the cluster centers corresponds to the number of contour lines crossed. We see that as the group on the right is more spread out, the red point is in fact closer to it.\n\nFigure 5.7: The lower triangle of distances can be computed by any of a hundred different functions in various R packages (vegdist in vegan , daisy in cluster , genetic_distance in gstudio , dist.dna in ape , Dist in amap , distance in ecodist , dist.multiPhylo in distory , shortestPath in gdistance , % dudi.dist and dist.genet in ade4).\n\n7.2.1 5.3.1 Computations related to distances in R\nThe dist function in R is designed to use less space than the full \\(n^2\\) positions a complete \\(n n\\) distance matrix between \\(n\\) objects would require. The function computes one of six choices of distance (euclidean, maximum, manhattan, canberra, binary, minkowski) and outputs a vector of values sufficient to reconstruct the complete distance matrix. The function returns a special object of class dist that encodes the relevant vector of size \\(n(n-1)/2\\). Here is the output for a \\(3\\) by \\(3\\) matrix:\nmx  = c(0, 0, 0, 1, 1, 1)\nmy  = c(1, 0, 1, 1, 0, 1)\nmz  = c(1, 1, 1, 0, 1, 1)\nmat = rbind(mx, my, mz)\ndist(mat)__\n\n\n         mx       my\nmy 1.732051         \nmz 2.000000 1.732051\n\n\ndist(mat, method = \"binary\")__\n\n\n          mx        my\nmy 0.6000000          \nmz 0.6666667 0.5000000\nIn order to access a particular distance (for example the distance between observations 1 and 2), one has to turn the dist class object back into a matrix.\nload(\"../data/Morder.RData\")\nsqrt(sum((Morder[1, ] - Morder[2, ])^2))__\n\n\n[1] 5.593667\n\n\nas.matrix(dist(Morder))[2, 1]__\n\n\n[1] 5.593667\nLet’s look at how we would compute the Jaccard distance we defined above between HIV strains.\nmut = read.csv(\"../data/HIVmutations.csv\")\nmut[1:3, 10:16]__\n\n\n  p32I p33F p34Q p35G p43T p46I p46L\n1    0    1    0    0    0    0    0\n2    0    1    0    0    0    1    0\n3    0    1    0    0    0    0    0\n__\nQuestion 5.2\nCompare the Jaccard distance (available as the function vegdist in the R package vegan) between mutations in the HIV data mut to the correlation based distance.\n__\nSolution\n__\nlibrary(\"vegan\")\nmutJ = vegdist(mut, \"jaccard\")\nmutC = sqrt(2 * (1 - cor(t(mut))))\nmutJ __\n\n\n      1     2     3     4\n2 0.800                  \n3 0.750 0.889            \n4 0.900 0.778 0.846      \n5 1.000 0.800 0.889 0.900\n\n\nas.dist(mutC)__\n\n\n     1    2    3    4\n2 1.19               \n3 1.10 1.30          \n4 1.32 1.13 1.30     \n5 1.45 1.19 1.30 1.32\n\nFigure 5.8: An example of computing the cophenetic distance (xkcd).\nIt can also be interesting to compare complex objects that are not traditional vectors or real numbers using dissimilarities or distances. Gower’s distance for data of mixed modalities (both categorical factors and continuous variables) can be computed with the daisy function. In fact, distances can be defined between any pairs of objects, not just points in \\({ R}^p\\) or character sequences. For instance, the shortest.paths function from the igraph package that we will see in Chapter 10 computes the distance between vertices on a graph and the function cophenetic computes the distance between leaves of a tree as illustrated in Figure 5.8. We can compute the distance between trees using dist.multiPhylo in the distory package.\nThe Jaccard index between graphs can be computed by looking at two graphs built on the same nodes and counting the number of co-occurring edges. This is implemented in the function similarity in the igraph package. Distances and dissimilarities are also used to compare images, sounds, maps and documents. A distance can usefully encompass domain knowledge and, if carefully chosen, can lead to the solution of many hard problems involving heterogeneous data. Asking yourself what is the relevant notion of “closeness” or similarity for your data can provide useful ways of representing them, as we will explore in Chapter 9.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "05-chap.html#nonparametric-mixture-detection",
    "href": "05-chap.html#nonparametric-mixture-detection",
    "title": "7  5.1 Goals for this chapter",
    "section": "7.3 5.4 Nonparametric mixture detection",
    "text": "7.3 5.4 Nonparametric mixture detection\n\n7.3.1 5.4.1 \\(k\\)-methods: \\(k\\)-means, \\(k\\)-medoids and PAM\n\n\n\nThe centers of the groups are sometimes called medoids, thus the name PAM (partitioning around medoids).\n\n\nThe centers of the groups are sometimes called medoids, thus the name PAM (partitioning around medoids).\nPartitioning or iterative relocation methods work well in high-dimensional settings, where we cannot4 easily use probability densities, the EM algorithm and parametric mixture modeling in the way we did in Chapter 4. Besides the distance measure, the main choice to be made is the number of clusters \\(k\\). The PAM (partitioning around medoids, Kaufman and Rousseeuw (2009)) method is as follows:\n4 This is due to the so-called curse of dimensionality. We will discuss this in more detail in Chapter 12.\n\nStarts from a matrix of \\(p\\) features measured on a set of \\(n\\) observations.\nRandomly pick \\(k\\) distinct cluster centers out of the \\(n\\) observations (“seeds”).\nAssign each of the remaining observation to the group to whose center it is the closest.\nFor each group, choose a new center from the observations in the group, such that the sum of the distances of group members to the center is minimal; this is called the medoid.\nRepeat Steps 3 and 4 until the groups stabilize.\n\nEach time the algorithm is run, different initial seeds will be picked in Step 2, and in general, this can lead to different final results. A popular implementation is the pam function in the package cluster.\nA slight variation of the method replaces the medoids by the arithmetic means (centers of gravity) of the clusters and is called \\(k\\)-means. While in PAM, the centers are observations, this is not, in general, the case with \\(k\\)-means. The function kmeans comes with every installation of R in the stats package; an example run is shown in Figure 5.9.\n[](05-chap_files/figure-html/fig-clust-kmeansastep1-1.png “Figure 5.9 (a):”)\n\n\n\n[](05-chap_files/figure-html/fig-clust-kmeansastep1-2.png “Figure 5.9 (b):”)\n\n\n\n[](05-chap_files/figure-html/fig-clust-kmeansastep1-3.png “Figure 5.9 (c):”)\n\n\n\nFigure 5.9: An example run of the \\(k\\)-means algorithm. The initial, randomly chosen centers (black circles) and groups (colors) are shown in (a). The group memberships are assigned based on their distance to centers. At each iteration (b) and (c), the group centers are redefined, and the points reassigned to the cluster centers.\nThese so-called \\(k\\)-methods are the most common off-the-shelf methods for clustering; they work particularly well when the clusters are of comparable size and convex (blob-shaped). On the other hand, if the true clusters are very different in size, the larger ones will tend to be broken up; the same is true for groups that have pronounced non-spherical or non-elliptic shapes.\n__\nQuestion 5.3\nThe \\(k\\)-means algorithm alternates between computing the average point and assigning the points to clusters. How does this alternating, iterative method differ from an EM-algorithm?\n__\nSolution\n__\nIn the EM algorithm, each point participates in the computation of the mean of all the groups through a probabilistic weight assigned to it. In the \\(k\\)-means method, the points are either attributed to a cluster or not, so each point participates only, and entirely, in the computation of the center of one cluster.\n\n\n7.3.2 5.4.2 Tight clusters with resampling\nThere are clever schemes that repeat the process many times using different initial centers or resampled datasets. Repeating a clustering procedure multiple times on the same data, but with different starting points creates strong forms according to Diday and Brito (1989). Repeated subsampling of the dataset and applying a clustering method will result in groups of observations that are “almost always” grouped together; these are called tight clusters (Tseng and Wong 2005). The study of strong forms or tight clusters facilitates the choice of the number of clusters. A recent package developed to combine and compare the output from many different clusterings is clusterExperiment. Here we give an example from its vignette. Single-cell RNA-Seq experiments provide counts of reads, representing gene transcripts, from individual cells. The single cell resolution enables scientists, among other things, to follow cell lineage dynamics. Clustering has proved very useful for analysing such data.\n__\nQuestion 5.4\nFollow the vignette of the package clusterExperiment. Call the ensemble clustering function clusterMany, using pam for the individual clustering efforts. Set the choice of genes to include at either the 60, 100 or 150 most variable genes. Plot the clustering results for \\(k\\) varying between 4 and 9. What do you notice?\n__\nSolution\n__\nThe following code produces Figure 5.10.\nlibrary(\"clusterExperiment\")\nfluidigm = scRNAseq::ReprocessedFluidigmData()\nse = fluidigm[, fluidigm$Coverage_Type == \"High\"]\nassays(se) = list(normalized_counts = as.matrix(limma::normalizeQuantiles(assay(se))))\nce = clusterMany(se, clusterFunction = \"pam\", ks = c(5, 7, 9), run = TRUE,\n                 isCount = TRUE, reduceMethod = \"var\", nFilterDims = c(60, 100, 150))__\n\n\n9 parameter combinations, 0 use sequential method, 0 use subsampling method\nRunning Clustering on Parameter Combinations...\ndone.\n\n\nclusterLabels(ce) = sub(\"FilterDims\", \"\", clusterLabels(ce))\nplotClusters(ce, whichClusters = \"workflow\", axisLine = -1)__\n\nFigure 5.10: Comparison of clustering results (rows), for different numbers of included genes and for varying numbers of clusters, \\(k\\). Each column of the heatmap corresponds to a cell, and the colors represent the cluster assignments.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "05-chap.html#clustering-examples-flow-cytometry-and-mass-cytometry",
    "href": "05-chap.html#clustering-examples-flow-cytometry-and-mass-cytometry",
    "title": "7  5.1 Goals for this chapter",
    "section": "7.4 5.5 Clustering examples: flow cytometry and mass cytometry",
    "text": "7.4 5.5 Clustering examples: flow cytometry and mass cytometry\n\n\n\nYou can find reviews of bioinformatics methods for flow cytometry in (O’Neill et al. 2013) and a well-kept wikipedia article.\n\n\nYou can find reviews of bioinformatics methods for flow cytometry in (O’Neill et al. 2013) and a well-kept wikipedia article.\nStudying measurements on single cells improves both the focus and resolution with which we can analyze cell types and dynamics. Flow cytometry enables the simultaneous measurement of about 10 different cell markers. Mass cytometry expands the collection of measurements to as many as 80 proteins per cell. A particularly promising application of this technology is the study of immune cell dynamics.\n\n7.4.1 5.5.1 Flow cytometry and mass cytometry\nAt different stages of their development, immune cells express unique combinations of proteins on their surfaces. These protein-markers are called CD s (clusters of differentiation) and are collected by flow cytometry (using fluorescence, see Hulett et al. (1969)) or mass cytometry (using single-cell atomic mass spectrometry of heavy element reporters, see Bendall et al. (2012)). An example of a commonly used CD is CD4, this protein is expressed by helper T cells that are referred to as being “CD4+”. Note however that some cells express CD4 (thus are CD4+), but are not actually helper T cells. We start by loading some useful Bioconductor packages for cytometry data, flowCore and flowViz , and read in an examplary data object fcsB as follows:\nlibrary(\"flowCore\")\nlibrary(\"flowViz\")\nfcsB = read.FCS(\"../data/Bendall_2011.fcs\", truncate_max_range = FALSE)\nslotNames(fcsB)__\n\n\n[1] \"exprs\"       \"parameters\"  \"description\"\nFigure 5.11 shows a scatterplot of two of the variables available in the fcsB data. (We will see how to make such plots below.) We can see clear bimodality and clustering in these two dimensions.\n__\nQuestion 5.5\n\nLook at the structure of the fcsB object (hint: the colnames function). How many variables were measured?\nSubset the data to look at the first few rows (hint: use Biobase::exprs(fcsB)). How many cells were measured?\n\n\n\n7.4.2 5.5.2 Data preprocessing\nFirst we load the table data that reports the mapping between isotopes and markers (antibodies), and then we replace the isotope names in the column names of fcsB with the marker names. This makes the subsequent analysis and plotting code more readable:\nmarkersB = readr::read_csv(\"../data/Bendall_2011_markers.csv\")\nmt = match(markersB$isotope, colnames(fcsB))\nstopifnot(!any(is.na(mt)))\ncolnames(fcsB)[mt] = markersB$marker __\nNow we are ready to generate Figure 5.11\nflowPlot(fcsB, plotParameters = colnames(fcsB)[2:3], logy = TRUE)__\n\nFigure 5.11: Cell measurements that show clear clustering in two dimensions.\nPlotting the data in two dimensions as in Figure 5.11 already shows that the cells can be grouped into subpopulations. Sometimes just one of the markers can be used to define populations on their own; in that case simple rectangular gating is used to separate the populations; for instance, CD4+ cells can be gated by taking the subpopulation with high values for the CD4 marker. Cell clustering can be improved by carefully choosing transformations of the data. The left part of Figure 5.12 shows a simple one dimensional histogram before transformation; on the right of Figure 5.12 we see the distribution after transformation. It reveals a bimodality and the existence of two cell populations.\nData Transformation: hyperbolic arcsin (asinh). It is standard to transform both flow and mass cytometry data using one of several special functions. We take the example of the inverse hyperbolic sine (asinh):\n\\[ (x) = . \\]\nFrom this we can see that for large values of \\(x\\), \\((x)\\) behaves like the logarithm and is practically equal to \\((x)+(2)\\); for small \\(x\\) the function is close to linear in \\(x\\).\n__\nTask\nTry running the following code to see the two main regimes of the transformation: small values and large values.\nv1 = seq(0, 1, length.out = 100)\nplot(log(v1), asinh(v1), type = 'l')__\n\n\n plot(v1, asinh(v1), type = 'l')__\n\n\nv3 = seq(30, 3000, length = 100)\nplot(log(v3), asinh(v3), type= 'l')__\nThis is another example of a variance stabilizing transformation, also mentioned in Chapters 4 and 8. Figure 5.12 is produced by the following code, which uses the arcsinhTransform function in the flowCore package.\nasinhtrsf = arcsinhTransform(a = 0.1, b = 1)\nfcsBT = transform(fcsB, transformList(colnames(fcsB)[-c(1, 2, 41)], asinhtrsf))\ndensityplot(~`CD3all`, fcsB)\ndensityplot(~`CD3all`, fcsBT)__\n\n\n\n\n\n\n\n\nFigure 5.12: Panel (a) shows the histogram of the CD3all variable: the cells are clustered around 0 with a few large values. In (b), we see that after an asinh transformation, the cells cluster and fall into two groups or types.\n__\nQuestion 5.6\nHow many dimensions does the following code use to split the data into 2 groups using \\(k\\)-means ?\nkf = kmeansFilter(\"CD3all\" = c(\"Pop1\",\"Pop2\"), filterId=\"myKmFilter\")\nfres = flowCore::filter(fcsBT, kf)\nsummary(fres)__\n\n\nPop1: 33434 of 91392 events (36.58%)\nPop2: 57958 of 91392 events (63.42%)\n\n\nfcsBT1 = flowCore::split(fcsBT, fres, population = \"Pop1\")\nfcsBT2 = flowCore::split(fcsBT, fres, population = \"Pop2\")__\nFigure 5.13, generated by the following code, shows a naïve projection of the data into the two dimensions spanned by the CD3 and CD56 markers:\nlibrary(\"flowPeaks\")\nfp = flowPeaks(Biobase::exprs(fcsBT)[, c(\"CD3all\", \"CD56\")])\nplot(fp)__\n\nFigure 5.13: After transformation these cells were clustered using kmeans.\nWhen plotting points that densely populate an area we should try to avoid overplotting. We saw some of the preferred techniques in Chapter 3; here we use contours and shading. This is done as follows:\nflowPlot(fcsBT, plotParameters = c(\"CD3all\", \"CD56\"), logy = FALSE)\ncontour(fcsBT[, c(40, 19)], add = TRUE)__\n\nFigure 5.14: Like Figure 5.13, using contours.\nThis produces Figure 5.14—a more informative version of Figure 5.13.\n__\nTask\nThe Bioconductor package ggcyto enables the plotting of each patient’s data in a different facet using ggplot. Try comparing the output using this approach to what we did above, along the following lines:\nlibrary(\"ggcyto\")\nlibrary(\"labeling\")\n\np1 = ggcyto(fcsB, aes(x = CD4)) + geom_histogram(bins = 60)\np2 = ggcyto(fcsB, aes(x = CD8)) + geom_histogram(bins = 60)\np3 = ggcyto(fcsB, aes(x = CD4, y = CD8)) + geom_density2d(colour = \"black\")\n\nfcsBT = transform(fcsB, transformList(colnames(fcsB)[-c(1, 2, 41)], \n                                      arcsinhTransform(a = 0, b = 1)))\n                                      \np1t = ggcyto(fcsBT, aes(x = CD4))            + geom_histogram(bins = 90)\np2t = ggcyto(fcsBT, aes(x = CD4,y = CD8))    + geom_density2d(colour = \"black\")\np3t = ggcyto(fcsBT, aes(x = CD45RA,y = CD20))+ geom_density2d(colour = \"black\")__\n\n\n7.4.3 5.5.3 Density-based clustering\nData sets such as flow cytometry, that contain only a few markers and a large number of cells, are amenable to density-based clustering. This method looks for regions of high density separated by sparser regions. It has the advantage of being able to cope with clusters that are not necessarily convex. One implementation of such a method is called dbscan. Let’s look at an example by running the following code.\nlibrary(\"dbscan\")\nmc5 = Biobase::exprs(fcsBT)[, c(15,16,19,40,33)]\nres5 = dbscan::dbscan(mc5, eps = 0.65, minPts = 30)\nmc5df = data.frame(mc5, cluster = as.factor(res5$cluster))\ntable(mc5df$cluster)__\n\n\n    0     1     2     3     4     5     6     7     8 \n76053  4031  5450  5310   257   160    63    25    43 \n\n\nggplot(mc5df, aes(x=CD4,    y=CD8,  col=cluster))+geom_density2d()\nggplot(mc5df, aes(x=CD3all, y=CD20, col=cluster))+geom_density2d()__\n\n\n\n\n\n\n\n\nFigure 5.15: These two plots show the results of clustering with dbscan using five markers. Here we only show the projections of the data into the CD4-CD8 and C3all-CD20 planes.\nThe output is shown in Figure 5.15. The overlaps of the clusters in the 2D projections enable us to appreciate the multidimensional nature of the clustering.\n__\nQuestion 5.7\nTry increasing the dimension to 6 by adding one CD marker-variables from the input data.\nThen vary eps, and try to find four clusters such that at least two of them have more than 100 points.\nRepeat this will 7 CD marker-variables, what do you notice?\n__\nSolution\n__\nAn example with the following 6 markers\nmc6 = Biobase::exprs(fcsBT)[, c(15, 16, 19, 33, 25, 40)]\nres = dbscan::dbscan(mc6, eps = 0.65, minPts = 20)\nmc6df = data.frame(mc6, cluster = as.factor(res$cluster))\ntable(mc6df$cluster)__\n\n\n    0     1     2     3     4     5     6 \n91068    34    61    20    67   121    21 \nWe see that with eps= 0.75 it is easier to find large enough clusters than if we take eps=0.65, with eps=0.55 it is impossible. As we increase the dimensionality to 7, we have to make eps even larger.\nmc7 = Biobase::exprs(fcsBT)[, c(11, 15, 16, 19, 25, 33, 40)]\nres = dbscan::dbscan(mc7, eps = 0.95, minPts = 20)\nmc7df = data.frame(mc7, cluster = as.factor(res$cluster))\ntable(mc7df$cluster)__\n\n\n    0     1     2     3     4     5     6     7     8     9    10 \n90249    21   102   445   158   119    19   224    17    20    18 \nThis shows us the so-called curse of dimensionality in action, of which more in Chapter 12.\n\n7.4.3.1 How does density-based clustering (dbscan) work ?\nThe dbscan method clusters points in dense regions according to the density- connectedness criterion. It looks at small neighborhood spheres of radius \\(\\) to see if points are connected.\nThe building block of dbscan is the concept of density-reachability: a point \\(q\\) is directly density-reachable from a point \\(p\\) if it is not further away than a given threshold \\(\\), and if \\(p\\) is surrounded by sufficiently many points such that one may consider \\(p\\) (and \\(q\\)) be part of a dense region. We say that \\(q\\) is density-reachable from \\(p\\) if there is a sequence of points \\(p_1,…,p_n\\) with \\(p_1 = p\\) and \\(p_n = q\\), so that each \\(p_{i + 1}\\) is directly density- reachable from \\(p_i\\).\n\n\n\nIt is important that the method looks for high density of points in a neighborhood. Other methods exist that try to define clusters by a void, or “missing points” between clusters. But these are vulnerable to the curse of dimensionality; these can create spurious “voids”.\n\n\nIt is important that the method looks for high density of points in a neighborhood. Other methods exist that try to define clusters by a void, or “missing points” between clusters. But these are vulnerable to the curse of dimensionality; these can create spurious “voids”.\nA cluster is then a subset of points that satisfy the following properties:\n\nAll points within the cluster are mutually density-connected.\nIf a point is density-connected to any point of the cluster, it is part of the cluster as well.\nGroups of points must have at least MinPts points to count as a cluster.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "05-chap.html#hierarchical-clustering",
    "href": "05-chap.html#hierarchical-clustering",
    "title": "7  5.1 Goals for this chapter",
    "section": "7.5 5.6 Hierarchical clustering",
    "text": "7.5 5.6 Hierarchical clustering\n\nFigure 5.16: A snippet of Linnus’ taxonomy that clusters organisms according to feature similarities.\nHierarchical clustering is a bottom-up approach, where similar observations and subclasses are assembled iteratively. Figure 5.16 shows how Linnæus made nested clusters of organisms according to specific characteristics. Such hierarchical organization has been useful in many fields and goes back to Aristotle who postulated a ladder of nature.\nDendrogram ordering. As you can see in the example of Figure 5.17, the order of the labels does not matter within sibling pairs. Horizontal distances are usually meaningless, while the vertical distances do encode some information. These properties are important to remember when making interpretations about neighbors that are not monophyletic (i.e., not in the same subtree or clade), but appear as neighbors in the plot (for instance B and D in the right hand tree are non-monophyletic neighbors).\n\nFigure 5.17: Three representations of the same hierarchical clustering tree.\nTop-down hierarchies. An alternative, top-down, approach takes all the objects and splits them sequentially according to a chosen criterion. Such so- called recursive partitioning methods are often used to make decision trees. They can be useful for prediction (say, survival time, given a medical diagnosis): we are hoping in those instances to split heterogeneous populations into more homogeneous subgroups by partitioning. In this chapter, we concentrate on the bottom-up approaches. We will return to partitioning when we talk about supervised learning and classification in Chapter 12.\n\n7.5.1 5.6.1 How to compute (dis)similarities between aggregated clusters?\n\nFigure 5.18: In the single linkage method, the distance between groups \\(C_1\\) and \\(C_2\\) is defined as the distance between the closest two points from the groups.\nA hierarchical clustering algorithm, which works by aggregation, is easy enough to get started, by grouping the most similar observations together. But we will need more than just the distances between all pairs of individual objects. Once an aggregation is made, one is required to say how the distance between the newly formed cluster and all other points (or existing clusters) is computed. There are different choices, all based on the object-object distances, and each choice results in a different type of hierarchical clustering.\nThe minimal jump method, also called single linkage or nearest neighbor method computes the distance between clusters as the smallest distance between any two points in the two clusters (as shown in Figure 5.18):\n\\[ d_{12} = {i C_1, j C_2 } d{ij}. \\]\nThis method tends to create clusters that look like contiguous strings of points. The cluster tree often looks like a comb.\n\nFigure 5.19: In the complete linkage method, the distance between groups \\(C_1\\) and \\(C_2\\) is defined as the maximum distance between pairs of points from the two groups.\nThe maximum jump (or complete linkage) method defines the distance between clusters as the largest distance between any two objects in the two clusters, as represented in Figure 5.19:\n\\[ d_{12} = {i C_1, j C_2 } d{ij}. \\]\nThe average linkage method is half way between the two above (here, \\(|C_k|\\) denotes the number of elements of cluster \\(k\\)):\n\\[ d_{12} = {i C_1, j C_2 } d{ij} \\]\n\nFigure 5.20: The Ward method maximizes the between group sum of squares (red edges), while minimizing the sums of squares within groups (black edges).\nWard’s method takes an analysis of variance approach, where the goal is to minimize the variance within clusters. This method is very efficient, however, it tends to create clusters of smaller sizes.\n\n\n\nAdvantages and disadvantages of various choices of defining distances between aggregates (Chakerian and Holmes 2012). Method\nPros\nCons\n\n\n\n\nSingle linkage\nnumber of clusters\ncomblike trees\n\n\nComplete linkage\ncompact classes\none observation can alter groups\n\n\nAverage linkage\nsimilar size and variance\nnot robust\n\n\nCentroid\nrobust to outliers\nsmaller number of clusters\n\n\nWard\nminimising an inertia\nclasses small if high variability\n\n\n\n\nFigure 5.21: Hierarchical clustering output has similar properties to a mobile: the branches can rotate freely from their suspension points.\nThese are the choices we have to make building hierarchical clustering trees. An advantage of hierarchical clustering compared to the partitioning methods is that it offers a graphical diagnostic of the strength of groupings: the length of the inner edges in the tree.\nWhen we have prior knowledge that the clusters are about the same size, using average linkage or Ward’s method of minimizing the within class variance is the best tactic.\n__\nQuestion 5.8\nHierarchical clustering for cell populations The Morder data are gene expression measurements for 156 genes on T cells of 3 types (naïve, effector, memory) from 10 patients (Holmes et al. 2005). Using the pheatmap package, make two simple heatmaps, without dendogram or reordering, for Euclidean and Manhattan distances of these data.\n__\nQuestion 5.9\nNow, look at the differences in orderings in the hierarchical clustering trees with these two distances. What differences are noticeable?\n\n\n\n\n\n\n\n\n[](imgs/complete14heatmap.png “Figure 5.22 (c):”)\n\n\n\nFigure 5.22: Three hierarchical clustering plots made with different agglomeration choices. Note the comb-like structure for single linkage in (a). The average (b) and complete linkage (c) trees only differ by the lengths of their inner branches.\n\nFigure 5.23: This tree can be drawn in many different ways. The ordering of the leaves as it is appears here is \\((8,11,9,10,7,5,6,1,4,2,3)\\).\n__\nQuestion 5.10\nA hierarchical clustering tree is like the Calder mobile in Figure 5.21 that can swing around many internal pivot points, giving many orderings of the tips consistent with a given tree. Look at the tree in Figure 5.23. How many ways are there of ordering the tip labels and still maintain consistence with that tree?\nIt is common to see heatmaps whose rows and/or columns are ordered based on a hierarchical clustering tree. Sometimes this makes some clusters look very strong – stronger than what the tree really implies. There are alternative ways of ordering the rows and columns in heatmaps, for instance, in the package NeatMap , that uses ordination methods5 to find orderings.\n5 These will be explained in Chapter 9.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "05-chap.html#validating-and-choosing-the-number-of-clusters",
    "href": "05-chap.html#validating-and-choosing-the-number-of-clusters",
    "title": "7  5.1 Goals for this chapter",
    "section": "7.6 5.7 Validating and choosing the number of clusters",
    "text": "7.6 5.7 Validating and choosing the number of clusters\nThe clustering methods we have described are tailored to deliver good groupings of the data under various constraints. However, keep in mind that clustering methods will always deliver groups, even if there are none. If, in fact, there are no real clusters in the data, a hierarchical clustering tree may show relatively short inner branches; but it is difficult to quantify this. In general it is important to validate your choice of clusters with more objective criteria.\nOne criterion to assess the quality of a clustering result is to ask to what extent it maximizes the between group differences while keeping the within- group distances small (maximizing the lengths of red lines and minimizing those of the black lines in Figure 5.20). We formalize this with the within- groups sum of squared distances (WSS):\n\\[ k={}^k {x_i C} d^2(x_i, {x}_{}) \\]\nHere, \\(k\\) is the number of clusters, \\(C_\\) is the set of objects in the \\(\\)-th cluster, and \\({x}_\\) is the center of mass (the average point) in the \\(\\)-th cluster. We state the dependence on \\(k\\) of the WSS in Equation 5.4 as we are interested in comparing this quantity across different values of \\(k\\), for the same cluster algorithm. Stated as it is, the WSS is however not a sufficient criterion: the smallest value of WSS would simply be obtained by making each point its own cluster. The WSS is a useful building block, but we need more sophisticated ideas than just looking at this number alone.\nOne idea is to look at \\(_k\\) as a function of \\(k\\). This will always be a decreasing function, but if there is a pronounced region where it decreases sharply and then flattens out, we call this an elbow and might take this as a potential sweet spot for the number of clusters.\n__\nQuestion 5.11\n**An alternative expression for \\(_k\\)**. Use R to compute the sum of distances between all pairs of points in a cluster and compare it to \\(_k\\). Can you see how \\(_k\\) can also be written:\n\\[ k={}^k {x_i C} {x_j C} d^2(x_i,x_j), \\]\nwhere \\(n_\\) is the size of the \\(\\)-th cluster.\nQuestion 5.11 shows us that the within-cluster sums of squares \\(_k\\) measures both the distances of all points in a cluster to its center, and the average distance between all pairs of points in the cluster.\nWhen looking at the behavior of various indices and statistics that help us decide how many clusters are appropriate for the data, it can be useful to look at cases where we actually know the right answer.\nTo start, we simulate data coming from four groups. We use the pipe (%&gt;%) operator and the bind_rows function from dplyr to concatenate the four tibble s corresponding to each cluster into one big tibble.6\n6 The pipe operator passes the value to its left into the function to its right. This can make the flow of data easier to follow in code: f(x) %&gt;% g(y) is equivalent to g(f(x), y).\nlibrary(\"dplyr\")\nsimdat = lapply(c(0, 8), function(mx) {\n  lapply(c(0,8), function(my) {\n    tibble(x = rnorm(100, mean = mx, sd = 2),\n           y = rnorm(100, mean = my, sd = 2),\n           class = paste(mx, my, sep = \":\"))\n   }) %&gt;% bind_rows\n}) %&gt;% bind_rows\nsimdat __\n\n\n# A tibble: 400 × 3\n        x      y class\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;\n 1 -2.42  -4.59  0:0  \n 2  1.89  -1.56  0:0  \n 3  0.558  2.17  0:0  \n 4  2.51  -0.873 0:0  \n 5 -2.52  -0.766 0:0  \n 6  3.62   0.953 0:0  \n 7  0.774  2.43  0:0  \n 8 -1.71  -2.63  0:0  \n 9  2.01   1.28  0:0  \n10  2.03  -1.25  0:0  \n# ℹ 390 more rows\n\n\nsimdatxy = simdat[, c(\"x\", \"y\")] # without class label __\n\n\nggplot(simdat, aes(x = x, y = y, col = class)) + geom_point() +\n  coord_fixed()__\n\nFigure 5.24: The simdat data colored by the class labels. Here, we know the labels since we generated the data – usually we do not know them.\nWe compute the within-groups sum of squares for the clusters obtained from the \\(k\\)-means method:\nwss = tibble(k = 1:8, value = NA_real_)\nwss$value[1] = sum(scale(simdatxy, scale = FALSE)^2)\nfor (i in 2:nrow(wss)) {\n  km  = kmeans(simdatxy, centers = wss$k[i])\n  wss$value[i] = sum(km$withinss)\n}\nggplot(wss, aes(x = k, y = value)) + geom_col()__\n\nFigure 5.25: The barchart of the WSS statistic as a function of \\(k\\) shows that the last substantial jump is just before \\(k=4\\). This indicates that the best choice for these data is \\(k=4\\).\n__\nQuestion 5.12\n\nRun the code above several times and compare the wss values for different runs. Why are they different?\nCreate a set of data with uniform instead of normal distributions with the same range and dimensions as simdat. Compute the WSS values for for thess data. What do you conclude?\n\n__\nQuestion 5.13\nThe so-called Calinski-Harabasz index uses the WSS and BSS (between group sums of squares). It is inspired by the \\(F\\) statistic — the ratio of the mean sum of squares explained by a factor to the mean residual sum of squares — used in analysis of variance:\n\\[ (k)= k = {}^k n_({x}_{}-{x})^2, \\]\nwhere \\({x}\\) is the overall center of mass (average point). Plot the Calinski-Harabasz index for the simdat data.\n__\nSolution\n__\nHere is the code to generate Figure 5.26.\nlibrary(\"fpc\")\nlibrary(\"cluster\")\nCH = tibble(\n  k = 2:8,\n  value = sapply(k, function(i) {\n    p = pam(simdatxy, i)\n    calinhara(simdatxy, p$cluster)\n  })\n)\nggplot(CH, aes(x = k, y = value)) + geom_line() + geom_point() +\n  ylab(\"CH index\")__\n\nFigure 5.26: The Calinski-Harabasz index, i.,e., the ratio of the between and within group variances for different choices of \\(k\\), computed on the simdat data.\n\n7.6.1 5.7.1 Using the gap statistic\nTaking the logarithm of the within-sum-of-squares (\\((_k)\\)) and comparing it to averages from simulated data with less structure can be a good way of choosing \\(k\\). This is the basic idea of the gap statistic introduced by Tibshirani, Walther, and Hastie (2001). We compute \\((_k)\\) for a range of values of \\(k\\), the number of clusters, and compare it to that obtained on reference data of similar dimensions with various possible ‘non-clustered’ distributions. We can use uniformly distributed data as we did above or data simulated with the same covariance structure as our original data.\n\nThis algorithm is a Monte Carlo method that compares the gap statistic for the observed data to an average over simulations of data with similar structure.\nAlgorithm for computing the gap statistic(Tibshirani, Walther, and Hastie 2001):\n\nCluster the data with \\(k\\) clusters and compute \\(_k\\) for the various choices of \\(k\\).\nGenerate \\(B\\) plausible reference data sets, using Monte Carlo sampling from a homogeneous distribution and redo Step 1 above for these new simulated data. This results in \\(B\\) new within-sum-of-squares for simulated data \\(W_{kb}^*\\), for \\(b=1,…,B\\).\nCompute the \\((k)\\)-statistic:\n\n\\[ (k) = _k - _k k ={b=1}^B W^*_{kb} \\]\nNote that the first term is expected to be bigger than the second one if the clustering is good (i.e., the WSS is smaller); thus the gap statistic will be mostly positive and we are looking for its highest value.\n\nWe can use the standard deviation\n\n\\[ k^2 = {b=1}B((W*_{kb})-_k)^2 \\]\nto help choose the best \\(k\\). Several choices are available, for instance, to choose the smallest \\(k\\) such that\n\\[ (k) (k+1) - s’{k+1} s’{k+1}=_{k+1}. \\]\nThe packages cluster and clusterCrit provide implementations.\n__\nQuestion 5.14\nMake a function that plots the gap statistic as in Figure 5.27. Show the output for the simdat example dataset clustered with the pam function.\n__\nSolution\n__\nlibrary(\"cluster\")\nlibrary(\"ggplot2\")\npamfun = function(x, k)\n  list(cluster = pam(x, k, cluster.only = TRUE))\n\ngss = clusGap(simdatxy, FUN = pamfun, K.max = 8, B = 50,\n              verbose = FALSE)\nplot_gap = function(x) {\n  gstab = data.frame(x$Tab, k = seq_len(nrow(x$Tab)))\n  ggplot(gstab, aes(k, gap)) + geom_line() +\n    geom_errorbar(aes(ymax = gap + SE.sim,\n                      ymin = gap - SE.sim), width=0.1) +\n    geom_point(size = 3, col=  \"red\")\n}\nplot_gap(gss)__\n\nFigure 5.27: The gap statistic, see Question 5.14.\nLet’s now use the method on a real example. We load the Hiiragi data that we already explored in Chapter 3 and will see how the cells cluster.\nlibrary(\"Hiiragi2013\")__\n\n\nIn chunk 'Hiiragi': Warning: replacing previous import 'boot::logit' by 'gtools::logit' whenloading 'Hiiragi2013'\n\n\nIn chunk 'Hiiragi': Warning: replacing previous import 'boot::inv.logit' by 'gtools::inv.logit'when loading 'Hiiragi2013'\n\n\ndata(\"x\")__\nWe start by choosing the 50 most variable genes (features)7.\n7 The intention behind this step is to reduce the influence of technical (or batch) effects. Although individually small, when accumulated over all the 45101 features in x, many of which match genes that are weakly or not expressed, without this feature selection step, such effects are prone to suppress the biological signal.\nselFeats = order(rowVars(Biobase::exprs(x)), decreasing = TRUE)[1:50]\nembmat = t(Biobase::exprs(x)[selFeats, ])\nembgap = clusGap(embmat, FUN = pamfun, K.max = 24, verbose = FALSE)\nk1 = maxSE(embgap$Tab[, \"gap\"], embgap$Tab[, \"SE.sim\"])\nk2 = maxSE(embgap$Tab[, \"gap\"], embgap$Tab[, \"SE.sim\"],\n           method = \"Tibs2001SEmax\")\nc(k1, k2)__\n\n\n[1] 9 7\nThe default choice for the number of clusters, k1, is the first value of \\(k\\) for which the gap is not larger than the first local maximum minus a standard error \\(s\\) (see the manual page of the clusGap function). This gives a number of clusters \\(k = 9\\), whereas the choice recommended by Tibshirani, Walther, and Hastie (2001) is the smallest \\(k\\) such that \\((k) (k+1) - s_{k+1}’\\), this gives \\(k = 7\\). Let’s plot the gap statistic (Figure 5.28).\nplot(embgap, main = \"\")\ncl = pamfun(embmat, k = k1)$cluster\ntable(pData(x)[names(cl), \"sampleGroup\"], cl)__\n\n\n                 cl\n                   1  2  3  4  5  6  7  8  9\n  E3.25           23 11  1  1  0  0  0  0  0\n  E3.25 (FGF4-KO)  0  0  1 16  0  0  0  0  0\n  E3.5 (EPI)       2  1  0  0  0  8  0  0  0\n  E3.5 (FGF4-KO)   0  0  8  0  0  0  0  0  0\n  E3.5 (PE)        0  0  0  0  9  2  0  0  0\n  E4.5 (EPI)       0  0  0  0  0  0  0  4  0\n  E4.5 (FGF4-KO)   0  0  0  0  0  0  0  0 10\n  E4.5 (PE)        0  0  0  0  0  0  4  0  0\n\nFigure 5.28: The gap statistic for the Hiiragi2013 data.\nAbove we see the comparison between the clustering that we got from pamfun with the sample labels in the annotation of the data.\n__\nQuestion 5.15\nHow do the results change if you use all the features in x, rather than subsetting the top 50 most variable genes?\n\n\n7.6.2 5.7.2 Cluster validation using the bootstrap\n[](imgs/BootstrapClusterNew.png “Figure 5.29 (a):”)\n\n\n\n[](imgs/BootstrapCluster2New.png “Figure 5.29 (b):”)\n\n\n\nFigure 5.29: Different samples from the same distribution \\(F\\) lead to different clusterings. In (a), we see the true sampling variability. The bootstrap simulates this sampling variability by drawing subsamples using the empirical distribution function \\(_n\\) as shown in (b).\nWe saw the bootstrap principle in Chapter 4: ideally, we would like to use many new samples (sets of data) from the underlying data generating process, for each of them apply our clustering method, and then see how stable the clusterings are, or how much they change, using an index such as those we used above to compare clusterings. Of course, we don’t have these additional samples. So we are, in fact, going to create new datasets simply by taking different random subsamples of the data, look at the different clusterings we get each time, and compare them. Tibshirani, Walther, and Hastie (2001) recommend using bootstrap resampling to infer the number of clusters using the gap statistic.\nWe will continue using the Hiiragi2013 data. Here we follow the investigation of the hypothesis that the inner cell mass (ICM) of the mouse blastocyst in embyronic day 3.5 (E3.5) falls “naturally” into two clusters corresponding to pluripotent epiblast (EPI) versus primitive endoderm (PE), while the data for embryonic day 3.25 (E3.25) do not yet show this symmetry breaking.\nWe will not use the true group labels in our clustering and only use them in the final interpretation of the results. We will apply the bootstrap to the two different data sets (E3.5) and (E3.25) separately. Each step of the bootstrap will generate a clustering of a random subset of the data and we will need to compare these through a consensus of an ensemble of clusters. There is a useful framework for this in the clue package (Hornik 2005). The function clusterResampling, taken from the supplement of Ohnishi et al. (2014), implements this approach:\nclusterResampling = function(x, ngenes = 50, k = 2, B = 250,\n                             prob = 0.67) {\n  mat = Biobase::exprs(x)\n  ce = cl_ensemble(list = lapply(seq_len(B), function(b) {\n    selSamps = sample(ncol(mat), size = round(prob * ncol(mat)),\n                      replace = FALSE)\n    submat = mat[, selSamps, drop = FALSE]\n    sel = order(rowVars(submat), decreasing = TRUE)[seq_len(ngenes)]\n    submat = submat[sel,, drop = FALSE]\n    pamres = pam(t(submat), k = k)\n    pred = cl_predict(pamres, t(mat[sel, ]), \"memberships\")\n    as.cl_partition(pred)\n  }))\n  cons = cl_consensus(ce)\n  ag = sapply(ce, cl_agreement, y = cons)\n  list(agreements = ag, consensus = cons)\n}__\nThe function clusterResampling performs the following steps:\n\nDraw a random subset of the data (the data are either all E3.25 or all E3.5 samples) by selecting 67% of the samples without replacement.\nSelect the top ngenes features by overall variance (in the subset).\nApply \\(k\\)-means clustering and predict the cluster memberships of the samples that were not in the subset with the cl_predict method from the clue package, through their proximity to the cluster centres.\nRepeat steps 1-3 B times.\nApply consensus clustering (cl_consensus).\nFor each of the B clusterings, measure the agreement with the consensus through the function(cl_agreement). Here a good agreement is indicated by a value of 1, and less agreement by smaller values. If the agreement is generally high, then the clustering into \\(k\\) classes can be considered stable and reproducible; inversely, if it is low, then no stable partition of the samples into \\(k\\) clusters is evident.\n\nAs a measure of between-cluster distance for the consensus clustering, the Euclidean dissimilarity of the memberships is used, i.e., the square root of the minimal sum of the squared differences of \\(\\) and all column permutations of \\(\\), where \\(\\) and \\(\\) are the cluster membership matrices. As agreement measure for Step [clueagree], the quantity \\(1 - d/m\\) is used, where \\(d\\) is the Euclidean dissimilarity, and \\(m\\) is an upper bound for the maximal Euclidean dissimilarity.\niswt = (x$genotype == \"WT\")\ncr1 = clusterResampling(x[, x$Embryonic.day == \"E3.25\" & iswt])\ncr2 = clusterResampling(x[, x$Embryonic.day == \"E3.5\"  & iswt])__\nThe results are shown in Figure 5.30. They confirm the hypothesis that the E.35 data fall into two clusters.\nag1 = tibble(agreements = cr1$agreements, day = \"E3.25\")\nag2 = tibble(agreements = cr2$agreements, day = \"E3.5\")\np1 &lt;- ggplot(bind_rows(ag1, ag2), aes(x = day, y = agreements)) +\n  geom_boxplot() +\n  ggbeeswarm::geom_beeswarm(cex = 1.5, col = \"#0000ff40\")\nmem1 = tibble(y = sort(cl_membership(cr1$consensus)[, 1]),\n              x = seq(along = y), day = \"E3.25\")\nmem2 = tibble(y = sort(cl_membership(cr2$consensus)[, 1]),\n              x = seq(along = y), day = \"E3.5\")\np2 &lt;- ggplot(bind_rows(mem1, mem2), aes(x = x, y = y, col = day)) +\n  geom_point() + facet_grid(~ day, scales = \"free_x\")\ngridExtra::grid.arrange(p1, p2, widths = c(2.4,4.0))__\n\nFigure 5.30: Cluster stability analysis with E3.25 and E3.5 samples. Left: beeswarm plots of the cluster agreements with the consensus, for the B clusterings; \\(1\\) indicates perfect agreement, lower values indicate lower degrees of agreement. Right: membership probabilities of the consensus clustering. For E3.25, the probabilities are diffuse, indicating that the individual clusterings often disagree, whereas for E3.5, the distribution is bimodal, with only one ambiguous sample.\n\n7.6.2.1 Computational and memory Issues\n\n\n\nComputational complexity. An algorithm is said to be O(n^k), if, as n gets larger, the resource consumption (CPU time or memory) grows proportionally to n^k. There may be other (sometimes considerable) baseline costs, or costs that grow proportionally to lower powers of n, but these always become negligible compared to the leading term as n\\to\\infty.\n\n\nComputational complexity. An algorithm is said to be \\(O(n^k)\\), if, as \\(n\\) gets larger, the resource consumption (CPU time or memory) grows proportionally to \\(n^k\\). There may be other (sometimes considerable) baseline costs, or costs that grow proportionally to lower powers of \\(n\\), but these always become negligible compared to the leading term as \\(n\\).\nIt is important to remember that the computation of all versus all distances of \\(n\\) objects is an \\(O(n^2)\\) operation (in time and memory). Classic hierarchical clustering approaches (such as hclust in the stats package) are even \\(O(n^3)\\) in time. For large \\(n\\), this may become impractical8. We can avoid the complete computation of the all-vs-all distance matrix. For instance, \\(k\\)-means has the advantage of only requiring \\(O(n)\\) computations, since it only keeps track of the distances between each object and the cluster centers, whose number remains the same even if \\(n\\) increases.\n8 E.g., the distance matrix for one million objects, stored as 8-byte floating point numbers, would take up about 4 Terabytes, and an hclust-like algorithm would run 30 years even under the optimistic assumption that each of the iterations only takes a nanosecond.\nFast implementations such as fastclust (Müllner 2013) and dbscan have been carefully optimized to deal with a large number of observations.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "05-chap.html#clustering-as-a-means-for-denoising",
    "href": "05-chap.html#clustering-as-a-means-for-denoising",
    "title": "7  5.1 Goals for this chapter",
    "section": "7.7 5.8 Clustering as a means for denoising",
    "text": "7.7 5.8 Clustering as a means for denoising\nConsider a set of measurements that reflect some underlying true values (say, species represented by DNA sequences from their genomes), but have been degraded by technical noise. Clustering can be used to remove such noise.\n\n7.7.1 5.8.1 Noisy observations with different baseline frequencies\nSuppose that we have a bivariate distribution of observations made with the same error variances. However, the sampling is from two groups that have very different baseline frequencies. Suppose, further, that the errors are continuous independent bivariate normally distributed. We have \\(10^{3}\\) of seq1 and \\(10^{5}\\) of seq2, as generated for instance by the code:\nlibrary(\"mixtools\")\nseq1 = rmvnorm(n = 1e3, mu = -c(1, 1), sigma = 0.5 * diag(c(1, 1)))\nseq2 = rmvnorm(n = 1e5, mu =  c(1, 1), sigma = 0.5 * diag(c(1, 1)))\ntwogr = data.frame(\n  rbind(seq1, seq2),\n  seq = factor(c(rep(1, nrow(seq1)),\n                 rep(2, nrow(seq2))))\n)\ncolnames(twogr)[1:2] = c(\"x\", \"y\")\nlibrary(\"ggplot2\")\nggplot(twogr, aes(x = x, y = y, colour = seq,fill = seq)) +\n  geom_hex(alpha = 0.5, bins = 50) + coord_fixed()__\n\nFigure 5.31: Although both groups have noise distributions with the same variances, the apparent radii of the groups are very different. The \\(10^{5}\\) instances in seq2 have many more opportunities for errors than what we see in seq1, of which there are only \\(10^{3}\\). Thus we see that frequencies are important in clustering the data.\nThe observed values would look as in Figure 5.31.\n__\nQuestion 5.16\nTake the data seq1 and seq2 and cluster them into two groups according to distance from group center. Do you think the results should depend on the frequencies of each of the two sequence types?\n__\nSolution\n__\nSuch an approach, often used in taxonomic clustering, also called OTU -operational taxonomic unit clustering (Caporaso et al. 2010; P. D. Schloss et al. 2009)) methods is sub-optimal.\nThe methods based solely on similarities suffer from the biases inherent in the representativeness heuristic. Let’s make a brief digression into the world of cognitive psychology that helps explain how our natural inclination to use only representativeness and a distance-based heuristic in clustering and taxonomic assignment can lead to biased results.\nIn the 1970s, Tversky and Kahneman (1975) pointed out that we generally assign groups by looking at the most similar representatives. In clustering and group assignments that would mean assigning a new sequence to the group according to the distance to its center. In fact this is equivalent to taking balls with the same radius regardless of the differences in prevalence of the different groups. This psychological error was first discussed in an important Science paper that covers many different heuristics and biases(Tversky and Kahneman 1974).\n\n\n\nSee Kahneman (2011) for a book-length treatment of our natural heuristics and the ways in which they can mislead us when we make probability calculations (we recommend especially Chapters 14 and 15).\n\n\nSee Kahneman (2011) for a book-length treatment of our natural heuristics and the ways in which they can mislead us when we make probability calculations (we recommend especially Chapters 14 and 15).\n__\nTask\nSimulate n=2000 binary variables of length len=200 that indicate the quality of n sequencing reads of length len. For simplicity, let us assume that sequencing errors occur independently and uniformly with probability perr=0.001. That is, we only care whether a base was called correctly (TRUE) or not (FALSE).\nn    = 2000\nlen  = 200\nperr = 0.001\nseqs = matrix(runif(n * len) &gt;= perr, nrow = n, ncol = len)__\nNow, compute all pairwise distances between reads.\ndists = as.matrix(dist(seqs, method = \"manhattan\"))__\nFor various values of number of reads k (from 2 to n), the maximum distance within this set of reads is computed by the code below and shown in Figure 5.32.\nlibrary(\"tibble\")\ndfseqs = tibble(\n  k = 10 ^ seq(log10(2), log10(n), length.out = 20),\n  diameter = vapply(k, function(i) {\n    s = sample(n, i)\n    max(dists[s, s])\n    }, numeric(1)))\nggplot(dfseqs, aes(x = k, y = diameter)) + geom_point()+geom_smooth()__\n\nFigure 5.32: The diameter of a set of sequences as a function of the number of sequences.\nWe will now improve the 16SrRNA-read clustering using a denoising mechanism that incorporates error probabilities.\n\n\n7.7.2 5.8.2 Denoising 16S rRNA sequences\nWhat are the data? In the bacterial 16SrRNA gene there are so-called variable regions that are taxa-specific. These provide fingerprints that enables taxon 9 identification. The raw data are FASTQ-files with quality scored sequences of PCR-amplified DNA regions10. We use an iterative alternating approach11 to build a probabilistic noise model from the data. We call this a de novo method, because we use clustering, and we use the cluster centers as our denoised sequence variants (a.k.a. Amplicon Sequence Variants, ASVs, see (Benjamin J. Callahan, McMurdie, and Holmes 2017)). After finding all the denoised variants, we create contingency tables of their counts across the different samples. We will show in Chapter 10 how these tables can be used to infer properties of the underlying bacterial communities using networks and graphs.\n9 Calling different groups of bacteria taxa rather than species highlights the approximate nature of the concept, as the notion of species is more fluid in bacteria than, say, in animals.\n10 The FASTQ format is described here.\n11 Similar to the EM algorithm we saw in Chapter 4.\nIn order to improve data quality, one often has to start with the raw data and model all the sources of variation carefully. One can think of this as an example of cooking from scratch (see the gruesome details in Ben J. Callahan et al. (2016) and Exercise 5.5).\n__\nQuestion 5.17\nSuppose that we have two sequences of length 200 (seq1 and seq2) present in our sample at very different abundances. We are told that the technological sequencing errors occur as independent Bernoulli(0.0005) random events for each nucleotide.\nWhat is the distribution of the number of errors per sequence?\n__\nSolution\n__\nProbability theory tells us that the sum of 200 independent Poisson(0.0005) will be Poisson(0.1).\nWe can also verify this by Monte Carlo simulation:\nsimseq10K = replicate(1e5, sum(rpois(200, 0.0005)))\nmean(simseq10K)__\n\n\n[1] 0.10143\n\n\nvcd::distplot(simseq10K, \"poisson\")__\n\nFigure 5.33: distplot for the simseq10K data.\nFigure 5.33 shows us how close the distribution is to being Poisson distributed.\n\n\n7.7.3 5.8.3 Infer sequence variants\nThe DADA method (Divisive Amplicon Denoising Algorithm, Rosen et al. (2012)) uses a parameterized model of substitution errors that distinguishes sequencing errors from real biological variation. The model computes the probabilities of base substitutions, such as seeing an \\({}\\) instead of a \\({}\\). It assumes that these probabilities are independent of the position along the sequence. Because error rates vary substantially between sequencing runs and PCR protocols, the model parameters are estimated from the data themselves using an EM-type approach. A read is classified as noisy or exact given the current parameters, and the noise model parameters are updated accordingly12.\n12 In the case of a large data set, the noise model estimation step does not have to be done on the complete set. See https://benjjneb.github.io/dada2/bigdata.html for tricks and tools when dealing with large data sets.\n13 F stands for forward strand and R for reverse strand.\nThe dereplicated sequences13 are read in and then divisive denoising and estimation is run with the dada function as in the following code:\nderepFs = readRDS(file=\"../data/derepFs.rds\")\nderepRs = readRDS(file=\"../data/derepRs.rds\")\nlibrary(\"dada2\")\nddF = dada(derepFs, err = NULL, selfConsist = TRUE)\nddR = dada(derepRs, err = NULL, selfConsist = TRUE)__\nIn order to verify that the error transition rates have been reasonably well estimated, we inspect the fit between the observed error rates (black points) and the fitted error rates (black lines) (Figure 5.34).\nplotErrors(ddF)__\n\n\nIn chunk 'fig-rerrorprofile1': Warning: Removed 82 rows containing missing values or values outside the scale range(`geom_line()`).\n\nFigure 5.34: Forward transition error rates as provided by plotErrors(ddF). This shows the frequencies of each type of nucleotide transition as a function of quality.\nOnce the errors have been estimated, the algorithm is rerun on the data to find the sequence variants:\ndadaFs = dada(derepFs, err=ddF[[1]]$err_out, pool = TRUE)\ndadaRs = dada(derepRs, err=ddR[[1]]$err_out, pool = TRUE)__\nNote: The sequence inference function can run in two different modes: Independent inference by sample (pool = FALSE), and pooled inference from the sequencing reads combined from all samples. Independent inference has two advantages: as a functions of the number of samples, computation time is linear and memory requirements are constant. Pooled inference is more computationally taxing, however it can improve the detection of rare variants that occur just once or twice in an individual sample but more often across all samples. As this dataset is not particularly large, we performed pooled inference.\nSequence inference removes nearly all substitution and indel 14 errors from the data. We merge the inferred forward and reverse sequences, while removing paired sequences that do not perfectly overlap as a final control against residual errors.\n14 The term indel stands for insertion-deletion; when comparing two sequences that differ by a small stretch of characters, it is a matter of viewpoint whether this is an insertion or a deletion, thus the name.\nmergers = mergePairs(dadaFs, derepFs, dadaRs, derepRs)__\nWe produce a contingency table of counts of ASVs. This is a higher-resolution analogue of the “OTU15 table”, i.e., a samples by features table whose cells contain the number of times each sequence variant was observed in each sample.\n15 operational taxonomic units\nseqtab.all = makeSequenceTable(mergers[!grepl(\"Mock\",names(mergers))])__\n__\nQuestion 5.18\nExplore the components of the objects dadaRs and mergers.\n__\nSolution\n__\ndadaRs is a list of length 20. Its elements are objects class dada that contain the denoised reads. We will see in Chapter 10 how to align the sequences, assign their taxonomies and combine them with the sample information for downstream analyses.\n[1] 20\n\n\n[1] 20\n\n\n[1] \"list\"\n\n\n [1] \"F3D0\"   \"F3D1\"   \"F3D141\" \"F3D142\" \"F3D143\" \"F3D144\" \"F3D145\" \"F3D146\"\n [9] \"F3D147\" \"F3D148\" \"F3D149\" \"F3D150\" \"F3D2\"   \"F3D3\"   \"F3D5\"   \"F3D6\"  \n[17] \"F3D7\"   \"F3D8\"   \"F3D9\"   \"Mock\"  \n\n\n[1] \"list\"\n\n\n[1] 20\nChimera are sequences that are artificially created during PCR amplification by the melding of two (in rare cases, more) of the original sequences. To complete our denoising workflow, we remove them with a call to the function removeBimeraDenovo, leaving us with a clean contingency table we will use later on.\nseqtab = removeBimeraDenovo(seqtab.all)__\n__\nQuestion 5.19\nWhy do you think the chimera are quite easy to recognize?\nWhat proportion of the reads were chimeric in the seqtab.all data?\nWhat proportion of unique sequence variants are chimeric?\n__\nSolution\n__\nHere we observed some sequence variants as chimeric, but these only represent 7% of all reads.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "05-chap.html#summary-of-this-chapter",
    "href": "05-chap.html#summary-of-this-chapter",
    "title": "7  5.1 Goals for this chapter",
    "section": "7.8 5.9 Summary of this chapter",
    "text": "7.8 5.9 Summary of this chapter\nOf a feather: how to compare observations We saw at the start of the chapter how finding the right distance is an essential first step in a clustering analysis; this is a case where the garbage in, garbage out motto is in full force. Always choose a distance that is scientifically meaningful and compare output from as many distances as possible; sometimes the same data require different distances when different scientific objectives are pursued.\nTwo ways of clustering We saw there are two approaches to clustering:\n\niterative partitioning approaches such as \\(k\\)-means and \\(k\\)-medoids (PAM) that alternated between estimating the cluster centers and assigning points to them;\nhierarchical clustering approaches that first agglomerate points, and subsequently the growing clusters, into nested sequences of sets that can be represented by hierarchical clustering trees.\n\nBiological examples Clustering is important tool for finding latent classes in single cell measurements, especially in immunology and single cell data analyses. We saw how density-based clustering is useful for lower dimensional data where sparsity is not an issue.\nValidating Clustering algorithms always deliver clusters, so we need to assess their quality and the number of clusters to choose carefully. Such validation steps are performed using visualization tools and repeating the clustering on many resamples of the data. We saw how statistics such as WSS/BSS or \\(()\\) can be calibrated using simulations on data where we understand the group structure and can provide useful benchmarks for choosing the number of clusters on new data. Of course, the use of biologically relevant information to inform and confirm the meaning of clusters is always the best validation approach.\nThere is arguably no ground truth to compare a clustering result against, in general. The old adage of “all models are wrong, some are useful” also applies here. A good clustering is one that turns out to be useful.\nDistances and probabilities Finally: distances are not everything. We showed how important it was to take into account baseline frequencies and local densities when clustering. This is essential in a cases such as clustering to denoise 16S rRNA sequence reads where the true class or taxa group occur at very different frequencies.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "05-chap.html#further-reading",
    "href": "05-chap.html#further-reading",
    "title": "7  5.1 Goals for this chapter",
    "section": "7.9 5.10 Further reading",
    "text": "7.9 5.10 Further reading\nFor a complete book on Finding groups in data , see Kaufman and Rousseeuw (2009). The vignette of the clusterExperiment package contains a complete workflow for generating clusters using many different techniques, including preliminary dimension reduction (PCA) that we will cover in Chapter 7. There is no consensus on methods for deciding how many clusters are needed to describe data in the absence of contiguous biological information. However, making hierarchical clusters of the strong forms is a method that has the advantage of allowing the user to decide how far down to cut the hierarchical tree and be careful not to cut in places where these inner branches are short. See the vignette of clusterExperiment for an application to single cell RNA experimental data.\nIn analyzing the Hiiragi data, we used cluster probabilities, a concept already mentioned in Chapter 4, where the EM algorithm used them as weights to compute expected value statistics. The notion of probabilistic clustering is well-developed in the Bayesian nonparametric mixture framework, which enriches the mixture models we covered in Chapter 4 to more general settings. See Dundar et al. (2014) for a real example using this framework for flow cytometry. In the denoising and assignment of high-throughput sequencing reads to specific strains of bacteria or viruses, clustering is essential. In the presence of noise, clustering into groups of true strains of very unequal sizes can be challenging. Using the data to create a noise model enables both denoising and cluster assignment concurrently. Denoising algorithms such as those by Rosen et al. (2012) or Benjamin J. Callahan et al. (2016) use an iterative workflow inspired by the EM method (McLachlan and Krishnan 2007).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "05-chap.html#exercises",
    "href": "05-chap.html#exercises",
    "title": "7  5.1 Goals for this chapter",
    "section": "7.10 5.11 Exercises",
    "text": "7.10 5.11 Exercises\n__\nExercise 5.1\nWe can define the average dissimilarity of a point \\(x_i\\) to a cluster \\(C_k\\) as the average of the distances from \\(x_i\\) to all points in \\(C_k\\). Let \\(A(i)\\) be the average dissimilarity of all points in the cluster that \\(x_i\\) belongs to. Let \\(B(i)\\) be the lowest average dissimilarity of \\(x_i\\) to any other cluster of which \\(x_i\\) is not a member. The cluster with this lowest average dissimilarity is said to be the neighboring cluster of \\(x_i\\), because it is the next best fit cluster for point \\(x_i\\). The silhouette index is\n\\[ S(i)=. \\]\nCompute the silhouette index for the simdat data we simulated in Section 5.7.\nlibrary(\"cluster\")\npam4 = pam(simdatxy, 4)\nsil = silhouette(pam4, 4)\nplot(sil, col=c(\"red\",\"green\",\"blue\",\"purple\"), main=\"Silhouette\")__\nChange the number of clusters \\(k\\) and assess which \\(k\\) gives the best silhouette index.\nNow, repeat this for groups that have uniform (unclustered) data distributions over a whole range of values.\n__\nExercise 5.2\nMake a “character” representation of the distance between the 20 locations in the dune data from the vegan package using the function symnum.\nMake a heatmap plot of these distances.\n__\nExercise 5.3\nLoad the spirals data from the kernlab package. Plot the results of using \\(k\\)-means on the data. This should give you something similar to Figure 5.35.\n\n\n\n\n\n\n\n\nFigure 5.35: An example of non-convex clusters. In (a), we show the result of \\(k\\)-means clustering with \\(k=2\\). In (b), we have the output from dbscan. The colors represent the three clusters found by the algorithm for the settings .\nYou’ll notice that the clustering in Figure 5.35 seems unsatisfactory. Show how a different method, such as specc or dbscan, could cluster spirals data in a more useful manner.\nRepeat the dbscan clustering with different parameters. How robust is the number of groups?\n__\nExercise 5.4\nLooking at graphical representations in simple two-dimensional maps can often reveal important clumping patterns. We saw an example for this with the map that enabled Snow to discover the source of the London cholera outbreak. Such clusterings can often indicate important information about hidden variables acting on the observations. Look at a map for breast cancer incidence in the US at:\nhttp://www.huffingtonpost.com/bill-davenhall/post_1663_b_817254.html (Mandal et al. 2009); the areas of high incidence seem spatially clustered. Can you guess the reason(s) for this clustering and high incidence rates on the West and East coasts and around Chicago?\n__\nExercise 5.5\nAmplicon bioinformatics: from raw reads to dereplicated sequences. As a supplementary exercise, we provide the intermediate steps necessary to a full data preprocessing workflow for denoising 16S rRNA sequences. We start by setting the directories and loading the downloaded data:\nbase_dir = \"../data\"\nmiseq_path = file.path(base_dir, \"MiSeq_SOP\")\nfilt_path = file.path(miseq_path, \"filtered\")\nfnFs = sort(list.files(miseq_path, pattern=\"_R1_001.fastq\"))\nfnRs = sort(list.files(miseq_path, pattern=\"_R2_001.fastq\"))\nsampleNames = sapply(strsplit(fnFs, \"_\"), `[`, 1)\nif (!file_test(\"-d\", filt_path)) dir.create(filt_path)\nfiltFs = file.path(filt_path, paste0(sampleNames, \"_F_filt.fastq.gz\"))\nfiltRs = file.path(filt_path, paste0(sampleNames, \"_R_filt.fastq.gz\"))\nfnFs = file.path(miseq_path, fnFs)\nfnRs = file.path(miseq_path, fnRs)\nprint(length(fnFs))__\n\n\n[1] 20\nThe data are highly-overlapping Illumina Miseq \\(2\\) amplicon sequences from the V4 region of the 16S rRNA gene (Kozich et al. 2013). There were originally 360 fecal samples collected longitudinally from 12 mice over the first year of life. These were collected by P. D. Schloss et al. (2012) to investigate the development and stabilization of the murine microbiome. We have selected 20 samples to illustrate how to preprocess the data.\nWe will need to filter out low-quality reads and trim them to a consistent length. While generally recommended filtering and trimming parameters serve as a starting point, no two datasets are identical and therefore it is always worth inspecting the quality of the data before proceeding. We show the sequence quality plots for the two first samples in Figure 5.36. They are generated by:\nplotQualityProfile(fnFs[1:2]) + ggtitle(\"Forward\")\nplotQualityProfile(fnRs[1:2]) + ggtitle(\"Reverse\")__\n\n\n\n\n\n\n\n\nFigure 5.36: Quality scores. The lines show positional summary statistics: green is the mean, orange is the median, and the dashed orange lines are the 25th and 75th quantiles.\nNote that we also see the background distribution of quality scores at each position in Figure 5.36 as a grey-scale heat map. The dark colors correspond to higher frequency.\n__\nExercise 5.6\nGenerate similar plots for four randomly selected sets of forward and reverse reads. Compare forward and reverse read qualities; what do you notice?\n__\nSolution\n__\nii = sample(length(fnFs), 4)\nplotQualityProfile(fnFs[ii]) + ggtitle(\"Forward\")__\n\n\n plotQualityProfile(fnRs[ii]) + ggtitle(\"Reverse\")__\n__\nExercise 5.7\nHere, the forward reads maintain high quality throughout, while the quality of the reverse reads drops significantly at about position 160. Therefore, we truncate the forward reads at position 240, and trimm the first 10 nucleotides as these positions are of lower quality. The reverse reads are trimmed at position 160. Combine these trimming parameters with standard filtering parameters remember to enforce a maximum of 2 expected errors per-read. (Hint: Trim and filter on paired reads jointly, i.e., both reads must pass the filter for the pair to pass. The input arguments should be chosen following the dada2 vignette carefully. We recommend filtering out all reads with any ambiguous nucleotides.)\n__\nSolution\n__\nMost Illumina sequencing data show a trend of decreasing quality towards the end of the reads.\nout = filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),\n        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,  trimLeft=10,\n        compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE\nhead(out)__\n\n\n                              reads.in reads.out\nF3D0_S188_L001_R1_001.fastq       7793      7139\nF3D1_S189_L001_R1_001.fastq       5869      5314\nF3D141_S207_L001_R1_001.fastq     5958      5478\nF3D142_S208_L001_R1_001.fastq     3183      2926\nF3D143_S209_L001_R1_001.fastq     3178      2955\nF3D144_S210_L001_R1_001.fastq     4827      4323\nThe maxN parameter omits all reads with more than maxN = 0 ambiguous nucleotides and maxEE at 2 excludes reads with more than 2 expected errors.\nThe sequence data was imported into R from demultiplexed fastq files (i.e. one fastq for each sample) and simultaneously dereplicated to remove redundancy. Name the resulting objects by their sample provenance; they will have derep as their class.\nderepFs = derepFastq(filtFs, verbose = FALSE)\nderepRs = derepFastq(filtRs, verbose = FALSE)\nnames(derepFs) = sampleNames\nnames(derepRs) = sampleNames __\n__\nExercise 5.8\nUse R to create a map like the one shown in Figure 5.2. Hint: go to the website of the British National Archives and download street addresses of hits, use an address resolution service to convert these into geographic coordinates, and display these as points on a map of London.\n__\nSolution\n__\nSee the Gist https://gist.github.com/wolfganghuber/523622aff6a156d26e77a87ccbe7bd0d by Andrzej Oles.\nAure, Miriam Ragle, Valeria Vitelli, Sandra Jernström, Surendra Kumar, Marit Krohn, Eldri U Due, Tonje Husby Haukaas, et al. 2017. “Integrative Clustering Reveals a Novel Split in the Luminal A Subtype of Breast Cancer with Impact on Outcome.” Breast Cancer Research 19 (1): 44.\nBendall, Sean C, Garry P Nolan, Mario Roederer, and Pratip K Chattopadhyay. 2012. “A Deep Profiler’s Guide to Cytometry.” Trends in Immunology 33 (7): 323–32.\nCallahan, Ben J, Kris Sankaran, Julia A Fukuyama, Paul J McMurdie, and Susan P Holmes. 2016. “Bioconductor Workflow for Microbiome Data Analysis: From Raw Reads to Community Analyses.” F1000Research 5.\nCallahan, Benjamin J, Paul J McMurdie, and Susan P Holmes. 2017. “Exact Sequence Variants Should Replace Operational Taxonomic Units in Marker Gene Data Analysis.” ISME Journal , 1–5.\nCallahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy J Johnson, and Susan P Holmes. 2016. “DADA2: High Resolution Sample Inference from Amplicon Data.” Nature Methods , 1–4.\nCaporaso, J. G., J. Kuczynski, J. Stombaugh, K. Bittinger, F. D. Bushman, E. K. Costello, N. Fierer, et al. 2010. “QIIME Allows Analysis of High-Throughput Community Sequencing Data.” Nature Methods 7 (5): 335–36.\nChakerian, John, and Susan Holmes. 2012. “Computational Tools for Evaluating Phylogenetic and Hierarchical Clustering Trees.” Journal of Computational and Graphical Statistics 21 (3): 581–99.\nDiday, Edwin, and M Paula Brito. 1989. “Symbolic Cluster Analysis.” In Conceptual and Numerical Analysis of Data , 45–84. Springer.\nDundar, Murat, Ferit Akova, Halid Z. Yerebakan, and Bartek Rajwa. 2014. “A Non-Parametric Bayesian Model for Joint Cell Clustering and Cluster Matching: Identification of Anomalous Sample Phenotypes with Random Effects.” BMC Bioinformatics 15 (1): 1–15. https://doi.org/10.1186/1471-2105-15-314.\nFreedman, David A. 1991. “Statistical Models and Shoe Leather.” Sociological Methodology 21 (2): 291–313.\nHallett, Robin M, Anna Dvorkin-Gheva, Anita Bane, and John A Hassell. 2012. “A Gene Signature for Predicting Outcome in Patients with Basal-Like Breast Cancer.” Scientific Reports 2.\nHolmes, Susan, Michael He, Tong Xu, and Peter P Lee. 2005. “Memory t Cells Have Gene Expression Patterns Intermediate Between Naive and Effector.” PNAS 102 (15): 5519–23.\nHornik, Kurt. 2005. “A CLUE for CLUster Ensembles.” Journal of Statistical Software 14 (12).\nHulett, Henry R, William A Bonner, Janet Barrett, and Leonard A Herzenberg. 1969. “Cell Sorting: Automated Separation of Mammalian Cells as a Function of Intracellular Fluorescence.” Science 166 (3906): 747–49.\nKahneman, Daniel. 2011. Thinking, Fast and Slow. Macmillan.\nKaufman, Leonard, and Peter J Rousseeuw. 2009. Finding Groups in Data: An Introduction to Cluster Analysis. Vol. 344. John Wiley & Sons.\nKozich, James J, Sarah L Westcott, Nielson T Baxter, Sarah K Highlander, and Patrick D Schloss. 2013. “Development of a Dual-Index Sequencing Strategy and Curation Pipeline for Analyzing Amplicon Sequence Data on the MiSeq Illumina Sequencing Platform.” Applied and Environmental Microbiology 79 (17): 5112–20.\nMandal, Rakesh, Sophie St-Hilaire, John G Kie, and DeWayne Derryberry. 2009. “Spatial Trends of Breast and Prostate Cancers in the United States Between 2000 and 2005.” International Journal of Health Geographics 8 (1): 53.\nMcLachlan, Geoffrey, and Thriyambakam Krishnan. 2007. The EM Algorithm and Extensions. Vol. 382. John Wiley & Sons.\nMüllner, Daniel. 2013. “Fastcluster: Fast Hierarchical, Agglomerative Clustering Routines for r and Python.” Journal of Statistical Software 53 (9): 1–18.\nO’Neill, Kieran, Nima Aghaeepour, Josef Špidlen, and Ryan Brinkman. 2013. “Flow Cytometry Bioinformatics.” PLoS Computational Biology 9 (12): e1003365.\nOhnishi, Y., W. Huber, A. Tsumura, M. Kang, P. Xenopoulos, K. Kurimoto, A. K. Oles, et al. 2014. “Cell-to-Cell Expression Variability Followed by Signal Reinforcement Progressively Segregates Early Mouse Lineages.” Nature Cell Biology 16 (1): 27–37.\nRosen, Michael J, Benjamin J Callahan, Daniel S Fisher, and Susan P Holmes. 2012. “Denoising PCR-Amplified Metagenome Data.” BMC Bioinformatics 13 (1): 283.\nSchloss, P D, S L Westcott, T Ryabin, J R Hall, M Hartmann, E B Hollister, R A Lesniewski, et al. 2009. “Introducing mothur: Open-Source, Platform- Independent, Community-Supported Software for Describing and Comparing Microbial Communities.” Applied and Environmental Microbiology 75 (23): 7537–41.\nSchloss, P. D., A. M. Schuber, J. P. Zackular, K. D. Iverson, Young V. B., and Petrosino J. F. 2012. “Stabilization of the Murine Gut Microbiome Following Weaning.” Gut Microbes 3 (4): 383–93.\nTibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. “Estimating the Number of Clusters in a Data Set via the Gap Statistic.” JRSSB 63 (2): 411–23.\nTseng, George C, and Wing H Wong. 2005. “Tight Clustering: A Resampling-Based Approach for Identifying Stable and Tight Patterns in Data.” Biometrics 61 (1): 10–16.\nTversky, Amos, and Daniel Kahneman. 1974. “Heuristics and Biases: Judgement Under Uncertainty.” Science 185: 1124–30.\n———. 1975. “Judgment Under Uncertainty: Heuristics and Biases.” In Utility, Probability, and Human Decision Making , 141–62. Springer.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "06-chap.html",
    "href": "06-chap.html",
    "title": "8  6.1 Goals for this Chapter",
    "section": "",
    "text": "8.0.1 6.1.1 Drinking from the firehose\nHypothesis testing is one of the workhorses of science. It is how we can draw conclusions or make decisions based on finite samples of data. For instance, new treatments for a disease are usually approved on the basis of clinical trials that aim to decide whether the treatment has better efficacy compared to the other available options, and an acceptable trade-off of side effects. Such trials are expensive and can take a long time. Therefore, the number of patients we can enroll is limited, and we need to base our inference on a limited sample of observed patient responses. The data are noisy, since a patient’s response depends not only on the treatment, but on many other factors outside of our control. The sample size needs to be large enough to enable us to make a reliable conclusion. On the other hand, it also must not be too large, so that we do not waste precious resources or time, e.g., by making drugs more expensive than necessary, or by denying patients that would benefit from the new drug access to it. The machinery of hypothesis testing was developed largely with such applications in mind, although today it is used much more widely.\nIn biological data analysis (and in many other fields1) we see hypothesis testing applied to screen thousands or millions of possible hypotheses to find the ones that are worth following up. For instance, researchers screen genetic variants for associations with a phenotype, or gene expression levels for associations with disease. Here, “worthwhile” is often interpreted as “statistically significant”, although the two concepts are clearly not the same. It is probably fair to say that statistical significance is a necessary condition for making a data-driven decision to find something interesting, but it’s clearly not sufficient. In any case, such large-scale association screening is closely related to multiple hypothesis testing.\n1 Detecting credit card fraud, email spam detection, \\(…\\)\nIn this chapter we will:\nFigure 6.1: High-throughput data in modern biology are screened for associations with millions of hypothesis tests. (Source: Bayer)\nIf statistical testing—decision making with uncertainty—seems a hard task when making a single decision, then brace yourself: in genomics, or more generally with “big data”, we need to accomplish it not once, but thousands or millions of times. In Chapter 2, we saw the example of epitope detection and the challenges from considering not only one, but several positions. Similarly, in whole genome sequencing, we scan every position in the genome for evidence of a difference in the DNA sequencing data at hand and a reference sequence (or, another set of sequencing data): that’s in the order of six billion tests if we are looking at human data! In genetic or chemical compound screening, we test each of the reagents for an effect in the assay, compared to a control: that’s again tens of thousands, if not millions of tests. In Chapter 8, we will analyse RNA-Seq data for differential expression by applying a hypothesis test to each of the thousands of genes assayed.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 Goals for this Chapter</span>"
    ]
  },
  {
    "objectID": "06-chap.html#an-example-coin-tossing",
    "href": "06-chap.html#an-example-coin-tossing",
    "title": "8  6.1 Goals for this Chapter",
    "section": "8.1 6.2 An example: coin tossing",
    "text": "8.1 6.2 An example: coin tossing\nSo now let’s dive into hypothesis testing, starting with single testing. To really understand the mechanics, we use one of the simplest possible examples: suppose we are flipping a coin to see if it is fair4. We flip the coin 100 times and each time record whether it came up heads or tails. So, we have a record that could look something like this:\n4 We don’t look at coin tossing because it’s inherently important, but because it is an easy “model system” (just as we use model systems in biology): everything can be calculated easily, and you do not need a lot of domain knowledge to understand what coin tossing is. All the important concepts come up, and we can apply them, only with more additional details, to other applications.\nHHTTHTHTT…\nwhich we can simulate in R. Let’s assume we are flipping a biased coin, so we set probHead different from 1/2:\nset.seed(0xdada)\nnumFlips = 100\nprobHead = 0.6\ncoinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n  replace = TRUE, prob = c(probHead, 1 - probHead))\nhead(coinFlips)__\n\n\n[1] \"T\" \"T\" \"H\" \"T\" \"H\" \"H\"\nNow, if the coin were fair, we would expect half of the time to get heads. Let’s see.\ntable(coinFlips)__\n\n\ncoinFlips\n H  T \n59 41 \nSo that is different from 50/50. Suppose we showed the data to a friend without telling them whether the coin is fair, and their prior assumption, i.e., their null hypothesis, is that coins are, by and large, fair. Would the data be strong enough to make them conclude that this coin isn’t fair? They know that random sampling differences are to be expected. To decide, let’s look at the sampling distribution of our test statistic – the total number of heads seen in 100 coin tosses – for a fair coin5. As we saw in Chapter 1, the number, \\(k\\), of heads, in \\(n\\) independent tosses of a coin is\n5 We haven’t really defined what we mean be fair – a reasonable definition would be that head and tail are equally likely, and that the outcome of each coin toss does not depend on the previous ones. For more complex applications, nailing down the most suitable null hypothesis can take some thought.\n\\[ P(K=k,|,n, p) = (]\nwhere \\(p\\) is the probability of heads (0.5 if we assume a fair coin). We read the left hand side of the above equation as “the probability that the observed value for \\(K\\) is \\(k\\), given the values of \\(n\\) and \\(p\\)”. Statisticians like to make a difference between all the possible values of a statistic and the one that was observed6, and we use the upper case \\(K\\) for the possible values (so \\(K\\) can be anything between 0 and 100), and the lower case \\(k\\) for the observed value.\n6 In other words, \\(K\\) is the abstract random variable in our probabilistic model, whereas \\(k\\) is its realization, that is, a specific data point.\nWe plot Equation 6.3 in Figure 6.5; for good measure, we also mark the observed value numHeads with a vertical blue line.\nlibrary(\"dplyr\")\nk = 0:numFlips\nnumHeads = sum(coinFlips == \"H\")\nbinomDensity = tibble(k = k,\n     p = dbinom(k, size = numFlips, prob = 0.5))__\n\n\nlibrary(\"ggplot2\")\nggplot(binomDensity) +\n  geom_bar(aes(x = k, y = p), stat = \"identity\") +\n  geom_vline(xintercept = numHeads, col = \"blue\")__\n\nFigure 6.5: The binomial distribution for the parameters \\(n=100\\) and \\(p=0.5\\), according to Equation 6.3.\nSuppose we didn’t know about Equation 6.3. We can still use Monte Carlo simulation to give us something to compare with:\nnumSimulations = 10000\noutcome = replicate(numSimulations, {\n  coinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n                     replace = TRUE, prob = c(0.5, 0.5))\n  sum(coinFlips == \"H\")\n})\nggplot(tibble(outcome)) + xlim(-0.5, 100.5) +\n  geom_histogram(aes(x = outcome), binwidth = 1, center = 50) +\n  geom_vline(xintercept = numHeads, col = \"blue\")__\n\nFigure 6.6: An approximation of the binomial distribution from \\(10000\\) simulations (same parameters as Figure 6.5).\nAs expected, the most likely number of heads is 50, that is, half the number of coin flips. But we see that other numbers near 50 are also quite likely. How do we quantify whether the observed value, 59, is among those values that we are likely to see from a fair coin, or whether its deviation from the expected value is already large enough for us to conclude with enough confidence that the coin is biased? We divide the set of all possible \\(k\\) (0 to 100) in two complementary subsets, the rejection region and the region of no rejection. Our choice here7 is to fill up the rejection region with as many \\(k\\) as possible while keeping their total probability, assuming the null hypothesis, below some threshold \\(\\) (say, 0.05).\n7 More on this in Section 6.3.1.\nIn the code below, we use the function arrange from the dplyr package to sort the p-values from lowest to highest, then pass the result to mutate, which adds another dataframe column reject that is defined by computing the cumulative sum (cumsum) of the p-values and thresholding it against alpha. The logical vector reject therefore marks with TRUE a set of ks whose total probability is less than alpha. These are marked in Figure 6.7, and we can see that our rejection region is not contiguous – it comprises both the very large and the very small values of k.\nlibrary(\"dplyr\")\nalpha = 0.05\nbinomDensity = arrange(binomDensity, p) |&gt;\n        mutate(reject = (cumsum(p) &lt;= alpha))\n\nggplot(binomDensity) +\n  geom_bar(aes(x = k, y = p, col = reject), stat = \"identity\") +\n  scale_colour_manual(\n    values = c(`TRUE` = \"red\", `FALSE` = \"darkgrey\")) +\n  geom_vline(xintercept = numHeads, col = \"blue\") +\n  theme(legend.position = \"none\")__\n\nFigure 6.7: As Figure 6.5, with rejection region (red) that has been chosen such that it contains the maximum number of bins whose total area is at most \\(\\).\nThe explicit summation over the probabilities is clumsy, we did it here for pedagogic value. For one-dimensional distributions, R provides not only functions for the densities (e.g., dbinom) but also for the cumulative distribution functions (pbinom), which are more precise and faster than cumsum over the probabilities. These should be used in practice.\n__\nTask\nDo the computations for the rejection region and produce a plot like Figure 6.7 without using dbinom and cumsum, and with using pbinom instead.\nWe see in Figure 6.7 that the observed value, 59, lies in the grey shaded area, so we would not reject the null hypothesis of a fair coin from these data at a significance level of \\(\\).\n__\nQuestion 6.1\nDoes the fact that we don’t reject the null hypothesis mean that the coin is fair?\n__\nQuestion 6.2\nWould we have a better chance of detecting that the coin is not fair if we did more coin tosses? How many?\n__\nQuestion 6.3\nIf we repeated the whole procedure and again tossed the coin 100 times, might we then reject the null hypothesis?\n__\nQuestion 6.4\nThe rejection region in Figure 6.7 is asymmetric – its left part ends with \\(k=40\\), while its right part starts with \\(k=61\\). Why is that? Which other ways of defining the rejection region might be useful?\nWe have just gone through the steps of a binomial test. In fact, this is such a frequent activity in R that it has been wrapped into a single function, and we can compare its output to our results.\nbinom.test(x = numHeads, n = numFlips, p = 0.5)__\n\n\n    Exact binomial test\n\ndata:  numHeads and numFlips\nnumber of successes = 59, number of trials = 100, p-value = 0.08863\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4871442 0.6873800\nsample estimates:\nprobability of success \n                  0.59",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 Goals for this Chapter</span>"
    ]
  },
  {
    "objectID": "06-chap.html#the-five-steps-of-hypothesis-testing",
    "href": "06-chap.html#the-five-steps-of-hypothesis-testing",
    "title": "8  6.1 Goals for this Chapter",
    "section": "8.2 6.3 The five steps of hypothesis testing",
    "text": "8.2 6.3 The five steps of hypothesis testing\nLet’s summarise the general principles of hypothesis testing:\n\nDecide on the effect that you are interested in, design a suitable experiment or study, pick a data summary function and test statistic.\nSet up a null hypothesis , which is a simple, computationally tractable model of reality that lets you compute the null distribution , i.e., the possible outcomes of the test statistic and their probabilities under the assumption that the null hypothesis is true.\nDecide on the rejection region , i.e., a subset of possible outcomes whose total probability is small8.\nDo the experiment and collect the data9; compute the test statistic.\nMake a decision: reject the null hypothesis10 if the test statistic is in the rejection region.\n\n8 More on this in Section 6.3.1.\n9 Or if someone else has already done it, download their data.\n10 That is, conclude that it is unlikely to be true.\nNote how in this idealized workflow, we make all the important decisions in Steps 1–3 before we have even seen the data. As we already alluded to in the Introduction (Figures 1 and 2), this is often not realistic. We will also come back to this question in Section 6.6.\nThere was also idealization in our null hypothesis that we used in the example above: we postulated that a fair coin should have a probability of exactly 0.5 (not, say, 0.500001) and that there should be absolutely no dependence between tosses. We did not worry about any possible effects of air drag, elasticity of the material on which the coin falls, and so on. This gave us the advantage that the null hypothesis was computationally tractable, namely, with the binomial distribution. Here, these idealizations may not seem very controversial, but in other situations the trade-off between how tractable and how realistic a null hypothesis is can be more substantial. The problem is that if a null hypothesis is too idealized to start with, rejecting it is not all that interesting. The result may be misleading, and certainly we are wasting our time.\nThe test statistic in our example was the total number of heads. Suppose we observed 50 tails in a row, and then 50 heads in a row. Our test statistic ignores the order of the outcomes, and we would conclude that this is a perfectly fair coin. However, if we used a different test statistic, say, the number of times we see two tails in a row, we might notice that there is something funny about this coin.\n__\nQuestion 6.5\nWhat is the null distribution of this different test statistic?\n__\nQuestion 6.6\nWould a test based on that statistic be generally preferable?\n__\nSolution\n__\nNo, while it has more power to detect such correlations between coin tosses, it has less power to detect bias in the outcome.\nWhat we have just done is look at two different classes of alternative hypotheses. The first class of alternatives was that subsequent coin tosses are still independent of each other, but that the probability of heads differed from 0.5. The second one was that the overall probability of heads may still be 0.5, but that subsequent coin tosses were correlated.\n__\nQuestion 6.7\nRecall the concept of sufficient statistics from Chapter 1. Is the total number of heads a sufficient statistic for the binomial distribution? Why might it be a good test statistic for our first class of alternatives, but not for the second?\nSo let’s remember that we typically have multiple possible choices of test statistic (in principle it could be any numerical summary of the data). Making the right choice is important for getting a test with good power11. What the right choice is will depend on what kind of alternatives we expect. This is not always easy to know in advance.\n11 See Section 1.4.1 and 6.4.\n12 The assumptions don’t need to be exactly true – it is sufficient that the theory’s predictions are an acceptable approximation of the truth.\nOnce we have chosen the test statistic we need to compute its null distribution. You can do this either with pencil and paper or by computer simulations. A pencil and paper solution is parametric and leads to a closed form mathematical expression (like Equation 6.3), which has the advantage that it holds for a range of model parameters of the null hypothesis (such as \\(n\\), \\(p\\)). It can also be quickly computed for any specific set of parameters. But it is not always as easy as in the coin tossing example. Sometimes a pencil and paper solution is impossibly difficult to compute. At other times, it may require simplifying assumptions. An example is a null distribution for the \\(t\\)-statistic (which we will see later in this chapter). We can compute this if we assume that the data are independent and normally distributed: the result is called the \\(t\\)-distribution. Such modelling assumptions may be more or less realistic. Simulating the null distribution offers a potentially more accurate, more realistic and perhaps even more intuitive approach. The drawback of simulating is that it can take a rather long time, and we need extra work to get a systematic understanding of how varying parameters influence the result. Generally, it is more elegant to use the parametric theory when it applies12. When you are in doubt, simulate – or do both.\n\n8.2.1 6.3.1 The rejection region\nHow to choose the right rejection region for your test? First, what should its size be? That is your choice of the significance level or false positive rate \\(\\), which is the total probability of the test statistic falling into this region even if the null hypothesis is true13.\n13 Some people at some point in time for a particular set of questions colluded on \\(\\) as being “small”. But there is nothing special about this number, and in any particular case the best choice for a decision threshold may very much depend on context (Wasserstein and Lazar 2016; Altman and Krzywinski 2017).\nGiven the size, the next question is about its shape. For any given size, there are usually multiple possible shapes. It makes sense to require that the probability of the test statistic falling into the rejection region is as large possible if the alternative hypothesis is true. In other words, we want our test to have high power , or true positive rate.\nThe criterion that we used in the code for computing the rejection region for Figure 6.7 was to make the region contain as many k as possible. That is because in absence of any information about the alternative distribution, one k is as good as any other, and we maximize their total number.\nA consequence of this is that in Figure 6.7 the rejection region is split between the two tails of the distribution. This is because we anticipate that unfair coins could have a bias either towards head or toward tail; we don’t know. If we did know, we would instead concentrate our rejection region all on the appropriate side, e.g., the right tail if we think the bias would be towards head. Such choices are also referred to as two-sided and one-sided tests. More generally, if we have assumptions about the alternative distribution, this can influence our choice of the shape of the rejection region.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 Goals for this Chapter</span>"
    ]
  },
  {
    "objectID": "06-chap.html#types-of-error",
    "href": "06-chap.html#types-of-error",
    "title": "8  6.1 Goals for this Chapter",
    "section": "8.3 6.4 Types of error",
    "text": "8.3 6.4 Types of error\nHaving set out the mechanics of testing, we can assess how well we are doing. Table 6.1 compares reality (whether or not the null hypothesis is in fact true) with our decision whether or not to reject the null hypothesis after we have seen the data.\n\n\n\n\n\n\n\n\nTest vs reality\nNull hypothesis is true\n\\(…\\) is false\n\n\n\n\nReject null hypothesis\nType I error (false positive)\nTrue positive\n\n\nDo not reject\nTrue negative\nType II error (false negative)\n\n\n\nTable 6.1: Types of error in a statistical test.\nIt is always possible to reduce one of the two error types at the cost of increasing the other one. The real challenge is to find an acceptable trade- off between both of them. This is exemplified in Figure 6.2. We can always decrease the false positive rate (FPR) by shifting the threshold to the right. We can become more “conservative”. But this happens at the price of higher false negative rate (FNR). Analogously, we can decrease the FNR by shifting the threshold to the left. But then again, this happens at the price of higher FPR. A bit on terminology: the FPR is the same as the probability \\(\\) that we mentioned above. \\(1 - \\) is also called the specificity of a test. The FNR is sometimes also called \\(\\), and \\(1 - \\) the power , sensitivity or true positive rate of a test.\n__\nQuestion 6.8\nAt the end of Section 6.3, we learned about one- and two-sided tests. Why does this distinction exist? Why don’t we always just use the two-sided test, which is sensitive to a larger class of alternatives?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 Goals for this Chapter</span>"
    ]
  },
  {
    "objectID": "06-chap.html#the-t-test",
    "href": "06-chap.html#the-t-test",
    "title": "8  6.1 Goals for this Chapter",
    "section": "8.4 6.5 The t-test",
    "text": "8.4 6.5 The t-test\nMany experimental measurements are reported as rational numbers, and the simplest comparison we can make is between two groups, say, cells treated with a substance compared to cells that are not. The basic test for such situations is the \\(t\\)-test. The test statistic is defined as\n\\[ t = c ; , \\]\nwhere \\(m_1\\) and \\(m_2\\) are the mean of the values in the two groups, \\(s\\) is the pooled standard deviation and \\(c\\) is a constant that depends on the sample sizes, i.e., the numbers of observations \\(n_1\\) and \\(n_2\\) in the two groups. In formulas14,\n14 Everyone should try to remember Equation 6.4, whereas many people get by with looking up Equation 6.5 when they need it.\n\\[ \\[\\begin{align} m_g &= \\frac{1}{n_g} \\sum_{i=1}^{n_g} x_{g, i}\n\\quad\\quad\\quad g=1,2\\\\\\ s^2 &= \\frac{1}{n_1+n_2-2} \\left( \\sum_{i=1}^{n_1}\n\\left(x_{1,i} - m_1\\right)^2 + \\sum_{j=1}^{n_2} \\left(x_{2,j} - m_2\\right)^2\n\\right)\\\\\\ c &= \\sqrt{\\frac{n_1n_2}{n_1+n_2}} \\end{align}\\] \\]\nwhere \\(x_{g, i}\\) is the \\(i^{}\\) data point in the \\(g^{}\\) group. Let’s try this out with the PlantGrowth data from R’s datasets package.\nlibrary(\"ggbeeswarm\")\ndata(\"PlantGrowth\")\nggplot(PlantGrowth, aes(y = weight, x = group, col = group)) +\n  geom_beeswarm() + theme(legend.position = \"none\")\ntt = with(PlantGrowth, \n       t.test(weight[group ==\"ctrl\"],\n              weight[group ==\"trt2\"],\n              var.equal = TRUE))\ntt __\n\n\n    Two Sample t-test\n\ndata:  weight[group == \"ctrl\"] and weight[group == \"trt2\"]\nt = -2.134, df = 18, p-value = 0.04685\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.980338117 -0.007661883\nsample estimates:\nmean of x mean of y \n    5.032     5.526 \n\nFigure 6.8: The PlantGrowth data.\n__\nQuestion 6.9\nWhat do you get from the comparison with trt1? What for trt1 versus trt2?\nTo compute the p-value, the t.test function uses the asymptotic theory for the \\(t\\)-statistic Equation 6.4; this theory states that under the null hypothesis of equal means in both groups, the statistic follows a known, mathematical distribution, the so-called \\(t\\)-distribution with \\(n_1+n_2-2\\) degrees of freedom. The theory uses additional technical assumptions, namely that the data are independent and come from a normal distribution with the same standard deviation. We could be worried about these assumptions. Clearly they do not hold: weights are always positive, while the normal distribution extends over the whole real axis. The question is whether this deviation from the theoretical assumption makes a real difference. We can use a permutation test to figure this out (we will discuss the idea behind permutation tests in a bit more detail in Section 6.5.1).\n__\nQuestion 6.10\nPlot the theoretical and simulated null distribution for the above \\(t\\)-test, in a similar fashion as in Figure 6.5 and Figure 6.6 for the coin flips.\n__\nSolution\n__\nplgr = dplyr::filter(PlantGrowth, group %in% c(\"ctrl\", \"trt2\"))\n\nalpha  = 0.05\nxrange = 5 * c(-1, 1)\ndeckel = function(x) ifelse(x &lt; xrange[1], xrange[1], ifelse(x &gt; xrange[2], xrange[2], x))\n\nsim_null = tibble(\n  t = replicate(10000, t.test(weight ~ sample(group), var.equal = TRUE, data = plgr)$statistic)\n)\nsim_thresh = quantile(sim_null$t, c(alpha/2, 1-alpha/2))\nsim_null = mutate(sim_null, \n  t = deckel(t),        # avoid warnings about out of range data\n  reject = ifelse(t &lt;= sim_thresh[1], \"low\", ifelse(t &gt; sim_thresh[2], \"high\", \"none\"))\n) \n\ntheo_thresh = qt(c(alpha/2, 1-alpha/2), df =  nrow(plgr) - 2)\ntheo_null = tibble(\n  t = seq(-5, 5, by = 0.05),\n  density = dt (x = t, df = nrow(plgr)  - 2),\n  reject = ifelse(t &lt;= theo_thresh[1], \"low\", ifelse(t &gt; theo_thresh[2], \"high\", \"none\"))\n)\n\np1 = ggplot(sim_null, aes(x = t, col = reject, fill = reject)) +\n       geom_bar(stat = \"bin\", breaks = seq(-5, 5, by = 0.2)) \np2 = ggplot(theo_null, aes(x = t, y = density, col = reject, fill = reject)) +\n       geom_area() \n\nfor (p in list(p1, p2))\n  print(p + \n        geom_vline(xintercept = tt$statistic, col = \"#101010\") +\n        scale_colour_manual(values = c(low = \"blue\", high = \"red\", none = \"darkgrey\")) +\n        scale_fill_manual(values = c(low = \"blue\", high = \"red\", none = \"darkgrey\")) + \n        xlim(xrange) + theme(legend.position = \"none\"))__\n\nFigure 6.9\n\nFigure 6.10\n__\nQuestion 6.11\nWould the solution to the preceding question get simpler if we considered the absolute value of \\(t\\) instead of \\(t\\) itself?\n__\nSolution\n__\nYes, if we are not interested in the sign of the difference, we can directly work on \\((t)\\), and have only a single rejection region instead of a lower and an upper one.\nThe \\(t\\)-test comes in multiple flavors, all of which can be chosen through parameters of the t.test function. What we did above is called a two-sided two-sample unpaired test with equal variance. Two-sided refers to the fact that we were open to reject the null hypothesis if the weight of the treated plants was either larger or smaller than that of the untreated ones.\nTwo-sample 15 indicates that we compared the means of two groups to each other; another option is to compare the mean of one group against a given, fixed number.\n15 It can be confusing that the term sample has a different meaning in statistics than in biology. In biology, a sample is a single specimen on which an assay is performed; in statistics, it is a set of measurements, e.g., the \\(n_1\\)-tuple \\((x_{1,1},…,x_{1,n_1})\\) in Equation 6.5, which can comprise several biological samples. In contexts where this double meaning might create confusion, we refer to the data from a single biological sample as an observation.\nUnpaired means that there was no direct 1:1 mapping between the measurements in the two groups. If, on the other hand, the data had been measured on the same plants before and after treatment, then a paired test would be more appropriate, as it looks at the change of weight within each plant, rather than their absolute weights.\nEqual variance refers to the way the statistic Equation 6.4 is calculated. That expression is most appropriate if the variances within each group are about the same. If they are very different, an alternative form (Welch’s \\(t\\)-test) and associated asymptotic theory exist.\nThe independence assumption. Now let’s try something peculiar: duplicate the data.\nwith(rbind(PlantGrowth, PlantGrowth),\n       t.test(weight[group == \"ctrl\"],\n              weight[group == \"trt2\"],\n              var.equal = TRUE))__\n\n\n    Two Sample t-test\n\ndata:  weight[group == \"ctrl\"] and weight[group == \"trt2\"]\nt = -3.1007, df = 38, p-value = 0.003629\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.8165284 -0.1714716\nsample estimates:\nmean of x mean of y \n    5.032     5.526 \nNote how the estimates of the group means (and thus, of the difference) are unchanged, but the p-value is now much smaller! We can conclude two things from this:\n\nThe power of the \\(t\\)-test depends on the sample size. Even if the underlying biological differences are the same, a dataset with more observations tends to give more significant results16.\nThe assumption of independence between the measurements is really important. Blatant duplication of the same data is an extreme form of dependence, but to some extent the same thing happens if you mix up different levels of replication. For instance, suppose you had data from 8 plants, but measured the same thing twice on each plant (technical replicates), then pretending that these are now 16 independent measurements is wrong.\n\n16 You can also see this from the way the numbers \\(n_1\\) and \\(n_2\\) appear in Equation 6.5.\n\n8.4.1 6.5.1 Permutation tests\nWhat happened above when we contrasted the outcome of the parametric \\(t\\)-test with that of the permutation test applied to the \\(t\\)-statistic? It’s important to realize that these are two different tests, and the similarity of their outcomes is desirable, but coincidental. In the parametric test, the null distribution of the \\(t\\)-statistic follows from the assumed null distribution of the data, a multivariate normal distribution with unit covariance in the \\((n_1+n_2)\\)-dimensional space \\(^{n_1+n_2}\\), and is continuous: the \\(t\\)-distribution. In contrast, the permutation distribution of our test statistic is discrete, as it is obtained from the finite set of \\((n_1+n_2)!\\) permutations17 of the observation labels, from a single instance of the data (the \\(n_1+n_2\\) observations). All we assume here is that under the null hypothesis, the variables \\(X_{1,1},…,X_{1,n_1},X_{2,1},…,X_{2,n_2}\\) are exchangeable. Logically, this assumption is implied by that of the parametric test, but is weaker. The permutation test employs the \\(t\\)-statistic, but not the \\(t\\)-distribution (nor the normal distribution). The fact that the two tests gave us a very similar result is a consequence of the Central Limit Theorem.\n17 Or a random subset, in case we want to save computation time.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 Goals for this Chapter</span>"
    ]
  },
  {
    "objectID": "06-chap.html#p-value-hacking",
    "href": "06-chap.html#p-value-hacking",
    "title": "8  6.1 Goals for this Chapter",
    "section": "8.5 6.6 P-value hacking",
    "text": "8.5 6.6 P-value hacking\nLet’s go back to the coin tossing example. We did not reject the null hypothesis (that the coin is fair) at a level of 5%—even though we “knew” that it is unfair. After all, probHead was chosen as 0.6 in Section 6.2. Let’s suppose we now start looking at different test statistics. Perhaps the number of consecutive series of 3 or more heads. Or the number of heads in the first 50 coin flips. And so on. At some point we will find a test that happens to result in a small p-value, even if just by chance (after all, the probability for the p-value to be less than 0.05 under the null hypothesis—fair coin—is one in twenty). We just did what is called p-value hacking 18 (Head et al. 2015). You see what the problem is: in our zeal to prove our point we tortured the data until some statistic did what we wanted. A related tactic is hypothesis switching or HARKing – hypothesizing after the results are known: we have a dataset, maybe we have invested a lot of time and money into assembling it, so we need results. We come up with lots of different null hypotheses and test statistics, test them, and iterate, until we can report something.\n18 http://fivethirtyeight.com/features/science-isnt-broken\nThese tactics violate the rules of hypothesis testing, as described in Section 6.3, where we laid out one sequential procedure of choosing the hypothesis and the test, and then collecting the data. But, as we saw in Chapter 2, such tactics can be tempting in reality. With biological data, we tend to have so many different choices for “normalising” the data, transforming the data, trying to adjust for batch effects, removing outliers, …. The topic is complex and open-ended. Wasserstein and Lazar (2016) give a readable short summary of the problems with how p-values are used in science, and of some of the misconceptions. They also highlight how p-values can be fruitfully used. The essential message is: be completely transparent about your data, what analyses were tried, and how they were done. Provide the analysis code. Only with such contextual information can a p-value be useful.\nAvoid fallacy. Keep in mind that our statistical test is never attempting to prove our null hypothesis is true - we are simply saying whether or not there is evidence for it to be false. If a high p-value were indicative of the truth of the null hypothesis, we could formulate a completely crazy null hypothesis, do an utterly irrelevant experiment, collect a small amount of inconclusive data, find a p-value that would just be a random number between 0 and 1 (and so with some high probability above our threshold \\(\\)) and, whoosh, our hypothesis would be demonstrated!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 Goals for this Chapter</span>"
    ]
  },
  {
    "objectID": "06-chap.html#multiple-testing",
    "href": "06-chap.html#multiple-testing",
    "title": "8  6.1 Goals for this Chapter",
    "section": "8.6 6.7 Multiple testing",
    "text": "8.6 6.7 Multiple testing\n__\nQuestion 6.12\nLook up xkcd cartoon 882. Why didn’t the newspaper report the results for the other colors?\nThe quandary illustrated in the cartoon occurs with high-throughput data in biology. And with force! You will be dealing not only with 20 colors of jellybeans, but, say, with 20,000 genes that were tested for differential expression between two conditions, or with 6 billion positions in the genome where a DNA mutation might have happened. So how do we deal with this? Let’s look again at our table relating statistical test results with reality (Table 6.1), this time framing everything in terms of many hypotheses.\n\n\n\n\n\n\n\n\n\nTest vs reality\nNull hypothesis is true\n\\(…\\) is false\nTotal\n\n\n\n\nRejected\n\\(V\\)\n\\(S\\)\n\\(R\\)\n\n\nNot rejected\n\\(U\\)\n\\(T\\)\n\\(m-R\\)\n\n\nTotal\n\\(m_0\\)\n\\(m-m_0\\)\n\\(m\\)\n\n\n\nTable 6.2: Types of error in multiple testing. The letters designate the number of times each type of error occurs.\n\n\\(m\\): total number of tests (and null hypotheses)\n\\(m_0\\): number of true null hypotheses\n\\(m-m_0\\): number of false null hypotheses\n\\(V\\): number of false positives (a measure of type I error)\n\\(T\\): number of false negatives (a measure of type II error)\n\\(S\\), \\(U\\): number of true positives and true negatives\n\\(R\\): number of rejections\n\nIn the rest of this chapter, we look at different ways of taking care of the type I and II errors.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 Goals for this Chapter</span>"
    ]
  },
  {
    "objectID": "06-chap.html#the-family-wise-error-rate",
    "href": "06-chap.html#the-family-wise-error-rate",
    "title": "8  6.1 Goals for this Chapter",
    "section": "8.7 6.8 The family wise error rate",
    "text": "8.7 6.8 The family wise error rate\nThe family wise error rate (FWER) is the probability that \\(V&gt;0\\), i.e., that we make one or more false positive errors. We can compute it as the complement of making no false positive errors at all19.\n19 Assuming independence.\n\\[ \\[\\begin{align} P(V&gt;0) &= 1 - P(\\text{no rejection of any of $m_0$ nulls})\n\\\\\\ &= 1 - (1 - \\alpha)^{m_0} \\to 1 \\quad\\text{as } m_0\\to\\infty. \\end{align}\\] \\]\nFor any fixed \\(\\), this probability is appreciable as soon as \\(m_0\\) is in the order of \\(1/\\), and it tends towards 1 as \\(m_0\\) becomes larger. This relationship can have serious consequences for experiments like DNA matching, where a large database of potential matches is searched. For example, if there is a one in a million chance that the DNA profiles of two people match by random error, and your DNA is tested against a database of 800000 profiles, then the probability of a random hit with the database (i.e., without you being in it) is:\n1 - (1 - 1/1e6)^8e5 __\n\n\n[1] 0.5506712\nThat’s pretty high. And once the database contains a few million profiles more, a false hit is virtually unavoidable.\n__\nQuestion 6.13\nProve that the probability Equation 6.6 does indeed become very close to 1 when \\(m_0\\) is large.\n\n8.7.1 6.8.1 Bonferroni method\nHow are we to choose the per-hypothesis \\(\\) if we want FWER control? The above computations suggest that the product of \\(\\) with \\(m_0\\) may be a reasonable ballpark estimate. Usually we don’t know \\(m_0\\), but we know \\(m\\), which is an upper limit for \\(m_0\\), since \\(m_0m\\). The Bonferroni method is simply that if we want FWER control at level \\({}\\), we should choose the per hypothesis threshold \\(= {}/m\\). Let’s check this out on an example.\nm = 10000\nggplot(tibble(\n  alpha = seq(0, 7e-6, length.out = 100),\n  p     = 1 - (1 - alpha)^m),\n  aes(x = alpha, y = p)) +  geom_line() +\n  xlab(expression(alpha)) +\n  ylab(\"Prob( no false rejection )\") +\n  geom_hline(yintercept = 0.05, col = \"red\")__\n\nFigure 6.11: Bonferroni method. The plot shows the graph of 6.6 for \\(m=10000\\) as a function of \\(\\).\nIn Figure 6.11, the black line intersects the red line (which corresponds to a value of 0.05) at \\(^{-6}\\), which is just a little bit more than the value of \\(0.05/m\\) implied by the Bonferroni method.\n__\nQuestion 6.14\nWhy are the two values not exactly the same?\nA potential drawback of this method, however, is that if \\(m_0\\) is large, the rejection threshold is very small. This means that the individual tests need to be very powerful if we want to have any chance of detecting something. Often FWER control is too stringent, and would lead to an ineffective use of the time and money that was spent to generate and assemble the data. We will now see that there are more nuanced methods of controlling our type I error.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 Goals for this Chapter</span>"
    ]
  },
  {
    "objectID": "06-chap.html#the-false-discovery-rate",
    "href": "06-chap.html#the-false-discovery-rate",
    "title": "8  6.1 Goals for this Chapter",
    "section": "8.8 6.9 The false discovery rate",
    "text": "8.8 6.9 The false discovery rate\nLet’s look at some data. We load up the RNA-Seq dataset airway, which contains gene expression measurements (gene-level counts) of four primary human airway smooth muscle cell lines with and without treatment with dexamethasone, a synthetic glucocorticoid. We’ll use the DESeq2 method that we’ll discuss in more detail in Chapter 8 For now it suffices to say that it performs a test for differential expression for each gene. Conceptually, the tested null hypothesis is similar to that of the \\(t\\)-test, although the details are slightly more involved since we are dealing with count data.\nlibrary(\"DESeq2\")\nlibrary(\"airway\")\ndata(\"airway\")\naw   = DESeqDataSet(se = airway, design = ~ cell + dex)\naw   = DESeq(aw)\nawde = as.data.frame(results(aw)) |&gt; dplyr::filter(!is.na(pvalue))__\n__\nTask\nHave a look at the content of awde.\n__\nTask\n(Optional) Consult the DESeq2 vignette and/or Chapter 8 for more information on what the above code chunk does.\n\n8.8.1 6.9.1 The p-value histogram\nThe p-value histogram is an important sanity check for any analysis that involves multiple tests. It is a mixture composed of two components:\nnull: the p-values resulting from the tests for which the null hypothesis is true.\nalt: the p-values resulting from the tests for which the null hypothesis is not true. The relative size of these two components depends on the fraction of true nulls and true alternatives (i.e., on \\(m_0\\) and \\(m\\)), and it can often be visually estimated from the histogram. If our analysis has high statistical power, then the second component (“alt”) consists of mostly small p-values, i.e., appears as a peak near 0 in the histogram; if the power is not high for some of the alternatives, we expect that this peak extends towards the right, i.e., has a “shoulder”. For the “null” component, we expect (by definition of the p-value for continuous data and test statistics) a uniform distribution in \\([0,1]\\). Let’s plot the histogram of p-values for the airway data.\nggplot(awde, aes(x = pvalue)) +\n  geom_histogram(binwidth = 0.025, boundary = 0)__\n\nFigure 6.12: p-value histogram of for the airway data.\nIn Figure 6.12 we see the expected mixture. We also see that the null component is not exactly flat (uniform): this is because the data are counts. While these appear quasi-continuous when high, for the tests with low counts the discreteness of the data and the resulting p-values shows up in the spikes towards the right of the histogram.\nNow suppose we reject all tests with a p-value less than \\(\\). We can visually determine an estimate of the false discovery proportion with a plot such as in Figure 6.13, generated by the following code.\nalpha = binw = 0.025\npi0 = 2 * mean(awde$pvalue &gt; 0.5)\nggplot(awde,\n  aes(x = pvalue)) + geom_histogram(binwidth = binw, boundary = 0) +\n  geom_hline(yintercept = pi0 * binw * nrow(awde), col = \"blue\") +\n  geom_vline(xintercept = alpha, col = \"red\")__\n\nFigure 6.13: Visual estimation of the FDR with the p-value histogram.\nWe see that there are 4772 p-values in the first bin \\([0,]\\), among which we expect around 945 to be nulls (as indicated by the blue line). Thus we can estimate the fraction of false rejections as\npi0 * alpha / mean(awde$pvalue &lt;= alpha)__\n\n\n[1] 0.1980092\nThe false discovery rate (FDR) is defined as\n\\[ = \\!, \\]\nwhere \\(R\\) and \\(V\\) are as in Table 6.2. The expression in the denominator makes sure that the FDR is well-defined even if \\(R=0\\) (in that case, \\(V=0\\) by implication). Note that the FDR becomes identical to the FWER if all null hypotheses are true, i.e., if \\(V=R\\). \\(\\) stands for the expected value. That means that the FDR is not a quantity associated with a specific outcome of \\(V\\) and \\(R\\) for one particular experiment. Rather, given our choice of tests and associated rejection rules for them, it is the average20 proportion of type I errors out of the rejections made, where the average is taken (at least conceptually) over many replicate instances of the experiment.\n20 Since the FDR is an expectation value, it does not provide worst case control: in any single experiment, the so-called false discovery proportion (FDP), that is the realized value \\(v/r\\) (without the \\(\\)), could be much higher or lower.\n\n\n8.8.2 6.9.2 The Benjamini-Hochberg algorithm for controlling the FDR\nThere is a more elegant alternative to the “visual FDR” method of the last section. The procedure, introduced by Benjamini and Hochberg (1995) has these steps:\n\nFirst, order the p-values in increasing order, \\(p_{(1)} … p_{(m)}\\)\nThen for some choice of \\(\\) (our target FDR), find the largest value of \\(k\\) that satisfies: \\(p_{(k)} , k / m\\)\nFinally reject the hypotheses \\(1, …, k\\)\n\nWe can see how this procedure works when applied to our RNA-Seq p-values through a simple graphical illustration:\nphi  = 0.10\nawde = mutate(awde, rank = rank(pvalue))\nm    = nrow(awde)\n\nggplot(dplyr::filter(awde, rank &lt;= 7000), aes(x = rank, y = pvalue)) +\n  geom_line() + geom_abline(slope = phi / m, col = \"red\")__\n\nFigure 6.14: Visualization of the Benjamini-Hochberg procedure. Shown is a zoom-in to the 7000 lowest p-values.\nThe method finds the rightmost point where the black (our p-values) and red lines (slope \\(/ m\\)) intersect. Then it rejects all tests to the left.\nkmax = with(arrange(awde, rank),\n         last(which(pvalue &lt;= phi * rank / m)))\nkmax __\n\n\n[1] 4099\n__\nQuestion 6.15\nCompare the value of kmax with the number of 4772 from above (Figure 6.13). Why are they different?\n__\nQuestion 6.16\nLook at the code associated with the option method=\"BH\" of the p.adjust function that comes with R. How does it compare to what we did above?\n__\nQuestion 6.17\nSchweder and Spj øtvoll plot: check out Figures 1–3 in Schweder and Spjøtvoll (1982). Make a similar plot for the data in awde. How does it relate to Figures 6.14 and 6.13?\n__\nSolution\n__\nThirteen years before Benjamini and Hochberg (1995), Schweder and Spjøtvoll (1982) suggested a diagnostic plot of the observed \\(p\\)-values that permits estimation of the fraction of true null hypotheses. For a series of hypothesis tests \\(H_1, …, H_m\\) with \\(p\\)-values \\(p_i\\), they suggested plotting\n\\[ ( 1-p_i, N(p_i) ) i , …, m, \\]\nwhere \\(N(p)\\) is the number of \\(p\\)-values greater than \\(p\\). An application of this diagnostic plot to awde$pvalue is shown in Figure 6.15. When all null hypotheses are true, each of the \\(p\\)-values is uniformly distributed in \\([0,1]\\), Consequently, the empirical cumulative distribution of the sample \\((p_1, …, p_m)\\) is expected to be close to the line \\(F(t)=t\\). By symmetry, the same applies to \\((1 - p_1, …, 1 - p_m)\\). When (without loss of generality) the first \\(m_0\\) null hypotheses are true and the other \\(m-m_0\\) are false, the empirical cumulative distribution of \\((1-p_1, …, 1-p_{m_0})\\) is again expected to be close to the line \\(F_0(t)=t\\). The empirical cumulative distribution of \\((1-p_{m_0+1}, …, 1-p_{m})\\), on the other hand, is expected to be close to a function \\(F_1(t)\\) which stays below \\(F_0\\) but shows a steep increase towards 1 as \\(t\\) approaches \\(1\\). In practice, we do not know which of the null hypotheses are true, so we only observe a mixture whose empirical cumulative distribution is expected to be close to\n\\[ F(t) = F_0(t) + F_1(t). \\]\nSuch a situation is shown in Figure 6.15. If \\(F_1(t)/F_0(t)\\) is small for small \\(t\\) (i.e., the tests have reasonable power), then the mixture fraction \\(\\) can be estimated by fitting a line to the left- hand portion of the plot, and then noting its height on the right. Such a fit is shown by the red line. Here, we focus on those tests for which the count data are not all very small numbers (baseMean&gt;=1), since for these the p-value null distribution is sufficiently close to uniform (i.e., does not show the discreteness mentioned above), but you could try the making the same plot on all of the genes.\nawdef = awde |&gt;\n  dplyr::filter(baseMean &gt;=1) |&gt; \n  arrange(pvalue) |&gt;\n  mutate(oneminusp = 1 - pvalue,\n         N = n() - row_number())\njj = round(nrow(awdef) * c(1, 0.5))\nslope = with(awdef, diff(N[jj]) / diff(oneminusp[jj]))\nggplot(awdef) +\n  geom_point(aes(x = oneminusp, y = N), size = 0.15) + \n  xlab(expression(1-p[i])) +\n  ylab(expression(N(p[i]))) +\n  geom_abline(intercept = 0, slope = slope, col = \"red3\") +\n  geom_hline(yintercept = slope, linetype = \"dotted\") +\n  geom_vline(xintercept = 1, linetype = \"dotted\") +\n  geom_text(x = 0, y = slope, label = paste(round(slope)), \n            hjust = -0.1, vjust = -0.25) __\n\nFigure 6.15: Schweder and Spjtvoll plot, as described in the answer to Question 6.17.\nThere are 22853 rows in awdef, thus, according to this simple estimate, there are 22853-17302=5551 alternative hypotheses.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 Goals for this Chapter</span>"
    ]
  },
  {
    "objectID": "06-chap.html#the-local-fdr",
    "href": "06-chap.html#the-local-fdr",
    "title": "8  6.1 Goals for this Chapter",
    "section": "8.9 6.10 The local FDR",
    "text": "8.9 6.10 The local FDR\n\nFigure 6.16: From http://xkcd.com/1132 – While the frequentist only has the currently available data, the Bayesian can draw on her understanding of the world or on previous experience. As a Bayesian, she would know enough about physics to understand that our sun’s mass is too small to become a nova. Even if she does not know physics, she might be an empirical Bayesian and draw her prior from a myriad previous days where the sun did not go nova.\nWhile the xkcd cartoon in the chapter’s opening figure ends with a rather sinister interpretation of the multiple testing problem as a way to accumulate errors, Figure 6.16 highlights the multiple testing opportunity: when we do many tests, we can use the multiplicity to increase our understanding beyond what’s possible with a single test.\n\nFigure 6.17: Local false discovery rate and the two-group model, with some choice of \\(f_{}(p)\\), and \\(_0=0.6\\); densities (top) and distribution functions (bottom).\nLet’s get back to the histogram in Figure 6.13. Conceptually, we can think of it in terms of the so-called two-groups model (Efron 2010):\n\\[ f(p)= _0 + (1-0) f{}(p), \\]\nHere, \\(f(p)\\) is the density of the distribution (what the histogram would look like with an infinite amount of data and infinitely small bins), \\(0\\) is a number between 0 and 1 that represents the size of the uniform component, and \\(f{}\\) is the alternative component. This is a mixture model, as we already saw in Chapter 4. The mixture densities and the marginal density \\(f(p)\\) are visualized in the upper panel of Figure 6.17: the blue areas together correspond to the graph of \\(f_{}(p)\\), the grey areas to that of \\(f_{}(p) = _0\\). If we now consider one particular cutoff \\(p\\) (say, \\(p=0.1\\) as in Figure 6.17), then we can compute the probability that a hypothesis that we reject at this cutoff is a false positive, as follows. We decompose the value of \\(f\\) at the cutoff (red line) into the contribution from the nulls (light red, \\(_0\\)) and from the alternatives (darker red, \\((1-0) f{}(p)\\)). The local false discovery rate is then\n\\[ (p) = . \\]\nBy definition this quantity is between 0 and 1. Note how the \\(\\) in Figure 6.17 is a monotonically increasing function of \\(p\\), and this goes with our intuition that the fdr should be lowest for the smallest \\(p\\) and then gradually get larger, until it reaches 1 at the very right end. We can make a similar decomposition not only for the red line, but also for the area under the curve. This is\n\\[ F(p) = _0^p f(t),dt, \\]\nand the ratio of the dark grey area (that is, \\(_0\\) times \\(p\\)) to the overall area \\(F(p)\\) is the tail area false discovery rate (Fdr21),\n21 The convention is to use the lower case abbreviation fdr for the local, and the abbreviation Fdr for the tail-area false discovery rate in the context of the two-groups model Equation 6.10. The abbreviation FDR is used for the original definition Equation 6.7, which is a bit more general, namely, it does not depend on the modelling assumptions of Equation 6.10.\n\\[ (p) = . \\]\nWe’ll use the data version of \\(F\\) for diagnostics in Figure 6.21.\nThe packages qvalue and fdrtool offer facilities to fit these models to data.\nlibrary(\"fdrtool\")\nft = fdrtool(awde$pvalue, statistic = \"pvalue\")__\nIn fdrtool , what we called \\(_0\\) above is called eta0:\nft$param[,\"eta0\"]__\n\n\n     eta0 \n0.8822922 \n__\nQuestion 6.18\nWhat do the plots that are produced by the above call to fdrtool show?\n__\nTask\nExplore the other elements of the list ft.\n__\nQuestion 6.19\nWhat does the empirical in empirical Bayes methods stand for?\n\n8.9.1 6.10.1 Local versus total\nThe FDR (or the Fdr) is a set property. It is a single number that applies to a whole set of rejections made in the course of a multiple testing analysis. In contrast, the fdr is a local property. It applies to an individual hypothesis. Recall Figure 6.17, where the fdr was computed for each point along the \\(x\\)-axis of the density plot, whereas the Fdr depends on the areas to the left of the red line.\n__\nQuestion 6.20\nCheck out the concepts of total cost and marginal cost in economics. Can you see an analogy with Fdr and fdr?\n__\nSolution\n__\nFor a production process that produces a set of \\(m\\) products, the total cost is the sum of the all costs involved. The average cost of a product is a hypothetical quantity, computed as the total cost divided by \\(m\\). The marginal cost is the cost of making one additional product, and is often very different from the average cost. For instance, learning to play a single Beethoven sonata on the piano may take an uninitiated person a substantial amount of time, but then playing it once more requires comparatively little additional effort: the marginal costs are much less than the fixed (and thus the total) costs. An example for marginal costs that are higher than the average costs is running: putting on your shoes and going out for a 10km run may be quite tolerable (perhaps even fun) to most people, whereas each additional 10km could add disproportional discomfort.\n\n\n8.9.2 6.10.2 Terminology\nHistorically, the terms multiple testing correction and adjusted p-value have been used for process and output. In the context of false discovery rates, these terms are not helpful, if not confusing. We advocate avoiding them. They imply that we start out with a set of p-values \\((p_1,…,p_m)\\), apply some canonical procedure, and obtain a set of “corrected” or “adjusted” p-values \\((p_1{},…,p_m{})\\). However, the output of the Benjamini-Hochberg method is not p-values, and neither are the FDR, Fdr or the fdr. Remember that FDR and Fdr are set properties, and associating them with an individual test makes as much sense as confusing average and marginal costs. Fdr and fdr also depend on a substantial amount of modelling assumptions. In the next session, you will also see that the method of Benjamini-Hochberg is not the only game in town, and that there are important and useful extensions, which further displace any putative direct correspondence between the set of hypotheses and p-values that are input into a multiple testing procedure, and its outputs.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 Goals for this Chapter</span>"
    ]
  },
  {
    "objectID": "06-chap.html#independent-hypothesis-weighting",
    "href": "06-chap.html#independent-hypothesis-weighting",
    "title": "8  6.1 Goals for this Chapter",
    "section": "8.10 6.11 Independent hypothesis weighting",
    "text": "8.10 6.11 Independent hypothesis weighting\nThe Benjamini-Hochberg method and the two-groups model, as we have seen them so far, implicitly assume exchangeability of the hypotheses: all we use are the p-values. Beyond these, we do not take into account any additional information. This is not always optimal, and here we’ll study ways of how to improve on this.\nLet’s look at an example. Intuitively, the signal-to-noise ratio for genes with larger numbers of reads mapped to them should be better than for genes with few reads, and that should affect the power of our tests. We look at the mean of normalized counts across observations. In the DESeq2 package this quantity is called the baseMean.\nawde$baseMean[1]__\n\n\n[1] 708.6022\n\n\ncts = counts(aw, normalized = TRUE)[1, ]\ncts __\n\n\nSRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 \n  663.3142   499.9070   740.1528   608.9063   966.3137   748.3722   836.2487 \nSRR1039521 \n  605.6024 \n\n\nmean(cts)__\n\n\n[1] 708.6022\nNext we produce the histogram of this quantity across genes, and plot it against the p-values (Figures 6.18 and 6.19).\nggplot(awde, aes(x = asinh(baseMean))) +\n  geom_histogram(bins = 60)__\n\nFigure 6.18: Histogram of baseMean. We see that it covers a large dynamic range, from close to 0 to around 330000.\nggplot(awde, aes(x = rank(baseMean), y = -log10(pvalue))) +\n  geom_hex(bins = 60) +\n  theme(legend.position = \"none\")__\n\nFigure 6.19: Scatterplot of the rank of baseMean versus the negative logarithm of the p-value. For small values of baseMean, no small p-values occur. Only for genes whose read counts across all observations have a certain size, the test for differential expression has power to come out with a small p-value.\n__\nQuestion 6.21\nWhy did we use the \\(\\) transformation for the histogram? How does it look like with no transformation, the logarithm, the shifted logarithm, i.e., \\((x+)\\)?\n__\nQuestion 6.22\nIn the scatterplot, why did we use \\(-_{10}\\) for the p-values? Why the rank transformation for the baseMean?\nFor convenience, we discretize baseMean into a factor variable group, which corresponds to six equal-sized groups.\nawde = mutate(awde, stratum = cut(baseMean, include.lowest = TRUE,\n  breaks = signif(quantile(baseMean,probs=seq(0,1,length.out=7)),2)))__\nIn Figures 6.20 and 6.21 we see the histograms of p-values and the ECDFs stratified by stratum.\nggplot(awde, aes(x = pvalue)) + facet_wrap( ~ stratum, nrow = 4) +\n  geom_histogram(binwidth = 0.025, boundary = 0)__\n\nFigure 6.20: p-value histograms of the airway data, stratified into equally sized groups defined by increasing value of baseMean.\nggplot(awde, aes(x = pvalue, col = stratum)) +\n  stat_ecdf(geom = \"step\") + theme(legend.position = \"bottom\")__\n\nFigure 6.21: Same data as in Figure 6.20, shown with ECDFs.\nIf we were to fit the two-group model to these strata separately, we would get quite different estimates for \\(0\\) and \\(f{}\\). For the most lowly expressed genes, the power of the DESeq2 -test is low, and the p-values essentially all come from the null component. As we go higher in average expression, the height of the small-p-values peak in the histograms increases, reflecting the increasing power of the test.\nCan we use that to improve our handling of the multiple testing? It turns out that this is possible. One approach is independent hypothesis weighting (IHW) (Ignatiadis et al. 2016; Ignatiadis and Huber 2021)22.\n22 There are a number of other approaches, see e.g., a benchmark study by Korthauer et al. (2019) or the citations in the paper by Ignatiadis and Huber (2021).\nlibrary(\"IHW\")\nihw_res = ihw(awde$pvalue, awde$baseMean, alpha = 0.1)\nrejections(ihw_res)__\n\n\n[1] 4892\nLet’s compare this to what we get from the ordinary (unweighted) Benjamini- Hochberg method:\npadj_BH = p.adjust(awde$pvalue, method = \"BH\")\nsum(padj_BH &lt; 0.1)__\n\n\n[1] 4099\nWith hypothesis weighting, we get more rejections. For these data, the difference is notable though not spectacular; this is because their signal-to- noise ratio is already quite high. In other situations, where there is less power to begin with (e.g., where there are fewer replicates, the data are more noisy, or the effect of the treatment is less drastic), the difference from using IHW can be more pronounced.\nWe can have a look at the weights determined by the ihw function (Figure 6.22).\nplot(ihw_res)__\n\nFigure 6.22: Hypothesis weights determined by the ihw function. Here the function’s default settings chose 22 strata, while in our manual exploration above (Figures 6.20, 6.21) we had used 6; in practice, this is a minor detail.\nIntuitively, what happens here is that IHW chooses to put more weight on the hypothesis strata with higher baseMean, and low weight on those with very low counts. The Benjamini-Hochberg method has a certain type-I error budget, and rather than spreading it equally among all hypotheses, here we take it away from those strata that have little change of small fdr anyway, and “invest” it in strata where many hypotheses can be rejected at small fdr.\n__\nQuestion 6.23\nWhy does Figure 6.22 show 5 curves, rather than only one?\nSuch possibilities for stratification by an additional summary statistic besides the p-value—in our case, the baseMean—exist in many multiple testing situations. Informally, we need such a so-called covariate to be\n\nstatistically independent from our p-values under the null, but\ninformative of the prior probability \\(0\\) and/or the power of the test (the shape of the alternative density, \\(f{}\\)) in the two-groups model.\n\nThese requirements can be assessed through diagnostic plots as in Figures 6.18—6.21.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 Goals for this Chapter</span>"
    ]
  },
  {
    "objectID": "06-chap.html#summary-of-this-chapter",
    "href": "06-chap.html#summary-of-this-chapter",
    "title": "8  6.1 Goals for this Chapter",
    "section": "8.11 6.12 Summary of this chapter",
    "text": "8.11 6.12 Summary of this chapter\nWe have explored the concepts behind single hypothesis testing and then moved on to multiple testing. We have seen how some of the limitations of interpreting a single p-value from a single test can be overcome once we are able to consider a whole distribution of outcomes from many tests. We have also seen that there are often additional summary statistics of our data, besides the p-values. We called them informative covariates, and we saw how we can use them to weigh the p-values and overall get more (or better) discoveries.\nThe usage of hypothesis testing in the multiple testing scenario is quite different from that in the single test case: for the latter, the hypothesis test might literally be the final result, the culmination of a long and expensive data acquisition campaign (ideally, with a prespecified hypothesis and data analysis plan). In the multiple testing case, its outcome will often just be an intermediate step: a subset of most worthwhile hypotheses selected by screening a large initial set. This subset is then followed up by more careful analyses.\nWe have seen the concept of the false discovery rate (FDR). It is important to keep in mind that this is an average property, for the subset of hypotheses that were selected. Like other averages, it does not say anything about the individual hypotheses. Then there is the concept of the local false discovery rate (fdr), which indeed does apply to an individual hypothesis. The local false discovery rate is however quite unrelated to the p-value, as the two- group model showed us. Much of the confusion and frustration about p-values seems to come from the fact that people would like to use them for purposes that the fdr is made for. It is perhaps a historical aberration that so much of applied sciences focuses on p-values and not local false discovery rate. On the other hand, there are also practical reasons, since a p-value is readily computed, whereas a fdr is difficult to estimate or control from data without making strong modelling assumptions.\nWe saw the importance of diagnostic plots, in particular, to always look at the p-value histograms when encountering a multiple testing analysis.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 Goals for this Chapter</span>"
    ]
  },
  {
    "objectID": "06-chap.html#further-reading",
    "href": "06-chap.html#further-reading",
    "title": "8  6.1 Goals for this Chapter",
    "section": "8.12 6.13 Further reading",
    "text": "8.12 6.13 Further reading\n\nA comprehensive text book treatment of multiple testing is given by Efron (2010).\nOutcome switching in clinical trials: http://compare-trials.org\nFor hypothesis weighting, the IHW vignette, the IHW paper (Ignatiadis et al. 2016) and the references therein.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 Goals for this Chapter</span>"
    ]
  },
  {
    "objectID": "06-chap.html#exercises",
    "href": "06-chap.html#exercises",
    "title": "8  6.1 Goals for this Chapter",
    "section": "8.13 6.14 Exercises",
    "text": "8.13 6.14 Exercises\n__\nExercise 6.1\nIdentify an application from your scientific field of expertise that relies on multiple testing. Find an exemplary dataset and plot the histogram of p-values. Are the hypotheses all exchangeable, or is there one or more informative covariates? Plot the stratified histograms.\n__\nExercise\nWhy do mathematical statisticians focus so much on the null hypothesis of a test, compared to the alternative hypothesis?\n__\nExercise 6.2\nHow can we ever prove that the null hypothesis is true? Or that the alternative is true?\n__\nExercise 6.3\nMake a less extreme example of correlated test statistics than the data duplication at the end of Section 6.5. Simulate data with true null hypotheses only, and let the data morph from having completely independent replicates (columns) to highly correlated as a function of some continuous-valued control parameter. Check type-I error control (e.g., with the p-value histogram) as a function of this control parameter.\n__\nExercise 6.4\nFind an example in the published literature that looks as if p-value hacking, outcome switching, HARKing played a role.\n__\nExercise 6.5\nThe FDR is an expectation value, i.e., it is used if we want to control the average behavior of a procedure. Are there methods for worst case control?\n__\nExercise 6.6\nWhat is the memory and time complexity of the Benjamini-Hochberg algorithm? How about the IHW method? Can you fit polynomial functions as a function of the number of tests \\(m\\)? Hint: Simulate data with increasing numbers of hypothesis tests, measure time and memory consumption with functions such as pryr::object_size or microbenchmark from the eponymous package, and plot these against \\(m\\) in a double-logarithmic plot.\nAltman, Naomi, and Martin Krzywinski. 2017. “Points of Significance: Interpreting p Values.” Nature Methods 14 (3): 213–14. https://doi.org/10.1038/nmeth.4210.\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society B 57: 289–300.\nEfron, Bradley. 2010. Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction. Cambridge University Press.\nHead, Megan L, Luke Holman, Rob Lanfear, Andrew T Kahn, and Michael D Jennions. 2015. “The Extent and Consequences of p-Hacking in Science.” PLoS Biology 13 (3): e1002106.\nIgnatiadis, Nikolaos, and Wolfgang Huber. 2021. “Covariate Powered Cross- Weighted Multiple Testing.” Journal of the Royal Statistical Society: Series B 83: 720–51. https://doi.org/10.1111/rssb.12411.\nIgnatiadis, Nikolaos, Bernd Klaus, Judith Zaugg, and Wolfgang Huber. 2016. “Data-Driven Hypothesis Weighting Increases Detection Power in Genome-Scale Multiple Testing.” Nature Methods 13: 577–80.\nKorthauer, K., P. K. Kimes, C. Duvallet, A. Reyes, A. Subramanian, M. Teng, C. Shukla, E. J. Alm, and S. C. Hicks. 2019. “A practical guide to methods controlling false discoveries in computational biology.” Genome Biology 20 (1): 118.\nSchweder, T., and E. Spjøtvoll. 1982. “Plots of P-values to Evaluate Many Tests Simultaneously.” Biometrika 69: 493–502. https://doi.org/10.1093/biomet/69.3.493.\nStorey, John D. 2003. “The Positive False Discovery Rate: A Bayesian Interpretation and the q-Value.” The Annals of Statistics 31 (6). https://doi.org/10.1214/aos/1074290335.\nWasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA’s Statement on p-Values: Context, Process, and Purpose.” The American Statistician.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 Goals for this Chapter</span>"
    ]
  },
  {
    "objectID": "07-chap.html",
    "href": "07-chap.html",
    "title": "9  7.1 Goals for this chapter",
    "section": "",
    "text": "9.1 7.2 What are the data? Matrices and their motivation\nMany datasets consist of several variables measured on the same set of subjects: patients, samples, or organisms. For instance, we may have biometric characteristics such as height, weight, age as well as clinical variables such as blood pressure, blood sugar, heart rate, and genetic data for, say, a thousand patients. The raison d’être for multivariate analysis is the investigation of connections or associations between the different variables measured. Usually the data are reported in a tabular data structure with one row for each subject and one column for each variable. In the following, we will focus on the special case where each of the variables is numeric, so we can represent the data structure as a matrix in R.\nIf the columns of the matrix are all independent of each other (unrelated), we can simply study each column separately and do standard “univariate” statistics on them one by one; there would be no benefit in studying them as a matrix.\nMore often, there will be patterns and dependencies. For instance, in the biology of cells, we know that the proliferation rate will influence the expression of many genes simultaneously. Studying the expression of 25,000 gene (columns) on many samples (rows) of patient-derived cells, we notice that many of the genes act together, either that they are positively correlated or that they are anti-correlated. We would miss a lot of important information if we were to only study each gene separately. Important connections between genes are detectable only if we consider the data as a whole: each row representing the many measurements made on the same observational unit. However, having 25,000 dimensions of variation to consider at once is daunting; we will show how to reduce our data to a smaller number of most important dimensions1 without losing too much information.\n1 We will elaborate this idea of dimension reduction in much more detail below. For the time being, remember we live in a four-dimensional world.\nThis chapter presents many examples of multivariate data matrices that we encounter in high-throughput experiments, as well as some more elementary examples that we hope will enhance your intuition. We will focus in this chapter on P rincipal C omponent A nalysis, abbreviated as PCA , a dimension reduction method. We will provide geometric explanations of the algorithm as well as visualizations that help interprete the output of PCA analyses.\nIn this chapter we will:\nFirst, let’s look at a set of examples of rectangular matrices used to represent tables of measurements. In each matrix, the rows and columns represent specific entities.\nTurtles: A simple data set that will help us understand the basic principles is a matrix of three dimensions of biometric measurements on painted turtles (Jolicoeur and Mosimann 1960).\nThe last three columns are length measurements (in millimetres), whereas the first column is a factor variable that tells us the sex of each animal.\nAthletes: This matrix is an interesting example from the sports world. It reports the performances for 33 athletes in the ten disciplines of the decathlon: m100, m400 and m1500 are times in seconds for the 100 meters, 400 meters, and 1500 meters respectively; m110 is the time to finish the 110 meters hurdles; pole is the pole-vault height, and high and long are the results of the high and long jumps, all in meters; weight, disc, and javel are the lengths in meters the athletes were able to throw the weight, discus and javelin. Here are these variables for the first three athletes:\nCell Types: Holmes et al. (2005) studied gene expression profiles of sorted T-cell populations from different subjects. The columns are a subset of gene expression measurements, they correspond to 156 genes that show differential expression between cell types.\nBacterial Species Abundances: Matrices of counts are used in microbial ecology studies (as we saw in Chapter 4). Here the columns represent different species (or operational taxonomic units, OTUs) of bacteria, which are identified by numerical tags. The rows are labeled according to the samples in which they were measured, and the (integer) numbers represent the number of times of each of the OTUs was observed in each of the samples.\nNotice the propensity of the matrix entries to be zero; we call such data sparse.\nmRNA reads: RNA-Seq transcriptome data report the number of sequence reads matching each gene2 in each of several biological samples. We will study this type of data in detail in Chapter 8\n2 Or sub-gene structures, such as exons.\nIt is customary in the RNA-Seq field—and so it is for the airway data above—to report the genes in the rows and the samples in the columns. Compared to the other matrices we look at here, this is transposed : rows and columns are swapped. Such different conventions easily lead to errors, so they are worthwhile paying attention to3. Proteomic profiles: Here, the columns are aligned mass spectroscopy peaks or molecules identified through their \\(m/z\\)-ratios; the entries in the matrix are the measured intensities4.\n3 The Bioconductor project tries to help users and developers to avoid such ambiguities by defining data containers in which such conventions are explicitly fixed. In Chapter 8, we will see the example of the SummarizedExperiment class.\n4 More details can be found, e.g., on Wikipedia.\nIn many of the matrices we have seen here, important information about the samples (subjects) and the measured features is stored in the row or column names, often through some ad hoc string concatenation. This is not the best place to store all available information, and quickly becomes limiting and error-prone. A much better approach is the Bioconductor SummarizedExperiment class.\n__\nTask\nWhen a peak was not detected for a particular \\(m/z\\) score in the mass spectrometry run, a zero was recorded in metab. Similarly, zeros in GPOTUs or in the airway object occur when there were no matching sequence reads detected. Tabulate the frequencies of zeros in these data matrices.\n__\nQuestion 7.1",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "07-chap.html#what-are-the-data-matrices-and-their-motivation",
    "href": "07-chap.html#what-are-the-data-matrices-and-their-motivation",
    "title": "9  7.1 Goals for this chapter",
    "section": "",
    "text": "turtles = read.table(\"../data/PaintedTurtles.txt\", header = TRUE)\nturtles[1:4, ]__\n\n\n  sex length width height\n1   f     98    81     38\n2   f    103    84     38\n3   f    103    86     42\n4   f    105    86     40\n\n\ndata(\"olympic\", package = \"ade4\")\nathletes = setNames(olympic$tab, \n  c(\"m100\", \"long\", \"weight\", \"high\", \"m400\", \"m110\", \"disc\", \"pole\", \"javel\", \"m1500\"))\nathletes[1:3, ]__\n\n\n   m100 long weight high  m400  m110  disc pole javel  m1500\n1 11.25 7.43  15.48 2.27 48.90 15.13 49.28  4.7 61.32 268.95\n2 10.87 7.45  14.97 1.97 47.71 14.46 44.36  5.1 61.76 273.02\n3 11.18 7.44  14.20 1.97 48.29 14.81 43.66  5.2 64.16 263.20\n\nload(\"../data/Msig3transp.RData\")\nround(Msig3transp,2)[1:5, 1:6]__\n\n\n             X3968 X14831 X13492 X5108 X16348  X585\nHEA26_EFFE_1 -2.61  -1.19  -0.06 -0.15   0.52 -0.02\nHEA26_MEM_1  -2.26  -0.47   0.28  0.54  -0.37  0.11\nHEA26_NAI_1  -0.27   0.82   0.81  0.72  -0.90  0.75\nMEL36_EFFE_1 -2.24  -1.08  -0.24 -0.18   0.64  0.01\nMEL36_MEM_1  -2.68  -0.15   0.25  0.95  -0.20  0.17\n\ndata(\"GlobalPatterns\", package = \"phyloseq\")\nGPOTUs = as.matrix(t(phyloseq::otu_table(GlobalPatterns)))\nGPOTUs[1:4, 6:13]__\n\n\nOTU Table:          [4 taxa and 8 samples]\n                     taxa are rows\n        246140 143239 244960 255340 144887 141782 215972 31759\nCL3          0      7      0    153      3      9      0     0\nCC1          0      1      0    194      5     35      3     1\nSV1          0      0      0      0      0      0      0     0\nM31Fcsw      0      0      0      0      0      0      0     0\n\n\n\nNotice the propensity of the matrix entries to be zero; we call such data sparse.\n\n\n\n\n\nlibrary(\"SummarizedExperiment\")\ndata(\"airway\", package = \"airway\")\nassay(airway)[1:3, 1:4]__\n\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513\nENSG00000000003        679        448        873        408\nENSG00000000005          0          0          0          0\nENSG00000000419        467        515        621        365\n\n\n\nmetab = t(as.matrix(read.csv(\"../data/metabolites.csv\", row.names = 1)))\nmetab[1:4, 1:4]__\n\n\n         146.0985388 148.7053275 310.1505057 132.4512963\nKOGCHUM1    29932.36    17055.70     1132.82    785.5129\nKOGCHUM2    94067.61    74631.69    28240.85   5232.0499\nKOGCHUM3   146411.33   147788.71    64950.49  10283.0037\nWTGCHUM1   229912.57   384932.56   220730.39  26115.2007\n\n\n\nIn many of the matrices we have seen here, important information about the samples (subjects) and the measured features is stored in the row or column names, often through some ad hoc string concatenation. This is not the best place to store all available information, and quickly becomes limiting and error-prone. A much better approach is the Bioconductor SummarizedExperiment class.\n\n\n\n\n\n\n\n\n\nWhat are the columns of these data matrices usually called?\nIn each of these examples, what are the rows of the matrix?\nWhat does a cell in a matrix represent?\nIf the data matrix is called athletes and you want to see the value of the third variable for the fifth athlete, what do you type into R?\n\n\n9.1.1 7.2.1 Low-dimensional data summaries and preparation\n\nFigure 7.1: xkcd: What do we mean by low-dimensional? We live in 3 dimensions, or 4 if you count time, a plane has 2 dimensions, a line has one dimension. A point is said to be zero-dimensional. For the amusing novel referenced in the cartoon see Abbott (1884).\nIf we are studying only one variable, i.e., just the third column of the turtles matrix5, we say we are looking at one-dimensional data. Such a vector, say all the turtle weights, can be visualized by plots such as those that we saw in Section 3.6, e.g., a histogram. If we compute a one number summary, say mean or median, we have made a zero- dimensional summary of our one-dimensional data. This is already an example of dimension reduction.\n5 The third column of a matrix \\(X\\) is denoted mathematically by \\({x}_{}\\) or accessed in R using X[, 3].\nIn Chapter 3 we studied two-dimensional scatterplots. We saw that if there are too many observations, it can be beneficial to group the data into (hexagonal) bins: these are two-dimensional histograms. When considering two variables (\\(x\\) and \\(y\\)) measured together on a set of observations, the correlation coefficient measures how the variables co- vary. This is a single number summary of two-dimensional data. Its formula involves the summaries \\({x}\\) and \\({y}\\):\n\\[ = { } \\]\nIn R, we use the cor function to calculate its value. Applied to a matrix this function computes all the two way correlations between continuous variables. In Chapter 9 we will see how to analyse multivariate categorical data.\n__\nQuestion 7.2\nCompute the matrix of all correlations between the measurements from the turtles data. What do you notice ?\n__\nSolution\n__\nWe take out the categorical variable and compute the matrix.\ncor(turtles[, -1])__\n\n\n          length     width    height\nlength 1.0000000 0.9783116 0.9646946\nwidth  0.9783116 1.0000000 0.9605705\nheight 0.9646946 0.9605705 1.0000000\nWe see that this square matrix is symmetric and the values are all close to 1. The diagonal values are always 1.\nIt is always beneficial to start a multidimensional analysis by checking these simple one-dimensional and two-dimensional summary statistics using visual displays such as those we look at in the next two questions.\n__\nQuestion 7.3\n\nProduce all pairwise scatterplots, as well as the one-dimensional histograms on the diagonal, for the turtles data. Use the package GGally.\nGuess the underlying or “true dimension” of these data?\n\n__\nSolution\n__\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"GGally\")\nggpairs(turtles[, -1], axisLabels = \"none\")__\n\nFigure 7.2: All pairs of bivariate scatterplots for the three biometric measurements on painted turtles.\nFrom Figure 7.2, it looks like all three of the variables are highly correlated and mostly reflect the same “underlying” variable, which we might interpret as the size of the turtle.\n__\nQuestion 7.4\nCompute all pairwise correlations of the variables in the athletes data and display the matrix in a heatmap. What do you notice?\n__\nSolution\n__\nlibrary(\"pheatmap\")\npheatmap(cor(athletes), cell.width = 10, cell.height = 10)__\n\nFigure 7.3: Heatmap of correlations between variables in the athletes data. Higher values are color coded red-orange. The hierarchical clustering shows a grouping of related disciplines.\nFigure 7.3 shows how the 10 variables cluster into groups: running, throwing and jumping.\n\n\n9.1.2 7.2.2 Preprocessing the data\nIn many cases, different variables are measured in different units, so they have different baselines and different scales6. These are not directly comparable in their original form.\n6 Common measures of scale are the range and the standard deviation. For instance, the times for the 110 metres vary between 14.18 and 16.2, with a standard deviation of 0.51, whereas the times to complete the 1500 metres vary between 256.64 and 303.17, with a standard deviation of 13.66; more than an order of magnitude larger. Moreover, the athletes data also contain measurements in different units (seconds, metres), whose choice is arbitrary (lengths could also be recorded in centimetres or feet, times in milliseconds).\nFor PCA and many other methods, we therefore need to transform the numeric values to some common scale in order to make comparisons meaningful. Centering means subtracting the mean, so that the mean of the centered data is at the origin. Scaling or standardizing then means dividing by the standard deviation, so that the new standard deviation is \\(1\\). In fact, we have already encountered these operations when computing the correlation coefficient (Equation 7.1): the correlation coefficient is simply the vector product of the centered and scaled variables. To perform these operations, there is the R function scale, whose default behavior when given a matrix or a data frame is to make every column have a mean of zero and a standard deviation of \\(1\\).\n__\nQuestion 7.5\n\nCompute the means and standard deviations of the turtle data, then use the scale function to center and standardize the continuous variables. Call this scaledTurtles, then verify the new values for mean and standard deviation of scaledTurtles.\nMake a scatterplot of the scaled and centered width and height variables of the turtle data and color the points by their sex.\n\n__\nSolution\n__\napply(turtles[,-1], 2, sd)__\n\n\n   length     width    height \n20.481602 12.675838  8.392837 \n\n\napply(turtles[,-1], 2, mean)__\n\n\n   length     width    height \n124.68750  95.43750  46.33333 \n\n\nscaledTurtles = scale(turtles[, -1])\napply(scaledTurtles, 2, mean)__\n\n\n       length         width        height \n-1.432050e-18  1.940383e-17 -2.870967e-16 \n\n\napply(scaledTurtles, 2, sd)__\n\n\nlength  width height \n     1      1      1 \n\n\ndata.frame(scaledTurtles, sex = turtles[, 1]) %&gt;%\n  ggplot(aes(x = width, y = height, group = sex)) +\n    geom_point(aes(color = sex)) + coord_fixed()__\n\nFigure 7.4: Turtles data projected onto the plane defined by the width and height variables: each point colored according to sex.\nWe have already encountered other data transformation choices in Chapters 4 and 5, where we used the log and asinh functions. The aim of these transformations is (usually) variance stabilization, i.e., to make the variances of replicate measurements of one and the same variable in different parts of its dynamic range more similar. In contrast, the standardizing transformation described above aims to make the scale (as measured by mean and standard deviation) of different variables the same.\nSometimes it is preferable to leave variables at different scales because they are truly of different importance. If their original scale is relevant, then we can (should) leave the data as is. In other cases, the variables have different precisions known a priori. We will see in Chapter 9 that there are several ways of weighting such variables.\nAfter preprocessing the data, we are ready to undertake data simplification through dimension reduction.\n\nUseful books with relevant chapters are Flury (1997) for an introductory account and Mardia, Kent, and Bibby (1979) for a detailed mathematical approach.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "07-chap.html#dimension-reduction",
    "href": "07-chap.html#dimension-reduction",
    "title": "9  7.1 Goals for this chapter",
    "section": "9.2 7.3 Dimension reduction",
    "text": "9.2 7.3 Dimension reduction\nWe will explain dimension reduction from several different perspectives. It was invented in 1901 by Karl Pearson (Pearson 1901) as a way to reduce a two-variable scatterplot to a single coordinate. It was used by statisticians in the 1930s to summarize a battery of psychological tests run on the same subjects (Hotelling 1933); thus providing overall scores that summarize many tested variables at once.\n\n\n\nPrincipal and principle are two different words, which have different meanings. So please do not confuse them. With PCA, it is always principal.\n\n\nPrincipal and principle are two different words, which have different meanings. So please do not confuse them. With PCA, it is always principal.\nThis idea of principal scores inspired the name Principal Component Analysis (abbreviated PCA). PCA is called an unsupervised learning technique because, as in clustering, it treats all variables as having the same status. We are not trying to predict or explain one particular variable’s value from the others; rather, we are trying to find a mathematical model for an underlying structure for all the variables. PCA is primarily an exploratory technique that produces maps that show the relations between variables and between observations in a useful way.\nWe first provide a flavor of what this multivariate analysis does to the data. There is an elegant mathematical formulation of these methods through linear algebra, although here we will try to minimize its use and focus on visualization and data examples.\nWe use geometrical projections that take points in higher-dimensional spaces and projects them down onto lower dimensions. Figure 7.5 shows the projection of the point \\(A\\) onto the line generated by the vector \\({v}\\).\n\nFigure 7.5: Point \\(A\\) is projected onto the red line generated by the vector \\(v\\). The dashed projection line is perpendicular (or orthogonal) to the red line. The intersection point of the projection line and the red line is called the orthogonal projection of A onto the red line generated by the vector \\(v\\).\nPCA is a linear technique, meaning that we look for linear relations between variables and that we will use new variables that are linear functions of the original ones (\\(f(ax+by)=af(x)+b(y)\\)). The linearity constraints makes computations particularly easy. We will see non-linear techniques in Chapter 9.\n\n9.2.1 7.3.1 Lower-dimensional projections\nHere we show one way of projecting two-dimensional data onto a line using the athletes data. The code below provides the preprocessing and plotting steps that were used to generate Figure 7.6:\nathletes = data.frame(scale(athletes))\nath_gg = ggplot(athletes, aes(x = weight, y = disc)) +\n  geom_point(size = 2, shape = 21)\nath_gg + geom_point(aes(y = 0), colour = \"red\") +\n  geom_segment(aes(xend = weight, yend = 0), linetype = \"dashed\")__\n\nFigure 7.6: Scatterplot of two variables showing the projection on the horizontal x axis (defined by \\(y=0\\)) in red and the lines of projection appear as dashed.\n__\nTask\n\nCalculate the variance of the red points in Figure 7.6.\nMake a plot showing projection lines onto the \\(y\\) axis and projected points.\nCompute the variance of the points projected onto the vertical \\(y\\) axis.\n\n\n\n9.2.2 7.3.2 How do we summarize two-dimensional data by a line?\nIn general, we lose information about the points when we project from two dimensions (a plane) to one (a line). If we do it just by using the original coordinates, as we did on the weight variable in Figure 7.6, we lose all the information about the disc variable. Our goal is to keep as much information as we can about both variables. There are actually many ways of projecting the point cloud onto a line. One is to use what are known as regression lines. Let’s look at these lines and how they are constructed in R.\n\n9.2.2.1 Regressing one variable on the other\nIf you have seen linear regression, you already know how to compute lines that summarize scatterplots; linear regression is a supervised method that gives preference minimizing the residual sum of squares in one direction: that of the response variable.\n\n\n9.2.2.2 Regression of the disc variable on weight.\nIn Figure 7.7, we use the lm (linear model) function to find the regression line. Its slope and intercept are given by the values in the coefficients slot of the resulting object reg1.\nreg1 = lm(disc ~ weight, data = athletes)\na1 = reg1$coefficients[1] # intercept\nb1 = reg1$coefficients[2] # slope\npline1 = ath_gg + geom_abline(intercept = a1, slope = b1,\n    col = \"blue\", linewidth = 1.5)\npline1 + geom_segment(aes(xend = weight, yend = reg1$fitted),\n    colour = \"red\", arrow = arrow(length = unit(0.15, \"cm\")))__\n\nFigure 7.7: The blue line minimizes the sum of squares of the vertical residuals (in red).\n\n\n9.2.2.3 Regression of weight on discus.\nFigure 7.8 shows the line produced when reversing the roles of the two variables; weight becomes the response variable.\nreg2 = lm(weight ~ disc, data = athletes)\na2 = reg2$coefficients[1] # intercept\nb2 = reg2$coefficients[2] # slope\npline2 = ath_gg + geom_abline(intercept = -a2/b2, slope = 1/b2,\n    col = \"darkgreen\", linewidth = 1.5)\npline2 + geom_segment(aes(xend=reg2$fitted, yend=disc),\n    colour = \"orange\", arrow = arrow(length = unit(0.15, \"cm\")))__\n\nFigure 7.8: The green line minimizes the sum of squares of the horizontal residuals (in orange).\nEach of the regression lines in Figures 7.7 and 7.8 gives us an approximate linear relationship between disc and weight. However, the relationship differs depending on which of the variables we choose to be the predictor and which the response.\n__\nQuestion 7.6\nHow large is the variance of the projected points that lie on the blue regression line of Figure 7.7? Compare this to the variance of the data when projected on the original axes, weight and disc.\n__\nSolution\n__\nPythagoras’ theorem tells us that the squared length of the hypotenuse of a right-angled triangle is equal to the sum of the squared lengths of the other two sides, which we apply as follows:\nvar(athletes$weight) + var(reg1$fitted)__\n\n\n[1] 1.650204\nThe variances of the points along the original axes weight and disc are 1, since we scaled the variables.\n\n\n9.2.2.4 A line that minimizes distances in both directions\nFigure 7.9 shows the line chosen to minimize the sum of squares of the orthogonal (perpendicular) projections of data points onto it; we call this the principal component line. All our three ways of fitting a line (Figures 7.7–7.9) together in one plot are shown in Figure 7.10.\nxy = cbind(athletes$disc, athletes$weight)\nsvda = svd(xy)\npc = xy %*% svda$v[, 1] %*% t(svda$v[, 1])\nbp = svda$v[2, 1] / svda$v[1, 1]\nap = mean(pc[, 2]) - bp * mean(pc[, 1])\nath_gg + geom_segment(xend = pc[, 1], yend = pc[, 2]) +\n  geom_abline(intercept = ap, slope = bp, col = \"purple\", linewidth = 1.5)__\n\nFigure 7.9: The purple principal component line minimizes the sums of squares of the orthogonal projections.\n\nFigure 7.10: The blue line minimizes the sum of squares of the vertical residuals, the green line minimizes the horizontal residuals, the purple line, called the principal component , minimizes the orthogonal projections. Notice the ordering of the slopes of the three lines.\n__\nQuestion 7.7\n\nWhat is particular about the slope of the purple line?\nRedo the plots on the original (unscaled) variables. What happens?\n\n__\nSolution\n__\nThe lines computed here depend on the choice of units. Because we have made the standard deviations equal to one for both variables, the PCA line is the diagonal that cuts exactly in the middle of both regression lines. Since the data were centered by subtracting their means, the line passes through the origin \\((0,0)\\).\n__\nQuestion 7.8\nCompute the variance of the points on the purple line.\n__\nSolution\n__\nWe have computed the coordinates of the points when we made the plot, these are in the pc vector:\napply(pc, 2, var)__\n\n\n[1] 0.9031761 0.9031761\n\n\nsum(apply(pc, 2, var))__\n\n\n[1] 1.806352\nWe see that the variance along this axis is larger than the other variances we calculated in Question 7.6.\nPythagoras’ theorem tells us two interesting things here:\n\nIf we are minimizing in both horizontal and vertical directions we are in fact minimizing the orthogonal projections onto the line from each point.\nThe total variability of the points is measured by the sum of squares of the projection of the points onto the center of gravity, which is the origin (0,0) if the data are centered. This is called the total variance or the inertia of the point cloud. This inertia can be decomposed into the sum of the squares of the projections onto the line plus the variances along that line. For a fixed variance, minimizing the projection distances also maximizes the variance along that line. Often we define the first principal component as the line with maximum variance.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "07-chap.html#the-new-linear-combinations",
    "href": "07-chap.html#the-new-linear-combinations",
    "title": "9  7.1 Goals for this chapter",
    "section": "9.3 7.4 The new linear combinations",
    "text": "9.3 7.4 The new linear combinations\n\nThe PC line we found in the previous section could be written\nImage credit: Sara Holmes\n\\[ PC = + . \\]\nPrincipal components are linear combinations of the variables that were originally measured, they provide a new coordinate system. To understand what a linear combination really is, we can take an analogy. When making a healthy juice mix, you will follow a recipe like\n\\[ \\[\\begin{align} V &= 2 \\times \\text{Beet} + 1 \\times \\text{Carrot} \\\\\\ &\\+\n\\tfrac{1}{2} \\text{Gala} + \\tfrac{1}{2} \\text{GrannySmith} \\\\\\ &\\+ 0.02 \\times\n\\text{Ginger} + 0.25 \\times \\text{Lemon}. \\end{align}\\] \\]\nThis recipe is a linear combination of individual juice types (the original variables). The result is a new variable, \\(V\\), and the coefficients \\((2,1,,,0.02,0.25)\\) are called the loadings.\n__\nQuestion 7.9\nHow would you compute the calories in a glass of juice?\n\n9.3.1 7.4.1 Optimal lines\nA linear combination of variables defines a line in higher dimensions in the same way we constructed lines in the scatterplot plane of two dimensions. As we saw in that case, there are many ways to choose lines onto which we project the data, there is however a `best’ line for our purpose.\nThe total variance of all the points in all the variables can de decomposed. In PCA, we use the fact that the total sums of squares of the distances between the points and any line can be decomposed into the distance to the line and the variance along the line.\nWe saw that the principal component minimizes the distance to the line, and it also maximizes the variance of the projections along the line.\nWhy is maximizing the variance along a line a good idea? Let’s look at another example of a projection from three dimensions into two. In fact, human vision depends on such dimension reduction:\n\nFigure 7.11: A mystery silhouette.\n__\nQuestion 7.10\nIn Figure 7.11, there is a two-dimensional projection of a three-dimensional object. What is the object?\n__\nQuestion 7.11\nWhich of the two projections, Figure 7.11 or 7.13, do you find more informative, and why?\n__\nSolution\n__\nOne can argue that the projection that maximizes the area of the shadow shows more `information’.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "07-chap.html#the-pca-workflow",
    "href": "07-chap.html#the-pca-workflow",
    "title": "9  7.1 Goals for this chapter",
    "section": "9.4 7.5 The PCA workflow",
    "text": "9.4 7.5 The PCA workflow\n\nFigure 7.12: Many choices have to be made during PCA processing.\nPCA is based on the principle of finding the axis showing the largest inertia/variability, removing the variability in that direction and then iterating to find the next best orthogonal axis, and so on. In fact, we do not have to run iterations, all the axes can be found in one linear algebra operation called the S ingular V alue D ecomposition (we will delve more deeply into the details below).\nIn the diagram in Figure 7.12, we see that first the means and variances are computed and the choice of whether to work directly with the covariance matrix or with the correlation matrix has to be made. The next step is the choice of \\(k\\), the number of components we deem relevant to the data. We say that \\(k\\) is the rank of the approximation. The best choice of \\(k\\) is a difficult question, and we discuss on how to approach it below. The choice of \\(k\\) requires looking at a plot of the variances explained by the successive principal components. Once we have chosen \\(k\\), we can proceed to the projections of the data in the new \\(k\\)-dimensional subspace.\nThe end results of the PCA workflow are useful maps of both the variables and the samples. Understanding how these maps are constructed will maximize the information we can gather from them.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "07-chap.html#the-inner-workings-of-pca-rank-reduction",
    "href": "07-chap.html#the-inner-workings-of-pca-rank-reduction",
    "title": "9  7.1 Goals for this chapter",
    "section": "9.5 7.6 The inner workings of PCA: rank reduction",
    "text": "9.5 7.6 The inner workings of PCA: rank reduction\nThis is a small section for those whose background in linear algebra is but a faint memory. It tries to give some intuition to the singular value decomposition method underlying PCA, without too much notation.\n\nFigure 7.13: Another two-dimensional projection of the same object shown in Figure 7.11. Here, the perspective is more informative. Generally, choosing the perspective such that the spread (in other words, the variance) of the points is maximal generally provides most information. We want to see as much of the variation as possible, that’s what PCA does.\nThe singular value decomposition of a matrix finds horizontal and vertical vectors (called the singular vectors) and normalizing values (called singular values). As before, we start by giving the forward-generative explanation before doing the actual reverse engineering that is used in creating the decomposition. To calibrate the meaning of each step, we will start with an artificial example before moving to the complexity of real data.\n\n9.5.1 7.6.1 Rank-one matrices\nA simple generative model demonstrates the meaning of the rank of a matrix and explains how we find it in practice. Suppose we have two vectors, \\(u\\) (a one-column matrix), and \\(v^t=t(v)\\) (a one-row matrix–the transpose of a one-column matrix \\(v\\)). For instance, \\(u =(\n\\[\\begin{smallmatrix}\n1\\\\\\2\\\\\\3\\\\\\4 \\end{smallmatrix}\\]\n)\\) and \\(v =(\n\\[\\begin{smallmatrix} 2\\\\\\4\\\\\\8 \\end{smallmatrix}\\]\n)\\). The transpose of \\(v\\) is written \\(v^t = t(v) = (2; 4; 8)\\). We multiply a copy of \\(u\\) by each of the elements of \\(v^t\\) in turn as follows:\nStep 0:\n\n\n\nX\n2\n4\n8\n\n\n\n\n1\n\n\n\n\n\n2\n\n\n\n\n\n3\n\n\n\n\n\n4\n\n\n\n\n\n\nStep 1:\n\n\n\nX\n2\n4\n8\n\n\n\n\n1\n2\n\n\n\n\n2\n4\n\n\n\n\n3\n6\n\n\n\n\n4\n8\n\n\n\n\n\nStep 2:\n\n\n\nX\n2\n4\n8\n\n\n\n\n1\n2\n4\n\n\n\n2\n4\n8\n\n\n\n3\n6\n12\n\n\n\n4\n8\n16\n\n\n\n\nStep 3:\n\n\n\nX\n2\n4\n8\n\n\n\n\n1\n2\n4\n8\n\n\n2\n4\n8\n16\n\n\n3\n6\n12\n24\n\n\n4\n8\n16\n32\n\n\n\nThus, the \\((2,3)\\) entry of the matrix \\(X\\), written \\(x_{2,3}\\), is obtained by multiplying \\(u_2\\) by \\(v_3\\). We can write this\n\\[ X=]\nThe matrix \\(X\\) we obtain here is said to be of rank 1, because both \\(u\\) and \\(v\\) have one column.\n__\nQuestion 7.12\nWhy can we say that writing \\(X = u*v^t\\) is more economical than spelling out the full matrix \\(X\\)?\n__\nSolution\n__\n\\(X\\) has \\(4=12\\) elements, while in terms of \\(u\\) and \\(v\\) it can be expressed by only \\(4+3=7\\) numbers. The compression is even more impressive when \\(u\\) or \\(v\\) are longer.\nOn the other hand, suppose that we want to reverse the process and simplify another matrix \\(X\\) given below with 3 rows and 4 columns (12 numbers). Can we always express it in a similar way as a product of vectors without loss of information? In the diagrams shown in Figures 7.14 and 7.15, the colored boxes have areas proportional to the numbers in the cells of the matrix (7.4).\n\nFigure 7.14: Some special matrices have numbers in them that make them easy to decompose. Each colored rectangle in this diagram has an area that corresponds to the number in it.\n__\nQuestion 7.13\nHere is a matrix \\(X\\) we want to decompose.\n\\[ ]\n\\(X\\) has been redrawn as series of rectangles in Figure 7.14. What numbers could we put in the white \\(u\\) and \\(v\\) boxes so that the values of the sides of the rectangle give the numbers as their product?\nA matrix with the special property of being perfectly “rectangular” like \\(X\\) is said to be of rank 1. We can represent the numbers in \\(X\\) by the areas of rectangles, where the sides of rectangles are given by the values in the side vectors (\\(u\\) and \\(v\\)).\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.15: The numbers in the cells are equal to the product of the corresponding margins in (a), (b) and (c). We could make the cells from products in several ways. In (c), we force the margins to have norm \\(1\\).\nWe see in Figure 7.15 that the decomposition of \\(X\\) is not unique: there are several candidate choices for the vectors \\(u\\) and \\(v\\). We will make the choice unique by requiring that the sum of the squares of each vector’s elements add to 1 (we say the vectors \\(v\\) and \\(u\\) have norm 1). Then we have to keep track of one extra number by which to multiply each of the products, and which represents the “overall scale” of \\(X\\). This is the value we have put in the upper left hand corner. It is called the singular value \\(s_1\\). In the R code below, we start by supposing we know the values in u, v and s1; later we will see a function that finds them for us. Let’s check the multiplication and norm properties in R:\nX = matrix(c(780,  75, 540,\n             936,  90, 648,\n            1300, 125, 900,\n             728,  70, 504), nrow = 3)\nu = c(0.8196, 0.0788, 0.5674)\nv = c(0.4053, 0.4863, 0.6754, 0.3782)\ns1 = 2348.2\nsum(u^2)__\n\n\n[1] 1\n\n\nsum(v^2)__\n\n\n[1] 1\n\n\ns1 * u %*% t(v)__\n\n\n     [,1] [,2] [,3] [,4]\n[1,]  780  936 1300  728\n[2,]   75   90  125   70\n[3,]  540  648  900  504\n\n\nX - s1 * u %*% t(v)__\n\n\n         [,1]   [,2]   [,3]   [,4]\n[1,] -0.03419 0.0745 0.1355 0.1221\n[2,]  0.00403 0.0159 0.0252 0.0186\n[3,] -0.00903 0.0691 0.1182 0.0982\n__\nQuestion 7.14\nTry svd(X) in R. Look at the components of the output of the svd function carefully. Check the norm of the columns of the matrices that result from this call. Where did the above value of s1 = 2348.2 come from?\n__\nSolution\n__\nsvd(X)$u[, 1]\nsvd(X)$v[, 1]\nsum(svd(X)$u[, 1]^2)\nsum(svd(X)$v[, 1]^2)\nsvd(X)$d __\nIn fact, in this particular case we were lucky: we see that the second and third singular values are 0 (up to the numeric precision we care about). That is why we say that \\(X\\) is of rank 1. For a more general matrix \\(X\\), it is rare to be able to write \\(X\\) exactly as this type of two- vector product. The next subsection shows how we can decompose \\(X\\) when it is not of rank 1: we will just need more pieces.\n\n\n9.5.2 7.6.2 How do we find such a decomposition in a unique way?\nIn the above decomposition, there were three elements: the horizontal and vertical singular vectors, and the diagonal corner, called the singular value. These can be found using the singular value decomposition function (svd). For instance:\nXtwo = matrix(c(12.5, 35.0, 25.0, 25, 9, 14, 26, 18, 16, 21, 49, 32,\n       18, 28, 52, 36, 18, 10.5, 64.5, 36), ncol = 4, byrow = TRUE)\nUSV = svd(Xtwo)__\n__\nQuestion 7.15\nLook at the USV object, the result of calling the svd function. What are its components?\n__\nSolution\n__\nnames(USV)__\n\n\n[1] \"d\" \"u\" \"v\"\n\n\nUSV$d __\n\n\n[1] 1.350624e+02 2.805191e+01 3.111680e-15 2.290270e-15\nSo 135.1 is the first singular value USV$d[1].\n__\nQuestion 7.16\nCheck how each successive pair of singular vectors improves our approximation to Xtwo. What do you notice about the third and fourth singular values?\n__\nSolution\n__\nXtwo - USV$d[1] * USV$u[, 1] %*% t(USV$v[, 1])\nXtwo - USV$d[1] * USV$u[, 1] %*% t(USV$v[, 1]) -\n       USV$d[2] * USV$u[, 2] %*% t(USV$v[, 2])__\nThe third and fourth singular values are so small that they do not improve the approximation (within rounding errors), so we can conclude that Xtwo is of rank 2.\nAgain, there are many ways to write a rank two matrix such as Xtwo as a sum of rank one matrices: in order to ensure uniqueness, we impose yet another7 condition on the singular vectors. The output vectors of the singular decomposition do not only have their norms equal to 1, each column vector in the \\(U\\) matrix is orthogonal to all the previous ones. We write \\(u_{} u_{}\\), this means that the sum of the products of the values in the same positions is \\(0\\): \\(i u{i1} u_{i2} = 0\\). Ditto for the \\(V\\) matrix.\n7 Above, we had chosen the norm of the vectors to be 1.\n__\nTask\nCheck the orthonormality by computing the cross product of the \\(U\\) and \\(V\\) matrices:\nt(USV$u) %*% USV$u\nt(USV$v) %*% USV$v __\nLet’s submit our scaledTurtles matrix to a singular value decomposition.\nturtles.svd = svd(scaledTurtles)\nturtles.svd$d __\n\n\n[1] 11.746475  1.419035  1.003329\n\n\nturtles.svd$v __\n\n\n          [,1]       [,2]        [,3]\n[1,] 0.5787981  0.3250273  0.74789704\n[2,] 0.5779840  0.4834699 -0.65741263\n[3,] 0.5752628 -0.8127817 -0.09197088\n\n\ndim(turtles.svd$u)__\n\n\n[1] 48  3\n__\nQuestion 7.17\nWhat can you conclude about the turtles matrix from the svd output?\n__\nSolution\n__\nThe first column of turtles.svd$v shows that the coefficients for the three variables are practically equal. Other noticeable “coincidences” include:\nsum(turtles.svd$v[,1]^2)__\n\n\n[1] 1\n\n\nsum(turtles.svd$d^2) / 47 __\n\n\n[1] 3\nWe see that the coefficients are in fact \\(\\) and the sum of squares of the singular values is equal to \\((n-1)p\\).\n\n\n9.5.3 7.6.3 Singular value decomposition\n\n\\(X\\) is decomposed additively into rank-one pieces. Each of the \\(u\\) vectors is combined into the \\(U\\) matrix, and each of \\(v\\) vectors into \\(V\\). The Singular Value Decomposition is\n\\[ {X} = U S V^t, V^t V={I}, U^t U={I}, \\]\nwhere \\(S\\) is the diagonal matrix of singular values, \\(V^t\\) is the transpose of \\(V\\), and \\({I}\\) is the Identity matrix. Expression 7.5 can be written elementwise as\n\\[ X_{ij} = u_{i1}s_1v_{1j} + u_{i2}s_2v_{2j} + u_{i3}s_3v_{3j} +… + u_{ir}s_rv_{rj}, \\]\n\\(U\\) and \\(V\\) are said to be orthonormal8, because their self- crossproducts are the identity matrix.\n8 Nothing to do with the normal distribution, it stands for orthogonal and having norm 1.\n\n\n9.5.4 7.6.4 Principal components\nThe singular vectors from the singular value decomposition (provided by the svd function in R) contain the coefficients to put in front of the original variables to make the more informative ones we call the principal components. We write this as:\n\\[ Z_1=v_{11} X_{} +v_{21} X_{} + v_{31} X_{}+ + v_{p1} X_{p}. \\]\nIf usv = svd(X), then \\((v_{11},v_{21},v_{31},…)\\) are given by the first column of usv$v; similarly for \\(Z_2\\) with the second column of usv$v, and so son. \\(p\\) is the number of columns of \\(X\\) and the number of rows of \\(V\\). These new variables \\(Z_1, Z_2, Z_3, …\\) have variances that decrease in size: \\(s_1^2 s_2^2 s_3^2 …\\).\n__\nQuestion 7.18\nCompute the first principal component for the turtles data by multiplying by the first singular value d[1] by u[,1]. What is another way of computing it ?\n__\nSolution\n__\nWe show this using the code:\nUS = turtles.svd$u[,1, drop = FALSE] %*% turtles.svd$d[1]\nXV = scaledTurtles %*% turtles.svd$v[, 1, drop = FALSE]\nmax(abs(US-XV))__\nWe can also see using matrix algebra that \\(XV\\) and \\(US\\) are the same. Remember that \\(V\\) is orthogonal, so \\(V^t V={I}\\) and \\(XV = USV^tV=US,{I}\\).\nNote: The drop = FALSE argument in the first line of the below code makes sure that the selected matrix column retains matrix / array class attributes and thus is eligible for the matrix multiplication operator. Alternatively, you could use the regular multiplication operator *. In the second line, the drop = FALSE is not strictly necessary, but we have it there for symmetry.\nHere are two useful facts, first in words, then with the mathematical shorthand.\nThe number of principal components \\(k\\) is always chosen to be fewer than the number of original variables or the number of observations. We are “lowering” the dimension of the problem:\n\\[ k(n,p). \\]\nThe principal component transformation is defined so that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each successive component in turn has the highest variance possible under the constraint that it be orthogonal to the preceding components:\n\\[ {aX bX}({aX} (X)), bX= \\]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "07-chap.html#plotting-the-observations-in-the-principal-plane",
    "href": "07-chap.html#plotting-the-observations-in-the-principal-plane",
    "title": "9  7.1 Goals for this chapter",
    "section": "9.6 7.7 Plotting the observations in the principal plane",
    "text": "9.6 7.7 Plotting the observations in the principal plane\nWe revisit our two-variable athletes data with the discus and the weight variables. In Section 7.3.2, we computed the first principal component and represented it as the purple line in Figure 7.10. We showed that \\(Z_1\\) was the linear combination given by the diagonal. As the coefficients have to have their sum of squares add to \\(1\\), we have that \\[Z_1=-0.707- 0.707.\\]\nThis is the same as if the two coordinates were \\(c_1=0.7071\\) and \\(c_2=0.7071\\).\n__\nQuestion 7.19\nWhat part of the output of the svd functions leads us to the first PC coefficients, also known as the PC loadings ?\nNote that we use svda which was the svd applied to the two variables discus and weight.\n__\nSolution\n__\nsvda$v[,1]__\n\n\n[1] -0.7071068 -0.7071068\nIf we rotate the (discus, weight) plane by making the purple line the horizontal \\(x\\) axis, we obtain what is know as the first principal plane.\nppdf = tibble(PC1n = -svda$u[, 1] * svda$d[1],\n              PC2n =  svda$u[, 2] * svda$d[2])\ngg = ggplot(ppdf, aes(x = PC1n, y = PC2n)) + \n    geom_point() + \n    geom_hline(yintercept = 0, color = \"purple\", linewidth = 1.5, alpha = 0.5) +\n    xlab(\"PC1 \")+ ylab(\"PC2\") + xlim(-3.5, 2.7) + ylim(-2, 2) + coord_fixed()\ngg + geom_point(aes(x = PC1n, y = 0), color = \"red\") +\n     geom_segment(aes(xend = PC1n, yend = 0), color = \"red\") \ngg + geom_point(aes(x = 0, y = PC2n), color = \"blue\") +\n     geom_segment(aes(yend = PC2n, xend = 0), color = \"blue\") +\n     geom_vline(xintercept = 0, color = \"skyblue\", linewidth = 1.5, alpha = 0.5) __\n\n\n\n\n\n\n\n\nFigure 7.16: In the case where we only have two original variables, the PCA transformation is a simple rotation; the new coordinates are always chosen to be the horizontal and vertical axes.\n__\nQuestion 7.20\n\nWhat is the mean of the sums of squares of the red segments in Figure 7.16 equal to?\nHow does this compare to the variance of the red points?\nCompute the ratio of the standard deviation of the blue segments to the red segments in Figure 7.16. Compare this to the ratio of singular values 1 and 2.\n\n__\nSolution\n__\n\nThe sum of squares of the red segments corresponds to the square of the second singular value:\n\nsum(ppdf$PC2n^2) __\n\n\n[1] 6.196729\n\n\nsvda$d[2]^2 __\n\n\n[1] 6.196729\nSince the mean of the red segments is zero, the above quantities are also proportional to the variance:\nmean(ppdf$PC2n) __\n\n\n[1] 5.451106e-16\n\n\nvar(ppdf$PC2n) * (nrow(ppdf)-1)__\n\n\n[1] 6.196729\n\nThe variance of the red points is var(ppdf$PC1n), which is larger than what we calculated in a) by design of the first PC.\n\nvar(ppdf$PC1n) __\n\n\n[1] 1.806352\n\n\nvar(ppdf$PC2n) __\n\n\n[1] 0.1936478\n\nWe take the ratios of the standard deviations explained by the points on the vertical and horizontal axes by computing:\n\nsd(ppdf$PC1n) / sd(ppdf$PC2n)__\n\n\n[1] 3.054182\n\n\nsvda$d[1] / svda$d[2]__\n\n\n[1] 3.054182\n__\nTask\nUse prcomp to compute the PCA of the first two columns of the athletes data, look at the output. Compare to the singular value decomposition.\n\n9.6.1 7.7.1 PCA of the turtles data\nWe now want to do a complete PCA analysis on the turtles data. Remember, we already looked at the summary statistics for the one- and two-dimensional data. Now we are going to answer the question about the “true” dimensionality of these rescaled data.\nIn the following code, we use the function princomp. Its return value is a list of all the important pieces of information needed to plot and interpret a PCA.\n\n\n\nIn fact, PCA is such a fundamental technique that there are many different implementations of it in various R packages. Unfortunately, the input arguments and the formatting and naming of their output is not standardized, and some even use different conventions for the scaling of their output. We will experiment with several different ones to familiarize ourselves with these choices.\n\n\nIn fact, PCA is such a fundamental technique that there are many different implementations of it in various R packages. Unfortunately, the input arguments and the formatting and naming of their output is not standardized, and some even use different conventions for the scaling of their output. We will experiment with several different ones to familiarize ourselves with these choices.\ncor(scaledTurtles)__\n\n\n          length     width    height\nlength 1.0000000 0.9783116 0.9646946\nwidth  0.9783116 1.0000000 0.9605705\nheight 0.9646946 0.9605705 1.0000000\n\n\npcaturtles = princomp(scaledTurtles)\npcaturtles __\n\n\nCall:\nprincomp(x = scaledTurtles)\n\nStandard deviations:\n   Comp.1    Comp.2    Comp.3 \n1.6954576 0.2048201 0.1448180 \n\n 3  variables and  48 observations.\n\n\nlibrary(\"factoextra\")\nfviz_eig(pcaturtles, geom = \"bar\", bar_width = 0.4) + ggtitle(\"\")__\n\nFigure 7.17: The screeplot shows the eigenvalues for the standardized turtles data (scaledTurtles): there is one large value and two small ones. The data are (almost) one-dimensional. We will see why this dimension is called an axis of size, a frequent phenomenon in biometric data (Jolicoeur and Mosimann 1960).\n__\nQuestion 7.21\nMany PCA functions have been created by different teams who worked in different areas at different times. This can lead to confusion, especially because they have different naming conventions. Let’s compare three of them; run the following lines of code and look at the resulting objects:\nsvd(scaledTurtles)$v[, 1]\nprcomp(turtles[, -1])$rotation[, 1]\nprincomp(scaledTurtles)$loadings[, 1]\nlibrary(\"ade4\")\ndudi.pca(turtles[, -1], nf = 2, scannf = FALSE)$c1[, 1]__\nWhat happens when you disable the scaling in the prcomp and princomp functions?\nIn what follows, we always suppose that the matrix \\(X\\) represents the centered and scaled matrix.\n__\nQuestion 7.22\nThe coordinates of the observations in the new variables from the prcomp function (call it res) are in the scores slot of the result. Take a look at PC1 for the turtles and compare it to res$scores. Compare the standard deviation sd1 to that in the res object and to the standard deviation of the scores.\n__\nSolution\n__\nres = princomp(scaledTurtles)\nPC1 = scaledTurtles %*% res$loadings[,1]\nsd1 = sqrt(mean(res$scores[, 1]^2))__\n__\nQuestion 7.23\nCheck the orthogonality of the res$scores matrix. Why can’t we say that it is orthonormal?\nNow we are going to combine both the PC scores (\\(US\\)) and the loadings- coefficients (\\(V\\)). The plots with both the samples and the variables represented are called biplots. This can be done in one line using the following factoextra package function.\nfviz_pca_biplot(pcaturtles, label = \"var\", habillage = turtles[, 1]) +\n  ggtitle(\"\")__\n\nFigure 7.18: A biplot of the first two dimensions showing both variables and observations. The arrows show the variables. The turtles are labeled by sex. The extended horizontal direction is due to the size of the first eigenvalue, which is much larger than the second.\n\n\n\nBeware the aspect ratio when plotting a PCA. It is rare to have the two components be of similar norm, so square shaped plots will be the exception. More common are elongated plots, which show that the horizontal (first) principal component is more important than the second. This matters, e.g., for interpreting distances between points in the plots.\n\n\nBeware the aspect ratio when plotting a PCA. It is rare to have the two components be of similar norm, so square shaped plots will be the exception. More common are elongated plots, which show that the horizontal (first) principal component is more important than the second. This matters, e.g., for interpreting distances between points in the plots.\n__\nQuestion 7.24\nIs it possible to have a PCA plot with the PC1 as the horizontal axis whose height is longer than its width?\n__\nSolution\n__\nThe variance of points in the PC1 direction is \\(_1=s_1^2\\) which is always larger than \\(_2=s_2^2\\), so the PCA plot will always be wider than high.\n__\nQuestion 7.25\nLooking at Figure 7.18: a) Did the males or female turtles tend to be larger?\nb) What do the arrows tell us about the correlations?\n__\nQuestion 7.26\nCompare the variance of each new coordinate to the eigenvalues returned by the PCA dudi.pca function.\n__\nSolution\n__\npcadudit = dudi.pca(scaledTurtles, nf = 2, scannf = FALSE)\napply(pcadudit$li, 2, function(x) sum(x^2)/48)__\n\n\n     Axis1      Axis2 \n2.93573765 0.04284387 \n\n\npcadudit$eig __\n\n\n[1] 2.93573765 0.04284387 0.02141848\nNow we look at the relationships between the variables, both old and new by drawing what is known as the correlation circle. The aspect ratio is 1 here and the variables are represented by arrows as shown in Figure 7.19. The lengths of the arrows indicate the quality of the projections onto the first principal plane:\nfviz_pca_var(pcaturtles, col.circle = \"black\") + ggtitle(\"\") +\n  xlim(c(-1.2, 1.2)) + ylim(c(-1.2, 1.2))__\n\nFigure 7.19: Part of the “circle of correlations” showing the original variables. Their correlations with each other and with the new principal components are given by the angles between the vectors and between the axes and the vectors.\n__\nQuestion 7.27\nExplain the relationships between the number of rows of our turtles data matrix and the following numbers:\nsvd(scaledTurtles)$d/pcaturtles$sdev\nsqrt(47)__\n__\nSolution\n__\nWhen computing the variance covariance matrix, many implementations use \\(1/(n-1)\\) as the denominator. Here, \\(n=48\\) so the sum of the variances are off by a factor of 48/47.\nThese data are a good example of how sometimes almost all the variation in the data can be captured in a lower-dimensional space: here, three-dimensional data can be essentially replaced by a line. Keep in mind: \\(XtC=VSUtUS=VS^2.\\) The principal components are the columns of the matrix \\(C=US\\). The \\(p\\) columns of \\(U\\) (the matrix given as USV$u in the output from the svd function above) are rescaled to have norms \\((s_12,s_22,…,s_p^2)\\). Each column has a different variance it is responsible for explaining. Notice that these will be decreasing numbers.\nIf we only want the first one then it is just \\(c_1=s_1 u_1\\). Notice that \\(||c_1||2=s_1tu_1 u_1^t s_1= s_1^2 u_1tu_1=s_12=_1\\)\nIf the matrix \\(X\\) comes from the study of \\(n\\) different samples or specimens, then the principal components provides new coordinates for these \\(n\\) points as in Figure 7.16. These are sometimes called the scores in the results of PCA functions.\n\nFigure 7.20: Another great xkcd take: this time eigenvectors.\nBefore we go into more detailed examples, let’s summarize what SVD and PCA provide:\n\nEach principal component has a variance measured by the corresponding eigenvalue, the square of the corresponding singular value.\nThe new variables are made to be orthogonal. Since they are also centered, this means they are uncorrelated. In the case of normal distributed data, this also means they are independent.\nWhen the variables are have been rescaled, the sum of the variances of all the variables is the number of variables (\\(=p\\)). The sum of the variances is computed by adding the diagonal of the crossproduct matrix9.\nThe principal components are ordered by the size of their eigenvalues. We always check the screeplot before deciding how many components to retain. It is also best practice to do as we did in Figure 7.18 and annotate each PC axis with the proportion of variance it explains.\n\nEigen Decomposition: The crossproduct of X with itself verifies \\[XtX=VSUtUSVt=VS2Vt=VVt\\] where \\(V\\) is called the eigenvector matrix of the symmetric matrix \\(X^tX\\) and \\(\\) is the diagonal matrix of eigenvalues of \\(X^tX\\).\n9 This sum of the diagonal elements is called the trace of the matrix.\n__\nTask\nLook up eigenvalue in the Wikipedia. Try to find a sentence that defines it without using a formula. Why would eigenvectors come into use in Cinderella (at a stretch)? (See the xkcd cartoon in Figure 7.20.)\n\nFor help with the basics of linear algebra, a motivated student pressed for time may consult Khan’s Academy. If you have more time and would like in depth coverage, Gil Strang’s MIT course is a classic, and some of the book is available online (Strang 2009).\n\n\n9.6.2 7.7.2 A complete analysis: the decathlon athletes\nWe started looking at these data earlier in this chapter. Here, we will follow step by step a complete multivariate analysis. First, let us have another look at the correlation matrix (rounded to 2 digits after the decimal point), which captures the bivariate associations. We already plotted it as a colored heatmap in Figure 7.3.\ncor(athletes) |&gt; round(2)__\n\n\n        m100  long weight  high  m400  m110  disc  pole javel m1500\nm100    1.00 -0.54  -0.21 -0.15  0.61  0.64 -0.05 -0.39 -0.06  0.26\nlong   -0.54  1.00   0.14  0.27 -0.52 -0.48  0.04  0.35  0.18 -0.40\nweight -0.21  0.14   1.00  0.12  0.09 -0.30  0.81  0.48  0.60  0.27\nhigh   -0.15  0.27   0.12  1.00 -0.09 -0.31  0.15  0.21  0.12 -0.11\nm400    0.61 -0.52   0.09 -0.09  1.00  0.55  0.14 -0.32  0.12  0.59\nm110    0.64 -0.48  -0.30 -0.31  0.55  1.00 -0.11 -0.52 -0.06  0.14\ndisc   -0.05  0.04   0.81  0.15  0.14 -0.11  1.00  0.34  0.44  0.40\npole   -0.39  0.35   0.48  0.21 -0.32 -0.52  0.34  1.00  0.27 -0.03\njavel  -0.06  0.18   0.60  0.12  0.12 -0.06  0.44  0.27  1.00  0.10\nm1500   0.26 -0.40   0.27 -0.11  0.59  0.14  0.40 -0.03  0.10  1.00\nThen we look at the screeplot, which will help us choose a rank \\(k\\) for representing the essence of these data.\npca.ath = dudi.pca(athletes, scannf = FALSE)\npca.ath$eig __\n\n\n [1] 3.4182381 2.6063931 0.9432964 0.8780212 0.5566267 0.4912275 0.4305952\n [8] 0.3067981 0.2669494 0.1018542\n\n\nfviz_eig(pca.ath, geom = \"bar\", bar_width = 0.3) + ggtitle(\"\")__\n\nFigure 7.21: The screeplot of the athletes data indicates that most of the variation in the data can be captured in a two-dimensional plane (spanned by the first two principal components).\nThe screeplot in Figure 7.21 shows a clear drop in the eigenvalues after the second one. This indicates that a good approximation will be obtained at rank 2. Let’s look at an interpretation of the first two axes by projecting the loadings of the original variables onto the two new ones, the principal components.\nfviz_pca_var(pca.ath, col.var = \"blue\", repel = TRUE) + ggtitle(\"\")__\n\nFigure 7.22: Correlation circle of the original variables.\nThe correlation circle Figure 7.22 displays the projection of the original variables onto the two first new principal axes. The angles between vectors are interpreted as correlations. On the right side of the plane, we have the track and field events (m110, m100, m400, m1500), and on the left, we have the throwing and jumping events. Maybe there is an opposition of skills as characterized in the correlation matrix. We did see the correlations were negative between variables from these two groups. How can we interpret this?\nIt seems that those who throw the best have lower scores in the track competitions. In fact, if we look at the original measurements, we can see what is happening. The athletes who run in short times are the stronger ones, as are the ones who throw or jump longer distances. We should probably change the scores of the track variables and redo the analysis.\n__\nQuestion 7.28\nWhat transformations of the variables induce the best athletic performances to vary in the same direction, i.e., be mostly positively correlated?\n__\nSolution\n__\nIf we change the signs on the running performances, almost all the variables will be positively correlated.\nrunningvars = grep(\"^m\", colnames(athletes), value = TRUE)\nrunningvars __\n\n\n[1] \"m100\"  \"m400\"  \"m110\"  \"m1500\"\n\n\nathletes[, runningvars] = -athletes[, runningvars]\ncor(athletes) |&gt; round(2)__\n\n\n       m100 long weight high  m400 m110  disc pole javel m1500\nm100   1.00 0.54   0.21 0.15  0.61 0.64  0.05 0.39  0.06  0.26\nlong   0.54 1.00   0.14 0.27  0.52 0.48  0.04 0.35  0.18  0.40\nweight 0.21 0.14   1.00 0.12 -0.09 0.30  0.81 0.48  0.60 -0.27\nhigh   0.15 0.27   0.12 1.00  0.09 0.31  0.15 0.21  0.12  0.11\nm400   0.61 0.52  -0.09 0.09  1.00 0.55 -0.14 0.32 -0.12  0.59\nm110   0.64 0.48   0.30 0.31  0.55 1.00  0.11 0.52  0.06  0.14\ndisc   0.05 0.04   0.81 0.15 -0.14 0.11  1.00 0.34  0.44 -0.40\npole   0.39 0.35   0.48 0.21  0.32 0.52  0.34 1.00  0.27  0.03\njavel  0.06 0.18   0.60 0.12 -0.12 0.06  0.44 0.27  1.00 -0.10\nm1500  0.26 0.40  -0.27 0.11  0.59 0.14 -0.40 0.03 -0.10  1.00\n\n\npcan.ath = dudi.pca(athletes, nf = 2, scannf = FALSE)\npcan.ath$eig __\n\n\n [1] 3.4182381 2.6063931 0.9432964 0.8780212 0.5566267 0.4912275 0.4305952\n [8] 0.3067981 0.2669494 0.1018542\nNow all the negative correlations are quite small. The screeplot will show no change, as the eigenvalues of the matrix are unaffected by the above sign flips. The only ouput that changes are the signs of the coefficients of the principal component loadings for the variables whose signs we flipped.\nfviz_pca_var(pcan.ath, col.var = \"blue\", repel = TRUE) + ggtitle(\"\")__\n\nFigure 7.23: Correlation circle after changing the signs of the running variables.\nFigure 7.23 shows the correlation circle of the transformed variables. We now see we have a broad common overall axis: all the arrows are pointing broadly in the same direction.\nWe now plot the athletes projected in the principal plane using:\nfviz_pca_ind(pcan.ath, repel = TRUE) + ggtitle(\"\") __\n\nFigure 7.24: First principal plane showing the projections of the athletes. Do you notice something about the organization of the numbers?\n__\nQuestion 7.29\nIf we look at the athletes themselves as they are shown in Figure 7.24, we notice a slight ordering effect. Do you see a relation between the performance of the athletes and their numbering in Figure 7.24 ?\n__\nSolution\n__\nIf you play join the dots following the order of the numbers, you will probably realize you are spending more time on one side of the plot than you would be if the numbers were randomly assigned.\nIt turns out there is complementary information available in the olympic dataset. An extra vector variable called score reports the final scores at the competition, the men’s decathlon at the 1988 Olympics.\nolympic$score __\n\n\n [1] 8488 8399 8328 8306 8286 8272 8216 8189 8180 8167 8143 8114 8093 8083 8036\n[16] 8021 7869 7860 7859 7781 7753 7745 7743 7623 7579 7517 7505 7422 7310 7237\n[31] 7231 7016 6907\nSo let us look at the scatterplot comparing the first principal component coordinate of the athletes to this score. This is shown in Figure 7.25. We can see a strong correlation between the two variables. We note that athlete number 1 (who in fact won the Olympic decathlon gold medal) has the highest score, but not the highest value in PC1. Why do you think that is?\nggplot(data = tibble(pc1 = pcan.ath$li[, 1], score = olympic$score, label = rownames(athletes)),\n       mapping = aes(y = score, x = pc1)) + \n   geom_text(aes(label = label)) + stat_smooth(method = \"lm\", se = FALSE)__\n\nFigure 7.25: Scatterplot between olympic$score and the first principal component. The points are labeled by their order in the data set. We can see a strong correlation. Why is it not a perfectly linear fit?\n\n\n9.6.3 7.7.3 How to choose \\(k\\), the number of dimensions ?\n\nFigure 7.26: A screeplot showing ‘dangerously’ similar variances. Choosing to cutoff at a hard threshold of 80% of the variance would give unstable PC plots. With so such cutoff, the axes corresponding to the 3D subspace of 3 similar eigenvalues are unstable and cannot be individually interpreted.\nWe have seen in the examples that the first step in PCA is to make the screeplot of the variances of the new variables (equal to the eigenvalues). We cannot decide how many dimensions are needed before seeing this plot. The reason is that there are situations when the principal components are ill-defined: when two or three successive PCs have very similar variances, giving a screeplot as in Figure 7.26, the subspace corresponding to a group of similar eigenvalues exists. In this case this would be 3D space generated by \\(u_2,u_3,u_4\\). The vectors are not meaningful individually and one cannot interpret their loadings. This is because a very slight change in one observations could give a completely different set of three vectors. These would generate the same 3D space, but could have very different loadings. We say the PCs are unstable.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "07-chap.html#pca-as-an-exploratory-tool-using-extra-information",
    "href": "07-chap.html#pca-as-an-exploratory-tool-using-extra-information",
    "title": "9  7.1 Goals for this chapter",
    "section": "9.7 7.8 PCA as an exploratory tool: using extra information",
    "text": "9.7 7.8 PCA as an exploratory tool: using extra information\nWe have seen that unlike regression, PCA treats all variables equally (to the extent that they were preprocessed to have equivalent standard deviations). However, it is still possible to map other continuous variables or categorical factors onto the plots in order to help interpret the results. Often we have complementary information on the samples, for example, diagnostic labels in the diabetes data or the cell types in the T-cell gene expression data.\nHere we see how we can use such extra variables to inform our interpretation. The best place to store such so-called metadata is in appropriate slots of the data object (such as in the Bioconductor SummarizedExperiment class); the second-best, in additional columns of the data frame that also contains the numeric data. In practice, such information is often stored in a more or less cryptic manner in the row names of the matrix. Below, we need to face the latter scenario, and we use substr gymnastics to extract the cell types and show the screeplot in Figure 7.27 and the PCA in Figure 7.28.\npcaMsig3 = dudi.pca(Msig3transp, center = TRUE, scale = TRUE,\n                    scannf = FALSE, nf = 4)\nfviz_screeplot(pcaMsig3) + ggtitle(\"\")__\n\nFigure 7.27: T-cell expression PCA screeplot.\nids = rownames(Msig3transp)\ncelltypes = factor(substr(ids, 7, 9))\nstatus = factor(substr(ids, 1, 3))\ntable(celltypes)__\n\n\ncelltypes\nEFF MEM NAI \n 10   9  11 \n\n\ncbind(pcaMsig3$li, tibble(Cluster = celltypes, sample = ids)) %&gt;%\nggplot(aes(x = Axis1, y = Axis2)) +\n  geom_point(aes(color = Cluster), size = 5) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  geom_vline(xintercept = 0, linetype = 2) +\n  scale_color_discrete(name = \"Cluster\") + coord_fixed()__\n\nFigure 7.28: PCA of gene expression for a subset of 156 genes involved in specificities of each of the three separate T-cell types: effector, naïve and memory. Again, we see that the plot is elongated along the the first axis, as that explains much of the variance. Notice that one of the T-cells seems to be mislabeled.\n\n9.7.1 7.8.1 Mass Spectroscopy Data Analysis\nThese data requires delicate preprocessing before we obtain our desired matrix with the relevant features as columns and the samples as rows. Starting with the raw mass spectroscopy readings, the steps involve extracting peaks of relevant features, aligning them across multiple samples and estimating peak heights. We refer the reader to the vignette of the Bioconductor xcms package for gruesome details. We load a matrix of data generated in such a way from the file mat1xcms.RData. The output of the below code is in Figures 7.29 and 7.30.\nload(\"../data/mat1xcms.RData\")\ndim(mat1)__\n\n\n[1] 399  12\n\n\npcamat1 = dudi.pca(t(mat1), scannf = FALSE, nf = 3)\nfviz_eig(pcamat1, geom = \"bar\", bar_width = 0.7) + ggtitle(\"\")__\n\nFigure 7.29: Screeplot showing the eigenvalues for the mice data.\ndfmat1 = cbind(pcamat1$li, tibble(\n    label = rownames(pcamat1$li),\n    number = substr(label, 3, 4),\n    type = factor(substr(label, 1, 2))))\npcsplot = ggplot(dfmat1,\n  aes(x=Axis1, y=Axis2, label=label, group=number, colour=type)) +\n geom_text(size = 4, vjust = -0.5)+ geom_point(size = 3)+ylim(c(-18,19))\npcsplot + geom_hline(yintercept = 0, linetype = 2) +\n  geom_vline(xintercept = 0, linetype = 2)__\n\nFigure 7.30: The first principal plane for the mat1 data. It explains 59% of the variance.\n__\nQuestion 7.30\nLooking at Figure 7.30, do the samples seem to be randomly placed in the plane? Do you notice any structure explained by the labels?\n__\nSolution\n__\nThe answer becomes (even more) evident if you make this plot. Knockouts are always below their paired wildtype sample. We will revisit this example when we look at supervised multivariate methods in our next chapter.\npcsplot + geom_line(colour = \"red\")__\n\n\n9.7.2 7.8.2 Biplots and scaling\nIn the previous example, the number of variables measured was too large to enable useful concurrent plotting of both variables and samples. In this example we plot the PCA biplot of a simple data set where chemical measurements were made on different wines for which we also have a categorical wine.class variable. We start the analysis by looking at the two-dimensional correlations and a heatmap of the variables.\nlibrary(\"pheatmap\")\nload(\"../data/wine.RData\")\nload(\"../data/wineClass.RData\")\nwine[1:2, 1:7]__\n\n\n  Alcohol MalicAcid  Ash AlcAsh  Mg Phenols Flav\n1   14.23      1.71 2.43   15.6 127    2.80 3.06\n2   13.20      1.78 2.14   11.2 100    2.65 2.76\n\n\npheatmap(1 - cor(wine), treeheight_row = 0.2)__\n\nFigure 7.31: The difference between 1 and the correlation can be used as a distance between variables and is used to make a heatmap of the associations between the variables.\nA biplot is a simultaneous representation of both the space of observations and the space of variables. In the case of a PCA biplot like Figure 7.32 the arrows represent the directions of the old variables as they project onto the plane defined by the first two new axes. Here the observations are just colored dots, the color has been chosen according to which type of wine is being plotted. We can interpret the variables’ directions with regards to the sample points, for instance the blue points are from the barbera group and show higher Malic Acid content than the other wines.\nwinePCAd = dudi.pca(wine, scannf=FALSE)\ntable(wine.class)__\n\n\nwine.class\n    barolo grignolino    barbera \n        59         71         48 \n\n\nfviz_pca_biplot(winePCAd, geom = \"point\", habillage = wine.class,\n   col.var = \"violet\", addEllipses = TRUE, ellipse.level = 0.69) +\n   ggtitle(\"\") + coord_fixed()__\n\nFigure 7.32: PCA biplot including ellipses for the three types of wine: barolo, grignolino and barbera. For each ellipsis, the axis lengths are given by one standard deviation. Small angles between the vectors Phenols, Flav and Proa indicate that they are strongly correlated, whereas Hue and Alcohol are uncorrelated.\nInterpretation of multivariate plots requires the use of as much of the available information as possible; here we have used the samples and their groups as well as the variables to understand the main differences between the wines.\n\n\n9.7.3 7.8.3 An example of weighted PCA\nSometimes we want to see variability between different groups or observations, but want to weight them. This can be the case if, e.g., the groups have very different sizes. Let’s re-examine the Hiiragi data we already saw in Chapter 3. In the code below, we select the wildtype (WT) samples and the top 100 features with the highest overall variance.\ndata(\"x\", package = \"Hiiragi2013\")\nxwt = x[, x$genotype == \"WT\"]\nsel = order(rowVars(Biobase::exprs(xwt)), decreasing = TRUE)[1:100]\nxwt = xwt[sel, ]\ntab = table(xwt$sampleGroup)\ntab __\n\n\n     E3.25 E3.5 (EPI)  E3.5 (PE) E4.5 (EPI)  E4.5 (PE) \n        36         11         11          4          4 \n\n\nxwt$weight = 1 / as.numeric(tab[xwt$sampleGroup])\npcaMouse = dudi.pca(as.data.frame(t(Biobase::exprs(xwt))),\n  row.w = xwt$weight,\n  center = TRUE, scale = TRUE, nf = 2, scannf = FALSE)\nfviz_eig(pcaMouse) + ggtitle(\"\")__\n\nFigure 7.33: Screeplot from the weighted PCA of the Hiiragi data. The drop after the second eigenvalue suggests that a two-dimensional PCA is appropriate.\nfviz_pca_ind(pcaMouse, geom = \"point\", col.ind = xwt$sampleGroup) +\n  ggtitle(\"\") + coord_fixed()__\nWe see from tab that the groups are represented rather unequally. To account for this, we reweigh each sample by the inverse of its group size. The function dudi.pca in the ade4 package has a row.w argument into which we can enter the weights. The output of the code is in Figures 7.33 and 7.34.\n\nFigure 7.34: Output from weighted PCA on the Hiiragi data. The samples are colored according to their groups.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "07-chap.html#summary-of-this-chapter",
    "href": "07-chap.html#summary-of-this-chapter",
    "title": "9  7.1 Goals for this chapter",
    "section": "9.8 7.9 Summary of this chapter",
    "text": "9.8 7.9 Summary of this chapter\nPreprocessing matrices Multivariate data analyses require “conscious” preprocessing. After consulting all the means, variances and one-dimensional histograms we saw how to rescale and recenter the data.\nProjecting onto new variables We saw how we can make projections into lower dimensions (2D planes and 3D are the most frequently used) of high dimensional data without losing too much information. PCA searches for new “more informative” variables that are linear combinations of the original (old) ones.\nMatrix decomposition PCA is based on finding decompositions of the matrix \\(X\\) called SVD. This decomposition provides a lower rank approximation and is equivalent to the eigendecomposition of \\(X^tX\\). The squares of the singular values are equal to the eigenvalues and to the variances of the new variables. We systematically plotted these values before deciding how many axes are necessary to reproduce the signal in the data.\nTwo eigenvalues which are quite close can give rise to scores or PC scores which are highly unstable. It is always necessary to look at the screeplot of the eigenvalues and avoid separating the axes corresponding to the these close eigenvalues. This may require using interactive three or four-dimensional projections, which are available in several R packages.\nBiplot representations The space of observations is naturally a \\(p\\)-dimensional space (the \\(p\\) original variables provide the coordinates). The space of variables is \\(n\\)-dimensional. Both decompositions we have studied (singular values / eigenvalues and singular vectors / eigenvectors) provide new coordinates for both of these spaces, sometimes we call one the dual of the other. We can plot the projection of both the observations and the variables onto the same eigenvectors. This provides a biplot that can be useful for interpreting the PCA output.\nProjecting other group variables Interpretation of PCA can also be facilitated by redundant or contiguous data about the observations.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "07-chap.html#further-reading",
    "href": "07-chap.html#further-reading",
    "title": "9  7.1 Goals for this chapter",
    "section": "9.9 7.10 Further reading",
    "text": "9.9 7.10 Further reading\nThe best way to deepen your understanding of singular value decomposition is to read Chapter 7 of Strang (2009). The whole book sets the foundations for the linear algebra necessary to understanding the meaning of the rank of matrix and the duality between row spaces and column spaces (Holmes 2006).\nComplete textbooks have been written on the subject of PCA and related methods. Mardia, Kent, and Bibby (1979) is a standard text that covers all multivariate methods in a classical way, with linear algebra and matrices. By making the parametric assumptions that the data come from multivariate normal distributions, Mardia, Kent, and Bibby (1979) also provide inferential tests for the number of components and limiting properties for principal components. Jolliffe (2002) is a book-long treatment of everything to do with PCA with extensive examples.\nWe can incorporate supplementary information into weights for the observations and the variables. This was introduced in the 1970’s by French data scientists, see Holmes (2006) for a review and Chapter 9 for further examples.\nImprovements to the interpretation and stability of PCA can be obtained by adding a penalty that minimizes the number of nonzero coefficients that appear in the linear combinations. Zou, Hastie, and Tibshirani (2006) and Witten, Tibshirani, and Hastie (2009) have developed sparse versions of principal component analysis, and their packages elasticnet and PMA provide implementations in R.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "07-chap.html#exercises",
    "href": "07-chap.html#exercises",
    "title": "9  7.1 Goals for this chapter",
    "section": "9.10 7.11 Exercises",
    "text": "9.10 7.11 Exercises\n__\nExercise 7.1\nRevise the material about svd by reading sections 1, 2, and 3 of the Wikipedia article about SVD. It will also be beneficial to read about the related eigenvalue decomposition by reading sections 1, 2, and 2.1 of the Wikipedia article about eigendecomposition of a matrix. We know that we can decompose a \\(n\\) row by \\(p\\) column rank 1 matrix \\(X\\) as:\n\\[ X = ]\n\nIf \\(X\\) has no rows and no columns which are all zeros, then is this decomposition unique?\nGenerate a rank-one matrix. Start by taking a vector of length 15 with values from 2 to 30 in increments of 2, and a vector of length 4 with values 3, 6, 9, 12, then take their outer product.\n\nu = seq(2, 30, by = 2)\nv = seq(3, 12, by = 3)\nX1 = u %*% t(v)__\nWhy do we have to take t(v)?\n\nNow we add some noise in the form a matrix we call Materr so we have an “approximately rank-one” matrix.\n\nMaterr = matrix(rnorm(60,1),nrow=15,ncol=4)\nX = X1+Materr __\nVisualize \\(X\\) using ggplot.\n\nRedo the same analyses with a rank 2 matrix.\n\n__\nSolution\n__\nNote that X1 can also be computed as\nouter(u, v)__\n\n\nggplot(data=data.frame(X), aes(x=X1, y=X2, col=X3, size=X4)) + geom_point()__\nHere we see that the data looks linear in all four dimensions. This is what it means to be of rank-one. Now let’s consider a rank 2 matrix.\nn = 100\np = 4\nY2 = outer(rnorm(n), rnorm(p)) + outer(rnorm(n), rnorm(p))\nhead(Y2)__\n\n\n            [,1]       [,2]         [,3]        [,4]\n[1,] -0.44143871  2.3213197  0.433215525 -1.35523790\n[2,]  0.79620920 -1.0748037  1.217052906 -1.13096295\n[3,]  0.16787281  0.2259296  0.547203332 -0.75836031\n[4,]  0.87269426 -1.9208649  0.856966180 -0.38621340\n[5,]  0.03751521 -0.1480678 -0.005217966  0.05864122\n[6,]  0.50195482 -2.0409896 -0.108241027  0.85336630\n\n\nggplot(data=data.frame(Y2), aes(x=X1, y=X2, col=X3, size=X4)) + geom_point()__\nNow there are obviously at least two dimensions because if we project the data onto the first two coordinates (by default called X1 and X2 when you convert a matrix into a data frame in R), then the data varies in both dimensions. So the next step is to try to decide if there are more than two dimensions. The top right points are the closest to you (they’re biggest) and as you go down and left in the plot those points are farther away. In the left are the bluest points and they seem to get darker linearly as you move right. As you can probably tell, it is very hard to visually discover a low dimensional space in higher dimensions, even when “high dimensions” only means 4! This is one reason why we rely on the singular value decomposition.\nsvd(Y2)$d # two non-zero eigenvalues __\n\n\n[1] 2.637465e+01 1.266346e+01 3.144564e-15 1.023131e-15\n\n\nY = Y2 + matrix(rnorm(n*p, sd=0.01),n,p) # add some noise to Y2\nsvd(Y)$d # four non-zero eigenvalues (but only 2 big ones)__\n\n\n[1] 26.39673712 12.68547439  0.10735103  0.09104741\nHere we have two dimensions which are non-zero and two dimensions which are approximately 0 (for “Y2”, they are within square root of computer tolerance of 0).\n__\nExercise 7.2\n\ncreate a first a matrix of highly correlated bivariate data such as that shown in Figure 7.35.\nHint: Use the function mvrnorm.\n\nCheck the rank of the matrix by looking at its singular values.\n\nperform a PCA and show the rotated principal component axes.\n\n__\nSolution\n__\n\nwe generate correlated bivariate normal data using:\n\nlibrary(\"MASS\")\nmu1 = 1; mu2 = 2; s1=2.5; s2=0.8; rho=0.9;\nsigma = matrix(c(s1^2, s1*s2*rho, s1*s2*rho, s2^2),2)\nsim2d = data.frame(mvrnorm(50, mu = c(mu1,mu2), Sigma = sigma))\nsvd(scale(sim2d))$d __\n\n\n[1] 9.647686 2.218592\n\n\nsvd(scale(sim2d))$v[,1]__\n\n\n[1] 0.7071068 0.7071068\n\nWe use prcomp to perform a PCA and the scores provide the desired rotation.\n\nrespc = princomp(sim2d)\ndfpc  = data.frame(pc1=respc$scores[,1], \n                   pc2=respc$scores[,2])\n\nggplot(data.frame(sim2d), aes(x=X1,y=X2)) + geom_point()\nggplot(dfpc, aes(x=pc1, y=pc2)) + geom_point() + coord_fixed(2)__\n\n\n\\(\\)\n\n\n\n\\(\\)\n\nFigure 7.35: The original data shown in scatterplot (A) and the plot obtained using the principal component rotation (B).\n__\nExercise 7.3\nPart (a) in Figure 7.35 shows a very elongated plotting region, why?\nWhat happens if you do not use the coord_fixed() option and have a square plotting zone? Why can this be misleading?\n__\nExercise 7.4\nLet’s revisit the Hiiragi data and compare the weighted and unweighted approaches.\n\nmake a correlation circle for the unweighted Hiiragi data xwt. Which genes have the best projections on the first principal plane (best approximation)?\nmake a biplot showing the labels of the extreme gene-variables that explain most of the variance in the first plane. Add the the sample-points.\n\nAbbott, Edwin A. 1884. Flatland: A Romance of Many Dimensions. OUP Oxford.\nFlury, Bernard. 1997. A First Course in Multivariate Statistics. Springer.\nHolmes, Susan. 2006. “Multivariate Analysis: The French way.” In Probability and Statistics: Essays in Honor of David a. Freedman , edited by D. Nolan and T. P. Speed. Vol. 56. IMS Lecture Notes–Monograph Series. Beachwood, OH: IMS. http://www.imstat.org/publications/lecnotes.htm.\nHolmes, Susan, Michael He, Tong Xu, and Peter P Lee. 2005. “Memory t Cells Have Gene Expression Patterns Intermediate Between Naive and Effector.” PNAS 102 (15): 5519–23.\nHotelling, Harold. 1933. “Analysis of a Complex of Statistical Variables into Principal Components.” Journal of Educational Psychology 24 (6): 417–41.\nJolicoeur, Pierre, and James E Mosimann. 1960. “Size and Shape Variation in the Painted Turtle. A Principal Component Analysis.” Growth 24: 339–54.\nJolliffe, Ian. 2002. Principal Component Analysis. Wiley Online Library.\nMardia, Kanti, John T Kent, and John M Bibby. 1979. Multiariate Analysis. New York: Academic Press.\nPearson, Karl. 1901. “LIII. On Lines and Planes of Closest Fit to Systems of Points in Space.” The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 2 (11): 559–72.\nStrang, Gilbert. 2009. Introduction to Linear Algebra. Fourth. Wellesley- Cambridge Press.\nWitten, Daniela M, Robert Tibshirani, and Trevor Hastie. 2009. “A Penalized Matrix Decomposition, with Applications to Sparse Principal Components and Canonical Correlation Analysis.” Biostatistics , kxp008.\nZou, Hui, Trevor Hastie, and Robert Tibshirani. 2006. “Sparse Principal Component Analysis.” Journal of Computational and Graphical Statistics 15 (2): 265–86.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "08-chap.html",
    "href": "08-chap.html",
    "title": "10  8.1 Goals of this chapter",
    "section": "",
    "text": "10.1 8.2 Some core concepts\nMany measurement devices in biotechnology are based on massively parallel sampling and counting of molecules. One example is high-throughput DNA sequencing. Its applications fall broadly into two main classes of data output: in the first case, the output of interest are the sequences themselves, perhaps also their polymorphisms or differences to other sequences seen before. In the second case, the sequences themselves are more or less well-understood (say, we have a well-assembled and annotated genome), and our interest is on how abundant different sequence regions are in our sample.\nFor instance, in RNA-Seq (Ozsolak and Milos 2011), we sequence the RNA molecules found in a population of cells or in a tissue.\nStrictly speaking, we don’t sequence the RNA but the complementary DNA (cDNA) obtained from reverse transcription. The pool of all RNA might be reduced to a subset of interest (e.,g., messenger RNA) by biochemical means, such as poly-A selection or ribosomal RNA depletion. Sensitive variants of RNA-Seq exist that enable assaying single cells, and large numbers of them.\nIn ChIP-Seq , we sequence DNA regions that are bound to particular DNA- binding proteins (selected by immuno-precipitation); in RIP-Seq , RNA molecules or regions of them bound to a particular RNA-binding protein; in DNA-Seq , we sequence genomic DNA and are interested in the prevalence of genetic variants in heterogeneous populations of cells, for instance the clonal composition of a tumor. In high-throughput chromatin conformation capture (HiC) we aim to map the 3D spatial arrangement of DNA; in genetic screens (using, say, RNAi or CRISPR-Cas9 libraries for perturbation and high-throughput sequencing for readout), we’re interested in the proliferation or survival of cells upon gene knockdown, knockout or modification. In microbiome analysis, we study the abundance of different microbial species in complex microbial habitats.\nIdeally we might want to sequence and count all molecules of interest in the sample. Generally this is not possible: the biochemical protocols are not 100% efficient, and some molecules or intermediates get lost along the way. Moreover it’s often also not even necessary. Instead, we sequence and count a statistical sample. The sample size will depend on the complexity of the sequence pool assayed; it can go from tens of thousands to billions. This sampling nature of the data is important when it comes to analyzing them. We hope that the sampling is sufficiently representative for us to identify interesting trends and patterns.\nIn this chapter, we will become familiar with count data in high-throughput sequencing applications such as RNA-Seq. We will understand and model the sampling processes that underlie the data in order to interpret them. Our main aim is to detect and quantify systematic changes between samples from different conditions, say untreated versus treated , where the task is to distinguish such systematic changes from sampling variations and experimental variability within the same conditions. In order to do this, we will also equip ourselves with a set of needed statistical concepts and tools:\nIn fact, these concepts have a much wider range of applications: they can also be applied to other types of data where want to detect differences in noisy data as a function of some experimental covariate. In particular, the framework of generalized linear models is quite abstract and generic, but this has the advantage that it can be adapted to many different data types, so that we don’t need to reinvent the wheel, but rather can immediately enjoy a wide range of associated tools and diagnostics.\nAs a bonus, we will also look at data transformations that make the data amenable to unsupervised methods such as those that we saw in Chapters 5 and 7, and which make it easier to visualize the data.\nBefore we start, let’s settle some key terminology.\n1 We refer to https://www.illumina.com/techniques/sequencing.html\n2 For any particular application, it’s best to check the recent literature on the most appropriate approaches and choices.\n3 E.g., in the case of RNA-Seq, the genome together with an annotation of its transcripts.\nBetween sequencing and counting, there is an important aggregation or clustering step involved, which aggregates sequences that belong together: for instance, all reads belonging to the same gene (in RNA-Seq), or to the same binding region (ChIP-Seq). There are several approaches to this and choices to be made, depending on the aim of the experiment2. The methods include explicit alignment or hash-based mapping to a reference sequence3, and reference- independent sequence-similarity based clustering of the reads – especially if there is no obvious reference, such as in metagenomics or metatranscriptomics. We need to choose whether to consider different alleles or isoforms separately, or to merge them into an equivalence class. For simplicity, we’ll use the term gene in this chapter for these operational aggregates, even though they can be various things depending on the particular application.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 Goals of this chapter</span>"
    ]
  },
  {
    "objectID": "08-chap.html#some-core-concepts",
    "href": "08-chap.html#some-core-concepts",
    "title": "10  8.1 Goals of this chapter",
    "section": "",
    "text": "A sequencing library is the collection of DNA molecules used as input for the sequencing machine.\nFragments are the molecules being sequenced. Since the currently most widely used technology1 can only deal with molecules of length around 300–1000 nucleotides, these are obtained by fragmenting the (generally longer) DNA or cDNA molecules of interest.\nA read is the sequence obtained from a fragment. With the current technology, the read covers not the whole fragment, but only one or both ends of it, and the read length on either side is up to around 150 nucleotides.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 Goals of this chapter</span>"
    ]
  },
  {
    "objectID": "08-chap.html#count-data",
    "href": "08-chap.html#count-data",
    "title": "10  8.1 Goals of this chapter",
    "section": "10.2 8.3 Count data",
    "text": "10.2 8.3 Count data\nLet us load an example dataset. It resides in the experiment data package pasilla.\nfn = system.file(\"extdata\", \"pasilla_gene_counts.tsv\",\n                  package = \"pasilla\", mustWork = TRUE)\ncounts = as.matrix(read.csv(fn, sep = \"\\t\", row.names = \"gene_id\"))__\n\n\n\nIn the code shown here, we use the function system.file to locate a file that is shipped together with the pasilla package. When you work with your own data, you will need to prepare the matrix counts yourself.\n\n\nIn the code shown here, we use the function system.file to locate a file that is shipped together with the pasilla package. When you work with your own data, you will need to prepare the matrix counts yourself.\nThe data are stored as a rectangular table in a tab-delimited file, which we’ve read into the matrix counts.\ndim(counts)__\n\n\n[1] 14599     7\n\n\ncounts[ 2000+(0:3), ]__\n\n\n            untreated1 untreated2 untreated3 untreated4 treated1 treated2\nFBgn0020369       3387       4295       1315       1853     4884     2133\nFBgn0020370       3186       4305       1824       2094     3525     1973\nFBgn0020371          1          0          1          1        1        0\nFBgn0020372         38         84         29         28       63       28\n            treated3\nFBgn0020369     2165\nFBgn0020370     2120\nFBgn0020371        0\nFBgn0020372       27\nThe matrix tallies the number of reads seen for each gene in each sample. We call it the count table. It has 14599 rows, corresponding to the genes, and 7 columns, corresponding to the samples. When loading data from a file, a good plausibility check is to print out some of the data, and maybe not only at the very beginning, but also at some random point in the middle, as we have done above.\nThe table is a matrix of integer values: the value in the \\(i\\)th row and the \\(j\\)th column of the matrix indicates how many reads have been mapped to gene \\(i\\) in sample \\(j\\). The statistical sampling models that we discuss in this chapter rely on the fact that the values are the direct, “raw” counts of sequencing reads – not some derived quantity, such as normalized counts, counts of covered base pairs, or the like; this would only lead to nonsensical results.\n\n10.2.1 8.3.1 The challenges of count data\nWhat are the challenges that we need to overcome with such count data?\n\nThe data have a large dynamic range, starting from zero up to millions. The variance, and more generally, the distribution shape of the data in different parts of the dynamic range are very different. We need to take this phenomenon, called heteroskedasticity , into account.\nThe data are non-negative integers, and their distribution is not symmetric – thus normal or log-normal distribution models may be a poor fit.\nWe need to understand the systematic sampling biases and adjust for them. Confusingly, this is often called normalization. Examples are the total sequencing depth of an experiment (even if the true abundance of a gene in two libraries is the same, we expect different numbers of reads for it depending on the total number of reads sequenced), or differing sampling probabilities (even if the true abundance of two genes within a biological sample is the same, we expect different numbers of reads for them if their biophysical properties differ, such as length, GC content, secondary structure, binding partners).\nWe need to understand the stochastic properties of the sampling, as well as other sources of stochastic experimental variation. For studies with large numbers of biological samples, this is usually straightforward, and we can even fall back on resampling- or permutation-based methods. For designed experiments, however, sample sizes tend to be limited.\n\n\n\n\nThere are important conceptual and practical differences between experiments and studies – see also sec- design.\n\n\nThere are important conceptual and practical differences between experiments and studies – see also Chapter 13.\nFor instance, there are four replicates from the untreated and three from the treated condition in the pasilla data. This means that resampling- or permutation-based methods will not have enough power. To proceed, we need to make distributional assumptions. Essentially, what such assumptions do is that they let us compute the probabilities of rare events in the tails of the distribution – i.e., extraordinarily high or low counts – from a small number of distribution parameters.\n\nBut even that is often not enough, in particular the estimation of dispersion parameters4 is difficult with small sample sizes. In that case, we need to make further assumptions, such as that genes with similar locations also have similar dispersions. This is called sharing of information across genes, and we’ll come back to it in Section 8.10.1.\n\n4 Distributions can be parameterized in various ways; often the parameters correspond to some measure of location and some measure of dispersion; a familiar measure of location is the mean, and a familiar measure of dispersion is the variance (or standard deviation), but for some distributions other measures are also in use.\n\n\n10.2.2 8.3.2 RNA-Seq: what about gene structures, splicing, isoforms?\nEukaryotic genes are complex: most of them consist of multiple exons, and mRNAs result from concatenation of exons through a process called splicing. Alternative splicing and multiple possible choices of start and stop sites enable the generation of multiple, alternative isoforms from the same gene locus. It is possible to use high-throughput sequencing to detect the isoform structures of transcripts. From the fragments that are characteristic for specific isoforms, it is also possible to detect isoform specific abundances. With current RNA-Seq data, which only give us relatively short fragments of the full-length isoforms, it tends to be difficult to assemble and deconvolute full-length isoform structures and abundances (Steijger et al. 2013). Because of that, procedures with the more modest aim of making only local statements (e.g., inclusion or exclusion of individual exons) have been formulated (Anders, Reyes, and Huber 2012), and these can be more robust. We can expect that future technologies will sequence full-length transcripts.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 Goals of this chapter</span>"
    ]
  },
  {
    "objectID": "08-chap.html#modeling-count-data",
    "href": "08-chap.html#modeling-count-data",
    "title": "10  8.1 Goals of this chapter",
    "section": "10.3 8.4 Modeling count data",
    "text": "10.3 8.4 Modeling count data\n\n10.3.1 8.4.1 Dispersion\nConsider a sequencing library that contains \\(n_1\\) fragments corresponding to gene 1, \\(n_2\\) fragments for gene 2, and so on, with a total library size of \\(n = n_1+n_2+\\). We submit the library to sequencing and determine the identity of \\(r\\) randomly sampled fragments. A welcome simplification comes from looking at the orders of magnitude of these numbers:\n\nthe number of genes is in the tens of thousands;\nthe value of \\(n\\) depends on the amount of cells that were used to prepare, but for bulk RNA-Seq it will be in the billions or trillions;\nthe number of reads \\(r\\) is usually in the tens of millions, and thus much smaller than \\(n\\).\n\nFrom this we can conclude that the probability that a given read maps to the \\(i^{}\\) gene is \\(p_i=n_i/n\\), and that this is pretty much independent of the outcomes for all the other reads. So we can model the number of reads for gene \\(i\\) by a Poisson distribution, where the rate of the Poisson process is the product of \\(p_i\\), the initial proportion of fragments for the \\(i^{}\\) gene, times \\(r\\), that is: \\(_i=rp_i\\).\n\n\n\nIn principle, we should consider sampling without replacement and the multinomial distribution here: the probability of sampling a read for the i^{\\text{th}} gene depends on how many times the same gene, and other genes, have already been sampled. However, these dependencies are so negligibly small that we’ll ignore them. This is because n is so much larger than r, the number of genes is large, and each individual n_i is small compared to n.\n\n\nIn principle, we should consider sampling without replacement and the multinomial distribution here: the probability of sampling a read for the \\(i^{}\\) gene depends on how many times the same gene, and other genes, have already been sampled. However, these dependencies are so negligibly small that we’ll ignore them. This is because \\(n\\) is so much larger than \\(r\\), the number of genes is large, and each individual \\(n_i\\) is small compared to \\(n\\).\nIn practice, we are usually not interested in modeling the read counts within a single library, but in comparing the counts between libraries. That is, we want to know whether any differences that we see between different biological conditions – say, the same cell line with and without drug treatment – are larger than expected “by chance”, i.e., larger than what we may expect even between biological replicates. Empirically, it turns out that replicate experiments vary more than what the Poisson distribution predicts. Intuitively, what happens is that \\(p_i\\) and therefore also \\(_i\\) vary even between biological replicates; perhaps the temperature at which the cells grew was slightly different, or the amount of drug added varied by a few percent, or the incubation time was slightly longer. To account for that, we need to add another layer of modeling on top. We already saw hierarchical models and mixtures in Chapter 4. It turns out that the gamma-Poisson (a.k.a. negative binomial) distribution suits our modeling needs. Instead of a single \\(\\) – which represents both mean and variance –, this distribution has two parameters. In principle, these can be different for each gene, and we will come back to the question of how to estimate them from the data.\n\n\n10.3.2 8.4.2 Normalization\nOften, there are systematic biases that have affected the data generation and are worth taking into account. Unfortunately, the term normalization is commonly used for that aspect of the analysis, even though it is misleading: it has nothing to do with the normal distribution, norms in a vector space, or normal vectors. Rather, what we aim for is identifying the nature and estimating the magnitude of systematic biases, and take them into account in our model-based analysis of the data.\nThe most important systematic bias stems from variations in the total number of reads in each sample. If we have more reads for one library than in another, then we might assume that, everything else being equal, the counts are proportional to each other with some proportionality factor \\(s\\). Naively, we could propose that a decent estimate of \\(s\\) for each sample is simply given by the sum of the counts of all genes. However, it turns out that we can do better. To understand this, a toy example helps.\n\nFigure 8.1: Size factor estimation. The points correspond to hypothetical genes whose counts in two samples are indicated by their \\(x\\)- and \\(y\\)-coordinates. The lines indicate two different ways of size factor estimation explained in the text.\nConsider a dataset with 5 genes and two samples as displayed in Figure 8.1. If we estimate \\(s\\) for each of the two samples by its sum of counts, then the slope of the blue line represents their ratio. According to this, gene C is down-regulated in sample 2 compared to sample 1, while the other genes are all somewhat up-regulated. If we now instead estimate \\(s\\) such that their ratios correspond to the red line, then we will still conclude that gene C is down-regulated, while the other genes are unchanged. The second version is more parsimonious and is often preferred by scientists. The slope of the red line can be obtained by robust regression. This is what the DESeq2 method does.\n__\nQuestion 8.1\nFor the example dataset count of Section 8.3, how does the output of DESeq2 ’s estimateSizeFactorsForMatrix compare to what you get by simply taking the column sums?\n__\nSolution\n__\nSee Figure 8.2, produced by the code below. In this case, there is not much difference, the results are nearly proportional.\nlibrary(\"tibble\")\nlibrary(\"ggplot2\")\nlibrary(\"DESeq2\")\nggplot(tibble(\n  `size factor` = estimateSizeFactorsForMatrix(counts),\n  `sum` = colSums(counts)), aes(x = `size factor`, y = `sum`)) +\n  geom_point()__\n\nFigure 8.2: Size factors versus sums for the pasilla data.\n__\nTask\nLocate the R sources for this book and have a look at the code that produces Figure 8.1.\n__\nQuestion 8.2\nPlot the mean-variance relationship for the biological replicates in the pasilla dataset.\n__\nSolution\n__\nSee Figure 8.3, produced by the following code.\nlibrary(\"matrixStats\")\nsf = estimateSizeFactorsForMatrix(counts)\nncounts  = counts / matrix(sf,\n   byrow = TRUE, ncol = ncol(counts), nrow = nrow(counts))\nuncounts = ncounts[, grep(\"^untreated\", colnames(ncounts)),\n                     drop = FALSE]\nggplot(tibble(\n        mean = rowMeans(uncounts),\n        var  = rowVars( uncounts)),\n     aes(x = log(mean), y = log(var))) +\n  geom_hex() + coord_fixed() + theme(legend.position = \"none\") +\n  geom_abline(slope = 1:2, color = c(\"forestgreen\", \"red\"))__\n\nFigure 8.3: Variance versus mean for the (size factor adjusted) counts data. The axes are logarithmic. Also shown are lines through the origin with slopes 1 (green) and 2 (red).\nThe green line (slope 1) is what we expect if the variance (\\(v\\)) equals the mean (\\(m\\)), as is the case for a Poisson-distributed random variable: \\(v=m\\). We see that this approximately fits the data in the lower range. The red line (slope 2) corresponds to the quadratic mean-variance relationship \\(v=m^2\\); lines parallel to it (not shown) would represent \\(v = cm^2\\) for various values of \\(c\\). We can see that in the upper range of the data, the quadratic relationship approximately fits the data, for some value of \\(c&lt;1\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 Goals of this chapter</span>"
    ]
  },
  {
    "objectID": "08-chap.html#a-basic-analysis",
    "href": "08-chap.html#a-basic-analysis",
    "title": "10  8.1 Goals of this chapter",
    "section": "10.4 8.5 A basic analysis",
    "text": "10.4 8.5 A basic analysis\n\n10.4.1 8.5.1 Example dataset: the pasilla data\nLet’s return to the pasilla data from Section 8.3. These data are from an experiment on Drosophila melanogaster cell cultures that investigated the effect of RNAi knock-down of the splicing factor pasilla (Brooks et al. 2011) on the cells’ transcriptome. There were two experimental conditions, termed untreated and treated in the header of the count table that we loaded. They correspond to negative control and to siRNA against pasilla. The experimental metadata of the 7 samples in this dataset are provided in a spreadsheet-like table, which we load.\n\n\n\nIn the code shown here, we load the file pasilla_sample_annotation.csv that comes with the pasilla package. We locate it with the function system.file. When you work with your own data, you will need to prepare an analogous file, or directly a dataframe like pasillaSampleAnno.\n\n\nIn the code shown here, we load the file pasilla_sample_annotation.csv that comes with the pasilla package. We locate it with the function system.file. When you work with your own data, you will need to prepare an analogous file, or directly a dataframe like pasillaSampleAnno.\nannotationFile = system.file(\"extdata\",\n  \"pasilla_sample_annotation.csv\",\n  package = \"pasilla\", mustWork = TRUE)\npasillaSampleAnno = readr::read_csv(annotationFile)\npasillaSampleAnno __\n\n\n# A tibble: 7 × 6\n  file    condition type  `number of lanes` total number of read…¹ `exon counts`\n  &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                          &lt;dbl&gt;\n1 treate… treated   sing…                 5 35158667                    15679615\n2 treate… treated   pair…                 2 12242535 (x2)               15620018\n3 treate… treated   pair…                 2 12443664 (x2)               12733865\n4 untrea… untreated sing…                 2 17812866                    14924838\n5 untrea… untreated sing…                 6 34284521                    20764558\n6 untrea… untreated pair…                 2 10542625 (x2)               10283129\n7 untrea… untreated pair…                 2 12214974 (x2)               11653031\n# ℹ abbreviated name: ¹​`total number of reads`\nAs we see here, the overall dataset was produced in two batches, the first one consisting of three sequencing libraries that were subjected to single read sequencing, the second batch consisting of four libraries for which paired end sequencing was used. As so often, we need to do some data wrangling: we replace the hyphens in the type column by underscores, as arithmetic operators in factor levels are discouraged by DESeq2 , and convert the type and condition columns into factors, explicitly specifying our prefered order of the levels (the default is alphabetical).\nlibrary(\"dplyr\")\npasillaSampleAnno = mutate(pasillaSampleAnno,\ncondition = factor(condition, levels = c(\"untreated\", \"treated\")),\ntype = factor(sub(\"-.*\", \"\", type), levels = c(\"single\", \"paired\")))__\nWe note that the design is approximately balanced between the factor of interest, condition, and the “nuisance factor” type:\nwith(pasillaSampleAnno,\n       table(condition, type))__\n\n\n           type\ncondition   single paired\n  untreated      2      2\n  treated        1      2\nDESeq2 uses a specialized data container, called DESeqDataSet to store the datasets it works with. Such use of specialized containers – or, in R terminology, classes – is a common principle of the Bioconductor project, as it helps users to keep together related data. While this way of doing things requires users to invest a little more time upfront to understand the classes, compared to just using basic R data types like matrix and dataframe, it helps avoiding bugs due to loss of synchronization between related parts of the data. It also enables the abstraction and encapsulation of common operations that could be quite wordy if always expressed in basic terms5. DESeqDataSet is an extension of the class SummarizedExperiment in Bioconductor. The SummarizedExperiment class is also used by many other packages, so learning to work with it will enable you to use quite a range of tools.\n5 Another advantage is that classes can contain validity methods, which make sure that the data always fulfill certain expectations, for instance, that the counts are positive integers, or that the columns of the counts matrix align with the rows of the sample annotation dataframe.\n6 Note how in the code below, we have to put in extra work to match the column names of the counts object with the file column of the pasillaSampleAnno dataframe, in particular, we need to remove the \"fb\" that happens to be used in the file column for some reason. Such data wrangling is very common. One of the reasons for storing the data in a DESeqDataSet object is that we then no longer have to worry about such things.\nWe use the constructor function DESeqDataSetFromMatrix to create a DESeqDataSet from the count data matrix counts and the sample annotation dataframe pasillaSampleAnno6.\nmt = match(colnames(counts), sub(\"fb$\", \"\", pasillaSampleAnno$file))\nstopifnot(!any(is.na(mt)))\n\npasilla = DESeqDataSetFromMatrix(\n  countData = counts,\n  colData   = pasillaSampleAnno[mt, ],\n  design    = ~ condition)\nclass(pasilla)__\n\n\n[1] \"DESeqDataSet\"\nattr(,\"package\")\n[1] \"DESeq2\"\n\n\nis(pasilla, \"SummarizedExperiment\")__\n\n\n[1] TRUE\nThe SummarizedExperiment class – and therefore DESeqDataSet – also contains facilities for storing annotation of the rows of the count matrix. For now, we are content with the gene identifiers from the row names of the counts table.\n__\nQuestion 8.3\nHow can we access the row metadata of a SummarizedExperiment object, i.e., how can we read it out, how can we change it?\n__\nSolution\n__\nCheck the manual page of the SummarizedExperiment class and of the methods rowData and rowData&lt;-.\n\n\n10.4.2 8.5.2 The DESeq2 method\nAfter these preparations, we are now ready to jump straight into differential expression analysis. Our aim is to identify genes that are differentially abundant between the treated and the untreated cells. To this end, we will apply a test that is conceptually similar to the \\(t\\)-test, which we encountered in Section 6.5, although mathematically somewhat more involved. We will postpone these details for now, and will come back to them in Section 8.7. A choice of standard analysis steps are wrapped into a single function, DESeq.\npasilla = DESeq(pasilla)__\nThe DESeq function is simply a wrapper that calls, in order, the functions estimateSizeFactors (for normalization, as discussed in Section 8.4.2), estimateDispersions (dispersion estimation) and nbinomWaldTest (hypothesis tests for differential abundance). The test is between the two levels extttuntreated and exttttreated of the factor condition, since this is what we specified when we constructed the pasilla object through the argument design=\\simcondition. You can always call each of these three functions individually if you want to modify their behavior or interject custom steps. Let us look at the results.\nres = results(pasilla)\nres[order(res$padj), ] |&gt; head()__\n\n\nlog2 fold change (MLE): condition treated vs untreated \nWald test p-value: condition treated vs untreated \nDataFrame with 6 rows and 6 columns\n             baseMean log2FoldChange     lfcSE      stat       pvalue\n            &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;    &lt;numeric&gt;\nFBgn0039155   730.596       -4.61901 0.1687068  -27.3789 4.88599e-165\nFBgn0025111  1501.411        2.89986 0.1269205   22.8479 1.53430e-115\nFBgn0029167  3706.117       -2.19700 0.0969888  -22.6521 1.33042e-113\nFBgn0003360  4343.035       -3.17967 0.1435264  -22.1539 9.56283e-109\nFBgn0035085   638.233       -2.56041 0.1372952  -18.6490  1.28772e-77\nFBgn0039827   261.916       -4.16252 0.2325888  -17.8965  1.25663e-71\n                    padj\n               &lt;numeric&gt;\nFBgn0039155 4.06661e-161\nFBgn0025111 6.38497e-112\nFBgn0029167 3.69104e-110\nFBgn0003360 1.98979e-105\nFBgn0035085  2.14354e-74\nFBgn0039827  1.74316e-68\n\n\n10.4.3 8.5.3 Exploring the results\nThe first step after a differential expression analysis is the visualization of the following three or four basic plots:\n\nthe histogram of p-values (Figure 8.4),\nthe MA plot (Figure 8.5) and\nan ordination plot (Figure 8.6).\nIn addition, a heatmap (Figure 8.7) can be instructive.\n\nThese are essential data quality assessment measures – and the general advice on quality assessment and control given in Section 13.6 also applies here.\nThe p-value histogram is straightforward (Figure 8.4).\nggplot(as(res, \"data.frame\"), aes(x = pvalue)) +\n  geom_histogram(binwidth = 0.01, fill = \"Royalblue\", boundary = 0)__\n\nFigure 8.4: Histogram of p-values of a differential expression analysis.\nThe distribution displays two main components: a uniform background with values between 0 and 1, and a peak of small p-values at the left. The uniform background corresponds to the non-differentially expressed genes. Usually this is the majority of genes. The left hand peak corresponds to differentially expressed genes7. As we already saw in Chapter 6, the ratio of the level of the background to the height of the peak gives us a rough indication of the false discovery rate (FDR) that would be associated with calling the genes in the leftmost bin differentially expressed. In our case, the leftmost bin contains all p-values between 0 and 0.01, which correspond to 993 genes. The background level is at around 100, so the FDR associated with calling all genes in the leftmost bin would be around 10%.\n7 For the data shown here, the histogram also contains a few isolated peaks in the middle or towards the right; these stem from genes with small counts and reflect the discreteness of the data.\nSometimes it turns out that the background distribution is not uniform, but shows a tilted shape with an increase towards the right. This tends to be an indication of batch effects; you can explore this further in Exercise 8.1.\nTo produce the MA plot, we can use the function plotMA in the DESeq2 package (Figure 8.5).\nplotMA(pasilla, ylim = c( -2, 2))__\n\nFigure 8.5: MA plot: fold change versus mean of size-factor normalized counts. Logarithmic scaling is used for both axes. By default, points are colored red if the adjusted p-value is less than 0.1. Points which fall out of the \\(y\\)-axis range are plotted as triangles.\nTo produce PCA plots similar to those we saw in Chapter 7, we can use the DESeq2 function plotPCA (Figure 8.6).\npas_rlog = rlogTransformation(pasilla)\nplotPCA(pas_rlog, intgroup=c(\"condition\", \"type\")) + coord_fixed()__\n\nFigure 8.6: PCA plot. The 7 samples are shown in the 2D plane spanned by their first two principal components.\nAs we saw in the previous chapter, this type of plot is useful for visualizing the overall effect of experimental covariates and/or to detect batch effects. Here, the first principal axis, PC1, is mostly aligned with the experimental covariate of interest (untreated / treated), while the second axis is roughly aligned with the sequencing protocol (single / paired).\nWe used a data transformation, the regularized logarithm or rlog , which we will investigate more closely in Section 8.10.2.\n__\nQuestion 8.4\nDo the axes of PCA plot always have to align with known experimental covariates?\nHeatmaps can be a powerful way of quickly getting an overview over a matrix- like dataset, count tables included. Below you see how to make a heatmap from the rlog-transformed data. For a matrix as large as counts(pasilla), it is not practical to plot all of it, so we plot the submatrix of the 30 genes with the highest average expression.\nlibrary(\"pheatmap\")\nselect = order(rowMeans(assay(pas_rlog)), decreasing = TRUE)[1:30]\npheatmap( assay(pas_rlog)[select, ],\n     scale = \"row\",\n     annotation_col = as.data.frame(\n        colData(pas_rlog)[, c(\"condition\", \"type\")] ))__\n\nFigure 8.7: Heatmap of regularized log transformed data of the top 30 genes.\nIn Figure 8.7, pheatmap arranged the rows and columns of the matrix by the dendrogram from an unsupervised clustering, and the clustering of the columns (samples) is dominated by the type factor. This highlights that our differential expression analysis above was probably too naive, and that we should adjust for this strong “nuisance” factor when we are interested in testing for differentially expressed genes between conditions. We will do this in Section 8.9.\n__\nTask\nProduce a plot similar to Figure 8.7, but selecting the 30 most highly variable genes instead. What is different? How do the genes with very high mean and those with very high variance relate? How does their data look?\n\n\n10.4.4 8.5.4 Exporting the results\nAn HTML report of the results with plots and sortable/filterable columns can be exported using the ReportingTools package on a DESeqDataSet that has been processed by the DESeq function. For a code example, see the RNA-Seq differential expression vignette of the ReportingTools package or the manual page for the publish method for the DESeqDataSet class.\nA CSV file of the results can be exported using write.csv (or its counterpart from the readr package).\nwrite.csv(as.data.frame(res), file = \"treated_vs_untreated.csv\")__",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 Goals of this chapter</span>"
    ]
  },
  {
    "objectID": "08-chap.html#critique-of-default-choices-and-possible-modifications",
    "href": "08-chap.html#critique-of-default-choices-and-possible-modifications",
    "title": "10  8.1 Goals of this chapter",
    "section": "10.5 8.6 Critique of default choices and possible modifications",
    "text": "10.5 8.6 Critique of default choices and possible modifications\n\n10.5.1 8.6.1 The few changes assumption\nUnderlying the default normalization and the dispersion estimation in DESeq2 (and many other differential expression methods) is that most genes are not differentially expressed.\n\n\n\nFor the normalization, although not for the dispersion estimation, one can slightly relax this assumption: it is still valid if many genes are changing, but in a way that is balanced between up- and downward directions.\n\n\nFor the normalization, although not for the dispersion estimation, one can slightly relax this assumption: it is still valid if many genes are changing, but in a way that is balanced between up- and downward directions.\nThis assumption is often reasonable (well-designed experiments usually ask specific questions, so that not everything changes all at once), but what should we do if it does not hold? Instead of applying these operations on the data from all genes, we will then need to identify a subset of (“negative control”) genes for which we believe the assumption is tenable, either because of prior biological knowledge, or because we explicitly controlled their abundance as external “spiked in” features.\n__\nTask\nRun the DESeq2 workflow with size factors and dispersion parameters estimated only from a predefined subset of genes.\n\n\n10.5.2 8.6.2 Point-like null hypothesis\nAs a default, the DESeq function tests against the null hypothesis that each gene has the same abundance across conditions; this is a simple and pragmatic choice. Indeed, if the sample size is limited, what is statistically significant also tends to be strong enough to be biologically interesting. But as sample size increases, statistical significance in these tests may be present without much biological relevance. For instance, many genes may be slightly perturbed by downstream, indirect effects. We can modify the test to use a more permissive, interval-based null hypothesis; we will further explore this in Section 8.10.4.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 Goals of this chapter</span>"
    ]
  },
  {
    "objectID": "08-chap.html#multi-factor-designs-and-linear-models",
    "href": "08-chap.html#multi-factor-designs-and-linear-models",
    "title": "10  8.1 Goals of this chapter",
    "section": "10.6 8.7 Multi-factor designs and linear models",
    "text": "10.6 8.7 Multi-factor designs and linear models\n\n10.6.1 8.7.1 What is a multifactorial design?\nLet’s assume that in addition to the siRNA knockdown of the pasilla gene, we also want to test the effect of a certain drug. We could then envisage an experiment in which the experimenter treats the cells either with negative control, with the siRNA against pasilla, with the drug, or with both. To analyse this experiment, we can use the notation\n\\[ y = _0 + x_1 _1 + x_2 2 + x_1x_2{12}. \\]\nThis equation can be parsed as follows. The left hand side, \\(y\\), is the experimental measurement of interest. In our case, this is the suitably transformed expression level (we’ll discuss this in Section 8.8.3) of a gene. Since in an RNA-Seq experiment there are lots of genes, we’ll have as many copies of Equation 8.1, one for each. The coefficient \\(_0\\) is the base level of the measurement in the negative control; often it is called the intercept.\n\n\n\nSometimes Equation eq-countdata-basiclm is written with an additional term x_0 that is multiplied with \\beta_0, where it is understood that x_0=1 always. It turns out that this makes subsequent notation and bookkeeping easier since then the intercept can be handled consistently together with the other \\betas, instead of being a separate case.\n\n\nSometimes Equation 8.1 is written with an additional term \\(x_0\\) that is multiplied with \\(_0\\), where it is understood that \\(x_0=1\\) always. It turns out that this makes subsequent notation and bookkeeping easier since then the intercept can be handled consistently together with the other \\(\\)s, instead of being a separate case.\nThe design factors \\(x_1\\) and \\(x_2\\) are binary indicator variables: \\(x_1\\) takes the value 1 if the siRNA was transfected and 0 if not, and similarly, \\(x_2\\) indicates whether the drug was administered. In the experiment where only the siRNA is used, \\(x_1=1\\) and \\(x_2=0\\), and the third and fourth terms of Equation 8.1 vanish. Then, the equation simplifies to \\(y=_0+_1\\). This means that \\(_1\\) represents the difference between treatment and control. If our measurements are on a logarithmic scale, then\n\\[ \\[\\begin{align} \\beta_1 = y-\\beta_0\n&=\\log_2(\\text{expression}_{\\text{treated}})\n-\\log_2(\\text{expression}_{\\text{untreated}})\\\\\\ &=\\log_2\\frac\n{\\text{expression}_{\\text{treated}}} {\\text{expression}_{\\text{untreated}}}\n\\end{align}\\] \\]\nis the logarithmic fold change due to treatment with the siRNA. In exactly the same way, \\(_2\\) is the logarithmic fold change due to treatment with the drug. What happens if we treat the cells with both siRNA and drug? In that case, \\(x_1=x_2=1\\), and Equation 8.1 can be rewritten as\n\\[ _{12} = y - (_0 + _1 + _2). \\]\nThis means that \\(_{12}\\) is the difference between the observed outcome, \\(y\\), and the outcome expected from the individual treatments, obtained by adding to the baseline the effect of siRNA alone, \\(_1\\), and of drug alone, \\(_2\\).\nWe call \\(_{12}\\) the interaction effect of siRNA and drug. It has nothing to do with a physical interaction, the terminology indicates that the effects of these two different experimental factors do not simply add up, but combine in a more complicated fashion.\n\n\n\nNote that the addition is on the logarithmic scale, which corresponds to multiplication on the original scale.\n\n\nNote that the addition is on the logarithmic scale, which corresponds to multiplication on the original scale.\nFor instance, if the target of the drug and of the siRNA were equivalent, leading to the same effect on the cells, then we biologically expect that \\(_1=2\\). We also expect that their combination has no further effect, so that \\({12}=-_1\\). If, on the other hand, the targets of the drug and of the siRNA are in parallel pathways that can buffer each other, we’ll expect that \\(_1\\) and \\(2\\) are both relatively small, but that the combined effect is synergistic, and \\({12}\\) is large.\nNot always do we care about interactions. Many experiments are designed with multiple factors where we care most about each of their individual effects. In that case, the combinatorial treatment might not be present in the experimental design, and the model to use for the analysis is a version of Equation 8.1 with the rightmost term removed.\nWe can succinctly encode the design of the experiment in the design matrix. For instance, for the combinatorial experiment described above, the design matrix is\n\\[ ]\nThe columns of the design matrix correspond to the experimental factors, and its rows represent the different experimental conditions, four in our case. If, instead, the combinatorial treatment is not performed, then the design matrix is reduced to only the first three rows of 8.4.\n\n\n10.6.2 8.7.2 What about noise and replicates?\nEquation 8.1 provides a conceptual decomposition of the observed data into the effects caused by the different experimental variables. If our data (the \\(y\\)s) were absolutely precise, we could set up a linear system of equations, one equation for each of the four possible experimental conditions represented by the \\(x\\)s, and solve for the \\(\\)s.\nOf course, we usually wish to analyze real data that are affected by noise. We then need replicates to estimate the levels of noise and assess the uncertainty of our estimated \\(\\)s. Only then we can empirically assess whether any of the observed changes between conditions are significantly larger than those occuring just due to experimental or natural variation. We need to slightly extend the equation,\n\\[ y_{j} = x_{j0} ; 0 + x{j1} ; 1 + x{j2} ; 2 + x{j1},x_{j2};_{12} + _j. \\]\nWe have added the index \\(j\\) and a new term \\(j\\). The index \\(j\\) now explicitly counts over our individual replicate experiments; for instance, if for each of the four conditions we perform three replicates, then \\(j\\) counts from 1 to 12. The design matrix has now 12 rows, and \\(x{jk}\\) is the value of the matrix in its \\(j\\)th row and \\(k\\)th column.\n\n\n\nRemember that since \\beta_0 is the intercept, x_{j0}=1 for all j.\n\n\nRemember that since \\(0\\) is the intercept, \\(x{j0}=1\\) for all \\(j\\).\nThe additional terms \\(_j\\), which we call the residuals , are there to absorb differences between replicates. However, one additional modeling component is needed: the system of twelve equations 8.5 would be underdetermined without further information, since it has now more variables (twelve epsilons and four betas) than it has equations (twelve, one for each \\(j\\)). To fix this, we require that the \\(_j\\) be small. One popular way – we’ll encounter others – to overcome this is to minimize the sum of squared residuals,\n\\[ _j _j^2 . \\]\nIt turns out that with this requirement satisfied, the \\(\\)s represent the average effects of each of the experimental factors, while the residuals \\(_j\\) reflect the experimental fluctuations around the mean between the replicates. This approach, which is called the least sum of squares fitting , is mathematically convenient, since it can achieved by straightforward matrix algebra. It is what the R function lm does.\n__\nQuestion 8.5\nAn alternative way to write Equation 8.5 is\n\\[ y_{j} = k x{jk} ; _k + _j. \\]\nHow can this be mapped to Equation 8.5, i.e., what’s with the interaction term \\(x_{j1},x_{j2};_{12}\\)?\n__\nSolution\n__\nThis is really just a trivial matter of notation: the sum extends over \\(k=0,…,3\\), where the terms for \\(k=0,1,2\\) are exactly as we know them already. We write \\({3}\\) instead of \\({12}\\), and \\(x_{j3}\\) is defined to be \\(x_{j1}x_{j2}\\). The generic notation 8.7 is practical to use in computer software that implements linear models and in mathematical proofs. It also highlights that the “scientific content” of a linear model is condensed in its design matrix.\n__\nTask\nShow that if we have fit Equation 8.5 to data such that objective 8.6 holds, the fit residuals \\(_j\\) have an average of 0.\n\n\n10.6.3 8.7.3 Analysis of variance\nA model like 8.5 is called a linear model , and often it is implied that criterion 8.6 is used to fit it to data. This approach is elegant and powerful, but for novices it can take some time to appreciate all its facets. What is the advantage over just simply taking, for each distinct experimental condition, the average over replicates and comparing these values across conditions? In simple cases, the latter approach can be intuitive and effective. However, it comes to its limits when the replicate numbers are not all the same in the different groups, or when one or more of the \\(x\\)-variables is continuous-valued. In these cases, one will invariably end up with something like fitting 8.5 to the data. A useful way to think about 8.5 is contained in the term analysis of variance , abbreviated ANOVA. In fact, what Equation 8.5 does is decompose the variability of \\(y\\) that we observed in the course of our experiments into elementary components: its baseline value \\(_0\\), its variability caused by the effect of the first variable, \\(_1\\), its variability caused by the effect of the second variable, \\(2\\), its variability caused by the effect of the interaction, \\({12}\\), and variability that is unaccounted for. The last of these we commonly call noise , the other ones, systematic variability.\n\n\n\nThe distinction between noise and systematic variability is in the eye of the beholder, and depends on our model, not on reality.\n\n\nThe distinction between noise and systematic variability is in the eye of the beholder, and depends on our model, not on reality.\n\n\n10.6.4 8.7.4 Robustness\nThe sum 8.6 is sensitive to outliers in the data. A single measurement \\(y_{j}\\) with an outlying value can draw the \\(\\) estimates far away from the values implied by the other replicates. This is the well-known fact that methods based on least sum of squares have a low breakdown point : if even only a single data point is outlying, the whole statistical result can be strongly affected. For instance, the average of a set of \\(n\\) numbers has a breakdown point of \\(\\), meaning that it can be arbitrarily changed by changing only a single one of the numbers. On the other hand, the median has a much higher breakdown point. Changing a single number often has no effect at all, and when it does, the effect is limited to the range of the data points in the middle of the ranking (i.e., those adjacent to rank \\(\\)). To change the median by an arbitrarily high amount, you need to change half the observations. We call the median robust , and its breakdown point is \\(\\). Remember that the median of a set of numbers \\(y_1, y_2, …\\) minimizes the sum \\(_j|y_j-_0|\\).\nTo achieve a higher degree of robustness against outliers, other choices than the sum of squares 8.6 can be used as the objective of minimization. Among these are:\n\\[ \\[\\begin{align} R &= \\sum_j |\\varepsilon_j| & \\text{Least absolute\ndeviations} \\\\\\ R &= \\sum_j \\rho_s(\\varepsilon_j) & \\text{M-estimation} \\\\\\ R\n&= Q_{\\theta}\\left( \\\\{\\varepsilon_1^2, \\varepsilon_2^2,... \\\\} \\right) &\n\\text{LTS, LQS} \\\\\\ R &= \\sum_j w_j \\varepsilon_j^2 & \\text{general weighted\nregression} \\end{align}\\] \\]\nHere, \\(R\\) is the quantity to be minimized. The first choice in Equation 8.8 is called least absolute deviations regression. It can be viewed as a generalization of the median. Although conceptually simple, and attractive on first sight, it is harder to minimize than the sum of squares, and it can be less stable and less efficient especially if the data are limited, or do not fit the model8. The second choice in Equation 8.8, also called M-estimation , uses a penalization function \\(_s\\) (least-squares regression is the special case with \\(_s()=^2\\)) that looks like a quadratic function for a limited range of \\(\\), but has a smaller slope, flattens out, or even drops back to zero, for absolute values \\(||\\) that are larger than the scale parameter \\(s\\). The intention behind this is to downweight the effect of outliers, i.e. of data points that have large residuals (Huber 1964). A choice of \\(s\\) needs to be made and determines what is called an outlier. One can even drop the requirement that \\(_s\\) is quadratic around 0 (as long as its second derivative is positive), and a variety of choices for the function \\(_s\\) have been proposed in the literature. The aim is to give the estimator desirable statistical properties (say, bias and efficiency) when and where the data fit the model, but to limit or nullify the influence of those data points that do not, and to keep computations tractable.\n8 The Wikipedia article gives an overview.\n__\nQuestion 8.6\nPlot the graph of the function \\(_s()\\) proposed by Huber (1964) for M-estimators.\n__\nSolution\n__\nHuber’s paper defines, on Page 75:\n\\[ _s() = \\{ ]\nThe graph produced by the below code is shown in Figure 8.8.\nrho = function(x, s)\n  ifelse(abs(x) &lt; s, x^2 / 2,  s * abs(x) - s^2 / 2)\n\ndf = tibble(\n  x        = seq(-7, 7, length.out = 100),\n  parabola = x ^ 2 / 2,\n  Huber    = rho(x, s = 2))\n\nggplot(reshape2::melt(df, id.vars = \"x\"),\n  aes(x = x, y = value, col = variable)) + geom_line()__\n\nFigure 8.8: Graph of \\(_s()\\), for a choice of \\(s=2\\).\nChoice three in 8.8 generalises the least sum of squares method in yet another way. In least quantile of squares (LQS) regression, the the sum over the squared residuals is replaced with a quantile, for instance, \\(Q_{50}\\), the median, or \\(Q_{90}\\), the 90%-quantile (Peter J. Rousseeuw 1987). In a variation thereof, least trimmed sum of squares (LTS) regression, a sum of squared residuals is used, but the sum extends not over all residuals, but only over the fraction \\(0\\) of smallest residuals. The motivation in either case is that outlying data points lead to large residuals, and as long as they are rare, they do not affect the quantile or the trimmed sum.\nHowever, there is a price: while the least sum of squares optimization 8.6 can be done through straightforward linear algebra, more complicated iterative optimization algorithms are needed for M-estimation, LQS and LTS regression.\nThe final approach in 8.8 represents an even more complex way of weighting down outliers. It assumes that we have some way of deciding what weight \\(w_j\\) we want to give to each observation, presumably down-weighting outliers. For instance, in Section 8.10.3, we will encounter the approach used by the DESeq2 package, in which the leverage of each data point on the estimated \\(\\)s is assessed using a measure called Cook’s distance. For those data whose Cook’s distance is deemed too large, the weight \\(w_j\\) is set to zero, whereas the other data points get \\(w_j=1\\). In effect, this means that the outlying data points are discarded and that ordinary regression is performed on the others. The extra computational effort of carrying the weights along is negligible, and the optimization is still straightforward linear algebra.\nAll of these approaches to outlier robustness introduce a degree of subjectiveness and rely on sufficient replication. The subjectiveness is reflected by the parameter choices that need to be made: \\(s\\) in 8.8 (2), \\(\\) in 8.8 (3), the weights in 8.8 (4). One scientist’s outlier may be the Nobel prize of another. On the other hand, outlier removal is no remedy for sloppy experiments and no justification for wishful thinking.\n__\nTask\nSearch the documentation of R and CRAN packages for implementations of the above robust regression methods. A good place to start is the CRAN task view on robust statistical methods.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 Goals of this chapter</span>"
    ]
  },
  {
    "objectID": "08-chap.html#generalized-linear-models",
    "href": "08-chap.html#generalized-linear-models",
    "title": "10  8.1 Goals of this chapter",
    "section": "10.7 8.8 Generalized linear models",
    "text": "10.7 8.8 Generalized linear models\nWe need to explore two more theoretical concepts before we can proceed to our next application example. Equations of the form 8.5 model the expected value of the outcome variable, \\(y\\), as a linear function of the design matrix, and they are fit to data according to the least sum of squares criterion 8.6; or a robust variant thereof. We now want to generalize these assumptions.\n\n10.7.1 8.8.1 Modeling the data on a transformed scale\nWe already saw that it can be fruitful to consider the data not on the scale that we obtained them, but after some transformation, for instance, the logarithm. This idea can be generalized, since depending on the context, other transformations are useful. For instance, the linear model 8.5 would not directly be useful for modeling outcomes that are bounded within an interval, say, \\([0,1]\\) as an indicator of disease risk. In a linear model, the values of \\(y\\) cover, in principle, the whole real axis. However, if we transform the expression on the right hand with a sigmoid function, for instance, \\(f(y) = 1/(1+e^{-y})\\), then the range of this function9, is bounded between 0 and 1 and can be used to model such an outcome.\n9 It is called the logistic function (Verhulst 1845), and the associated regression model is called logistic regression.\n\n\n10.7.2 8.8.2 Other error distributions\nThe other generalization regards the minimization criterion 8.6. In fact, this criterion can be derived from a specific probabilistic model and the maximum likelihood principle (we already encountered this in Chapter 2). To see this, consider the probabilistic model\n\\[ p(_j) = (-), \\]\nthat is, we believe that the residuals follow a normal distribution with mean 0 and standard deviation \\(\\). Then it is plausible to demand from a good model (i.e., from a good set of \\(\\)s) that these probabilities are large. Formally,\n\\[ _j p(_j) . \\]\n__\nQuestion 8.7\nShow that the maximizing the likelihood 8.10 is equivalent to minimizing the sum of squared residuals 8.6.\n__\nSolution\n__\nInsert 8.9 into 8.10 and take the logarithm.\nLet’s revise some core concepts: the left hand side of Equation 8.10, i.e., the product of the probabilities of the residuals, is a function of both the model parameters \\(_1, _2, …\\) and the data \\(y_1, y_2, …\\); call it \\(f(,y)\\). If we think of the model parameters \\(\\) as given and fixed, then the collapsed function \\(f(y)\\) simply indicates the probability of the data. We could use it, for instance, to simulate data. If, on the other hand, we consider the data as given, then \\(f()\\) is a function of the model parameters, and it is called the likelihood. The second view is the one we take when we optimise 8.6 (and thus 8.10), and hence the \\(\\)s obtained this way are what is called maximum-likelihood estimates.\n\n\n\nIt is good to remember that, while we can use the normal distribution as a convenient argument to motivate least sum of squares regression through the maximum likelihood principle, the data do not have to be distributed according to the normal for least sum of squares regression to provide a useful result. In fact, least sum of squares fitting often provides useful estimates for the \\betas even when the data are non-normal, although that depends on the specific circumstances.\n\n\nIt is good to remember that, while we can use the normal distribution as a convenient argument to motivate least sum of squares regression through the maximum likelihood principle, the data do not have to be distributed according to the normal for least sum of squares regression to provide a useful result. In fact, least sum of squares fitting often provides useful estimates for the \\(\\)s even when the data are non-normal, although that depends on the specific circumstances.\nThe generalization that we can now make is to use a different probabilistic model. We can use the densities of other distributions than the normal instead of Equation 8.9. For instance, to be able to deal with count data, we will use the gamma-Poisson distribution.\n\n\n10.7.3 8.8.3 A generalized linear model for count data\nThe differential expression analysis in DESeq2 uses a generalized linear model of the form:\n\\[ \\[\\begin{align} K_{ij} & \\sim \\text{GP}(\\mu_{ij}, \\alpha_i) \\\\\\ \\mu_{ij} &=\ns_j\\, q_{ij} \\\\\\ \\log_2(q_{ij}) &= \\sum_k x_{jk} \\beta_{ik}. \\end{align}\\] \\]\nLet us unpack this step by step. The counts \\(K_{ij}\\) for gene \\(i\\), sample \\(j\\) are modeled using a gamma-Poisson (GP) distribution with two parameters, the mean \\({ij}\\) and the dispersion \\(i\\). By default, the dispersion is different for each gene \\(i\\), but the same across all samples, therefore it has no index \\(j\\). The second line in Equation 8.11 states that the mean is composed of a sample-specific size factor \\(s_j\\)10 and \\(q{ij}\\), which is proportional to the true expected concentration of fragments for gene \\(i\\) in sample \\(j\\). The value of \\(q{ij}\\) is given by the linear model in the third line via the link function , \\(2\\). The design matrix \\((x{jk})\\) is the same for all genes (and therefore does not depend on \\(i\\)). Its rows \\(j\\) correspond to the samples, its columns \\(k\\) to the experimental factors. In the simplest case, for a pairwise comparison, the design matrix has only two columns, one of them everywhere filled with 1 (corresponding to \\(0\\) of Section 8.7.1) and the other one containing 0 or 1 depending on whether the sample belongs to one or the other group. The coefficients \\({ik}\\) give the \\(_2\\) fold changes for gene \\(i\\) for each column of the design matrix \\(X\\).\n10 The model can be generalized to use sample- and gene-dependent normalization factors \\(s_{ij}\\). This is explained in the documentation of the DESeq2 package.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 Goals of this chapter</span>"
    ]
  },
  {
    "objectID": "08-chap.html#two-factor-analysis-of-the-pasilla-data",
    "href": "08-chap.html#two-factor-analysis-of-the-pasilla-data",
    "title": "10  8.1 Goals of this chapter",
    "section": "10.8 8.9 Two-factor analysis of the pasilla data",
    "text": "10.8 8.9 Two-factor analysis of the pasilla data\nBesides the treatment with siRNA, which we have already considered in Section 8.5, the pasilla data have another covariate, type, which indicates the type of sequencing that was performed.\nWe saw in the exploratory data analysis (EDA) plots in Section 8.5.3 that the latter had a considerable systematic effect on the data. Our basic analysis of Section 8.5 did not take this account, but we will do so now. This should help us get a more correct picture of which differences in the data are attributable to the treatment, and which are confounded – or masked – by the sequencing type.\npasillaTwoFactor = pasilla\ndesign(pasillaTwoFactor) = formula(~ type + condition)\npasillaTwoFactor = DESeq(pasillaTwoFactor)__\nOf the two variables type and condition, the one of primary interest is condition, and in DESeq2 , the convention is to put it at the end of the formula. This convention has no effect on the model fitting, but it helps simplify some of the subsequent results reporting. Again, we access the results using the results function, which returns a dataframe with the statistics of each gene.\nres2 = results(pasillaTwoFactor)\nhead(res2, n = 3)__\n\n\nlog2 fold change (MLE): condition treated vs untreated \nWald test p-value: condition treated vs untreated \nDataFrame with 3 rows and 6 columns\n             baseMean log2FoldChange     lfcSE       stat    pvalue      padj\n            &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt;  &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\nFBgn0000003  0.171569      0.6745518  3.871091  0.1742537  0.861666        NA\nFBgn0000008 95.144079     -0.0406731  0.222215 -0.1830351  0.854770  0.951975\nFBgn0000014  1.056572     -0.0849880  2.111821 -0.0402439  0.967899        NA\nIt is also possible to retrieve the \\(_2\\) fold changes, p-values and adjusted p-values associated with the type variable. The function results takes an argument contrast that lets users specify the name of the variable, the level that corresponds to the numerator of the fold change and the level that corresponds to the denominator of the fold change.\nresType = results(pasillaTwoFactor,\n  contrast = c(\"type\", \"single\", \"paired\"))\nhead(resType, n = 3)__\n\n\nlog2 fold change (MLE): type single vs paired \nWald test p-value: type single vs paired \nDataFrame with 3 rows and 6 columns\n             baseMean log2FoldChange     lfcSE      stat    pvalue      padj\n            &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\nFBgn0000003  0.171569      -1.611546  3.871083 -0.416304  0.677188        NA\nFBgn0000008 95.144079      -0.262255  0.220686 -1.188362  0.234691  0.543822\nFBgn0000014  1.056572       3.290586  2.087243  1.576522  0.114905        NA\nSo what did we gain from this analysis that took into account type as a nuisance factor (sometimes also called, more politely, a blocking factor), compared to the simple comparison between two groups of Section 8.5? Let us plot the p-values from both analyses against each other.\ntrsf = function(x) ifelse(is.na(x), 0, (-log10(x)) ^ (1/6))\nggplot(tibble(pOne = res$pvalue,\n              pTwo = res2$pvalue),\n    aes(x = trsf(pOne), y = trsf(pTwo))) +\n    geom_hex(bins = 75) + coord_fixed() +\n    xlab(\"Single factor analysis (condition)\") +\n    ylab(\"Two factor analysis (type + condition)\") +\n    geom_abline(col = \"orange\")__\n\nFigure 8.9: Comparison of p-values from the models with a single factor (condition) and with two factors (type + condition). The axes correspond to \\((-_{10}p)^{}\\), an arbitrarily chosen monotonically decreasing transformation that compresses the dynamic range of the p-values for the purpose of visualization. We can see a trend for the joint distribution to lie above the bisector, indicating that the small p-values in the two-factor analysis are generally smaller than those in the one-factor analysis.\nAs we can see in Figure 8.9, the p-values in the two-factor analysis are similar to those from the one-factor analysis, but are generally smaller. The more sophisticated analysis has led to an, albeit modest, increase in power. We can also see this by counting the number of genes that pass a certain significance threshold in each case:\ncompareRes = table(\n   `simple analysis` = res$padj &lt; 0.1,\n   `two factor` = res2$padj &lt; 0.1 )\naddmargins( compareRes )__\n\n\n               two factor\nsimple analysis FALSE TRUE  Sum\n          FALSE  6973  289 7262\n          TRUE     25 1036 1061\n          Sum    6998 1325 8323\nThe two-factor analysis found 1325 genes differentially expressed at an FDR threshold of 10%, while the one-factor analysis found 1061. The two-factor analysis has increased detection power. In general, the gain can be even much larger, or also smaller, depending on the data. The proper choice of the model requires informed adaptation to the experimental design and data quality.\n__\nQuestion 8.8\nWhy do we detect fewer significant genes when we do not take into account the type variable? More generally, what does this mean about the benefit of taking into account (or not) blocking factors?\n__\nSolution\n__\nWithout modeling the blocking factor, the variability in the data that is due to it has to be absorbed by the \\(\\)s. This means that they are generally larger than in the model with the blocking factor. The higher level of noise leads to higher uncertainty in the \\(\\)-estimates. On the other hand, the model with the blocking factor has more parameters that need to be estimated. In statistical parlance, the fit has fewer “degrees of freedom”. Both of these effects are counteracting, and which of them prevails, and which of the modeling choices yields more or fewer significant results depends on the data.\n__\nQuestion 8.9\nWhat is confounding? Can not taking into account a blocking factor also lead to the detection of more genes?\n__\nSolution\n__\nYes. Imagine the variables condition and type were not as nicely balanced as they are, but partially or fully confounded. In that case, differences in the data due to type could be attributed to condition if a model is fit that does not make it possible to absorb them in the type-effect. Scientifically, such an experiment (and analysis) can be quite an embarrassment.\n__\nQuestion 8.10\nConsider a paired experimenal design, say, 10 different cell lines each with and without drug treatment. How should this be analyzed?\n__\nSolution\n__\nIf we just did a simple two-group comparison (treated versus untreated) many of the treatment effects would probably go under in the strong cell line to cell line variation. However, we can set up a paired analysis simply by adding cell line identity as a blocking factor. (Cell line is then really an R factor with 10 different levels, rather than just a 0 vs 1 indicator variable as with the variables that we looked at so far; R’s linear modeling facilities, and also DESeq2 , have no problem dealing with that.)\n__\nQuestion 8.11\nWhat can you do if you suspect there are “hidden” factors that affect your data, but they are not documented? (Sometimes, such undocumented covariates are also called batch effects.)\n__\nSolution\n__\nThere are methods that try to identify blocking factors in an unsupervised fashion, see e.g., Leek and Storey (2007; Stegle et al. 2010).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 Goals of this chapter</span>"
    ]
  },
  {
    "objectID": "08-chap.html#further-statistical-concepts",
    "href": "08-chap.html#further-statistical-concepts",
    "title": "10  8.1 Goals of this chapter",
    "section": "10.9 8.10 Further statistical concepts",
    "text": "10.9 8.10 Further statistical concepts\n\n10.9.1 8.10.1 Sharing of dispersion information across genes\nWe already saw an explanation of Bayesian (or empirical Bayes) analysis in Figure 6.16. The idea is to use additional information to improve our estimates (information that we either known a priori, or have from analysis of other, but similar data). This idea is particularly useful if the data per se are relatively noisy. DESeq2 uses an empirical Bayes approach for the estimation of the dispersion parameters (the \\(\\)s in the third line of Equation 8.11) and, optionally, the logarithmic fold changes (the \\(\\)s). The priors are, in both cases, taken from the distributions of the maximum-likelihood estimates (MLEs) across all genes. It turns out that both of these distributions are uni-modal; in the case of the \\(\\)s, with a peak at around 0, in the case of the \\(\\), at a particular value, the typical dispersion. The empirical Bayes machinery then shrinks each per-gene MLE towards that peak, by an amount that depends on the sharpness of the empirical prior distribution and the precision of the ML estimate (the better the latter, the less shrinkage will be done). The mathematics are explained in (Michael I. Love, Huber, and Anders 2014), and Figure 8.10 visualizes the approach for the \\(\\)s.\n__\nTask\nAdvanced: check the R code that produces Figure 8.10.\n[](08-chap_files/figure-html/fig-countdata-posterior-1.png “Figure 8.10 (a):”)\n\n\n\n[](08-chap_files/figure-html/fig-countdata-posterior-2.png “Figure 8.10 (b):”)\n\n\n\nFigure 8.10: Shrinkage estimation of logarithmic fold change estimates by use of an empirical prior in DESeq2. Two genes with similar mean count and MLE logarithmic fold change are highlighted in green and blue. The normalized counts for these genes (a) reveal low dispersion for the gene in blue and high dispersion for the gene in green. In (b), the density plots are shown of the normalized likelihoods (solid lines) and of the posteriors (dashed lines) for the green and blue gene. In addition, the solid black line shows the prior estimated from the MLEs of all genes. Due to the higher dispersion of the green gene, its likelihood is wider and less sharp (indicating less information), and the prior has more influence on its posterior than in the case of the blue gene.\n\n\n10.9.2 8.10.2 Count data transformations\nFor testing for differential expression we operate on raw counts and use discrete distributions. For other downstream analyses – e.g., for visualization or clustering – it might however be useful to work with transformed versions of the count data.\nMaybe the most obvious choice of transformation is the logarithm. However, since count values for a gene can become zero, some advocate the use of pseudocounts , i.e., transformations of the form\n\\[ y = _2(n + 1)y = _2(n + n_0), \\]\nwhere \\(n\\) represents the count values and \\(n_0\\) is a somehow chosen positive constant.\nLet’s look at two alternative approaches that offer more theoretical justification, and a rational way of choosing the parameter equivalent to \\(n_0\\) above. One method incorporates priors on the sample differences, and the other uses the concept of variance-stabilizing transformations.\n\n10.9.2.1 Variance-stabilizing transformation\nWe already explored variance-stabilizing transformations in Section 4.4.4. There we computed a piece-wise linear transformation for a discrete set of random variables (Figure 4.26) and also saw how to use calculus to derive a smooth variance-stabilizing transformation for a gamma-Poisson mixture. These computations are implemented in the DESeq2 package (Anders and Huber 2010):\nvsp = varianceStabilizingTransformation(pasilla)__\nLet us explore the effect of this on the data, using the first sample as an example, and comparing it to the \\(_2\\) transformation; the plot is shown in Figure 8.11 and is made with the following:\nj = 1\nggplot(\n  tibble(\n    counts = rep(assay(pasilla)[, j], 2),\n    transformed = c(\n      assay(vsp)[, j],\n      log2(assay(pasilla)[, j])\n      ),\n    transformation = rep(c(\"VST\", \"log2\"), each = nrow(pasilla))\n  ),\n  aes(x = counts, y = transformed, col = transformation)) +\n  geom_line() + xlim(c(0, 600)) + ylim(c(0, 9))__\n\nFigure 8.11: Graph of variance-stabilizing transformation for the data of one of the samples, and for comparison also of the \\(_2\\) transformation. The variance-stabilizing transformation has finite values and finite slope even for counts close to zero, whereas the slope of \\(_2\\) becomes very steep for small counts and is undefined for counts of zero. For large counts, the two transformation are essentially the same.\n\n\n10.9.2.2 Regularized logarithm (rlog) transformation\nThere is a second way to come up with a data transformation. It is conceptually distinct from variance stabilization. Instead, it builds upon the shrinkage estimation that we already explored in Section 8.10.1. It works by transforming the original count data to a \\(2\\)-like scale by fitting a “trivial” model with a separate term for each sample and a prior distribution on the coefficients which is estimated from the data. The fitting employs the same regularization as what we discussed in Section 8.10.1. The transformed data \\(q{ij}\\) are defined by the third line of Equation 8.11, where the design matrix \\((x_{jk})\\) is of size \\(K (K+1)\\) – here \\(K\\) is the number of samples– and has the form\n\\[ X=(]\nWithout priors, this design matrix would lead to a non-unique solution, however the addition of a prior on non-intercept \\(\\)s allows for a unique solution to be found.\nIn DESeq2 , this functionality is implemented in the function rlogTransformation. It turns out in practice that the rlog transformation is also approximately variance- stabilizing, but in contrast to the variance-stabilizing transformation of Section 8.10.2 it deals better with data in which the size factors of the different samples are very distinct.\n__\nQuestion 8.12\nPlot mean against standard deviation between replicates for the shifted logarithm 8.12, the regularized log transformation and the variance- stabilizing transformation.\n__\nSolution\n__\nSee Figure 8.12.\nlibrary(\"vsn\")\nrlp = rlogTransformation(pasilla)\n\nmsd = function(x)\n  meanSdPlot(x, plot = FALSE)$gg + ylim(c(0, 1)) +\n     theme(legend.position = \"none\")\n\ngridExtra::grid.arrange(\n  msd(log2(counts(pasilla, normalized = TRUE) + 1)) +\n    ylab(\"sd(log2)\"),\n  msd(assay(vsp)) + ylab(\"sd(vst)\"),\n  msd(assay(rlp)) + ylab(\"sd(rlog)\"),\n  ncol = 3\n)__\n\nFigure 8.12: Per-gene standard deviation (sd, taken across samples) against the rank of the mean, for the shifted logarithm \\(_2(n+1)\\), the variance-stabilizing transformation (vst) and the rlog. Note that for the leftmost \\(\\) 2,500 genes, the counts are all zero, and hence their standard deviation is zero. The mean-sd dependence becomes more interesting for genes with non-zero counts. Note also the high value of the standard deviation for genes that are weakly detected (but not with all zero counts) when the shifted logarithm is used, and compare to the relatively flat shape of the mean-sd relationship for the variance-stabilizing transformation.\n\n\n\n10.9.3 8.10.3 Dealing with outliers\nThe data sometimes contain isolated instances of very large counts that are apparently unrelated to the experimental or study design, and which may be considered outliers. There are many reasons why outliers can arise, including rare technical or experimental artifacts, read mapping problems in the case of genetically differing samples, and genuine, but rare biological events. In many cases, users appear primarily interested in genes that show a consistent behaviour, and this is the reason why by default, genes that are affected by such outliers are set aside by DESeq. The function calculates, for every gene and for every sample, a diagnostic test for outliers called Cook’s distance(Cook 1977). Cook’s distance is a measure of how much a single sample is influencing the fitted coefficients for a gene, and a large value of Cook’s distance is intended to indicate an outlier count. DESeq2 automatically flags genes with Cook’s distance above a cutoff and sets their p-values and adjusted p-values to NA.\nThe default cutoff depends on the sample size and number of parameters to be estimated; DESeq2 uses the \\(99%\\) quantile of the \\(F(p,m-p)\\) distribution (with \\(p\\) the number of parameters including the intercept and \\(m\\) number of samples).\n__\nQuestion 8.13\nCheck the documentation to see how the default cutoff can be changed, and how the outlier removal functionality can be disabled altogether. How can the computed Cook’s distances be accessed?\nWith many degrees of freedom – i.e., many more samples than number of parameters to be estimated – it might be undesirable to remove entire genes from the analysis just because their data include a single count outlier. An alternate strategy is to replace the outlier counts with the trimmed mean over all samples, adjusted by the size factor for that sample. This approach is conservative: it will not lead to false positives, as it replaces the outlier value with the value predicted by the null hypothesis.\n\n\n10.9.4 8.10.4 Tests of \\(_2\\) fold change above or below a threshold\nLet’s come back to the point we raised in Section 8.6: how to build into the tests our requirement that we want to detect effects that have a strong enough size, as opposed to ones that are statistically significant, but very small. Two arguments to the results function allow for threshold-based Wald tests: lfcThreshold, which takes a numeric of a non-negative threshold value, and altHypothesis, which specifies the kind of test. It can take one of the following four values, where \\(\\) is the \\(_2\\) fold change specified by the name argument, and \\(\\) represents lfcThreshold:\n\ngreater: \\(&gt; \\)\nless: \\(&lt; (-)\\)\ngreaterAbs: \\(|| &gt; \\) (two-tailed test)\nlessAbs: \\(|| &lt; \\) (p-values are the maximum of the upper and lower tests)\n\nThese are demonstrated in the following code and visually by MA-plots in Figure 8.13. (Note that the plotMA method, which is defined in the DESeq2 package, uses base graphics.)\npar(mfrow = c(4, 1), mar = c(2, 2, 1, 1))\nmyMA = function(h, v, theta = 0.5) {\n  plotMA(pasilla, lfcThreshold = theta, altHypothesis = h,\n         ylim = c(-2.5, 2.5))\n  abline(h = v * theta, col = \"dodgerblue\", lwd = 2)\n}\nmyMA(\"greaterAbs\", c(-1, 1))\nmyMA(\"lessAbs\",    c(-1, 1))\nmyMA(\"greater\",          1)\nmyMA(\"less\",         -1   )__\n[](08-chap_files/figure-html/fig-countdata-lfcThresh-1.png “Figure 8.13: MA-plots of tests of \\log_2 fold change with respect to a threshold value. From top to bottom, the tests are for altHypothesis =”greaterAbs”, “lessAbs”, “greater”, and “less”.”)\nFigure 8.13: MA-plots of tests of \\(_2\\) fold change with respect to a threshold value. From top to bottom, the tests are for altHypothesis = \"greaterAbs\", \"lessAbs\", \"greater\", and \"less\".\nTo produce the results tables instead of MA plots, the same arguments as to plotMA (except ylim) would be provided to the results function.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 Goals of this chapter</span>"
    ]
  },
  {
    "objectID": "08-chap.html#summary-of-this-chapter",
    "href": "08-chap.html#summary-of-this-chapter",
    "title": "10  8.1 Goals of this chapter",
    "section": "10.10 8.11 Summary of this chapter",
    "text": "10.10 8.11 Summary of this chapter\nWe have seen how to analyze count tables from high-throughput sequencing (and analagous data types) for differential abundance. We built upon the powerful and elegant framework of linear models. In this framework, we can analyze a basic two-groups comparison as well as more complex multifactorial designs, or experiments with covariates that have more than two levels or are continuous. In ordinary linear models, the sampling distribution of the data around the expected value is assumed to be independent and normal, with zero mean and the same variances. For count data, the distributions are discrete and tend to be skewed (asymmetric) with highly different variances across the dynamic range. We therefore employed a generalization of ordinary linear models, called generalized linear models (GLMs), and in particular considered gamma-Poisson distributed data with dispersion parameters that we needed to estimate from the data.\nSince the sampling depth is typically different for different sequencing runs (replicates), we need to estimate the effect of this variable parameter and take it into account in our model. We did this through the size factors \\(s_i\\). Often this part of the analysis is called normalization (the term is not particularly descriptive, but unfortunately it is now well-settled in the literature).\nFor designed experiments, the number of replicates is (and should be) usually too small to estimate the dispersion parameter (and perhaps even the model coefficients) from the data for each gene alone. Therefore we use shrinkage or empirical Bayes techniques, which promise large gains in precision for relatively small costs of bias.\nWhile GLMs let us model the data on their original scale, sometimes it is useful to transform the data to a scale where the data are more homoskedastic and fill out the range more uniformly – for instance, for plotting the data, or for subjecting them to general purpose clustering, dimension reduction or learning methods. To this end, we saw the variance stabilizing transformation.\nA major, and quite valid critique of differential expression testing such as exercised here is that the null hypothesis – the effect size is exactly zero – is almost never true, and therefore our approach does not provide consistent estimates of what the differentially expressed gene are. In practice, this may be overcome by considering effect size as well as statistical significance. Moreover, we saw how to use “banded” null hypotheses.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 Goals of this chapter</span>"
    ]
  },
  {
    "objectID": "08-chap.html#further-reading",
    "href": "08-chap.html#further-reading",
    "title": "10  8.1 Goals of this chapter",
    "section": "10.11 8.12 Further reading",
    "text": "10.11 8.12 Further reading\n\nThe DESeq2 method is explained in the paper by Michael I. Love, Huber, and Anders (2014), and practical aspects of the software in the package vignette. See also the edgeR package and paper (Robinson, McCarthy, and Smyth 2009) for a related approach.\nA classic textbook on robust regression and outlier detection is the book by Peter J. Rousseeuw and Leroy (1987). For more recent developments the CRAN task view on Robust Statistical Methods is a good starting point.\nThe Bioconductor RNA-Seq workflow at https://www.bioconductor.org/help/workflows/rnaseqGene (Michael I. Love et al. 2015) covers a number of issues related specifically to RNA-Seq that we have sidestepped here.\nAn extension of the generalized linear model that we saw to detecting alternative exon usage from RNA-Seq data is presented in the DEXSeq paper (Anders, Reyes, and Huber 2012), and applications of these ideas to biological discovery were described by Reyes et al. (2013) and Reyes and Huber (2017).\nFor some sequencing-based assays, such as RIP-Seq, CLIP-Seq, the biological analysis goal boils down to testing whether the ratio of input and immunoprecipitate (IP) has changed between conditions. Mike Love’s post on the Bioconductor forum provides a clear and quick how-to: https://support.bioconductor.org/p/61509.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 Goals of this chapter</span>"
    ]
  },
  {
    "objectID": "08-chap.html#exercises",
    "href": "08-chap.html#exercises",
    "title": "10  8.1 Goals of this chapter",
    "section": "10.12 8.13 Exercises",
    "text": "10.12 8.13 Exercises\n__\nExercise 8.1\nDepletion of small p-values. Consider the following simple generative model for a histogram of p-values that shows a depletion of small p-values. In Figure 8.14, p-values are shown from a differential expression analysis (in this case, simple \\(t\\)-tests) in the absence of an association with the tested two-level categorical variable groups. While the histogram is approximately uniform for x1, small p-values are depleted for x2. This is because the batch (encoded by the eponymous variable), which is orthogonal to groups and balanced, introduces additional variability that inflates the denominator of the test statistic.\nlibrary(\"magrittr\")\nng = 10000\nns = 12\nx1 = x2 = matrix(rnorm(ns * ng), ncol = ns, nrow= ng)\ngroup = factor(letters[1 + seq_len(ns) %% 2])  %T&gt;% print __\n\n\n [1] b a b a b a b a b a b a\nLevels: a b\n\n\nbatch = factor(ifelse(seq_len(ns) &lt;= ns/2, \"B1\", \"B2\")) %T&gt;% print __\n\n\n [1] B1 B1 B1 B1 B1 B1 B2 B2 B2 B2 B2 B2\nLevels: B1 B2\n\n\ntable(group, batch)__\n\n\n     batch\ngroup B1 B2\n    a  3  3\n    b  3  3\n\n\nx2[, batch==\"B2\"] = x2[, batch==\"B2\"] + 2 * rnorm(ng)\npvals = rbind(\n  cbind(type = \"x1\", genefilter::rowttests(x1, fac = group)),\n  cbind(type = \"x2\", genefilter::rowttests(x2, fac = group)))\nggplot(pvals, aes(x = p.value)) + \n  geom_histogram(binwidth = 0.02, boundary = 0) +\n  facet_grid(type ~ .)__\nReplace the \\(t\\)-test by a linear model, first, one with only group as a factor, second, one with group + batch (in R’s formula language). Show that the histogram of p-values for the coefficient of group is uniform in both cases, x1 and x2.\n\nFigure 8.14: p-values for the tests performed on x1 and x2 (see code).\n__\nExercise 8.2\nedgeR. Do the analyses of Section 8.5 with the edgeR package and compare the results: make a scatterplot of the \\(_{10}\\) p-values, pick some genes where there are large differences, and visualize the raw data to see what is going on. Based on this can you explain the differences?\n__\nExercise 8.3\nRobustness. Write a shiny app that performs linear regression on an example \\((x, y)\\) dataset (for instance, from the mtcars data) and displays the data as well as the fitted line. Add a widget that lets you move one of the points in \\(x\\)- and/or \\(y\\)- direction in a wide range (extending a few times outside the original data range). Add a radio buttons widget that lets you choose between lm, rlm and lqs with its different choices of method (the latter two are in the MASS package). Bonus: add functions from the robustbase package.\n__\nSolution\n__\nCode for the file ui.R in the app:\nlibrary(\"shiny\")\nshinyUI(fluidPage(\n  titlePanel(\"Breakdown\"),\n  sidebarLayout(\n    sidebarPanel(     # select oulier shift\n      sliderInput(\"shift\", \"Outlier:\", min = 0, max = 100, value = 0),\n      radioButtons(\"method\", \"Method:\",\n                   c(\"Non-robust least squares\" = \"lm\",\n                     \"M-estimation\" = \"rlm\"))\n    ),\n    mainPanel(       # show fit\n      plotOutput(\"regPlot\")\n    )\n  )\n))__\nCode for the file server.R in the app:\nlibrary(\"shiny\")\nlibrary(\"ggplot2\")\nlibrary(\"MASS\")\nshinyServer(function(input, output) {\n  output$regPlot = renderPlot({\n    whpt = 15\n    mtcars_new = mtcars\n    mtcars_new$mpg[whpt] = mtcars_new$mpg[whpt] + input$shift\n    reg = switch(input$method,\n      lm = lm(mpg ~ disp, data = mtcars_new),\n      rlm = rlm(mpg ~ disp, data = mtcars_new),\n      stop(\"Unimplemented method:\", input$method)\n    )\n    ggplot(mtcars_new, aes(x = disp, y = mpg)) + geom_point() +\n      geom_abline(intercept = reg$coefficients[\"(Intercept)\"],\n                  slope = reg$coefficients[\"disp\"], col = \"blue\")\n  })\n})__\nOf course you can add many more features.\nAnders, Simon, and Wolfgang Huber. 2010. “Differential Expression Analysis for Sequence Count Data.” Genome Biology 11: R106. http://genomebiology.com/2010/11/10/R106.\nAnders, Simon, Alejandro Reyes, and Wolfgang Huber. 2012. “Detecting differential usage of exons from RNA-Seq data.” Genome Research 22 (10): 2008–17.\nBrooks, Angela N, Li Yang, Michael O Duff, Kasper D Hansen, Jung W Park, Sandrine Dudoit, Steven E Brenner, and Brenton R Graveley. 2011. “Conservation of an RNA Regulatory Map Between Drosophila and Mammals.” Genome Research , 193–202. https://doi.org/10.1101/gr.108662.110.\nCook, R. Dennis. 1977. “Detection of Influential Observation in Linear Regression.” Technometrics.\nHuber, Peter J. 1964. “Robust Estimation of a Location Parameter.” The Annals of Mathematical Statistics 35: 73–101.\nLeek, Jeffrey T., and John D. Storey. 2007. “Capturing heterogeneity in gene expression studies by surrogate variable analysis.” PLoS Genetics 3 (9): 1724–35.\nLove, Michael I., Simon Anders, Vladislav Kim, and Wolfgang Huber. 2015. “RNA- Seq Workflow: Gene-Level Exploratory Analysis and Differential Expression.” F1000Research 4 (1070). https://doi.org/10.12688/f1000research.7035.1.\nLove, Michael I, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation of Fold Change and Dispersion for RNA-seq Data with DESeq2.” Gnome Biology 15 (12): 1–21.\nOzsolak, Fatih, and Patrice M Milos. 2011. “RNA sequencing: advances, challenges and opportunities.” Nature Reviews Genetics 12: 87–98.\nReyes, Alejandro, Simon Anders, Robert J. Weatheritt, Toby J. Gibson, Lars M. Steinmetz, and Wolfgang Huber. 2013. “Drift and Conservation of Differential Exon Usage Across Tissues in Primate Species.” Proceedings of the National Academy of Sciences 110 (38): 15377–82. https://doi.org/10.1073/pnas.1307202110.\nReyes, Alejandro, and Wolfgang Huber. 2017. “Alternative Start and Termination Sites of Transcription Drive Most Transcript Isoform Differences Across Human Tissues.” Nucleic Acids Research 46 (2): 582–92. https://doi.org/10.1093/nar/gkx1165.\nRobinson, M. D., D. J. McCarthy, and G. K. Smyth. 2009. “edgeR: A Bioconductor Package for Differential Expression Analysis of Digital Gene Expression Data.” Bioinformatics 26 (1): 139–40. https://doi.org/10.1093/bioinformatics/btp616.\nRousseeuw, Peter J. 1987. “Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis.” Journal of Computational and Applied Mathematics 20: 53–65.\nRousseeuw, Peter J., and Annick M. Leroy. 1987. Robust Regression and Outlier Detection. Wiley. https://doi.org/10.1002/0471725382.\nStegle, O., L. Parts, R. Durbin, and J. Winn. 2010. “A Bayesian framework to account for complex non-genetic factors in gene expression levels greatly increases power in eQTL studies.” PLoS Computational Biology 6 (5): e1000770.\nSteijger, T., J. F. Abril, P. G. Engstrom, F. Kokocinski, T. J. Hubbard, R. Guigo, J. Harrow, et al. 2013. “Assessment of transcript reconstruction methods for RNA-seq.” Nature Methods 10 (12): 1177–84.\nVerhulst, Pierre-François. 1845. “Recherches mathématiques Sur La Loi d’accroissement de La Population.” Nouveaux Mémoires de l’Académie Royale Des Sciences Et Belles-Lettres de Bruxelles 18: 1–42.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 Goals of this chapter</span>"
    ]
  },
  {
    "objectID": "09-chap.html",
    "href": "09-chap.html",
    "title": "11  9.1 Goals for this chapter",
    "section": "",
    "text": "11.1 9.2 Multidimensional scaling and ordination\nReal situations often involve point clouds, gradients, graphs, attraction points, noise and different spatial milieux, a little like this picture where we have a rigid skeleton, waves, sun and starlings.\nIn Chapter 7, we saw how to summarize rectangular matrices whose columns were continuous variables. The maps we made used unsupervised dimensionality reduction techniques such as principal component analysis aimed at isolating the most important signal component in a matrix \\(X\\) when all the columns have meaningful variances.\nHere we extend these ideas to more complex heterogeneous data where continuous and categorical variables are combined. Indeed, sometimes our observations cannot be easily described by sets of individual variables or coordinates – but it is possible to determine distances or (dis)similarities between them, or to describe relationships between them using a graph or a tree. Examples include species in a species tree or biological sequences. Outside of biology, examples include text documents or movie files, where we may have a reasonable method to determine (dis)similarity between them, but no obvious variables or coordinates.\nThis chapter contains more advanced techniques, for which we often omit technical details. Having come this far, we hope that by giving you some hands-on experience with examples, and extensive references, to enable you to understand and use some of the more `cutting edge’ techniques in nonlinear multivariate analysis.\nIn this chapter, we will:\nSometimes, data are not represented as points in a feature space. This can occur when we are provided with (dis)similarity matrices between objects such as drugs, images, trees or other complex objects, which have no obvious coordinates in \\({R}^n\\).\nIn Chapter 5 we saw how to produce clusters from distances. Here our goal is to visualize the data in maps in low dimensional spaces (e.g., planes) reminiscent of the ones we make from the first few principal axes in PCA.\nWe start with an intuitive example using geography data. In Figure 9.1, a heatmap and clustering of the distances between cities and places in Ukraine1 are shown.\n1 The provenance of these data are described in the script ukraine-dists.R in the data folder.\nFigure 9.1: A heatmap of the ukraine_dists distance matrix. Distances are measured in kilometres. The function has re-arranged the order of the cities, and grouped the closest ones.\nBesides ukraine_dists, which contains the pairwise distances, the RData file that we loaded above also contains the dataframe ukraine_coords with the longitudes and latitudes; we will use this later as a ground truth. Given the distances, multidimensional scaling (MDS) provides a “map” of their relative locations. It will not be possible to arrange the cities such that their Euclidean distances on a 2D plane exactly reproduce the given distance matrix: the cities lie on the curved surface of the Earth rather than in a plane. Nevertheless, we can expect to find a two dimensional embedding that represents the data well. With biological data, our 2D embeddings are likely to be much less clearcut. We call the function with:\nWe make a function that we will reuse several times in this chapter to make a screeplot from the result of a call to the cmdscale function:\nFigure 9.2: Screeplot of the first four eigenvalues. There is a pronounced drop after the first two eigenvalues, which indicates that the data are well described by a two-dimensional embedding.\n__\nQuestion 9.1\nLook at all the eigenvalues output by the cmdscale function: what do you notice?\n__\nSolution\n__\nIf you execute:\nFigure 9.3: Screeplot of all the eigenvalues.\nyou will note that unlike in PCA, there are some negative eigenvalues. These are due to the way cmdscale works.\nThe main output from the cmdscale function are the coordinates of the two- dimensional embedding, which we show in Figure 9.4 (we will discuss how the algorithm works in the next section).\nFigure 9.4: MDS map based on the distances.\nNote that while relative positions are correct, the orientation of the map is unconventional: Crimea is at the top. This is a common phenomenon with methods that reconstruct planar embeddings from distances. Since the distances between the points are invariant under rotations and reflections (axis flips), any solution is as good as any other solution that relates to it via rotation or reflection. Functions like cmdscale will pick one of the equally optimal solutions, and the particular choice can depend on minute details of the data or the computing platform being used. Here, we can transform our result into a more conventional orientation by reversing the sign of the \\(y\\)-axis. We redraw the map in Figure 9.5 and compare this to the true longitudes and latitudes from the ukraine_coords dataframe (Figure 9.6).\nFigure 9.5: Same as Figure 9.4, but with y-axis flipped.\nFigure 9.6: True latitudes and longitudes, taken from the ukraine_coords dataframe.\n__\nQuestion 9.2\nWe drew the longitudes and latitudes in the right panel of Figure 9.6 without attention to aspect ratio. What is the right aspect ratio for this plot?\n__\nSolution\n__\nThere is no simple relationship between the distances that correspond to 1 degree change in longitude and to 1 degree change in latitude, so the choice is difficult to make. Even under the simplifying assumption that our Earth is spherical and has a radius of 6371 km, it’s complicated: one degree in latitude always corresponds to a distance of 111 km (\\(6371/360\\)), as does one degree of longitude on the equator. However, when you move away from the equator, a degree of longitude corresponds to shorter and shorter distances (and to no distance at all at the poles). Pragmatically, for displays such as in Figure 9.6, we could choose a value for the aspect ratio that’s somewhere in the middle between the Northern and Southern most points, say, the cosine for 48 degrees.\n__\nQuestion 9.3\nAdd international borders and geographic features such as rivers to Figure 9.6.\n__\nSolution\n__\nA start point is provided by the code below, which adds the international borders as a polygon (Figure 9.7).\nThere is a lot of additional infrastructure available in R for geospatial data, including vector and raster data types.\nFigure 9.7: International borders added to Figure 9.6.\nNote: MDS creates similar output as PCA, however there is only one ‘dimension’ to the data (the sample points). There is no ‘dual’ dimension, there are no biplots and no loading vectors. This is a drawback when coming to interpreting the maps. Interpretation can be facilitated by examining carefully the extreme points and their differences.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "09-chap.html#multidimensional-scaling-and-ordination",
    "href": "09-chap.html#multidimensional-scaling-and-ordination",
    "title": "11  9.1 Goals for this chapter",
    "section": "",
    "text": "library(\"pheatmap\")\ndata(\"ukraine_dists\", package = \"MSMB\")\nas.matrix(ukraine_dists)[1:4, 1:4]__\n\n\n               Kyiv    Odesa Sevastopol Chernihiv\nKyiv         0.0000 441.2548   687.7551  128.1287\nOdesa      441.2548   0.0000   301.7482  558.6483\nSevastopol 687.7551 301.7482     0.0000  783.6561\nChernihiv  128.1287 558.6483   783.6561    0.0000\n\n\npheatmap(as.matrix(ukraine_dists), \n  color = colorRampPalette(c(\"#0057b7\", \"#ffd700\"))(50),\n  breaks = seq(0, max(ukraine_dists)^(1/2), length.out = 51)^2,\n  treeheight_row = 10, treeheight_col = 10)__\n\n\n\nukraine_mds = cmdscale(ukraine_dists, eig = TRUE)__\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nplotscree = function(x, m = length(x$eig)) {\n  ggplot(tibble(eig = x$eig[seq_len(m)], k = seq(along = eig)),\n    aes(x = k, y = eig)) + theme_minimal() +\n    scale_x_discrete(\"k\", limits = as.factor(seq_len(m))) + \n    geom_bar(stat = \"identity\", width = 0.5, fill = \"#ffd700\", col = \"#0057b7\")\n}__\n\n\nplotscree(ukraine_mds, m = 4)__\n\n\n\n\n\n\n\n\n\nukraine_mds$eig |&gt; signif(3)__\n\n\n [1]  3.91e+06  1.08e+06  3.42e+02  4.84e-01  2.13e-01  3.83e-05  5.90e-06\n [8]  5.82e-07  8.79e-08  4.94e-08  6.52e-10  2.84e-10  1.84e-10  5.22e-11\n[15]  4.89e-11  4.57e-11 -3.26e-12 -2.55e-11 -5.90e-11 -6.55e-11 -1.40e-10\n[22] -1.51e-10 -3.46e-10 -3.76e-10 -4.69e-10 -2.24e-09 -1.51e-08 -9.60e-05\n[29] -2.51e-04 -1.41e-02 -1.19e-01 -3.58e+02 -8.85e+02\n\n\nplotscree(ukraine_mds)__\n\n\n\n\nukraine_mds_df = tibble(\n  PCo1 = ukraine_mds$points[, 1],\n  PCo2 = ukraine_mds$points[, 2],\n  labs = rownames(ukraine_mds$points)\n)\nlibrary(\"ggrepel\")\ng = ggplot(ukraine_mds_df, aes(x = PCo1, y = PCo2, label = labs)) +\n  geom_point() + geom_text_repel(col = \"#0057b7\") + coord_fixed() \ng __\n\n\n\ng %+% mutate(ukraine_mds_df, PCo1 = PCo1, PCo2 = -PCo2)__\n\n\ndata(\"ukraine_coords\", package = \"MSMB\")\nprint.data.frame(ukraine_coords[1:4,  c(\"city\", \"lat\", \"lon\")])__\n\n\n        city      lat      lon\n1       Kyiv 50.45003 30.52414\n2      Odesa 46.48430 30.73229\n3 Sevastopol 44.60544 33.52208\n4  Chernihiv 51.49410 31.29433\n\n\nggplot(ukraine_coords, aes(x = lon, y = lat, label = city)) +\n  geom_point() + geom_text_repel(col = \"#0057b7\")__\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(\"maps\")\nua_borders = dplyr::filter(map_data(\"world\"), region == \"Ukraine\")\nggplot(ukraine_coords, aes(x = lon, y = lat)) + \n  geom_polygon(data = ua_borders, aes(x = long, y = lat, group = subregion), fill = \"#ffd700\", color = \"#0057b7\") +\n  geom_point() + \n  geom_text_repel(aes(label = city)) +\n  coord_fixed(1/cos(48/180*pi))__\n\n\n\n\n\n11.1.1 9.2.1 How does the method work?\nLet’s take a look at what would happen if we really started with points whose coordinates were known2. We put these coordinates into the two columns of a matrix with as many rows as there are points. Now we compute the distances between points based on these coordinates. To go from the coordinates \\(X\\) to distances, we write \\[d^2_{i,j} = (x_i^1 - x_j1)2 + + (x_i^p - x_jp)2.\\] We will call the matrix of squared distances DdotD in R and \\(DD\\) in the text . We want to find points such that the square of their distances is as close as possible to the \\(DD\\) observed.3\nThis is different from \\(DD\\) or \\(D^2\\), the matrix-multiplication of \\(D\\) with itself.\n3 Here we commit a slight ‘abuse’ by using the longitudes and latitudes of our cities as Cartesian coordinates and ignoring the fact that they are curvilinear coordinates on a sphere-like surface.\n2 Here we commit a slight ‘abuse’ by using the longitude and longitude of our cities as Cartesian coordinates and ignoring the curvature of the earth’s surface. Check out the internet for information on the Haversine formula.\nX = with(ukraine_coords, cbind(lon, lat * cos(48)))\nDdotD = as.matrix(dist(X)^2)__\nThe relative distances do not depend on the point of origin of the data. We center the data by using the centering matrix \\(H\\) defined as \\(H=I-^t\\). Let’s check the centering property of \\(H\\) using:\nn = nrow(X)\nH = diag(rep(1,n))-(1/n) * matrix(1, nrow = n, ncol = n)\nXc = sweep(X,2,apply(X,2,mean))\nXc[1:2, ]__\n\n\n            lon          \n[1,] -1.1722946 -1.184705\n[2,] -0.9641429  1.353935\n\n\nHX = H %*% X\nHX[1:2, ]__\n\n\n            lon          \n[1,] -1.1722946 -1.184705\n[2,] -0.9641429  1.353935\n\n\napply(HX, 2, mean)__\n\n\n          lon               \n-1.618057e-15  1.747077e-16 \n__\nQuestion 9.4\nCall B0 the matrix obtained by applying the centering matrix both to the right and to the left of DdotD Consider the points centered at the origin given by the \\(HX\\) matrix and compute its cross product, we’ll call this B2. What do you have to do to B0 to make it equal to B2?\n__\nSolution\n__\nB0 = H  %*% DdotD %*% H\nB2 = HX %*% t(HX)\nB2[1:3, 1:3] / B0[1:3, 1:3]__\n\n\n     [,1] [,2] [,3]\n[1,] -0.5 -0.5 -0.5\n[2,] -0.5 -0.5 -0.5\n[3,] -0.5 -0.5 -0.5\n\n\nmax(abs(-0.5 * B0 - B2))__\n\n\n[1] 9.237056e-14\nTherefore, given the squared distances between rows (\\(DD\\)) and the cross product of the centered matrix \\(B=(HX)(HX)^t\\), we have shown:\n\\[ - H(DD) H=B \\]\nThis is always true, and we use it to reverse-engineer an \\(X\\) which satisfies Equation 9.1 when we are given \\(DD\\) to start with.\n\n11.1.1.1 From \\(DD\\) to \\(X\\) using singular vectors.\nWe can go backwards from a matrix \\(DD\\) to \\(X\\) by taking the eigen-decomposition of \\(B\\) as defined in Equation 9.1. This also enables us to choose how many coordinates, or columns, we want for the \\(X\\) matrix. This is very similar to how PCA provides the best rank \\(r\\) approximation.\nNote : As in PCA, we can write this using the singular value decomposition of \\(HX\\) (or the eigen decomposition of \\(HX(HX)^t\\)):\n\\[ HX^{(r)} = US{(r)}Vt S^{(r)} r , \\]\nThis provides the best approximate representation in an Euclidean space of dimension \\(r\\). The algorithm gives us the coordinates of points that have approximately the same distances as those provided by the \\(D\\) matrix.\n\\[S^{(r)} = ]The method is often called Principal Coordinates Analysis, or PCoA which stresses the connection to PCA.\n\n\n11.1.1.2 Classical MDS Algorithm.\nIn summary, given an \\(n n\\) matrix of squared interpoint distances \\(DD\\), we can find points and their coordinates \\(\\) by the following operations:\n\nDouble center the interpoint distance squared and multiply it by \\(-\\):\n\\(B = -H DD H\\).\nDiagonalize \\(B\\): \\(B = U U^t\\).\nExtract \\(\\): \\( = U ^{1/2}\\).\n\n\n\n11.1.1.3 Finding the right underlying dimensionality.\nAs an example, let’s take objects for which we have similarities (surrogrates for distances) but for which there is no natural underlying Euclidean space.\nIn a psychology experiment from the 1950s, Ekman (1954) asked 31 subjects to rank the similarities of 14 different colors. His goal was to understand the underlying dimensionality of color perception. The similarity or confusion matrix was scaled to have values between 0 and 1. The colors that were often confused had similarities close to 1. We transform the data into a dissimilarity by subtracting the values from 1:\nekm = read.table(\"../data/ekman.txt\", header=TRUE)\nrownames(ekm) = colnames(ekm)\ndisekm = 1 - ekm - diag(1, ncol(ekm))\ndisekm[1:5, 1:5]__\n\n\n     w434 w445 w465 w472 w490\nw434 0.00 0.14 0.58 0.58 0.82\nw445 0.14 0.00 0.50 0.56 0.78\nw465 0.58 0.50 0.00 0.19 0.53\nw472 0.58 0.56 0.19 0.00 0.46\nw490 0.82 0.78 0.53 0.46 0.00\n\n\ndisekm = as.dist(disekm)__\nWe compute the MDS coordinates and eigenvalues. We combine the eigenvalues in the screeplot shown in Figure 9.8:\nmdsekm = cmdscale(disekm, eig = TRUE)\nplotscree(mdsekm)__\n\nFigure 9.8: The screeplot shows us that the phenomenon is largely two dimensional.\nWe plot the different colors using the first two principal coordinates as follows:\ndfekm = mdsekm$points[, 1:2] |&gt;\n  `colnames&lt;-`(paste0(\"MDS\", 1:2)) |&gt;\n  as_tibble() |&gt;\n  mutate(\n    name = rownames(ekm),\n    rgb = photobiology::w_length2rgb(as.numeric(sub(\"w\", \"\", name))))\nggplot(dfekm, aes(x = MDS1, y = MDS2)) +\n  geom_point(col = dfekm$rgb, size = 4) +\n  geom_text_repel(aes(label = name)) + coord_fixed()__\n\nFigure 9.9: The layout of the scatterpoints in the first two dimensions has a horseshoe shape. The labels and colors show that the arch corresponds to the wavelengths.\nFigure 9.9 shows the Ekman data in the new coordinates. There is a striking pattern that calls for explanation. This horseshoe or arch structure in the points is often an indicator of a sequential latent ordering or gradient in the data (Diaconis, Goel, and Holmes 2008). We will revisit this in Section 9.5.\n\n\n\n11.1.2 9.2.2 Robust versions of MDS\nMultidimensional scaling aims to minimize the difference between the squared distances as given by \\(DD\\) and the squared distances between the points with their new coordinates. Unfortunately, this objective tends to be sensitive to outliers: one single data point with large distances to everyone else can dominate, and thus skew, the whole analysis. Often, we like to use something that is more robust, and one way to achieve this is to disregard the actual values of the distances and only ask that the relative rankings of the original and the new distances are as similar as possible. Such a rank based approach is robust: its sensitivity to outliers is reduced.\nRobustness: A method is robust if it is not too influenced by a few outliers. For example, the median of a set of \\(n\\) numbers does not change by a lot even if we change 20 the numbers by arbitrarily large amounts; to drastically shift the median, we need to change more than half of the numbers. In contrast, we can change the mean by a large amount by just manipulating one of the numbers. We say that the breakdown point of the median is 1/2, while that of the mean is only \\(1/n\\). Both mean and median are estimators of the location of a distribution (i.e., what is a “typical” value of the numbers), but the median is more robust. The median is based on the ranks; more generally, methods based on ranks are often more robust than those based on the actual values. Many nonparametric tests are based on reductions of data to their ranks.\nWe will use the Ekman data to show how useful robust methods are when we are not quite sure about the ‘scale’ of our measurements. Robust ordination, called non metric multidimensional scaling (NMDS for short) only attempts to embed the points in a new space such that the order of the reconstructed distances in the new map is the same as the ordering of the original distance matrix.\nNon metric MDS looks for a transformation \\(f\\) of the given dissimilarities in the matrix \\(d\\) and a set of coordinates in a low dimensional space (the map) such that the distance in this new map is \\(\\) and \\(f(d)\\). The quality of the approximation can be measured by the standardized residual sum of squares (stress) function:\n\\[ ^2=. \\]\nNMDS is not sequential in the sense that we have to specify the underlying dimensionality at the outset and the optimization is run to maximize the reconstruction of the distances according to that number. There is no notion of percentage of variation explained by individual axes as provided in PCA. However, we can make a simili-screeplot by running the program for all the successive values of \\(k\\) (\\(k=1, 2, 3, …\\)) and looking at how well the stress drops. Here is an example of looking at these successive approximations and their goodness of fit. As in the case of diagnostics for clustering, we will take the number of axes after the stress has a steep drop.\nBecause each calculation of a NMDS result requires a new optimization that is both random and dependent on the \\(k\\) value, we use a similar procedure to what we did for clustering in Chapter 4. We execute the metaMDS function, say, 100 times for each of the four possible values of \\(k\\) and record the stress values.\nlibrary(\"vegan\")\nnmds.stress = function(x, sim = 100, kmax = 4) {\n  sapply(seq_len(kmax), function(k)\n    replicate(sim, metaMDS(x, k = k, autotransform = FALSE)$stress))\n}\nstress = nmds.stress(disekm, sim = 100)\ndim(stress)__\nLet’s look at the boxplots of the results. This can be a useful diagnostic plot for choosing \\(k\\) (Figure 9.10).\ndfstr = reshape2::melt(stress, varnames = c(\"replicate\",\"dimensions\"))\nggplot(dfstr, aes(y = value, x = dimensions, group = dimensions)) +\n  geom_boxplot()__\n\nFigure 9.10: Several replicates at each dimension were run to evaluate the stability of the stress. We see that the stress drops dramatically with two or more dimensions, thus indicating that a two dimensional solution is appropriate here.\nWe can also compare the distances and their approximations using what is known as a Shepard plot for \\(k=2\\) for instance, computed with:\nnmdsk2 = metaMDS(disekm, k = 2, autotransform = FALSE)\nstressplot(nmdsk2, pch = 20)__\n\nFigure 9.11: The Shepard’s plot compares the original distances or dissimilarities (along the horizonal axis) to the reconstructed distances, in this case for \\(k=2\\) (vertical axis).\nBoth the Shepard’s plot in Figure 9.11 and the screeplot in Figure 9.10 point to a two-dimensional solution for Ekman’s color confusion study. Let us compare the output of the two different MDS programs, the classical metric least squares approximation and the nonmetric rank approximation method. The right panel of Figure 9.12 shows the result from the nonmetric rank approximation, the left panel is the same as Figure 9.9. The projections are almost identical in both cases. For these data, it makes little difference whether we use a Euclidean or nonmetric multidimensional scaling method.\nnmdsk2$points[, 1:2] |&gt; \n  `colnames&lt;-`(paste0(\"NmMDS\", 1:2)) |&gt;\n  as_tibble() |&gt; \n  bind_cols(dplyr::select(dfekm, rgb, name)) |&gt;\n  ggplot(aes(x = NmMDS1, y = NmMDS2)) +\n    geom_point(col = dfekm$rgb, size = 4) +\n    geom_text_repel(aes(label = name))__\n\n\n\n\n\n\n\n\nFigure 9.12: Comparison of the output from (a) the classical multidimensional scaling (same as Figure 9.9) and (b) the nonmetric version.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "09-chap.html#contiguous-or-supplementary-information",
    "href": "09-chap.html#contiguous-or-supplementary-information",
    "title": "11  9.1 Goals for this chapter",
    "section": "11.2 9.3 Contiguous or supplementary information",
    "text": "11.2 9.3 Contiguous or supplementary information\n\n\n\nMetadata: Many programs and workflows for biological sequence analysis or assays separate the environmental and contextual information, which they call metadata, from the assay data or sequence reads. We discourage such practice as the exact connections between the samples and covariates are important. A lost connection between the assays and covariates makes later analyses impossible. Covariates such as clinical history, time, batch or location are important and should be considered components of the data.\n\n\nMetadata: Many programs and workflows for biological sequence analysis or assays separate the environmental and contextual information, which they call metadata , from the assay data or sequence reads. We discourage such practice as the exact connections between the samples and covariates are important. A lost connection between the assays and covariates makes later analyses impossible. Covariates such as clinical history, time, batch or location are important and should be considered components of the data.\nIn Chapter 3 we introduced the R data.frame class that enables us to combine heterogeneous data types: categorical factors, text and continuous values. Each row of a dataframe corresponds to an object, or a record, and the columns are the different variables, or features.\nExtra information about sample batches, dates of measurement, different protocols are often named metadata ; this can be misnomer if it is implied that metadata are somehow less important. Such information is real data that need to be integrated into the analyses. We typically store it in a data.frame or a similar R class and tightly link it to the primary assay data.\n\n11.2.1 9.3.1 Known batches in data\nHere we show an example of an analysis that was done by Holmes et al. (2011) on bacterial abundance data from Phylochip (Brodie et al. 2006) microarrays. The experiment was designed to detect differences between a group of healthy rats and a group who had Irritable Bowel Disease (Nelson et al. 2010). This example shows a case where the nuisance batch effects become apparent in the analysis of experimental data. It is an illustration of the fact that best practices in data analyses are sequential and that it is better to analyse data as they are collected to adjust for severe problems in the experimental design as they occur , instead of having to deal with them post mortem 4.\n4 Fisher’s terminology, see Chapter 13.\nWhen data collection started on this project, days 1 and 2 were delivered and we made the plot that appears in Figure 9.14. This showed a definite day effect. When investigating the source of this effect, we found that both the protocol and the array were different in days 1 and 2. This leads to uncertainty in the source of variation, we call this confounding of effects.\n\n\n\nBioconductor container: These data are an example of an awkward way of combining batch information with the actual data. The day information has been combined with the array data and encoded as a number and could be confused with a continuous variable. We will see in the next section a better practice for storing and manipulating heterogeneous data using a Bioconductor container called SummarizedExperiment.\n\n\nBioconductor container: These data are an example of an awkward way of combining batch information with the actual data. The day information has been combined with the array data and encoded as a number and could be confused with a continuous variable. We will see in the next section a better practice for storing and manipulating heterogeneous data using a Bioconductor container called SummarizedExperiment.\nWe load the data and the packages we use for this section:\nIBDchip = readRDS(\"../data/vsn28Exprd.rds\")\nlibrary(\"ade4\")\nlibrary(\"factoextra\")\nlibrary(\"sva\")__\n__\nQuestion 9.5\nWhat class is the IBDchip ? Look at the last row of the matrix, what do you notice?\n__\nSolution\n__\nclass(IBDchip)__\n\n\n[1] \"matrix\" \"array\" \n\n\ndim(IBDchip)__\n\n\n[1] 8635   28\n\n\ntail(IBDchip[,1:3])__\n\n\n                                 20CF     20DF     20MF\nbm-026.1.sig_st              7.299308 7.275802 7.383103\nbm-125.1.sig_st              8.538857 8.998562 9.296096\nbru.tab.d.HIII.Con32.sig_st  6.802736 6.777566 6.859950\nbru.tab.d.HIII.Con323.sig_st 6.463604 6.501139 6.611851\nbru.tab.d.HIII.Con5.sig_st   5.739235 5.666060 5.831079\nday                          2.000000 2.000000 2.000000\n\n\ntable(IBDchip[nrow(IBDchip), ])__\n\n\n 1  2  3 \n 8 16  4 \nThe data are normalized abundance measurements of 8634 taxa measured on 28 samples. We use a rank-threshold transformation, giving the top 3000 most abundant taxa scores from 3000 to 1, and letting the remaining (low abundant) ones all have a score of 1. We also separate out the proper assay data from the (awkwardly placed) day variable, which should be considered a factor5:\n5 Below, we show how to arrange these data into a Bioconductor SummarizedExperiment, which is a much more sane way of storing such data.\nassayIBD = IBDchip[-nrow(IBDchip), ]\nday      = factor(IBDchip[nrow(IBDchip), ])__\nInstead of using the continuous, somehow normalized data, we use a robust analysis replacing the values by their ranks. The lower values are considered ties encoded as a threshold chosen to reflect the number of expected taxa thought to be present:\nrankthreshPCA = function(x, threshold = 3000) {\n  ranksM = apply(x, 2, rank)\n  ranksM[ranksM &lt; threshold] = threshold\n  ranksM = threshold - ranksM\n  dudi.pca(t(ranksM), scannf = FALSE, nf = 2)\n}\npcaDay12 = rankthreshPCA(assayIBD[, day != 3])\nfviz_eig(pcaDay12, bar_width = 0.6) + ggtitle(\"\")__\n\nFigure 9.13: The screeplot shows us that the samples can be usefully represented in a two dimensional embedding.\nday12 = day[ day!=3 ]\nrtPCA1 = fviz(pcaDay12, element = \"ind\", axes = c(1, 2), geom = c(\"point\", \"text\"),\n  habillage = day12, repel = TRUE, palette = \"Dark2\",\n  addEllipses = TRUE, ellipse.type = \"convex\") + ggtitle(\"\") +\n  coord_fixed()\nrtPCA1 __\n\nFigure 9.14: We have used colors to identify the different days and have kept the sample labels as well. We have also added convex hulls for each day. The group mean is identified as the point with the larger symbol (circle, triangle or square).\n__\nQuestion 9.6\nWhy do we use a threshold for the ranks?\n__\nSolution\n__\nLow abundances, at noise level occur for species that are not really present, of which there are more than half. A large jump in rank for these observations could easily occur without any meaningful reason. Thus we create a large number of ties for low abundance.\nFigure 9.14 shows that the sample arrange themselves naturally into two different groups according to the day of the samples. After discovering this effect, we delved into the differences that could explain these distinct clusters. There were two different protocols used (protocol 1 on day 1, protocol 2 on day 2) and unfortunately two different provenances for the arrays used on those two days (array 1 on day 1, array 2 on day 2).\nA third set of data of four samples had to be collected to deconvolve the confounding effect. Array 2 was used with protocol 2 on Day 3, Figure 9.15 shows the new PCA plot with all the samples created by the following:\npcaDay123 = rankthreshPCA(assayIBD)\nfviz(pcaDay123, element = \"ind\", axes = c(1, 2), geom = c(\"point\", \"text\"),\n  habillage = day, repel = TRUE, palette = \"Dark2\",\n  addEllipses = TRUE, ellipse.type = \"convex\") + \n  ggtitle(\"\") + coord_fixed()__\n[](09-chap_files/figure-html/fig-Threesetspca123-1.png “Figure 9.15 (a):”)\n\n\n\n[](09-chap_files/figure-html/fig-Threesetspca123-2.png “Figure 9.15 (b):”)\n\n\n\nFigure 9.15: When comparing the three day analysis to that of the first two days, we notice the inversion of signs in the coordinates on the second axis: this has no biological relevance. The important finding is that group 3 overlaps heavily with group 1 indicating that it was the protocol change on Day 2 which created the variability.\n__\nQuestion 9.7\nIn which situation would it be preferable to make confidence ellipses around the group means using the following code?\nfviz_pca_ind(pcaDay123, habillage = day, labelsize = 3,\n  palette = \"Dark2\", addEllipses = TRUE, ellipse.level = 0.69)__\n\nFigure 9.16: The eigenvalue screeplot the case of 3 groups is extremely similar to that with two groups shown in Figure 9.13.\nThrough this visualization we were able to uncover a flaw in the original experimental design. The first two batches shown in green and brown were both balanced with regards to IBS and healthy rats. They do show very different levels of variability and overall multivariate coordinates. In fact, there are two confounded effects. Both the arrays and protocols were different on those two days. We had to run a third batch of experiments on day 3, represented in purple, this used protocol from day 1 and the arrays from day 2. The third group faithfully overlaps with batch 1, telling us that the change in protocol was responsible for the variability.\n\n\n11.2.2 9.3.2 Removing batch effects\nThrough the combination of the continuous measurements from assayIBD and the supplementary batch number as a factor, the PCA map has provided an invaluable investigation tool. This is a good example of the use of supplementary points 6. The mean-barycenter points are created by using the group-means of points in each of the three groups and serve as extra markers on the plot.\n6 This is called a supplementary point because the new observation-point is not used in the matrix decomposition.\nWe can decide to re-align the three groups by subtracting the group means so that all the batches are centered on the origin. A slightly more effective way is to use the ComBat function available in the sva package. This function uses a similar, but slightly more sophisticated method (Empirical Bayes mixture approach (Leek et al. 2010)). We can see its effect on the data by redoing our robust PCA (see the result in Figure 9.17):\nmodel0 = model.matrix(~1, day)\ncombatIBD = ComBat(dat = assayIBD, batch = day, mod = model0)\npcaDayBatRM = rankthreshPCA(combatIBD)\nfviz(pcaDayBatRM, element = \"ind\", geom = c(\"point\", \"text\"),\n  habillage = day, repel=TRUE, palette = \"Dark2\", addEllipses = TRUE,\n  ellipse.type = \"convex\", axes =c(1,2)) + coord_fixed() + ggtitle(\"\")__\n\nFigure 9.17: The modified data with the batch effects removed now show three batch-groups heavily overlapping and centered almost at the origin.\n\n\n11.2.3 9.3.3 Hybrid data and Bioconductor containers\nA more rational way of combining the batch and treatment information into compartments of a composite object is to use the SummarizedExperiment class. It includes special slots for the assay(s) where rows represent features of interest (e.g., genes, transcripts, exons, etc.) and columns represent samples. Supplementary information about the features can be stored in a DataFrame object, accessible using the function rowData. Each row of the DataFrame provides information on the feature in the corresponding row of the SummarizedExperiment object.\n\n\n\nA confusing notational similarity occurs here, in the SummarizedExperiment framework a DataFrame is not the same as a data.frame.\n\n\nA confusing notational similarity occurs here, in the SummarizedExperiment framework a DataFrame is not the same as a data.frame.\nHere we insert the two covariates day and treatment in the colData object and combine it with assay data in a new SummarizedExperiment object.\nlibrary(\"SummarizedExperiment\")\ntreatment  = factor(ifelse(grepl(\"Cntr|^C\", colnames(IBDchip)), \"CTL\", \"IBS\"))\nsampledata = DataFrame(day = day, treatment = treatment)\nchipse = SummarizedExperiment(assays  = list(abundance = assayIBD),\n                              colData = sampledata)__\nThis is the best way to keep all the relevant data together, it will also enable you to quickly filter the data while keeping all the information aligned properly.\n\n\n\nYou can explore composite objects using the Environment pane in RStudio. You will see that in chipse, some of the slots are empty.\n\n\nYou can explore composite objects using the Environment pane in RStudio. You will see that in chipse, some of the slots are empty.\n__\nQuestion 9.8\nMake a new SummarizedExperiment object by choosing the subset of the samples that were created on day 2.\n__\nSolution\n__\nchipse[, day == 2]__\n\n\nclass: SummarizedExperiment \ndim: 8634 16 \nmetadata(0):\nassays(1): abundance\nrownames(8634): 01010101000000.2104_gPM_GC 01010101000000.2141_gPM_GC\n  ... bru.tab.d.HIII.Con323.sig_st bru.tab.d.HIII.Con5.sig_st\nrowData names(0):\ncolnames(16): 20CF 20DF ... IBSM IBSP\ncolData names(2): day treatment\nColumns of the DataFrame represent different attributes of the features of interest, e.g., gene or transcript IDs, etc. Here is an example of hybrid data container from single cell experiments (see Bioconductor workflow in Perraudeau et al. (2017) for more details).\ncorese = readRDS(\"../data/normse.rds\")\nnorm = assays(corese)$normalizedValues __\nAfter the pre-processing and normalization steps prescribed in the workflow, we retain the 1000 most variable genes measured on 747 cells.\n__\nQuestion 9.9\nHow many different batches do the cells belong to ?\n__\nSolution\n__\nlength(unique(colData(corese)$Batch))__\n\n\n[1] 18\nWe can look at a PCA of the normalized values and check graphically that the batch effect has been removed:\nrespca = dudi.pca(t(norm), nf = 3, scannf = FALSE)\nplotscree(respca, 15)\nPCS = respca$li[, 1:3]__\n\nFigure 9.18: Screeplot of the PCA of the normalized data.\n\n\n\nWe have set up colors for the clusters as in the workflow, (the code is not shown here).\n\n\nWe have set up colors for the clusters as in the workflow, (the code is not shown here).\nSince the screeplot in Figure 9.18 shows us that we must not dissociate axes 2 and 3, we will make a three dimensional plot with the rgl package. We use the following interactive code:\nlibrary(\"rgl\")\nbatch = colData(corese)$Batch\nplot3d(PCS,aspect=sqrt(c(84,24,20)),col=col_clus[batch])\nplot3d(PCS,aspect=sqrt(c(84,24,20)),\ncol = col_clus[as.character(publishedClusters)])__\n[](imgs/plotnormpcabatch1.png “Figure 9.19 (a):”)\n\n\n\n\n\n\n\nFigure 9.19: Two-dimensional screenshots of three-dimensional rgl plots. The points are colored according to batch numbers in (a), and according to the original clustering in (b). We can see that the batch effect has been effectively removed and that the cells show the original clustering.\nNote: Of course, the book medium is limiting here, as we are showing two static projections that do not do justice to the depth available when looking at the interactive dynamic plots as they appear using the plot3d function. We encourage the reader to experiment extensively with these and other interactive packages and they provide a much more intuitive experience of the data.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "09-chap.html#correspondence-analysis-for-contingency-tables",
    "href": "09-chap.html#correspondence-analysis-for-contingency-tables",
    "title": "11  9.1 Goals for this chapter",
    "section": "11.3 9.4 Correspondence analysis for contingency tables",
    "text": "11.3 9.4 Correspondence analysis for contingency tables\n\n11.3.1 9.4.1 Cross-tabulation and contingency tables\nCategorical data abound in biological settings: sequence status (CpG/non-CpG), phenotypes, taxa are often coded as factors as we saw in Chapter 2. Cross- tabulation of two such variables gives us a contingency table ; the result of counting the co-occurrence of two phenotypes (sex and colorblindness was such an example). We saw that the first step is to look at the independence of the two categorical variables; the standard statistical measure of independence uses the chisquare distance. This quantity will replace the variance we used for continuous measurements.\nThe columns and rows of the table have the same `status’ and we are not in supervised/regression type setting. We won’t see a sample/variable divide; as a consequence the rows and columns will have the same status and we will ‘center’ both the rows and the columns. This symmetry will also translate in our use of biplots where both dimensions appear on the same plot.\nTable 9.1: Sample by mutation matrix.\n\n\n\nPatient\nMut1\nMut2\nMut3\n…\n\n\n\n\nAHX112\n0\n0\n0\n\n\n\nAHX717\n1\n0\n1\n\n\n\nAHX543\n1\n0\n0\n\n\n\n\n\n11.3.1.1 Transforming the data to tabular form.\nIf the data are collected as long lists with each subject (or sample) associated to its levels of the categorical variables, we may want to transform them into a contingency table. Here is an example. In Table 9.1 HIV mutations are tabulated as indicator (0/1) binary variables. These data are then transformed into a mutation co-occurrence matrix shown in Table 9.2.\nTable 9.2: Cross-tabulation of the HIV mutations showing two-way co- occurrences.\n\n\n\nPatient\nMut1\nMut2\nMut3\n…\n\n\n\n\nMut1\n853\n29\n10\n\n\n\nMut2\n29\n853\n52\n\n\n\nMut3\n10\n52\n853\n\n\n\n\n__\nQuestion 9.10\nWhat information is lost in this cross-tabulation ?\nWhen will this matter?\nHere are some co-occurrence data from the HIV database (Rhee et al. 2003). Some of these mutations have a tendency to co- occur.\n__\nQuestion 9.11\nTest the hypothesis of independence of the mutations.\nBefore explaining the details of how correspondence analysis works, let’s look at the output of one of many correspondence analysis functions. We use dudi.coa from the ade4 package to plot the mutations in a lower dimensional projection; the procedure follows what we did for PCA.\ncooc = read.delim2(\"../data/coccurHIV.txt\", header = TRUE, sep = \",\")\ncooc[1:4, 1:11]__\n\n\n    X4S X6D X6K X11R X20R X21I X35I X35L X35M X35T X39A\n4S    0  28   8    0   99    0   22    5   15    3   45\n6D   26   0   0   34  131    0  108    4   30   13   84\n6K    7   0   0    6   45    0    5   13   38   35   12\n11R   0  35   7    0  127   12   60   17   15    6   42\n\n\nHIVca = dudi.coa(cooc, nf = 4, scannf = FALSE)\nfviz_eig(HIVca, geom = \"bar\", bar_width = 0.6) + ggtitle(\"\")__\n\nFigure 9.20: The dependencies between HIV mutations is clearly a three dimensional phenomenon, the three first eigenvalues show a clear signal in the data.\n\nFigure 9.21: A screenshot of the output from an interactive 3d plotting function (plot3d).\nAfter looking at a screeplot, we see that dimensionality of the underlying variation is definitely three dimensional, we plot these three dimensions. Ideally this would be done with an interactive three-dimensional plotting function such as that provided through the package rgl as shown in Figure 9.21.\n__\nQuestion 9.12\nUsing the car and rgl packages make 3d scatterplot similar to Figure 9.21.\nCompare to the plot obtained using aspect=FALSE with the plot3d function from rgl.\nWhat structure do you notice by rotating the cloud of points?\n__\nSolution\n__\nlibrary(\"rgl\")\nCA1=HIVca$li[,1];CA2=HIVca$li[,2];CA3=HIVca$li[,3]\nplot3d(CA1,CA2,CA3,aspect=FALSE,col=\"purple\")__\n\n\nfviz_ca_row(HIVca,axes = c(1, 2),geom=\"text\", col.row=\"purple\",\n  labelsize=3)+ggtitle(\"\") + xlim(-0.55, 1.7) + ylim(-0.53,1.1) +\n  theme_bw() +  coord_fixed()\nfviz_ca_row(HIVca,axes = c(1, 3), geom=\"text\",col.row=\"purple\",\n    labelsize=3)+ggtitle(\"\")+ xlim(-0.55, 1.7)+ylim(-0.5,0.6) +\n    theme_bw() + coord_fixed()__\n\n\n\n\n\n\n\n\nFigure 9.22: Two planar maps of the mutations defined with the horizontal axis corresponding to the first eigenvector of the CA and the vertical axis being the second axis in (a), and the third in (b); notice the difference in heights.\n__\nQuestion 9.13\nShow the code for plotting the plane defined by axes 1 and 3 of the correspondence analysis respecting the scaling of the vertical axis as shown in the bottom figure of Figure 9.22.\n__\nSolution\n__\nfviz_ca_row(HIVca, axes=c(1, 3), geom=\"text\", col.row=\"purple\", labelsize=3) +\n  ggtitle(\"\") + theme_minimal() + coord_fixed()__\nThis first example showed how to map all the different levels of one categorical variable (the mutations) in a similar way to how PCA projects continuous variables. We will now explore how this can be extended to two or more categorical variables.\n\n\n\n11.3.2 9.4.2 Hair color, eye color and phenotype co-occurrence\nWe will consider a small table, so we can follow the analysis in detail. The data are a contingency table of hair-color and eye-color phenotypic co- occurrence from students as shown in Table 9.3. In Chapter 2, we used a \\(^2\\) test to detect possible dependencies:\nHairColor = HairEyeColor[,,2]\nchisq.test(HairColor)__\n\n\n    Pearson's Chi-squared test\n\ndata:  HairColor\nX-squared = 106.66, df = 9, p-value &lt; 2.2e-16\nTable 9.3: Cross tabulation of students hair and eye color.\n\n\n\nBrown\nBlue\nHazel\nGreen\n\n\n\n\n\nBlack\n36\n9\n5\n2\n\n\nBrown\n66\n34\n29\n14\n\n\nRed\n16\n7\n7\n7\n\n\nBlond\n4\n64\n5\n8\n\n\n\nHowever, stating non independence between hair and eye color is not enough. We need a more detailed explanation of where the dependencies occur: which hair color occurs more often with green eyes ? Are some of the variable levels independent? In fact we can study the departure from independence using a special weighted version of SVD. This method can be understood as a simple extension of PCA and MDS to contingency tables.\n\n11.3.2.1 Independence: computationally and visually.\nWe start by computing the row and column sums; we use these to build the table that would be expected if the two phenotypes were independent. We call this expected table HCexp.\nrowsums = as.matrix(apply(HairColor, 1, sum))\nrowsums __\n\n\n      [,1]\nBlack   52\nBrown  143\nRed     37\nBlond   81\n\n\ncolsums = as.matrix(apply(HairColor, 2, sum))\nt(colsums)__\n\n\n     Brown Blue Hazel Green\n[1,]   122  114    46    31\n\n\nHCexp = rowsums %*%t (colsums) / sum(colsums)__\nNow we compute the \\(^2\\) (chi-squared) statistic, which is the sum of the scaled residuals for each of the cells of the table:\nsum((HairColor  - HCexp)^2/HCexp)__\n\n\n[1] 106.6637\nWe can study these residuals from the expected table, first numerically then in Figure 9.23.\nround(t(HairColor-HCexp))__\n\n\n       Hair\nEye     Black Brown Red Blond\n  Brown    16    10   2   -28\n  Blue    -10   -18  -6    34\n  Hazel    -3     8   2    -7\n  Green    -3     0   3     0\n\n\nlibrary(\"vcd\")\nmosaicplot(HairColor, shade=TRUE, las=1, type=\"pearson\", cex.axis=0.7, main=\"\")__\n\nFigure 9.23: Visualization of the departure from independence. Now, the boxes are proportional in size to the actual observed counts and we no longer have a ‘rectangular’ property. The departure from independence is measured in Chisquared distance for each of the boxes and colored according to whether the residuals are large and positive. Dark blue indicates a positive association, for instance between blue eyes and blonde hair, red indicates a negative association such as in the case of blond hair and brown eyes.\n\n\n11.3.2.2 Mathematical Formulation.\nHere are the computations we just did in R in a more mathematical form. For a general contingency table \\({N}\\) with \\(I\\) rows and \\(J\\) columns and a total sample size of \\(n={i=1}^I {j=1}^J n_{ij}= n_{}\\). If the two categorical variables were independent, each cell frequency would be approximately equal to\n\\[ n_{ij} = n \\]\ncan also be written:\n\\[ {N} = {c r’} n, c= {{N}} {}_m ;; r’= {N}’ {}_p \\]\nThe departure from independence is measured by the \\(^2\\) statistic\n\\[ {X}^2=_{i,j} {n} \\]\nOnce we have ascertained that the two variables are not independent, we use a weighted multidimensional scaling using \\(^2\\) distances to visualize the associations.\nCorrespondece Analysis functions CCA in vegan , CA in FactoMineR , ordinate in phyloseq , dudi.coa in ade4.\nThe method is called Correspondence Analysis (CA) or Dual Scaling and there are multiple R packages that implement it.\nHere we make a simple biplot of the Hair and Eye colors.\nHC = as.data.frame.matrix(HairColor)\ncoaHC = dudi.coa(HC,scannf=FALSE,nf=2)\nround(coaHC$eig[1:3]/sum(coaHC$eig)*100)__\n\n\n[1] 89 10  2\n\n\nfviz_ca_biplot(coaHC, repel=TRUE, col.col=\"brown\", col.row=\"purple\") +\n  ggtitle(\"\") + ylim(c(-0.5,0.5))__\n\nFigure 9.24: The CA plot gives a representation of a large proportion of the chisquare distance between the data and the values expected under independence. The first axis shows a contrast between black haired and blonde haired students, mirrored by the brown eye, blue eye opposition. In CA the two categories play symmetric roles and we can interpret the proximity of Blue eyes and Blond hair has meaning that there is strong co-occurence of these categories.\n__\nQuestion 9.14\nWhat percentage of the Chisquare statistic is explained by the first two axes of the Correspondence Analysis?\n__\nQuestion 9.15\nCompare the results with those obtained by using CCA in the vegan package with the appropriate value for the scaling parameter.\n__\nSolution\n__\nlibrary(\"vegan\")\nres.ca = vegan::cca(HairColor)\nplot(res.ca, scaling=3)__\n\n\n11.3.2.3 Interpreting the biplots\nCA has a special barycentric property: the biplot scaling is chosen so that the row points are placed at the center of gravity of the column levels with their respective weights. For instance, the Blue eyes column point is at the center gravity of the (Black, Brown, Red, Blond) with weights proportional to (9, 34, 7, 64). The Blond row point is very heavily weighted, this is why Figure 9.24 shows Blond and Blue quite close together.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "09-chap.html#finding-timeand-other-important-gradients.",
    "href": "09-chap.html#finding-timeand-other-important-gradients.",
    "title": "11  9.1 Goals for this chapter",
    "section": "11.4 9.5 Finding time…and other important gradients.",
    "text": "11.4 9.5 Finding time…and other important gradients.\nAll the methods we have studied in the last sections are commonly known as ordination methods. In the same way clustering allowed us to detect and interpret a hidden factor/categorical variable, ordination enables us to detect and interpret a hidden ordering, gradient or latent variable in the data.\n\nEcologists have a long history of interpreting the arches formed by observations points in correspondence analysis and principal components as ecological gradients (Prentice 1977). Let’s illustrate this first with a very simple data set on which we perform a correspondence analysis.\nThe first examples of seriation or chronology detection was that of archaelogical artifacts by Kendall (1969), who used presence/absence of features on pottery to date them. These so-called seriation methods are still relevant today as we follow developmental trajectories in single cell data for instance.\nload(\"../data/lakes.RData\")\nlakelike[1:3,1:8]__\n\n\n     plant1 plant2 plant3 plant4 plant5 plant6 plant7 plant8\nloc1      6      4      0      3      0      0      0      0\nloc2      4      5      5      3      4      2      0      0\nloc3      3      4      7      4      5      2      1      1\n\n\nreslake=dudi.coa(lakelike,scannf=FALSE,nf=2)\nround(reslake$eig[1:8]/sum(reslake$eig),2)__\n\n\n[1] 0.56 0.25 0.09 0.03 0.03 0.02 0.01 0.00\nWe plot both the row-location points (Figure 9.25 (a)) and the biplot of both location and plant species in the lower part of Figure 9.25 (b); this plot was made with:\nfviz_ca_row(reslake,repel=TRUE)+ggtitle(\"\")+ylim(c(-0.55,1.7))\nfviz_ca_biplot(reslake,repel=TRUE)+ggtitle(\"\")+ylim(c(-0.55,1.7))__\n\n\n\n\n\n\n\n\nFigure 9.25: The locations near the lake are ordered along an arch as shown in (a). In the biplot (b), we can see which plants are most frequent at which locations by looking at the red triangles closest to the blue points.\n__\nQuestion 9.16\nLooking back at the raw matrix lakes as it appears, do you see a pattern in its entries?\nWhat would happen if the plants had been ordered by actual taxa names for instance?\n\n11.4.1 9.5.1 Dynamics of cell development\nWe will now analyse a more interesting data set that was published by Moignard et al. (2015). This paper describes the dynamics of blood cell development. The data are single cell gene expression measurements of 3,934 cells with blood and endothelial potential from five populations from between embryonic days E7.0 and E8.25.\n\nFigure 9.26: The four cell populations studied here are representative of three sequential states (PS,NP,HF) and two possible final branches (4SG and 4SFG\\(^{-}\\)).\nRemember from Chapter 4 that several different distances are available for comparing our cells. Here, we start by computing both an \\(L_2\\) distance and the \\(_1\\) distance between the 3,934 cells.\nMoignard = readRDS(\"../data/Moignard.rds\")\ncellt = rowData(Moignard)$celltypes\ncolsn = c(\"red\", \"purple\", \"orange\", \"green\", \"blue\")\nblom = assay(Moignard)\ndist2n.euclid = dist(blom)\ndist1n.l1     = dist(blom, \"manhattan\")__\nThe classical multidimensional scaling on these two distances matrices can be carried out using:\nce1Mds = cmdscale(dist1n.l1,     k = 20, eig = TRUE)\nce2Mds = cmdscale(dist2n.euclid, k = 20, eig = TRUE)\nperc1  = round(100*sum(ce1Mds$eig[1:2])/sum(ce1Mds$eig))\nperc2  = round(100*sum(ce2Mds$eig[1:2])/sum(ce2Mds$eig))__\nWe look at the underlying dimension and see in Figure 9.27 that two dimensions can provide a substantial fraction of the variance.\nplotscree(ce1Mds, m = 4)\nplotscree(ce2Mds, m = 4)__\n\n\n\n\n\n\n\n\nFigure 9.27: Screeplots from MDS on \\(_1\\) (a) and \\(L_2\\) (b) distances. We see that the eigenvalues are extremely similar and both point to a \\(2\\) dimensional phenomenon.\nThe first 2 coordinates account for 78 % of the variability when the \\(_1\\) distance is used between cells, and 57% when the \\(L^2\\) distance is used. We see in Figure 9.28 (a) the first plane for the MDS on the \\(_1\\) distances between cells:\nc1mds = ce1Mds$points[, 1:2] |&gt;\n        `colnames&lt;-`(paste0(\"L1_PCo\", 1:2)) |&gt;\n        as_tibble()\nggplot(c1mds, aes(x = L1_PCo1, y = L1_PCo2, color = cellt)) +\n  geom_point(aes(color = cellt), alpha = 0.6) +\n  scale_colour_manual(values = colsn) + guides(color = \"none\")\nc2mds = ce2Mds$points[, 1:2] |&gt;\n        `colnames&lt;-`(paste0(\"L2_PCo\", 1:2)) |&gt;\n        as_tibble()\nggplot(c2mds, aes(x = L2_PCo1, y = L2_PCo2, color = cellt)) +\n  geom_point(aes(color = cellt), alpha = 0.6) +\n   scale_colour_manual(values = colsn) + guides(color = \"none\")__\n\n\n\n\n\n\n\n\nFigure 9.28: Moignard cell data colored according to the cell types (blue: PS, green: NP, yellow: HF, red: 4SG, purple: 4SFG\\(^-\\)) in the two dimensional MDS plots created. In (a) using \\(_1\\) distances and in (b) using the L2 distances.\nFigure 9.28 (b) is created in the same way and shows the two-dimensional projection created by using MDS on the L2 distances.\nFigure 9.28 shows that both distances (L1 and L2) give the same first plane for the MDS with very similar representations of the underlying gradient followed by the cells.\nWe can see from Figure 9.28 that the cells are not distributed uniformly in the lower dimensions we have been looking at, we see a definite organization of the points. All the cells of type 4SG represented in red form an elongated cluster who are much less mixed with the other cell types.\n\n\n11.4.2 9.5.2 Local, nonlinear methods\nMultidimensional scaling and non metric multidimensional scaling aims to represent all distances as precisely as possible and the large distances between far away points skew the representations. It can be beneficial when looking for gradients or low dimensional manifolds to restrict ourselves to approximations of points that are close together. This calls for methods that try to represent local (small) distances well and do not try to approximate distances between faraway points with too much accuracy.\nThere has been substantial progress in such methods in recent years. The use of kernels computed using the calculated interpoint distances allows us to decrease the importance of points that are far apart. A radial basis kernel is of the form\n\\[ 1-(-), ^2 \\]\nIt has the effect of heavily discounting large distances. This can be very useful as the precision of interpoint distances is often better at smaller ranges; several examples of such methods are covered in Exercise 9.6 at the end of this chapter.\n__\nQuestion 9.17\nWhy do we take the difference between the 1 and the exponential?\nWhat happens when the distance between \\(x\\) and \\(y\\) is very big?\n\n11.4.2.1 t-SNE.\nThis widely used method adds flexibility to the kernel defined above and allows the \\(^2\\) parameter to vary locally (there is a normalization step so that it averages to one). The t-SNE method starts out from the positions of the points in the high dimensional space and derives a probability distribution on the set of pairs of points, such that the probabilities are proportional to the points’ proximities or similarities. It then uses this distribution to construct a representation of the dataset in low dimensions. The method is not robust and has the property of separating clusters of points artificially; however, this property can also help clarify a complex situation. One can think of it as a method akin to graph (or network) layout algorithms. They stretch the data to clarify relations between the very close (in the network: connected) points, but the distances between more distal (in the network: unconnected) points cannot be interpreted as being on the same scales in different regions of the plot. In particular, these distances will depend on the local point densities. Here is an example of the output of t-SNE on the cell data:\nlibrary(\"Rtsne\")\nrestsne = Rtsne(blom, dims = 2, perplexity = 30, verbose = FALSE,\n                max_iter = 900)\ndftsne = restsne$Y[, 1:2] |&gt;\n         `colnames&lt;-`(paste0(\"axis\", 1:2)) |&gt;\n         as_tibble()\nggplot(dftsne,aes(x = axis1, y = axis2, color = cellt)) +\n  geom_point(aes(color = cellt), alpha = 0.6) +\n   scale_color_manual(values = colsn) + guides(color = \"none\")\nrestsne3 = Rtsne(blom, dims = 3, perplexity = 30, verbose = FALSE,\n                 max_iter = 900)\ndftsne3 = restsne3$Y[, 1:3] |&gt;\n          `colnames&lt;-`(paste0(\"axis\", 1:3)) |&gt; \n          as_tibble()\nggplot(dftsne3,aes(x = axis3, y = axis2, group = cellt)) +\n      geom_point(aes(color = cellt), alpha = 0.6) +\n      scale_colour_manual(values = colsn) + guides(color = \"none\")__\n\n\n\n\n\n\n\n\nFigure 9.29: The four cell populations studied here are representative of three sequential states (PS,NP,HF) and two possible final branches (4SG and 4SFG\\(^{-}\\)). The plot on the left was obtained by choosing 2 dimensions for t-sne at a perplexity of 30. The lower plot has obtained by choosing 3 dimensions, we can see that this third t-SNE axis represented here as the horizontal axis.\nIn this case in order to see the subtle differences between MDS and t-SNE, it is really necessary to use 3d plotting.\n__\nTask\nUse the rgl package to look at the three t-SNE dimensions and add the correct cell type colors to the display.\nTwo of these 3d snapshots are shown in Figure 9.30, we see a much stronger grouping of the purple points than in the MDS plots.\nNote: A site worth visiting in order to appreciate more about the sensitivity of the t-SNE method to the complexity and \\(\\) parameters can be found at http://distill.pub/2016/misread-tsne.\n[](imgs/tsnemoignard3scrop.png “Figure 9.30 (a):”)\n\n\n\n[](imgs/tsnemoignard3crop.png “Figure 9.30 (b):”)\n\n\n\nFigure 9.30: Moignard cell data colored according to the cell types (blue: PS, green: NP, yellow: HF, red: 4SG, purple: 4SFG\\(^-\\)) in the three- dimensional t-SNE layouts. We can see that the purple cells (4SFG\\(^-\\)) segregate at the outer shell on the top of the point cloud.\n__\nQuestion 9.18\nVisualize a two-dimensional t-SNE embedding of the Ukraine distances from Section 9.2.\n__\nSolution\n__\nukraine_tsne = Rtsne(ukraine_dists, is_distance = TRUE, perplexity = 8)\nukraine_tsne_df = tibble(\n  PCo1 = ukraine_tsne$Y[, 1],\n  PCo2 = ukraine_tsne$Y[, 2],\n  labs = attr(ukraine_dists, \"Labels\")\n)\nggplot(ukraine_tsne_df, aes(x = PCo1, y = PCo2, label = labs)) +\n  geom_point() + geom_text_repel(col = \"#0057b7\") + coord_fixed() __\n\nFigure 9.31: t-SNE map based of Ukraine.\nThere are several other nonlinear methods for estimating nonlinear trajectories followed by points in the relevant state spaces. Here are a few examples.\nRDRToolbox Local linear embedding (LLE) and isomap\ndiffusionMap This package models connections between points as a Markovian kernel.\nkernlab Kernel methods\nLPCM-package Local principal curves",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "09-chap.html#multitable-techniques",
    "href": "09-chap.html#multitable-techniques",
    "title": "11  9.1 Goals for this chapter",
    "section": "11.5 9.6 Multitable techniques",
    "text": "11.5 9.6 Multitable techniques\nCurrent studies often attempt to quantify variation in the microbial, genomic, and metabolic measurements across different experimental conditions. As a result, it is common to perform multiple assays on the same biological samples and ask what features – microbes, genes, or metabolites, for example – are associated with different sample conditions. There are many ways to approach these questions. Which to apply depends on the study’s focus.\n\n11.5.1 9.6.1 Co-variation, inertia, co-inertia and the RV coefficient\nAs in physics, we define inertia as a sum of distances with ‘weighted’ points. This enables us to compute the inertia of counts in a contingency table as the weighted sum of the squares of distances between observed and expected frequencies (as in the chisquare statistic).\nAnother generalization of variance-inertia is the useful Phylogenetic diversity index. (computing the sum of distances between a subset of taxa through the tree). Other useful generalizations include using variability of points on a graph taken from standard spatial statistics.\nIf we want to study two standardized variables measured at the same 10 locations together, we use their covariance. If \\(x\\) represents the standardized PH, and and \\(y\\) the standardized humidity, we measure their covariation using the mean\n\\[ (x,y) = (x1y1 + x2y2 + x3y3 + + x10y10). \\]\nIf \\(x\\) and \\(y\\) co-vary in the same direction, this will be big. We saw how useful the correlation coefficient we defined in Chapter 8 was to our multivariate analyses. Multitable generalizations will be just as useful.\n\n\n11.5.2 9.6.2 Mantel coefficient and a test of distance correlation\n\n\n\nThere are some precautions to be taken when using the Mantel coefficient, see a critical review in Guillot and Rousset (2013).\n\n\nThere are some precautions to be taken when using the Mantel coefficient, see a critical review in Guillot and Rousset (2013).\nThe Mantel coefficient, one of the earliest version of association measures, is probably also the most popular now, especially in ecology (Josse and Holmes 2016). Given two dissimilarity matrices \\(D^X\\) and \\(D^Y\\) associated with \\(X\\) and \\(Y\\), make these matrices into vectors the way the R dist function does, and compute their linear correlation. A prototypical application is, for instance, to compute \\(D^X\\) from the soil chemistry at 17 different locations and to use \\(D^Y\\) to represent dissimilarities in plant occurences as measured by the Jaccard index between the same 17 locations. The Mantel coefficient is defined mathematically as:\n\\[ r_m(X, Y) = { }, \\]\nwith \\({d}^X\\) (resp. \\({d}^Y\\)) the mean of the upper diagonal elements of the dissimilarity matrix associated to \\(d^X\\) (resp. to \\(d^Y\\)). The main difference between the Mantel coefficient and the others such as the RV or the dCov is the absence of double centering. Due to the dependences within distances matrix, the Mantel correlation’s null distribution and its statistical significance cannot be assessed as simply for regular correlation ccoefficients. Instead, it is usually assessed via permutation testing. See Josse and Holmes (2016) for a review with historical background and modern incarnations. The coefficient and associated tests are implemented in several R packages such as ade4 (Chessel, Dufour, and Thioulouse 2004), vegan and ecodist (Goslee, Urban, et al. 2007).\n\n\n11.5.3 9.6.3 The RV coefficient\nThe global measure of similarity of two data tables as opposed to two vectors can be done by a generalization of covariance provided by an inner product between tables that gives the RV coefficient, a number between 0 and 1, like a correlation coefficient, but for tables.\n\\[ RV(A,B)= \\]\nThere are several other measures of matrix correlation available in the package MatrixCorrelation.\nIf we do ascertain a link between two matrices, we then need to find a way to understand that link. One such method is explained in the next section.\n\n\n11.5.4 9.6.4 Canonical correlation analysis (CCA)\nCCA is a method similar to PCA as it was developed by Hotelling in the 1930s to search for associations between two sets of continuous variables \\(X\\) and \\(Y\\). Its goal is to find a linear projection of the first set of variables that maximally correlates with a linear projection of the second set of variables.\nFinding correlated functions (covariates) of the two views of the same phenomenon by discarding the representation-specific details (noise) is expected to reveal the underlying hidden yet influential factors responsible for the correlation.\nLet us consider two matrices:\n\nthe \\(np\\) matrix \\(X\\), and\nthe \\(np\\) matrix \\(Y\\).\n\nThe \\(p\\) columns of \\(X\\) and the \\(q\\) columns of \\(Y\\) correspond to variables, and the rows correspond to the same \\(n\\) experimental units. We denote the \\(j\\)-th column of the matrix \\(X\\) by \\(X_j\\), likewise the \\(k\\)-th column of \\(Y\\) by \\(Y_k\\). Without loss of generality it will be assumed that the columns of \\(X\\) and \\(Y\\) are standardized (mean 0 and variance 1).\nClassical CCA assumes that \\(p n\\) and \\(q n\\), and that the matrices \\(X\\) and \\(Y\\) are of full column rank \\(p\\) and \\(q\\) respectively. In the following, CCA is presented as a problem solved through an iterative algorithm. The first stage of CCA consists of finding two vectors \\(a =(a_1,…,a_p)^t\\) and \\(b =(b_1,…,b_q)^t\\) that maximize the correlation between the linear combinations \\(U\\) and \\(V\\) defined as\n\\[ ]\nand assuming that the vectors \\(a\\) and \\(b\\) are normalized so that \\((U) = (V) = 1\\). In other words, the problem consists of finding \\(a\\) and \\(b\\) such that\n\\[ 1 = (U, V) = {a,b} (Xa, Yb) (Xa)=(Yb) = 1. \\]\nThe resulting variables \\(U\\) and \\(V\\) are called the first canonical variates and \\(_1\\) is referred to as the first canonical correlation.\nNote: Higher order canonical variates and canonical correlations can be found as a stepwise problem. For \\(s = 1,…,p\\), we can successively find positive correlations \\(_1 _2 … _p\\) with corresponding vectors \\((a^1, b^1), …, (a^p, b^p)\\), by maximizing\n\\[ s = (Us,Vs) = {as,bs} (Xas,Ybs) (Xa^s) = (Yb^s)=1 \\]\nunder the additional restrictions\n\\[ (Us,Ut) = (V^s, V^t)=0 t &lt; s p. \\]\nWe can think of CCA as a generalization of PCA where the variance we maximize is the ‘covariance’ between the two matrices (see Holmes (2006) for more details).\n\n\n11.5.5 9.6.5 Sparse canonical correlation analysis (sCCA)\nWhen the number of variables in each table is very large finding two very correlated vectors can be too easy and unstable: we have too many degrees of freedom.\n\n\n\nWe will see many examples of regularization and danger of overfitting in sec-supervised.\n\n\nWe will see many examples of regularization and danger of overfitting in Chapter 12.\nThen it is beneficial to add a penalty maintains the number of non-zero coefficients to a minimum. This approach is called sparse canonical correlation analysis (sparse CCA or sCCA), a method well-suited to both exploratory comparisons between samples and the identification of features with interesting co variation. We will use an implementation from the PMA package.\nHere we study a dataset collected by Kashyap et al. (2013) with two tables. One is a contingency table of bacterial abundances and another an abundance table of metabolites. There are 12 samples, so \\(n = 12\\). The metabolite table has measurements on \\(p = 637\\) feature and the bacterial abundances had a total of $ q = 20,609$ OTUs, which we will filter down to around 200. We start by loading the data.\nlibrary(\"genefilter\")\nload(\"../data/microbe.rda\")\nmetab = read.csv(\"../data/metabolites.csv\", row.names = 1) |&gt; as.matrix()__\nWe first filter down to bacteria and metabolites of interest, removing (“by hand”) those that are zero across many samples and giving an upper threshold of 50 to the large values. We transform the data to weaken the heavy tails.\nlibrary(\"phyloseq\")\nmetab   = metab[rowSums(metab == 0) &lt;= 3, ]\nmicrobe = prune_taxa(taxa_sums(microbe) &gt; 4, microbe)\nmicrobe = filter_taxa(microbe, filterfun(kOverA(3, 2)), TRUE)\nmetab   = log(1 + metab, base = 10)\nX       = log(1 + as.matrix(otu_table(microbe)), base = 10)__\nA second step in our preliminary analysis is to look if there is any association between the two matrices using the RV.test from the ade4 package:\ncolnames(metab) = colnames(X)\npca1 = dudi.pca(t(metab), scal = TRUE, scann = FALSE)\npca2 = dudi.pca(t(X), scal = TRUE, scann = FALSE)\nrv1 = RV.rtest(pca1$tab, pca2$tab, 999)\nrv1 __\n\n\nMonte-Carlo test\nCall: RV.rtest(df1 = pca1$tab, df2 = pca2$tab, nrepet = 999)\n\nObservation: 0.8400429 \n\nBased on 999 replicates\nSimulated p-value: 0.002 \nAlternative hypothesis: greater \n\n    Std.Obs Expectation    Variance \n6.231661953 0.314166070 0.007121318 \nWe can now apply sparse CCA. This method compares sets of features across high-dimensional data tables, where there may be more measured features than samples. In the process, it chooses a subset of available features that capture the most covariance – these are the features that reflect signals present across multiple tables. We then apply PCA to this selected subset of features. In this sense, we use sparse CCA as a screening procedure, rather than as an ordination method.\nThe implementation is below. The parameters penaltyx and penaltyz are sparsity penalties. Smaller values of penaltyx will result in fewer selected microbes, similarly penaltyz modulates the number of selected metabolites. We tune them manually to facilitate subsequent interpretation – we generally prefer more sparsity than the default parameters would provide.\nlibrary(\"PMA\")\nccaRes = CCA(t(X), t(metab), penaltyx = 0.15, penaltyz = 0.15, \n             typex = \"standard\", typez = \"standard\")__\n\n\n123456789\n\n\nccaRes __\n\n\nCall: CCA(x = t(X), z = t(metab), typex = \"standard\", typez = \"standard\", \n    penaltyx = 0.15, penaltyz = 0.15)\n\n\nNum non-zeros u's:  5 \nNum non-zeros v's:  16 \nType of x:  standard \nType of z:  standard \nPenalty for x: L1 bound is  0.15 \nPenalty for z: L1 bound is  0.15 \nCor(Xu,Zv):  0.9904707\nWith these parameters, 5 bacteria and 16 metabolites were selected based on their ability to explain covariation between tables. Further, these features result in a correlation of 0.99 between the two tables. We interpret this to mean that the microbial and metabolomic data reflect similar underlying signals, and that these signals can be approximated well by the selected features. Be wary of the correlation value, however, since the scores are far from the usual bivariate normal cloud. Further, note that it is possible that other subsets of features could explain the data just as well – sparse CCA has minimized redundancy across features, but makes no guarantee that these are the “true” features in any sense.\nNonetheless, we can still use these 21 features to compress information from the two tables without much loss. To relate the recovered metabolites and OTUs to characteristics of the samples on which they were measured, we use them as input to an ordinary PCA. We have omitted the code we used to generate Figure 9.32, we refer the reader to the online material accompanying the book or the workflow published in Callahan et al. (2016).\nFigure 9.32 displays the PCA triplot , where we show different types of samples and the multidomain features (Metabolites and OTUs). This allows comparison across the measured samples – triangles for knockout and circles for wild type –and characterizes the influence the different features – diamonds with text labels. For example, we see that the main variation in the data is across PD and ST samples, which correspond to the different diets. Further, large values of 15 of the features are associated with ST status, while small values for 5 of them indicate PD status.\n\nFigure 9.32: A PCA triplot produced from the CCA selected features from muliple data types (metabolites and OTUs).\nThe advantage of the sparse CCA screening is now clear – we can display most of the variation across samples using a relatively simple plot, and can avoid plotting the hundreds of additional points that would be needed to display all of the features.\n\n\n11.5.6 9.6.6 Canonical (or constrained) correspondence analysis (CCpnA)\n\n\n\nNotational overload for CCA: Originally invented by Braak (1985) and called Canonical Correspondence analysis, we will call this method Constrained Correspondence Analysis and abbreviate it CCpnA to avoid confusion with Canonical Correlation Analysis (CCA). However several R packages, such as ade4 and vegan use the name cca for their correspondence analyses function.\n\n\nNotational overload for CCA : Originally invented by Braak (1985) and called Canonical Correspondence analysis, we will call this method Constrained Correspondence Analysis and abbreviate it CCpnA to avoid confusion with Canonical Correlation Analysis (CCA). However several R packages, such as ade4 and vegan use the name cca for their correspondence analyses function.\nThe term constrained correspondence analysis translates the fact that this method is similar to a constrained regression. The method attempts to force the latent variables to be correlated with the environmental variables provided as `explanatory’.\nCCpnA creates biplots where the positions of samples are determined by similarity in both species signatures and environmental characteristics. In contrast, principal components analysis or correspondence analysis only look at species signatures. More formally, it ensures that the resulting CCpnA directions lie in the span of the environmental variables. For thorough explanations see Braak (1985; Greenacre 2007).\nThis method can be run using the function ordinate in phyloseq. In order to use the covariates from the sample data, we provide an extra argument, specifying which of the features to consider.\nHere, we take the data we denoised using dada2 in Chapter 4. We will see more details about creating the phyloseq object in Chapter 10. For the time being, we use the otu_table component containing a contingency table of counts for different taxa. We would like to compute the constrained correspondence analyses that explain the taxa abundances by the age and family relationship (both variables are contained in the sample_data slot of the ps1 object).\nWe would like to make two dimensional plots showing only using the four most abundant taxa (making the biplot easier to read):\nps1=readRDS(\"../data/ps1.rds\")\nps1p=filter_taxa(ps1, function(x) sum(x) &gt; 0, TRUE)\npsCCpnA = ordinate(ps1p, \"CCA\",\n                 formula = ps1p ~ ageBin + family_relationship)__\nTo access the positions for the biplot, we can use the scores function in the vegan. Further, to facilitate figure annotation, we also join the site scores with the environmental data in the sample_data slot. Of the 23 total taxonomic orders, we only explicitly annotate the four most abundant – this makes the biplot easier to read.\nevalProp = 100 * psCCpnA$CCA$eig[1:2] / sum(psCCpnA$CA$eig)\nggplot() +\n geom_point(data = sites,aes(x =CCA2, y =CCA1),shape =2,alpha=0.5) +\n geom_point(data = species,aes(x =CCA2,y =CCA1,col = Order),size=1)+\n geom_text_repel(data = dplyr::filter(species, CCA2 &lt; (-2)),\n                   aes(x = CCA2, y = CCA1, label = otu_id),\n                   size = 2, segment.size = 0.1) +\n facet_grid(. ~ ageBin) +\n guides(col = guide_legend(override.aes = list(size = 2))) +\n labs(x = sprintf(\"Axis2 [%s%% variance]\", round(evalProp[2])),\n      y = sprintf(\"Axis1 [%s%% variance]\", round(evalProp[1]))) +\n scale_color_brewer(palette = \"Set1\") + theme(legend.position=\"bottom\")__\n\nFigure 9.33: The mouse and taxa scores generated by CCpnA. The sites (mice samples) are triangles; species are circles, respectively. The separate panels indicate different age groups.\n__\nQuestion 9.19\nLook up the extra code for creating the tax and species objects in the online resources accompanying the book. Then make the analogue of Figure 9.33 but using litter as the faceting variable.\n__\nSolution\n__\n\nFigure 9.34: The analogue to Figure 9.33, faceting by litter membership rather than age bin.\nFigures 9.33 and 9.34 show the plots of these annotated scores, splitting sites by their age bin and litter membership, respectively. Note that to keep the appropriate aspect ratio in the presence of faceting, we have taken the vertical axis as our first canonical component. We have labeled individual bacteria that are outliers along the second CCpnA direction.\nEvidently, the first CCpnA direction distinguishes between mice in the two main age bins. Circles on the left and right of the biplot represent bacteria that are characteristic of younger and older mice, respectively. The second CCpnA direction splits off the few mice in the oldest age group; it also partially distinguishes between the two litters. These samples low in the second CCpnA direction have more of the outlier bacteria than the others.\nThis CCpnA analysis supports the conclusion that the main difference between the microbiome communities of the different mice lies along the age axis. However, in situations where the influence of environmental variables is not so strong, CCA can have more power in detecting such associations. In general, it can be applied whenever it is desirable to incorporate supplemental data, but in a way that (1) is less aggressive than supervised methods, and (2) can use several environmental variables at once.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "09-chap.html#summary-of-this-chapter",
    "href": "09-chap.html#summary-of-this-chapter",
    "title": "11  9.1 Goals for this chapter",
    "section": "11.6 9.7 Summary of this chapter",
    "text": "11.6 9.7 Summary of this chapter\nHeterogeneous data A mixture of many continuous and a few categorical variables can be handled by adding the categorical variables as supplementary information to the PCA. This is done by projecting the mean of all points in a group onto the map.\nUsing distances Relations between data objects can often be summarized as interpoint distances (whether distances between trees, images, graphs, or other complex objects).\nOrdination A useful representation of these distances is available through a method similar to PCA called multidimensional scaling (MDS), otherwise known as PCoA (principal coordinate analysis). It can be helpful to think of the outcome of these analyses as uncovering latent variable. In the case of clustering the latent variables are categorical, in ordination they are latent variables like time or environmental gradients like distance to the water. This is why these methods are often called ordination.\nRobust versions can be used when interpoint distances are wildly different. NMDS (nonmetric multidimensional scaling) aims to produce coordinates such that the order of the interpoint distances is respected as closely as possible.\nCorrespondence analysis : a method for computing low dimensional projections that explain dependencies in categorical data. It decomposes chisquare distance in much the same way that PCA decomposes variance. Correspondence analysis is usually the best way to follow up on a significant chisquare test. Once we have ascertained there are significant dependencies between different levels of categories, we can map them and interpret proximities on this map using plots and biplots.\nPermutation test for distances Given two sets of distances between the same points, we can measure whether they are related using the Mantel permutation test.\nGeneralizations of variance and covariance When dealing with more than one matrix of measurements on the same data, we can generalize the notion of covariance and correlations to vectorial measurements of co-inertia.\nCanonical correlation is a method for finding a few linear combinations of variables from each table that are as correlated as possible. When using this method on matrices with large numbers of variables, we use a regularized version with an L1 penalty that reduces the number of non-zero coefficients.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "09-chap.html#further-reading",
    "href": "09-chap.html#further-reading",
    "title": "11  9.1 Goals for this chapter",
    "section": "11.7 9.8 Further reading",
    "text": "11.7 9.8 Further reading\nInterpretation of PCoA maps and nonlinear embeddings can also be enhanced the way we did for PCA using generalizations of the supplementary point method, see Trosset and Priebe (2008) or Bengio et al. (2004). We saw in Chapter 7 how we can project one categorical variable onto a PCA. The correspondence analysis framework actually allows us to mix several categorical variables in with any number of continuous variables. This is done through an extension called multiple correspondence analysis (MCA) whereby we can do the same analysis on a large number of binary categorical variables and obtain useful maps. The trick here will be to turn the continuous variables into categorical variables first. For extensive examples using R see for instance the book by Pagès (2016).\nA simple extension to PCA that allows for nonlinear principal curve estimates instead of principal directions defined by eigenvectors was proposed in Hastie and Stuetzle (1989) and is available in the package princurve.\nFinding curved subspaces containing a high density data for dimensions higher than \\(1\\) is now called manifold embedding and can be done through Laplacian eigenmaps (Belkin and Niyogi 2003), local linear embedding as in Roweis and Saul (2000) or using the isomap method (Tenenbaum, De Silva, and Langford 2000). For textbooks covering nonlinear unsupervised learning methods see Hastie, Tibshirani, and Friedman (2008, chap. 14) or Izenman (2008).\nA review of many multitable correlation coefficients, and analysis of applications can be found in Josse and Holmes (2016).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "09-chap.html#exercises",
    "href": "09-chap.html#exercises",
    "title": "11  9.1 Goals for this chapter",
    "section": "11.8 9.9 Exercises",
    "text": "11.8 9.9 Exercises\n__\nExercise 9.1\nWe are going to take another look at the Phylochip data, replacing the original expression values by presence/absence. We threshold the data to retain only those that have a value of at least 8.633 in at least 8 samples7.\nibd.pres = ifelse(assayIBD[, 1:28] &gt; 8.633, 1, 0)__\nPerform a correspondence analysis on these binary data and compare the plot you obtain to what we saw in Figure 9.15.\n7 These values were chosen to give about retain about 3,000 taxa, similar to our previous choice of threshold.\n__\nSolution\n__\nSee Figure 9.35.\nIBDca = dudi.coa(ibd.pres, scannf = FALSE, nf = 4)\nfviz_eig(IBDca, geom = \"bar\", bar_width = 0.7) +\n    ylab(\"Percentage of chisquare\") + ggtitle(\"\")\nfviz(IBDca, element = \"col\", axes = c(1, 2), geom = \"point\",\n     habillage = day, palette = \"Dark2\", addEllipses = TRUE, color = day,\n     ellipse.type = \"convex\", alpha = 1, col.row.sup =  \"blue\",\n     select = list(name = NULL, cos2 = NULL, contrib = NULL),\n     repel = TRUE)__\n\n\n\\(\\)\n\n\n\n\\(\\)\n\nFigure 9.35: Correspondence analysis on binary data.\n__\nExercise 9.2\nCorrespondence Analysis on color association tables:\nHere is an example of data collected by looking at the number of Google hits resulting from queries of pairs of words. The numbers in Table 9.4 are to be multiplied by 1000. For instance, the combination of the words “quiet” and “blue” returned 2,150,000 hits.\nTable 9.4: Contingency table of co-occurring terms from search engine results.\n\n\n\nblack\nblue\ngreen\ngrey\norange\npurple\nwhite\n\n\n\n\n\nquiet\n2770\n2150\n2140\n875\n1220\n821\n2510\n\n\nangry\n2970\n1530\n1740\n752\n1040\n710\n1730\n\n\nclever\n1650\n1270\n1320\n495\n693\n416\n1420\n\n\ndepressed\n1480\n957\n983\n147\n330\n102\n1270\n\n\nhappy\n19300\n8310\n8730\n1920\n4220\n2610\n9150\n\n\nlively\n1840\n1250\n1350\n659\n621\n488\n1480\n\n\nperplexed\n110\n71\n80\n19\n23\n15\n109\n\n\nvirtuous\n179\n80\n102\n20\n25\n17\n165\n\n\n\nPerform a correspondence analysis of these data. What do you notice when you look at the two-dimensional biplot?\n__\nSolution\n__\nSee Figure 9.36. The code is not rendered here, but is shown in the document’s source file.\n\nFigure 9.36: Correspondence Analysis allows for a symmetrical graphical representation of two categorical variables, in this case colors and emotions for a contingency table of co-occurrences such as Table 9.4.\n\n__\nExercise 9.3\nThe dates Plato wrote his various books are not known. We take the sentence endings and use those pattern frequencies as the data.\nplatof = read.table(\"../data/platof.txt\", header = TRUE)\nplatof[1:4, ]__\n\n\n      Rep Laws Crit Phil Pol Soph Tim\nuuuuu  42   91    5   24  13   26  18\n-uuuu  60  144    3   27  19   33  30\nu-uuu  64   72    3   20  24   31  46\nuu-uu  72   98    2   25  20   24  14\n\n\nresPlato = dudi.coa(platof, scannf = FALSE, nf = 2)\nfviz_ca_biplot(resPlato, axes=c(2, 1)) + ggtitle(\"\")\nfviz_eig(resPlato, geom = \"bar\", width = 0.6) + ggtitle(\"\")__\nFigure 9.37: Biplot of Plato’s sentence endings.\n\nFrom the biplot in Figure 9.37 can you guess at the chronological order of Plato’s works?\nHint: the first (earliest) is known to be Republica. The last (latest) is known to be Laws.\nWhich sentence ending did Plato use more frequently early in his life?\nWhat percentage of the inertia (\\(^2\\)-distance) is explained by the map in Figure 9.37?\n\n\n\n\n\n\n\n\n\n__\nSolution\n__\nTo compute the percentage of inertia explained by the first two axes we take the cumulative sum of the eigenvalues at the value 2:\nnames(resPlato)__\n\n\n [1] \"tab\"  \"cw\"   \"lw\"   \"eig\"  \"rank\" \"nf\"   \"c1\"   \"li\"   \"co\"   \"l1\"  \n[11] \"call\" \"N\"   \n\n\nsum(resPlato$eig)__\n\n\n[1] 0.132618\n\n\npercentageInertia=round(100*cumsum(resPlato$eig)/sum(resPlato$eig))\npercentageInertia __\n\n\n[1]  69  85  92  96  98 100\n\n\npercentageInertia[2]__\n\n\n[1] 85\n__\nExercise 9.4\nWe are going to look at two datasets, one is a perturbed version of the other and they both present gradients as often seen in ecological data. Read in the two species count matrices lakelike and lakelikeh, which are stored as the object lakes.RData. Compare the output of correspondence analysis and principal component analysis on each of the two data sets; restrict yourself two dimensions. In the plots and the eigenvalues, what do you notice?\n__\nSolution\n__\nload(\"../data/lakes.RData\")\nlakelike[ 1:3, 1:8]__\n\n\n     plant1 plant2 plant3 plant4 plant5 plant6 plant7 plant8\nloc1      6      4      0      3      0      0      0      0\nloc2      4      5      5      3      4      2      0      0\nloc3      3      4      7      4      5      2      1      1\n\n\nlakelikeh[1:3, 1:8]__\n\n\n     plant1 plant2 plant3 plant4 plant5 plant6 plant7 plant8\nloc1      6      4      0      3      0      0      0      0\nloc2      4      5      5      3      4      2      0      0\nloc3      3      4      7      4      5      2      1      1\n\n\ne_coa  = dudi.coa(lakelike,  scannf = FALSE, nf = 2)\ne_pca  = dudi.pca(lakelike,  scannf = FALSE, nf = 2)\neh_coa = dudi.coa(lakelikeh, scannf = FALSE, nf = 2)\neh_pca = dudi.pca(lakelikeh, scannf = FALSE, nf = 2)__\nComparison (output not shown):\nscatter(e_pca)__\n\n\n scatter(e_coa)__\n\n\n s.label(e_pca$li)__\n\n\n s.label(e_coa$li)__\n\n\n s.label(eh_pca$co)__\n\n\n s.label(eh_pca$li)__\n\n\n s.label(eh_coa$li)__\n\n\n s.label(eh_coa$co)__\n__\nExercise 9.5\nWe analyzed the normalized Moignard data in Section 9.5.1. Now redo the analysis with the raw data (in file nbt.3154-S3-raw.csv) and compare the output with that obtained using the normalized values.\n__\nSolution\n__\nmoignard_raw = as.matrix(read.csv(\"../data/nbt.3154-S3-raw.csv\", row.names = 1))\ndist2r.euclid = dist(moignard_raw)\ndist1r.l1     = dist(moignard_raw, \"manhattan\")\ncells1.cmds = cmdscale(dist1r.l1,     k = 20, eig = TRUE)\ncells2.cmds = cmdscale(dist2r.euclid, k = 20, eig = TRUE)\nsum(cells1.cmds$eig[1:2]) / sum(cells1.cmds$eig)__\n\n\n[1] 0.776075\n\n\nsum(cells2.cmds$eig[1:2]) / sum(cells2.cmds$eig)__\n\n\n[1] 0.6297133\n__\nExercise 9.6\nWe are going to explore the use of kernel methods.\n\nCompute kernelized distances using the kernlab for the Moignard data using various values for the sigma tuning parameter in the definition of the kernels. Then perform MDS on these kernelized distances. What difference is there in variability explained by the first four components of kernel multidimensional scaling?\nMake interactive three dimensional representations of the components: is there a projection where you see a branch for the purple points?\n\n__\nSolution\n__\n\nkernelized distances\n\nlibrary(\"kernlab\")\nlaplacedot1 = laplacedot(sigma = 1/3934)\nrbfdot1     = rbfdot(sigma = (1/3934)^2 )\nKlaplace_cellsn   = kernelMatrix(laplacedot1, blom)\nKGauss_cellsn     = kernelMatrix(rbfdot1, blom)\nKlaplace_rawcells = kernelMatrix(laplacedot1, moignard_raw)\nKGauss_rawcells   = kernelMatrix(rbfdot1, moignard_raw)__\nUse kernelized distances to protect against outliers and allows discovery of non-linear components.\ndist1kr = 1 - Klaplace_rawcells\ndist2kr = 1 - KGauss_rawcells\ndist1kn = 1 - Klaplace_cellsn\ndist2kn = 1 - KGauss_cellsn\n\ncells1.kcmds = cmdscale(dist1kr, k = 20, eig = TRUE) \ncells2.kcmds = cmdscale(dist2kr, k = 20, eig = TRUE) \n\npercentage = function(x, n = 4) round(100 * sum(x[seq_len(n)]) / sum(x[x&gt;0]))\nkperc1 = percentage(cells1.kcmds$eig)\nkperc2 = percentage(cells2.kcmds$eig)\n\ncellsn1.kcmds = cmdscale(dist1kn, k = 20, eig = TRUE) \ncellsn2.kcmds = cmdscale(dist2kn, k = 20, eig = TRUE)__\n\nusing a 3d scatterplot interactively:\n\ncolc = rowData(Moignard)$cellcol\nlibrary(\"scatterplot3d\")\nscatterplot3d(cellsn2.kcmds$points[, 1:3], color=colc, pch = 20,\n   xlab = \"Axis k1\", ylab = \"Axis k2\", zlab = \"Axis k3\", angle=15)\nscatterplot3d(cellsn2.kcmds$points[, 1:3], color=colc, pch = 20,\n   xlab = \"Axis k1\", ylab = \"Axis k2\", zlab = \"Axis k3\", angle = -70)__\n\n\n\\(\\)\n\n\n\n\\(\\)\n\nFigure 9.38: Kernel multidimensional scaling.\n__\nExercise 9.7\nHigher resolution study of cell data.\nTake the original expression data blom we generated in Section 9.5.1. Map the intensity of expression of each of the top 10 most variable genes onto the 3d plot made with the diffusion mapping. Which dimension, or which one of the principal coordinates (1,2,3,4) can be seen as the one that clusters the 4SG (red) points the most?\n__\nSolution\n__\nlibrary(\"rgl\")\nplot3d(cellsn2.kcmds$points[, 1:3], col = colc, size = 3,\n       xlab = \"Axis1\", ylab = \"Axis2\", zlab = \"Axis3\")\nplot3d(cellsn2.kcmds$points[, c(1,2,4)], col = colc, size = 3,\n       xlab = \"Axis1\", ylab = \"Axis2\", zlab = \"Axis4\")\n# Using an L1 distance instead.\nplot3d(cellsn1.kcmds$points[, 1:3], col = colc, size = 3,\n       xlab = \"Axis1\", ylab = \"Axis2\", zlab = \"Axis3\")\nplot3d(cellsn1.kcmds$points[, c(1,2,4)], col = colc, size = 3,\n       xlab = \"Axis1\", ylab = \"Axis2\", zlab = \"Axis4\")__\nAn implementation in the package LPCM provides the function lpc, which estimates principal curves. Here we constrain ourselves to three dimensions chosen from the output of the diffusion map and create smoothed curves.\nlibrary(\"LPCM\")\nlibrary(\"diffusionMap\")\ndmap1 = diffuse(dist1n.l1, neigen = 10)__\n\n\nPerforming eigendecomposition\nComputing Diffusion Coordinates\nElapsed time: 5.014 seconds\n\n\ncombs = combn(4, 3)\nlpcplots = apply(combs, 2, function(j) lpc(dmap1$X[, j], scale = FALSE))__\nTo get a feel for what the smoothed data are showing us, we take a look at the interactive graphics using the function plot3d from them rgl package.\nlibrary(\"rgl\")\nfor (i in seq_along(lpcplots))\n  plot(lpcplots[[i]], type = \"l\", lwd = 3,\n  xlab = paste(\"Axis\", combs[1, i]),\n  ylab = paste(\"Axis\", combs[2, i]),\n  zlab = paste(\"Axis\", combs[3, i]))__\nOne way of plotting both the smoothed line and the data points is to add the line using the plot3d function.\noutlpce134 = lpc(dmap1$X[,c(1,3,4)], scale=FALSE, h=0.5)\nplot3d(dmap1$X[,c(1,3,4)], col=colc, pch=20, \n       xlab=\"Axis1\", ylab=\"Axis3\", zlab=\"Axis4\")\nplot3d(outlpce134$LPC, type=\"l\", lwd=7, add=TRUE)\n\noutlpce134 = lpc(dmap1$X[,c(1,3,4)], scale=FALSE, h=0.7)\nplot3d(outlpce134$LPC, type=\"l\", lwd=7,\n       xlab=\"Axis1\", ylab=\"Axis3\", zlab=\"Axis4\")\nplot3d(dmap1$X[,c(1,3,4)], col=colc, \n       xlab=\"\", ylab=\"\", zlab=\"\", add=TRUE)__\n\nFigure 9.39: Diffusion map projection for Axes 1, 3 and 4. The lower figure shows the smoothed path followed by the cells in their development.\n\nFigure 9.40: Diffusion map projection for Axes 1, 3 and 4. The lower figure shows the smoothed path followed by the cells in their development.\n__\nExercise 9.8\nHere we explore more refined distances and diffusion maps that can show cell development trajectories as in Figure 9.41.\nThe diffusion map method restricts the estimation of distances to local points, thus further pursuing the idea that often only local distances should be represented precisely and as points become further apart they are not being measured with the same ‘reference’. This method also uses the distances as input but then creates local probabilistic transitions as indicators of similarity, these are combined into an affinity matrix for which the eigenvalues and eigenvectors are also computed much like in standard MDS.\nCompare the output of the diffuse function from the diffusionMap package on both the l1 and l2 distances computed between the cells available in the dist2n.euclid and dist1n.l1 objects from Section 9.5.1.\n\nFigure 9.41: Ouput from a three-dimensional diffusion map projection.\n__\nSolution\n__\nlibrary(\"diffusionMap\")\ndmap2 = diffuse(dist2n.euclid, neigen = 11)__\n\n\nPerforming eigendecomposition\nComputing Diffusion Coordinates\nElapsed time: 7.243 seconds\n\n\ndmap1 = diffuse(dist1n.l1, neigen = 11)__\n\n\nPerforming eigendecomposition\nComputing Diffusion Coordinates\nElapsed time: 4.849 seconds\n\n\nplot(dmap2)__\nNotice that the vanilla plot for a dmap object does not allow the use of colors. As this essential to our understanding of cell development, we add the colors by hand. Of course, here we use static 3d plots but these should supplemented by the plot3d examples we give in the code.\nWe use a tailored wrapper function scp3d, so that we can easily insert relevant parameters:\nlibrary(\"scatterplot3d\")\nscp3d = function(axestop = 1:3, dmapRes = dmap1, color = colc,\n           anglea = 20, pch = 20)\nscatterplot3d(dmapRes$X[, axestop], color = colc,\n    xlab = paste(\"Axis\",axestop[1]), ylab = paste(\"Axis\", axestop[2]),\n    zlab = paste(\"Axis\",axestop[3]), pch = pch, angle = anglea)__\n\n\nscp3d()\nscp3d(anglea=310)\nscp3d(anglea=210)\nscp3d(anglea=150)__\nThe best way of visualizing the data is to make a rotatable interactive plot using the rgl package.\n# interactive plot\nlibrary(\"rgl\")\nplot3d(dmap1$X[,1:3], col=colc, size=3)\nplot3d(dmap1$X[,2:4], col=colc, size=3)__\nBelkin, Mikhail, and Partha Niyogi. 2003. “Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.” Neural Computation 15 (6): 1373–96.\nBengio, Yoshua, Jean-François Paiement, Pascal Vincent, Olivier Delalleau, Nicolas Le Roux, and Marie Ouimet. 2004. “Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering.” Advances in Neural Information Processing Systems 16: 177–84.\nBraak, Cajo ter. 1985. “Correspondence Analysis of Incidence and Abundance Data: Properties in Terms of a Unimodal Respose.” Biometrics 41 (January).\nBrodie, Eoin L, Todd Z DeSantis, Dominique C Joyner, Seung M Baek, Joern T Larsen, Gary L Andersen, Terry C Hazen, et al. 2006. “Application of a High- Density Oligonucleotide Microarray Approach to Study Bacterial Population Dynamics During Uranium Reduction and Reoxidation.” Applied and Environmental Microbiology 72 (9): 6288–98.\nCallahan, Ben J, Kris Sankaran, Julia A Fukuyama, Paul J McMurdie, and Susan P Holmes. 2016. “Bioconductor Workflow for Microbiome Data Analysis: From Raw Reads to Community Analyses.” F1000Research 5.\nChessel, Daniel, Anne Dufour, and Jean Thioulouse. 2004. “The ade4 Package - i: One-Table Methods.” R News 4 (1): 5–10. http://CRAN.R-project.org/doc/Rnews/.\nDiaconis, Persi, Sharad Goel, and Susan Holmes. 2008. “Horseshoes in Multidimensional Scaling and Kernel Methods.” Annals of Applied Statistics 2: 777. https://doi.org/DOI:10.1214/08-AOAS165.\nEkman, Gosta. 1954. “Dimensions of Color Vision.” The Journal of Psychology 38 (2): 467–74.\nGoslee, Sarah C, Dean L Urban, et al. 2007. “The Ecodist Package for Dissimilarity-Based Analysis of Ecological Data.” Journal of Statistical Software 22 (7): 1–19.\nGreenacre, Michael J. 2007. Correspondence Analysis in Practice. Chapman & Hall.\nGuillot, Gilles, and François Rousset. 2013. “Dismantling the Mantel Tests.” Methods in Ecology and Evolution 4 (4): 336–44.\nHastie, Trevor, and Werner Stuetzle. 1989. “Principal Curves.” Journal of the American Statistical Association 84 (406): 502–16.\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2008. The Elements of Statistical Learning. 2^{} ed. Springer.\nHolmes, Susan. 2006. “Multivariate Analysis: The French way.” In Probability and Statistics: Essays in Honor of David a. Freedman , edited by D. Nolan and T. P. Speed. Vol. 56. IMS Lecture Notes–Monograph Series. Beachwood, OH: IMS. http://www.imstat.org/publications/lecnotes.htm.\nHolmes, Susan, Alexander V Alekseyenko, Alden Timme, Tyrrell Nelson, Pankaj Jay Pasricha, and Alfred Spormann. 2011. “Visualization and Statistical Comparisons of Microbial Communities Using r Packages on Phylochip Data.” In Pacific Symposium on Biocomputing , 142–53. World Scientific.\nIzenman, Alan Julian. 2008. “Nonlinear Dimensionality Reduction and Manifold Learning.” In Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning , 597–632. New York, NY: Springer New York.\nJosse, Julie, and Susan Holmes. 2016. “Measuring Multivariate Association and Beyond.” Statistics Surveys 10: 132–67.\nKashyap, Purna C, Angela Marcobal, Luke K Ursell, Samuel A Smits, Erica D Sonnenburg, Elizabeth K Costello, Steven K Higginbottom, et al. 2013. “Genetically Dictated Change in Host Mucus Carbohydrate Landscape Exerts a Diet-Dependent Effect on the Gut Microbiota.” PNAS 110 (42): 17059–64.\nKendall, David. 1969. “Incidence Matrices, Interval Graphs and Seriation in Archeology.” Pacific Journal of Mathematics 28 (3): 565–70.\nLeek, Jeffrey T, Robert B Scharpf, Héctor Corrada Bravo, David Simcha, Benjamin Langmead, W Evan Johnson, Donald Geman, Keith Baggerly, and Rafael A Irizarry. 2010. “Tackling the Widespread and Critical Impact of Batch Effects in High-Throughput Data.” Nature Reviews Genetics 11 (10): 733–39.\nMoignard, Victoria, Steven Woodhouse, Laleh Haghverdi, Andrew J Lilly, Yosuke Tanaka, Adam C Wilkinson, Florian Buettner, et al. 2015. “Decoding the Regulatory Network of Early Blood Development from Single-Cell Gene Expression Measurements.” Nature Biotechnology.\nNelson, Tyrell A, Susan Holmes, Alexander Alekseyenko, Masha Shenoy, Todd DeSantis, Cindy Wu, Gary Andersen, et al. 2010. “PhyloChip Microarray Analysis Reveals Altered Gastrointestinal Microbial Communities in a Rat Model of Colonic Hypersensitivity.” Neurogastroenterology & Motility.\nPagès, Jérôme. 2016. Multiple Factor Analysis by Example Using R. CRC Press.\nPerraudeau, Fanny, Davide Risso, Kelly Street, Elizabeth Purdom, and Sandrine Dudoit. 2017. “Bioconductor Workflow for Single-Cell RNA Sequencing: Normalization, Dimensionality Reduction, Clustering, and Lineage Inference.” F1000Research 6.\nPrentice, IC. 1977. “Non-Metric Ordination Methods in Ecology.” The Journal of Ecology , 85–94.\nRhee, Soo-Yon, Matthew J Gonzales, Rami Kantor, Bradley J Betts, Jaideep Ravela, and Robert W Shafer. 2003. “Human Immunodeficiency Virus Reverse Transcriptase and Protease Sequence Database.” Nucleic Acids Research 31 (1): 298–303.\nRoweis, Sam T, and Lawrence K Saul. 2000. “Nonlinear Dimensionality Reduction by Locally Linear Embedding.” Science 290 (5500): 2323–26.\nTenenbaum, Joshua B, Vin De Silva, and John C Langford. 2000. “A Global Geometric Framework for Nonlinear Dimensionality Reduction.” Science 290 (5500): 2319–23.\nTrosset, Michael W, and Carey E Priebe. 2008. “The Out-of-Sample Problem for Classical Multidimensional Scaling.” Computational Statistics & Data Analysis 52 (10): 4635–42.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "10-chap.html",
    "href": "10-chap.html",
    "title": "12  10.1 Goals for this chapter",
    "section": "",
    "text": "12.1 10.2 Graphs\nNetworks and trees are often used to represent knowledge about a biological system. They can also be used to directly encode observations from an experiment or study. Phylogenetic trees were drawn to represent family and similarity relationships between species, even before Darwin’s famous notebook sketch that gave these trees a mechanistic, causal interpretation. The meaning of the nodes and edges in a network can differ, and needs to be specified. For instance, a network might schematize relationships between proteins, such as in Figure 10.1, where the nodes could stand for genes or their encoded proteins, and edges could be direct physical interactions or more abstract “functional” or “genetic” interactions representing outcomes from an experiment. In this book, we use the terms graph and network largely exchangeably. The former term is a bit more evocative of the mathematical structure, the latter of the biological interpretation.\nFigure 10.1: A small protein-protein network that represents pairwise relationships between proteins.\nWe saw in Chapter 2 that we could model sequences of state transitions as a Markov chain, which can be represented as directed graphs with weights on the edges. Metabolic pathways in which nodes are chemical metabolites and the edges represent chemical reactions. Mutation history trees are used in cancer genomics to represent lineages of mutations.\nTransmission networks are important in studying the epidemiology of infectious diseases. As real networks can be very large, we will need special methods for representing and visualizing them. This chapter will be focused on ways of integrating graphs into a data analytic workflow.\nIn this chapter we will",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "10-chap.html#graphs",
    "href": "10-chap.html#graphs",
    "title": "12  10.1 Goals for this chapter",
    "section": "",
    "text": "12.1.1 10.2.1 What is a graph and how can it be encoded?\nA graph is defined as a combination of two sets, often denoted as \\((V,E)\\). Here, \\(V\\) is a set of nodes or vertices , \\(E\\) is a set of edges between vertices. Each element of \\(E\\) is a pair of nodes, i.e., consists of two elements of \\(V\\). An intuitive way to represent a graph is by its from-to representation. If we denote the set of vertices as \\(V=(, , , )\\), then the from- to (or edge list) representation is a table of the following form.\n  from to\n1    A  B\n2    B  C\n3    A  E\n4    C  D\n5    E  F\nThe ordering of the rows in the from-to table plays no role. In a directed or oriented graph , the edges are ordered pairs, i.e., the first line in the above table states that there is an edge from A to B, but does not say whether there is also an edge from B to A—this would need to be denoted in a separate row of the table.\nIn an undirected graph , the edges are unordered pairs, i.e., an edge from A to B is not distinguished from an edge from B to A. Undirected graphs encode symmetric relationships between the nodes, directed graphs represent asymmetric relationships.\nIt’s important not to confuse a graph with its visualization. It is possible to draw a graph onto a two-dimensional area like in Figure 10.1, but this is optional, and not unique—there are always many different ways to draw the same graph. There is also no guarantee that in such a visualization, edges do not overlap. Depending on the graph, this can happen. Graphs do not live in physical space (neither 2D nor 3D) but are literally just sets of nodes and edges.\nAn alternative representation, equivalent to the from-to table, is the adjacency matrix , a quadratic matrix with as many rows (and columns) as nodes in the graph. The matrix contains a non-zero entry in the \\(i\\)th row and \\(j\\)th column to encode that there is an edge between the \\(i\\)th and \\(j\\)th vertices.\n__\nQuestion 10.1\nFor undirected graphs, what is special about the adjacency matrix \\(A\\)?\n__\nSolution\n__\nThe adjacency matrix is symmetric, i.e., \\(M = M^T\\). An example is shown in Figures 10.2 and 10.3. g1 is created from a from-to table (encoded in the two-column matrix edges) by the code below.\nlibrary(\"igraph\")\nedges = matrix(c(1,3, 2,3, 3,4, 4,5, 4,6), byrow = TRUE, ncol = 2)\ng1 = graph_from_edgelist(edges, directed = FALSE)\nvertex_attr(g1, name = \"name\") = 1:6\nplot(g1, vertex.size = 25, edge.width = 5, vertex.color = \"coral\")__\n\nFigure 10.2: A small undirected graph with numbered nodes.\n\nFigure 10.3: The adjacency matrix of the graph shown in Figure 10.2 is is a symmetric \\(n n\\) matrix of \\(0\\)s and \\(1\\)s, where is \\(n\\) is the number of nodes.\n__\nQuestion 10.2\nCan you give an alternative way to that shown in the answer to Question 10.1 above for creating a graph from a list of edges dataframe?\n__\nSolution\n__\nedges = \"1,3\\n2,3\\n3,4\\n4,6\\n4,5\"\ndf = read.csv(textConnection(edges), header = FALSE)\nsg = graph_from_data_frame(df, directed = FALSE)\nsg __\n\n\nIGRAPH 1467322 UN-- 6 5 -- \n+ attr: name (v/c)\n+ edges from 1467322 (vertex names):\n[1] 1--3 2--3 3--4 4--6 4--5\nTypically, you would use the read.csv function to read the from-to table from a separate file. Here the authors of the book decided to create the character string edges on the fly and to turn into a the equivalent of a file using the function textConnection, to keep the example more compact. The notation \"\\n\" indicates a line break.\n\n12.1.1.1 Elements of a simple graph\n\nThe nodes or vertices. These are the colored circles with numbers in them in Figure 10.2.\nEdges or connections, the segments that join the nodes and which can be directed or not.\nEdge attributes, such edge length. When not specified, we suppose the edge lengths are all the same, typically, one. For instance, to compute the distance between two nodes in the graph, we sum up the lengths of the edges along the shortest path.\nEdge and node attributes: optionally, each edge or each node can be mapped to further continuous or categorical variables, such as type, color, weight, edge width, node size, \\(…\\) Pretty much anything is possible here, depending on the application and the intended computations.\n\nWe also call a directed graph with edge lengths a network. The adjacency matric of a network is an \\(nn\\) matrix of positive numbers corresponding to the edge lengths.\n\n\n12.1.1.2 Basic concepts\nThe degree of a node is the number of edges connected to it. In directed graphs we differentiate between in-degree and out-degree for incoming and outgoing edges. We may further distinguish between directed graphs that contain cycles and those that do not (termed cyclic and acyclic graphs).\nFor large graphs, on can summarize overall graph structure by looking at the distributions of vertex degrees, and we can identify particularly interesting regions or specific nodes and edge in a graph with measures such as centrality and betweenness. These measures are available in various packages (network , igraph).\nIf the number of edges is of the same order of magnitude as the number of nodes (written \\(\\#EO(\\#V)\\)), we say that the graph is sparse. Some graphs have many nodes, for instance, the package ppiData contains a predicted protein interaction (ppipred) graph on about 2500 proteins with around 20000 edges1. A complete adjacency matrix for such a graph requires more than 6 million memory units, of which most contain 0. This is needlessly wasteful. The edge list representation of the same graph is more compact: it only uses storage where there is an edge, in our example, this amounts to 20000 memory units. One particular choice of edge list representation is sparse matrix encodings, such as implemented in the package Matrix.\n1 Gene and species phylogenies may even be much larger.\nOn the other hand, in a dense graph, the number of edges is of the same order of magnitude as the number of potential edges, i.e., the square of the number of nodes (written \\(\\#EO(\\#V^2)\\). Memory space can be an issue for the storage of large dense graphs.\n\n\n12.1.1.3 Graph layout\nWe will see several examples where the same graph is plotted in different ways, either for aesthetic or practical reasons. This is done through the choice of the graph layout.\nWhen the edges have lengths representing distances the problem of a 2D representation of the graph is the same as the multidimensional scaling we saw in Chapter 9 It is often solved in a similar way by spreading out the vertex-points as much as possible. In the simple case of edges without lengths, the algorithms can choose different criteria. The method of Fruchterman and Reingold is a basic choice. It is based on a physics-inspired model where similar points attract and repel each other as if under the effect of (Newtonian) physical forces.\n__\nTask\nUse the igraph package to do the following\n\nCreate a dense random graph with 12 nodes and more than 50 edges.\nExperiment plotting the graph with different layouts: place the nodes on a circle, or represent the graph as symmetrically as possible, avoid any overlapping nodes or edges.\n\n\n\n12.1.1.4 Graphs from data\nUsually data do not arrive in the form of graphs. Graphical or network representations are often the result of transforming from other data types.\nFrom distances or similarities: graphs can simplify distance or similarity relationships between objects by binarising them. Nodes are connected if they are similar or close, and not connected if not. Thus, the input is a similarity or distance measure between all pairs of objects of interest (genes, proteins, species, phenotypes, \\(…\\)), to which a threshold is applied. The set of measures could be realized in a dense matrix or be computed on the fly.\nBipartite graphs: some data arrive naturally as absence or presence relationships between two types of objects, for instance, finch species and islands in the Galapagos archipelago (Figure 10.4), or transcription factors and gene regulatory regions they are considered to bind to. Such relationships can be encoded with 0/1 values in a rectangular matrix, where rows represent one object type and columns the other. The resulting graph has two types of nodes, e.g., finch-nodes and island-nodes, and edges can only exist between nodes of different types, e.g., between a taxon and an island, but not between taxa, nor between islands. Edges in Figure 10.4 represent lives on relationships.\n\nFigure 10.4: This bipartite graph connects each taxon to the sites where it was observed.\n__\nQuestion 10.3\nLoad the finch.csv data and experiment plotting them to highlight that they represent a bipartite network.\n__\nSolution\n__\nThe output of the following code is shown in Figure 10.5.\nfinch = readr::read_csv(\"../data/finch.csv\", comment = \"#\", col_types = \"cc\")\nfinch __\n\n\n# A tibble: 122 × 2\n   .tail .head             \n   &lt;chr&gt; &lt;chr&gt;             \n 1 C     Large ground finch\n 2 D     Large ground finch\n 3 E     Large ground finch\n 4 F     Large ground finch\n 5 G     Large ground finch\n 6 H     Large ground finch\n 7 I     Large ground finch\n 8 J     Large ground finch\n 9 L     Large ground finch\n10 M     Large ground finch\n# ℹ 112 more rows\n\n\nlibrary(\"network\")\nfinch.nw  = as.network(finch, bipartite = TRUE, directed = FALSE)\nis.island = nchar(network.vertex.names(finch.nw)) == 1\nplot(finch.nw, vertex.cex = 2.5, displaylabels = TRUE, \n     vertex.col = ifelse(is.island, \"forestgreen\", \"gold3\"),\n     label= sub(\" finch\", \"\", network.vertex.names(finch.nw)))\nfinch.nw |&gt; as.matrix() |&gt; t() |&gt; (\\(x) x[, order(colnames(x))])()__\n\n\n                          A B C D E F G H I J K L M N O P Q\nLarge ground finch        0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\nMedium ground finch       1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0\nSmall ground finch        1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0\nSharp-beaked ground finch 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1\nCactus ground finch       1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0\nLarge cactus ground finch 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\nLarge tree finch          0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0\nMedium tree finch         0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\nSmall tree finch          0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0\nVegetarian finch          0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0\nWoodpecker finch          0 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0\nMangrove finch            0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\nWarbler finch             1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\nFigure 10.5: The finches graph. There are many ways to improve the layout, including better taking into account the bipartite nature of the graph.\n__\nQuestion 10.4\nMake a plot of the graph g1 using the ggraph package, with a choice of layout and provided geoms such as geom_edge_link, geom_node_point and geom_node_text.\n__\nSolution\n__\nThe output of the following code is shown in Figure 10.6.\nlibrary(\"ggraph\")\nggraph(g1, layout = \"nicely\") + \n  geom_edge_link() + \n  geom_node_point(size=6,color=\"#8856a7\") + \n  geom_node_text(label=vertex_attr(g1)$name,  color=\"white\")__\n\nFigure 10.6: A ggraph example.\n\n\n12.1.1.5 An example: a four state Markov chain\nIn Chapter 2, we saw how a Markov chain can summarize transitions between nucleotides (considered the states of the system). This is often schematized by a graph. The igraph package provides many choices for graph “decoration”:\nlibrary(\"markovchain\")\nstatesNames = c(\"A\", \"C\", \"G\",\"T\")\nT1MC = new(\"markovchain\", states = statesNames, transitionMatrix =\n  matrix(c(0.2,0.1,0.4,0.3,0,1,0,0,0.1,0.2,0.2,0.5,0.1,0.1,0.8,0.0),\n         nrow = 4,byrow = TRUE, dimnames = list(statesNames, statesNames)))\nplot(T1MC, edge.arrow.size = 0.4, vertex.color = \"purple\",\n     edge.arrow.width = 2.2, edge.width = 5, edge.color = \"blue\",\n     edge.curved = TRUE, edge.label.cex = 2.5, vertex.size= 32,\n     vertex.label.cex = 3.5, edge.loop.angle = 3,\n     vertex.label.family = \"sans\", vertex.label.color = \"white\")__\n\nFigure 10.7: A four state Markov chain with arrows representing possible transitions between states.\nMarkov chains are simple models of dynamical systems, and the states are represented by the nodes in the graph. The transition matrix gives us the weights on the directed edges (arrows) between the states.\n__\nQuestion 10.5\nWhich state do you think this Markov chain will end up in?\n__\nSolution\n__\nAll nodes except C have outgoing edges, C only has in-going edges and a self- edge. Thus, C is an absorbing state. Sooner or later any process from this chain ends up and stays in C.\n__\nTask\n\nTry changing your set.seed function input and see if it changes the plot.\nAccess the help for this particular plot function.\nRedo the graph and label the edges with the transition probabilities in green and vertices in brown.\n\nWe will see how to build a complete example of annotated state space Markov chain graph in Exercise 10.3.\n\n\n\n12.1.2 10.2.2 Graphs with many layers: labels on edges and nodes\nHere is an example of plotting a graph downloaded from the STRING database with annotations at the vertices.\ndatf = read.table(\"../data/string_graph.txt\", header = TRUE)\ngrs = graph_from_data_frame(datf[, c(\"node1\", \"node2\")], directed = FALSE)\nE(grs)$weight = 1\nV(grs)$size = centr_degree(grs)$res\nggraph(grs) +\n  geom_edge_arc(color = \"black\",  strength = 0.05, alpha = 0.8)+\n  geom_node_point(size = 2.5, alpha = 0.5, color = \"orange\") +\n  geom_node_label(aes(label=vertex_attr(grs)$name), size = 3, alpha = 0.9, color = \"#8856a7\", repel = TRUE) __\n\nFigure 10.8: Perturbed chemokine subnetwork uncovered in Yu et al. (2012) using differential gene expression patterns in sorted T-cells. Notice the clique-like structure of the genes CXCR3, CXCL13, CCL19, CSCR5 and CCR7 in the right hand corner.\nFigure 10.8 shows the full perturbed chemokine subnetwork discovered in the study of breast cancer metastasis using GXNA (Nacu et al. 2007) and reported by Yu et al. (2012).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "10-chap.html#from-gene-set-enrichment-to-networks",
    "href": "10-chap.html#from-gene-set-enrichment-to-networks",
    "title": "12  10.1 Goals for this chapter",
    "section": "12.2 10.3 From gene set enrichment to networks",
    "text": "12.2 10.3 From gene set enrichment to networks\n\n\n\nA long unstructured laundry list of possibly differentially expressed genes can be daunting.\n\n\nA long unstructured laundry list of possibly differentially expressed genes can be daunting.\nIn Chapter 8, we studied methods for finding a list of differentially expressed genes. Small sample sizes, coupled with efforts to maintain low FDRs, often result in low power to detect differential expression. Therefore, obtaining a long list of genes that can be confidently declared as differentially expressed is, initially, a triumph. However, understanding the underlying biology requires more than just a laundry list of significant players in a biological system.\n\n12.2.1 10.3.1 Methods using pre-defined gene sets (GSEA)\nOne of the earliest approaches was to look for gene attributes that are overrepresented or enriched in the laundry list of significant genes. These gene classes are often based on Gene Ontology (GO) categories (for example, genes that are involved in organ growth, or genes that are involved in feeding behavior). The Gene Ontology (GO) is a collection of three ontologies that describe genes and gene products. These ontologies are restricted vocabularies that have the structure of directed acyclic graphs (DAGS). The most specific terms are the leaves of the graph. The GO graph consists of nodes (here, Gene Ontology terms) and edges from more specific terms (children) to less specific (parents), often these edges are directed. Nodes and edges can have multiple attributes that can be visualized. The main purpose of using GO annotations for a particular set of Genes designated as significant in an experiment is to look for the enrichment of a GO term in this list, we will give this term a statistical meaning below. Many other useful lists of important gene sets exist.\n__\nTask\nFind a useful database of gene sets.\nFor instance, the MsigDB Molecular Signature Database (Liberzon et al. 2011) contains many gene sets that can be accessed from within R using the function getBroadSets from the Bioconductor package GSEABase roughly as follows:\nlibrary(\"GSEABase\")\n## This requires a login to the website.\nfl   =  \"/path/to/msigdb_v5.1.xml\"\ngss  =  getBroadSets(fl) \norganism(gss[[1]])\ntable(sapply(gss, organism))__\n\n\n12.2.2 10.3.2 Gene set analysis with two-way table tests\nTable 10.1: Although there are the same number of each category of gene found in the significant set, both the simulation below and the theory of testing in two-way tables shows us that the blue category is enriched.\n\n\n\nYellow\nBlue\nRed\n\n\n\n\n\nSignificant\n25\n25\n25\n\n\nUniverse\n500\n100\n400\n\n\n\nHere, we start by explaining a basic approach often called Fisher’s “exact” test or hypergeometric test ing.\n\n\n\nSo-called ‘exact’ tests because they are nonparametric and based on exhaustive enumerations: not because we are sure of the answer – this is statistics after all.\n\n\nSo-called ‘exact’ tests because they are nonparametric and based on exhaustive enumerations: not because we are sure of the answer – this is statistics after all.\nDefine a universe of candidate genes that may potentially be significant; say this universe is of size \\(N\\). We also have a record of the genes that actually did come out significant, of which we suppose there were \\(m\\).\nWe make a toy model involving balls in boxes, with a total of \\(N\\) balls corresponding to the genes identified in the gene universe. These genes are split into different functional categories, suppose there are \\(N=1,000\\) genes, of which 500 are yellow, 100 are blue and 400 are red. Then a subset of \\(m=75\\) genes are labeled as significant , suppose among these significantly interesting genes, there are 25 yellow, 25 red and 25 blue. Is the blue category enriched or overrepresented?\nWe use this hypergeometric two-way table testing to account for the fact that some categories are extremely numerous and others are rarer.\n__\nQuestion 10.6\nRun a Monte Carlo experiment with 20,000 simulations and compute the p-value of significance of having 25 blues under the null hypothesis that no category is over-represented in the significant set.\n__\nSolution\n__\nUnder the null the 75 are sampled randomly from our unequal boxes as follows:\nuniverse = c(rep(\"Yellow\", 500), rep(\"Blue\", 100), rep(\"Red\", 400))\ncountblue = replicate(20000, {\n  pick75 = sample(universe, 75, replace = FALSE)\n  sum(pick75 == \"Blue\")\n})\nsummary(countblue)__\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   6.000   7.000   7.496   9.000  20.000 \nThe histogram in Figure 10.9 shows that having a value as large as 25 under the null model would be extremely rare.\n\nFigure 10.9: We can see that even in 20000 simulations, no blue count comes close to being 25. We can reject such an event as having happened by chance and conclude that the blue are enriched.\nIn the general case, the gene universe is an urn with \\(N\\) balls, if we pick the \\(m\\) balls at random and there is a proportion of \\(k/N\\) blue balls, we expect to see \\(km/N\\) blue balls in a draw of size \\(k\\).\n\n12.2.2.1 Plotting gene enrichment networks with\nGOplot\nHere we show an attractive way of summarizing the connections between the gene functional categories and the significant gene set.\nlibrary(\"GOplot\")\ndata(\"EC\")\ncirc  =  circle_dat(EC$david, EC$genelist)\nchord =  chord_dat(circ, EC$genes, EC$process)\nGOChord(chord, limit = c(0, 5))__\n\nFigure 10.10: This graph shows the correspondence between GO terms and significantly changed genes in a study on differential expression in endothelial cells from two steady state tissues (brain and heart, see Nolan et al. (2013)). After normalization a differential expression analysis was performed giving a list of genes. A gene-annotation enrichment analysis of the set of differentially expressed genes (adjusted p-value &lt; 0.05) was then performed with the GOplot package.\nIn fact, the Gene Ontology graph does not necessarily capture meaningful gene interactions as genes from different processes often interact productively. A large amount of information remains unused, for example, all significant genes are usually given equal weight, despite the potentially large variations in their p-values.\n\n\n\n12.2.3 10.3.3 Significant subgraphs and high scoring modules\nWe have at our disposal more than just the Gene Ontology. There are many different databases of gene networks from which we can choose a known skeleton graph onto which we project significance scores such as p-values from our differential expression experiment. We will follow an idea first suggested by Ideker et al. (2002). This is further developed in Nacu et al. (2007). A careful implementation with many improvements is available as the Bioconductor package BioNet (Beisser et al. 2010). These methods all search for the subgraphs or modules of a scored-skeleton network that seem to be particularly perturbed.\nEach gene-node in the network is assigned a score that can either be calculated from a t-statistic or a p-value. Often pathways contain both upregulated and downregulated genes; as pointed out in Ideker et al. (2002), this can be captured by taking absolute values of the test statistic or just incorporating scores computed from the p-values2. Beisser et al. (2010) model the p-values of the genes as we did in Chapter 6: mixture of non- perturbed genes whose p-values will be uniformly distributed and non uniformly distributed p-values from the perturbed genes. We model the signal in the data using a beta distribution for the p-values following Pounds and Morris (2003).\n2 We’ll want something like \\(-p\\), so that small p-values give large scores.\nGiven our node-scoring function, we search for connected hotspots in the graph, i.e., a subgraph of genes with high combined scores.\n\n12.2.3.1 Using a subgraph search algorithm\nFinding the maximal scoring subgraph of a generic graph is known to be intractable in general (we say it is an NP-hard problem), so various approximate algorithms have been proposed. Ideker et al. (2002) suggested using simulated annealing, however this is slow and tends to produce large subgraphs that are difficult to interpret. Nacu et al. (2007) started with a seed vertex and gradually expand around it. Beisser et al. (2010) started the search with a so-called minimal spanning tree (MST), a graph we we will study later in this chapter.\n\n\n\n12.2.4 10.3.4 An example with the BioNet implementation\nTo illustrate the method, we show data from the BioNet package.\nThe interactome data contains a connected component of the network comprising 2034 different gene products and 8399 interactions. This constitutes the skeleton graph with which we will work, see Beisser et al. (2010).\nThe dataLym contains the relevant pvalues and \\(t\\) statistics for 3,583 genes, you can access them and do the analysis as follows:\nlibrary(\"BioNet\")\nlibrary(\"DLBCL\")\ndata(\"dataLym\")\ndata(\"interactome\")\ninteractome __\n\n\nA graphNEL graph with undirected edges\nNumber of Nodes = 9386 \nNumber of Edges = 36504 \n\n\npval = dataLym$t.pval\nnames(pval)  =  dataLym$label\nsubnet = subNetwork(dataLym$label, interactome)\nsubnet = rmSelfLoops(subnet)\nsubnet __\n\n\nA graphNEL graph with undirected edges\nNumber of Nodes = 2559 \nNumber of Edges = 7788 \n\n12.2.4.1 Fit a Beta-Uniform model\nThe p-values are fit to the type of mixture we studied in Chapter 4, with a uniform component from the null with probability \\(_0\\) and a beta distribution (proportional to \\(a x^{a - 1}\\)) for the p-values corresponding to the alternatives (Pounds and Morris 2003). \\[f(x|a,_0)= _0 + (1-_0) a x^{a - 1} 0 &lt;x ; ; 0&lt;a&lt;1\\] Running the model with an \\[fdr\\] of 0.001:\nThe package actually gives a different name to \\(\\_0\\): it uses \\(\\\\) and calls it the mixing parameter.\nfb = fitBumModel(pval, plot = FALSE)\nfb __\n\n\nBeta-Uniform-Mixture (BUM) model\n\n3583 pvalues fitted\n\nMixture parameter (lambda): 0.482\nshape parameter (a):        0.180\nlog-likelihood:         4471.8\n\n\nscores=scoreNodes(subnet, fb, fdr = 0.001)__\n\nFigure 10.11: The qqplot shows the quality of the fit of beta-uniform mixture model to the data. The red points have the theoretical quantiles from the beta distribution as the x coordinates the observed quantiles and the y coordinates. The blue line shows that this model fits nicely.\n\nFigure 10.12: A histogram of the mixture components for the p-values, the beta in red and the uniform in blue, \\(_0\\) is the mixing proportion assigned to the null component whose distribution should be uniform.\nThen we run a heuristic search for a high scoring subgraph using:\nhotSub  =  runFastHeinz(subnet, scores)\nhotSub __\n\n\nA graphNEL graph with undirected edges\nNumber of Nodes = 144 \nNumber of Edges = 221 \n\n\nlogFC=dataLym$diff\nnames(logFC)=dataLym$label __\n__\nQuestion 10.7\nWe made Figure 10.13 using the following code:\nplotModule(hotSub, layout = layout.davidson.harel, scores = scores,\n                  diff.expr = logFC)__\n\nFigure 10.13: The subgraph found as maximally enriched for differential expression between ABC and GCB B-cell lymphoma. The nodes are colored in red and green: green shows an upregulation in ACB and red an upregulation in GBC. The shape of the nodes depicts the score: rectangles indicate a negative score, circles a positive score.\nUsing the function igraph.from.graphNEL, transform the module object and plot it using the ggraph method shown in Section 10.2.2.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "10-chap.html#phylogenetic-trees",
    "href": "10-chap.html#phylogenetic-trees",
    "title": "12  10.1 Goals for this chapter",
    "section": "12.3 10.4 Phylogenetic Trees",
    "text": "12.3 10.4 Phylogenetic Trees\n\nFigure 10.14: As mathematical objects, the hierarchical clustering trees (studied in Chapter 5) are the same as phylogenetic trees. They are rooted binary trees with labels at the tips.\nOne really important use of graphs in biology is the construction of phylogenetic trees. Trees are graphs with no cycles (the official word for loops, whether self loops, or ones that go through several vertices). Phylogenetic trees are usually rooted binary trees that only have labels on the leaves corresponding to contemporary3 taxa at the tips. The inner nodes correspond to ancestral sequences which have to be inferred from the contemporaneous data on the tips. Many methods use aligned DNA sequences from the different species or populations to infer or estimate the tree. The tips of the tree are usually called OTU s (Operational Taxonomic Units). The statistical parameter of interest in these analyses is the rooted binary tree with OTU labels on its leaves (see Holmes (1999, 2003b) for details).\n3 Because they are contemporary, the trees are often represented so that the leaves are a ll the same distance from the root.\n\n12.3.0.1 The example of HIV\n\nFigure 10.15: This phylogenetic tree describes the history of different HIV/SIV strains in Africa (Wertheim and Worobey 2009), [Figure from].\nHIV is a virus that protects itself by evolving very fast (within months, several mutations can appear). Its evolution can thus be followed in real time; whereas the evolution of large organisms which has happened over millions of years. HIV trees are built for medical purposes such as the detection and understanding of drug resistance. They are estimated for individual genes. Different genes can show differences in their evolutionary histories and thus produce different gene trees. The phylogenetic tree in Figure 10.15 shows times when the virus switched from monkeys to humans (Wertheim and Worobey 2009).\n\n\n12.3.0.2 Special elements in phylogenies\n\nMost phylogenetic trees are shown rooted, the `root’ is usually found by including an outgroup in the tree tips, as we will see later.\nCharacters that are derived from this common ancestry are called homologous (geneticists doing population studies replace the term homology by identity by descent (IBD)).\nSisters on the tree defined by a common ancestor are called clades or monophyletic groups, they have more than just similarities in common.\n\n\n\n12.3.1 10.4.1 Markovian models for evolution.\nTo infer what happened in the ancestral species from contemporary data collected on the tips of the tree, we have to make assumptions about how substitutions and deletions occur through time. The models we use are all Markovian and said to be time homogeneous: the mutation rate is constant across history.\n\n\n\nThis is called the molecular clock hypothesis, if we do not make this assumption we run into what is known as non-identifiability (ie we can’t tell the difference between the many possible mutational histories given the observed data).\n\n\nThis is called the molecular clock hypothesis, if we do not make this assumption we run into what is known as non-identifiability (ie we can’t tell the difference between the many possible mutational histories given the observed data).\n\n12.3.1.1 Continuous Markov chain and generator matrix\nWe are going to use the Markov chain we saw in Figure 10.7 on the states [A,C,G,T]; however now we consider that the changes of state, i.e. mutations, occur at random times. The gaps between these mutational events will follow an exponential distribution. These continuous time Markov chains have the following properties:\n\nNo Memory. \\(P(Y(u+t)=j;|;Y(t)=i)\\) does not depend on times before \\(t\\).\nTime homogeneity. The probability \\(P(Y(h+t)=j,|,Y(t)=i)\\) does not depend on \\(t\\), but on \\(h\\), the time between the events and on \\(i\\) and \\(j\\).\nLinearity. The instantaneous transition rate is of an approximately linear form\n\n\n\n\nWe use an error term written here as o(h), we read this little o of h, which just means that this error terms grows much slower (i.e., sublinear) than h.\n\n\nWe use an error term written here as \\(o(h)\\), we read this little \\(o\\) of \\(h\\), which just means that this error terms grows much slower (i.e., sublinear) than \\(h\\).\n\\[ \\[\\begin{align} P_{ij}(h)&=q_{ij}h+o(h), \\quad\\text{for }j\\neq i\\\\\\\nP_{ii}(h)&=1-q_i(h)+ o(h), \\qquad\\text{where }q_i=\\sum_{j\\neq i}q_{ij}.\n\\end{align}\\]1-q_i(h)+ o(h), q_i={ji}q{ij}. \\end{align} \\]\n\\(q_{ij}\\) is known as the instantaneous transition rate. These rates define matrices as in Table 10.2.\n\nExponential distribution. Times between changes are supposed to be exponentially distributed.\n\n\n\n\n\n\n\n\n\\(Q = \n\\(Q = \n\n\n\n\n\nTable 10.2: Two examples of rate matrices, on the left:the Jukes-Cantor (JC69) model, on the right is shown the Kimura (K80) two parameter model.\nthe instantaneous change probability matrix called the generator. In the simplest possible model, called a Jukes-Cantor model; all the mutations are equally likely (see the left of Table 10.2). A slightly more flexible model, called the Kimura model is shown on the right in Table 10.2.\n__\nQuestion 10.8\nWhy do we say the Kimura model is more flexible?\n__\nSolution\n__\nThe Jukes-Cantor only has one parameter, that model supposes all transitions and transversions are equally likely. In the Kimura, there is one parameter for transitions and another for the transversions (mutations occurring from purine to pyrimidines, or vice versa).\n![Vocabulary overload here! : Transitions in this context mean mutational changes within the purines (A&lt;-&gt;G]) or within the pyrimidines (C &lt;-&gt; T); whereas when we talked about Markov chains earlier our transition matrix contains all probabilities of any state changes.](imgs/devil.png){fig- align=‘center’ width=123}\nThe most flexible model is called the Generalized Time Reversible (GTR) model; it has 6 free parameters. We are going to show an example of data simulated according to these generative models from a known tree.\n\n\n\n12.3.2 10.4.2 Simulating data and plotting a tree\nSuppose we already know our phylogenetic tree and want to simulate the evolution of the nucleotides down this tree. First we visualize the tree tree1 using ggtree; loading the tree and the relevant packages with:\nlibrary(\"phangorn\")\nlibrary(\"ggtree\")\nload(file.path(DATA,\"tree1.RData\"))__\n__\nTask\nUse the ggtree function to plot tree1; make the tips of the tree green triangles, the ancestral nodes, red circles.\nggtree(tree1, lwd = 2, color = \"darkgreen\", alpha = 0.8, right = TRUE) +\n  geom_tiplab(size = 7, angle = 90, offset = 0.05) +\n  geom_point(aes(shape = isTip, color = isTip), size = 5, alpha = 0.6)__\n\nFigure 10.16: This is the tree we use as our true parameter. We generate nucleotides one at a time from the root and `dropping’ them down the tree. With some probability proportional to the edge lengths, mutations occur down the branches.\nNow we generate some sequences from our tree. Each sequence starts with a new nucleotide letter generated randomly at the root; mutations may occur as we go down the tree. You can see in Figure 10.17 that the colors are not equally represented, because the frequency at the root was chosen to be different from the uniform, see the following code.\nseqs6 = simSeq(tree1, l = 60, type = \"DNA\", bf = c(1, 1, 3, 3)/8, rate = 0.1)\nseqs6 __\n\n\n6 sequences with 60 character and 30 different site patterns.\nThe states are a c g t \n\n\nmat6df = data.frame(as.character(seqs6))\np = ggtree(tree1, lwd = 1.2) + geom_tiplab(aes(x = branch), size = 5, vjust = 2)\ngheatmap(p, mat6df[, 1:60], offset = 0.01, colnames = FALSE)__\n\nFigure 10.17: The tree on the left was used to generate the sequences on the right according to a Jukes Cantor model. The nucleotide frequencies generated at the root were quite unequal, with A and C being generated more rarely. As the sequences percolate down the tree, mutations occur, they are more likely to occur on the longer branches.\n__\nQuestion 10.9\nExperiment with the code above. Change the bf and rate arguments in the simSeq function to make mutations more likely. Do you think sequences generated with a very high mutation rate would make it easier to infer the tree that generated them?\n__\nSolution\n__\nVery high mutation rates result in mutations overwriting themselves and make inference more difficult. Of course, there is a sweet spot because some mutations have to occur in order for us to resolve the tree branches. After a certain time and a certain number of mutations it may be very difficult to see what was happening at the root. See Mossel (2003) for details.\n__\nQuestion 10.10\nEstimation bias: distance underestimation.\n1) If we only count the number of changes between two sequences using a simple Hamming distance, but there has been much evolutionary change between the two, why do we underestimate the distance between the sequences?\n2) Will be the bias be larger for smaller evolutionary distances?\nThe standard Markovian models of evolution we saw above enable us to improve these estimates.\n\n\n12.3.3 10.4.3 Estimating a phylogenetic tree\n\n“In solving a problem of this sort, the grand thing is to be able to reason backward. That is a very useful accomplishment, and a very easy one, but people do not practise it much. In the everyday affairs of life it is more useful to reason forward, and so the other comes to be neglected. There are fifty who can reason synthetically for one who can reason analytically”. Sherlock Holmes\nWhen the true tree-parameter is known, the above-mentioned probabilistic generative models of evolution tells us what patterns to expect in the sequences. As we have seen in earlier chapters, statistics means going back from the data to reasonable estimates of the parameters; here the tree itself and the branch lengths, even the evolutionary rates can be considered to be the parameters.\n\nFigure 10.18: A Steiner tree, the inner points are represented as squares. The method for creating the shortest tree that passes through all outer 1,2,5,6 is to create two inside (“ancester”) points 3 and 4.\nThere are several approaches to estimation: tree `building’ is no exception, here are the main ones:\nA nonparametric estimate: the parsimony tree Parsimony is a nonparametric method that minimizes the number of changes necessary to explain the data, it’s solution is the same as that of the Steiner tree problem (see Figure 10.18).\nA parametric estimate: the maximum likelihood tree In order to estimate the tree using a maximum likelihood or Bayesian approach one needs a model for molecular evolution that integrates mutation rates and branch edge lengths. ML estimation (e.g., Phyml, FastML, RaxML) use efficient optimization algorithms to maximize the likelihood of a tree under the model assumptions.\nBayesian posterior distributions for trees Bayesian estimation, MrBayes (Ronquist et al. 2012) or BEAST (Bouckaert et al. 2014) both use MCMC to find posterior distributions of the phylogenies. Bayesian methods are not directly integrated into R and require the user to import the collections of trees generated by Monte Carlo methods in order to summarize them and make confidence statements see Chakerian and Holmes (2012) for simple examples.\nThe semi-parametric approach: distance based methods These methods, called Neighbor Joining and UPGMA, are quite similar to the hierachical clusering algorithms we already encountered in Chapter 5. However, the distance estimation steps uses the parametric evolutionary models of Table 10.2; the `parametric’ part of why we call the method semi-parametric.\nThe neighbor-joining algorithm itself uses Steiner points as the summary of two combined points, and proceeds iteratively as in hierarchical clustering. It can be quite fast and is often used as a good starting point for the more time-consuming methods.\nLet’s start by estimating the tree from the data seqs6 using the nj (neighbor joining) on DNA distances based on the one-parameter Jukes-Cantor model, we make Figure 10.19 using the ggtree function:\ntree.nj = nj(dist.ml(seqs6, \"JC69\"))\nggtree(tree.nj) + geom_tiplab(size = 7) __\n\nFigure 10.19: Trees built with a neighbor joining algorithm are very fast to compute and are often used as initial values for more expensive estimation procedures such as the maximum likelihood or parsimony.\n__\nQuestion 10.11\nGenerate the maximum likelihood scores of the tree1 given the seqs6 data and compare them to those of the neighbor joining tree.\n__\nSolution\n__\nfit = pml(tree1, seqs6, k = 4)__\n__\nQuestion 10.12\nWhen we have aligned amino acids from which we want to infer a tree, we use (\\(20 \\)) transition matrices. The methods for estimating the phylogenetic are very similar. Try this in phangorn with an HIV amino acid sequence downloaded from https://www.hiv.lanl.gov/content/sequence/NEWALIGN/align.html.\nThe quality of the tree estimates depend on the number of sequences per taxa and the distance to the root. We can evaluate the quality of the estimates either by using parametric and nonparametric bootstraps or performing Bayesian tree estimation using MCMC. For examples of how to visualize and compare the sampling distribution of trees, see Chakerian and Holmes (2012).\n\n\n12.3.4 10.4.4 Application to 16S rRNA data\nIn Chapter 5 we saw how to use a probabilistic clustering method to denoise 16S rRNA sequences. We can now reload these denoised sequences and preprocess them before building their phylogeny4.\n4 In order to keep all the information and be able to compare sequences from different experiments, we use the sequences themselves as their label(Callahan, McMurdie, and Holmes 2017).\nlibrary(\"dada2\")\nseqtab = readRDS(file.path(DATA,\"seqtab.rds\"))\nseqs = getSequences(seqtab)\nnames(seqs) = seqs __\nOne of the benefits of using well-studied marker loci such as the 16S rRNA gene is the ability to taxonomically classify the sequenced variants. dada2 includes a naive Bayesian classifier method for this purpose (Wang et al. 2007). This classifier compares sequence variants to training sets of classified sequences. Here we use the RDP v16 training set (Cole et al. 2009)5. For example, code for such a classification might look like this.\n5 See the download link on the dada2 website: https://benjjneb.github.io/dada2/training.html\nfastaRef = \"../tmp/rdp_train_set_16.fa.gz\"\ntaxtab = assignTaxonomy(seqtab, refFasta = fastaRef)__\nSince the assignTaxonomy function runs for a while, the above code is not live and we here load a previously computed result, a table of taxonomic information:\ntaxtab = readRDS(file.path(DATA,\"taxtab16.rds\"))\ndim(taxtab)__\n\n\n[1] 268   6\n__\nQuestion 10.13\nWrite one line of code using R’s pipe operator |&gt; that shows just the first 6 rows of the taxonomic information without the row names.\n__\nSolution\n__\nhead(taxtab) |&gt; `rownames&lt;-`(NULL)__\n\n\n     Kingdom    Phylum          Class         Order          \n[1,] \"Bacteria\" \"Bacteroidetes\" \"Bacteroidia\" \"Bacteroidales\"\n[2,] \"Bacteria\" \"Bacteroidetes\" \"Bacteroidia\" \"Bacteroidales\"\n[3,] \"Bacteria\" \"Bacteroidetes\" \"Bacteroidia\" \"Bacteroidales\"\n[4,] \"Bacteria\" \"Bacteroidetes\" \"Bacteroidia\" \"Bacteroidales\"\n[5,] \"Bacteria\" \"Bacteroidetes\" \"Bacteroidia\" \"Bacteroidales\"\n[6,] \"Bacteria\" \"Bacteroidetes\" \"Bacteroidia\" \"Bacteroidales\"\n     Family               Genus        \n[1,] \"Porphyromonadaceae\" NA           \n[2,] \"Porphyromonadaceae\" NA           \n[3,] \"Porphyromonadaceae\" NA           \n[4,] \"Porphyromonadaceae\" \"Barnesiella\"\n[5,] \"Bacteroidaceae\"     \"Bacteroides\"\n[6,] \"Porphyromonadaceae\" NA           \n__\nQuestion 10.14\nWhat is the difference between taxonomic and phylogenetic information?\nNote that as the seqs data are randomly generated, they are “cleaner” than the real data we will have to handle.\nIn particular naturally occurring raw sequences have to be aligned. This is necessary as there are often extra nucleotides in some sequences, a consequence of what we call indel events6. Also mutations occur and appear as substitutions of one nucleotide by another.\n6 A nucleotide is deleted or inserted and it is often hard to distinguish which took place.\nHere is an example of what the first few characters of aligned sequences looks like:\nreadLines(file.path(DATA,\"mal2.dna.txt\")) |&gt; head(12) |&gt; cat(sep=\"\\n\")__\n\n\n    11   1620\nPre1        GTACTTGTTA GGCCTTATAA GAAAAAAGT- TATTAACTTA AGGAATTATA\nPme2        GTATCTGTTA AGCCTTATAA AAAGATAGT- T-TAAATTAA AGGAATTATA\nPma3        GTATTTGTTA AGCCTTATAA GAGAAAAGTA TATTAACTTA AGGA-TTATA\nPfa4        GTATTTGTTA GGCCTTATAA GAAAAAAGT- TATTAACTTA AGGAATTATA\nPbe5        GTATTTGTTA AGCCTTATAA GAAAAA--T- TTTTAATTAA AGGAATTATA\nPlo6        GTATTTGTTA AGCCTTATAA GAAAAAAGT- TACTAACTAA AGGAATTATA\nPfr7        GTACTTGTTA AGCCTTATAA GAAAGAAGT- TATTAACTTA AGGAATTATA\nPkn8        GTACTTGTTA AGCCTTATAA GAAAAGAGT- TATTAACTTA AGGAATTATA\nPcy9        GTACTCGTTA AGCCTTTTAA GAAAAAAGT- TATTAACTTA AGGAATTATA\nPvi10       GTACTTGTTA AGCCTTTTAA GAAAAAAGT- TATTAACTTA AGGAATTATA\nPga11       GTATTTGTTA AGCCTTATAA GAAAAAAGT- TATTAATTTA AGGAATTATA\nWe will perform this multiple-alignment on our seqs data using the DECIPHER package (Wright 2015):\nlibrary(\"DECIPHER\")\nalignment = AlignSeqs(DNAStringSet(seqs), anchor = NA, verbose = FALSE)__\nWe use the phangorn package to build the MLE tree (under the GTR model), but will use the neighbor-joining tree as our starting point.\nphangAlign = phangorn::phyDat(as(alignment, \"matrix\"), type = \"DNA\")\ndm = phangorn::dist.ml(phangAlign)\ntreeNJ = phangorn::NJ(dm)   # Note: tip order != sequence order\nfit = phangorn::pml(treeNJ, data = phangAlign)\nfitGTR = update(fit, k = 4, inv = 0.2)\nfitGTR = phangorn::optim.pml(fitGTR, model = \"GTR\", optInv = TRUE,\n         optGamma = TRUE,  rearrangement = \"stochastic\",\n         control = phangorn::pml.control(trace = 0))__",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "10-chap.html#combining-phylogenetic-trees-into-a-data-analysis",
    "href": "10-chap.html#combining-phylogenetic-trees-into-a-data-analysis",
    "title": "12  10.1 Goals for this chapter",
    "section": "12.4 10.5 Combining phylogenetic trees into a data analysis",
    "text": "12.4 10.5 Combining phylogenetic trees into a data analysis\nWe now need to combine the phylogenetic tree and the denoised read abundances with the complementary information provided about the samples from which the reads were gathered. This information about the sample is often provided as a spreadhseet (or .csv), and sometimes called the meta -data7. This data combination step is facilitated by the specialized containers and accessors that phyloseq provides.\n7 We consider the prefix meta unhelpful and potentially confusing here: the data about the samples is just that: data.\nThe following set of steps contains a few data cleanup and reorganization tasks—a dull but necessary part of applied statistics—that end in the creation of the object ps1.\nsamples = read.csv(\"../data/MIMARKS_Data_combined.csv\", header = TRUE)\nsamples$SampleID = paste0(gsub(\"00\", \"\", samples$host_subject_id), \n                          \"D\", samples$age-21) \nsamples = samples[!duplicated(samples$SampleID), ] \nstopifnot(all(rownames(seqtab) %in% samples$SampleID))\nrownames(samples) = samples$SampleID \nkeepCols = c(\"collection_date\", \"biome\", \"target_gene\", \"target_subfragment\", \n  \"host_common_name\", \"host_subject_id\", \"age\", \"sex\", \"body_product\", \"tot_mass\",\n  \"diet\", \"family_relationship\", \"genotype\", \"SampleID\") \nsamples = samples[rownames(seqtab), keepCols] __\nThe sample-by-sequence feature table, the sample (meta)data, the sequence taxonomies, and the phylogenetic tree—are combined into a single object as follows:\nlibrary(\"phyloseq\")\npso = phyloseq(tax_table(taxtab), \n               sample_data(samples),\n               otu_table(seqtab, taxa_are_rows = FALSE), \n               phy_tree(fitGTR$tree))__\nWe have already encountered several cases of combining heterogeneous datasets into special data classes that automate the linking and keeping consistent the different parts of the dataset (e.g., in Chapter 8, when we studied the pasilla data).\n__\nTask\nLook at the detailed phyloseq documentation here. Try a few filtering operations. For instance, create a subset of the data that contains the tree, taxa abundance table, the sample and taxa information for only the samples that have more than 5000 reads.\nThis can be done in one line:\nprune_samples(rowSums(otu_table(pso)) &gt; 5000, pso)__\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 268 taxa and 10 samples ]\nsample_data() Sample Data:       [ 10 samples by 14 sample variables ]\ntax_table()   Taxonomy Table:    [ 268 taxa by 6 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 268 tips and 266 internal nodes ]\nWe can also make other data transformations while maintaining the integrity of the links between all the data components.\n__\nQuestion 10.15\nWhat do the following lines of code do?\nprevalence = apply(X = otu_table(pso),\n                   MARGIN = ifelse(taxa_are_rows(pso), yes = 1, no = 2),\n                   FUN = function(x) {sum(x &gt; 0)})\nprevdf = data.frame(Prevalence = prevalence,\n                    TotalAbundance = taxa_sums(pso),\n                    tax_table(pso))\ntab = table(prevdf$Phylum)\nkeepPhyla = names(tab)[tab&gt;5]\nprevdf1   = subset(prevdf,   Phylum %in% keepPhyla)\nps2v      = subset_taxa(pso, Phylum %in% keepPhyla)__\nPlotting the abundances for certain bacteria can be done using barcharts. ggplot2 expressions have been hardwired into suitable one-line calls in the phyloseq package. There is also an interactive Shiny-phyloseq browser based tool (McMurdie and Holmes 2015). For more details, please see the online vignettes.\n\n12.4.1 10.5.1 Hierarchical multiple testing\nHypothesis testing can identify individual bacteria whose abundance relates to sample variables of interest. A standard approach is very similar to the approach we already visited in Chapter 6. Compute a test statistic for each taxa individually; then jointly adjust p-values to ensure a false discovery rate upper bound. However, this procedure does not exploit the structure among the tested hypotheses. For example, if we observe that one Ruminococcus species is strongly associated with age, but the biological reason for this sits at the genus level, then we would expect other species to have such an association as well. To integrate such information, Benjamini and Yekutieli (2003) and Benjamini and Bogomolov (2014) proposed a hierarchical testing procedure, where lower level taxonomic groups are only tested if higher levels are found to be be associated. In the case where many related species have a slight signal, this pooling of information can increase power.\nWe apply this method to test the association between microbial abundance and age. We use the data object ps1, which is similar to pso from above, but has undergone some additional transformation and filtering steps. We also need to apply the normalization protocols available in the DESeq2 package, which we discussed in Chapter 8, following Love, Huber, and Anders (2014) for RNA-Seq data and McMurdie and Holmes (2014) for 16S rRNA generated count data.\n# warning: !expr c(\"DESeqDataSet.se, design = design, ignoreRank.: some variables in design formula are characters, converting to factors\")\nlibrary(\"DESeq2\")\nps1 = readRDS(file.path(DATA,\"ps1.rds\"))\nps_dds = phyloseq_to_deseq2(ps1, design = ~ ageBin + family_relationship)\ngeometricmean = function(x)\n   if (all(x == 0)) { 0 } else { exp(mean(log(x[x != 0]))) }\ngeoMeans = apply(counts(ps_dds), 1, geometricmean)\nps_dds = estimateSizeFactors(ps_dds, geoMeans = geoMeans)\nps_dds = estimateDispersions(ps_dds)\nabund = getVarianceStabilizedData(ps_dds)__\nWe use the structSSI package to perform the hierarchical testing (Sankaran and Holmes 2014). For more convenient printing, we first shorten the names of the taxa:\nrownames(abund) = substr(rownames(abund), 1, 5) |&gt; make.names(unique = TRUE)__\nThe hierarchical testing procedure we are now going to do differs from standard multiple hypothesis testing in that univariate tests are done not only for every taxon, but for each higher-level taxonomic group. A helper function, treePValues, is available for this: it expects an edge list encoding parent-child relationships, with the first row specifying the root node.\nlibrary(\"structSSI\")\nel = phy_tree(ps1)$edge\nel0 = el\nel0 = el0[rev(seq_len(nrow(el))), ]\nel_names = c(rownames(abund), seq_len(phy_tree(ps1)$Nnode))\nel[, 1] = el_names[el0[, 1]]\nel[, 2] = el_names[el0[, 2]]\nunadj_p = treePValues(el, abund, sample_data(ps1)$ageBin)__\nWe can now do our FDR calculations using the hierarchical testing procedure. The test results are guaranteed to control several variants of FDR, but at different levels; we defer details to (Benjamini and Yekutieli 2003; Benjamini and Bogomolov 2014; Sankaran and Holmes 2014).\n__\nTask\nTry the following code, including the interactive plotting command that will open a browser window:\nhfdr_res = hFDR.adjust(unadj_p, el, 0.75)\nsummary(hfdr_res)\n#plot(hfdr_res, height = 5000) # not run: opens in a browser __\n\nFigure 10.20: A screenshot of a subtree with many differentially abundant microbes, as determined by the hierarchical testing procedure. Currently the user is hovering over the node associated with microbe GCGAG.33; this causes the adjusted p-value (0.029) to appear.\nThe plot opens in a new browser – a static screenshot of a subtree is displayed in Figure 10.20. Nodes are shaded according to p-values, from blue to orange, representing the strongest to weakest associations. Grey nodes were never tested, to focus power on more promising subtrees. Scanning the full tree; it becomes clear that the association between age group and taxonomic abundances is present in only a few isolated taxonomic groups. It is quite strong in those groups. To give context to these results, we can retrieve the taxonomic identity of the rejected hypotheses.\nlibrary(\"dplyr\")\noptions(digits = 3)\ntax = tax_table(ps1)[, c(\"Family\", \"Genus\")] |&gt; data.frame()\ntax$seq = rownames(abund)\nhfdr_res@p.vals$seq = rownames(hfdr_res@p.vals)\nleft_join(tax, hfdr_res@p.vals[,-3]) |&gt;\n  arrange(adjp) |&gt; head(9) |&gt; dplyr::select(1,2,4,5)__\n\n\n              Family       Genus hypothesisName hypothesisIndex\n1 Porphyromonadaceae        &lt;NA&gt;           &lt;NA&gt;              NA\n2 Porphyromonadaceae        &lt;NA&gt;           &lt;NA&gt;              NA\n3 Porphyromonadaceae        &lt;NA&gt;           &lt;NA&gt;              NA\n4 Porphyromonadaceae Barnesiella           &lt;NA&gt;              NA\n5     Bacteroidaceae Bacteroides           &lt;NA&gt;              NA\n6 Porphyromonadaceae Barnesiella           &lt;NA&gt;              NA\n7      Rikenellaceae   Alistipes           &lt;NA&gt;              NA\n8 Porphyromonadaceae        &lt;NA&gt;           &lt;NA&gt;              NA\n9 Porphyromonadaceae        &lt;NA&gt;           &lt;NA&gt;              NA\nIt seems that the most strongly associated bacteria all belong to family Lachnospiraceae.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "10-chap.html#minimum-spanning-trees",
    "href": "10-chap.html#minimum-spanning-trees",
    "title": "12  10.1 Goals for this chapter",
    "section": "12.5 10.6 Minimum spanning trees",
    "text": "12.5 10.6 Minimum spanning trees\nA very simple and useful graph is the so-called minimum spanning tree (MST). Given a set of vertices, a spanning tree is a tree that goes through all points at least once. Examples are shown in Figure 10.21. Given distances between vertices, the MST is the spanning tree with the minimum total length (see Figure 10.21).\nGreedy algorithms work well for computing the MST and there are many implementations in R: mstree in ade4 , mst in ape , spantree in vegan , mst in igraph.\n\n\n\n\n\n\n\n\nFigure 10.21: Two spanning trees for the same set of six vertices. The blue graph is the minimum spanning tree, if the Euclidean distances between the points in the 2D plane are used.\nHere we are going to take the DNA sequence distances between strains of HIV from patients all over the world and construct their minimum spanning tree. The result is shown in Figure 10.22.\nload(file.path(DATA, \"dist2009c.RData\"))\ncountry09 = attr(dist2009c, \"Label\")\nmstree2009 = ape::mst(dist2009c)\ngr09 = graph_from_adjacency_matrix(mstree2009, mode = \"undirected\")\nggraph(gr09, layout=\"fr\") +\n  geom_edge_link(color = \"black\",alpha=0.5) +\n  geom_node_point(aes(color = vertex_attr(gr09)$name), size = 2) +\n  geom_node_text(aes(label = vertex_attr(gr09)$name), color=\"black\",size=2) +\n  theme_void() +\n  guides(color=guide_legend(keyheight=0.1,keywidth=0.1,\n      title=\"Countries\"))__\n\nFigure 10.22: The minimum spanning tree computed from DNA distances between HIV sequences from samples taken in 2009 and whose country of origin was known, data as published in the HIVdb database (Rhee et al. 2003).\n__\nQuestion 10.16\nMake the network plot again, but replace geom_node_text with labels that repel each other to minimize the overlapping node labels.\n__\nSolution\n__\nSee Figure 10.23. Maybe a better, or additional approach would be to first cluster those vertices that are very close together and from the same country.\nlibrary(\"ggraph\")\nggraph(gr09, layout=\"fr\") +\n  geom_edge_link(color = \"black\",alpha=0.5) +\n  geom_node_point(aes(color = vertex_attr(gr09)$name), size = 2) +\n  geom_node_label(aes(label = vertex_attr(gr09)$name), color=\"black\",size=2,repel=TRUE) +\n  theme_void() +\n  guides(color=guide_legend(keyheight=0.1,keywidth=0.1,\n      title=\"Countries\"))__\n\nFigure 10.23: Solution to Question 10.16.\nIt could be preferable to use a graph layout that incorporates the known geographic coordinates. Thus, we might be able to see how the virus jumped large distances across the world through traveller mobility. We introduce approximate country coordinates, which we then jitter slightly to reduce overlapping.\nlibrary(\"rworldmap\")\nmat = match(country09, countriesLow$NAME)\ncoords2009 = data.frame(\n  lat = countriesLow$LAT[mat],\n  lon = countriesLow$LON[mat],\n  country = country09)\nlayoutCoordinates = cbind(\n  x = jitter(coords2009$lon, amount = 15),\n  y = jitter(coords2009$lat, amount = 8))\nlabc = names(table(country09)[which(table(country09) &gt; 1)])\nmatc = match(labc, countriesLow$NAME)\ndfc = data.frame(\n  latc = countriesLow$LAT[matc],\n  lonc = countriesLow$LON[matc],\n  labc)\ndfctrans = dfc\ndfctrans[, 1] = (dfc[,1] + 31) / 93\ndfctrans[, 2] = (dfc[,2] + 105) / 238\nCountries = vertex_attr(gr09)$name \n\nggraph(gr09, layout=layoutCoordinates) +\n  geom_node_point(aes(color=Countries),size = 3, alpha=0.75) +\n  geom_edge_arc(color = \"black\", alpha = 0.5, strength=0.15) +\n  geom_label(data=dfc,aes(x=lonc,y=latc,label=labc,fill=labc),colour=\"white\",alpha=0.8,size=3,show.legend=F) +\n  theme_void()  __\n\nFigure 10.24: A minimum spanning tree between HIV cases. The geographic locations of the cases were jittered to reduce overlapping. The DNA sequence distances between the HIV strains were used as input to an undirected minimum spanning tree algorithm.\nThe input to the minimum spanning tree algorithm is a distance matrix or a graph with a length edge attribute. Figure 10.24 is the minimum spanning tree between cases of HIV, for which strain information was made available through the HIVdb database@HIVdb. The DNA distances were computed using the Jukes- Cantor mutation model.\n__\nQuestion 10.17\nThe above analysis provided an undirected network of connections, in fact several implementations of the minimum spanning tree (ie for instance mstree in ade4) provide a directed path through the points, which can provide meaningful information on the (apparent) spread of disases. Make a directed network version of the above maps.\nMST is a very useful component of a simple nonparametric test for detecting differences between factors that are mapped onto its vertices.\n\n12.5.1 10.6.1 MST based testing: the Friedman–Rafsky test\nGraph-based two-sample tests8 were introduced by Friedman and Rafsky (Friedman and Rafsky 1979) as a generalization of the Wald-Wolfowitz runs test (see Figure 10.25). Our previous examples show graph vertices associated with covariates such as country of origin. Here we test whether the covariate is significantly associated to the graph structure.\n8 Tests that explore whether two samples are drawn from the same distribution.\nThe Friedman-Rafsky tests for two/multiple sample segregation on a minimum spanning tree. It was conceived as a generalization of the univariate Wald- Wolfowitz runs test. If we are comparing two samples, say men and women, whose coordinates represent a measurement of interest. We color the two groups blue and red as in Figure 10.25, the Wald-Wolfowitz test looks for long runs of the same color that would indicate that the two groups have different means.\n\nFigure 10.25: Seeing the number of runs in a one-dimensional, two-sample, nonparametric Wald-Wolfowitz test can indicate whether the two groups have the same distributions.\nInstead of looking for consecutive values of one type (‘runs’), we count the number of connected nodes of the same type.\nOnce the minimum spanning tree has been constructed, the vertices are assigned `colors’ according to the different levels of a categorical variable. We call pure edges those whose two nodes have the same level of the factor variable. We use \\(S_O\\), the number of pure edges as our test statistic. To evaluate whether our observed value could have occurred by chance when the groups have the same distributions, we permute the vertix labels (colors) randomly and recount how many pure edges there are. This label swapping is repeated many times, creating our null distribution for \\(S\\).\n\n\n12.5.2 10.6.2 Example: Bacteria sharing between mice\nHere we illustrate the idea on a collection of samples from mice whose stool were analyzed for their microbial content. We read in a data set with many mice and many taxa, we compute the Jaccard distance and then use the mst function from the igraph package. We annotate the graph with the relevant covariates as shown in the code below:\nps1  = readRDS(file.path(DATA,\"ps1.rds\"))\nsampledata = data.frame( sample_data(ps1))\nd1 = as.matrix(phyloseq::distance(ps1, method=\"jaccard\"))\ngr = graph_from_adjacency_matrix(d1,  mode = \"undirected\", weighted = TRUE)\nnet = igraph::mst(gr)\nV(net)$id = sampledata[names(V(net)), \"host_subject_id\"]\nV(net)$litter = sampledata[names(V(net)), \"family_relationship\"]__\nWe make a ggraph object from the resulting igraph generated minimum spanning tree and then plot it, as shown in Figure 10.26.\nggraph(net, layout=\"fr\")+\n  geom_edge_arc(color = \"darkgray\") +\n  geom_node_point(aes(color = id, shape = litter)) + \n  theme(legend.position=\"bottom\")__\n\nFigure 10.26: The minimum spanning tree based on Jaccard dissimilarity and annotated with the mice ID and litter factors\nNow we compute the null distribution and p-value for the test, this is implemented in the phyloseqGraphTest package:\nlibrary(\"phyloseqGraphTest\")\ngt = graph_perm_test(ps1, \"host_subject_id\", distance=\"jaccard\",\n                     type=\"mst\",  nperm=1000)\ngt$pval __\n\n\n[1] 0.000999\nWe can take a look at the complete histogram of the null distribution generatedby permutation using:\nplot_permutations(gt)__\n\nFigure 10.27: The permutation histogram of the number of pure edges in the network obtained from the minimal spanning tree with Jaccard similarity.\n\n12.5.2.1 Different choices for the skeleton graph\nIt is not necessary to use an MST for the skeleton graph that defines the edges. Graphs made by linking nearest neighbors (Schilling 1986) or distance thresholding work as well.\nThe Bioconductor package phyloseq has functionality for creating graphs based on thresholding a distance matrix through the function make_network. We create a network by creating an edge between samples whose Jaccard dissimilarity is less than a threshold, which we set below via the parameter max.dist. We can also use the ggraph package to add attributes to the vertices indicating which mouse the sample came from and which litter the mouse was in. We see that in the resulting network, shown in Figure 10.28, there is grouping of the samples by both mouse and litter.\nnet = make_network(ps1, max.dist = 0.35)\nsampledata = data.frame(sample_data(ps1))\nV(net)$id = sampledata[names(V(net)), \"host_subject_id\"]\nV(net)$litter = sampledata[names(V(net)), \"family_relationship\"]__\n\n\nggraph(net, layout=\"fr\") +\n  geom_edge_link(color = \"darkgray\") +\n  geom_node_point(aes(color = id, shape = litter)) + \n    theme(plot.margin = unit(c(0, 5, 2, 0), \"cm\"))+\n    theme(legend.position = c(1.4, 0.3),legend.background = element_blank(),\n          legend.margin=margin(0, 3, 0, 0, \"cm\"))+\n         guides(color=guide_legend(ncol=2))+\n  theme_graph(background = \"white\")__\n\nFigure 10.28: A co-occurrence network created by using a threshold on the Jaccard dissimilarity matrix. The colors represent which mouse the sample came from; the shape represents which litter the mouse was in.\nNote that no matter which graph we build between the samples, we can approximate a null distribution by permuting the labels of the nodes of the graph. However, sometimes it will preferable to adjust the permutation distribution to account for known structure between the covariates.\n\n\n\n12.5.3 10.6.3 Friedman–Rafsky test with nested covariates\nIn the test above, we took a rather naïve approach and showed there was a significant difference between individual mice (the host_subject_id variable). Here we perform a slightly different permutation test to find out if we control for the difference between mice; is there a litter (the family_relationship variable) effect? The setup of the test is similar, it is simply how the permutations are generated which differs. We maintain the nested structure of the two factors using the grouping argument. We permute the family_relationship labels but keep the host_subject_id structure intact.\ngt = graph_perm_test(ps1, \"family_relationship\",\n        grouping = \"host_subject_id\",\n        distance = \"jaccard\", type = \"mst\", nperm= 1000)\ngt$pval __\n\n\n[1] 0.002\nThis test has a small p-value, and we reject the null hypothesis that the two samples come from the same distribution. From the plot of the minimum spanning tree in Figure 10.27, we see by eye that the samples group by litter more than we would expect by chance.\nplot_permutations(gt)__\n\nFigure 10.29: The permutation histogram obtained from the minimal spanning tree with Jaccard similarity.\n__\nQuestion 10.18\nThe \\(k\\)-nearest neighbors graph is obtained by putting an edge between two samples whenever one of them is in the set of \\(k\\)-nearest neighbors of the other. Redo the test, defining the graph using nearest neighbors defined with the Jaccard distance. What would you conclude?\n__\nSolution\n__\ngtnn1 = graph_perm_test(ps1, \"family_relationship\",\n                      grouping = \"host_subject_id\",\n                      distance = \"jaccard\", type = \"knn\", knn = 1)\ngtnn1$pval __\n\n\n[1] 0.004\nFigure 10.30 shows that pairs of samples having edges between them in this nearest neighbor graph are much more likely to be from the same litter.\nplot_test_network(gtnn1)__\n\nFigure 10.30: The graph obtained from a nearest-neighbor graph with Jaccard similarity.\nNote: The dual graph\nIn the examples above we sought to show relationships between samples through their shared taxa. It can also be of interest to ask the question about taxa: do some of the taxa co-occur more often than one would expect? This approach can help study microbial `communities’ as they assemble in the microbiome. The methods we developed above all apply to this use-case, all one really does is transpose the data. It is always preferable with sparse data such as the microbiome to use Jaccard and not build correlation networks that can be appropriate in other settings.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "10-chap.html#summary-of-this-chapter",
    "href": "10-chap.html#summary-of-this-chapter",
    "title": "12  10.1 Goals for this chapter",
    "section": "12.6 10.7 Summary of this chapter",
    "text": "12.6 10.7 Summary of this chapter\n\n12.6.0.1 Annotated graphs\nIn this chapter we have learnt how to store and plot data that have more structure than simple arrays: graphs have edges and nodes that can also be associated to extra annotations that can be displayed usefully.\n\n\n12.6.0.2 Important examples of graphs and useful R packages\nWe started by specific examples such as Markov chain graphs, phylogenetic trees and minimum spanning trees. We saw how to use the ggraph and igraph packages to visualize graphs and show as much information as possible by using specific graph layout algorithms.\n\n\n12.6.0.3 Combining graphs with statistical data\nWe then approached the problem of incorporating a known `skeleton’ graph into differential expression analyses. This enables use to pinpoint perturbation hotspots in a network. We saw how evolutionary models defined along rooted binary trees serve as the basis for phylogenetic tree estimation and how we can incorporate these trees as supplementary information in a differential abundance analysis using the R packages structSSI and phyloseq.\n\n\n12.6.0.4 Linking co-occurrence to other variables\nGraph and network tools also enable the creation of networks from co- occurrence data and can be used to visualize and test the effect of factor covariates. We saw the Friedman-Rafsky test which provides an easy way of testing dependencies of a variable with the edge structure of a skeleton graph.\n\n\n12.6.0.5 Context and intepretation aids\nThis chapter illustrated ways of incorporating interactions of players in a network and we saw how useful it was to combine this with statistical scores. This often provides biological insight into analyses of complex biological systems.\n\n\n12.6.0.6 Previous knowledge or outcome\nWe saw that graphs can be both useful to encode our previous knowledge, metabolic network information, gene ontologies and phylogenetic trees of known bacteria are all available in standard databases. It is beneficial in a study to incorprate all known information and doing so by combining these skeleton networks with observed data enhances our understanding of experimental results in the context of what is already known.\nOn the other hand, the graph can be the outcome that we want to predict and we saw how to build graphs from data (phylogenetic trees, co-occurrence networks and minimum spanning trees).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "10-chap.html#further-reading",
    "href": "10-chap.html#further-reading",
    "title": "12  10.1 Goals for this chapter",
    "section": "12.7 10.8 Further reading",
    "text": "12.7 10.8 Further reading\nFor complete developments and many important consequences of the evolutionary models used in phylogenetic trees, see the books by Li (1997; Li and Graur 1991). The book by Felsenstein (2004) is the classic text on estimating phylogenetic trees.\nThe book written by the author of the ape packages, Paradis (2011) contains many use-cases and details about manipulation of trees in R. A review of bootstrapping for phylogenetic trees can be found in Holmes (2003a).\nWe can use a tree as well as abundances in a contingency table data through an extension of PCoA-MDS called DPCoA (Double principal coordinate analysis). For microbiome data, the phylogenetic tree provides distances between taxa; these distances serve as the basis for the first PCoA. A second PCoA enables the projection of the weighted sample points. This has proved very effective in microbial ecology applications, see Purdom (2010) or Fukuyama et al. (2012) for details.\nGraphs can be used to predict vertex covariates. There is a large field of applied statistics and machine learning that considers the edges in the graph as a response variable for which one can make predictions based on covariates or partial knowledge of the graph; these include ERGM ’s (Exponential Random Graph Models, Robins et al. (2007)) and kernel methods for graphs (Schölkopf, Tsuda, and Vert 2004).\nFor theoretical properties of the Friedman-Rafsky test and more examples see Bhattacharya (2015).\nA full list packages that deal with graphs and networks is available at: http://www.bioconductor.org/packages/release/BiocViews.html#___GraphAndNetwork.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "10-chap.html#exercises",
    "href": "10-chap.html#exercises",
    "title": "12  10.1 Goals for this chapter",
    "section": "12.8 10.9 Exercises",
    "text": "12.8 10.9 Exercises\n__\nExercise 10.1\nCreate a function that plots a graph starting from an adjacency matrix. Show how it works on an example.\n__\nSolution\n__\nggplotadjacency = function(a) {\n  n = nrow(a)\n  p = ncol(a)\n  fromto  = reshape2::melt(a)\n  stopifnot(identical(nrow(fromto), n*p))\n  fromto$value = as.factor(fromto$value)\n  cols = c(\"white\", \"darkblue\")\n  ggplot(data = fromto, aes(x = Var1, y = Var2, fill = value)) +\n    geom_tile(colour = \"black\") +\n    coord_fixed(ratio = 1, ylim = c(0.5, n + 0.5), xlim = c(0.5, p + 0.5)) +\n    scale_fill_manual(values = cols) +\n    scale_x_continuous(name = \"\" , breaks = 1:p, labels = paste(1:p)) +\n    scale_y_reverse(  name = \"\" , breaks = n:1, labels = paste(n:1)) + \n    theme_bw() +\n    theme(axis.text = element_text(size = 14),\n      legend.key = element_rect(fill = \"white\"),\n      legend.background = element_rect(fill = \"white\"),\n      panel.border = element_blank(),\n      panel.grid.major = element_blank(),\n      panel.grid.minor = element_blank(),\n      axis.line = element_line(color = \"white\"),\n      axis.ticks.x = element_blank(),\n      axis.ticks.y = element_blank() \n    )\n}__\n__\nExercise 10.2\nThe relationships between gene functions is organized hierarchically into a graph called the Gene Ontology (GO) graph. The biological processes are organized at finer and finer scale. Take one of the databases providing the GO information for the organisms you are interested in. Choose a gene list and build the GO graph for that list.\nHint: Some examples can be found in the packages , , .\n__\nExercise 10.3\nMarkov chain graph of transitions between states of the vaginal microbiota : In DiGiulio et al. (2015) the authors use an igraph plot to represent the transitions rates between community state types CSTs using the markovchain package. Load the data and the transition rates and state names into an object of the special class markovchain and tailor the layout carefully to include the percentage of preterm birth as a covariate for the vertices (make the vertex size proportional to this variable). Include the size of transitions between states as the width of the arrows.\n__\nSolution\n__\nNote: the below code is not live, a version of it used to run for the authors at one point, here it is given as a starting point for the reader to finish, it has opportunities for modernization and improvement.\n\nFigure 10.31: This figure was originally created for the study done in DiGiulio et al. (2015), where the nodes designate states of the vaginal microbiome and arrows represent transitions between states with different probabilities.\nlibrary(\"markovchain\")\n# Make Markov chain object\nmcPreg  =  new(\"markovchain\", states = CSTs,\n              transitionMatrix = trans, name=\"PregCST\")\nmcPreg\n# Set up igraph of the markov chain\nnetMC  =  markovchain:::.getNet(mcPreg, round = TRUE)__\nNow define a number of plotting parameters, and assign node colors based on the association of that CST and preterm outcome.\nwts  =  E(netMC)$weight/100\nedgel  =  get.edgelist(netMC)\nelcat  =  paste(edgel[,1], edgel[,2])\nelrev  =  paste(edgel[,2], edgel[,1])\nedge.curved  =  sapply(elcat, function(x) x %in% elrev)\nsamples_def  =  data.frame(sample_data(ps))\nsamples_def  =  samples_def[samples$Preterm | samples$Term,] # Only those definitely assigned, i.e. not marginal\npremat  =  table(samples_def$CST, samples_def$Preterm)\nrownames(premat)  =  markovchain::states(mcPreg)\ncolnames(premat)  =  c(\"Term\", \"Preterm\")\npremat\npremat  =  premat/rowSums(premat)\nvert.CSTclrs  =  CSTColors __\n\n\ndefault.par  =  par(no.readonly = TRUE)\n# Define color scale\n# Plotting function for markov chain\nplotMC  =  function(object, ...) {\n    netMC  =  markovchain:::.getNet(object, round = TRUE)\n    plot.igraph(x = netMC, ...)\n}\n# Color bar for the markov chain visualization, gradient in strength of preterm association\ncolor.bar  =  function(lut, min, max=-min, nticks=11, ticks=seq(min, max, len=nticks), title=NULL) {\n    scale = (length(lut)-1)/(max-min)\n    cur.par = par(no.readonly = TRUE)\n    par(mar = c(0, 4, 1, 4) + 0.1, oma = c(0, 0, 0, 0) + 0.1)\n    par(ps = 10, cex = 0.8)\n    par(tcl=-0.2, cex.axis=0.8, cex.lab = 0.8)\n    plot(c(min,max), c(0,10), type='n', bty='n', xaxt='n', xlab=\", yaxt='n', ylab=\", main=title)\n    axis(1, c(0, 0.5, 1))\n    for (i in 1:(length(lut)-1)) {\n      x = (i-1)/scale + min\n      rect(x,0,x+1/scale,10, col=lut[i], border=NA)\n    }\n}\n\npal  =  colorRampPalette(c(\"grey50\", \"maroon\", \"magenta2\"))(101)\nvert.clrs  =  sapply(states(mcPreg), function(x) pal[1+round(100*premat[x,\"Preterm\"])])\nvert.sz  =  4 + 2*sapply(states(mcPreg),\n              function(x) nrow(unique(sample_data(ps)[sample_data(ps)$CST==x,\"SubjectID\"])))\nvert.sz  =  vert.sz * 0.85\nvert.font.clrs  =  c(\"white\", \"white\", \"white\", \"white\", \"white\")\n\n# E(netMC) to see edge list, have to define loop angles individually by the # in edge list, not vertex\nedge.loop.angle = c(0, 0, 0, 0, 3.14, 3.14, 0, 0, 0, 0, 3.14, 0, 0, 0, 0, 0)-0.45\nlayout  =  matrix(c(0.6,0.95, 0.43,1, 0.3,0.66, 0.55,0.3, 0.75,0.65), nrow = 5, ncol = 2, byrow = TRUE)\n\n# Color by association with preterm birth\nlayout(matrix(c(1,1,2,2), 2, 2, byrow = TRUE), heights=c(1,10))\ncolor.bar(pal, min=0, max=1, nticks=6, title=\"Fraction preterm\")\npar(mar=c(0,1,1,1)+0.1)\nedge.arrow.size=0.8\nedge.arrow.width=1.4\nedge.width = (15*wts + 0.1)*0.6\nedge.labels  =  as.character(E(netMC)$weight/100)\nedge.labels[edge.labels&lt;0.4]  =  NA  # labels only for self-loops\nplotMC(mcPreg, edge.arrow.size=edge.arrow.size, edge.arrow.width = edge.arrow.width,\n       edge.width=edge.width, edge.curved=edge.curved,\n       vertex.color=vert.clrs, vertex.size=(vert.sz),\n       vertex.label.font = 2, vertex.label.cex = 1,\n       vertex.label.color = vert.font.clrs, vertex.frame.color = NA,\n       layout=layout, edge.loop.angle = edge.loop.angle)\npar(default.par)__\n__\nExercise 10.4\nProtein interaction networks : Read the Wikipedia article about the STRING database (http://www.string- db.org).\nThe protein Cyclin B1 is encoded by the CCNB1 gene. You can read about it on wikipedia here: https://en.wikipedia.org/wiki/Cyclin_B1.\nUse STRING to generate a text file (call it ccnb1datsmall.txt) of edges around the CCNB1 gene. Choose nodes that are connected by evidence of co- expression with a confidence higher than 0.9. Collect no more than 50 interactions and additional nodes that are two steps away from CCNB1 in the graph.\n__\nSolution\n__\n\nGo to http://www.string-db.org.\nEnter CCNB1 as the protein name and Homo sapiens as the organism. Click “Continue!”\nSelect the option with protein CCNB1 (the top one).\nScroll down to “info and Parameters …” 4a. For Active Prediction Methods – unselect everything except “Co-Expression” 4b. For required confidence – select “highest confidence (0.900) 4c. For interactors shown – select”no more than 50 interactors” 4d. For additional (white) nodes – select “100” (these are nodes two steps away from CCNB1) 4e. Click “Update Parameters”. You should get something that looks like the image below.\nClick “save” under the picture (showing a diskette). This will open up a new window so you can choose which format to save the data.\nScroll down to the “Text Summary (TXT - simple tab delimited flatfile)” file and save that document as ccnb1datsmall.txt.\n\n__\nExercise 10.5\nRead the txt file ccnb1datsmall.txt into R and make a plot of the graph using one of the graph visualization methods covered in this chapter.\n__\nSolution\n__\n\nFigure 10.32: This network was created with the STRING website by setting a 2 step neighborhood around the CCNB1 gene for co-expression levels \\(\\) 0.900.\ndat = read.table(file.path(DATA,\"ccnb1datsmall.txt\"), header = TRUE, comment.char = \"\", stringsAsFactors = TRUE)\nv = levels(unlist(dat[,1:2]))        # vertex names\nn = length(v)                        # number of vertices\ne = matrix(match(as.character(unlist(dat[,1:2])), v),ncol=2) # edge list\nw = dat$coexpression                 # edge weights __\nM is our co-expression network adjacency matrix. Since the STRING data only says if proteins i and j are co-expressed and doesn’t distinguish between (i,j) and (j,i) we want to make M symmetric (undirected) by considering the weight on (i,j) is the same as from (j,i). A is our co-expression graph adjacency matrix and we make \\(A_{ij} = 1\\) if they are coexpressed.\nM = matrix(0, n, n)\nM[e] = w\nM = M + t(M)\ndimnames(M) = list(v, v)\nA = 1*(M &gt; 0)__\nWe use default plotting parameters and generate the graph using the package igraph starting with e, the vector of edges (an alternative is to use the adjacency matrix A).\nNote: We use a seed to make the graph always look the same. Graph layout often contains an optimization with a random component that makes the picture look different, although the graph itself is the same.\nlibrary(igraph)\nnet = network(e, directed=FALSE)\npar(mar=rep(0,4))\nplot(net, label=v)__\nYou could make a graph with ggraph.\n__\nExercise 10.6\nMake a heatmap showing the adjacency matrix of the graph created in Exercise 10.5.\n__\nSolution\n__\nWe use defaults in making a heatmap except for changing the colors, you can experiment and add additional parameters.\nbreaks  =  c(0, seq(0.9, 1, length=11))\ncols  =  grey(1-c(0,seq(0.5,1,length=10)))\nccnb1ind  =  which(v == \"CCNB1\")\nvcols  =  rep(\"white\",n)\nvcols[ccnb1ind]  =  \"blue\"\nvcols[which(M[,ccnb1ind]&gt;0 | M[ccnb1ind,])]  =  \"red\"\npar(mar = rep(0, 4))\nheatmap(M, symm = TRUE, ColSideColors = vcols, RowSideColors = vcols,\n        col = cols, breaks = breaks,  frame = TRUE)\nlegend(\"topleft\", c(\"Neighbors(CCNB1)\", \"CCNB1\"),\n       fill = c(\"red\",\"blue\"),\n       bty = \"n\", inset = 0, xpd = TRUE,  border = FALSE)__\n\nFigure 10.33: This represents the adjacency of the CCNB1 network – 2 step neighborhood with co-expression levels \\(\\) 0.900, generated from R (darker is closer to 1, we ignore values &lt; 0.9).\n__\nExercise 10.7\nThe visualization shows the strongest interactions in the two step neighborhood of CCNB1. Both the plotted graph and the heatmap image show the same data: there seems to be a cluster of proteins which are all similar to CCNB1 and there is also another cluster in the other proteins. Many of the proteins in the CCNB1 cluster are coexpressed at the same time as each other.\nWhy might this be the case?\nConversely, proteins which are coexpressed with a protein that is coexpressed with CCNB1 (two steps away) do not tend to be coexpressed with each other.\nIs it easier for you to see this in one of the figures (the plot or the heatmap) than the other?\n__\nExercise 10.8\nCompare the use of ape and phangorn in the analysis of HIV GAG data. Compute the Jukes Cantor distances between the sequences using both packages and compare them to the Hamming distances.\nlibrary(\"ape\")\nlibrary(\"phangorn\")\nGAG = read.dna(file.path(DATA, \"DNA_GAG_20.txt\"))__\n__\nExercise 10.9\nPerform the Friedman–Rafksy type test with a “two-nearest” neighbor-graph using the Bray-Curtis dissimilarity.\n__\nSolution\n__\ngt = graph_perm_test(ps1, \"family_relationship\", distance = \"bray\", \n                     grouping = \"host_subject_id\", type = \"knn\", knn = 2)\ngt$pval __\n\n\n[1] 0.004\n\n\nplot_test_network(gt)\npermdf = data.frame(perm=gt$perm)\nobs = gt$observed\nymax = max(gt$perm)\nggplot(permdf, aes(x = perm)) + geom_histogram(bins = 20) +\n  geom_segment(aes(x = obs, y = 0, xend = obs, yend = ymax/10), color = \"red\") +\n  geom_point(aes(x = obs, y = ymax/10), color = \"red\") + xlab(\"Number of pure edges\")__\n\n\n\\(\\)\n\n\n\n\\(\\)\n\nFigure 10.34: The graph (a) and permutation histogram (b) obtained from a two nearest-neighbor graph with Jaccard similarity.\nBeisser, Daniela, Gunnar W Klau, Thomas Dandekar, Tobias Müller, and Marcus T Dittrich. 2010. “BioNet: An R-Package for the Functional Analysis of Biological Networks.” Bioinformatics 26 (8): 1129–30.\nBenjamini, Yoav, and Marina Bogomolov. 2014. “Selective Inference on Multiple Families of Hypotheses.” Journal of the Royal Statistical Society: Series B 76 (1): 297–318.\nBenjamini, Yoav, and Daniel Yekutieli. 2003. “Hierarchical FDR Testing of Trees of Hypotheses.” Technical report, Department of Statistics; Operations Research, Tel Aviv University.\nBhattacharya, Bhaswar B. 2015. “Power of Graph-Based Two-Sample Tests.” arXiv Preprint arXiv:1508.07530.\nBouckaert, Remco, Joseph Heled, Denise Kühnert, Tim Vaughan, Chieh-Hsi Wu, Dong Xie, Marc A Suchard, Andrew Rambaut, and Alexei J Drummond. 2014. “BEAST 2: A Software Platform for Bayesian Evolutionary Analysis.” PLoS Computational Biology 10 (4): e1003537.\nCallahan, Benjamin J, Paul J McMurdie, and Susan P Holmes. 2017. “Exact Sequence Variants Should Replace Operational Taxonomic Units in Marker Gene Data Analysis.” ISME Journal , 1–5.\nChakerian, John, and Susan Holmes. 2012. “Computational Tools for Evaluating Phylogenetic and Hierarchical Clustering Trees.” Journal of Computational and Graphical Statistics 21 (3): 581–99.\nCole, J. R., Q. Wang, E. Cardenas, J. Fish, B. Chai, R. J. Farris, A. S. Kulam-Syed-Mohideen, et al. 2009. “The Ribosomal Database Project: Improved Alignments and New Tools for rRNA Analysis.” Nucleic Acids Research 37 (Supplement 1): D141–45.\nDiGiulio, Daniel B., Benjamin J. Callahan, Paul J. McMurdie, Elizabeth K. Costello, Deirdre J. Lyelle, Anna Robaczewska, Christine L. Sun, et al. 2015. “Temporal and Spatial Variation of the Human Microbiota During Pregnancy.” PNAS.\nFelsenstein, Joseph. 2004. Inferring Phylogenies. Boston: Sinauer.\nFriedman, Jerome H, and Lawrence C Rafsky. 1979. “Multivariate Generalizations of the Wald-Wolfowitz and Smirnov Two-Sample Tests.” The Annals of Statistics , 697–717.\nFukuyama, Julia, Paul J McMurdie, Les Dethlefsen, David A Relman, and Susan Holmes. 2012. “Comparisons of Distance Methods for Combining Covariates and Abundances in Microbiome Studies.” In Pac Symp Biocomput. World Scientific.\nHolmes, Susan. 1999. “Phylogenetic Trees: An Overview.” In Statistics and Genetics , 81–118. IMA 112. New York: Springer.\n———. 2003a. “Bootstrapping Phylogenetic Trees: Theory and Methods.” Statistical Science 18 (2): 241–55.\n———. 2003b. “Statistics for phylogenetic trees.” Theoretical Population Biology 63 (1): 17–32.\nIdeker, Trey, Owen Ozier, Benno Schwikowski, and Andrew F Siegel. 2002. “Discovering Regulatory and Signalling Circuits in Molecular Interaction Networks.” Bioinformatics 18 Suppl 1 (January): S233–40. http://bioinformatics.oxfordjournals.org/cgi/reprint/18/suppl\\_1/S233.\nLi, Wen-Hsiung. 1997. Molecular Evolution. Sinauer Associates Incorporated.\nLi, Wen-Hsiung, and Dan Graur. 1991. Fundamentals of Molecular Evolution. Vol. 48. Sinauer Associates Sunderland, MA.\nLiberzon, Arthur, Aravind Subramanian, Reid Pinchback, Helga Thorvaldsdóttir, Pablo Tamayo, and Jill P Mesirov. 2011. “Molecular Signatures Database (MSigDB) 3.0.” Bioinformatics 27 (12): 1739–40.\nLove, Michael I, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation of Fold Change and Dispersion for RNA-seq Data with DESeq2.” Gnome Biology 15 (12): 1–21.\nMcMurdie, Paul J, and Susan Holmes. 2014. “Waste Not, Want Not: Why Rarefying Microbiome Data Is Inadmissible.” PLoS Computational Biology 10 (4): e1003531.\n———. 2015. “Shiny-Phyloseq: Web Application for Interactive Microbiome Analysis with Provenance Tracking.” Bioinformatics 31 (2): 282–83.\nMossel, Elchanan. 2003. “On the Impossibility of Reconstructing Ancestral Data and Phylogenies.” Journal of Computational Biology 10 (5): 669–76.\nNacu, Serban, Rebecca Critchley-Thorne, Peter Lee, and Susan Holmes. 2007. “Gene Expression Network Analysis and Applications to Immunology.” Bioinformatics 23 (7, 7): 850–58. https://doi.org/10.1093/bioinformatics/btm019.\nNolan, Daniel J, Michael Ginsberg, Edo Israely, Brisa Palikuqi, Michael G Poulos, Daylon James, Bi-Sen Ding, et al. 2013. “Molecular Signatures of Tissue-Specific Microvascular Endothelial Cell Heterogeneity in Organ Maintenance and Regeneration.” Developmental Cell 26 (2): 204–19.\nParadis, Emmanuel. 2011. Analysis of Phylogenetics and Evolution with r. Springer Science & Business Media.\nPounds, Stan, and Stephan W Morris. 2003. “Estimating the Occurrence of False Positives and False Negatives in Microarray Studies by Approximating and Partitioning the Empirical Distribution of p-Values.” Bioinformatics 19 (10): 1236–42.\nPurdom, Elizabeth. 2010. “Analysis of a Data Matrix and a Graph: Metagenomic Data and the Phylogenetic Tree.” Annals of Applied Statistics , July.\nRhee, Soo-Yon, Matthew J Gonzales, Rami Kantor, Bradley J Betts, Jaideep Ravela, and Robert W Shafer. 2003. “Human Immunodeficiency Virus Reverse Transcriptase and Protease Sequence Database.” Nucleic Acids Research 31 (1): 298–303.\nRobins, Garry, Tom Snijders, Peng Wang, Mark Handcock, and Philippa Pattison. 2007. “Recent Developments in Exponential Random Graph (p*) Models for Social Networks.” Social Networks 29 (2): 192–215.\nRonquist, Fredrik, Maxim Teslenko, Paul van der Mark, Daniel L Ayres, Aaron Darling, Sebastian Höhna, Bret Larget, Liang Liu, Marc A Suchard, and John P Huelsenbeck. 2012. “MrBayes 3.2: Efficient Bayesian Phylogenetic Inference and Model Choice Across a Large Model Space.” Systematic Biology 61 (3): 539–42.\nSankaran, Kris, and Susan Holmes. 2014. “structSSI: Simultaneous and Selective Inference for Grouped or Hierarchically Structured Data.” Journal of Statistical Software 59 (1): 1–21.\nSchilling, Mark F. 1986. “Multivariate Two-Sample Tests Based on Nearest Neighbors.” Journal of the American Statistical Association 81 (395): 799–806.\nSchölkopf, Bernhard, Koji Tsuda, and Jean-Philippe Vert. 2004. Kernel Methods in Computational Biology. MIT press.\nWang, Q., G. M. Garrity, J. M. Tiedje, and J. R. Cole. 2007. “Naive Bayesian Classifier for Rapid Assignment of rRNA Sequences into the New Bacterial Taxonomy.” Applied and Environmental Microbiology 73 (16): 5261.\nWertheim, Joel O, and Michael Worobey. 2009. “Dating the Age of the SIV Lineages That Gave Rise to HIV-1 and HIV-2.” PLoS Computational Biology 5 (5): e1000377.\nWright, Erik S. 2015. “DECIPHER: Harnessing Local Sequence Context to Improve Protein Multiple Sequence Alignment.” BMC Bioinformatics 16 (1): 1.\nYu, Hongxiang, Diana L Simons, Ilana Segall, Valeria Carcamo-Cavazos, Erich J Schwartz, Ning Yan, Neta S Zuckerman, et al. 2012. “PRC2/EED-EZH2 Complex Is up-Regulated in Breast Cancer Lymph Node Metastasis Compared to Primary Tumor and Correlates with Tumor Proliferation in Situ.” PloS One 7 (12): e51239.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html",
    "href": "11-chap.html",
    "title": "13  11.1 Goals for this chapter",
    "section": "",
    "text": "13.1 11.2 Loading images\nImages are a rich source of data. In this chapter, we will see how quantitative information can be extracted from images, and how we can use statistical methods to summarize and understand the data. The goal of the chapter is to show that getting started working with image data is easy – if you are able to handle the basic R environment, you are ready to start working with images. That said, this chapter is not a general introduction to image analysis. The field is extensive; it touches many areas of signal processing, information theory, mathematics, engineering and computer science, and there are excellent books that present a systematic overview.\nWe will mainly study series of two-dimensional images, in particular, images of cells. We will learn how to identify the cells’ positions and shapes and how to quantitatively measure characteristics of the identified shapes and patterns, such as sizes, intensities, color distributions and relative positions. Such information can then be used in down-stream analyses: for instance, we can compare cells between different conditions, say under the effect of different drugs, or in different stages of differentiation and growth; or we can measure how the objects in the image relate to each other, e.g., whether they like to cluster together or repel each other, or whether certain characteristics tend to be shared between neighboring objects, indicative of cell-to-cell communication. In the language of genetics, what this means is that we can use images as complex phenotypes or as multivariate quantitative traits.\nWe will here not touch upon image analysis in more than two dimensions: we won’t consider 3D segmentation and registration, nor temporal tracking. These are sophisticated tasks for which specialized software would likely perform better than what we could assemble in the scope of this chapter.\nThere are similarities between data from high-throughput imaging and other high-throughput data in genomics. Batch effects tend to play a role, for instance because of changes in staining efficiency, illumination or many other factors. We’ll need to take appropriate precautions in our experimental design and analysis choices. In principle, the intensity values in an image can be calibrated in physical units, corresponding, say to radiant energy or fluorophore concentration; however this is not always done in practice in biological imaging, and perhaps also not needed. Somewhat easier to achieve and clearly valuable is a calibration of the spatial dimensions of the image, i.e., the conversion factor between pixel units and metric distances.\nIn this chapter we will:\nA useful toolkit for handling images in R is the Bioconductor package EBImage (Pau et al. 2010). We start out by reading in a simple picture to demonstrate the basic functions.\nEBImage currently supports three image file formats: jpeg, png and tiff. Above, we loaded a sample image from the MSMB package. When you are working with your own data, you do not need that package, just provide the name(s) of your file(s) to the readImage function. As you will see later in this chapter, readImage can read multiple images in one go, which are then all assembled into a single image data object. For this to work, the images need to have the same dimensions and color mode.\n__\nQuestion 11.1\nThe RBioFormats package (available on GitHub: https://github.com/aoles/RBioFormats) provides functionality for reading and writing many more image file formats. How many different file formats are supported?\n__\nSolution\n__\nSee the manual page of the read.image function in the RBioFormats package (note that this is distinct from EBImage::readImage) and the online documentation of the Bio-Formats project on the website of The Open Microscopy Environment, &lt;http://www.openmicroscopy.org/site/support/bio- formats5.5/supported-formats.html&gt;.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#loading-images",
    "href": "11-chap.html#loading-images",
    "title": "13  11.1 Goals for this chapter",
    "section": "",
    "text": "library(\"EBImage\")\nimagefile = system.file(\"images\", \"mosquito.png\", package = \"MSMB\")\nmosq = readImage(imagefile)__",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#displaying-images",
    "href": "11-chap.html#displaying-images",
    "title": "13  11.1 Goals for this chapter",
    "section": "13.2 11.3 Displaying images",
    "text": "13.2 11.3 Displaying images\nLet’s visualize the image that we just read in. The basic function is EBImage::display.\nEBImage::display(mosq)__\nThe above command opens the image in a window of your web browser (as set by getOption(\"browser\")). Using the mouse or keyboard shortcuts, you can zoom in and out of the image, pan and cycle through multiple image frames.\nAlternatively, we can also display the image using R’s built-in plotting by calling display with the argument method = \"raster\". The image then goes to the current device. In this way, we can combine image data with other plotting functionality, for instance, to add text labels.\nEBImage::display(mosq)\ntext(x = 85, y = 800, label = \"A mosquito\", adj = 0, col = \"orange\", cex = 1.5)__\n\nFigure 11.1: Mosquito discovered deceased in the suburbs of Decatur, Georgia (credit: CDC / Janice Haney Carr).\nThe resulting plot is shown in Figure 11.1. As usual, the graphics displayed in an R device can be saved using the base R functions dev.print or dev.copy.\nNote that we can also read and view color images, see Figure 11.2.\nimagefile = system.file(\"images\", \"hiv.png\", package = \"MSMB\")\nhivc = readImage(imagefile)__\n\n\nEBImage::display(hivc, method = \"raster\")__\n\nFigure 11.2: Scanning electron micrograph of HIV-1 virions budding from a cultured lymphocyte (credit: CDC / C. Goldsmith, P. Feorino, E.L. Palmer, W.R. McManus).\nFurthermore, if an image has multiple frames, they can be displayed all at once in a grid arrangement by specifying the function argument all = TRUE (Figure 11.3),\nnuc = readImage(system.file(\"images\", \"nuclei.tif\", package = \"EBImage\"))\nEBImage::display(1 - nuc, all = TRUE)__\n\nFigure 11.3: Tiled display of four images of cell nuclei from the EBImage package.\nor we can just view a single frame, for instance, the second one.\nEBImage::display(1 - nuc, frame = 2)__\n__\nQuestion 11.2\nWhy did we pass the argument 1 - nuc to the display function in the code for Figure 11.3? How does it look if we display nuc directly?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#how-are-images-stored-in-r",
    "href": "11-chap.html#how-are-images-stored-in-r",
    "title": "13  11.1 Goals for this chapter",
    "section": "13.3 11.4 How are images stored in R?",
    "text": "13.3 11.4 How are images stored in R?\nLet’s dig into what’s going on by first identifying the class of the image object.\nclass(mosq)__\n\n\n[1] \"Image\"\nattr(,\"package\")\n[1] \"EBImage\"\nSo we see that this object has the class Image. This is not one of the base R classes, rather, it is defined by the package EBImage. We can find out more about this class through the help browser or by typing class ? Image. The class is derived from the base R class array , so you can do with Image objects everything that you can do with R arrays; in addition, they have some extra features and behaviors1.\n1 In R’s parlance, the extra features are called slots and the behaviors are called methods; methods are a special kind of function.\n__\nQuestion 11.3\nHow can you find out what the slots of an Image object are and which methods can be applied to it?\n__\nSolution\n__\nThe class definition is easy, it is accessed with showClass(\"Image\"). Finding all the methods applicable to the Image class by an analogous call to an R function is painful; your best bet is to consult the manual page of the class and see which methods the author chose to mention.\nThe dimensions of the image can be extracted using the dim method, just like for regular arrays.\ndim(mosq)__\n\n\n[1] 1400  952\nThe hist method has been redefined2 compared to the ordinary hist function for arrays: it uses different and possibly more useful defaults (Figure 11.4).\n2 In object oriented parlance, overloaded.\nhist(mosq)__\n\nFigure 11.4: Histogram of the pixel intensities in mosq. Note that the range is between 0 and 1.\nIf we want to directly access the data matrix as an R array , we can use the accessor function imageData.\nimageData(mosq)[1:3, 1:6]__\n\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n[1,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784\n[2,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784\n[3,] 0.1960784 0.1960784 0.2000000 0.2039216 0.2000000 0.1960784\nA useful summary of an Image object is printed if we simply type the object’s name.\nmosq __\n\n\nImage \n  colorMode    : Grayscale \n  storage.mode : double \n  dim          : 1400 952 \n  frames.total : 1 \n  frames.render: 1 \n\nimageData(object)[1:5,1:6]\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n[1,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784\n[2,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784\n[3,] 0.1960784 0.1960784 0.2000000 0.2039216 0.2000000 0.1960784\n[4,] 0.1960784 0.1960784 0.2039216 0.2078431 0.2000000 0.1960784\n[5,] 0.1960784 0.2000000 0.2117647 0.2156863 0.2000000 0.1921569\nNow let us look at the color image.\nhivc __\n\n\nImage \n  colorMode    : Color \n  storage.mode : double \n  dim          : 1400 930 3 \n  frames.total : 3 \n  frames.render: 1 \n\nimageData(object)[1:5,1:6,1]\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    0    0    0    0    0    0\n[2,]    0    0    0    0    0    0\n[3,]    0    0    0    0    0    0\n[4,]    0    0    0    0    0    0\n[5,]    0    0    0    0    0    0\nThe two images differ by their property colorMode, which is Grayscale for mosq and Color for hivc. What is the point of this property? It turns out to be convenient when we are dealing with stacks of images. If colorMode is Grayscale, then the third and all higher dimensions of the array are considered as separate image frames corresponding, for instance, to different \\(z\\)-positions, time points, replicates, etc. On the other hand, if colorMode is Color, then the third dimension is assumed to hold different color channels, and only the fourth and higher dimensions – if present – are used for multiple image frames. In hivc, there are three color channels, which correspond to the red, green and blue intensities of our photograph. However, this does not necessarily need to be the case, there can be any number of color channels.\n__\nQuestion 11.4\nDescribe how R stores the data nuc.\n__\nSolution\n__\nnuc __\n\n\nImage \n  colorMode    : Grayscale \n  storage.mode : double \n  dim          : 510 510 4 \n  frames.total : 4 \n  frames.render: 4 \n\nimageData(object)[1:5,1:6,1]\n           [,1]       [,2]       [,3]       [,4]       [,5]       [,6]\n[1,] 0.06274510 0.07450980 0.07058824 0.08235294 0.10588235 0.09803922\n[2,] 0.06274510 0.05882353 0.07843137 0.09019608 0.09019608 0.10588235\n[3,] 0.06666667 0.06666667 0.08235294 0.07843137 0.09411765 0.09411765\n[4,] 0.06666667 0.06666667 0.07058824 0.08627451 0.08627451 0.09803922\n[5,] 0.05882353 0.06666667 0.07058824 0.08235294 0.09411765 0.10588235\n\n\ndim(imageData(nuc))__\n\n\n[1] 510 510   4\nWe see that we have 4 frames in total, which correspond to the 4 separate images (frames.render).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#writing-images-to-file",
    "href": "11-chap.html#writing-images-to-file",
    "title": "13  11.1 Goals for this chapter",
    "section": "13.4 11.5 Writing images to file",
    "text": "13.4 11.5 Writing images to file\nDirectly saving images to disk in the array representation that we saw in the previous section would lead to large file sizes – in most cases, needlessly large. It is common to use compression algorithms to reduce the storage consumption. There are two main types of image3 compression:\n3 In an analogous way, this is also true for movies and music.\n\nLossless compression: it is possible to exactly reconstruct the original image data from the compressed file. Simple priciples of lossless compression are: (i) do not spend more bits on representing a pixel than needed (e.g., the pixels in the mosq image have a range of 256 gray scale values, and this could be represented by 8 bits, although mosq stores them in a 64-bit numeric format4); and (2) identify patterns (such as those that you saw above in the printed pixel values for mosq and hivc) and represent them by much shorter to write down rules instead.\nLossy compression: additional savings are made compared to lossless compression by dropping details that a human viewer would be unlikely to notice anyway.\n\n4 While this is somewhat wasteful of memory, it is more compatible with the way the rest of R works, and is rarely a limiting factor on modern computer hardware.\n5 https://en.wikipedia.org/wiki/Portable_Network_Graphics\n6 https://en.wikipedia.org/wiki/JPEG\nAn example for a storage format with lossless compression is PNG5, an example for lossy compression is the JPEG6 format. While JPEG is good for your holiday pictures, it is good practice to store scientific images in a lossless format.\nWe read the image hivc from a file in PNG format, so let’s now write it out as a JPEG file. The lossiness is specified by the quality parameter, which can lie between 1 (worst) and 100 (best).\noutput_file = file.path(tempdir(), \"hivc.jpeg\")\nwriteImage(hivc, output_file, quality = 85)__\nSimilarly, we could have written the image as a TIFF file and chosen among several compression algorithms (see the manual page of the writeImage and writeTiff functions). The package RBioFormats lets you write to many further image file formats.\n__\nQuestion 11.5\nHow big is the hivc object in R’s memory? How big is the JPEG file? How much RAM would you expect a three color, 16 Megapixel image to occupy?\n__\nSolution\n__\nobject.size(hivc) |&gt; format(units = \"Mb\")__\n\n\n[1] \"29.8 Mb\"\n\n\n(object.size(hivc) / prod(dim(hivc))) |&gt; format() |&gt; paste(\"per pixel\")__\n\n\n[1] \"8 bytes per pixel\"\n\n\nfile.info( output_file )$size __\n\n\n[1] 294904\n\n\n16 * 3 * 8 __\n\n\n[1] 384",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#manipulating-images",
    "href": "11-chap.html#manipulating-images",
    "title": "13  11.1 Goals for this chapter",
    "section": "13.5 11.6 Manipulating images",
    "text": "13.5 11.6 Manipulating images\nNow that we know that images are stored as arrays of numbers in R, our method of manipulating images becomes clear – simple algebra! For example, we can take our original image, shown again in Figure 11.5a, and flip the bright areas to dark and vice versa by multiplying the image with -1 Figure 11.5b).\nmosqinv = normalize(-mosq)__\n__\nQuestion 11.6\nWhat does the function normalize do?\nWe could also adjust the contrast through multiplication (Figure 11.5c) and the gamma-factor through exponentiation Figure 11.5d).\nmosqcont = mosq * 3\nmosqexp = mosq ^ (1/3)__\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.5: The original mosquito image (a) and three different image transformations: (b) subtraction, (c) multiplication, (d) power transformation.\nFurthermore, we can crop, threshold and transpose images with matrix operations (Figure 11.6).\nmosqcrop   = mosq[100:438, 112:550]\nmosqthresh = mosq &gt; 0.5\nmosqtransp = transpose(mosq)__\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.6: Three further image transformations: (a) cropping, (b) thresholding, (c) transposition.\n__\nQuestion 11.7\nWhat data type is mosqthresh, the result of the thresholding?\n__\nSolution\n__\nIt is an Image object whose pixels are binary values represented by an R array of type logical. You can inspect the object by typing its name into the console.\nmosqthresh __\n\n\nImage \n  colorMode    : Grayscale \n  storage.mode : logical \n  dim          : 1400 952 \n  frames.total : 1 \n  frames.render: 1 \n\nimageData(object)[1:5,1:6]\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n[1,] FALSE FALSE FALSE FALSE FALSE FALSE\n[2,] FALSE FALSE FALSE FALSE FALSE FALSE\n[3,] FALSE FALSE FALSE FALSE FALSE FALSE\n[4,] FALSE FALSE FALSE FALSE FALSE FALSE\n[5,] FALSE FALSE FALSE FALSE FALSE FALSE\n__\nQuestion 11.8\nInstead of the transpose function as above, could we also use R’s base function t?\n__\nSolution\n__\nIn this instance, the values of t(mosq) and transpose(mosq) happen to be the same, but transpose is preferable since it also works with color and multiframe images.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#spatial-transformations",
    "href": "11-chap.html#spatial-transformations",
    "title": "13  11.1 Goals for this chapter",
    "section": "13.6 11.7 Spatial transformations",
    "text": "13.6 11.7 Spatial transformations\nWe just saw one type of spatial transformation, transposition, but there are many more—here are some examples:\nmosqrot   = EBImage::rotate(mosq, angle = 30)\nmosqshift = EBImage::translate(mosq, v = c(100, 170))\nmosqflip  = flip(mosq)\nmosqflop  = flop(mosq)__\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.7: Spatial transformations: (a) rotation, (b) translation, (c) reflection about the central horizontal axis (flip), (d) reflection about the central vertical axis (flop).\nIn the code above, the function rotate7 rotates the image clockwise with the given angle, translate moves the image by the specified two-dimensional vector (pixels that end up outside the image region are cropped, and pixels that enter into the image region are set to zero). The functions flip and flop reflect the image around the central horizontal and vertical axis, respectively. The results of these operations are shown in Figure 11.7.\n7 Here we call the function with its namespace qualifier EBImage:: to avoid confusion with a function of the same name in the namespace of the spatstat package, which we will attach later.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#linear-filters",
    "href": "11-chap.html#linear-filters",
    "title": "13  11.1 Goals for this chapter",
    "section": "13.7 11.8 Linear filters",
    "text": "13.7 11.8 Linear filters\nLet’s now switch to an application in cell biology. We load images of human cancer cells that were studied by Laufer, Fischer and co-workers (Laufer et al. 2013). They are shown in Figure 11.8.\nimagefiles = system.file(\"images\", c(\"image-DAPI.tif\", \"image-FITC.tif\", \"image-Cy3.tif\"), package = \"MSMB\")\ncells = readImage(imagefiles)__\n\nFigure 11.8: Human colon cancer cells (HCT116). The four images show the same cells: the leftmost image corresponds to DAPI staining of the cells’ DNA, the second to immunostaining against alpha-tubulin, the third to actin. They are displayed as gray-scale images. The rightmost image is obtained by overlaying the three images as color channels of an RGB image (red: actin, green: alpha- tubulin, blue: DNA).\nThe Image object cells is a three-dimensional array of size 340 \\(\\) 490 \\(\\) 3, where the last dimension indicates that there are three individual grayscale frames. Our goal now is to computationally identify and quantitatively characterize the cells in these images. That by itself would be a modest goal, but note that the dataset of Laufer et al.contains over 690,000 images, each of which has 2,048 \\(\\) 2,048 pixels. Here, we are looking at three of these, out of which a small region was cropped. Once we know how to achieve our stated goal, we can apply our abilities to such large image collections, and that is no longer a modest aim!\n\n13.7.1 11.8.1 Interlude: the intensity scale of images\nHowever, before we can start with real work, we need to deal with a slightly mundane data conversion issue. This is, of course, not unusual. Let us inspect the dynamic range (the minimum and the maximum value) of the images.\napply(cells, 3, range)__\n\n\n      image-DAPI  image-FITC   image-Cy3\n[1,] 0.001586938 0.002899214 0.001663233\n[2,] 0.031204700 0.062485695 0.055710689\nWe see that the maximum values are small numbers well below 1. The reason for this is that the readImage function recognizes that the TIFF images uses 16 bit integers to represent each pixel, and it returns the data – as is common for numeric variables in R – in an array of double precision floating point numbers, with the integer values (whose theoretical range is from 0 to \\(2^{16}-1=65535\\)) stored in the mantissa of the floating point representation and the exponents chosen so that the theoretical range is mapped to the interval \\([0,1]\\). However, the scanner that was used to create these images only used the lower 11 or 12 bits, and this explains the small maximum values in the images. We can rescale these data to approximately cover the range \\([0,1]\\) as follows8.\n8 The function normalize provides a more flexible interface to the scaling of images.\ncells[,,1]   = 32 * cells[,,1]\ncells[,,2:3] = 16 * cells[,,2:3]\napply(cells, 3, range)__\n\n\n     image-DAPI image-FITC  image-Cy3\n[1,] 0.05078202 0.04638743 0.02661173\n[2,] 0.99855039 0.99977111 0.89137102\nWe can keep in mind that these multiplications with a multiple of 2 have no impact on the underlying precision of the stored data.\n\n\n13.7.2 11.8.2 Noise reduction by smoothing\nNow we are ready to get going with analyzing the images. As our first goal is segmentation of the images to identify the individual cells, we can start by removing local artifacts or noise from the images through smoothing. An intuitive approach is to define a window of a selected size around each pixel and average the values within that window. After applying this procedure to all pixels, the new, smoothed image is obtained. Mathematically, we can express this as\n\\[ f^*(x,y) = {s=-a}^{a}{t=-a}^{a} f(x+s, y+t), \\]\nwhere \\(f(x,y)\\) is the value of the pixel at position \\(x\\), \\(y\\), and \\(a\\) determines the window size, which is \\(2a+1\\) in each direction. \\(N=(2a+1)^2\\) is the number of pixels averaged over, and \\(f^*\\) is the new, smoothed image.\nMore generally, we can replace the moving average by a weighted average, using a weight function \\(w\\), which typically has highest weight at the window midpoint (\\(s=t=0\\)) and then decreases towards the edges.\n\\[ (w * f)(x,y) = {s=-}^{+} {t=-}^{+} w(s,t), f(x+s, y+s) \\]\nFor notational convenience, we let the summations range from \\(-\\) to \\(\\), even if in practice the sums are finite as \\(w\\) has only a finite number of non-zero values. In fact, we can think of the weight function \\(w\\) as another image, and this operation is also called the convolution of the images \\(f\\) and \\(w\\), indicated by the the symbol \\(*\\). In EBImage , the 2-dimensional convolution is implemented by the function filter2, and the auxiliary function makeBrush can be used to generate weight functions \\(w\\).\nw = makeBrush(size = 51, shape = \"gaussian\", sigma = 7)\nnucSmooth = filter2(getFrame(cells, 1), w)__\n\nFigure 11.9: nucSmooth, a smoothed version of the DNA channel in the image object cells (the original version is shown in the leftmost panel of Figure 11.8).\n__\nQuestion 11.9\nHow does the weight matrix w look like?\n__\nSolution\n__\nSee Figure 11.10\nlibrary(\"tibble\")\nlibrary(\"ggplot2\")\ntibble(w = w[(nrow(w)+1)/2, ]) |&gt;\n  ggplot(aes(y = w, x = seq(along = w))) + geom_point()__\n\nFigure 11.10: The middle row of the weight matrix, w[26, ].\nIn fact, the filter2 function does not directly perform the summation indicated in Equation 11.2. Instead, it uses the Fast Fourier Transformation in a way that is mathematically equivalent and computationally more efficient.\nThe convolution in Equation 11.2 is a linear operation, in the sense that \\(w(c_1f_1+c_2f_2)= c_1wf_1 + c_2w*f_2\\) for any two images \\(f_1\\), \\(f_2\\) and numbers \\(c_1\\), \\(c_2\\). There is beautiful and powerful theory underlying linear filters (Vetterli, Kovačević, and Goyal 2014).\nTo proceed we now use smaller smoothing bandwidths than what we displayed in Figure 11.9 for demonstration. Let’s use a sigma of 1 pixel for the DNA channel and 3 pixels for actin and tubulin.\ncellsSmooth = Image(dim = dim(cells))\nsigma = c(1, 3, 3)\nfor(i in seq_along(sigma))\n  cellsSmooth[,,i] = filter2( cells[,,i],\n         filter = makeBrush(size = 51, shape = \"gaussian\",\n                            sigma = sigma[i]) )__\nThe smoothed images have reduced pixel noise, yet still the needed resolution.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#adaptive-thresholding",
    "href": "11-chap.html#adaptive-thresholding",
    "title": "13  11.1 Goals for this chapter",
    "section": "13.8 11.9 Adaptive thresholding",
    "text": "13.8 11.9 Adaptive thresholding\nThe idea of adaptive thresholding is that, compared to straightforward thresholding as we did for Figure 11.6b, the threshold is allowed to be different in different regions of the image. In this way, one can anticipate spatial dependencies of the underlying background signal caused, for instance, by uneven illumination or by stray signal from nearby bright objects. In fact, we have already seen an example for uneven background in the bottom right image of Figure 11.3.\nOur colon cancer images (Figure 11.8) do not have such artefacts, but for demonstration, let’s simulate uneven illumination by multiplying the image with a two-dimensional bell function illuminationGradient, which has highest value in the middle and falls off to the sides (Figure 11.11).\npy = seq(-1, +1, length.out = dim(cellsSmooth)[1])\npx = seq(-1, +1, length.out = dim(cellsSmooth)[2])\nilluminationGradient = Image(outer(py, px, function(x, y) exp(-(x^2 + y^2))))\nnucBadlyIlluminated = cellsSmooth[,,1] * illuminationGradient __\nWe now define a smoothing window, disc, whose size is 21 pixels, and therefore bigger than the nuclei we want to detect, but small compared to the length scales of the illumination artifact. We use it to compute the image localBackground (shown in Figure 11.11 (c)) and the thresholded image nucBadThresh.\ndisc = makeBrush(21, \"disc\")\ndisc = disc / sum(disc)\nlocalBackground = filter2(nucBadlyIlluminated, disc)\noffset = 0.02\nnucBadThresh = (nucBadlyIlluminated - localBackground &gt; offset)__\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.11: a: illuminationGradient, a function that has its maximum at the center and falls off towards the sides, and which simulates uneven illumination sometimes seen in images. (b) nucBadlyIlluminated, the image that results from multiplying the DNA channel in cellsSmooth with illuminationGradient. (c) localBackground, the result of applying a linear filter with a bandwidth that is larger than the objects to be detected. (d) nucBadThresh, the result of adaptive thresholding. The nuclei at the periphery of the image are reasonably well identified, despite the drop off in signal strength.\nAfter having seen that this may work, let’s do the same again for the actual (not artificially degraded) image, as we need this for the next steps.\nnucThresh = (cellsSmooth[,,1] - filter2(cellsSmooth[,,1], disc) &gt; offset)__\nBy comparing each pixel’s intensity to a background determined from the values in a local neighborhood, we assume that the objects are relatively sparse distributed in the image, so that the signal distribution in the neighborhood is dominated by background. For the nuclei in our images, this assumption makes sense, for other situations, you may need to make different assumptions. The adaptive thresholding that we have done here uses a linear filter, filter2, and therefore amounts to (weighted) local averaging. Other distribution summaries, e.g. the median or a low quantile, tend to be preferable, even if they are computationally more expensive. For local median filtering, EBimage provides the function medianFilter.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#morphological-operations-on-binary-images",
    "href": "11-chap.html#morphological-operations-on-binary-images",
    "title": "13  11.1 Goals for this chapter",
    "section": "13.9 11.10 Morphological operations on binary images",
    "text": "13.9 11.10 Morphological operations on binary images\nThe thresholded image nucThresh (shown in the left panel of Figure [morphop] is not yet satisfactory. The boundaries of the nuclei are slightly rugged, and there is noise at the single-pixel level. An effective and simple way to remove these nuisances is given by a set of morphological operations (Serra 1983).\nProvided a binary image (with values, say, 0 and 1, representing back- and foreground pixels), and a binary mask9 (which is sometimes also called the structuring element), these operations work as follows.\n9 An example for a mask is a circle with a given radius, or more precisely, the set of pixels within a certain distance from a center pixel.\n\nerode: For every foreground pixel, put the mask around it, and if any pixel under the mask is from the background, then set all these pixels to background.\ndilate: For every background pixel, put the mask around it, and if any pixel under the mask is from the foreground, then set all these pixels to foreground.\nopen: perform erode followed by dilate.\n\nWe can also think of these operations as filters, however, in contrast to the linear filters of Section 11.8 they operate on binary images only, and there is no linearity.\nLet us apply morphological opening to our image.\nnucOpened = EBImage::opening(nucThresh, kern = makeBrush(5, shape = \"disc\"))__\nThe result of this is subtle, and you will have to zoom into the images in Figure 11.12 to spot the differences, but this operation manages to smoothen out some pixel-level features in the binary images that for our application are undesirable.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#segmentation-of-a-binary-image-into-objects",
    "href": "11-chap.html#segmentation-of-a-binary-image-into-objects",
    "title": "13  11.1 Goals for this chapter",
    "section": "13.10 11.11 Segmentation of a binary image into objects",
    "text": "13.10 11.11 Segmentation of a binary image into objects\nThe binary image nucOpened represents a segmentation of the image into foreground and background pixels, but not into individual nuclei. We can take one step further and extract individual objects defined as connected sets of pixels. In EBImage , there is a handy function for this purpose, bwlabel.\nnucSeed = bwlabel(nucOpened)\ntable(nucSeed)__\n\n\nnucSeed\n     0      1      2      3      4      5      6      7      8      9     10 \n155408    511    330    120    468    222    121    125    159    116    520 \n    11     12     13     14     15     16     17     18     19     20     21 \n   115    184    179    116    183    187    303    226    164    309    194 \n    22     23     24     25     26     27     28     29     30     31     32 \n   148    345    287    203    379    371    208    222    320    443    409 \n    33     34     35     36     37     38     39     40     41     42     43 \n   493    256    169    225    376    214    228    341    269    119    315 \nThe function returns an image, nucSeed, of integer values, where 0 represents the background, and the numbers from 1 to 43 index the different identified objects.\n__\nQuestion 11.10\nWhat are the numbers in the above table?\n__\nSolution\n__\nThey correspond to the area (in pixels) of each of the objects. We could use this information to remove objects that are too large or too small compared to what we expect.\nTo visualize such images, the function colorLabels is convenient, which converts the (grayscale) integer image into a color image, using distinct, arbitrarily chosen colors for each object.\nEBImage::display(colorLabels(nucSeed))__\nThis is shown in the middle panel of Figure 11.12. The result is already encouraging, although we can spot two types of errors:\n\nSome neighboring objects were not properly separated.\nSome objects contain holes.\n\nIndeed, we could change the occurrences of these by playing with the disc size and the parameter offset in Section 11.9: making the offset higher reduces the probability that two neighboring object touch and are seen as one object by bwlabel; on the other hand, that leads to even more and even bigger holes. Vice versa for making it lower.\nSegmentation is a rich and diverse field of research and engineering, with a large body of literature, software tools (Schindelin et al. 2012; Chaumont et al. 2012; Carpenter et al. 2006; Held et al. 2010) and practical experience in the image analysis and machine learning communities. What is the adequate approach to a given task depends hugely on the data and the underlying question, and there is no universally best method. It is typically even difficult to obtain a “ground truth” or “gold standards” by which to evaluate an analysis – relying on manual annotation of a modest number of selected images is not uncommon. Despite the bewildering array of choices, it is easy to get going, and we need not be afraid of starting out with a simple solution, which we can successively refine. Improvements can usually be gained from methods that allow inclusion of more prior knowledge of the expected shapes, sizes and relations between the objects to be identified.\nFor statistical analyses of high-throughput images, we may choose to be satisfied with a simple method that does not rely on too many parameters or assumptions and results in a perhaps sub-optimal but rapid and good enough result (Rajaram et al. 2012). In this spirit, let us proceed with what we have. We generate a lenient foreground mask, which surely covers all nuclear stained regions, even though it also covers some regions between nuclei. To do so, we simply apply a second, less stringent adaptive thresholding.\nnucMask = cellsSmooth[,,1] - filter2(cellsSmooth[,,1], disc) &gt; 0 __\nand apply another morphological operation, fillHull, which fills holes that are surrounded by foreground pixels.\nnucMask = fillHull(nucMask)__\nTo improve nucSeed, we can now propagate its segmented objects until they fill the mask defined by nucMask. Boundaries between nuclei, in those places where the mask is connected, can be drawn by Voronoi tessellation, which is implemented in the function propagate, and will be explained in the next section.\nnuclei = propagate(cellsSmooth[,,1], nucSeed, mask = nucMask)__\nThe result is displayed in the rightmost panel of Figure 11.12.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.12: Different steps in the segmentation of the nuclei. From (a-e): nucThresh, nucOpened, nucSeed, nucMask, nuclei.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#voronoi-tessellation",
    "href": "11-chap.html#voronoi-tessellation",
    "title": "13  11.1 Goals for this chapter",
    "section": "13.11 11.12 Voronoi tessellation",
    "text": "13.11 11.12 Voronoi tessellation\nVoronoi tessellation is useful if we have a set of seed points (or regions) and want to partition the space that lies between these seeds in such a way that each point in the space is assigned to its closest seed. As this is an intuitive and powerful idea, we’ll use this section for a short digression on it. Let us consider a basic example. We use the image nuclei as seeds. To call the function propagate, we also need to specify another image: for now we just provide a trivial image of all zeros, and we set the parameter lambda to a large positive value (we will come back to these choices).\nzeros        = Image(dim = dim(nuclei))\nvoronoiExamp = propagate(seeds = nuclei, x = zeros, lambda = 100)\nvoronoiPaint = paintObjects(voronoiExamp, 1 - nucOpened)__\n\nFigure 11.13: Example of a Voronoi segmentation, indicated by the gray lines, using the nuclei (indicated by black regions) as seeds.\n__\nQuestion 11.11\nHow do you select partition elements from the tessellation?\n__\nSolution\n__\nThe result, voronoiExamp, of the above call to propagate is simply an image of integers whose values indicate the different partitions.\nhead(table(voronoiExamp))__\n\n\nvoronoiExamp\n   1    2    3    4    5    6 \n5645 4735  370 5964 3333 1377 \n\n\nind = which(voronoiExamp == 13, arr.ind = TRUE)\nhead(ind, 3)__\n\n\n     row col\n[1,] 112 100\n[2,] 113 100\n[3,] 114 100\nThe result is shown in Figure 11.13. This looks interesting, but perhaps not yet as useful as the image nuclei in Figure [morphop]. We note that the basic definition of Voronoi tessellation, which we have given above, allows for two generalizations:\n\nBy default, the space that we partition is the full, rectangular image area – but indeed we could restrict ourselves to any arbitrary subspace. This is akin to finding the shortest distance from each point to the next seed not in a simple flat landscape, but in a landscape that is interspersed by lakes and rivers (which you cannot cross), so that all paths need to remain on the land. propagate allows for this generalization through its mask parameter.\nBy default, we think of the space as flat – but in fact it could have hills and canyons, so that the distance between two points in the landscape not only depends on their \\(x\\)- and \\(y\\)-positions but also on the ascents and descents, up and down in \\(z\\)-direction, that lie in between. We can think of \\(z\\) as an “elevation”. You can specify such a landscape to propagate through its x argument.\n\nMathematically, we say that instead of the simple default case (a flat rectangle, or image, with a Euclidean metric on it), we perform the Voronoi segmentation on a Riemann manifold that has a special shape and a special metric. Let us use the notation \\(x\\) and \\(y\\) for the column and row coordinates of the image, and \\(z\\) for the elevation. For two neighboring points, defined by coordinates \\((x, y, z)\\) and \\((x+x, y+y, z+z)\\), the distance \\(s\\) between them is thus not obtained by the usual Euclidean metric on the 2D image,\n\\[ s^2 = x^2 + y^2 \\]\nbut instead\n\\[ s^2 = , \\]\nwhere the parameter \\(\\) is a real number \\(\\). To understand this, lets look at some important cases:\n\\[ ]\nFor \\(\\), the metric becomes the isotropic Euclidean metric, i.e., a movement in \\(z\\)-direction is equally “expensive” or “far” as in \\(x\\)- or \\(y\\)-direction. In the extreme case of \\(\\), only the \\(z\\)-movements matter, whereas lateral movements (in \\(x\\)- or \\(y\\)-direction) do not contribute to the distance. In the other extreme case, \\(\\), only lateral movements matter, and movement in \\(z\\)-direction is “free”. Distances between points further apart are obtained by summing \\(s\\) along the shortest path between them. The parameter \\(\\) serves as a convenient control of the relative weighting between sideways movement (along the \\(x\\) and \\(y\\) axes) and vertical movement. Intuitively, if you imagine yourself as a hiker in such a landscape, by choosing \\(\\) you can specify how much you are prepared to climb up and down to overcome a mountain, versus walking around it. When we used lambda = 100 in our call to propagate at the begin of this section, this value was effectively infinite, so we were in the third boundary case of Equation 11.5.\nFor the purpose of cell segmentation, these ideas were put forward by Thouis Jones et al. (Jones, Carpenter, and Golland 2005; Carpenter et al. 2006), who also wrote the efficient algorithm that is used by propagate.\n__\nTask\nTry out the effect of using different \\(\\)s.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#segmenting-the-cell-bodies",
    "href": "11-chap.html#segmenting-the-cell-bodies",
    "title": "13  11.1 Goals for this chapter",
    "section": "13.12 11.13 Segmenting the cell bodies",
    "text": "13.12 11.13 Segmenting the cell bodies\n\nFigure 11.14: Histogram of the actin channel in cellsSmooth, after taking the logarithm.\n\nFigure 11.15: Zoom into Figure 11.14.\nTo determine a mask of cytoplasmic area in the images, let us explore a different way of thresholding, this time using a global threshold which we find by fitting a mixture model to the data. The histograms show the distributions of the pixel intensities in the actin image. We look at the data on the logarithmic scale, and in Figure 11.15 zoom into the region where most of the data lie.\nhist(log(cellsSmooth[,,3]) )\nhist(log(cellsSmooth[,,3]), xlim = -c(3.6, 3.1), breaks = 300)__\nLooking at the these histograms for many images, we can set up the following model for the purpose of segmentation: the signal in the cytoplasmic channels of the Image cells is a mixture of two distributions, a log-Normal background and a foreground with another, unspecified, rather flat, but mostly non-overlapping distribution10. Moreover the majority of pixels are from the background. We can then find robust estimates for the location and width parameters of the log-Normal component from the half range mode (implemented in the package genefilter) and from the root mean square of the values that lie left of the mode.\n10 This is an application of the ideas we saw in Chapter 4 on mixture models.\nlibrary(\"genefilter\")\nbgPars = function(x) {\n  x    = log(x)\n  loc  = half.range.mode( x )\n  left = (x - loc)[ x &lt; loc ]\n  wid  = sqrt( mean(left^2) )\n  c(loc = loc, wid = wid, thr = loc + 6*wid)\n}\ncellBg = apply(cellsSmooth, MARGIN = 3, FUN = bgPars)\ncellBg __\n\n\n           [,1]        [,2]        [,3]\nloc -2.90176965 -2.94427499 -3.52191681\nwid  0.00635322  0.01121337  0.01528207\nthr -2.86365033 -2.87699477 -3.43022437\nThe function defines as a threshold the location loc plus 6 widths wid11.\n11 The choice of the number 6 here is ad hoc; we could make the choice of threshold more objective by estimating the weights of the two mixture components and assigning each pixel to either fore- or background based on its posterior probability according to the mixture model. More advanced segmentation methods use the fact that this is really a classification problem and include additional features and more complex classifiers to separate foreground and background regions (e.g., (Berg et al. 2019)).\nhist(log(cellsSmooth[,,3]), xlim = -c(3.6, 3.1), breaks = 300)\nabline(v = cellBg[c(\"loc\", \"thr\"), 3], col = c(\"brown\", \"red\"))__\n\nFigure 11.16: As in Figure 11.15, but with loc and thr shown by vertical lines.\nWe can now define cytoplasmMask by the union of all those pixels that are above the threshold in the actin or tubulin image, or that we have already classified as nuclear in the image nuclei.\ncytoplasmMask = (cellsSmooth[,,2] &gt; exp(cellBg[\"thr\", 2])) |\n       nuclei | (cellsSmooth[,,3] &gt; exp(cellBg[\"thr\", 3]))__\nThe result is shown in the left panel of Figure 11.17. To define the cellular bodies, we can now simply extend the nucleus segmentation within this mask by the Voronoi tessellation based propagation algorithm of Section 11.12. This method makes sure that there is exactly one cell body for each nucleus, and the cell bodies are delineated in such a way that a compromise is reached between compactness of cell shape and following the actin and \\(\\)-tubulin intensity signal in the images. In the terminology of the propagate algorithm, cell shape is kept compact by the \\(x\\) and \\(y\\) components of the distance metric 11.4, and the actin signal is used for the \\(z\\) component. \\(\\) controls the trade-off.\ncellbodies = propagate(x = cellsSmooth[,,3], seeds = nuclei,\n                       lambda = 1.0e-2, mask = cytoplasmMask)__\nAs an alternative representation to the colorLabel plots, we can also display the segmentations of nuclei and cell bodies on top of the original images using the paintObjects function; the Images nucSegOnNuc, nucSegOnAll and cellSegOnAll that are computed below are show in the middle to right panels of Figure 11.17\ncellsColor = EBImage::rgbImage(red   = cells[,,3],\n                               green = cells[,,2],\n                               blue  = cells[,,1])\nnucSegOnNuc  = paintObjects(nuclei, tgt = EBImage::toRGB(cells[,,1]), col = \"#ffff00\")\nnucSegOnAll  = paintObjects(nuclei,     tgt = cellsColor,    col = \"#ffff00\")\ncellSegOnAll = paintObjects(cellbodies, tgt = nucSegOnAll,   col = \"#ff0080\")__\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.17: Steps in the segmentation of the cell bodies. From (a-d): cytoplasmMask, cellbodies (blue: DAPI, red: actin, green: alpha-tubulin), nucSegOnNuc, nucSegOnAll, cellSegOnAll.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#feature-extraction",
    "href": "11-chap.html#feature-extraction",
    "title": "13  11.1 Goals for this chapter",
    "section": "13.13 11.14 Feature extraction",
    "text": "13.13 11.14 Feature extraction\nNow that we have the segmentations nuclei and cellbodies together with the original image data cells, we can compute various descriptors, or features, for each cell. We already saw in the beginning of Section 11.11 how to use the base R function table to determine the total number and sizes of the objects. Let us now take this further and compute the mean intensity of the DAPI signal (cells[,,1]) in the segmented nuclei, the mean actin intensity (cells[,,3]) in the segmented nuclei and the mean actin intensity in the cell bodies.\nmeanNucInt       = tapply(cells[,,1], nuclei, mean)\nmeanActIntInNuc  = tapply(cells[,,3], nuclei, mean)\nmeanActIntInCell = tapply(cells[,,3], cellbodies, mean)__\nWe can visualize the features in pairwise scatterplots (Figure 11.18). We see that they are correlated with each other, although each feature also carries independent information.\nlibrary(\"GGally\")\nggpairs(tibble(meanNucInt, meanActIntInNuc, meanActIntInCell))__\n\nFigure 11.18: Pairwise scatterplots of per-cell intensity descriptors.\nWith a little more work, we could also compute more sophisticated summary statistics – e.g., the ratio of nuclei area to cell body area; or entropies, mutual information and correlation of the different fluorescent signals in each cell body, as more or less abstract measures of cellular morphology. Such measures can be used, for instance, to detect subtle drug induced changes of cellular architecture.\nWhile it is easy and intuitive to perform these computations using basic R idioms like in the tapply expressions above, the package EBImage also provides the function computeFeatures which efficiently computes a large collection of features that have been commonly used in the literature (a pioneering reference is Boland and Murphy. (2001)). Details about this function are described in its manual page, and an example application is worked through in the HD2013SGI vignette. Below, we compute features for intensity, shape and texture for each cell from the DAPI channel using the nucleus segmentation (nuclei) and from the actin and tubulin channels using the cell body segmentation (cytoplasmRegions).\nF1 = computeFeatures(nuclei,     cells[,,1], xname = \"nuc\",  refnames = \"nuc\")\nF2 = computeFeatures(cellbodies, cells[,,2], xname = \"cell\", refnames = \"tub\")\nF3 = computeFeatures(cellbodies, cells[,,3], xname = \"cell\", refnames = \"act\")\ndim(F1)__\n\n\n[1] 43 89\nF1 is a matrix with 43 rows (one for each cell) and 89 columns, one for each of the computed features.\nF1[1:3, 1:5]__\n\n\n  nuc.0.m.cx nuc.0.m.cy nuc.0.m.majoraxis nuc.0.m.eccentricity nuc.0.m.theta\n1   119.5523   17.46895          44.86819            0.8372059     -1.314789\n2   143.4511   15.83709          26.15009            0.6627672     -1.213444\n3   336.5401   11.48175          18.97424            0.8564444      1.470913\nThe column names encode the type of feature, as well the color channel(s) and segmentation mask on which it was computed. We can now use multivariate analysis methods – like those we saw in Chapters 5, 7 and 9 – for many dfferent tasks, such as\n\ndetecting cell subpopulations (clustering)\nclassifying cells into pre-defined cell types or phenotypes (classification)\nseeing whether the absolute or relative frequencies of the subpopulations or cell types differ between images that correspond to different biological conditions\n\nIn addition to these “generic” machine learning tasks, we also know the cell’s spatial positions, and in the following we will explore some ways to make use of these in our analyses.\n__\nTask\nUse explorative multivariate methods to visualize the matrices F1, F2, F3: PCA, heatmap. What’s special about the “outlier” cells?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#spatial-statistics-point-processes",
    "href": "11-chap.html#spatial-statistics-point-processes",
    "title": "13  11.1 Goals for this chapter",
    "section": "13.14 11.15 Spatial statistics: point processes",
    "text": "13.14 11.15 Spatial statistics: point processes\nIn the previous sections, we have seen ways how to use images of cells to extract their positions and various shape and morphological features. We’ll now explore spatial distributions of the position. In order to have interesting data to work on, we’ll change datasets and look at breast cancer lymph node biopsies.\n\n13.14.1 11.15.1 A case study: Interaction between immune cells and cancer cells\nThe lymph nodes function as an immunologic filter for the bodily fluid known as lymph. Antigens are filtered out of the lymph in the lymph node before returning it to the circulation. Lymph nodes are found throughout the body, and are composed mostly of T cells, B cells, dendritic cells and macrophages. The nodes drain fluid from most of our tissues. The lymph ducts of the breast usually drain to one lymph node first, before draining through the rest of the lymph nodes underneath the arm. That first lymph node is called the sentinel lymph node. In a similar fashion as the spleen, the macrophages and dendritic cells that capture antigens present these foreign materials to T and B cells, consequently initiating an immune response.\nT lymphocytes are usually divided into two major subsets that are functionally and phenotypically different.\n\nCD4+ T-cells, or T helper cells: they are pertinent coordinators of immune regulation. The main function of T helper cells is to augment or potentiate immune responses by the secretion of specialized factors that activate other white blood cells to fight off infection.\nCD8+ T cells, or T killer/suppressor cells: these cells are important in directly killing certain tumor cells, viral-infected cells and sometimes parasites. The CD8+ T cells are also important for the down-regulation of immune responses.\n\nBoth types of T cells can be found throughout the body. They often depend on the secondary lymphoid organs (the lymph nodes and spleen) as sites where activation occurs.\nDendritic Cells or CD1a cells are antigen-presenting cells that process antigen and present peptides to T cells.\nTyping the cells can be done by staining the cells with protein antibodies that provide specific signatures. For instance, different types of immune cells have different proteins expressed, mostly in their cell membranes.\n\nFigure 11.19: Biopsy of an enlarged lymph node revealed an intact capsule and obliterated sinuses (upper left panel, stained with hematoxylin and eosin, original magnification \\(\\) 100). The infiltrate was composed of an admixture of small lymphocytes, macrophages, and plasma cells (upper right panel, hematoxylin and eosin, original magnification \\(\\) 400). The infiltrate was composed of a mixture of CD3 positive T-cells (including both CD4 and CD8 positive cells) and CD20 positive B-cells. Numerous macrophages were also CD4 positive. (From: Hurley et al., Diagnostic Pathology (2008) 3:13)\n\nFigure 11.20: A stained lymph node; this image is the basis for the spatial data in brcalymphnode.\nWe’ll look at data by Setiadi et al. (2010). After segmentating the image shown in Figure 11.20 using the segmentation method GemIdent (Holmes, Kapelner, and Lee 2009), the authors obtained the coordinates and the type of all the cells in the image. We call this type of data a marked point process , and it can be seen as a simple table with 3 columns.\nlibrary(\"readr\")\nlibrary(\"dplyr\")\ncellclasses = c(\"T_cells\", \"Tumor\", \"DCs\", \"other_cells\")\nbrcalymphnode = lapply(cellclasses, function(k) {\n    read_csv(file.path(\"..\", \"data\", sprintf(\"99_4525D-%s.txt\", k))) |&gt;\n    transmute(x = globalX, y = globalY, class = k)\n}) |&gt; bind_rows() |&gt; mutate(class = factor(class))\n\nbrcalymphnode __\n\n\n# A tibble: 209,462 × 3\n       x     y class  \n   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  \n 1  6355 10382 T_cells\n 2  6356 10850 T_cells\n 3  6357 11070 T_cells\n 4  6357 11082 T_cells\n 5  6358 10600 T_cells\n 6  6361 10301 T_cells\n 7  6369 10309 T_cells\n 8  6374 10395 T_cells\n 9  6377 10448 T_cells\n10  6379 10279 T_cells\n# ℹ 209,452 more rows\n\n\ntable(brcalymphnode$class)__\n\n\n        DCs other_cells     T_cells       Tumor \n        878       77081      103681       27822 \nWe see that there are over a 100,000 T cells, around 28,000 tumor cells, and only several hundred dendritic cells. Let’s plot the \\(x\\)- and \\(y\\)-positions of the cells (Figure 11.21).\nggplot(filter(brcalymphnode, class %in% c(\"T_cells\", \"Tumor\")),\n   aes(x = x, y = y, col = class)) + geom_point(shape = \".\") +\n   facet_grid( . ~ class) + guides(col = \"none\")__\n\nFigure 11.21: Scatterplot of the \\(x\\) and \\(y\\) positions of the T- and tumor cells in brcalymphnode. The locations were obtained by a segmentation algorithm from a high resolution version of Figure 11.20. Some rectangular areas in the T-cells plot are suspiciously empty, this could be because the corresponding image tiles within the overall composite image went missing, or were not analyzed.\n__\nQuestion 11.12\nCompare Figures 11.20 and 11.21. Why are the \\(y\\)-axis inverted relative to each other?\n__\nSolution\n__\nFigure 11.20 follows the convention for image data, where the origin is in the top left corner of the image, while Figure 11.21 follows the convention for Cartesian plots, with the origin at the bottom left.\nTo use the functionality of the spatstat package, it is convenient to convert our data in brcalymphnode into an object of class ppp ; we do this by calling the eponymous function.\nlibrary(\"spatstat\")__\n\n\nln = with(brcalymphnode, ppp(x = x, y = y, marks = class, \n                             xrange = range(x), yrange = range(y)))\nln __\n\n\nMarked planar point pattern: 209462 points\nMultitype, with levels = DCs, other_cells, T_cells, Tumor \nwindow: rectangle = [3839, 17276] x [6713, 23006] units\nppp objects are designed to capture realizations of a spatial point process , that is, a set of isolated points located in a mathematical space; in our case, as you can see above, the space is a two-dimensional rectangle that contains the range of the \\(x\\)- and \\(y\\)-coordinates. In addition, the points can be marked with certain properties. In ln, the mark is simply the factor variable class. More generally, it could be several attributes, times, or quantitative data as well. There are similarities between a marked point process and an image, although for the former, the points can lie anywhere within the space, whereas in an image, the pixels are covering the space in regular, rectangular way.\n\n\n13.14.2 11.15.2 Convex hull\nAbove, we (implicitly) confined the point process to lie in a rectangle. In fact, the data generating process is more confined, by the shape of the tissue section. We can approximate this and compute a tighter region from the convex hull of the points12.\n12 You can use str(cvxhull) to look at the internal structure of this S3 object.\ncvxhull = convexhull.xy(cbind(ln$x, ln$y))\nggplot(as_tibble(cvxhull$bdry[[1]]), aes(x = x, y = y)) +\n  geom_polygon(fill = NA, col = \"black\") + geom_point() + coord_fixed()__\n\nFigure 11.22: Polygon describing the convex hull of the points in ln.\nWe can see the polygon in Figure 11.22 and now call ppp again, this time with the polygon.\nln = with(brcalymphnode, ppp(x = x, y = y, marks = class, \n                             poly = cvxhull$bdry[[1]]))\nln __\n\n\nMarked planar point pattern: 209462 points\nMultitype, with levels = DCs, other_cells, T_cells, Tumor \nwindow: polygonal boundary\nenclosing rectangle: [3839, 17276] x [6713, 23006] units\n\n\n13.14.3 11.15.3 Other ways of defining the space for the point process\nWe do not have to use the convex hull to define the space on which the point process is considered. Alternatively, we could have provided an image mask to ppp that defines the space based on prior knowledge; or we could use density estimation on the sampled points to only identify a region in which there is a high enough point density, ignoring sporadic outliers. These choices are part of the analyst’s job when considering spatial point processes.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#first-order-effects-the-intensity",
    "href": "11-chap.html#first-order-effects-the-intensity",
    "title": "13  11.1 Goals for this chapter",
    "section": "13.15 11.16 First order effects: the intensity",
    "text": "13.15 11.16 First order effects: the intensity\nOne of the most basic questions of spatial statistics is whether neighboring points are “clustering”, i.e., whether and to what extent they are closer to each other than expected “by chance”; or perhaps the opposite, whether they seem to repel each other. There are many examples where this kind of question can be asked, for instance\n\ncrime patterns within a city,\ndisease patterns within a country,\nsoil measurements in a region.\n\nIt is usually not hard to find reasons why such patterns exist: good and bad neighborhoods, local variations in lifestyle or environmental exposure, the common geological history of the soil. Sometimes there may also be mechanisms by which the observed events attract or repel each other – the proverbial “broken windows” in a neighborhood, or the tendency of many cell types to stick close to other cells.\nThe cell example highlights that spatial clustering (or anticlustering) can depend on the objects’ attributes (or marks, in the parlance of spatial point processes). It also highlights that the answer can depend on the length scale considered. Even if cells attract each other, they have a finite size, and cannot occupy the same space. So there will be some minmal distance between them, on the scale of which they essentially repel each other, while at further distances, they attract.\nTo attack these questions more quantitatively, we need to define a probabilistic model of what we expect by chance. Let’s count the number of points lying in a subregion, say, a circle of area \\(a\\) around a point \\(p=(x,y)\\); call this \\(N(p, a)\\)13 The mean and covariance of \\(N\\) provide first and second order properties. The first order is the intensity of the process:\n13 As usual, we use the uppercase notation \\(N(p, a)\\) for the random variable, and the lowercase \\(n(p, a)\\) for its realizations, or samples.\n\\[ (p) = _{a} . \\]\nHere we used infinitesimal calculus to define the local intensity \\((p)\\). As for time series, a stationary process is one where we have homogeneity all over the region, i.e., \\((p) = \\); then the intensity in an area \\(A\\) is proportional to the area: \\(E[N(, A)] = A\\). Later we’ll also look at higher order statistics, such as the spatial covariance\n\\[ (p_1, p_2) = _{a } . \\]\nIf the process is stationary, this will only depend on the relative position of the two points (the vector between them). If it only depends on the distance, i.e., only on the length but not on the direction of the vector, it is called second order isotropic.\n\nFigure 11.23: Rain drops falling on the floor are modelled by a Poisson process. The number of drops falling on a particular spot only depends on the rate \\(\\) (and on the size of the spot), but not on what happens at other spots.\n\n13.15.1 11.16.1 Poisson Process\nThe simplest spatial process is the Poisson process. We will use it as a null model against which to compare our data. It is stationary with intensity \\(\\), and there are no further dependencies between occurrences of points in non-overlapping regions of the space. Moreover, the number of points in a region of area \\(A\\) follows a Poisson distribution with rate \\(A\\).\n\n\n13.15.2 11.16.2 Estimating the intensity\nTo estimate the intensity, divide up the area into subregions, small enough to see potential local variations of \\((p)\\), but big enough to contain a sufficient sample of points. This is analogous to 2D density estimation, and instead of hard region boundaries, we can use a smooth kernel function \\(K\\).\n\\[ (p) = _i e(p_i) K(p-p_i). \\]\nThe kernel function depends on a smoothing parameter, \\(\\), the larger it is, the larger the regions over which we compute the local estimate for each \\(p\\). \\(e(p)\\) is an edge correction factor, and takes into account the estimation bias caused when the support of the kernel (the “smoothing window”) would fall outside the space on which the point process is defined. The function density, which is defined for ppp objects in the spatstat package, implements Equation 11.8.\nd = density(subset(ln, marks == \"Tumor\"), edge=TRUE, diggle=TRUE)\nplot(d)__\n\nFigure 11.24: Intensity estimate for the cells marked Tumor in ppp. The support of the estimate is the polygon that we specified earlier on (Figure 11.22).\nThe plot is shown in Figure 11.24.\n__\nQuestion 11.13\nHow does the estimate look without edge correction?\n__\nSolution\n__\nd0 = density(subset(ln, marks == \"Tumor\"), edge = FALSE)\nplot(d0)__\n\nFigure 11.25: As Figure 11.24, but without edge correction.\nNow estimated intensity is smaller towards the edge of the space, reflecting edge bias (Figure 11.25).\ndensity gives us as estimate of the intensity of the point process. A related, but different task is the estimation of the (conditional) probability of being a particular cell class. The function relrisk computes a nonparametric estimate of the spatially varying risk of a particular event type. We’re interested in the probability that a cell that is present at particular spatial location will be a tumor cell (Figure 11.26).\nrr = relrisk(ln, sigma = 250)__\n\n\nplot(rr)__\n\nFigure 11.26: Estimates of the spatially varying probability of each of the cell classes, conditional on there being cells.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#second-order-effects-spatial-dependence",
    "href": "11-chap.html#second-order-effects-spatial-dependence",
    "title": "13  11.1 Goals for this chapter",
    "section": "13.16 11.17 Second order effects: spatial dependence",
    "text": "13.16 11.17 Second order effects: spatial dependence\nIf we pick a point at random in our spatial process, what is the distance \\(W\\) to its nearest neighbor? For a homogenous Poisson process, the cumulative distribution function of this distance is\n\\[ G(w) = P(Ww) = 1-e{-w2}. \\]\nPlotting \\(G\\) gives a way of noticing departure from the homogenous Poisson process. An estimator of \\(G\\), which also takes into account edge effects (A. J. Baddeley 1998; Ripley 1988), is provided by the function Gest of the spatstat package.\ngln = Gest(ln)\ngln __\n\n\nFunction value object (class 'fv')\nfor the function r -&gt; G(r)\n.....................................................................\n        Math.label      Description                                  \nr       r               distance argument r                          \ntheo    G[pois](r)      theoretical Poisson G(r)                     \nhan     hat(G)[han](r)  Hanisch estimate of G(r)                     \nrs      hat(G)[bord](r) border corrected estimate of G(r)            \nkm      hat(G)[km](r)   Kaplan-Meier estimate of G(r)                \nhazard  hat(h)[km](r)   Kaplan-Meier estimate of hazard function h(r)\ntheohaz h[pois](r)      theoretical Poisson hazard function h(r)     \n.....................................................................\nDefault plot formula:  .~r\nwhere \".\" stands for 'km', 'rs', 'han', 'theo'\nRecommended range of argument r: [0, 20.998]\nAvailable range of argument r: [0, 52.443]\n\n\nlibrary(\"RColorBrewer\")\nplot(gln, xlim = c(0, 10), lty = 1, col = brewer.pal(4, \"Set1\"))__\n\nFigure 11.27: Estimates of \\(G\\), using three different edge effect corrections –which here happen to essentially lie on top of each other– and the theoretical distribution for a homogenous Poisson process.\nThe printed summary of the object gln gives an overview of the computed estimates; further explanations are in the manual page of Gest. In Figure 11.27 we see that the empirical distribution function and that of our null model, a homogenous Poisson process with a suitably chosen intensity, cross at around 4.5 units. Cell to cell distances that are shorter than this value are less likely than for the null model, in particular, there are essentially no distances below around 2; this, of course, reflects the fact that our cells have finite size and cannot overlap the same space. There seems to be trend to avoid very large distances –compared to the Poisson process–, perhaps indicative of a tendency of the cells to cluster.\n\n13.16.1 11.17.1 Ripley’s \\(K\\) function\nIn homogeneous spatial Poisson process, if we randomly pick any point and count the number of points within a distance of at most \\(r\\), we expect this number to grow as the area of the circle, \\(r^2\\). For a given dataset, we can compare this expectation to the observed number of neighbors within distance \\(r\\), averaged across all points.\nThe \\(K\\) function (variously called Ripley’s \\(K\\)-function or the reduced second moment function) of a stationary point process is defined so that \\(K(r)\\) is the expected number of (additional) points within a distance \\(r\\) of a given, randomly picked point. Remember that \\(\\) is the intensity of the process, i.e., the expected number of points per unit area. The \\(K\\) function is a second order moment property of the process.\nThe definition of \\(K\\) can be generalized to inhomogeneous point processes and written as in (A. Baddeley, Moller, and Waagepetersen 2000),\n\\[ K_{}(r)= {i,j} 𝟙{d(p_i, p_j) r} { (x_i) (x_j) }, \\]\nwhere \\(d(p_i, p_j)\\) is the distance between points \\(p_i\\) and \\(p_j\\), and \\(e(p_i, p_j, r)\\) is an edge correction factor14. For estimation and visualisation, it is useful to consider a transformation of \\(K\\) (and analogously, of \\(K_{}\\)), the so- called \\(L\\) function.\n14 See the manual page of Kinhom for more.\n\\[ L(r)=. \\]\nFor a homogeneous spatial Poisson process, the theoretical value is \\(L(r) = r\\). By comparing that to the estimate of \\(L\\) for a dataset, we can learn about inter-point dependence and spatial clustering. The square root in Equation 11.11 has the effect of stabilising the variance of the estimator, so that compared to \\(K\\), \\(L\\) is more appropriate for data analysis and simulations. The computations in the function Linhom of the spatstat package take a few minutes for our data (Figure 11.28).\nLln = Linhom(subset(ln, marks == \"T_cells\"))__\n\n\nLln __\n\n\nFunction value object (class 'fv')\n\n\nfor the function r -&gt; L[inhom](r)\n\n\n................................................................................\n           Math.label                \nr          r                         \ntheo       L[pois](r)                \nborder     {hat(L)[inhom]^{bord}}(r) \nbord.modif {hat(L)[inhom]^{bordm}}(r)\n           Description                                      \nr          distance argument r                              \ntheo       theoretical Poisson L[inhom](r)                  \nborder     border-corrected estimate of L[inhom](r)         \nbord.modif modified border-corrected estimate of L[inhom](r)\n................................................................................\nDefault plot formula:  .~.x\nwhere \".\" stands for 'bord.modif', 'border', 'theo'\nRecommended range of argument r: [0, 694.7]\nAvailable range of argument r: [0, 694.7]\n\n\nplot(Lln, lty = 1, col = brewer.pal(3, \"Set1\"))__\n\nFigure 11.28: Estimate of \\(L_{}\\), Equations 11.10 and 11.11, of the T cell pattern.\nWe could now proceed with looking at the \\(L\\) function also for other cell types, and for different tumors as well as for healthy lymph nodes. This is what Setiadi and colleagues did in their report (Setiadi et al. 2010), where by comparing the spatial grouping patterns of T and B cells between healthy and breast cancer lymph nodes they saw that B cells appeared to lose their normal localization in the extrafollicular region of the lymph nodes in some tumors.\n\n13.16.1.1 The pair correlation function\ndescribes how point density varies as a function of distance from a reference point. It provides a perspective inspired by physics for looking at spatial clustering. For a stationary point process, it is defined as\n\\[ g(r)=(r). \\]\nFor a stationary Poisson process, the pair correlation function is identically equal to 1. Values \\(g(r) &lt; 1\\) suggest inhibition between points; values greater than 1 suggest clustering.\nThe spatstat package allows computing estimates of \\(g\\) even for inhomogeneous processes, if we call pcf as below, the definition 11.12 is applied to the estimate of \\(K_{}\\).\npcfln = pcf(Kinhom(subset(ln, marks == \"T_cells\")))__\n\n\nplot(pcfln, lty = 1)\nplot(pcfln, lty = 1, xlim = c(0, 10))__\n\n\n\n\n\n\n\n\nFigure 11.29: Estimate of the pair correlation function, Equation 11.12, of the T cell pattern.\nAs we see in Figure 11.29, the T cells cluster, although at very short distances, there is also evidence for avoidance.\n__\nQuestion 11.14\nThe sampling resolution in the plot of the pair correlation function in the bottom panel of Figure 11.29 is low; how can it be increased?\n__\nSolution\n__\nThe answer lies in the r argument of the Kinhom function; see Figure 11.30.\npcfln2 = pcf(Kinhom(subset(ln, marks == \"T_cells\"),\n                    r = seq(0, 10, by = 0.2)))\nplot(pcfln2, lty = 1)__\n\nFigure 11.30: Answer to Question 11.14: as in the bottom panel of Figure 11.29, but with denser sampling.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#summary-of-this-chapter",
    "href": "11-chap.html#summary-of-this-chapter",
    "title": "13  11.1 Goals for this chapter",
    "section": "13.17 11.18 Summary of this chapter",
    "text": "13.17 11.18 Summary of this chapter\nWe learned to work with image data in R. Images are basically just arrays, and we can use familiar idioms to manipulate them. We can extract quantitative features from images, and then many of the analytical questions are not unlike those with other high-throughput data: we summarize the features into statistics such as means and variances, do hypothesis testing for differences between conditions, perform analysis of variance, apply dimension reduction, clustering and classification.\nOften we want to compute such quantitative features not on the whole image, but for individual objects shown in the image, and then we need to first segment the image to demarcate the boundaries of the objects of interest. We saw how to do this for images of nuclei and cells.\nWhen the interest is on the positions of the objects and how these positions relate to each other, we enter the realm of spatial statistics. We have explored some of the functionality of the spatstat package, have encountered the point process class, and we learned some of the specific diagnostic statistics used for point patterns, like Ripley’s \\(K\\) function.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#further-reading",
    "href": "11-chap.html#further-reading",
    "title": "13  11.1 Goals for this chapter",
    "section": "13.18 11.19 Further reading",
    "text": "13.18 11.19 Further reading\n\nThere is a vast amount of literature on image analysis. When navigating it, it is helpful to realize that the field is driven by two forces: specific application domains (we saw the analysis of high-throughput cell-based assays) and available computer hardware. Some algorithms and concepts that were developed in the 1970s are still relevant, others have been superseeded by more systematic and perhaps computationally more intensive methods. Many algorithms imply certain assumptions about the nature of the data and and scientific questions asked, which may be fine for one application, but need a fresh look in another. A classic introduction is The Image Processing Handbook (Russ and Neal 2015), which now is its seventh edition.\nFor spatial point pattern analysis, Diggle (2013; Ripley 1988; Cressie 1991; Chiu et al. 2013).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "11-chap.html#exercises",
    "href": "11-chap.html#exercises",
    "title": "13  11.1 Goals for this chapter",
    "section": "13.19 11.20 Exercises",
    "text": "13.19 11.20 Exercises\n__\nExercise 11.1\nLoad some images from your personal photo library into R and try out the manipulations from Section 11.6 on them.\n__\nExercise 11.2\nExplore the effect of the parameter lambda in the propagate function (Sections 11.12, 11.13) using a shiny app that displays the cellbodies image as in Figure 11.17.\n__\nExercise 11.3\nConsider the two-dimensional empirical autocorrelation function,\n\\[ a(v_x, v_y) = _{(x,y)I} B(x, y);B(x+v_x, , y+v_y), \\]\nwhere \\(B\\) is an image, i.e., a function over the set of pixels \\(I\\), the tuple \\((x,y)\\) runs over all the pixel coordinates, and \\(v=(v_x, v_y)\\) is the offset vector. Using the Wiener–Khinchin theorem, we can compute this function efficiently using the Fast Fourier Transformation.\nautocorr2d = function(x) {\n  y = fft(x/sum(x))\n  abs(gsignal::fftshift(fft(y * Conj(y), inverse = TRUE), MARGIN = 1:2)) \n}__\nBelow, we’ll use this little helper function, which shows a matrix as a heatmap with ggplot2 (similar to base R’s image).\nmatrix_as_heatmap = function(m)\n  ggplot(reshape2::melt(m), aes(x = Var1, y = Var2, fill = value)) + \n    geom_tile() + coord_fixed() +\n    scale_fill_continuous(type = \"viridis\") +\n    scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))__\nNow let’s apply autocorr2d to each of the three color channels separately. The result is shown in Figure 11.31.\nnm = dimnames(cells)[[3]]\nac = lapply(nm, function(i) autocorr2d(cells[,, i])) |&gt; setNames(sub(\"^image-\", \"\", nm))\n\nfor (w in names(ac)) \n  print(matrix_as_heatmap(ac[[w]]) + ggtitle(w))\n\ncy = dim(cells)[1] / 2\ncx = dim(cells)[2] / 2\nr  = round(sqrt((col(cells[,,1]) - cx)^2 + (row(cells[,,1]) - cy)^2))\n\nmatrix_as_heatmap(r) + ggtitle(\"radius r\")__\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.31: Autocorrelation functions of the three color channels of the cells image, shown as heatmaps. The peaks in the centres correspond to signal correlations over short distances. Also shown is the radial coordinate r.\nSince the images are (or should be) isotropic, i.e., there is no preferred direction, we can average over the angular coordinate. The result is shown in Figure 11.32. We can see that the signals in the different color channels have different length scales.\naggregate_by_radius = function(x, r)\n  tibble(x = as.vector(x),\n         r = as.vector(r)) |&gt;\n  group_by(r) |&gt;\n  summarize(value = mean(x))\n\nlapply(names(ac), function(w) \n  cbind(channel = w, \n        aggregate_by_radius(ac[[w]], r))\n  ) |&gt; \n  bind_rows() |&gt; \n  dplyr::filter(r &lt;= 50) |&gt;\n  ggplot(aes(x = r, y = value, col = channel)) + geom_line() + \n    scale_color_manual(values = c(`Cy3` = \"red\", `FITC` = \"green\", `DAPI` = \"blue\"))__\nExtend the autocorr2d function to also compute the cross-correlation between different channels.\n\nWhat is the motivation behind the sum normalization in the above implementation autocorr2d?\nWould it make sense to subtract the mean of x before the other computations?\nWhat is the relation between this function and the usual empirical variance or correlation, i.e. the functions var and sd in base R?\nHow might plots such as Figure 11.32 be used for the construction of quality metrics in a high-throughput screening setting, i.e., when thousands or millions of images need to be analyzed?\nHow would a 3- or \\(n\\)-dimensional extension of autocorr2d look like? What would it be good for?\n\n\nFigure 11.32: Autocorrelation functions of the three color channels of the cells image, aggregated by radius.\n__\nExercise 11.4\nHave a look at the workshop “Working with Image Data” https://github.com/wolfganghuber/WorkingWithImageData, which goes through some of the same content as this chapter, but on different images, and also has additional examples on segmentation and optical flow.\n__\nExercise 11.5\nCompute and display the Voronoi tessellation for the Ukrainian cities from Chapter 9. Either use their MDS-coordinates in the 2D plane with Euclidean distances, or the latitudes and longitudes using the great circle distance (Haversine formula).\n__\nExercise 11.6\nDownload 3D image data from light sheet microscopy15, load it into an EBImage Image object and explore the data.\n15 For instance, http://www.digital-embryo.org\nBaddeley, Adrain, Jesper Moller, and Rasmus Waagepetersen. 2000. “Non- and Semiparametric Estimation of Interaction in Inhomogeneous Point Patterns.” Statistica Neerlandica 54: 329–50.\nBaddeley, Adrian J. 1998. “Spatial Sampling and Censoring.” In Stochastic Geometry: Likelihood and Computation , edited by O. E. Barndorff-Nielsen, W. S. Kendall, and M. N. M. van Lieshout, 37–78. Chapman; Hall.\nBerg, Stuart, Dominik Kutra, Thorben Kroeger, Christoph N Straehle, Bernhard X Kausler, Carsten Haubold, Martin Schiegg, et al. 2019. “Ilastik: Interactive Machine Learning for (Bio)image Analysis.” Nature Methods 16 (12): 1226–32.\nBoland, Michael V., and Robert F. Murphy. 2001. “A neural network classifier capable of recognizing the patterns of all major subcellular structures in fluorescence microscope images of HeLa cells.” Bioinformatics 17 (12): 1213–23.\nCarpenter, Anne E, Thouis R Jones, Michael R Lamprecht, Colin Clarke, In Han Kang, Ola Friman, David A Guertin, Joo Han Chang, Robert A Lindquist, and Jason Moffat. 2006. “CellProfiler: Image Analysis Software for Identifying and Quantifying Cell Phenotypes.” Genome Biology 7: R100.\nChaumont, Fabrice de, Stéphane Dallongeville, Nicolas Chenouard, Nicolas Hervé, Sorin Pop, Thomas Provoost, Vannary Meas-Yedid, et al. 2012. “Icy: an open bioimage informatics platform for extended reproducible research.” Nature Methods 9: 690–96.\nChiu, Sung Nok, Dietrich Stoyan, Wilfrid S. Kendall, and Joseph Mecke. 2013. Stochastic Geometry and Its Applications. Springer.\nCressie, Noel A. 1991. Statistics for Spatial Data. John Wiley; Sons.\nDiggle, Peter J. 2013. Statistical Analysis of Spatial and Spatio-Temporal Point Patterns. Chapman; Hall/CRCs.\nHeld, M., M. H. A. Schmitz, B. Fischer, T. Walter, B. Neumann, M. H. Olma, M. Peter, J. Ellenberg, and D. W. Gerlich. 2010. “CellCognition: Time-Resolved Phenotype Annotation in High-Throughput Live Cell Imaging.” Nature Methods 7: 747.\nHolmes, Susan, Adam Kapelner, and Peter P Lee. 2009. “An Interactive Java Statistical Image Segmentation System: GemIdent.” Journal of Statistical Software 30 (10).\nJones, T., A. Carpenter, and P. Golland. 2005. “Voronoi-Based Segmentation of Cells on Image Manifolds.” Computer Vision for Biomedical Image Applications , 535.\nLaufer, Christina, Bernd Fischer, Maximilian Billmann, Wolfgang Huber, and Michael Boutros. 2013. “Mapping genetic interactions in human cancer cells with RNAi and multiparametric phenotyping.” Nature Methods 10: 427–31.\nPau, Grégoire, Florian Fuchs, Oleg Sklyar, Michael Boutros, and Wolfgang Huber. 2010. “EBImage R Package for Image Processing with Applications to Cellular Phenotypes.” Bioinformatics 26 (7): 979–81.\nRajaram, S., B. Pavie, L. F. Wu, and S. J. Altschuler. 2012. “PhenoRipper: software for rapidly profiling microscopy images.” Nature Methods 9: 635–37.\nRipley, B. D. 1988. Statistical Inference for Spatial Processes. Cambridge University Press.\nRuss, John C., and F. Brent Neal. 2015. The Image Processing Handbook. 7th ed. CRC Press;\nSchindelin, Johannes, Ignacio Arganda-Carreras, Erwin Frise, Verena Kaynig, Mark Longair, Tobias Pietzsch, Stephan Preibisch, et al. 2012. “Fiji: an open- source platform for biological-image analysis.” Nature Methods 9: 676–82.\nSerra, Jean. 1983. Image Analysis and Mathematical Morphology. Academic Press.\nSetiadi, A Francesca, Nelson C Ray, Holbrook E Kohrt, Adam Kapelner, Valeria Carcamo-Cavazos, Edina B Levic, Sina Yadegarynia, et al. 2010. “Quantitative, Architectural Analysis of Immune Cell Subsets in Tumor-Draining Lymph Nodes from Breast Cancer Patients and Healthy Lymph Nodes.” PLoS One 5 (8): e12420.\nVetterli, Martin, Jelena Kovačević, and Vivek Goyal. 2014. Foundations of Signal Processing. Cambridge University Press.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html",
    "href": "12-chap.html",
    "title": "14  12.1 Goals for this chapter",
    "section": "",
    "text": "14.1 12.2 What are the data?\nIn a supervised learning setting, we have a yardstick or plumbline to judge how well we are doing: the response itself.\nA frequent question in biological and biomedical applications is whether a property of interest (say, disease type, cell type, the prognosis of a patient) can be “predicted”, given one or more other properties, called the predictors. Often we are motivated by a situation in which the property to be predicted is unknown (it lies in the future, or is hard to measure), while the predictors are known. The crucial point is that we learn the prediction rule from a set of training data in which the property of interest is also known. Once we have the rule, we can either apply it to new data, and make actual predictions of unknown outcomes; or we can dissect the rule with the aim of better understanding the underlying biology.\nCompared to unsupervised learning and what we have seen in Chapters 5, 7 and 9, where we do not know what we are looking for or how to decide whether our result is “right”, we are on much more solid ground with supervised learning: the objective is clearly stated, and there are straightforward criteria to measure how well we are doing.\nThe central issues in supervised learning 1 are overfitting and generalizability : did we just learn the training data “by heart” by constructing a rule that has 100% accuracy on the training data, but would perform poorly on any new data? Or did our rule indeed pick up some of the pertinent patterns in the system being studied, which will also apply to yet unseen new data? (Figure 12.1)\n1 Sometimes the term statistical learning is used, more or less exchangeably.\nFigure 12.1: An example for overfitting : two regression lines are fit to data in the \\((x, y)\\)-plane (black points). We can think of such a line as a rule that predicts the \\(y\\)-value, given an \\(x\\)-value. Both lines are smooth, but the fits differ in what is called their bandwidth , which intuitively can be interpreted their stiffness. The blue line seems overly keen to follow minor wiggles in the data, while the orange line captures the general trend but is less detailed. The effective number of parameters needed to describe the blue line is much higher than for the orange line. Also, if we were to obtain additional data, it is likely that the blue line would do a worse job than the orange line in modeling the new data. We’ll formalize these concepts –training error and test set error– later in this chapter. Although exemplified here with line fitting, the concept applies more generally to prediction models.\nIn this chapter we will:\nThe basic data structure for both supervised and unsupervised learning is (at least conceptually) a dataframe, where each row corresponds to an object and the columns are different features (usually numerical values) of the objects2. While in unsupervised learning we aim to find (dis)similarity relationships between the objects based on their feature values (e.g., by clustering or ordination), in supervised learning we aim to find a mathematical function (or a computational algorithm) that predicts the value of one of the features from the other features. Many implementations require that there are no missing values, whereas other methods can be made to work with some amount of missing data.\n2 This is a simplified description. Machine learning is a huge field, and lots of generalizations of this simple conceptual picture have been made. Already the construction of relevant features is an art by itself — we have seen examples with images of cells in Chapter 11, and more generally there are lots of possibilities to extract features from images, sounds, movies, free text, \\(…\\) Moreover, there is a variant of machine learning methods called kernel methods that do not need features at all; instead, kernel methods use distances or measures of similarity between objects. It may be easier, for instance, to define a measure of similarity between two natural language text objects than to find relevant numerical features to represent them. Kernel methods are beyond the scope of this book.\nThe feature that we select over all the others with the aim of predicting is called the objective or the response. Sometimes the choice is natural, but sometimes it is also instructive to reverse the roles, especially if we are interested in dissecting the prediction function for the purpose of biological understanding, or in disentangling correlations from causation.\nThe framework for supervised learning covers both continuous and categorical response variables. In the continuous case we also call it regression , in the categorical case, classification. It turns out that this distinction is not a detail, as it has quite far-reaching consequences for the choice of loss function (Section 12.5) and thus the choice of algorithm (Friedman 1997).\nThe first question to consider in any supervised learning task is how the number of objects compares to the number of predictors. The more objects, the better, and much of the hard work in supervised learning has to do with overcoming the limitations of having a finite (and typically, too small) training set.\nFigure 12.2: In supervised learning, we assign two different roles to our variables. We have labeled the explanatory variables \\(X\\) and the response variable(s) \\(Y\\). There are also two different sets of observations: the training set \\(X_\\) and \\(Y_\\) and the test set \\(X_v\\) and \\(Y_v\\). (The subscripts refer to alternative names for the two sets: “learning” and “validation”.)\n__\nTask\nGive examples where we have encountered instances of supervised learning with a categorical response in this book.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html#what-are-the-data",
    "href": "12-chap.html#what-are-the-data",
    "title": "14  12.1 Goals for this chapter",
    "section": "",
    "text": "14.1.1 12.2.1 Motivating examples\n\n14.1.1.1 Predicting diabetes type\nThe diabetes dataset (Reaven and Miller 1979) presents three different groups of diabetes patients and five clinical variables measured on them.\ndata(\"diabetes\", package = \"rrcov\")\nhead(diabetes)__\n\n\n    rw fpg glucose insulin sspg  group\n1 0.81  80     356     124   55 normal\n2 0.95  97     289     117   76 normal\n3 0.94 105     319     143  105 normal\n4 1.04  90     356     199  108 normal\n5 1.00  90     323     240  143 normal\n6 0.76  86     381     157  165 normal\nThe univariate distributions (more precisely, some density estimates of them) are shown in Figure 12.3.\nlibrary(\"reshape2\")\nggplot(melt(diabetes, id.vars = \"group\"), aes(x = value, col = group)) +\n geom_density() + facet_wrap( ~variable, ncol = 1, scales = \"free\") +\n theme(legend.position = \"bottom\")__\n\nFigure 12.3: We see already from the one-dimensional distributions that some of the individual variables could potentially predict which group a patient is more likely to belong to. Our goal is to combine variables to improve over such one-dimensional prediction models.\nThe variables are explained in the manual page of the dataset, and in the paper (Reaven and Miller 1979):\n\nrw: relative weight\nfpg: fasting plasma glucose\nglucose: area under the plasma glucose curve for the three hour oral glucose tolerance test (OGTT)\ninsulin: area under the plasma insulin curve for the OGTT\nsspg: steady state plasma glucose response\ngroup: normal, chemical diabetes and overt diabetes\n\n\n\n14.1.1.2 Predicting cellular phenotypes\nNeumann et al. (2010) observed human cancer cells using live-cell imaging. The cells were genetically engineered so that their histones were tagged with a green fluorescent protein (GFP). A genome- wide RNAi library was applied to the cells, and for each siRNA perturbation, movies of a few hundred cells were recorded for about two days, to see what effect the depletion of each gene had on cell cycle, nuclear morphology and cell proliferation. Their paper reports the use of an automated image classification algorithm that quantified the visual appearance of each cell’s nucleus and enabled the prediction of normal mitosis states or aberrant nuclei. The algorithm was trained on the data from around 3000 cells that were annotated by a human expert. It was then applied to almost 2 billions images of nuclei (Figure 12.4). Using automated image classification provided scalability (annotating 2 billion images manually would take a long time) and objectivity.\n\nFigure 12.4: The data were images of \\(2^9\\) nuclei from movies. The images were segmented to identify the nuclei, and numeric features were computed for each nucleus, corresponding to size, shape, brightness and lots of other more or less abstract quantitative summaries of the joint distribution of pixel intensities. From the features, the cells were classified into 16 different nuclei morphology classes, represented by the rows of the barplot. Representative images for each class are shown in black and white in the center column. The class frequencies, which are very unbalanced, are shown by the lengths of the bars.\n\n\n14.1.1.3 Predicting embryonic cell states\nWe will revisit the mouse embryo data (Ohnishi et al. 2014), which we have already seen in Chapters 3, 5 and 7. We’ll try to predict cell state and genotype from the gene expression measurements in Sections 12.3.2 and 12.6.3.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html#linear-discrimination",
    "href": "12-chap.html#linear-discrimination",
    "title": "14  12.1 Goals for this chapter",
    "section": "14.2 12.3 Linear discrimination",
    "text": "14.2 12.3 Linear discrimination\nWe start with one of the simplest possible discrimination problems3: we have objects described by two continuous features (so the objects can be thought of as points in the 2D plane) and falling into three groups. Our aim is to define class boundaries, which are lines in the 2D space.\n3 Arguably the simplest possible problem is a single continuous feature, two classes, and the task of finding a single threshold to discriminate between the two groups – as in Figure 6.2.\n\n14.2.1 12.3.1 Diabetes data\nLet’s see whether we can predict the group from the sspg and glucose variables in the diabetes data. It’s always a good idea to first visualise the data (Figure 12.5).\nggdb = ggplot(mapping = aes(x = sspg, y = glucose)) +\n  geom_point(aes(colour = group), data = diabetes)\nggdb __\n\nFigure 12.5: Scatterplot of two of the variables in the diabetes data. Each point is a sample, and the color indicates the diabetes type as encoded in the group variable.\nWe’ll start with a method called linear discriminant analysis (LDA). This method is a foundation stone of classification, many of the more complicated (and sometimes more powerful) algorithms are really just generalizations of LDA.\nlibrary(\"MASS\")\ndiabetes_lda = lda(group ~ sspg + glucose, data = diabetes)\ndiabetes_lda __\n\n\nCall:\nlda(group ~ sspg + glucose, data = diabetes)\n\nPrior probabilities of groups:\n   normal  chemical     overt \n0.5241379 0.2482759 0.2275862 \n\nGroup means:\n             sspg   glucose\nnormal   114.0000  349.9737\nchemical 208.9722  493.9444\novert    318.8788 1043.7576\n\nCoefficients of linear discriminants:\n                LD1         LD2\nsspg    0.005036943 -0.01539281\nglucose 0.005461400  0.00449050\n\nProportion of trace:\n   LD1    LD2 \n0.9683 0.0317 \n\n\nghat = predict(diabetes_lda)$class\ntable(ghat, diabetes$group)__\n\n\n          \nghat       normal chemical overt\n  normal       69       12     1\n  chemical      7       24     6\n  overt         0        0    26\n\n\nmean(ghat != diabetes$group)__\n\n\n[1] 0.1793103\n__\nQuestion 12.1\nWhat do the different parts of the above output mean?\nNow, let’s visualise the LDA result. We are going to plot the prediction regions for each of the three groups. We do this by creating a grid of points and using our prediction rule on each of them. We’ll then also dig a bit deeper into the mechanics of LDA and plot the class centers (diabetes_lda$means) and ellipses that correspond to the fitted covariance matrix (diabetes_lda$scaling). Assembling this visualization requires us to write a bit of code.\nmake1Dgrid = function(x) {\n  rg = grDevices::extendrange(x)\n  seq(from = rg[1], to = rg[2], length.out = 100)\n}__\nSet up the points for prediction, a \\(100 \\) grid that covers the data range.\ndiabetes_grid = with(diabetes,\n  expand.grid(sspg = make1Dgrid(sspg),\n              glucose = make1Dgrid(glucose)))__\nDo the predictions.\ndiabetes_grid$ghat =\n  predict(diabetes_lda, newdata = diabetes_grid)$class __\nThe group centers.\ncenters = diabetes_lda$means __\nCompute the ellipse. We start from a unit circle (approximated by a polygon with 360 sides) and apply the corresponding affine transformation from the LDA output.\nunitcircle = exp(1i * seq(0, 2*pi, length.out = 360)) |&gt;\n          (\\(z) cbind(Re(z), Im(z)))() \nellipse = unitcircle %*% solve(diabetes_lda$scaling) |&gt; as_tibble()__\nAll three ellipses, one for each group center.\nlibrary(\"dplyr\")\nellipses = lapply(rownames(centers), function(gr) {\n  mutate(ellipse,\n     sspg    = sspg    + centers[gr, \"sspg\"],\n     glucose = glucose + centers[gr, \"glucose\"],\n     group   = gr)\n}) |&gt; bind_rows()__\nNow we are ready to plot (Figure 12.6).\nggdb + geom_raster(aes(fill = ghat),\n            data = diabetes_grid, alpha = 0.25, interpolate = TRUE) +\n    geom_point(data = as_tibble(centers), pch = \"+\", size = 8) +\n    geom_path(aes(colour = group), data = ellipses) +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0))__\n\nFigure 12.6: As Figure 12.5, with the classification regions from the LDA model shown. The three ellipses represent the class centers and the covariance matrix of the LDA model; note that there is only one covariance matrix, which is the same for all three classes. Therefore also the sizes and orientations of the ellipses are the same for the three classes, only their centers differ. They represent contours of equal class membership probability.\n__\nQuestion 12.2\nWhy is the boundary between the prediction regions for chemical and overt not perpendicular to the line between the group centers?\n__\nSolution\n__\nThe boundaries would be perpendicular if the ellipses were circles. In general, a boundary is tangential to the contours of equal class probabilities, and due the elliptic shape of the contours, a boundary is in general not perpendicular to the line between centers.\n__\nQuestion 12.3\nHow confident would you be about the predictions in those areas of the 2D plane that are far from all of the cluster centers?\n__\nSolution\n__\nPredictions that are far from any cluster center should be assessed critically, as this amounts to an extrapolation into regions where the LDA model may not be very good and/or there may be no training data nearby to support the prediction. We could use the distance to the nearest center as a measure of confidence in the prediction for any particular point; although we will see that resampling and cross-validation based methods offer more generic and usually more reliable measures.\n__\nQuestion 12.4\nWhy is the boundary between the prediction regions for normal and chemical not half-way between the centers, but shifted in favor of normal? Hint: have a look at the prior argument of lda. Try again with uniform prior.\n__\nSolution\n__\nThe result of the following code chunk is shown in Figure 12.7. The suffix _up is short for “uniform prior”.\ndiabetes_up = lda(group ~ sspg + glucose, data = diabetes,\n  prior = (\\(n) rep(1/n, n)) (nlevels(diabetes$group)))\n\ndiabetes_grid$ghat_up =\n  predict(diabetes_up, newdata = diabetes_grid)$class\n\nstopifnot(all.equal(diabetes_up$means, diabetes_lda$means))\n\nellipse_up  = unitcircle %*% solve(diabetes_up$scaling) |&gt; as_tibble()\nellipses_up = lapply(rownames(centers), function(gr) {\n  mutate(ellipse_up,\n     sspg    = sspg    + centers[gr, \"sspg\"],\n     glucose = glucose + centers[gr, \"glucose\"],\n     group   = gr)\n}) |&gt; bind_rows()\n\nggdb + geom_raster(aes(fill = ghat_up),\n            data = diabetes_grid, alpha = 0.4, interpolate = TRUE) +\n    geom_point(data = data.frame(centers), pch = \"+\", size = 8) +\n    geom_path(aes(colour = group), data = ellipses_up) +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0))__\n\nFigure 12.7: As Figure 12.6, but with uniform class priors.\nThe stopifnot line confirms that the class centers are the same, as they are independent of the prior. The joint covariance is not.\n__\nQuestion 12.5\nFigures 12.6 and 12.7 show both the fitted LDA model, through the ellipses, and the prediction regions, through the area coloring. What part of this visualization is generic for all sorts of classification methods, what part is method-specific?\n__\nSolution\n__\nThe prediction regions can be shown for any classification method, including a “black box” method. The cluster centers and ellipses in Figures 12.6 and 12.7 are method-specific.\n__\nQuestion 12.6\nWhat is the difference in the prediction accuracy if we use all 5 variables instead of just glucose and sspg?\n__\nSolution\n__\ndiabetes_lda5 = lda(group ~ rw + fpg + glucose + sspg + insulin, data = diabetes)\ndiabetes_lda5 __\n\n\nCall:\nlda(group ~ rw + fpg + glucose + sspg + insulin, data = diabetes)\n\nPrior probabilities of groups:\n   normal  chemical     overt \n0.5241379 0.2482759 0.2275862 \n\nGroup means:\n                rw       fpg   glucose     sspg  insulin\nnormal   0.9372368  91.18421  349.9737 114.0000 172.6447\nchemical 1.0558333  99.30556  493.9444 208.9722 288.0000\novert    0.9839394 217.66667 1043.7576 318.8788 106.0000\n\nCoefficients of linear discriminants:\n                  LD1          LD2\nrw       1.3624356881 -3.784142444\nfpg     -0.0336487883  0.036633317\nglucose  0.0125763942 -0.007092017\nsspg     0.0042431866  0.001134070\ninsulin -0.0001022245 -0.006173424\n\nProportion of trace:\n   LD1    LD2 \n0.8812 0.1188 \n\n\nghat5 = predict(diabetes_lda5)$class\ntable(ghat5, diabetes$group)__\n\n\n          \nghat5      normal chemical overt\n  normal       73        5     1\n  chemical      3       31     5\n  overt         0        0    27\n\n\nmean(ghat5 != diabetes$group)__\n\n\n[1] 0.09655172\n__\nQuestion 12.7\nInstead of approximating the prediction regions by classification from a grid of points, compute the separating lines explicitly from the linear determinant coefficients.\n__\nSolution\n__\nSee Section 4.3, Equation (4.10) in (Hastie, Tibshirani, and Friedman 2008).\n\n\n14.2.2 12.3.2 Predicting embryonic cell state from gene expression\nAssume that we already know that the four genes FN1 , TIMD2 , GATA4 and SOX7 are relevant to the classification task4. We want to build a classifier that predicts the developmental time (embryonic days: E3.25, E3.5, E4.5). We load the data and select four corresponding probes.\n4 Later in this chapter we will see methods that can drop this assumption and screen all available features.\nlibrary(\"Hiiragi2013\")__\n\n\nIn chunk 'loadHiiragi2': Warning: replacing previous import 'boot::logit' by 'gtools::logit' whenloading 'Hiiragi2013'\n\n\nIn chunk 'loadHiiragi2': Warning: replacing previous import 'boot::inv.logit' by 'gtools::inv.logit'when loading 'Hiiragi2013'\n\n\ndata(\"x\")\nprobes = c(\"1426642_at\", \"1418765_at\", \"1418864_at\", \"1416564_at\")\nembryoCells = t(Biobase::exprs(x)[probes, ]) |&gt; as_tibble() |&gt;\n  mutate(Embryonic.day = x$Embryonic.day) |&gt;\n  dplyr::filter(x$genotype == \"WT\")__\nWe can use the Bioconductor annotation package associated with the microarray to verify that the probes correspond to the intended genes.\nannotation(x)__\n\n\n[1] \"mouse4302\"\n\n\nlibrary(\"mouse4302.db\")\nanno = AnnotationDbi::select(mouse4302.db, keys = probes,\n                             columns = c(\"SYMBOL\", \"GENENAME\"))\nanno __\n\n\n     PROBEID SYMBOL                                            GENENAME\n1 1426642_at    Fn1                                       fibronectin 1\n2 1418765_at  Timd2 T cell immunoglobulin and mucin domain containing 2\n3 1418864_at  Gata4                              GATA binding protein 4\n4 1416564_at   Sox7                SRY (sex determining region Y)-box 7\n\n\nmt = match(anno$PROBEID, colnames(embryoCells))\ncolnames(embryoCells)[mt] = anno$SYMBOL __\nNow we are ready to visualize the data in a pairs plot (Figure 12.8).\nlibrary(\"GGally\")\nggpairs(embryoCells, mapping = aes(col = Embryonic.day),\n  columns = anno$SYMBOL, upper = list(continuous = \"points\"))__\n\nFigure 12.8: Expression values of the discriminating genes, with the prediction target Embryonic.day shown by color.\nWe can now call lda on these data. The linear combinations LD1 and LD2 that serve as discriminating variables are given in the slot ed_lda$scaling of the output from lda.\nec_lda = lda(Embryonic.day ~ Fn1 + Timd2 + Gata4 + Sox7,\n             data = embryoCells)\nround(ec_lda$scaling, 1)__\n\n\n       LD1  LD2\nFn1   -0.2  0.4\nTimd2  0.5  0.0\nGata4 -0.1  0.6\nSox7  -0.7 -0.5\nFor the visualization of the learned model in Figure 12.9, we need to build the prediction regions and their boundaries by expanding the grid in the space of the two new coordinates LD1 and LD2.\nec_rot = predict(ec_lda)$x |&gt; as_tibble() |&gt;\n           mutate(ed = embryoCells$Embryonic.day)\nec_lda2 = lda(ec_rot[, 1:2], predict(ec_lda)$class)\nec_grid = with(ec_rot, expand.grid(\n  LD1 = make1Dgrid(LD1),\n  LD2 = make1Dgrid(LD2)))\nec_grid$edhat = predict(ec_lda2, newdata = ec_grid)$class\nggplot() +\n  geom_point(aes(x = LD1, y = LD2, colour = ed), data = ec_rot) +\n  geom_raster(aes(x = LD1, y = LD2, fill = edhat),\n            data = ec_grid, alpha = 0.4, interpolate = TRUE) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  coord_fixed()__\n\nFigure 12.9: LDA classification regions for Embryonic.day.\n__\nQuestion 12.8\nRepeat these analyses using quadratic discriminant analysis (qda). What difference do you see in the shape of the boundaries?\n__\nSolution\n__\nSee code below and Figure 12.10.\nlibrary(\"gridExtra\")\n\nec_qda = qda(Embryonic.day ~ Fn1 + Timd2 + Gata4 + Sox7,\n             data = embryoCells)\n\nvariables = colnames(ec_qda$means)\npairs = combn(variables, 2)\nlapply(seq_len(ncol(pairs)), function(i) {\n  grid = with(embryoCells,\n    expand.grid(x = make1Dgrid(get(pairs[1, i])),\n                y = make1Dgrid(get(pairs[2, i])))) |&gt;\n    `colnames&lt;-`(pairs[, i])\n\n  for (v in setdiff(variables, pairs[, i]))\n    grid[[v]] = median(embryoCells[[v]])\n\n  grid$edhat = predict(ec_qda, newdata = grid)$class\n\n  x &lt;- pairs[1,i]\n  y &lt;- pairs[2,i]\n  ggplot() + \n    geom_point(\n      data = embryoCells,\n      aes(x = .data[[x]], y = .data[[y]], colour = Embryonic.day)\n    ) +\n    geom_raster(\n      aes(x = .data[[x]], y = .data[[y]], fill = edhat),\n      data = grid, alpha = 0.4, interpolate = TRUE\n    ) +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0)) +\n    coord_fixed() +\n    if (i != ncol(pairs)) theme(legend.position = \"none\")\n}) |&gt; (\\(g) grid.arrange(grobs = g, ncol = 2))()__\n\nFigure 12.10: QDA for the mouse cell data. Shown are all pairwise plots of the four features. In each plot, the other two features are set to the median.\n__\nQuestion 12.9\nWhat happens if you call lda or qda with a lot more genes, say the first 1000, in the Hiiragi dataset?\n__\nSolution\n__\nlda(t(Biobase::exprs(x))[, 1:1000], x$Embryonic.day)__\n\n\n warnings()\nqda(t(Biobase::exprs(x))[, 1:1000], x$Embryonic.day)__\n\n\nError in qda.default(x, grouping, ...): some group is too small for 'qda'\nThe lda function manages to fit a model, but complains (with the warning) about the fact that there are more variables than replicates, which means that the variables are not linearly independent, and thus are redundant of each other. The qda function aborts with an error, since the QDA model with so many parameters cannot be fitted from the available data (at least, without making further assumptions, such as some sort of regularization, which it is not equipped for).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html#machine-learning-vs-rote-learning",
    "href": "12-chap.html#machine-learning-vs-rote-learning",
    "title": "14  12.1 Goals for this chapter",
    "section": "14.3 12.4 Machine learning vs rote learning",
    "text": "14.3 12.4 Machine learning vs rote learning\nComputers are really good at memorizing facts. In the worst case, a machine learning algorithm is a roundabout way of doing this5. The central goal in statistical learning, however, is generalizability. We want an algorithm that is able to generalize, i.e., interpolate and extrapolate from given data to make good predictions about future data.\n5 The not-so roundabout way is database technologies.\nLet’s look at the following example. We generate random data (rnorm) for n objects, with different numbers of features (given by p). We train a LDA on these data and compute the misclassification rate , i.e., the fraction of times the prediction is wrong (pred != resp).\np = 2:21\nn = 20\n\nmcl = lapply(p, function(pp) {\n  replicate(100, {\n    xmat = matrix(rnorm(n * pp), nrow = n)\n    resp = sample(c(\"apple\", \"orange\"), n, replace = TRUE)\n    fit  = lda(xmat, resp)\n    pred = predict(fit)$class\n    mean(pred != resp)\n  }) |&gt; mean() |&gt; (\\(x) tibble(mcl = x, p = pp))()\n}) |&gt; bind_rows()__\n\n\n ggplot(mcl, aes(x = p, y = mcl)) + \n  geom_line() + geom_point() +\n  ylab(\"Misclassification rate\")__\n\nFigure 12.11: Misclassification rate of LDA applied to random data. While the number of observations n is held constant (at 20), we are increasing the number of features p starting from 2 up to 21. The misclassification rate becomes almost zero as p approaches 20. The LDA model becomes so elaborate and over-parameterized that it manages to learn the random labels “by heart”. (As p becomes even larger, the “performance” degrades again somewhat, apparently due to numerical properties of the lda implementation used here.)\n__\nQuestion 12.10\nWhat is the purpose of the replicate loop in the above code? What happens if you omit it (or replace the 100 by 1)?\n__\nSolution\n__\nFor each single replicate, the curve is a noisier version of Figure 12.11. Averaging the measured misclassifications rate over 100 replicates makes the estimate more stable. We can do this since we are working with simulated data.\nFigure 12.11 seems to imply that we can perfectly predict random labels from random data, if we only fit a complex enough model, i.e., one with many parameters. How can we overcome such an absurd conclusion? The problem with the above code is that the model performance is evaluated on the same data on which it was trained. This generally leads to positive bias, as you see in this crass example. How can we overcome this problem? The key idea is to assess model performance on different data than those on which the model was trained.\n\n14.3.1 12.4.1 Cross-validation\nA naive approach might be to split the data in two halves, and use the first half for learning (“training”) and the second half for assessment (“testing”). It turns out that this is needlessly variable and needlessly inefficient. It is needlessly variable, since by splitting the data only once, our results can be quite affected by how the split happens to fall. It seems better to do the splitting many times, and average. This will give us more stable results. It is needlessly inefficient, since the performance of machine learning algorithms depends on the number of observations, and the performance measured on half the data is likely6 to be worse than what it is with all the data. For this reason, it is better to use unequal sizes of training and test data. In the extreme case, we’ll use as much as \\(n-1\\) observations for training, and the remaining one for testing. After we’ve done this likewise for all observations, we can average our performance metric. This is called leave- one-out cross-validation.\n6 Unless we have such an excess of data that it doesn’t matter.\n\nSee Chapter Model Assessment and Selection in the book by Hastie, Tibshirani, and Friedman (2008) for further discussion on these trade-offs.\nAn alternative is \\(k\\) -fold cross-validation, where the observations are repeatedly split into a training set of size of around \\(n(k-1)/k\\) and a test set of size of around \\(n/k\\). Both alternatives have pros and contras, and there is not a universally best choice. An advantage of leave- one-out is that the amount of data used for training is close to the maximally available data; this is especially important if the sample size is limiting and “every little matters” for the algorithm. A drawback of leave-one-out is that the training sets are all very similar, so they may not model sufficiently well the kind of sampling changes to be expected if a new dataset came along. For large \\(n\\), leave-one-out cross-validation can be needlessly time-consuming.\nestimate_mcl_loocv = function(x, resp) {\n  vapply(seq_len(nrow(x)), function(i) {\n    fit  = lda(x[-i, ], resp[-i])\n    ptrn = predict(fit, newdata = x[-i,, drop = FALSE])$class\n    ptst = predict(fit, newdata = x[ i,, drop = FALSE])$class\n    c(train = mean(ptrn != resp[-i]), test = (ptst != resp[i]))\n  }, FUN.VALUE = numeric(2)) |&gt; rowMeans() |&gt; t() |&gt; as_tibble()\n}\n\nxmat = matrix(rnorm(n * last(p)), nrow = n)\nresp = sample(c(\"apple\", \"orange\"), n, replace = TRUE)\n\nmcl = lapply(p, function(k) {\n  estimate_mcl_loocv(xmat[, 1:k], resp)\n}) |&gt; bind_rows() |&gt; data.frame(p) |&gt; melt(id.var = \"p\")__\n\n\n ggplot(mcl, aes(x = p, y = value, col = variable)) + geom_line() +\n  geom_point() + ylab(\"Misclassification rate\")__\n\nFigure 12.12: Cross-validation: the misclassification rate of LDA applied to random data, when evaluated on test data that were not used for learning, hovers around 0.5 independent of p. The misclassification rate on the training data is also shown. It behaves similar to what we already saw in Figure 12.11.\nThe result is show in Figure 12.12.\n__\nQuestion 12.11\nWhy are the curves in Figure 12.12 more variable (“wiggly”) than in Figure 12.11? How can you overcome this?\n__\nSolution\n__\nOnly one dataset (xmat, resp) was used to calculate Figure 12.12, whereas for Figure 12.11, we had the data generated within a replicate loop. You could similarly extend the above code to average the misclassification rate curves over many replicate simulated datasets.\n\n\n14.3.2 12.4.2 The curse of dimensionality\nIn Section 12.4.1 we have seen overfitting and cross-validation on random data, but how does it look if there is in fact a relevant class separation?\np   = 2:20\nmcl = replicate(100, {\n  xmat = matrix(rnorm(n * last(p)), nrow = n)\n  resp = sample(c(\"apple\", \"orange\"), n, replace = TRUE)\n  xmat[, 1:6] = xmat[, 1:6] + as.integer(factor(resp))\n\n  lapply(p, function(k) {\n    estimate_mcl_loocv(xmat[, 1:k], resp)\n  }) |&gt; bind_rows() |&gt; cbind(p = p) |&gt; melt(id.var = \"p\")\n}, simplify = FALSE) |&gt; bind_rows()__\n\n\nmcl = group_by(mcl, p, variable) |&gt; summarise(value = mean(value))\n\nggplot(mcl, aes(x = p, y = value, col = variable)) + geom_line() +\n   geom_point() + ylab(\"Misclassification rate\")__\n\nFigure 12.13: As we increase the number of features included in the model, the misclassification rate initially improves; as we start including more and more irrelevant features, it increases again, as we are fitting noise.\n\nFigure 12.14: Idealized version of Figure 12.13, from Hastie, Tibshirani, and Friedman (2008). A recurrent goal in machine learning is finding the sweet spot in the variance &lt;- bias trade- off.\nThe result is shown in Figure 12.13. The group centers are the vectors (in \\(^{20}\\)) given by the coordinates \\((1, 1, 1, 1, 1, 1, 0, 0, 0, …)\\) (apples) and \\((2, 2, 2, 2, 2, 2, 0, 0, 0, …)\\) (oranges), and the optimal decision boundary is the hyperplane orthogonal to the line between them. For \\(p\\) smaller than \\(6\\), the decision rule cannot reach this hyperplane – it is biased. As a result, the misclassification rate is suboptimal, and it decreases with \\(p\\). But what happens for \\(p\\) larger than \\(6\\)? The algorithm is, in principle, able to model the optimal hyperplane, and it should not be distracted by the additional features. The problem is that it is. The more additional features enter the dataset, the higher the probability that one or more of them happen to fall in a way that they look like good, discriminating features in the training data – only to mislead the classifier and degrade its performance in the test data. Shortly we’ll see how to use penalization to (try to) control this problem.\nThe term curse of dimensionality was coined by Bellman (1961). It refers to the fact that high- dimensional spaces are very hard, if not impossible, to sample thoroughly: for instance, to cover a 2-dimensional square of side length 1 with grid points that are 0.1 apart, we need \\(10^2=100\\) points. In 100 dimensions, we need \\(10^{100}\\) – which is more than the number of protons in the universe. In genomics, we often aim to fit models to data with thousands of features. Also our intuitions about distances between points or about the relationship between a volume and its surface break down in a high-dimensional settings. We’ll explore some of the weirdnesses of high-dimensional spaces in the next few questions.\n__\nQuestion 12.12\nAssume you have a dataset with 1 000 000 data points in \\(p\\) dimensions. The data are uniformly distributed in the unit hybercube (i.e., all features lie in the interval \\([0,1]\\)). What’s the side length of a hybercube that can be expected to contain just 10 of the points, as a function of \\(p\\)?\n__\nSolution\n__\nSee Figure 12.15.\nsideLength = function(p, pointDensity = 1e6, pointsNeeded = 10)\n  (pointsNeeded / pointDensity) ^ (1 / p)\nggplot(tibble(p = 1:400, sideLength = sideLength(p)),\n       aes(x = p, y = sideLength)) + geom_line(col = \"red\") +\n  geom_hline(aes(yintercept = 1), linetype = 2)__\n\nFigure 12.15: Side length of a \\(p\\)-dimensional hybercube expected to contain 10 points out of 1 million uniformly distributed ones, as a function of the \\(p\\). While for \\(p=1\\), this length is conveniently small, namely \\(10/106=10{-5}\\), for larger \\(p\\) it approaches 1, i.,e., becomes the same as the range of each the features. This means that a “local neighborhood” of 10 points encompasses almost the same data range as the whole dataset.\nNext, let’s look at the relation between inner regions of the feature space versus its boundary regions. Generally speaking, prediction at the boundaries of feature space is more difficult than in its interior, as it tends to involve extrapolation, rather than interpolation. In the next question you’ll see how this difficulty explodes with feature space dimension.\n__\nQuestion 12.13\nWhat fraction of a unit cube’s total volume is closer than 0.01 to any of its surfaces, as a function of the dimension?\n__\nSolution\n__\nSee code below and Figure 12.16.\ntibble(\n  p = 1:400,\n  volOuterCube = 1 ^ p,\n  volInnerCube = 0.98 ^ p,  # 0.98 = 1 - 2 * 0.01\n  `V(shell)` = volOuterCube - volInnerCube) |&gt;\nggplot(aes(x = p, y =`V(shell)`)) + geom_line(col = \"blue\")__\n\nFigure 12.16: Fraction of a unit cube’s total volume that is in its “shell” (here operationalised as those points that are closer than 0.01 to its surface) as a function of the dimension \\(p\\).\n__\nQuestion 12.14\nWhat is the coefficient of variation (ratio of standard deviation over average) of the distance between two randomly picked points in the unit hypercube, as a function of the dimension?\n__\nSolution\n__\nWe solve this one by simulation. We generate n pairs of random points in the hypercube (x1, x2) and compute their Euclidean distances. See Figure 12.17. This result can also be predicted from the central limit theorem.\nn = 1000\ndf = tibble(\n  p = round(10 ^ seq(0, 4, by = 0.25)),\n  cv = vapply(p, function(k) {\n    x1 = matrix(runif(k * n), nrow = n)\n    x2 = matrix(runif(k * n), nrow = n)\n    d = sqrt(rowSums((x1 - x2)^2))\n    sd(d) / mean(d)\n  }, FUN.VALUE = numeric(1)))\nggplot(df, aes(x = log10(p), y = cv)) + geom_line(col = \"orange\") +\n  geom_point()__\n\nFigure 12.17: Coefficient of variation (CV) of the distance between randomly picked points in the unit hypercube, as a function of the dimension. As the dimension increases, everybody is equally far away from everyone else: there is almost no variation in the distances any more.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html#objective-functions",
    "href": "12-chap.html#objective-functions",
    "title": "14  12.1 Goals for this chapter",
    "section": "14.4 12.5 Objective functions",
    "text": "14.4 12.5 Objective functions\nWe’ve already seen the misclassification rate (MCR) used to assess our classification performance in Figures 12.11–12.13. Its population version is defined as\n\\[ = , = , \\]\nand for a finite sample\n\\[ = {i=1}^n 𝟙{ y_i}. \\]\nThis is not the only choice we could make. Perhaps we care more about the misclassification of apples as oranges than vice versa, and we can reflect this by introducing weights that depend on the type of error made into the sum of Equation 12.2 (or the integral of Equation 12.1). This can get even more elaborate if we have more than two classes. Often we want to see the whole confusion table , which we can get via\ntable(truth, response)__\nAn important special case is binary classification with asymmetric costs – think about, say, a medical test. Here, the sensitivity (a.k.a. true positive rate or recall) is related to the misclassification of healthy as ill, and the specificity (or true negative rate) depends on the probability of misclassification of ill as healthy. Often, there is a single parameter (e.g., a threshold) that can be moved up and down, allowing a trade- off between sensitivity and specificity (and thus, equivalently, between the two types of misclassification). In those cases, we usually are not content to know the classifier performance at one single choice of threshold, but at many (or all) of them. This leads to receiver operating characteristic (ROC) or precision-recall curves.\n__\nQuestion 12.15\nWhat are the exact relationships between the per-class misclassification rates and sensitivity and specificity?\n__\nSolution\n__\nThe sensitivity or true positive rate is\n\\[ = , \\]\nwhere \\(\\) is the number of true positives and \\(\\) the number of all positives. The specificity or true negative rate is\n\\[ = , \\]\nwhere \\(\\) is the number of true negatives and \\(\\) the number of all negatives. See also https://en.wikipedia.org/wiki/Sensitivity_and_specificity\nAnother cost function can be computed from the Jaccard index , which we already saw in Chapter 5.\n\\[ J(A,B) = , \\]\nwhere \\(A\\) is the set of observations for which the true class is 1 (\\(A=\\{i,|,y_i=1\\}\\)) and \\(B\\) is the set of observations for which the predicted class is 1. The number \\(J\\) is between 0 and 1, and when \\(J\\) is large, it indicates high overlap of the two sets. Note that \\(J\\) does not depend on the number of observations for which both true and predicted class is 0 – so it is particularly suitable for measuring the performance of methods that try to find rare events.\nWe can also consider probabilistic class predictions, which come in the form \\((Y,|,X)\\). In this case, a possible risk function would be obtained by looking at distances between the true probability distribution and the estimated probability distributions. For two classes, the finite sample version of the \\(\\) is\n\\[ = -_{i=1}^n y_i(_i) + (1 - y_i)(1 - _i), \\]\nwhere \\(_i \\) is the prediction, and \\(y_i\\{0,1\\}\\) is the truth.\n\n\n\nNote that the \\log\\text{loss} will be infinite if a prediction is totally confident (\\hat{p}_i is exactly 0 or 1) but wrong.\n\n\nNote that the \\(\\) will be infinite if a prediction is totally confident (\\(_i\\) is exactly \\(0\\) or \\(1\\)) but wrong.\nFor continuous continuous response variables (regression), a natural choice is the mean squared error (MSE). It is the average squared error,\n\\[ = _{i=1}^n ( _i - Y_i )^2. \\]\nThe population version is defined analogously, by turning the summation into an integral as in Equations 12.1 and 12.2.\nStatisticians call functions like Equations 12.1—12.5 variously (and depending on context and predisposition) risk function , cost function , objective function 7.\n7 There is even an R package dedicated to evaluation of statistical learners called metrics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html#variancebias-trade-off",
    "href": "12-chap.html#variancebias-trade-off",
    "title": "14  12.1 Goals for this chapter",
    "section": "14.5 12.6 Variance–bias trade-off",
    "text": "14.5 12.6 Variance–bias trade-off\n\n\n\n\n\n\n\n\nFigure 12.18: In bull’s eye (a), the estimates are systematically off target, but in a quite reproducible manner. The green segment represents the bias. In bull’s eye (b), the estimates are not biased, as they are centered in the right place, however they have high variance. We can distinguish the two scenarios since we see the result from many shots. If we only had one shot and missed the bull’s eye, we could not easily know whether that’s because of bias or variance.\nAn important fact that helps us understand the tradeoffs when picking a statistical learning model is that the MSE is the sum of two terms, and often the choices we can make are such that one of those terms goes down while the other one goes up. The bias measures how different the average of all the different estimates is from the truth, and variance, how much an individual one might scatter from the average value (Figure 12.18). In applications, we often only get one shot, therefore being reliably almost on target can beat being right on the long term average but really off today. The decomposition\n\\[ = {} + {} \\]\nfollows by straightforward algebra.\nWhen trying to minimize the MSE, it is important to realize that sometimes we can pay the price of a small bias to greatly reduce variance, and thus overall improve MSE. We already encountered shrinkage estimation in Chapter 8. In classification (i.e., when we have categorical response variables), different objective functions than the MSE are used, and there is usually no such straightforward decomposition as in Equation 12.6. The good news is that we can usually go even much further than in the case of continuous responses with our trading biases for variance. This is because the discreteness of the response absorbs certain biases (Friedman 1997), so that the cost of higher bias is almost zero, while we still get the benefit of better (smaller) variance.\n\n14.5.1 12.6.1 Penalization\nIn high-dimensional statistics, we are constantly plagued by variance: there is just not enough data to fit all the possible parameters. One of the most fruitful ideas in high-dimensional statistics is penalization : a tool to actively control and exploit the variance-bias tradeoff. Penalization is part of a larger class of regularization methods that are used to ensure stable estimates.\nAlthough generalization of LDA to high-dimensional settings is possible (Clemmensen et al. 2011; Witten and Tibshirani 2011), it turns out that logistic regression is a more general approach8, and therefore we’ll now switch to that, using the glmnet package.\n8 It fits into the framework of generalized linear models, which we encountered in Chapter 8.\nFor multinomial—or, for the special case of two classes, binomial—logistic regression models, the posterior log-odds between \\(k\\) classes and can be written in the form (see the section on Logistic Regression in the book by Hastie, Tibshirani, and Friedman (2008) for a more complete presentation):\n\\[ = ^0_i + _i x, \\]\nwhere \\(i=1,…,k-1\\) enumerates the different classes and the \\(k\\)-th class is chosen as a reference. The data matrix \\(x\\) has dimensions \\(np\\), where \\(n\\) is number of observations and \\(p\\) the number of features. The \\(p\\)-dimensional vector \\(_i\\) determines how the classification odds for class \\(i\\) versus class \\(k\\) depend on \\(x\\). The numbers \\(^0_i\\) are intercepts and depend, among other things, on the classes’ prior probabilities. Instead of the log odds 12.7 (i.e., ratios of class probabilities), we can also write down an equivalent model for the class probabilities themselves, and the fact that we here used the \\(k\\)-th class as a reference is an arbitrary choice, as the model estimates are equivariant under this choice (Hastie, Tibshirani, and Friedman 2008). The model is fit by maximising the log-likelihood \\((, ^0; x)\\), where \\(=(1,…,{k-1})\\) and analogously for \\(^0\\).\nSo far, so good. But as \\(p\\) gets larger, there is an increasing chance that some of the estimates go wildly off the mark, due to random sampling happenstances in the data (remember Figure 12.1). This is true even if for each individual coordinate of the vector \\(_i\\), the error distribution is bounded: the probabilty of there being one coordinate that is in the far tails increases the more coordiates there are, i.e., the larger \\(p\\) is.\nA related problem can also occur, not in 12.7, but in other, non-linear models, as the model dimension \\(p\\) increases while the sample size \\(n\\) remains the same: the likelihood landscape around its maximum becomes increasingly flat, and the maximum-likelihood estimate of the model parameters becomes more and more variable. Eventually, the maximum is no longer a point, but a submanifold, and the maximum likelihood estimate is unidentifiable. Both of these limitations can be overcome with a modification of the objective: instead of maximising the bare log-likelihood, we maximise a penalized version of it,\n\\[ = _(, ^0; x) + (), \\]\nwhere \\(\\) is a real number, and \\(\\) is a convex function, called the penalty function. Popular choices are \\(()=||^2\\) (ridge regression) and \\(()=||^1\\) (lasso).\n\n\n\nHere, |\\beta|\\nu=\\sum_i\\beta_i\\nu is the L_\\nu-norm of the vector \\beta. Variations are possible, for instead we could include in this summation only some but not all of the elements of \\beta; or we could scale different elements differently, for instance based on some prior belief of their scale and importance.\n\n\nHere, \\(||^=_ii^\\) is the \\(L\\)-norm of the vector \\(\\). Variations are possible, for instead we could include in this summation only some but not all of the elements of \\(\\); or we could scale different elements differently, for instance based on some prior belief of their scale and importance.\nIn the elastic net , ridge and lasso are hybridized by using the penalty function \\(()=(1-)||1+||2\\) with some further parameter \\(\\). The crux is, of course, how to choose the right \\(\\), and we will discuss that in the following.\n\n\n14.5.2 12.6.2 Example: predicting colon cancer from stool microbiome composition\nZeller et al. (2014) studied metagenome sequencing data from fecal samples of 156 humans that included colorectal cancer patients and tumor-free controls. Their aim was to see whether they could identify biomarkers (presence or abundance of certain taxa) that could help with early tumor detection. The data are available from Bioconductor through its ExperimentHub service under the identifier EH361.\nlibrary(\"ExperimentHub\")\neh = ExperimentHub()\nzeller = eh[[\"EH361\"]]__\n\n\ntable(zeller$disease)__\n\n\n       cancer large_adenoma             n small_adenoma \n           53            15            61            27 \n__\nQuestion 12.16\nExplore the eh object to see what other datasets there are.\n__\nSolution\n__\nType eh into the R prompt and study the output.\nFor the following, let’s focus on the normal and cancer samples and set the adenomas aside.\nzellerNC = zeller[, zeller$disease %in% c(\"n\", \"cancer\")]__\nBefore jumping into model fitting, as always it’s a good idea to do some exploration of the data. First, let’s look at the sample annotations. The following code prints the data from three randomly picked samples. (Only looking at the first ones, say with the R function head, is also an option, but may not be representative of the whole dataset).\npData(zellerNC)[ sample(ncol(zellerNC), 3), ]__\n\n\n                   subjectID age gender bmi country disease tnm_stage\nCCIS50148151ST-4-0    FR-503  87 female  15  france  cancer    t2n1m0\nCCIS16383318ST-4-0    FR-139  61 female  24  france       n      &lt;NA&gt;\nCCIS95097901ST-4-0    FR-696  52   male  24  france       n      &lt;NA&gt;\n                   ajcc_stage localization     fobt wif-1_gene_methylation_test\nCCIS50148151ST-4-0        iii           rc negative                    negative\nCCIS16383318ST-4-0       &lt;NA&gt;         &lt;NA&gt; negative                    negative\nCCIS95097901ST-4-0       &lt;NA&gt;         &lt;NA&gt; negative                    negative\n                     group bodysite ethnicity number_reads\nCCIS50148151ST-4-0     crc    stool     white     54709150\nCCIS16383318ST-4-0 control    stool     white     78085760\nCCIS95097901ST-4-0 control    stool     white     51567166\nNext, let’s explore the feature names:\n\n\n\nWe define the helper function formatfn to line wrap these long character strings for the available space here.\n\n\nWe define the helper function formatfn to line wrap these long character strings for the available space here.\nformatfn = function(x)\n   gsub(\"|\", \"| \", x, fixed = TRUE) |&gt; lapply(strwrap)\n\nrownames(zellerNC)[1:4]__\n\n\n[1] \"k__Bacteria\"                  \"k__Viruses\"                  \n[3] \"k__Bacteria|p__Firmicutes\"    \"k__Bacteria|p__Bacteroidetes\"\n\n\nrownames(zellerNC)[nrow(zellerNC) + (-2:0)] |&gt; formatfn()__\n\n\n[[1]]\n[1] \"k__Bacteria| p__Proteobacteria| c__Deltaproteobacteria|\"         \n[2] \"o__Desulfovibrionales| f__Desulfovibrionaceae| g__Desulfovibrio|\"\n[3] \"s__Desulfovibrio_termitidis\"                                     \n\n[[2]]\n[1] \"k__Viruses| p__Viruses_noname| c__Viruses_noname| o__Viruses_noname|\"\n[2] \"f__Baculoviridae| g__Alphabaculovirus|\"                              \n[3] \"s__Bombyx_mori_nucleopolyhedrovirus|\"                                \n[4] \"t__Bombyx_mori_nucleopolyhedrovirus_unclassified\"                    \n\n[[3]]\n[1] \"k__Bacteria| p__Proteobacteria| c__Deltaproteobacteria|\"         \n[2] \"o__Desulfovibrionales| f__Desulfovibrionaceae| g__Desulfovibrio|\"\n[3] \"s__Desulfovibrio_termitidis| t__GCF_000504305\"                   \nAs you can see, the features are a mixture of abundance quantifications at different taxonomic levels, from k ingdom over p hylum to s pecies. We could select only some of these, but here we continue with all of them. Next, let’s look at the distribution of some of the features. Here, we show an arbitrary choice of two, number 510 and 527; in practice, it is helpful to scroll through many such plots quickly to get an impression (Figure 12.19).\nggplot(melt(Biobase::exprs(zellerNC)[c(510, 527), ]), aes(x = value)) +\n    geom_histogram(bins = 25) +\n    facet_wrap( ~ Var1, ncol = 1, scales = \"free\")__\n\nFigure 12.19: Histograms of the distributions for two randomly selected features. The distributions are highly skewed, with many zero values and a thin, long tail of non-zero values.\nIn the simplest case, we fit model 12.7 as follows.\nlibrary(\"glmnet\")\nglmfit = glmnet(x = t(Biobase::exprs(zellerNC)),\n                y = factor(zellerNC$disease),\n                family = \"binomial\")__\nA remarkable feature of the glmnet function is that it fits 12.7 not only for one choice of \\(\\), but for all possible \\(\\)s at once. For now, let’s look at the prediction performance for, say, \\(\\). The name of the function parameter is s:\npredTrsf = predict(glmfit, newx = t(Biobase::exprs(zellerNC)),\n                   type = \"class\", s = 0.04)\ntable(predTrsf, zellerNC$disease)__\n\n\n        \npredTrsf cancer  n\n  cancer     51  0\n  n           2 61\nNot bad – but remember that this is on the training data, without cross- validation. Let’s have a closer look at glmfit. The glmnet package offers a a diagnostic plot that is worth looking at (Figure 12.20).\nplot(glmfit, col = RColorBrewer::brewer.pal(8, \"Dark2\"), lwd = sqrt(3), ylab = \"\")__\n\nFigure 12.20: Regularization paths for glmfit.\n__\nQuestion 12.17\nWhat are the \\(x\\)- and \\(y\\)-axes in Figure 12.20? What are the different lines?\n__\nSolution\n__\nConsult the manual page of the function plot.glmnet in the glmnet package.\nLet’s get back to the question of how to choose the parameter \\(\\). We could try many different choices –and indeed, all possible choices– of \\(\\), assess classification performance in each case using cross- validation, and then choose the best \\(\\).\n\n\n\nYou’ll already realize from the description of this strategy that if we optimize \\lambda in this way, the resulting apparent classification performance will likely be exaggerated. We need a truly independent dataset, or at least another, outer cross-validation loop to get a more realistic impression of the generalizability. We will get back to this question at the end of the chapter.\n\n\nYou’ll already realize from the description of this strategy that if we optimize \\(\\) in this way, the resulting apparent classification performance will likely be exaggerated. We need a truly independent dataset, or at least another, outer cross-validation loop to get a more realistic impression of the generalizability. We will get back to this question at the end of the chapter.\nWe could do so by writing a loop as we did in the estimate_mcl_loocv function in Section 12.4.1. It turns out that the glmnet package already has built-in functionality for that, with the function cv.glmnet, which we can use instead.\ncvglmfit = cv.glmnet(x = t(Biobase::exprs(zellerNC)),\n                     y = factor(zellerNC$disease),\n                     family = \"binomial\")\nplot(cvglmfit)__\n\nFigure 12.21: Diagnostic plot for cv.glmnet: shown is a measure of cross- validated prediction performance, the deviance, as a function of \\(\\). The dashed vertical lines show lambda.min and lambda.1se.\nThe diagnostic plot is shown in Figure 12.21. We can access the optimal value with\ncvglmfit$lambda.min __\n\n\n[1] 0.0529391\nAs this value results from finding a minimum in an estimated curve, it turns out that it is often too small, i.e., that the implied penalization is too weak. A heuristic recommended by the authors of the glmnet package is to use a somewhat larger value instead, namely the largest value of \\(\\) such that the performance measure is within 1 standard error of the minimum.\ncvglmfit$lambda.1se __\n\n\n[1] 0.08830775\n__\nQuestion 12.18\nHow does the confusion table look like for \\(=;\\)lambda.1se?\n__\nSolution\n__\ns0 = cvglmfit$lambda.1se\npredict(glmfit, newx = t(Biobase::exprs(zellerNC)),type = \"class\", s = s0) |&gt;\n    table(zellerNC$disease)__\n\n\n        \n         cancer  n\n  cancer     38  5\n  n          15 56\n__\nQuestion 12.19\nWhat features drive the classification?\n__\nSolution\n__\ncoefs = coef(glmfit)[, which.min(abs(glmfit$lambda - s0))]\ntopthree = order(abs(coefs), decreasing = TRUE)[1:3]\nas.vector(coefs[topthree])__\n\n\n[1] -71.471393  -8.770704  -1.465249\n\n\nformatfn(names(coefs)[topthree])__\n\n\n[[1]]\n[1] \"k__Bacteria| p__Candidatus_Saccharibacteria|\"      \n[2] \"c__Candidatus_Saccharibacteria_noname|\"            \n[3] \"o__Candidatus_Saccharibacteria_noname|\"            \n[4] \"f__Candidatus_Saccharibacteria_noname|\"            \n[5] \"g__Candidatus_Saccharibacteria_noname|\"            \n[6] \"s__candidate_division_TM7_single_cell_isolate_TM7b\"\n\n[[2]]\n[1] \"k__Bacteria| p__Firmicutes| c__Clostridia| o__Clostridiales|\"        \n[2] \"f__Ruminococcaceae| g__Subdoligranulum| s__Subdoligranulum_variabile\"\n\n[[3]]\n[1] \"k__Bacteria| p__Firmicutes| c__Clostridia| o__Clostridiales|\"\n[2] \"f__Lachnospiraceae| g__Lachnospiraceae_noname|\"              \n[3] \"s__Lachnospiraceae_bacterium_7_1_58FAA\"                      \n__\nQuestion 12.20\nHow do the results change if we transform the data, say, with the asinh transformation as we saw in Chapter 5?\n__\nSolution\n__\nSee Figure 12.22.\ncv.glmnet(x = t(asinh(Biobase::exprs(zellerNC))),\n          y = factor(zellerNC$disease),\n          family = \"binomial\") |&gt; plot()__\n\nFigure 12.22: like Figure 12.21, but using an \\(\\) transformation of the data.\n__\nQuestion 12.21\nWould a good classification performance on these data mean that this assay is ready for screening and early cancer detection?\n__\nSolution\n__\nNo. The performance here is measured on a set of samples in which the cases have similar prevalence as the controls. This serves well enough to explore the biology. However, in a real-life application, the cases will be much less frequent. To be practically useful, the assay must have a much higher specificity, i.e., rarely diagnose disease where there is none. To establish specificity, a much larger set of normal samples need to be tested.\n\n\n14.5.3 12.6.3 Example: classifying mouse cells from their expression profiles\nFigures 12.21 and 12.22 are textbook examples of how we expect the dependence of (cross-validated) classification performance versus model complexity (\\(\\)) to look. Now let’s get back to the mouse embryo cells data. We’ll try to classify the cells from embryonic day E3.25 with respect to their genotype.\nsx = x[, x$Embryonic.day == \"E3.25\"]\nembryoCellsClassifier = cv.glmnet(t(Biobase::exprs(sx)), sx$genotype,\n                family = \"binomial\", type.measure = \"class\")\nplot(embryoCellsClassifier)__\n\nFigure 12.23: Cross-validated misclassification error versus penalty parameter for the mouse cells data.\nIn Figure 12.23 we see that the misclassification error is (essentially) monotonously increasing with \\(\\), and is smallest for \\(\\), i.e., if we apply no penalization at all.\n__\nQuestion 12.22\nWhat is going on with these data?\n__\nSolution\n__\nIt looks that inclusion of more, and even of all features, does not harm the classification performance. In a way, these data are “too easy”. Let’s do a \\(t\\)-test for all features:\nmouse_de = rowttests(sx, \"genotype\")\nggplot(mouse_de, aes(x = p.value)) +\n  geom_histogram(boundary = 0, breaks = seq(0, 1, by = 0.01))__\n\nFigure 12.24: Histogram of p-values for the per-feature \\(t\\)-tests between genotypes in the E3.25 cells.\nThe result, shown in Figure 12.24, shows that large number of genes are differentially expressed, and thus informative for the class distinction. We can also compute the pairwise distances between all cells, using all features.\ndists = as.matrix(dist(scale(t(Biobase::exprs(x)))))\ndiag(dists) = +Inf __\nand then for each cell determine the class of its nearest neighbor\nnn = sapply(seq_len(ncol(dists)), function(i) which.min(dists[, i]))\ntable(x$sampleGroup, x$sampleGroup[nn]) |&gt; `colnames&lt;-`(NULL)__\n\n\n                 \n                  [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n  E3.25             33    0    0    0    3    0    0    0\n  E3.25 (FGF4-KO)    1   15    0    1    0    0    0    0\n  E3.5 (EPI)         2    0    3    0    6    0    0    0\n  E3.5 (FGF4-KO)     0    0    0    8    0    0    0    0\n  E3.5 (PE)          0    0    0    0   11    0    0    0\n  E4.5 (EPI)         0    0    0    0    2    2    0    0\n  E4.5 (FGF4-KO)     1    0    0    0    0    0    9    0\n  E4.5 (PE)          0    0    0    0    2    0    0    2\nUsing all features, the 1 nearest-neighbor classifier is correct in almost all cases, including for the E3.25 wildtype vs FGF4-KO distinction. This means that for these data, there is no apparent benefit in regularization or feature selection. Limitations of using all features might become apparent with truly new data, but that is out of reach for cross-validation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html#a-large-choice-of-methods",
    "href": "12-chap.html#a-large-choice-of-methods",
    "title": "14  12.1 Goals for this chapter",
    "section": "14.6 12.7 A large choice of methods",
    "text": "14.6 12.7 A large choice of methods\nWe have now seen three classification methods: linear discriminant analysis (lda), quadratic discriminant analysis (qda) and logistic regression using elastic net penalization (glmnet). In fact, there are hundreds of different learning algorithms9 available in R and its add-on packages. You can get an overview in the CRAN task view Machine Learning & Statistical Learning. Some examples are:\n9 For an introduction to the subject that uses R and provides many examples and exercises, we recommend (James et al. 2013).\n\nSupport vector machines: the function svm in the package e1071 ; ksvm in kernlab\nTree based methods in the packages rpart , tree , randomForest\nBoosting methods: the functions glmboost and gamboost in package mboost\nPenalizedLDA in the package PenalizedLDA , dudi.discr and dist.pcaiv in ade4).\n\nThe complexity and heterogeneity of choices of learning strategies, tuning parameters and evaluation criteria in each of these packages can be confusing. You will already have noted differences in the interfaces of the lda, qda and glmnet functions, i.e., in how they expect their input data to presented and what they return. There is even greater diversity across all the other packages and functions. At the same time, there are common tasks such as cross-validation, parameter tuning and performance assessment that are more or less the same no matter what specific method is used. As you have seen, e.g., in our estimate_mcl_loocv function, the looping and data shuffling involved led to rather verbose code.\nSo what to do if you want to try out and explore different learning algorithms? Fortunately, there are several projects that provide unified interfaces to the large number of different machine learning interfaces in R, and also try to provide “best practice” implementations of the common tasks such as parameter tuning and performance assessment. The two most well-known ones are the packages caret and mlr. Here were have a look at caret. You can get a list of supported methods through its getModelInfo function. There are quite a few, here we just show the first 8.\nlibrary(\"caret\")\ncaretMethods = names(getModelInfo())\nhead(caretMethods, 8)__\n\n\n[1] \"ada\"         \"AdaBag\"      \"AdaBoost.M1\" \"adaboost\"    \"amdai\"      \n[6] \"ANFIS\"       \"avNNet\"      \"awnb\"       \n\n\nlength(caretMethods)__\n\n\n[1] 239\nWe will check out a neural network method, the nnet function from the eponymous package. The parameter slot informs us on the the available tuning parameters10.\n10 They are described in the manual of the nnet function.\ngetModelInfo(\"nnet\", regex = FALSE)[[1]]$parameter __\n\n\n  parameter   class         label\n1      size numeric #Hidden Units\n2     decay numeric  Weight Decay\nLet’s try it out.\ntrnCtrl = trainControl(\n  method = \"repeatedcv\",\n  repeats = 3,\n  classProbs = TRUE)\ntuneGrid = expand.grid(\n  size = c(2, 4, 8),\n  decay = c(0, 1e-2, 1e-1))\nnnfit = train(\n  Embryonic.day ~ Fn1 + Timd2 + Gata4 + Sox7,\n  data = embryoCells,\n  method = \"nnet\",\n  tuneGrid  = tuneGrid,\n  trControl = trnCtrl,\n  metric = \"Accuracy\")__\nThat’s quite a mouthful, but the nice thing is that this syntax is standardized and applies across many different methods. All you need to do specify the name of the method and the grid of tuning parameters that should be explored via the tuneGrid argument.\nNow we can have a look at the output (Figure 12.25).\nnnfit __\n\n\nNeural Network \n\n66 samples\n 4 predictor\n 3 classes: 'E3.25', 'E3.5', 'E4.5' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 60, 59, 60, 60, 59, 59, ... \nResampling results across tuning parameters:\n\n  size  decay  Accuracy   Kappa    \n  2     0.00   0.7083333  0.4279755\n  2     0.01   0.7676587  0.5858922\n  2     0.10   0.7681349  0.5767463\n  4     0.00   0.7515476  0.5638426\n  4     0.01   0.8004762  0.6486256\n  4     0.10   0.7638889  0.5676798\n  8     0.00   0.7385714  0.5393148\n  8     0.01   0.7348016  0.5281220\n  8     0.10   0.7532540  0.5525435\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were size = 4 and decay = 0.01.\n\n\nplot(nnfit)\npredict(nnfit) |&gt; head(10)__\n\n\n [1] E3.25 E3.25 E3.25 E3.25 E3.25 E3.25 E3.25 E3.25 E3.25 E3.25\nLevels: E3.25 E3.5 E4.5\n\nFigure 12.25: Parameter tuning of the neural net by cross-validation.\n__\nQuestion 12.23\nWill the accuracy that we obtained above for the optimal tuning parameters generalize to a new dataset? What could you do to address that?\n__\nSolution\n__\nNo, it is likely to be too optimistic, as we have picked the optimum. To get a somewhat more realistic estimate of prediction performance when generalized, we could formalize (into computer code) all our data preprocessing choices and the above parameter tuning procedure, and embed this in another, outer cross- validation loop (Ambroise and McLachlan 2002). However, this is likely still not enough, as we discuss in the next section.\n\n14.6.1 12.7.1 Method hacking\nIn Chapter 6 we encountered p-value hacking. A similar phenomenon exists in statistical learning: given a dataset, we explore various different methods of preprocessing (such as normalization, outlier detection, transformation, feature selection), try out different machine learning algorithms and tune their parameters until we are content with the result. The measured accuracy is likely to be too optimistic, i.e., will not generalize to a new dataset. Embedding as many of our methodical choices into a computational formalism and having an outer cross-validation loop (not to be confused with the inner loop that does the parameter tuning) will ameliorate the problem. But is unlikely to address it completely, since not all our choices can be formalized.\nThe gold standard remains validation on truly unseen data. In addition, it is never a bad thing if the classifier is not a black box but can be interpreted in terms of domain knowledge. Finally, report not just summary statistics, such as misclassification rates, but lay open the complete computational workflow, so that anyone (including your future self) can convince themselves of the robustness of the result or of the influence of the preprocessing, model selection and tuning choices (Holmes 2018).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html#summary-of-this-chapter",
    "href": "12-chap.html#summary-of-this-chapter",
    "title": "14  12.1 Goals for this chapter",
    "section": "14.7 12.8 Summary of this chapter",
    "text": "14.7 12.8 Summary of this chapter\nWe have seen examples of machine learning applications; we have focused on predicting categorical variables (like diabetes type or cell class). Predicting continuous outcomes is also part of machine learning, although we have not considered it here. There are many parallels and overlaps between machine learning and statistical regression (which we studied in Chapter 8). One can consider them two different names for pretty much the same activity, although each has its own flavors: in machine learning, the emphasis is on the prediction of the outcome variables, whereas in regression we often care at least as much about the role of the covariates – which of them have an effect on the outcome, and what is the nature of these effects? In other words, we do not only want predictions, we also want to understand them.\nWe saw linear and quadratic discriminant analysis, two intuitive methods for partitioning a two-dimensional data plane (or a \\(p\\)-dimensional space) into regions using either linear or quadratic separation lines (or hypersurfaces). We also saw logistic regression, which takes a slightly different approach but is more amenable to operating in higher dimensions and to regularization.\nWe encountered the main challenge of machine learning: how to avoid overfitting? We explored why overfitting happens in the context of the so- called curse of dimensionality, and we learned how it may be overcome using regularization.\nIn other words, machine learning would be easy if we had infinite amounts of data representatively covering the whole space of possible inputs and outputs11. The challenge is to make the best out of a finite amount of training data, and to generalize these to new, unseen inputs. There is a vigorous trade-off between the amount, resolution and coverage of training data and the complexity of the model. Many models have continuous parameters that enable us to “tune” their complexity or the strength of their regularization. Cross-validation can help us with such tuning, although it is not a panacea, and caveats apply, as we saw in Section 12.6.3.\n11 It would “just” be a formidable database / data management problem.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html#further-reading",
    "href": "12-chap.html#further-reading",
    "title": "14  12.1 Goals for this chapter",
    "section": "14.8 12.9 Further reading",
    "text": "14.8 12.9 Further reading\n\nAn introduction to statistical learning that employs many concrete data examples and uses little mathematical formalism is given by James et al. (2013). An extension, with more mathematical background, is the textbook by Hastie, Tibshirani, and Friedman (2008).\nThe CRAN task view on machine learning gives an overview over machine learning software in R.\nRStudio’s API for the “deep learning” platforms Keras and TensorFlow and the associated teaching materials and demos are a good place to try out some of the recent developments in this field.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html#exercises",
    "href": "12-chap.html#exercises",
    "title": "14  12.1 Goals for this chapter",
    "section": "14.9 12.10 Exercises",
    "text": "14.9 12.10 Exercises\n__\nExercise 12.1\nApply a kernel support vector machine , available in the kernlab package, to the zeller microbiome data. What kernel function works well?\n__\nExercise 12.2\nUse glmnet for a prediction of a continuous variable , i.e., for regression. Use the prostate cancer data from Chapter 3 of (Hastie, Tibshirani, and Friedman 2008). The data are available in the CRAN package ElemStatLearn. Explore the effects of using ridge versus lasso penalty.\n__\nExercise 12.3\nConsider smoothing as a regression and model selection problem (remember Figure 12.1). What is the equivalent quantity to the penalization parameter \\(\\) in Equation 12.8? How do you choose it?\n__\nSolution\n__\nWe refer to Chapter 5 of (Hastie, Tibshirani, and Friedman 2008)\n__\nExercise 12.4\nScale invariance. Consider a rescaling of one of the features in the (generalized) linear model 12.7. For instance, denote the \\(\\)-th column of \\(x\\) by \\(x_{}\\), and suppose that \\(p\\) and that we rescale \\(x_{} s, x_{}\\) with some number \\(s\\). What will happen to the estimate \\(\\) from Equation 12.8 in (a) the unpenalized case (\\(\\)) and (b) the penalized case (\\(&gt;0\\))?\n__\nSolution\n__\nIn the unpenalized case, the estimates will be scaled by \\(1/s\\), so that the resulting model is, in effect, the same. In the penalized case, the penalty from the \\(\\)-th component of \\(\\) will be different. If \\(|s|&gt;1\\), the amplitude of the feature is increased, smaller \\(\\)-components are required for it to have the same effect in the prediction, and therefore the feature is more likely to receive a non-zero and/or larger estimate, possibly on the cost of the other features; conversely for \\(|s|&lt;1\\). Regular linear regression is scale-invariant, whereas penalized regression is scale-dependent. It’s important to remember this when interpreting penalized model fits.\n__\nExercise 12.5\nIt has been quipped that all classification methods are just refinements of two archetypal ideas : discriminant analysis and \\(k\\) nearest neighbors. In what sense might that be a useful classification?\n__\nSolution\n__\nIn linear discriminant analysis, we consider our objects as elements of \\(^p\\), and the learning task is to define regions in this space, or boundary hyperplanes between them, which we use to predict the class membership of new objects. This is archetypal for classification by partition. Generalizations of linear discriminant analysis permit more general spaces and more general boundary shapes.\nIn \\(k\\) nearest neighbors, no embedding into a coordinate space is needed, but instead we require a distance (or dissimilarity) measure that can be computed between each pair of objects, and the classification decision for a new object depends on its distances to the training objects and their classes. This is archetypal for kernel-based methods.\nAmbroise, Christophe, and Geoffrey J. McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” PNAS 99 (10): 6562–66.\nBellman, Richard Ernest. 1961. Adaptive Control Processes: A Guided Tour. Princeton University Press.\nClemmensen, Line, Trevor Hastie, Daniela Witten, and Bjarne Ersbøll. 2011. “Sparse Discriminant Analysis.” Technometrics 53: 406–13.\nFriedman, Jerome H. 1997. “On Bias, Variance, 0/1—Loss, and the Curse-of- Dimensionality.” Data Mining and Knowledge Discovery 1: 55–77.\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2008. The Elements of Statistical Learning. 2^{} ed. Springer.\nHolmes, Susan. 2018. “Statistical Proof? The Problem of Irreproducibility.” Bulletin of the AMS 55 (1): 31–55.\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Springer.\nNeumann, B., T. Walter, J. K. Heriche, J. Bulkescher, H. Erfle, C. Conrad, P. Rogers, et al. 2010. “Phenotypic profiling of the human genome by time-lapse microscopy reveals cell division genes.” Nature 464 (7289): 721–27.\nOhnishi, Y., W. Huber, A. Tsumura, M. Kang, P. Xenopoulos, K. Kurimoto, A. K. Oles, et al. 2014. “Cell-to-Cell Expression Variability Followed by Signal Reinforcement Progressively Segregates Early Mouse Lineages.” Nature Cell Biology 16 (1): 27–37.\nReaven, GM, and RG Miller. 1979. “An Attempt to Define the Nature of Chemical Diabetes Using a Multidimensional Analysis.” Diabetologia 16 (1): 17–24.\nWitten, Daniela M, and Robert Tibshirani. 2011. “Penalized Classification Using Fisher’s Linear Discriminant.” JRSSB 73 (5): 753–72.\nZeller, Georg, Julien Tap, Anita Y Voigt, Shinichi Sunagawa, Jens Roat Kultima, Paul I Costea, Aurélien Amiot, et al. 2014. “Potential of Fecal Microbiota for Early-Stage Detection of Colorectal Cancer.” Molecular Systems Biology 10 (11): 766. https://doi.org/10.15252/msb.20145645.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html",
    "href": "13-chap.html",
    "title": "15  13.1 Goals for this chapter",
    "section": "",
    "text": "15.1 13.2 Types of experiments\nIn the same way a film director will view daily takes to correct potential lighting or shooting issues before they affect too much footage, it is a good idea not to wait until all the runs of an experiment have been finished before looking at the data. Intermediate data analyses and visualizations will track unexpected sources of variation and enable you to adjust the protocol. Much is known about sequential design of experiments (Mead 1990), but even in a more pragmatic setting it is important to be aware of sources of variation as they occur and adjust for them.\nWe have now seen many different biological datasets and data types, and methods for analyzing them. To conclude this book, we recapitulate some of the general lessons we learned. Three great pieces of good advice are:\n1 Presidential Address to the First Indian Statistical Congress, 1938. Sankhya 4, 14-17.\nIn this chapter we will:",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#types-of-experiments",
    "href": "13-chap.html#types-of-experiments",
    "title": "15  13.1 Goals for this chapter",
    "section": "",
    "text": "15.1.0.1 The art of “good enough”.\nWe need experimental design in order to deal with the fact that our resources are finite, our instruments not perfect, and that the real world is complicated. We want to get the the best possible outcome nonetheless. This invariably results in hard decisions and tradeoffs. Experimental design aims to rationalize such decisions. Our experimental interventions and our measurement instruments have limited precision and accuracy; often we don’t know these limitations at the outset and have to collect preliminary data to estimate them. We may only be able to observe the phenomenon of interest indirectly rather than directly. Our treatment conditions may have undesired but hard to avoid side effects, our measurements may be overlaid with interfering signals or “background noise”. Sample sizes are limited for practical and economic reasons. There is little point in prescribing unrealistic ideals – we need to make choices that are pragmatic and feasible. A quote from (Bacher and Kendziorski 2016) explains this clearly: “Generally speaking, a well-designed experiment is one that is sufficiently powered and one in which technical artifacts and biological features that may systematically affect measurements are balanced, randomized or controlled in some other way in order to minimize opportunities for multiple explanations for the effect(s) under study.”\nTo start with, let us discuss the major different types of experiments, since each of them requires different approaches.\nIn a controlled experiment , we have control over all relevant variables: the (model) system under study, the environmental conditions, the experimental readout. For instance, we could have a well-characterized cell line growing in laboratory conditions on defined media, temperature and atmosphere, we’ll administer a precise amount of a drug, and after 72h we measure the activity of a specific pathway reporter.\nIn a study , we have less control: important conditions that may affect the measured outcome are not under control of the researcher, usually because of ethical concerns or logistical constraints. For instance, in an ecological field study, this could be the weather, the availability of nutrition resources or the activity of predators. In an observational study , even the variable of interest is not controlled by the researcher. For instance, in a clinical trial, this might be the assignment of the individual subjects to groups. Since there are many possibilities for confounding (Section 13.4.1), interpretation of an observational study can be difficult. Here’s where the old adage “correlation is not causation” appertains.\nIn a randomized controlled trial , we still have to deal with lack of control over many of the factors that impact the outcome, but we control assignment of the variable of interest (say, the type of treatment in a clinical trial), therefore we can expect that –with high enough sample size– all the nuisance effects average out and the observed effect can really be causally assigned to the intervention. Such trials are usually prospective 2, i.e., the outcome is not known at the time of the assignment of the patients to the groups.\n2 The antonym is retrospective; observational studies can be prospective or retrospective.\nA meta-analysis is an observational study on several previous experiments or studies. One motivation of a meta-analysis is to increase power by increasing effective sample size. Another is to overcome the limitations of individual experiments or studies, which might suffer from researcher bias or other biases, be underpowered, or can otherwise be flawed or random. The hope is that by pooling results from many studies, such “study-level” problems average out.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#partitioning-error-bias-and-noise",
    "href": "13-chap.html#partitioning-error-bias-and-noise",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.2 13.3 Partitioning error: bias and noise",
    "text": "15.2 13.3 Partitioning error: bias and noise\n\n\n\nStatisticians use the term error for any deviations of a measured value from the true value. This is different from the everyday use of the word. In statistics, error is an unavoidable aspect of life. It is not “bad”, it is something to be cherished, reckoned with, tamed and controlled.\n\n\nStatisticians use the term error for any deviations of a measured value from the true value. This is different from the everyday use of the word. In statistics, error is an unavoidable aspect of life. It is not “bad”, it is something to be cherished, reckoned with, tamed and controlled.\nWe broadly distinguish between two types of error. The first, which we call noise , “averages out” if we just perform enough replicates. The second, which we call bias , remains; it even becomes more apparent with more replication. Recall the bull’s eye in Figure 12.18: in the lower panel, there is a lot of noise, but no bias, and the center of the cloud of points is in the right place. In the upper panel, there is much less noise, but bias. No amount of replication will remedy the fact that the center of the points is in the wrong place.\nBias is more difficult to deal with than noise: noise is easily recognized just from looking at replicates, and it averages out as we analyze more and more replicates. With bias, it can be hard to even recognize that it is there, and then we need to find ways to measure it and adjust for it, usually with some quantitative model.\n__\nQuestion 13.1\nGive two examples in previous chapters where we have modeled bias in high throughput data.\n__\nSolution\n__\nFor instance, in Chapter 8, we modeled the sampling noise with the gamma-Poisson distribution, we estimated sequencing depth bias with the library size factors and took it into account when testing for differential expression. We also modeled sampling biases caused by the two different protocols used (single-end, paired-end) by introducing a blocking factor into our generalized linear model.\n\n15.2.1 13.3.1 Error models: noise is in the eye of the beholder\nThe efficiency of most biochemical or physical processes involving DNA- polymers depends on their sequence content. For instance, occurrences of long homopolymer stretches, palindromes, overall or local GC content can modify the efficiency of PCR, or the dynamics of how the polymer is being pulled through a nanopore. The size and nature of such effects is challenging to model. They depend in subtle ways on factors like concentration, temperature, enzyme used, etc. So: when looking at RNA-Seq data, should we treat GC content as noise or as bias?\n__\nQuestion 13.2\nHow does the DESeq2 method address this issue?\n__\nSolution\n__\nDESeq2 offers both options. If size factors are used to model per-sample sampling bias, then such effects are not explicitly modeled.\nNote: The assumption is then that, for each gene, any such bias would effect the counts in the same way across all samples, so that for the purpose of differential expression analysis, it cancels out. To the extent that such effects are sample-specific, they are treated as noise. However, as described in its vignette, DESeq2 also allows specifying sample- and gene-dependent normalization factors for a matrix, and these are intended to contain explicit estimates of such biases.\nRemember that the noun sample here, by convention, refers to one column of the count matrix, e.g., one sequencing library corresponding to one replicate of one biological condition. The same term (here as the verb form sampling) is also used in its more general, statistical sense, as in “a sample of data from a distribution”. There is no easy way around this ambiguity, so we just need to be aware of it.\nFormal error models can help us decompose the variability into noise and bias. A standard decomposition you may have encountered is called ANOVA (ANalysis Of VAriance). In these types of models, variability is measured by sums of squares and aportioned according to its origin. For instance, when doing supervised classification in a linear discriminant analysis (LDA) in Chapter 12, we computed the total sum of squares \\(C\\) as\n\\[ C_{} = C_{} + C_{}. \\]\nHowever, there are usually multiple ways of doing such a decomposition: an effect that at one stage is considered within-group variation (noise) might be considered a between-groups effect once the right (sub)groups are assigned.\n\n\n\nMaybe this is akin to the vision of “personalized medicine”: better patient stratification that converts within group variation (incl. unsuccessful or unnecessary treatments) into between groups variation (where every group gets exactly what they need).\n\n\nMaybe this is akin to the vision of “personalized medicine”: better patient stratification that converts within group variation (incl. unsuccessful or unnecessary treatments) into between groups variation (where every group gets exactly what they need).\n\n15.2.1.1 Determinism versus chance.\n\nFigure 13.1: A carefully constructed coin tossing machine can be made to provide deterministic coin flips.\nEveryone thinks of the outcome of a coin toss as random, thus a perfect example of noise. But if we meticulously registered the initial conditions of the coin flip and solved the mechanical equations, we could predict which side has a higher probability of coming up (Diaconis, Holmes, and Montgomery 2007).\nSo, rather than asking whether a certain effect or process is random or deterministic, it is more fruitful to say whether we care to model it deterministically (as bias), or whether we ignore the details, treat it as stochastic, and use probabilistic modeling (noise). In this sense, probabilistic models are a way of quantifying our ignorance, taming our uncertainty.\n\n\n15.2.1.2 Latent factors.\nSometimes we explicitly know about factors that cause bias, for instance, when a different reagent batch was used in different phases of the experiments. We call this batch effects (Jeffrey T. Leek et al. 2010). At other times, we may expect that such factors are at work but have no explicit record of them. We call these latent factors. We can treat them as adding to the noise, and in Chapter 4 we saw how to use mixture models to do so. But this may not be enough: with high- dimensional data, noise caused by latent factors tends to be correlated, and this can lead to faulty inference (Jeffrey T. Leek et al. 2010). The good news is that these same correlations can be exploited to estimate latent factors from the data, model them as bias and thus reduce the noise (Jeffrey T. Leek and Storey 2007; Stegle et al. 2010).\n\n\n\n15.2.2 13.3.2 Biological versus technical replicates\n__\nQuestion 13.3\nImagine you want to test whether a weight loss drug works. Which of the following study designs would you use:\n\nA person is weighed on milligram precision scales, with 20 replicates. He follows the diet, and four weeks later, he is weighed again, with 20 replicates.\nTen people weigh themselves once on their bathroom scales and report the number. Four weeks later, they weigh themselves and report again.\n\nSurely the first option must be better since it has 20 replicates on a very precise instrument rather than only ten on an older piece of equipment?\n__\nSolution\n__\nWhat we have here is a (placative) instance of the difference between technical versus biological replicates. The number of replicates is less important than what types of variation are allowed to affect them. The 20 replicates in the first design are wasted on re-measuring something that we already know with more than enough precision. Whereas the far more important question –how does the effect generalize to different people– starts to be addressed with the second design, although in practice more people would be needed.\nNote: Inference or generalizations can only be made to a wider population if we have a representative, randomized sample of that population in our study. In the first case if a weight loss occurs, one could only infer about that person at that time.\nAnalogous questions arise in biological experimentation, e.g., do you rather do five replicates on the same cell line, or one replicate each on three different cell lines?\n__\nQuestion 13.4\nFor reliable variant calling with the sequencing technology used by the 1000 Genomes project, one needs about \\(30\\) coverage per genome. However, the average depth of the data produced was 5.1 for 1,092 individuals (1000 Genomes Project Consortium 2012). Why was that study design chosen?\n__\nSolution\n__\nThe project’s aim was finding common genetic variants, i.e., finding variants that have a prevalence of more than, say, 1% in the population. It was not to call high-confidence genotypes of individual people. Therefore, it was more cost-efficient to sample more individuals each with low coverage (say, 1092 individuals at 5x) than fewer individuals with high coverage (say, 182 at 30x). In this way, common variants would still be found with \\(&gt;=30\\) coverage (\\(1092 % = 55\\)), since they would be present in several of the 1000 people, but more of them would be found, and there would be more precise estimates of their population frequency.\nThe technical versus biological replicates terminology has some value, but is often too coarse. The observed effect may or may not be generalizable at many different levels: different labs, different operators within one lab, different technologies, different machines from the same technology, different variants of the protocol, different strains, litters, sexes, individual animals, and so forth. It’s better to name the levels of replication more explicitly.\n\n\n15.2.3 13.3.3 Units vs. fold-changes\nMeasurements in physics are usually reported as multiples of SI3 units, such as meters, kilograms, seconds. A length measured in meters by a lab in Australia using one instrument is directly comparable to one measured a year later by a lab in Canada using a different instrument, or by alien scientists in a far-away galaxy. In biology, it is rarely possible or practical to make measurements that are as standardized. The situation here is more like that where human body parts (feets, inches, etc.) are used for length measurements, and where the size of these body parts is even different in different towns and countries, let alone galaxies.\n3 International System of Units (French: Système International d’Unités)\nBiologists often report measurements as multipes of (i.e., fold changes with regard to) some local, more or less ad hoc reference. The challenge with this is that fold changes and proportions are ratios. The denominator is a random variable (as it changes from lab to lab and probably from experiment to experiment), which can create high instability and very unequal variances between experiments; see the sections on transformations and sufficiency a little later in this chapter. Even when seemingly absolute values exist (e.g., TPKM values in an RNA-Seq experiment), due to experiment-specific sampling biases they do not translate into universal units, and they often lack an indication of their precision.\n\n\n15.2.4 13.3.4 Regular and catastrophic noise\nRegular noise can be modelled by simple probability models such as independent normal distributions, Poissons, or mixtures such as gamma–Poisson or Laplace. We can use relatively straightforward methods to take such noise into account in our data analyses and to compute the probability of extraordinarily large or small values. In the real world, this is only part of the story: measurements can be completely off scale (a sample swap, a contamination or a software bug), and they can go awry all at the same time (a whole microtiter plate went bad, affecting all data measured from it). Such events are hard to model or even correct for – our best chance to deal with them is data quality assessment, outlier detection and documented removal.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#basic-principles-in-the-design-of-experiments",
    "href": "13-chap.html#basic-principles-in-the-design-of-experiments",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.3 13.4 Basic principles in the design of experiments",
    "text": "15.3 13.4 Basic principles in the design of experiments\n\n15.3.1 13.4.1 Confounding\n\nFigure 13.2: Comparison of a (hypothetical) biomarker between samples from disease and healthy states. If we are only given the information shown in the left panel, we might conclude that this biomarker performs well in detecting the disease. If, in addition, we are told that the data were acquired in two separate batches (e.g., different labs, different machines, different time points) as indicated in the panel on the right hand side, the conclusion will be different.\n__\nQuestion 13.5\nConsider the data shown in Figure 13.2. How can we decide whether the observed differences in the biomarker level are due to disease vs. healthy, or due to the batch?\n__\nSolution\n__\nIt is impossible to know from these data: the two variables are confounded.\nConfounding need not only be between a biological and a technical variable, it can also be more subtle. For instance, the biomarker might have nothing to do with the disease directly – it might just be a marker of a life style that causes the disease (as well as other things), or of an inflammation that is caused by the disease (as well as by many other things), etc.\n\n\n15.3.2 13.4.2 Effect size and replicates\n\nFigure 13.3: Confounding is the reason that one of the seven rules of experimental design listed by the Persian physician-scientist Abu ’Ali al- Husayn ibn Sina (Avicenna) around AD 1020 was “to study one possible cause of a disease at a time” (Stigler 2016).\nThe effect size is the difference between the group centers, as shown by the red arrow in Figure 13.4. A larger sample size in each group increases the precision with which the locations of each group and the effect size are known, thus increasing our power to detect a difference (Figure 13.5). On the other hand, the performance of the biomarker as a diagnostic for distinguishing individual samples between healthy and disease states depends on the within-group distributions (and the relative prevalences of both states), and is not improved by replication.\n\nFigure 13.4: The red arrow shows the effect size, as measured by the difference between the centers of the two groups. Here we locate the centers by the medians; sometimes the mean is used.\n\nFigure 13.5: On the left, the boxplot was created with samples of size 6. On the right the sample sizes are 60. The measurements have the same underlying error distribution in both cases.\n\n\n15.3.3 13.4.3 Clever combinations: Hotelling’s weighting example\nTo get the best data out of available resources, capitalizing on cancellations and symmetries is an important aspect. Here is a famous illustration of how Hotelling devised an improved weighing scheme. Suppose we are given a set of eight unknown weights \\(= (_1, …,_8)\\). In the following code, we simulate such a set of true weights using R’s random number generator.\n\nFigure 13.6: The example in this section uses the pharmacist’s balance weighing analogy introduced by Yates and developed by Hotelling (1944) and Mood (1946).\ntheta = round((2 * sample(8, 8) + rnorm(8)), 1)\ntheta __\n\n\n[1] 10.7 13.4 16.4  3.9  8.5 16.0  1.2  4.4\nMethod 1 : Naïve method, using eight weighings. Suppose we use a pharmacist’s balance (Figure 13.6) that weighs each weight \\(_i\\) individually, with errors distributed normally with a standard deviation of 0.1. We compute the vector of errors errors1 and their sum of squares as follows:\nX = theta + rnorm(length(theta), 0, 0.1)\nX __\n\n\n[1] 10.513279 13.268145 16.507673  3.881881  8.395974 16.073952  1.131341\n[8]  4.289040\n\n\nerrors1 = X - theta\nerrors1 __\n\n\n[1] -0.18672051 -0.13185519  0.10767279 -0.01811869 -0.10402607  0.07395242\n[7] -0.06865871 -0.11095993\n\n\nsum(errors1^2)__\n\n\n[1] 0.09748857\nMethod 2 : Hotelling’s method, also using eight weighings. The method is based on a Hadamard matrix, which we compute here.\nlibrary(\"survey\")\nh8 = hadamard(6)\ncoef8 = 2*h8 - 1\ncoef8 __\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    1    1    1    1    1    1    1    1\n[2,]    1   -1    1   -1    1   -1    1   -1\n[3,]    1    1   -1   -1    1    1   -1   -1\n[4,]    1   -1   -1    1    1   -1   -1    1\n[5,]    1    1    1    1   -1   -1   -1   -1\n[6,]    1   -1    1   -1   -1    1   -1    1\n[7,]    1    1   -1   -1   -1   -1    1    1\n[8,]    1   -1   -1    1   -1    1    1   -1\nWe use coef8 as the coefficients in a new weighing scheme, as follows: the first column of the matrix tells us to put all the weights on one side of the balance and to weigh that. Call the result Y[1]. The second column tell us to place weights 1, 3, 5, 7 on one side of the balance and weights 2, 4, 6, 8 on the other. We then measure the difference and call the result Y[2]. And so forth, for all eight columns of coef8. We can express the necessary computations in matrix multiplication form as below.\nY = theta  %*% coef8 + rnorm(length(theta), 0, 0.1)__\nAs in the first method, each of the eight weight measurements has a normal error with standard deviation of 0.1.\n__\nQuestion 13.6\n\nCheck that coef8 is -up to an overall factor- an orthogonal matrix (\\(C^t C = \\) for some \\(\\)).\nCheck that if we multiply theta with coef8 times coef8 transposed and divide by 8, we obtain theta again.\n\n__\nSolution\n__\ncoef8 %*% t(coef8)__\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    8    0    0    0    0    0    0    0\n[2,]    0    8    0    0    0    0    0    0\n[3,]    0    0    8    0    0    0    0    0\n[4,]    0    0    0    8    0    0    0    0\n[5,]    0    0    0    0    8    0    0    0\n[6,]    0    0    0    0    0    8    0    0\n[7,]    0    0    0    0    0    0    8    0\n[8,]    0    0    0    0    0    0    0    8\n\n\ntheta %*% coef8 %*% t(coef8) / ncol(coef8)__\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,] 10.7 13.4 16.4  3.9  8.5   16  1.2  4.4\nWe combine these results to estimate theta using the orthogonality of coef8.\nthetahat = Y %*% t(coef8) / ncol(coef8)__\nSince we know the true \\(\\), we can compute the errors and their sum of squares.\nerrors2 = as.vector(thetahat) - theta\nerrors2 __\n\n\n[1] -0.005213746  0.025216488  0.003201562  0.033880188 -0.029459127\n[6] -0.043173774  0.083202870 -0.025818188\n\n\nsum(errors2^2)__\n\n\n[1] 0.01214228\nWe see that the sum of squares here is substantially smaller than that of the first procedure. Were we just lucky?\n__\nQuestion 13.7\n\nRepeat the above experiment B = 10000 times, each time using a different theta, and look at the sampling distributions of sum of squared errors in both schemes.\nWhat do you think the relationship between the two variances is?\n\n__\nSolution\n__\nB  = 10000\ntc = t(coef8) / ncol(coef8)\nsse = replicate(B, {\n  theta = round((2 * sample(8, 8)) + rnorm(8), 1)\n  X = theta + rnorm(length(theta), 0, 0.1)\n  err1 = sum((X - theta)^2)\n  Y = coef8 %*% theta + rnorm(length(theta), 0, 0.1)\n  thetahat = tc %*% Y\n  err2 = sum((thetahat - theta)^2)\n  c(err1, err2)\n})\nrowMeans(sse)__\n\n\n[1] 0.079591221 0.009954419\n\n\nggplot(tibble(lr = log2(sse[1, ] / sse[2, ])), aes(x = lr)) +\n  geom_histogram(bins = 50) +\n  geom_vline(xintercept = log2(8), col = \"orange\") +\n  xlab(\"log2 ratio of SSE, Method 1 vs 2\")__\n\nFigure 13.7: Logarithm (base 2) of the ratios of sum of squared error for the two methods. The vertical orange line corresponds to 8.\nThe second scheme is more efficient than the first by a factor of 8 because the errors generated by the measurement have a sum of squares that is 8 times lower (Figure 13.7).\nThis example shows us that when several quantities are to be ascertained there is an opportunity to increase the accuracy and reduce the cost by combining measurements in one experiment and making comparisons between similar groups.\nIbn Sina’s rule that an optimal design can only vary one factor at a time was superseded in the 20th century by RA Fisher. He realized that one could modify the factors in combinations and still come to a conclusion—sometimes, an even better conclusion, as in the weighing example—as long as the contrasts were carefully designed.\n\nFigure 13.8: On the left, two samples each of size 6 are being compared. On the right, the same data are shown, but colored by the time of data collection. We note a tendency of the data to fall into blocks according to these times. Because of this, comparison between the groups is diluted. This effect can be mitigated by comparing within times, i.,e., by blocking into three groups. Paired analysis, such as demonstrated in Questions 13.8—13.10, is a special case of blocking.\n\n\n15.3.4 13.4.4 Blocking and pairing\nDarwin suspected that corn growth is affected by the composition of the soil and the humidity in the pots. For this reason, when he wanted to compare plants grown from cross-pollinated seeds to plants grown from self-pollinated seeds, he planted one seedling of each type in each of 15 pots. Each pot in Darwin’s Zea Mays experiment is a block, only the factor of interest (pollination method), called the treatment , is different within each block (Figure 13.9).\n\nFigure 13.9: A paired experiment is the simplest case of blocking.\nIn fact, RA Fisher criticized Darwin’s experiment because he systematically put the cross-pollinated plants on the same side of the pot. This could have induced confounding of a “side” effect with the cross effect, if one side of the pot received more sunlight for instance. It would have been preferable to randomize the side of the pot, e.,g., by flipping a coin.\nBlock what you can, randomize what you cannot.\n(George Box, 1978)\n\n15.3.4.1 Comparing a paired versus an unpaired design\nWhen comparing various possible designs, we do power simulations similar to what we saw in Chapter 1. Let’s suppose the sample size is 15 in each group and the effect size is 0.2. We also need to make assumptions about the standard deviations of the measurements, here we suppose both groups have the same sd=0.25 and simulate data:\nn = 15\neffect = 0.2\npots   = rnorm(n, 0, 1)\nnoiseh = rnorm(n, 0, 0.25)\nnoisea = rnorm(n, 0, 0.25)\nhybrid = pots + effect + noiseh\nautoz  = pots + noisea __\n__\nQuestion 13.8\nPerform both a simple \\(t\\)-test and a paired \\(t\\)-test. Which is more powerful in this case?\n__\nSolution\n__\nt.test(hybrid, autoz, paired = FALSE)__\n\n\n    Welch Two Sample t-test\n\ndata:  hybrid and autoz\nt = 0.77183, df = 26.012, p-value = 0.4472\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.3145706  0.6928591\nsample estimates:\nmean of x mean of y \n0.5073519 0.3182076 \n\n\nt.test(hybrid, autoz, paired = TRUE)__\n\n\n    Paired t-test\n\ndata:  hybrid and autoz\nt = 1.8783, df = 14, p-value = 0.08133\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.02683705  0.40512561\nsample estimates:\nmean difference \n      0.1891443 \nMaybe we were just lucky with our simulated data here?\n__\nQuestion 13.9\nCheck which method is generally more powerful. Repeat the above computations \\(1000\\) times and compute the average probability of rejection for these 1000 trials, using a false positive rate \\(\\).\n__\nSolution\n__\nB     = 1000\nalpha = 0.05\nwhat  = c(FALSE, TRUE)\npvs = replicate(B, {\n  pots   = rnorm(n, 0, 1)\n  noiseh = rnorm(n, 0, 0.25)\n  noisea = rnorm(n, 0, 0.25)\n  hybrid = pots + effect + noiseh\n  autoz  = pots + noisea\n  vapply(what,\n    function(paired)\n      t.test(hybrid, autoz, paired = paired)$p.value,\n    double(1)) |&gt; setNames(paste(what))\n})\nrowMeans(pvs &lt;= alpha)__\n\n\nFALSE  TRUE \n0.000 0.532 \nWe can compare the p-values obtained using both methods (Figure 13.10).\ntidyr::pivot_longer(as.data.frame(t(pvs)), cols = everything(), names_to = \"paired\") |&gt;\n  ggplot(aes(x = value, fill = paired)) +\n  geom_histogram(binwidth = 0.01, boundary = 0, alpha = 1/3)__\n\nFigure 13.10: Results from the power calculation, comparing the p-value distributions from the ordinary unpaired and the paired \\(t\\)-test.\n__\nQuestion 13.10\n\nWrite a function that compares the power of the two types of tests for different values of the effect size, sample size, size of the pot effects (as measured by their standard deviation), noise standard deviation and sample size.\nUse your function to find out which of the standard deviations (pots or noise) has the largest effect on the improvement produced by pairing for \\(n=15\\).\nHow big should \\(n\\) be to attain a power of 80% if the two standard deviations are both 0.5?\n\n__\nSolution\n__\npowercomparison = function(effect = 0.2, n = 15, alpha = 0.05,\n                sdnoise, sdpots, B = 1000) {\n  what = c(FALSE, TRUE)\n  pvs = replicate(B, {\n    pots   = rnorm(n, 0, sdpots)\n    noiseh = rnorm(n, 0, sdnoise)\n    noisea = rnorm(n, 0, sdnoise)\n    hybrid = pots + effect + noiseh\n    autoz  = pots + noisea\n    vapply(what,\n      function(paired)\n        t.test(hybrid, autoz, paired = paired)$p.value,\n      double(1)) |&gt; setNames(paste(what))\n  })\n  rowMeans(pvs &lt;= alpha)\n}__\nHere are a few simulations showing that when the pot effects are small compared to the noise standard deviation, pairing hardly makes a difference. If the pot effects are large, then pairing does make a big difference.\npowercomparison(sdpots = 0.5,  sdnoise = 0.25)__\n\n\nFALSE  TRUE \n0.034 0.533 \n\n\npowercomparison(sdpots = 0.25, sdnoise = 0.25)__\n\n\nFALSE  TRUE \n0.242 0.524 \n\n\npowercomparison(sdpots = 0.1,  sdnoise = 0.25)__\n\n\nFALSE  TRUE \n0.510 0.534 \nFor 100 plants of each type and both standard deviation at 0.5, the power of the paired test is about 80%.\npowercomparison(sdpots = 0.5, sdnoise = 0.5, n = 100)__\n\n\nFALSE  TRUE \n0.513 0.796 \n__\nQuestion 13.11\nPaired designs take into account a natural pairing of the observations — for instance, twin studies, or studies of patients before and after a treatment. What can be done when pairing is not available?\n__\nSolution\n__\nMatched designs try to create pairs of subjects that have as much similarity as possible through matching age, sex, background health etc. One is treated, the other serves as a control.\nA balanced design is an experimental design where all the different factor combinations have the same number of observation replicates. The effect of each factor is identifiable. If there are nuisance factors, it is good to make sure they are balanced with the factors of interest. Sometimes this is inconvenient or impractical for logistic or economic reasons – but in such cases analysts are on thin ice and need to proceed with caution.\n\n\n15.3.4.2 Randomization\nOften we don’t know which nuisance factors will be important, or we cannot plan for them ahead of time. In such cases, randomization is a practical strategy: at least in the limit of large enough sample size, the effect of any nuisance factor should average out.\nRandomization can also help reduce unconscious bias. For instance, if the samples from one of the groups are extremely hard to come by, we might be tempted to be extra careful when handling them, compared to samples from the other groups. Unfortunately this might bias the measurement outcomes and thus invalidate the comparison. See Senn (2004) for an extensive discussion of some of the pitfalls that occur when trying to improve on simple randomization.\n\n\n\n15.3.5 13.4.5 How many replicates do I need?\n\n\n\nBeware of underpowered me-too studies.\n\n\nBeware of underpowered me-too studies.\nIn Section 1.4.1 we showed a simulation experiment calculating how many nucleotides were necessary to achieve a 80% true positive rate, given that we knew the alternative. Now, recall the discussion of experiments versus studies from Section 13.2. For the cell line experiment, we might get the correct result already from one replicate; usually we’ll do two or three to be sure. On the other hand, for a study comparing the effect of two alternative drugs on patients, our intuition tells us that there is so much uncontrolled variability that we’ll likely need dozens (if not more) patients until we can be sure about the result. The number of replicates needed is highly context specific. It depends on the amount of uncontrolled variability and the effect size. A pragmatic approach is to check out previous successful (or unsuccessful) experiments or studies that did something comparable and use simulations, subsampling or bootstrapping to get an estimate of the planned study’s power.\n\n15.3.5.1 Power depends on sample sizes, effect sizes and variability.\n\nFigure 13.11: The elephant in the room with power calculations is the effect size. Especially in ’omics studies, when we are screening thousands of genes (or other features) for differences, we rarely have a precise idea of what effect size to expect. However, even so, power calculations are useful for order-of-magnitude calculations, or for qualitative comparisons such as shown in this section for paired versus unpaired tests. Source: Wikimedia CH.\nThe package pwr provides functions for doing the standard power calculations. There are always four quantities involved in these computations: sample size, effect size, significance level (false positive rate) and the power itself which is the probability of rejecting a hypothesis when you should (true positive rate). The functions pwr.2p.test, pwr.chisq.test, pwr.f2.test provide the calculations for tests of two proportions, the chisquared test and general linear tests respectively.\nHere is an example of the power calculcation for a two sample \\(t\\)-test with \\(n=15\\). The function requires several arguments:\nlibrary(\"pwr\")\nstr(pwr.t.test)__\n\n\nfunction (n = NULL, d = NULL, sig.level = 0.05, power = NULL, type = c(\"two.sample\", \n    \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))  \nIf you call the function with a value for power and effect size, it will return the sample size needed, or if you specify the sample size and effect size, it returns the power.\npwr.t.test(n = 15, d = 0.4, sig.level = 0.05, type = \"two.sample\")__\n\n\n     Two-sample t test power calculation \n\n              n = 15\n              d = 0.4\n      sig.level = 0.05\n          power = 0.1848496\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\npwr.t.test(n = 15, d = 0.4, sig.level = 0.05, type = \"paired\")__\n\n\n     Paired t test power calculation \n\n              n = 15\n              d = 0.4\n      sig.level = 0.05\n          power = 0.3031649\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\nIf we want to know what sample size would be required to detect a given effect size:\npwr.t.test(d = 0.4, sig.level = 0.05, type = \"two.sample\", power=0.8)__\n\n\n     Two-sample t test power calculation \n\n              n = 99.08032\n              d = 0.4\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\npwr.t.test(d = 0.4, sig.level = 0.05, type = \"paired\", power=0.8)__\n\n\n     Paired t test power calculation \n\n              n = 51.00945\n              d = 0.4\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\nWe see that we would need about twice as many observations for the same power when not using a paired test.\n\n\n15.3.5.2 Effective sample size\nA sample of independent observations is more informative than the same number of dependent observations. Suppose you want to do an opinion poll by knocking at people’s doors and asking them a question. In the first scenario, you pick \\(n\\) people at \\(n\\) random places throughout the country. In the second scenario, to save travel time, you pick \\(n/3\\) random places and then at each of these interview three people who live next door to each other. In both cases, the number of people polled is \\(n\\), but if we assume that people living in the same neighborhood are more likely to have the same opinion, the data from the second scenario are (positively) correlated. To explore this, let’s do a simulation.\ndoPoll = function(n = 100, numPeoplePolled = 12) {\n  opinion = sort(rnorm(n))\n  i1 = sample(n, numPeoplePolled)\n  i2 = sample(seq(3, n, by = 3), numPeoplePolled / 3)\n  i2 = c(i2, i2 - 1, i2 - 2)\n  c(independent = mean(opinion[i1]), correlated = mean(opinion[i2]))\n}\nresponses = replicate(5000, doPoll())\n\ntidyr::pivot_longer(as.data.frame(t(responses)), \n        cols = everything(), names_to = \"design\") |&gt;\nggplot(aes(x = value, col = design)) + geom_density() +\n  geom_vline(xintercept = 0) + xlab(\"Opinion poll result\")__\n\nFigure 13.12: Density estimates for the polling result using the two sampling methods. The correlated method has higher spread. The truth is indicated by the vertical line.\nThere are 100 people in the country, of which in the first approach (i1) we randomly sample 12. In the second approach, we sample 4 people as well as two neighbors for each (i2). The “opinion” in our case is a real number, normally distributed in the population with mean 0 and standard deviation 1. We model the spatio-sociological structure of our country by sorting the houses from most negative to most positive opinion in the first line of the doPoll function. The output is shown in Figure 13.12.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#mean-variance-relationships-and-variance-stabilizing-transformations",
    "href": "13-chap.html#mean-variance-relationships-and-variance-stabilizing-transformations",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.4 13.5 Mean-variance relationships and variance-stabilizing transformations",
    "text": "15.4 13.5 Mean-variance relationships and variance-stabilizing transformations\nIn Chapters 4 and 8 we saw examples for data transformations that compress or stretch the space of quantitative measurements in such a way that the measurements’ variance is more similar throughout. Thus the variance between replicate measurements is no longer highly dependent on the mean value.\nThe mean-variance relationship of our data before transformation can in principle be any function, but in many cases, the following prototypic relationships are found, at least approximately:\n\nconstant: the variance is independent of the mean, \\(v(m)=c\\).\nPoisson: the variance is proportional to to the mean, \\(v(m)=am\\).\nquadratic: the standard deviation is proportional to the mean, therefore the variance grows quadratically, \\(v(m)=bm^2\\).\n\nHere \\(v(m)\\) is the function that describes the trend of the variance \\(v\\) as a function of the mean \\(m\\). The real numbers \\(a, b, c\\) parameterize factors affecting the variance besides the mean.\n__\nQuestion 13.12\nGive examples for biological assays or measurement technologies whose data show these types of mean-variance relationships.\nReal data can also be affected by a combination of these. For instance, with DNA microarrays, the fluorescence intensities are subject to a combination of background noise that is largely independent of the signal, and multiplicative noise whose standard deviation is proportional to the signal (Rocke and Durbin 2001). Therefore, the mean-variance relationship is \\(v(m)=bm^2+c\\). For bright spots (large \\(m\\)), the multiplicative noise dominates (\\(bm^2\\)), whereas for faint ones, the background \\(c\\).\n__\nQuestion 13.13\nWhat is the point of applying a variance-stabilizing transformation?\n__\nSolution\n__\nAnalyzing the data on the transformed scale tends to:\n\nImprove visualization, since the physical space on the plot is used more “fairly” throughout the range of the data. A similar argument applies to the color space in the case of a heatmap.\nImprove the outcome of ordination methods such as PCA or clustering based on correlation, as the results are not so much dominated by the signal from a few very highly expressed genes, but more uniformly from many genes throughout the dynamic range.\nImprove the estimates and inference from statistical models that are based on assuming identically distributed (and hence, homoskedastic) noise.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#data-quality-assessment-and-quality-control",
    "href": "13-chap.html#data-quality-assessment-and-quality-control",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.5 13.6 Data quality assessment and quality control",
    "text": "15.5 13.6 Data quality assessment and quality control\nWe distinguish between data quality assessment (QA) –steps taken to measure and monitor data quality– and quality control (QC) –removing bad data. These activities pervade all phases of an analysis, from assembling the raw data over transformation, summarization, model fitting, hypothesis testing or screening for “hits” to interpretation. QA-related questions include:\n\nHow do the marginal distributions of the variables look (histograms, ECDF plots)?\nHow do their joint distributions look (scatter plots, pairs plot)?\nHow well do replicates agree (as compared to different biological conditions)? Are the magnitudes of the differences between several conditions plausible?\nIs there evidence of batch effects? These could be of a categorical (stepwise) or continuous (gradual) nature, e.g.. due to changes in experimental reagents, protocols or environmental factors. Factors associated with such effects may be explicitly known, or unkown and latent , and often they are somewhere in between (e.g., when a measurement apparatus slowly degrades over time, and we have recorded the times, but don’t really know exactly at what time the degradation is how bad).\n\nFor the last two sets of questions, heatmaps, principal component plots and other ordination plots (as we have seen in Chapters 7 and 9) are useful.\n\nFigure 13.13: Henry Ford’s (possibly apocryphal) quote: “If I had asked people what they wanted, they would have said faster horses.” expresses the view of quality as fitness for purpose , versus adherence to specifications. (Source: Ford)\nIt’s not easy to define quality , and the word is used with many meanings. The most pertinent for us is fitness for purpose 4, and this contrasts to other definitions of quality that are based on normative specifications. For instance, in differential expression analysis with RNA-Seq data, our purpose may be the detection of differentially expressed genes between two biological conditions. We can check specifications such as the number of reads, read length, base calling quality, fraction of aligned reads, but ultimately these measures in isolation have little bearing on our purpose. More to the point will be the identification of samples that are not behaving as expected, e.g., because of a sample swap or degradation; or genes that were not measured properly. We saw an example for this in Section 8.10.3. Useful plots include ordination plots, such as Figure 8.6, and heatmaps, such as Figure 8.7. A quality metric is any value that we use to measure quality, and having explicit quality metrics helps automating QA/QC.\n4 http://en.wikipedia.org/wiki/Quality_%28business%29",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#longitudinal-data",
    "href": "13-chap.html#longitudinal-data",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.6 13.7 Longitudinal data",
    "text": "15.6 13.7 Longitudinal data\nLongitudinal data 5 have time as a covariate. The first question is whether we are looking at a handful of time points –say, the response of a cell line measured 48h, 72h and 96h after exposure to a drug– or a long and densely sampled time series –say, patch clamp data in electrophysiology or a movie from life cell microscopy.\n5 A related but different concept is survival data , where time is the outcome variable.\nIn the first case, time is usually best thought of as just another discrete experimental factor. Perhaps the multiple time points were chosen because the experimenter was not sure which one would give the most useful results. One can then try to identify the best time point and focus on that. Depending on the data, the other time points could serve for validation, as “more-or-less” replicates. When designing the experiment, we’ll try to cover those time periods more densely when we expect most to happen, e.g., directly after a perturbation.\nIn a screening context, we can ask whether there is any effect at all, regardless of which time point and which shape, using something like an \\(F\\)-test. We then just need to make sure that we account for the dependencies between the measurements at the different time points and determine the null distribution accordingly.\nIn the second case, with time series, we may want to fit dynamical models to the data. We can write \\(X(t)\\) for the state of our system at time \\(t\\), and we have many choices, depending on whether\n\n\\(X\\) is continuous or discrete,\nthe dynamics of \\(X\\)6 are deterministic or stochastic,\nthe dynamics are smooth and/or jumpy,\nwe observe \\(X\\) directly or only some noisy and/or reduced version \\(Y = g(X)+\\)7 of it.\n\n6 The value of \\(X(t+t)\\), given \\(X(t)\\), in other words, the temporal evolution\n7 Here \\(g\\) denotes a function that looses information, e.g., by dropping some of the variables of a vector-valued \\(X\\), and \\(\\) is a noise term.\nWe have many modeling tools at hand, including\n\nMarkov Models: discrete state space; the dynamics are stochastic and occur by jumping between states.\nOrdinary or partial differential equations: continuous state space; the dynamics are deterministic and smooth and are described by a differential equation, possibly derived from first principles rooted in physics or chemistry.\nMaster equation, Fokker-Planck equation: the dynamics are stochastic and are described by (partial) differential equations for the probability distribution of \\(X\\) in space and time.\nPiece-wise deterministic stochastic processes: a combination of the above, samples from the process involve deterministic, smooth movements as well as occasional jumps.\n\nIf we don’t observe \\(X\\) directly, but only a noisy and/or summarized version \\(Y\\), then in the case of Markov models, the formalism of Hidden Markov Models (Durbin et al. 1998) makes it relatively straightforward to fit such models. For the other types of processes, analogous approaches are possible, but these are technically more demanding, and we refer to specialized literature.\nTaking a more data-driven (rather than model-driven) view, methods for analyzing time series data include:\n\nNon-parametric smoothing followed by clustering or classification into prototypic shapes\nChange point detection\nAutoregressive models\nFourier and wavelet decomposition\n\nIt’s outside the scope of this book to go into details, and there is a huge number of choices8. Many methods originated in physics, econometrics or signal processing, so it’s worthwhile to scan the literature in these fields.\n8 One start point is the CRAN taskview https://cran.r-project.org/web/views/TimeSeries.html.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#data-integration-use-everything-you-could-know",
    "href": "13-chap.html#data-integration-use-everything-you-could-know",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.7 13.8 Data integration: use everything you (could) know",
    "text": "15.7 13.8 Data integration: use everything you (could) know\n\n\n\nDon’t pretend you are dumb.\n\n\nDon’t pretend you are dumb.\nThere is an attraction to seemingly “unbiased” approaches that analyse the data at hand without reference to what is already known. Such tendencies are reinforced by the fact that statistical methods have often been developed to be generic and self-contained, for instance, to work of a general matrix without specific reference to what the rows and column mean in an application, or what other, more or less relevant data might be around.\nGeneric approaches are a good way to get started, and for analyses that are straightforward and highly powered, such an approach might work out. But often, it is wasteful. Recall the example of an RNA-Seq experiment for differential expression. As we saw in Chapters 6 and 8, we could perform a hypothesis test for each recorded gene, regardless of its signal strength9 or anything else, and then run a multiple testing method that treats all tests the same (i.e., as exchangeable). But this is inefficient: we can improve our detection power by filtering out or downweighting hypotheses with lower power or with higher prior probability \\(_0\\) of being true.\n9 i.e., average read counts\nSimilarly, in the interpretation of single p-values, we don’t need to ignore everything else we know, and for instance, blindly stick to an arbitrary 5% cutoff no matter what, but rather, we can let prior knowledge on the test’s power and on \\(_0\\) guide our interpretation (Altman and Krzywinski 2017).\nOther potential examples of misplaced objectivity include:\n\nPenalization or feature selection in high-dimensional regression or classification. It is easy to use schemes that treat all features the same, for instance, standardize all of them to zero mean and unit variance. But sometime we know that some classes of features are likely to be more or less informative than others (Wiel et al. 2016). We can also use graphs or networks to represent “other” data and use approaches like the group or graph lasso (Jacob, Obozinski, and Vert 2009) to structure your penalties in high-dimensional modeling.\nUnsupervised clustering of our objects of interest (samples, genes or sequences) and subsequent search for over-represented annotations. We can be better off by incorporating the different uncertainties with which these were measured as well as their different frequencies into the clustering algorithm. We can use probabilities and similarities to check whether the members of clusters are more similar than two randomly picked objects (Callahan et al. 2016).\n\nWhen embarking on an analysis, it’s important to anticipate that rarely we’ll be done by applying a single method and getting a straightforward result. We need to dig out other, related datasets, look for confirmations (or else) of our results, get further interpretation. An example is gene set enrichment analysis: after we’ve analyzed our data and found a list of genes that appear to be related to our comparison of interest, we’ll overlap them with other gene lists, such as those from the Molecular Signatures Database (Liberzon et al. 2011) in order to explore the broader biological processes involved; or we might load up datasets looking at levels of regulation10 up-stream or down-stream of ours in search for context.\n10 Genome, chromatin state, transcription, mRNA life cycle, translation, protein life cycle, localization and interactions; metabolites, \\(…\\)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#sharpen-your-tools-reproducible-research",
    "href": "13-chap.html#sharpen-your-tools-reproducible-research",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.8 13.9 Sharpen your tools: reproducible research",
    "text": "15.8 13.9 Sharpen your tools: reproducible research\nAnalysis projects often begin with a simple script, perhaps to try out a few initial ideas and explore the quality of the pilot data. Then more ideas are added, more data come in, other datasets are integrated, more people become involved. Eventually the paper needs to be written, figures be done ‘properly’, and the analysis be saved for the scientific record and to document its integrity. Here are a few principles that can help with such a process11.\n11 An excellent and very readable outline of good computing practices for researchers, including data management, programming, collaborating with colleagues, organizing projects, tracking work and writing manuscripts, is given by Wilson et al. (2017).\nUse an integrated development environment. RStudio is a great choice; there are also other platforms such as Emacs or Eclipse.\nUse literate programming tools such as Rmarkdown or Jupyter. This is more readable (for yourself and for others) than burying explanations and usage instructions in comments in the source code or in separate README files, in addition you can directly embed figures and tables in these documents. Such documents are good starting points for the supplementary material of your paper. Moreover, they’re great for reporting analyses to your collaborators. Anticipate re-engineering of the data formats and the software. The first version of how you represent the data and structure the analysis workflow will rarely be capable of supporting the project as it evolves. Don’t be afraid12 to make a clean cut and redesign as soon as you notice that you are doing a lot of awkward data manipulations or repetitive steps. This is time well- invested. Almost always it also helps to unearth bugs.\n12 The professionals do it, too: “Most software at Google gets rewritten every few years.” (Henderson 2017)\nReuse existing tools. Don’t reinvent the wheel; your time is better spent on things that are actually new. Before using a self-made “heuristic” or a temporary “short-cut”, spend a couple of minutes researching to see if something like this hasn’t been done before. More often than not, it has, and sometimes there is a clean, scalable and already tested solution.\nUse version control , such as . This takes time to learn, but this time is well-invested. In the long run it will be infinitely better than all your self-grown attempts at managing evolving code with version numbers, switches and the like. Moreover, this is the sanest option for collaborative work on code, and it provides an extra backup of your codebase, especially if the server is distinct from your personal computer.\nUse functions rather than copy-pasting (or repeatedly source-ing) stretches of code.\nUse the R package system. Soon you’ll note recurring function or variable definitions that you want to share between your different scripts. It is fine to use the R function source to manage them initially, but it is never too early to move them into your own package – at the latest when you find yourself starting to write emails or code comments explaining others (or yourself) how to use some functionality. Assembling existing code into an R package is not hard, and it offers you many goodies including standardized ways of documentation, showing code usage examples, code testing, versioning and provision to others. And quite likely you’ll soon appreciate the benefits of using namespaces.\nCentralize the location of the raw data files and automate the derivation of intermediate data. Store the input data at a centralized file server that is professionally backed up. Mark the files as read-only. Have a clear and linear workflow for computing the derived data (e.g., normalized, summarized, transformed etc.) from the raw files, and store these in a separate directory. Anticipate that this workflow will need to be run several times13, and version it. Use the BiocFileCache package to mirror these files on your personal computer14.\n13 Always once more than the final, final time before the final data freeze…\n14 A more basic alternative is the utility. A popular solution offered by some organizations is based on ownCloud. Commercial options include Dropbox, Google Drive, and the like.\n15 In computer science, the term data warehouse is sometimes used for such a concept.\nThink in terms of cooking recipes and try to automate them. When developing downstream analysis ideas that bring together several different data types, you don’t want to do the conversion from data type specific formats into a representation suitable for machine learning or generic statistical method each time anew, on an ad hoc basis. Have a recipe script that assembles the different ingredients and cooks them up as an easily consumable15 matrix, data frame or Bioconductor SummarizedExperiment.\nKeep a hyperlinked webpage with an index of all analyses. This is helpful for collaborators (especially if the page and the analysis can be accessed via a web browser) and also a good starting point for the methods part of your paper. Structure it in chronological or logical order, or a combination of both.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#data-representation",
    "href": "13-chap.html#data-representation",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.9 13.10 Data representation",
    "text": "15.9 13.10 Data representation\nGetting data ready for analysis or visualization often involves a lot of shuffling until they are in the right shape and format for an analytical algorithm or a graphics routine. As we saw in Chapter 3, ggplot2 likes its data in dataframe objects, with one row per measurement record. The reasons behind this choice are well explained in Hadley Wickham’s paper on tidy data (Wickham 2014).\n\n15.9.1 13.10.1 Wide vs long table format\nRecall the Hiiragi data (for space reasons we select only four genes, and print only the first five columns of xwdf):\nlibrary(\"magrittr\")\ndata(\"x\", package = \"Hiiragi2013\")\nxwdf = tibble(\n  probe  = c(\"1420085_at\", \"1418863_at\", \"1425463_at\", \"1416967_at\"),\n  symbol = c(      \"Fgf4\",      \"Gata4\",      \"Gata6\",       \"Sox2\"))\nxwdf %&lt;&gt;% bind_cols(as_tibble(Biobase::exprs(x)[xwdf$probe, ]))\ndim(xwdf)__\n\n\n[1]   4 103\n\n\nxwdf[, 1:5]__\n\n\n# A tibble: 4 × 5\n  probe      symbol `1 E3.25` `2 E3.25` `3 E3.25`\n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 1420085_at Fgf4        3.03      9.29      2.94\n2 1418863_at Gata4       4.84      5.53      4.42\n3 1425463_at Gata6       5.50      6.16      4.58\n4 1416967_at Sox2        1.73      9.70      4.16\nEach row of this dataframe corresponds to one of the selected genes. The first two column contain the Affymetrix probe identifier and the gene symbol. The remaining 101 columns report the measured expression values, one for each sample. The sample identifiers, together with information on the time point when the sample was taken, are recorded in the column names as a concatenated string. This is an example for a data table in wide format. Now let us call the pivot_longer function from the tidyr package and have a look at its output.\nlibrary(\"tidyr\")\nxldf = pivot_longer(xwdf, cols = !all_of(c(\"probe\", \"symbol\")),\n                          names_to = \"sample\")\ndim(xldf)__\n\n\n[1] 404   4\n\n\nhead(xldf)__\n\n\n# A tibble: 6 × 4\n  probe      symbol sample  value\n  &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;\n1 1420085_at Fgf4   1 E3.25  3.03\n2 1420085_at Fgf4   2 E3.25  9.29\n3 1420085_at Fgf4   3 E3.25  2.94\n4 1420085_at Fgf4   4 E3.25  9.72\n5 1420085_at Fgf4   5 E3.25  8.92\n6 1420085_at Fgf4   6 E3.25 11.3 \nIn xldf, each row corresponds to exactly one of the 404 measured values, stored in the column value. Then there are additional columns probe, symbol and sample, which store the associated covariates. This is an instance of long format.\nIn xwdf, some columns refer to data from all the samples (namely, probe and symbol), whereas other columns (those with the expression measurements) contain information that is sample-specific. We somehow have to “know” this when interpreting the dataframe. This is what Hadley Wickham calls untidy data 16. In contrast, in the tidy dataframe xldf each row forms exactly one observation, its value is in the column named value, and all other information associated with that observation is in the other colums of the same row. If we want to add additional columns, say, Ensembl gene identifiers or chromosome locations, we can simply add them. Similarly, if we want to add data from further genes or additional samples, we can simply add the corresponding rows to xldf. In either, we can assume that we will not break existing code. This is in contrast to xwdf, adding columns might invalidate existing code, as we cannot be sure how it differentiates between data columns (with measured values) and covariate columns.\n16 Recall the Anna Karenina principle: there are many different ways for data to be untidy.\nAlso, subsetting by probe identifier, by gene symbol, or by samples, or indeed by any other covariate, is straightforward and can always use the same dplyr::filter syntax. In contrast, for xwdf, we need to remember that subsetting samples amounts to column subsetting, whereas subsetting genes to row subsetting.\nThe Hiiragi data have another natural wide format representation besides xwdf: instead of one row per gene and columns for the different samples, we could also have the data in a dataframe with one row per sample and columns for the different genes. Both of these wide representations can be useful. For instance, if we want to produce scatterplots using ggplot2 of the expression values of all genes between two samples, or all samples between two genes, we need to use one or the other of the two wide formats.\nTo transform from the long format into the wide format (either of them), you can use the pivot_wider function from the tidyr package—the complement of the pivot_longer function that we already used above.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#tidy-data-using-it-wisely",
    "href": "13-chap.html#tidy-data-using-it-wisely",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.10 13.11 Tidy data – using it wisely",
    "text": "15.10 13.11 Tidy data – using it wisely\nIn tidy data (Wickham 2014),\n\neach variable forms a column,\neach observation forms a row,\neach type of observational unit forms a table.\n\nThe success of the tidyverse attests to the power of its underlying ideas and the quality of its implementation. Much of the code for this book has adopted these ideas and uses the tidyverse.\nNevertheless, dataframes in the long format are not a panacea. Here are some things to keep in mind:\nEfficiency and integrity. Even though there are only 4 probe-gene symbol relationships, we are repeatedly storing them 404 times in the rows of xldf. In this instance, the extra storage cost is negligible. In other cases it could be more considerable. More important is the diffusion of information: when we are given an object like xldf and want to know all the probe-gene symbol relationships it uses, we have to gather this information back from the many copies of it in the dataframe; we cannot be sure, without further checking, that the redundant copies of the information are consistent with each other; if we want to update the information, we have to change it in many places. This speaks for workflow designs in which an object like xldf is not used for long term data storage, but is assembled at a relatively late stage of analysis from more normalized17 data containers that contain the primary data objects.\n17 Data normalization is the process of organizing a database to reduce redundancy and improve integrity; see e.g. https://en.wikipedia.org/wiki/Database_normalization.\nLack of contracts and standardization. When we write a function that expects to work on an object like xldf, we have no guarantee that the column probe does indeed contain valid probe identifiers; nor that such a column even exists. There is not even a direct way to express programmatically what “an object like xldf” means in the tidyverse. Object oriented (OO) programming, and its incarnation S4 in R, solves such questions. For instance, the above-mentioned checks could be performed by a validObject method for a suitably defined class, and the class definition would formalize the notion of “an object like xldf”. Addressing such issues is behind the object-oriented design of the data structures in Bioconductor, such as the SummarizedExperiment class. Other potentially useful features of OO data representations include\n\nAbstraction of interface from implementation and encapsulation: the user accesses the data only through defined channels and does not need to see how the data are stored “inside” – which means the inside can be changed and optimized without breaking user-level code.\nPolymorphism: you can have different functions with the same name, such as plot or filter, for different classes of objects, and R figures out for you which one to call.\nInheritance: you can build up more complex data representations from simpler ones.\nReflection and self-documentation: you can send programmatic queries to an object to ask for information about itself.\n\nAll of these make it easier to write high-level code that focuses on the big picture functionality rather than on implementation details of the building blocks – albeit at the cost of more initial investment in infrastructure and “bureaucracy”.\nData provenance and metadata. There is no obvious place in an object like xldf to add information about data provenance, e.g., who performed the experiment, where it was published, where the data were downloaded from or which version of the data we’re looking at (data bugs exist \\(…\\)). Neither are there any explanations of the columns, such as units and assay type. Again, the data classes in Bioconductor try to address this need.\n\nFigure 13.14: Sequential data analyses workflows can be leaky. If insufficient information is passed from one stage to the next, the procedure can end up being suboptimal and losing power.\nMatrix-like data. Many datasets in biology have a natural matrix-like structure, since a number of features (e.g., genes; conventionally the rows of the matrix) were assayed on several samples (conventionally, columns of the matrix). Unrolling the matrix into a long form like xldf makes some operations (say, PCA, SVD, clustering of features or samples) more awkward.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#leaky-pipelines-and-statistical-sufficiency",
    "href": "13-chap.html#leaky-pipelines-and-statistical-sufficiency",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.11 13.12 Leaky pipelines and statistical sufficiency",
    "text": "15.11 13.12 Leaky pipelines and statistical sufficiency\nData analysis pipelines in high-throughput biology often work as ‘funnels’ that successively summarise and compress the data. In high-throughput sequencing, we may start with microscopy images of a flow cell, perform base calling to derive sequencing reads, then align them to a reference, then only count the aligned reads for each position, summarise positions to genes (or other kinds of regions), then “normalize” these numbers by library size to make them comparable across libraries, etc. At each step, we loose information, yet it is important to make sure we still have enough information for the task at hand18. The problem is particularly acute if we build our data pipeline from a series of components from separate developers.\n18 For instance, for the RNA-Seq differential expression analysis that we saw in Chapter 8, we needed the actual read counts, not “normalized” versions; for some analyses, gene-level summaries might suffice, for others, we’ll want to look at the exon or isoform level.\nStatisticians have a concept for whether certain summaries enable the reconstruction of all the relevant information in the data: sufficiency. In a Bernoulli random experiment with a known number of trials, \\(n\\), the number of successes is a sufficient statistic for estimating the probability of success \\(p\\).\n__\nQuestion 13.14\nIn a 4 state Markov chain (A, C, G, T) such as the one we saw in Chapter 13, what are the sufficient statistics for the estimation of the transition probabilities?\nIterative approaches akin to what we saw when we used the EM algorithm can sometimes help avoid information loss. For instance, when analyzing mass spectroscopy data, a first run guesses at peaks individually for every sample. After this preliminary spectra-spotting, another iteration allows us to borrow strength from the other samples to spot spectra that may have been overlooked (looked like noise) before.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#efficient-computing",
    "href": "13-chap.html#efficient-computing",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.12 13.13 Efficient computing",
    "text": "15.12 13.13 Efficient computing\nThe rapid progress in data acquisition technologies leads to ever large datasets, and dealing with these is a challenge. It is tempting to jump right into software technologies that are designed for big data and scalability. But usually it is more helpful to first take a step back. Software engineers know the risks of premature optimization , or to paraphrase John Tukey19: “A slow and clumsy solution to the right problem is worth a good deal more than a fast and scalable solution to the wrong problem.” Sometimes, a good strategy is to figure out what is the right solution on a subset of the data before embarking on the quest for scalability and performance.\n19 http://stats.stackexchange.com/a/744\nIt’s also good to keep in mind the value of your own time, versus CPU time. If you can save some of your time developing code, even at the cost of longer computations, that can be a worthwhile trade-off.\nHaving considered all that, let’s talk about performance. R has a reputation for being slow and wasteful of memory, and that perception is sometimes invoked to motivate choosing other platforms. In some cases, this is justified: nobody would advocate writing a short read aligner, or the steering logic of a self-driving car in R. For statistical analyses, however, it is possible to write very efficient code using one or more of these concepts:\nVectorization. Consider the following alternative choices of computing the same result.\na = runif(1e6)\nb = runif(length(a))\nsystem.time({\n  z1 = numeric(length(a))\n  for (i in seq(along = a))\n    z1[i] = a[i]^2 * b[i]\n})__\n\n\n   user  system elapsed \n  0.076   0.001   0.076 \n\n\nsystem.time({\n  z2 = a^2 * b\n})__\n\n\n   user  system elapsed \n  0.003   0.000   0.003 \n\n\nidentical(z1, z2)__\n\n\n[1] TRUE\nThe vectorized version (z2) is many times faster than the explicitly indexed one (z1) and even easier to read. Sometimes, translating an algorithm that is formulated with indices is a little harder — say, if there are if-conditions, or if the computation for index i involves results from index i-1. Language constructs such as vectorized conditionals with ifelse, shifting of vectors with functions such as lead and lag in the dplyr package, and generally the infrastructure of dplyr , which is designed to express computations on whole dataframes (rather than row by row), can help.\nParallelization. Parallelizing computations with R is easy, not least because it is a functional language in which it is natural to express computations as functions with explicit input, output, and no side effects. The landscape of R packages and functionality to support parallized computing is fast-moving; the CRAN task view “High-Performance and Parallel Computing” and the package BiocParallel are good starting points.\nOut-of-memory-data and chunking. Some datasets are too big to load into random access memory (RAM) and manipulate all at once. Chunking means splitting the data into manageable portions (“chunks”) and then sequentially loading each portion from mass storage, computing on it, storing the result and removing the portion from RAM before loading the next one. R also offers infrastructure for working with large datasets that are stored on disk in a relational database management systems (the DBI package) or in HDF5 (the rhdf5 package). The Bioconductor project provides the class SummarizedExperiment , which can store big data matrices either in RAM or in an HDF5 backend in a manner that is transparent to the user of objects of this class.\nJudicious use of lower level languages. The Rcpp package makes it easy to write portions of your code in C++ and include them seamlessly within your R code. Many convenient wrappers are provided, such as below the C++ class NumericVector that wraps the R class numeric vector.\nlibrary(\"Rcpp\")\ncppFunction(\"\n  NumericVector myfun(NumericVector x, NumericVector y) {\n    int n = x.size();\n    NumericVector out(n);\n    for(int i = 0; i &lt; n; ++i) {\n      out[i] = pow(x[i], 2) * y[i];\n    }\n    return out;\n  }\")\nz3 = myfun(a, b)\nidentical(z1, z3)__\n\n\n[1] TRUE\nIn practice, the above code should also contain a check on the length of y. Here, we provided the C++ code to Rcpp as an R character vector, and this is convenient for short injections. For larger functions, you can store the C++ code in an extra file. The idea is, of course, not to write a lot of code in C++, but only the most time critical parts.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#summary-of-this-chapter",
    "href": "13-chap.html#summary-of-this-chapter",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.13 13.14 Summary of this chapter",
    "text": "15.13 13.14 Summary of this chapter\nIn this last chapter, we have tried to collect, generalize and sort some of the concepts and ideas that popped up throughout the book, and that can help you design informative experiments or studies and analyze them effectively. Some of these ideas are intuitive and natural. Others are perhaps less intuitive, such as Hotelling’s weighting example in Section 13.4.3. It requires formal mathematical reasoning. Even when you cannot do an analytical computation, you might be able to do simulations or compute on existing, similar data to benchmark different, non-obvious design choices.\nYet again other ideas require discipline and foresight: for instance, the “dailies” might be easily forgotten or rationalized away in the heat of an experimental campaign, with so many other concerns competing for our time and attention. You might get away with skipping on keeping your kitchen tidy or eating healthily on individual occasions – as a general approach, it is not recommended.\nWe emphasized the importance of computing practices. Throughout the book, with its quantity of interweaved code and almost all “live” data visualizations, we have seen many examples of how to set up computational analyses. Nevertheless, running your own analysis on your own data is something very different from following the computations in a book – just like reading a cookbook is very different from preparing a banquet, or even just one dish. To equip you further, we highly recommend the resources mentioned in Section 13.15. And we wish you good cooking!",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#further-reading",
    "href": "13-chap.html#further-reading",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.14 13.15 Further reading",
    "text": "15.14 13.15 Further reading\n\nThis chapter presented merely a pragmatic and brief introduction to experimental design. There are many book-long treatments that offer detailed advice on setting up experiments to avoid confounding and optimize power (Wu and Hamada 2011; Box, Hunter, and Hunter 1978; Glass 2007).\nWe have not scratched the surface of more sophisticated procedures. For instance if you have the possibility of setting up a sequence of experiments that you might stop once you can make a decision, you will need to study sequential design (Lai 2001). Exploring complex response surfaces by choosing “good” starting points and then using successive results to choose further points can be very effective; Box, Draper, et al. (1987) is an invaluable resource.\nGentleman et al. (2004) explain the ideas behind Bioconductor data structures and software design, and Huber et al. (2015) give an update on how Bioconductor supports collaborative software development for users and developers.\nGit and GitHub. Jenny Bryan’s website Happy Git and GitHub for the useR is a great introduction to using version control with R.\nWickham (2014) explains the principles of tidy data.\nGood enough practices. Wilson et al. (2017) give a pragmatic and wise set of recommendations for how to be successful in scientific computing.\nThe manual Writing R Extensions is the ultimate reference for R package authoring. It can be consumed in conjunction with the Bioconductor package guidelines.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#exercises",
    "href": "13-chap.html#exercises",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.15 13.16 Exercises",
    "text": "15.15 13.16 Exercises\n__\nExercise 13.1\nSet up a simulation experiment to decide how many subjects you need, given that you know your measurements will be affected by noise that follows a symmetric Laplace distribution (infinite mixture of normal distributions as defined in Chapter 4). You will need to set up a table with different possible noise levels and effect sizes.\n__\nExercise 13.2\nUse the Bioconductor package PROPER to decide the number of samples for an RNA-Seq experiment, and compare the results to those from the RNASeqPower Bioconductor package.\n__\nExercise 13.3\nCheck out R’s model.matrix function. Read its manual page and explore the examples given there.\n__\nExercise 13.4\nGo back to one of your recent data analyses and assemble it into an R package.\n__\nSolution\n__\n\nCollect one or more recurrent operations (e.g., plots) into functions and document them with manual pages (you may use roxygen2).\nAdd the dataset under the data or inst/extdata directories.\nIf it is not already in that format, convert your analysis script to Rmarkdown.\nRun R CMD build and R CMD check until all errors and warnings disappear.\n\nA simple intro is given here: &lt;https://hilaryparker.com/2014/04/29/writing-an- r-package-from-scratch&gt;, futher details are in the manual Writing R Extensions that comes with every installation of R.\n__\nExercise 13.5\nOpen an account at GitHub and upload your package. Hint: follow the instructions at Jenny Bryan’s Happy Git and GitHub for the useR site.\n__\nExercise 13.6\nCheck out the renjin project and the renjin package. Compare code compiled with renjin with native R code, and with code translated into C/C++ with Rcpp as above.\n__\nSolution\n__\nSee the Gist at https://gist.github.com/wolfganghuber/909e14e45af6888eec384b82682b3766.\n1000 Genomes Project Consortium. 2012. “An Integrated Map of Genetic Variation from 1,092 Human Genomes.” Nature 491 (7422): 56–65.\nAltman, Naomi, and Martin Krzywinski. 2017. “Points of Significance: Interpreting p Values.” Nature Methods 14 (3): 213–14. https://doi.org/10.1038/nmeth.4210.\nBacher, Rhonda, and Christina Kendziorski. 2016. “Design and Computational Analysis of Single-Cell RNA-Sequencing Experiments.” Genome Biology 17 (1): 1.\nBox, George EP, Norman Richard Draper, et al. 1987. Empirical Model-Building and Response Surfaces. Vol. 424. Wiley New York.\nBox, George EP, William G Hunter, and J Stuart Hunter. 1978. Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building. John Wiley & Sons.\nCallahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy J Johnson, and Susan P Holmes. 2016. “DADA2: High Resolution Sample Inference from Amplicon Data.” Nature Methods , 1–4.\nDiaconis, Persi, Susan Holmes, and Richard Montgomery. 2007. “Dynamical Bias in the Coin Toss.” SIAM Review 49 (2): 211–35.\nDurbin, Richard, Sean Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis. Cambridge University Press.\nFisher, Ronald Aylmer. 1935. The Design of Experiments. Oliver & Boyd.\nGentleman, Robert C, Vincent J Carey, Douglas M Bates, Ben Bolstad, Marcel Dettling, Sandrine Dudoit, Byron Ellis, et al. 2004. “Bioconductor: Open Software Development for Computational Biology and Bioinformatics.” Genome Biology 5 (10): R80. https://doi.org/10.1186/gb-2004-5-10-r80.\nGlass, David J. 2007. Experimental Design for Biologists. Cold Spring Harbor Laboratory Press.\nHenderson, Fergus. 2017. “Software Engineering at Google.” ArXiv e-Prints. https://arxiv.org/abs/1702.01715.\nHotelling, Harold. 1944. “Some Improvements in Weighing and Other Experimental Techniques.” The Annals of Mathematical Statistics 15 (3): 297–306.\nHuber, Wolfgang, Vincent J Carey, Robert Gentleman, Simon Anders, Marc Carlson, Benilton S Carvalho, Hector Corrada Bravo, et al. 2015. “Orchestrating High-Throughput Genomic Analysis with Bioconductor.” Nature Methods 12 (2): 115–21.\nJacob, Laurent, Guillaume Obozinski, and Jean-Philippe Vert. 2009. “Group Lasso with Overlap and Graph Lasso.” In Proceedings of the 26th Annual International Conference on Machine Learning , 433–40. ACM.\nLai, Tze Leung. 2001. Sequential Analysis. Wiley Online Library.\nLeek, Jeffrey T, Robert B Scharpf, Héctor Corrada Bravo, David Simcha, Benjamin Langmead, W Evan Johnson, Donald Geman, Keith Baggerly, and Rafael A Irizarry. 2010. “Tackling the Widespread and Critical Impact of Batch Effects in High-Throughput Data.” Nature Reviews Genetics 11 (10): 733–39.\nLeek, Jeffrey T., and John D. Storey. 2007. “Capturing heterogeneity in gene expression studies by surrogate variable analysis.” PLoS Genetics 3 (9): 1724–35.\nLiberzon, Arthur, Aravind Subramanian, Reid Pinchback, Helga Thorvaldsdóttir, Pablo Tamayo, and Jill P Mesirov. 2011. “Molecular Signatures Database (MSigDB) 3.0.” Bioinformatics 27 (12): 1739–40.\nMead, Roger. 1990. The Design of Experiments: Statistical Principles for Practical Applications. Cambridge University Press.\nMood, Alexander M. 1946. “On Hotelling’s Weighing Problem.” The Annals of Mathematical Statistics , 432–46.\nRocke, David M, and Blythe Durbin. 2001. “A Model for Measurement Error for Gene Expression Arrays.” Journal of Computational Biology 8 (6): 557–69.\nSenn, Stephen. 2004. “Controversies Concerning Randomization and Additivity in Clinical Trials.” Statistics in Medicine 23: 3729–53.\nStegle, O., L. Parts, R. Durbin, and J. Winn. 2010. “A Bayesian framework to account for complex non-genetic factors in gene expression levels greatly increases power in eQTL studies.” PLoS Computational Biology 6 (5): e1000770.\nStigler, Stephen M. 2016. The Seven Pillars of Statistical Wisdom. Harvard University Press.\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10).\nWiel, Mark A, Tonje G Lien, Wina Verlaat, Wessel N Wieringen, and Saskia M Wilting. 2016. “Better Prediction by Use of Co-Data: Adaptive Group- Regularized Ridge Regression.” Statistics in Medicine 35 (3): 368–81.\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” Edited by Francis Ouellette. PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510.\nWu, CF Jeff, and Michael S Hamada. 2011. Experiments: Planning, Analysis, and Optimization. Vol. 552. John Wiley & Sons.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "14-chap.html",
    "href": "14-chap.html",
    "title": "Modern Statistics for Modern Biology",
    "section": "",
    "text": "As a supplement to the index, we provide a list of statistical concepts and procedures and the chapters in which they are covered.\n\n\n\nMethod\nChapter\n\n\n\n\nAnalysis of variance\n8 High-Throughput Count Data & Generalized Linear Models\n\n\nBayesian statistics\n2 Statistical Modeling\n\n\nBootstrap\n4 Mixture Models, 5 Clustering\n\n\nChi-square test\n2 Statistical Modeling\n\n\nClustering\n4 Mixture Models, 5 Clustering\n\n\nCorrespondence Analysis\n9 Multivariate methods for heterogeneous data\n\n\nData Transformations\n4 Mixture Models, 8 High-Throughput Count Data & Generalized Linear Models\n\n\nDiffusion Map models\n9 Multivariate methods for heterogeneous data\n\n\nDistances\n4 Mixture Models, 9 Multivariate methods for heterogeneous data\n\n\nExploratory Data Analysis (EDA)\n3 Data visualization\n\n\nFisher’s Exact test\n10 Networks and Trees\n\n\nFalse Discovery Rate (FDR)\n6 Testing\n\n\nGoodness of fit\n2 Statistical Modeling\n\n\nGeneralized Linear Models\n8 High-Throughput Count Data & Generalized Linear Models\n\n\nHypergeometric Test\n10 Networks and Trees\n\n\nHypothesis Testing\n1 Generative Models for Discrete Data, 6 Testing\n\n\nMarkov Chains\n2 Statistical Modeling, 10 Networks and Trees\n\n\nMaximum Likelihood\n2 Statistical Modeling\n\n\nMultidimensional Scaling (MDS)\n9 Multivariate methods for heterogeneous data\n\n\nMultiple hypothesis testing\n6 Testing\n\n\nMultivariate Regression\n8 High-Throughput Count Data & Generalized Linear Models, 9 Multivariate methods for heterogeneous data, 12 Supervised Learning\n\n\nOrdination and gradient detection\n9 Multivariate methods for heterogeneous data\n\n\nP-value\n1 Generative Models for Discrete Data\n\n\nPermutation tests\n10 Networks and Trees\n\n\nPhylogenetics\n10 Networks and Trees\n\n\nPower calculations\n13 Design of High Throughput Experiments and their Analyses\n\n\nPrincipal Components (PCA)\n7 Multivariate Analysis\n\n\nPrincipal Coordinates Analysis (PCoA)\n9 Multivariate methods for heterogeneous data\n\n\nRegression\n7 Multivariate Analysis,\n\n\nRobust methods\n12 Supervised Learning\n\n\nSpatial statistics\n11 Image data\n\n\n\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>14-chap.html</span>"
    ]
  },
  {
    "objectID": "15-chap.html",
    "href": "15-chap.html",
    "title": "Modern Statistics for Modern Biology",
    "section": "",
    "text": "For Sonia, Sara, Agnès, Johnny, Camille\n\\(…\\) and the “girls” who make me love the life sciences.\nFor Alexander and Daniel.\nThis work would not be imaginable without the R language and environment for statistical computing, the Comprehensive R Archive Network (CRAN) and the Bioconductor project. We thank everyone who has contributed to these projects. Today virtually every statistical algorithm, every imaginable interface for data handling and visualization, and many methods from all over computer science and mathematics are readily accessible through these projects.\nWe thank JJ Allaire and the RStudio (since 2022: Posit) team for making available such a powerful development environment and many useful R packages, which we have greatly enjoyed when writing this book. This includes the quarto publishing system, which builds upon knitr and pandoc.\nWe particularly thank the Bioconductor project, powered by its amazing community of developers, for fostering interoperability, scalability and usability of R-based methods for genome-scale data, for making a vast range of biological data and annotation resources easy to work with in R, and for orchestrating collaborative, distributed development – all these aspects are essential for the complex biological data analysis workflows that you will see in this book.\nWe are grateful to the package developers we have worked with and whose packages play at center stage in the different chapters of this book, including Simon Anders, Ben Callahan, Michael Love, Joey McMurdie, Andrzej Oleś. Trevor Martin was a student in Stats 366 at Stanford in 2012 and co-taught the class with Susan in 2013, 2014 and 2015. As a graduate student in genetics, he brought many of the examples to life and participated in earlier versions of the material we present here. We are thankful for his help and perspective. Teaching assistants for Stats 366 who have helped develop exercises and questions include Austen Head, Haben Michael, Julia Fukuyama, Lan Huong Nguyen, Christof Seiler and Nikolaos Ignatiadis. Their enthusiasm for making interesting quizzes and lab material helped nurture students from a wide range of backgrounds on the arduous journey of approaching challenging new concepts within a computational environment that has tremendous power, yet can also be overwhelming.\nMany students have provided valuable feedback over the years, and we are grateful for their many questions and quizzical looks that fed our motivation to keep evolving this course. In particular, we have received extensive feedback from Jessica Grembi, Kris Sankaran, Varun Gupta and Chao Jiang.\nMike Smith has created the infrastructure for the online HTML version that you are currently perusing, including a continuous integration framework that enables us to continually update the book and see the changes online within minutes, and the R package msmbstyle that was used for rendering the print version of the book and the first years of the HTML version. We thank Helena Lucia Crowell for her excellent work porting the book’s sources from Sweave to quarto. Sviatoslav Kharuk made further improvements. We are very grateful for their help and responsiveness.\nWe thank Lorraine Garchery for design of the cover art both for the printed and online version.\nWe thank David Tranah and Diana Gillooly from Cambridge University press for their constant effort helping us to make the book grammatically correct, aesthetically attractive and pedagogically coherent. Much potential for improvement remains, the responsibility for which stays with us.\nWe thank our family and supporters who have encouraged us and provided feedback on preliminary chapters: David Relman, Alfred Spormann, Catherine Blish, Don Knuth, Persi Diaconis, Gretchen and Barry Mazur, …..\nSusan Holmes, Stanford\nWolfgang Huber, Heidelberg\nThe following readers contributed to improvements of the online version by pointing out typos and opportunities for clarification: Eva-Maria Geissen, Nick Cox, Constantin Ahlmann-Eltze, Tümay Capraz, Irilenia Nobeli, Asger Hobolth, Iulian Ichim, …\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>15-chap.html</span>"
    ]
  },
  {
    "objectID": "16-chap.html",
    "href": "16-chap.html",
    "title": "Modern Statistics for Modern Biology",
    "section": "",
    "text": "1000 Genomes Project Consortium. 2012. “An Integrated Map of Genetic Variation from 1,092 Human Genomes.” Nature 491 (7422): 56–65.\nAbbott, Edwin A. 1884. Flatland: A Romance of Many Dimensions. OUP Oxford.\nAgresti, Alan. 2007. An Introduction to Categorical Data Analysis. John Wiley.\nAltman, Naomi, and Martin Krzywinski. 2017. “Points of Significance: Interpreting p Values.” Nature Methods 14 (3): 213–14. https://doi.org/10.1038/nmeth.4210.\nAmbroise, Christophe, and Geoffrey J. McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” PNAS 99 (10): 6562–66.\nAnders, Simon, and Wolfgang Huber. 2010. “Differential Expression Analysis for Sequence Count Data.” Genome Biology 11: R106. http://genomebiology.com/2010/11/10/R106.\nAnders, Simon, Alejandro Reyes, and Wolfgang Huber. 2012. “Detecting differential usage of exons from RNA-Seq data.” Genome Research 22 (10): 2008–17.\nAnscombe, Francis J. 1948. “The Transformation of Poisson, Binomial and Negative-Binomial Data.” Biometrika , 246–54.\nAure, Miriam Ragle, Valeria Vitelli, Sandra Jernström, Surendra Kumar, Marit Krohn, Eldri U Due, Tonje Husby Haukaas, et al. 2017. “Integrative Clustering Reveals a Novel Split in the Luminal A Subtype of Breast Cancer with Impact on Outcome.” Breast Cancer Research 19 (1): 44.\nBacher, Rhonda, and Christina Kendziorski. 2016. “Design and Computational Analysis of Single-Cell RNA-Sequencing Experiments.” Genome Biology 17 (1): 1.\nBaddeley, Adrain, Jesper Moller, and Rasmus Waagepetersen. 2000. “Non- and Semiparametric Estimation of Interaction in Inhomogeneous Point Patterns.” Statistica Neerlandica 54: 329–50.\nBaddeley, Adrian J. 1998. “Spatial Sampling and Censoring.” In Stochastic Geometry: Likelihood and Computation , edited by O. E. Barndorff-Nielsen, W. S. Kendall, and M. N. M. van Lieshout, 37–78. Chapman; Hall.\nBeisser, Daniela, Gunnar W Klau, Thomas Dandekar, Tobias Müller, and Marcus T Dittrich. 2010. “BioNet: An R-Package for the Functional Analysis of Biological Networks.” Bioinformatics 26 (8): 1129–30.\nBelkin, Mikhail, and Partha Niyogi. 2003. “Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.” Neural Computation 15 (6): 1373–96.\nBellman, Richard Ernest. 1961. Adaptive Control Processes: A Guided Tour. Princeton University Press.\nBendall, Sean C, Garry P Nolan, Mario Roederer, and Pratip K Chattopadhyay. 2012. “A Deep Profiler’s Guide to Cytometry.” Trends in Immunology 33 (7): 323–32.\nBengio, Yoshua, Jean-François Paiement, Pascal Vincent, Olivier Delalleau, Nicolas Le Roux, and Marie Ouimet. 2004. “Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering.” Advances in Neural Information Processing Systems 16: 177–84.\nBenjamini, Yoav, and Marina Bogomolov. 2014. “Selective Inference on Multiple Families of Hypotheses.” Journal of the Royal Statistical Society: Series B 76 (1): 297–318.\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society B 57: 289–300.\nBenjamini, Yoav, and Daniel Yekutieli. 2003. “Hierarchical FDR Testing of Trees of Hypotheses.” Technical report, Department of Statistics; Operations Research, Tel Aviv University.\nBerg, Stuart, Dominik Kutra, Thorben Kroeger, Christoph N Straehle, Bernhard X Kausler, Carsten Haubold, Martin Schiegg, et al. 2019. “Ilastik: Interactive Machine Learning for (Bio)image Analysis.” Nature Methods 16 (12): 1226–32.\nBhattacharya, Bhaswar B. 2015. “Power of Graph-Based Two-Sample Tests.” arXiv Preprint arXiv:1508.07530.\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\nBoland, Michael V., and Robert F. Murphy. 2001. “A neural network classifier capable of recognizing the patterns of all major subcellular structures in fluorescence microscope images of HeLa cells.” Bioinformatics 17 (12): 1213–23.\nBouckaert, Remco, Joseph Heled, Denise Kühnert, Tim Vaughan, Chieh-Hsi Wu, Dong Xie, Marc A Suchard, Andrew Rambaut, and Alexei J Drummond. 2014. “BEAST 2: A Software Platform for Bayesian Evolutionary Analysis.” PLoS Computational Biology 10 (4): e1003537.\nBox, George EP, Norman Richard Draper, et al. 1987. Empirical Model-Building and Response Surfaces. Vol. 424. Wiley New York.\nBox, George EP, William G Hunter, and J Stuart Hunter. 1978. Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building. John Wiley & Sons.\nBraak, Cajo ter. 1985. “Correspondence Analysis of Incidence and Abundance Data: Properties in Terms of a Unimodal Respose.” Biometrics 41 (January).\nBrodie, Eoin L, Todd Z DeSantis, Dominique C Joyner, Seung M Baek, Joern T Larsen, Gary L Andersen, Terry C Hazen, et al. 2006. “Application of a High- Density Oligonucleotide Microarray Approach to Study Bacterial Population Dynamics During Uranium Reduction and Reoxidation.” Applied and Environmental Microbiology 72 (9): 6288–98.\nBronštein, Il’ja N., and Konstantin A Semendjajew. 1979. Taschenbuch Der Mathematik. B.G. Teubner Verlagsgesellschaft, Leipzig; Verlag Nauka, Moscow.\nBrooks, Angela N, Li Yang, Michael O Duff, Kasper D Hansen, Jung W Park, Sandrine Dudoit, Steven E Brenner, and Brenton R Graveley. 2011. “Conservation of an RNA Regulatory Map Between Drosophila and Mammals.” Genome Research , 193–202. https://doi.org/10.1101/gr.108662.110.\nBulmer, Michael George. 2003. Francis Galton: Pioneer of Heredity and Biometry. JHU Press.\nCallahan, Ben J, Kris Sankaran, Julia A Fukuyama, Paul J McMurdie, and Susan P Holmes. 2016. “Bioconductor Workflow for Microbiome Data Analysis: From Raw Reads to Community Analyses.” F1000Research 5.\nCallahan, Benjamin J, Paul J McMurdie, and Susan P Holmes. 2017. “Exact Sequence Variants Should Replace Operational Taxonomic Units in Marker Gene Data Analysis.” ISME Journal , 1–5.\nCallahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy J Johnson, and Susan P Holmes. 2016. “DADA2: High Resolution Sample Inference from Amplicon Data.” Nature Methods , 1–4.\nCannings, Chris, and Anthony WF Edwards. 1968. “Natural Selection and the de Finetti Diagram.” Annals of Human Genetics 31 (4): 421–28.\nCaporaso, J. G., J. Kuczynski, J. Stombaugh, K. Bittinger, F. D. Bushman, E. K. Costello, N. Fierer, et al. 2010. “QIIME Allows Analysis of High-Throughput Community Sequencing Data.” Nature Methods 7 (5): 335–36.\nCarpenter, Anne E, Thouis R Jones, Michael R Lamprecht, Colin Clarke, In Han Kang, Ola Friman, David A Guertin, Joo Han Chang, Robert A Lindquist, and Jason Moffat. 2006. “CellProfiler: Image Analysis Software for Identifying and Quantifying Cell Phenotypes.” Genome Biology 7: R100.\nCarr, Daniel B, Richard J Littlefield, WL Nicholson, and JS Littlefield. 1987. “Scatterplot Matrix Techniques for Large N.” Journal of the American Statistical Association 82 (398): 424–36.\nChakerian, John, and Susan Holmes. 2012. “Computational Tools for Evaluating Phylogenetic and Hierarchical Clustering Trees.” Journal of Computational and Graphical Statistics 21 (3): 581–99.\nChaumont, Fabrice de, Stéphane Dallongeville, Nicolas Chenouard, Nicolas Hervé, Sorin Pop, Thomas Provoost, Vannary Meas-Yedid, et al. 2012. “Icy: an open bioimage informatics platform for extended reproducible research.” Nature Methods 9: 690–96.\nChen, Min, Yang Xie, and Michael Story. 2011. “An Exponential-Gamma Convolution Model for Background Correction of Illumina BeadArray Data.” Communications in Statistics-Theory and Methods 40 (17): 3055–69.\nChessel, Daniel, Anne Dufour, and Jean Thioulouse. 2004. “The ade4 Package - i: One-Table Methods.” R News 4 (1): 5–10. http://CRAN.R-project.org/doc/Rnews/.\nChiu, Sung Nok, Dietrich Stoyan, Wilfrid S. Kendall, and Joseph Mecke. 2013. Stochastic Geometry and Its Applications. Springer.\nClemmensen, Line, Trevor Hastie, Daniela Witten, and Bjarne Ersbøll. 2011. “Sparse Discriminant Analysis.” Technometrics 53: 406–13.\nCleveland, William S. 1988. The Collected Works of John w. Tukey: Graphics 1965-1985. Vol. 5. CRC Press.\nCleveland, William S., Marylyn E. McGill, and Robert McGill. 1988. “The Shape Parameter of a Two-Variable Graph.” Journal of the American Statistical Association 83: 289–300.\nCole, J. R., Q. Wang, E. Cardenas, J. Fish, B. Chai, R. J. Farris, A. S. Kulam-Syed-Mohideen, et al. 2009. “The Ribosomal Database Project: Improved Alignments and New Tools for rRNA Analysis.” Nucleic Acids Research 37 (Supplement 1): D141–45.\nCook, R. Dennis. 1977. “Detection of Influential Observation in Linear Regression.” Technometrics.\nCressie, Noel A. 1991. Statistics for Spatial Data. John Wiley; Sons.\nDiaconis, Persi, and David Freedman. 1980. “Finite Exchangeable Sequences.” The Annals of Probability , 745–64.\nDiaconis, Persi, Sharad Goel, and Susan Holmes. 2008. “Horseshoes in Multidimensional Scaling and Kernel Methods.” Annals of Applied Statistics 2: 777. https://doi.org/DOI:10.1214/08-AOAS165.\nDiaconis, Persi, and Susan Holmes. 1994. “Gray Codes for Randomization Procedures.” Statistics and Computing 4 (4): 287–302.\nDiaconis, Persi, Susan Holmes, and Richard Montgomery. 2007. “Dynamical Bias in the Coin Toss.” SIAM Review 49 (2): 211–35.\nDiday, Edwin, and M Paula Brito. 1989. “Symbolic Cluster Analysis.” In Conceptual and Numerical Analysis of Data , 45–84. Springer.\nDiggle, Peter J. 2013. Statistical Analysis of Spatial and Spatio-Temporal Point Patterns. Chapman; Hall/CRCs.\nDiGiulio, Daniel B., Benjamin J. Callahan, Paul J. McMurdie, Elizabeth K. Costello, Deirdre J. Lyelle, Anna Robaczewska, Christine L. Sun, et al. 2015. “Temporal and Spatial Variation of the Human Microbiota During Pregnancy.” PNAS.\nDundar, Murat, Ferit Akova, Halid Z. Yerebakan, and Bartek Rajwa. 2014. “A Non-Parametric Bayesian Model for Joint Cell Clustering and Cluster Matching: Identification of Anomalous Sample Phenotypes with Random Effects.” BMC Bioinformatics 15 (1): 1–15. https://doi.org/10.1186/1471-2105-15-314.\nDurbin, Richard, Sean Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis. Cambridge University Press.\nEfron, Bradley. 2010. Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction. Cambridge University Press.\nEfron, Bradley, and Robert J Tibshirani. 1994. An Introduction to the Bootstrap. CRC press.\nEfron, B., and R. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman & Hall/CRC.\nEkman, Gosta. 1954. “Dimensions of Color Vision.” The Journal of Psychology 38 (2): 467–74.\nElson, D, and E Chargaff. 1952. “On the Desoxyribonucleic Acid Content of Sea Urchin Gametes.” Experientia 8 (4): 143–45.\nFelsenstein, Joseph. 2004. Inferring Phylogenies. Boston: Sinauer.\nFinetti, Bruno de. 1926. “Considerazioni Matematiche Sull’ereditarieta Mendeliana.” Metron 6: 3–41.\nFisher, Ronald Aylmer. 1935. The Design of Experiments. Oliver & Boyd.\nFlury, Bernard. 1997. A First Course in Multivariate Statistics. Springer.\nFreedman, David A. 1991. “Statistical Models and Shoe Leather.” Sociological Methodology 21 (2): 291–313.\nFreedman, David, Robert Pisani, and Roger Purves. 1997. Statistics. New York, NY: WW Norton.\nFriedman, Jerome H. 1997. “On Bias, Variance, 0/1—Loss, and the Curse-of- Dimensionality.” Data Mining and Knowledge Discovery 1: 55–77.\nFriedman, Jerome H, and Lawrence C Rafsky. 1979. “Multivariate Generalizations of the Wald-Wolfowitz and Smirnov Two-Sample Tests.” The Annals of Statistics , 697–717.\nFukuyama, Julia, Paul J McMurdie, Les Dethlefsen, David A Relman, and Susan Holmes. 2012. “Comparisons of Distance Methods for Combining Covariates and Abundances in Microbiome Studies.” In Pac Symp Biocomput. World Scientific.\nGentleman, Robert C, Vincent J Carey, Douglas M Bates, Ben Bolstad, Marcel Dettling, Sandrine Dudoit, Byron Ellis, et al. 2004. “Bioconductor: Open Software Development for Computational Biology and Bioinformatics.” Genome Biology 5 (10): R80. https://doi.org/10.1186/gb-2004-5-10-r80.\nGlass, David J. 2007. Experimental Design for Biologists. Cold Spring Harbor Laboratory Press.\nGoslee, Sarah C, Dean L Urban, et al. 2007. “The Ecodist Package for Dissimilarity-Based Analysis of Ecological Data.” Journal of Statistical Software 22 (7): 1–19.\nGrantham, Richard, Christian Gautier, Manolo Gouy, M Jacobzone, and R Mercier. 1981. “Codon Catalog Usage Is a Genome Strategy Modulated for Gene Expressivity.” Nucleic Acids Research 9 (1): 213–13.\nGreenacre, Michael J. 2007. Correspondence Analysis in Practice. Chapman & Hall.\nGrolemund, Garrett, and Hadley Wickham. 2017. R for Data Science. O’Reilly.\nGrün, Bettina, Theresa Scharl, and Friedrich Leisch. 2012. “Modelling Time Course Gene Expression Data with Finite Mixtures of Linear Additive Models.” Bioinformatics 28 (2): 222–28. https://doi.org/10.1093/bioinformatics/btr653.\nGuillot, Gilles, and François Rousset. 2013. “Dismantling the Mantel Tests.” Methods in Ecology and Evolution 4 (4): 336–44.\nHallett, Robin M, Anna Dvorkin-Gheva, Anita Bane, and John A Hassell. 2012. “A Gene Signature for Predicting Outcome in Patients with Basal-Like Breast Cancer.” Scientific Reports 2.\nHastie, Trevor, and Werner Stuetzle. 1989. “Principal Curves.” Journal of the American Statistical Association 84 (406): 502–16.\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2008. The Elements of Statistical Learning. 2^{} ed. Springer.\nHead, Megan L, Luke Holman, Rob Lanfear, Andrew T Kahn, and Michael D Jennions. 2015. “The Extent and Consequences of p-Hacking in Science.” PLoS Biology 13 (3): e1002106.\nHeld, M., M. H. A. Schmitz, B. Fischer, T. Walter, B. Neumann, M. H. Olma, M. Peter, J. Ellenberg, and D. W. Gerlich. 2010. “CellCognition: Time-Resolved Phenotype Annotation in High-Throughput Live Cell Imaging.” Nature Methods 7: 747.\nHelmholtz, H. von. 1867. Handbuch Der Physiologischen Optik. Leipzig: Leopold Voss.\nHenderson, Fergus. 2017. “Software Engineering at Google.” ArXiv e-Prints. https://arxiv.org/abs/1702.01715.\nHoeting, Jennifer A, David Madigan, Adrian E Raftery, and Chris T Volinsky. 1999. “Bayesian Model Averaging: A Tutorial.” Statistical Science , 382–401.\nHolmes - Junca, Susan. 1985. “Outils Informatiques Pour l’évaluation de La Pertinence d’un résultat En Analyse Des Données.” PhD thesis, Université Montpellier II, France.\nHolmes, Susan. 1999. “Phylogenetic Trees: An Overview.” In Statistics and Genetics , 81–118. IMA 112. New York: Springer.\n———. 2003a. “Bootstrapping Phylogenetic Trees: Theory and Methods.” Statistical Science 18 (2): 241–55.\n———. 2003b. “Statistics for phylogenetic trees.” Theoretical Population Biology 63 (1): 17–32.\n———. 2006. “Multivariate Analysis: The French way.” In Probability and Statistics: Essays in Honor of David a. Freedman , edited by D. Nolan and T. P. Speed. Vol. 56. IMS Lecture Notes–Monograph Series. Beachwood, OH: IMS. http://www.imstat.org/publications/lecnotes.htm.\n———. 2018. “Statistical Proof? The Problem of Irreproducibility.” Bulletin of the AMS 55 (1): 31–55.\nHolmes, Susan, Alexander V Alekseyenko, Alden Timme, Tyrrell Nelson, Pankaj Jay Pasricha, and Alfred Spormann. 2011. “Visualization and Statistical Comparisons of Microbial Communities Using r Packages on Phylochip Data.” In Pacific Symposium on Biocomputing , 142–53. World Scientific.\nHolmes, Susan, Michael He, Tong Xu, and Peter P Lee. 2005. “Memory t Cells Have Gene Expression Patterns Intermediate Between Naive and Effector.” PNAS 102 (15): 5519–23.\nHolmes, Susan, Adam Kapelner, and Peter P Lee. 2009. “An Interactive Java Statistical Image Segmentation System: GemIdent.” Journal of Statistical Software 30 (10).\nHornik, Kurt. 2005. “A CLUE for CLUster Ensembles.” Journal of Statistical Software 14 (12).\nHotelling, Harold. 1933. “Analysis of a Complex of Statistical Variables into Principal Components.” Journal of Educational Psychology 24 (6): 417–41.\n———. 1944. “Some Improvements in Weighing and Other Experimental Techniques.” The Annals of Mathematical Statistics 15 (3): 297–306.\nHuber, Peter J. 1964. “Robust Estimation of a Location Parameter.” The Annals of Mathematical Statistics 35: 73–101.\nHuber, Wolfgang, Vincent J Carey, Robert Gentleman, Simon Anders, Marc Carlson, Benilton S Carvalho, Hector Corrada Bravo, et al. 2015. “Orchestrating High-Throughput Genomic Analysis with Bioconductor.” Nature Methods 12 (2): 115–21.\nHulett, Henry R, William A Bonner, Janet Barrett, and Leonard A Herzenberg. 1969. “Cell Sorting: Automated Separation of Mammalian Cells as a Function of Intracellular Fluorescence.” Science 166 (3906): 747–49.\nIdeker, Trey, Owen Ozier, Benno Schwikowski, and Andrew F Siegel. 2002. “Discovering Regulatory and Signalling Circuits in Molecular Interaction Networks.” Bioinformatics 18 Suppl 1 (January): S233–40. http://bioinformatics.oxfordjournals.org/cgi/reprint/18/suppl\\_1/S233.\nIgnatiadis, Nikolaos, and Wolfgang Huber. 2021. “Covariate Powered Cross- Weighted Multiple Testing.” Journal of the Royal Statistical Society: Series B 83: 720–51. https://doi.org/10.1111/rssb.12411.\nIgnatiadis, Nikolaos, Bernd Klaus, Judith Zaugg, and Wolfgang Huber. 2016. “Data-Driven Hypothesis Weighting Increases Detection Power in Genome-Scale Multiple Testing.” Nature Methods 13: 577–80.\nIhaka, Ross. 2003. “Color for Presentation Graphics.” In Proceedings of the 3rd International Workshop on Distributed Statistical Computing , edited by Kurt Hornik and Friedrich Leisch. Vienna, Austria: http://www.r-project.org/conferences/DSC-2003/Proceedings/; ISSN 1609-395X.\nIhaka, Ross, and Robert Gentleman. 1996. “R: A Language for Data Analysis and Graphics.” Journal of Computational and Graphical Statistics 5 (3): 299–314.\nIrizarry, R. A., B. Hobbs, F. Collin, Y. D. Beazer-Barclay, K. J. Antonellis, U. Scherf, and T. P. Speed. 2003. “Exploration, Normalization, and Summaries of High Density Oligonucleotide Array Probe Level Data.” Biostatistics 4 (2): 249–64.\nIrizarry, Rafael A, Hao Wu, and Andrew P Feinberg. 2009. “A Species- Generalized Probabilistic Model-Based Definition of CpG Islands.” Mammalian Genome 20 (9-10): 674–80.\nIzenman, Alan Julian. 2008. “Nonlinear Dimensionality Reduction and Manifold Learning.” In Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning , 597–632. New York, NY: Springer New York.\nJacob, Laurent, Guillaume Obozinski, and Jean-Philippe Vert. 2009. “Group Lasso with Overlap and Graph Lasso.” In Proceedings of the 26th Annual International Conference on Machine Learning , 433–40. ACM.\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Springer.\nJolicoeur, Pierre, and James E Mosimann. 1960. “Size and Shape Variation in the Painted Turtle. A Principal Component Analysis.” Growth 24: 339–54.\nJolliffe, Ian. 2002. Principal Component Analysis. Wiley Online Library.\nJones, T., A. Carpenter, and P. Golland. 2005. “Voronoi-Based Segmentation of Cells on Image Manifolds.” Computer Vision for Biomedical Image Applications , 535.\nJosse, Julie, and Susan Holmes. 2016. “Measuring Multivariate Association and Beyond.” Statistics Surveys 10: 132–67.\nKahneman, Daniel. 2011. Thinking, Fast and Slow. Macmillan.\nKashyap, Purna C, Angela Marcobal, Luke K Ursell, Samuel A Smits, Erica D Sonnenburg, Elizabeth K Costello, Steven K Higginbottom, et al. 2013. “Genetically Dictated Change in Host Mucus Carbohydrate Landscape Exerts a Diet-Dependent Effect on the Gut Microbiota.” PNAS 110 (42): 17059–64.\nKaufman, Leonard, and Peter J Rousseeuw. 2009. Finding Groups in Data: An Introduction to Cluster Analysis. Vol. 344. John Wiley & Sons.\nKendall, David. 1969. “Incidence Matrices, Interval Graphs and Seriation in Archeology.” Pacific Journal of Mathematics 28 (3): 565–70.\nKéry, Marc, and J Andrew Royle. 2015. Applied Hierarchical Modeling in Ecology: Analysis of Distribution, Abundance and Species Richness in r and BUGS: Volume 1: Prelude and Static Models. Academic Press.\nKorthauer, K., P. K. Kimes, C. Duvallet, A. Reyes, A. Subramanian, M. Teng, C. Shukla, E. J. Alm, and S. C. Hicks. 2019. “A practical guide to methods controlling false discoveries in computational biology.” Genome Biology 20 (1): 118.\nKozich, James J, Sarah L Westcott, Nielson T Baxter, Sarah K Highlander, and Patrick D Schloss. 2013. “Development of a Dual-Index Sequencing Strategy and Curation Pipeline for Analyzing Amplicon Sequence Data on the MiSeq Illumina Sequencing Platform.” Applied and Environmental Microbiology 79 (17): 5112–20.\nKristiansson, Erik, Michael Thorsen, Markus J Tamás, and Olle Nerman. 2009. “Evolutionary Forces Act on Promoter Length: Identification of Enriched Cis- Regulatory Elements.” Molecular Biology and Evolution 26 (6): 1299–1307.\nKuan, Pei Fen, Dongjun Chung, Guangjin Pan, James A Thomson, Ron Stewart, and Sündüz Keleş. 2011. “A Statistical Framework for the Analysis of ChIP-Seq Data.” Journal of the American Statistical Association 106 (495): 891–903.\nLai, Tze Leung. 2001. Sequential Analysis. Wiley Online Library.\nLange, Kenneth. 2016. MM Optimization Algorithms. SIAM.\nLaufer, Christina, Bernd Fischer, Maximilian Billmann, Wolfgang Huber, and Michael Boutros. 2013. “Mapping genetic interactions in human cancer cells with RNAi and multiparametric phenotyping.” Nature Methods 10: 427–31.\nLawrence, Michael S., Petar Stojanov, Paz Polak, Gregory V. Kryukov, Kristian Cibulskis, Andrey Sivachenko, Scott L. Carter, et al. 2013. “Mutational Heterogeneity in Cancer and the Search for New Cancer-Associated Genes.” Nature 499 (7457): 214–18. https://doi.org/10.1038/nature12213.\nLeek, Jeffrey T, Robert B Scharpf, Héctor Corrada Bravo, David Simcha, Benjamin Langmead, W Evan Johnson, Donald Geman, Keith Baggerly, and Rafael A Irizarry. 2010. “Tackling the Widespread and Critical Impact of Batch Effects in High-Throughput Data.” Nature Reviews Genetics 11 (10): 733–39.\nLeek, Jeffrey T., and John D. Storey. 2007. “Capturing heterogeneity in gene expression studies by surrogate variable analysis.” PLoS Genetics 3 (9): 1724–35.\nLi, Wen-Hsiung. 1997. Molecular Evolution. Sinauer Associates Incorporated.\nLi, Wen-Hsiung, and Dan Graur. 1991. Fundamentals of Molecular Evolution. Vol. 48. Sinauer Associates Sunderland, MA.\nLiberzon, Arthur, Aravind Subramanian, Reid Pinchback, Helga Thorvaldsdóttir, Pablo Tamayo, and Jill P Mesirov. 2011. “Molecular Signatures Database (MSigDB) 3.0.” Bioinformatics 27 (12): 1739–40.\nLove, Michael I., Simon Anders, Vladislav Kim, and Wolfgang Huber. 2015. “RNA- Seq Workflow: Gene-Level Exploratory Analysis and Differential Expression.” F1000Research 4 (1070). https://doi.org/10.12688/f1000research.7035.1.\nLove, Michael I, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation of Fold Change and Dispersion for RNA-seq Data with DESeq2.” Gnome Biology 15 (12): 1–21.\nMandal, Rakesh, Sophie St-Hilaire, John G Kie, and DeWayne Derryberry. 2009. “Spatial Trends of Breast and Prostate Cancers in the United States Between 2000 and 2005.” International Journal of Health Geographics 8 (1): 53.\nMardia, Kanti, John T Kent, and John M Bibby. 1979. Multiariate Analysis. New York: Academic Press.\nMarin, Jean-Michel, and Christian Robert. 2007. Bayesian Core: A Practical Approach to Computational Bayesian Statistics. Springer Science & Business Media.\nMcCormick Jr, William T, Paul J Schweitzer, and Thomas W White. 1972. “Problem Decomposition and Data Reorganization by a Clustering Technique.” Operations Research 20 (5): 993–1009.\nMcElreath, Richard. 2015. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Chapman; Hall/CRC.\nMcLachlan, Geoffrey, and Thriyambakam Krishnan. 2007. The EM Algorithm and Extensions. Vol. 382. John Wiley & Sons.\nMcLachlan, Geoffrey, and David Peel. 2004. Finite Mixture Models. John Wiley & Sons.\nMcMurdie, Paul J, and Susan Holmes. 2014. “Waste Not, Want Not: Why Rarefying Microbiome Data Is Inadmissible.” PLoS Computational Biology 10 (4): e1003531.\n———. 2015. “Shiny-Phyloseq: Web Application for Interactive Microbiome Analysis with Provenance Tracking.” Bioinformatics 31 (2): 282–83.\nMead, Roger. 1990. The Design of Experiments: Statistical Principles for Practical Applications. Cambridge University Press.\nMoignard, Victoria, Steven Woodhouse, Laleh Haghverdi, Andrew J Lilly, Yosuke Tanaka, Adam C Wilkinson, Florian Buettner, et al. 2015. “Decoding the Regulatory Network of Early Blood Development from Single-Cell Gene Expression Measurements.” Nature Biotechnology.\nMollon, John. 1995. “Seeing Colour.” In Colour: Art and Science , edited by T. Lamb and J. Bourriau. Cambridge Unversity Press.\nMood, Alexander M. 1946. “On Hotelling’s Weighing Problem.” The Annals of Mathematical Statistics , 432–46.\nMossel, Elchanan. 2003. “On the Impossibility of Reconstructing Ancestral Data and Phylogenies.” Journal of Computational Biology 10 (5): 669–76.\nMourant, AE, Ada Kopec, and K Domaniewska-Sobczak. 1976. “The Distribution of the Human Blood Groups 2nd Edition.” Oxford University Press London.\nMüllner, Daniel. 2013. “Fastcluster: Fast Hierarchical, Agglomerative Clustering Routines for r and Python.” Journal of Statistical Software 53 (9): 1–18.\nNacu, Serban, Rebecca Critchley-Thorne, Peter Lee, and Susan Holmes. 2007. “Gene Expression Network Analysis and Applications to Immunology.” Bioinformatics 23 (7, 7): 850–58. https://doi.org/10.1093/bioinformatics/btm019.\nNelson, Tyrell A, Susan Holmes, Alexander Alekseyenko, Masha Shenoy, Todd DeSantis, Cindy Wu, Gary Andersen, et al. 2010. “PhyloChip Microarray Analysis Reveals Altered Gastrointestinal Microbial Communities in a Rat Model of Colonic Hypersensitivity.” Neurogastroenterology & Motility.\nNeumann, B., T. Walter, J. K. Heriche, J. Bulkescher, H. Erfle, C. Conrad, P. Rogers, et al. 2010. “Phenotypic profiling of the human genome by time-lapse microscopy reveals cell division genes.” Nature 464 (7289): 721–27.\nNeyman, Jerzy, and Egon S Pearson. 1936. Sufficient Statistics and Uniformly Most Powerful Tests of Statistical Hypotheses. University California Press.\nNolan, Daniel J, Michael Ginsberg, Edo Israely, Brisa Palikuqi, Michael G Poulos, Daylon James, Bi-Sen Ding, et al. 2013. “Molecular Signatures of Tissue-Specific Microvascular Endothelial Cell Heterogeneity in Organ Maintenance and Regeneration.” Developmental Cell 26 (2): 204–19.\nO’Neill, Kieran, Nima Aghaeepour, Josef Špidlen, and Ryan Brinkman. 2013. “Flow Cytometry Bioinformatics.” PLoS Computational Biology 9 (12): e1003365.\nOhnishi, Y., W. Huber, A. Tsumura, M. Kang, P. Xenopoulos, K. Kurimoto, A. K. Oles, et al. 2014. “Cell-to-Cell Expression Variability Followed by Signal Reinforcement Progressively Segregates Early Mouse Lineages.” Nature Cell Biology 16 (1): 27–37.\nOzsolak, Fatih, and Patrice M Milos. 2011. “RNA sequencing: advances, challenges and opportunities.” Nature Reviews Genetics 12: 87–98.\nPagès, Jérôme. 2016. Multiple Factor Analysis by Example Using R. CRC Press.\nParadis, Emmanuel. 2011. Analysis of Phylogenetics and Evolution with r. Springer Science & Business Media.\nPau, Grégoire, Florian Fuchs, Oleg Sklyar, Michael Boutros, and Wolfgang Huber. 2010. “EBImage R Package for Image Processing with Applications to Cellular Phenotypes.” Bioinformatics 26 (7): 979–81.\nPearson, Karl. 1901. “LIII. On Lines and Planes of Closest Fit to Systems of Points in Space.” The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 2 (11): 559–72.\nPerraudeau, Fanny, Davide Risso, Kelly Street, Elizabeth Purdom, and Sandrine Dudoit. 2017. “Bioconductor Workflow for Single-Cell RNA Sequencing: Normalization, Dimensionality Reduction, Clustering, and Lineage Inference.” F1000Research 6.\nPerrière, Guy, and Jean Thioulouse. 2002. “Use and Misuse of Correspondence Analysis in Codon Usage Studies.” Nucleic Acids Research 30 (20): 4548–55.\nPounds, Stan, and Stephan W Morris. 2003. “Estimating the Occurrence of False Positives and False Negatives in Microarray Studies by Approximating and Partitioning the Empirical Distribution of p-Values.” Bioinformatics 19 (10): 1236–42.\nPrentice, IC. 1977. “Non-Metric Ordination Methods in Ecology.” The Journal of Ecology , 85–94.\nPurdom, Elizabeth. 2010. “Analysis of a Data Matrix and a Graph: Metagenomic Data and the Phylogenetic Tree.” Annals of Applied Statistics , July.\nPurdom, Elizabeth, and Susan P Holmes. 2005. “Error Distribution for Gene Expression Data.” Statistical Applications in Genetics and Molecular Biology 4 (1).\nRajaram, S., B. Pavie, L. F. Wu, and S. J. Altschuler. 2012. “PhenoRipper: software for rapidly profiling microscopy images.” Nature Methods 9: 635–37.\nReaven, GM, and RG Miller. 1979. “An Attempt to Define the Nature of Chemical Diabetes Using a Multidimensional Analysis.” Diabetologia 16 (1): 17–24.\nReyes, Alejandro, Simon Anders, Robert J. Weatheritt, Toby J. Gibson, Lars M. Steinmetz, and Wolfgang Huber. 2013. “Drift and Conservation of Differential Exon Usage Across Tissues in Primate Species.” Proceedings of the National Academy of Sciences 110 (38): 15377–82. https://doi.org/10.1073/pnas.1307202110.\nReyes, Alejandro, and Wolfgang Huber. 2017. “Alternative Start and Termination Sites of Transcription Drive Most Transcript Isoform Differences Across Human Tissues.” Nucleic Acids Research 46 (2): 582–92. https://doi.org/10.1093/nar/gkx1165.\nRhee, Soo-Yon, Matthew J Gonzales, Rami Kantor, Bradley J Betts, Jaideep Ravela, and Robert W Shafer. 2003. “Human Immunodeficiency Virus Reverse Transcriptase and Protease Sequence Database.” Nucleic Acids Research 31 (1): 298–303.\nRice, John. 2006. Mathematical Statistics and Data Analysis. Cengage Learning.\nRipley, B. D. 1988. Statistical Inference for Spatial Processes. Cambridge University Press.\nRobert, Christian, and George Casella. 2009. Introducing Monte Carlo Methods with R. Springer Science & Business Media.\nRobins, Garry, Tom Snijders, Peng Wang, Mark Handcock, and Philippa Pattison. 2007. “Recent Developments in Exponential Random Graph (p*) Models for Social Networks.” Social Networks 29 (2): 192–215.\nRobinson, M. D., D. J. McCarthy, and G. K. Smyth. 2009. “edgeR: A Bioconductor Package for Differential Expression Analysis of Digital Gene Expression Data.” Bioinformatics 26 (1): 139–40. https://doi.org/10.1093/bioinformatics/btp616.\nRocke, David M, and Blythe Durbin. 2001. “A Model for Measurement Error for Gene Expression Arrays.” Journal of Computational Biology 8 (6): 557–69.\nRonquist, Fredrik, Maxim Teslenko, Paul van der Mark, Daniel L Ayres, Aaron Darling, Sebastian Höhna, Bret Larget, Liang Liu, Marc A Suchard, and John P Huelsenbeck. 2012. “MrBayes 3.2: Efficient Bayesian Phylogenetic Inference and Model Choice Across a Large Model Space.” Systematic Biology 61 (3): 539–42.\nRosen, Michael J, Benjamin J Callahan, Daniel S Fisher, and Susan P Holmes. 2012. “Denoising PCR-Amplified Metagenome Data.” BMC Bioinformatics 13 (1): 283.\nRousseeuw, Peter J. 1987. “Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis.” Journal of Computational and Applied Mathematics 20: 53–65.\nRousseeuw, Peter J., and Annick M. Leroy. 1987. Robust Regression and Outlier Detection. Wiley. https://doi.org/10.1002/0471725382.\nRoweis, Sam T, and Lawrence K Saul. 2000. “Nonlinear Dimensionality Reduction by Locally Linear Embedding.” Science 290 (5500): 2323–26.\nRuss, John C., and F. Brent Neal. 2015. The Image Processing Handbook. 7th ed. CRC Press;\nSankaran, Kris, and Susan Holmes. 2014. “structSSI: Simultaneous and Selective Inference for Grouped or Hierarchically Structured Data.” Journal of Statistical Software 59 (1): 1–21.\nSchilling, Mark F. 1986. “Multivariate Two-Sample Tests Based on Nearest Neighbors.” Journal of the American Statistical Association 81 (395): 799–806.\nSchindelin, Johannes, Ignacio Arganda-Carreras, Erwin Frise, Verena Kaynig, Mark Longair, Tobias Pietzsch, Stephan Preibisch, et al. 2012. “Fiji: an open- source platform for biological-image analysis.” Nature Methods 9: 676–82.\nSchloss, P D, S L Westcott, T Ryabin, J R Hall, M Hartmann, E B Hollister, R A Lesniewski, et al. 2009. “Introducing mothur: Open-Source, Platform- Independent, Community-Supported Software for Describing and Comparing Microbial Communities.” Applied and Environmental Microbiology 75 (23): 7537–41.\nSchloss, P. D., A. M. Schuber, J. P. Zackular, K. D. Iverson, Young V. B., and Petrosino J. F. 2012. “Stabilization of the Murine Gut Microbiome Following Weaning.” Gut Microbes 3 (4): 383–93.\nSchölkopf, Bernhard, Koji Tsuda, and Jean-Philippe Vert. 2004. Kernel Methods in Computational Biology. MIT press.\nSchweder, T., and E. Spjøtvoll. 1982. “Plots of P-values to Evaluate Many Tests Simultaneously.” Biometrika 69: 493–502. https://doi.org/10.1093/biomet/69.3.493.\nSenn, Stephen. 2004. “Controversies Concerning Randomization and Additivity in Clinical Trials.” Statistics in Medicine 23: 3729–53.\nSerra, Jean. 1983. Image Analysis and Mathematical Morphology. Academic Press.\nSetiadi, A Francesca, Nelson C Ray, Holbrook E Kohrt, Adam Kapelner, Valeria Carcamo-Cavazos, Edina B Levic, Sina Yadegarynia, et al. 2010. “Quantitative, Architectural Analysis of Immune Cell Subsets in Tumor-Draining Lymph Nodes from Breast Cancer Patients and Healthy Lymph Nodes.” PLoS One 5 (8): e12420.\nShalizi, Cosma. 2017. Advanced Data Analysis from an Elementary Point of View. Cambridge University Press. https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf.\nSlonim, Noam, Gurinder Singh Atwal, Gašper Tkačik, and William Bialek. 2005. “Information-Based Clustering.” PNAS 102 (51): 18297–302.\nStegle, O., L. Parts, R. Durbin, and J. Winn. 2010. “A Bayesian framework to account for complex non-genetic factors in gene expression levels greatly increases power in eQTL studies.” PLoS Computational Biology 6 (5): e1000770.\nSteijger, T., J. F. Abril, P. G. Engstrom, F. Kokocinski, T. J. Hubbard, R. Guigo, J. Harrow, et al. 2013. “Assessment of transcript reconstruction methods for RNA-seq.” Nature Methods 10 (12): 1177–84.\nStigler, Stephen M. 2016. The Seven Pillars of Statistical Wisdom. Harvard University Press.\nStorey, John D. 2003. “The Positive False Discovery Rate: A Bayesian Interpretation and the q-Value.” The Annals of Statistics 31 (6). https://doi.org/10.1214/aos/1074290335.\nStrang, Gilbert. 2009. Introduction to Linear Algebra. Fourth. Wellesley- Cambridge Press.\nTenenbaum, Joshua B, Vin De Silva, and John C Langford. 2000. “A Global Geometric Framework for Nonlinear Dimensionality Reduction.” Science 290 (5500): 2319–23.\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society. Series B (Methodological) , 267–88.\nTibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. “Estimating the Number of Clusters in a Data Set via the Gap Statistic.” JRSSB 63 (2): 411–23.\nTrosset, Michael W, and Carey E Priebe. 2008. “The Out-of-Sample Problem for Classical Multidimensional Scaling.” Computational Statistics & Data Analysis 52 (10): 4635–42.\nTseng, George C, and Wing H Wong. 2005. “Tight Clustering: A Resampling-Based Approach for Identifying Stable and Tight Patterns in Data.” Biometrics 61 (1): 10–16.\nTukey, John W. 1977. “Exploratory Data Analysis.” Massachusetts: Addison- Wesley.\nTversky, Amos, and Daniel Kahneman. 1974. “Heuristics and Biases: Judgement Under Uncertainty.” Science 185: 1124–30.\n———. 1975. “Judgment Under Uncertainty: Heuristics and Biases.” In Utility, Probability, and Human Decision Making , 141–62. Springer.\nVerhulst, Pierre-François. 1845. “Recherches mathématiques Sur La Loi d’accroissement de La Population.” Nouveaux Mémoires de l’Académie Royale Des Sciences Et Belles-Lettres de Bruxelles 18: 1–42.\nVetterli, Martin, Jelena Kovačević, and Vivek Goyal. 2014. Foundations of Signal Processing. Cambridge University Press.\nWang, Q., G. M. Garrity, J. M. Tiedje, and J. R. Cole. 2007. “Naive Bayesian Classifier for Rapid Assignment of rRNA Sequences into the New Bacterial Taxonomy.” Applied and Environmental Microbiology 73 (16): 5261.\nWasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA’s Statement on p-Values: Context, Process, and Purpose.” The American Statistician.\nWertheim, Joel O, and Michael Worobey. 2009. “Dating the Age of the SIV Lineages That Gave Rise to HIV-1 and HIV-2.” PLoS Computational Biology 5 (5): e1000377.\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28.\n———. 2014. “Tidy Data.” Journal of Statistical Software 59 (10).\n———. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer New York. http://had.co.nz/ggplot2/book.\nWiel, Mark A, Tonje G Lien, Wina Verlaat, Wessel N Wieringen, and Saskia M Wilting. 2016. “Better Prediction by Use of Co-Data: Adaptive Group- Regularized Ridge Regression.” Statistics in Medicine 35 (3): 368–81.\nWilkinson, Leland. 1999. “Dot Plots.” The American Statistician 53 (3): 276.\n———. 2005. The Grammar of Graphics. Springer.\nWills, Quin F, Kenneth J Livak, Alex J Tipping, Tariq Enver, Andrew J Goldson, Darren W Sexton, and Chris Holmes. 2013. “Single-Cell Gene Expression Analysis Reveals Genetic Associations Masked in Whole-Tissue Experiments.” Nature Biotechnology 31 (8): 748–52.\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” Edited by Francis Ouellette. PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510.\nWitten, Daniela M, and Robert Tibshirani. 2011. “Penalized Classification Using Fisher’s Linear Discriminant.” JRSSB 73 (5): 753–72.\nWitten, Daniela M, Robert Tibshirani, and Trevor Hastie. 2009. “A Penalized Matrix Decomposition, with Applications to Sparse Principal Components and Canonical Correlation Analysis.” Biostatistics , kxp008.\nWright, Erik S. 2015. “DECIPHER: Harnessing Local Sequence Context to Improve Protein Multiple Sequence Alignment.” BMC Bioinformatics 16 (1): 1.\nWu, CF Jeff, and Michael S Hamada. 2011. Experiments: Planning, Analysis, and Optimization. Vol. 552. John Wiley & Sons.\nYu, Hongxiang, Diana L Simons, Ilana Segall, Valeria Carcamo-Cavazos, Erich J Schwartz, Ning Yan, Neta S Zuckerman, et al. 2012. “PRC2/EED-EZH2 Complex Is up-Regulated in Breast Cancer Lymph Node Metastasis Compared to Primary Tumor and Correlates with Tumor Proliferation in Situ.” PloS One 7 (12): e51239.\nZeileis, Achim, Christian Kleiber, and Simon Jackman. 2008. “Regression Models for Count Data in R.” Journal of Statistical Software 27 (8). http://www.jstatsoft.org/v27/i08/.\nZeller, Georg, Julien Tap, Anita Y Voigt, Shinichi Sunagawa, Jens Roat Kultima, Paul I Costea, Aurélien Amiot, et al. 2014. “Potential of Fecal Microbiota for Early-Stage Detection of Colorectal Cancer.” Molecular Systems Biology 10 (11): 766. https://doi.org/10.15252/msb.20145645.\nZou, Hui, Trevor Hastie, and Robert Tibshirani. 2006. “Sparse Principal Component Analysis.” Journal of Computational and Graphical Statistics 15 (2): 265–86.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>16-chap.html</span>"
    ]
  }
]