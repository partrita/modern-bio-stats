[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modern Statistics for Modern Biology",
    "section": "",
    "text": "1 홈\n이 책의 챕터 간 이동은 상단의 드롭다운 메뉴(“Chapters”)를 이용해 주세요.\n책 인용 방법: Modern Statistics for Modern Biology, by Susan Holmes and Wolfgang Huber, Cambridge University Press (2019). ISBN: 9781108705295.\n라이센스: CC BY-NC-SA\n소개\n이산 데이터를 위한 생성 모델\n통계적 모델링\n데이터 시각화\n혼합 모델\n클러스터링\n가설 검정\n다변량 분석\n고처리량 카운트 데이터 & 일반화 선형 모델\n이질적 데이터를 위한 다변량 방법\n네트워크와 트리\n이미지 데이터\n지도 학습\n고처리량 실험 설계 및 분석\n통계적 일치성\n감사의 말\n참고 문헌",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>홈</span>"
    ]
  },
  {
    "objectID": "index.html#독자들에게",
    "href": "index.html#독자들에게",
    "title": "Modern Statistics for Modern Biology",
    "section": "1.1 독자들에게",
    "text": "1.1 독자들에게\n이 책의 인쇄본(아래 참조)은 2019년에 출판되었지만, 온라인 버전은 계속 업데이트하고 있습니다. 오류, 실수, 오타를 발견하시면 알려주세요. 최선을 다해 수정하겠습니다.\n코드 예제와 관련하여: R과 CRAN 및 Bioconductor의 패키지들은 매우 역동적인 환경입니다. 인쇄본은 2018년 R 3.5와 Bioconductor 3.7을 기준으로 마무리되었습니다. 우리는 R이나 패키지의 변경 사항에 맞춰 코드를 지속적으로 업데이트하고 있습니다. 여기에서 보시는 내용은 2025-06-13 기준 R 버전 4.5.1과 2025-09-01 기준 최신 패키지 버전을 사용하여 빌드되었습니다. 자동화된 탐지로 발견하기 어려운 의도치 않은 변경 사항(예: 플롯 출력이나 특정 계산 결과)을 간과했을 수 있습니다. 우리는 이러한 위험이 독자들이 최신 컴퓨팅 환경에서 작업할 수 있도록 하는 가치 있는 대가라고 생각하지만, 혼란을 드릴 수 있는 점에 대해 사과드립니다.\n발견하신 점이 있다면 wolfgang.huber [at] embl.org 로 이메일을 보내주세요.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>홈</span>"
    ]
  },
  {
    "objectID": "index.html#코드와-데이터",
    "href": "index.html#코드와-데이터",
    "title": "Modern Statistics for Modern Biology",
    "section": "1.2 코드와 데이터",
    "text": "1.2 코드와 데이터\n\n1.2.1 패키지 설치\n이 책의 모든 코드 예제를 실행하는 데 필요한 모든 패키지는 다음 명령어로 설치할 수 있습니다:\nsource(\"https://www.huber.embl.de/msmb/install_packages.R\")\n\n\n1.2.2 데이터\n\n데이터 파일 (zip 폴더)\nlast modified: 2022-12-15 16:05:03\nmd5 hash: 4aefffbcd826d9645b9e0e5b12274f07 -\n\n\n\n1.2.3 코드\n\nR 코드 (zip 폴더)\n\n\n\n1.2.4 전체 책 전자 사본 다운로드\n\n책 HTML 트리 (zip)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>홈</span>"
    ]
  },
  {
    "objectID": "index.html#실물-책",
    "href": "index.html#실물-책",
    "title": "Modern Statistics for Modern Biology",
    "section": "1.3 실물 책",
    "text": "1.3 실물 책\nModern Statistics for Modern Biology의 하드카피를 원하신다면 Cambridge University Press 에서 구매하실 수 있습니다.\n\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>홈</span>"
    ]
  },
  {
    "objectID": "00-chap.html",
    "href": "00-chap.html",
    "title": "2  주제: 이질성",
    "section": "",
    "text": "2.1 이 책에는 어떤 내용이 있나요?\n이 책 제목에 있는 _현대_라는 두 가지 권위자는 생물학 데이터 분석에 이어 최근의 두 가지 주요 혁명을 불러일으켰습니다.\n이 책의 목적은 생물학 연구에 도움을 주는 세션들이 실험과 기타 활용 가능한 데이터를 최대한 활용하는 데 필요한 많은 중요한 아이디어와 방법을 빠르게 배울 수 있도록 하는 것입니다. 이 책은 접근 방식을 취합니다. 내러티브는 질문 클래스나 특정 데이터 유형에 따라 결정됩니다. 방법과 운영이 필요한 경우 소개합니다. 첫 번째 원칙으로부터 우리는 의도적으로 노력하지 않습니다. 이 책은 종종 분야별 수영장에 빠뜨리고 너무 많은 세부 사항이 있는 경우가 많습니다.\n이 책은 재미있는, 스쿠터 대수학, 미적분학, 컴퓨터 과학, 데이터베이스, 다변량 통계 등 기본에 대한 교육을 거부하지 않습니다. 이러한 훈련에는 여러 학기의 교육과 과정이 필요합니다. 아마도 이 책은 그러한 분야 중 하나에 더 많은 부분을 차지하고 싶은 활력을 자극할 것입니다.\n모든 생물학적 시스템이나 본체는 수만개 구성 요소로 구성되고, 다른 생물학적 시스템이나 본체에 있을 수 있으며 다양한 방식으로 결합할 수 있습니다. 영양학은 다중 공변량 및 거대 조합을 통해 시간적, 공간적 특성에서 전반적인 인(이는 고차원적) 데이터를 획득하여 시스템을 이해하는 것을 목표로 합니다. 이러한 것을 처리하는 것이 우리의 주요 과제가 될 것입니다. 여기에는 항상 존재하는 도구를 사용하여 얻을 수 있는 데이터의 실제적 연결과 이질성 자산이 아니라 실제 연결도 포함됩니다.\n생물학 데이터는 사냥꾼 및 아웃백 링, 조정 가능한 테이블, 다중 테이블, 스트링, 배치 요인, 표현형 이미지, 우주 탐색 등 모든 종류의 형태로 제공됩니다. 실험실 실험에서 측정된 데이터 외에 기능적 데이터, 환경 감시, 공간 및 시간 측정, 블록 및 시스템보리, 자유 텍스트 또는 제어 가능한 로분석 데이터베이스에 이전에 있는 지식의 소유자가 있습니다. \\(…\\)\n이 책의 컴퓨팅 플랫폼으로 R과 Bioconductor를 제외하는 것은 이러한 이질성 현상입니다. 이에 대한 자세한 내용은 아래에서 설명합니다.\n그림 1: R.A.가 추천하는 수준 이하 Fisher 테스트는 데이터를 수집하기 전에 귀무 시리즈의 공식화와 실험 설계부터 시작됩니다. 모델 피팅에 비슷한 방식으로 생각할 수 있습니다. _가설 H0_을 _파라메트릭 모델_과 _매개 다양한 맞춤형_으로 지정면입니다.\n그림 1은 분석 데이터를 분석하여 보기 쉽게 설명합니다. 1930년대 Fisher(1935)와 Neyman 및 Pearson(1936)의 중요한 성과에 대해 알아보기 위해 수학적 구조에 잘 맞으며, 이에 따라(null null 또는 null 또는 또는) 또는 그렇죠.\n그림 2: J.W. Tukey는 데이터로 분석을 시도하는 것을 권장하며 다음과 같이 답변해 드립니다. “어떤 기술 장비도 예상할 수 있는 관계 없이 볼 수 있는 것을 원하지 않을 것입니다.” (홈즈 - Junca 1985).\n실제적인 발견은 그림 1의 캐리커처 방식으로 거의 작동하지 않습니다. Tukey(1977)는 두 가지 별개의 접근 방식을 강조했습니다. 그는 첫 번째 번째를 탐색적 데이터 분석(EDA)이라고 합니다. EDA는 데이터 자체를 사용하여 통계 분석 방법을 결정합니다. EDA는 데이터 분석을 기반으로 구축된 확증 데이터 분석(CDA)으로 보완됩니다. 즉, 이상적으로는 끌어당기고 가정에 의존하지 않는 관심을 끌 수 있는 방법입니다. Tukey는 다양한 관점에서 데이터를 볼 수 있도록 그림 2에 도식화된 반복 방식을 권장했습니다. 이를 통해 데이터와 자연 현상에 대한 이해를 지속적으로 개선할 수 있습니다.\n1990년부터 생물학은 Big \\(p\\), 작은 \\(n\\) 문제를 제안했습니다. \\(p=20000\\) 코로나19에 대한 \\(n=200\\) 인구 통계 추출에 대한 이벤트 데이터 세트를 고려하세요. \\(20000\\) 제한 또는 특징으로부터 검증 가능(예: 가지 유형 또는 결과)를 “예측”하는 또는 유일하게 모델을 구축하려는 경우에는 다양한 모델을 확장할 수 있는 것보다 훨씬 더 클 수 있으므로 문제에 직면하게 됩니다. 일부는 개별적인 비식별성 또는 _과적합_으로 인해 발생합니다. 신형은 일반적인 다변형 모델과 동일한 모델의 경우입니다. 통계학자들은 표준화화 기술(Hastie, Tibshirani, and Friedman 2008)을 사용하여 희소성을 요구함으로써, 즉 많은 사람들을 위해 분자가 0이어야 해서 가까이에 요구함으로써 상황을 끌 수 있다는 것을 보았습니다.\n희소성 원리의 일반화는 경험적 뷰즈라는 이름으로 진행되는 고차원 통계에서 가장 최신 아이디어 중 하나로 호출됩니다. 즉, 모든 종류의 소규모를 처음부터 학습하려고 시도하지 않고 다양한 종류의 그룹과 동일하거나 심지어는 사실을 사용한다는 사실입니다. 합리적인 논리 및 합리적인 테스트에 대한 권한이 있는 주제에 대한 몇 가지 중요한 책이 있습니다.(Efron 2010)이 있습니다. 2010년대에 컴퓨터는 분류적으로 특정 특성을 가지거나 고유한 특성에 대해 너무 걱정하고 때때로 예기할 품질을 제공할 수 있는 “심층 신경망”을 결합하는 방법을 발견했습니다.\n카로 외곽법을 사용하는 시기의 분류하기 위해 룰렛 아이콘을 사용하겠습니다. 이 방법의 이름은 게임의 챔피언십과 동일하게 사용하기 쉽습니다. 아이러니하게도 많은 게임의 경우 승리는 분석적으로 이루어졌으며 자체 경험 데이터를 사용하여 승리를 재미있게 평가합니다.\n_시뮬레이션_은 우리가 필요로 하는 많은 결과가 표준 분석 접근 방식의 범위에서 벗어나기 때문에 이 책에서 작은 역할을 하게 됩니다. 즉, 조립품은 “종이와 연필 수학”으로 잊을 수 있는 방법만 있을 수 있는 방법과 가정 내부를 사랑하는 것이 대신하는 것에서 우리를 포함하는 것입니다.\n이 책에서 우리는 이러한 개발의 광범위한 스펙트럼과 현재 생물학 연구에 대해 계속 노력하겠습니다. 우리는 RNA-seq, 유세포 분석, 분류군 관련도, 소속 데이터 및 단일 세포 측정을 포함하여 현대 생물학자가 처리해야 하는 다양한 유형의 데이터를 다룹니다. 우리는 통계에 대한 사전 교육을 제공하지 않습니다. 그러나 R에 대한 어느 정도의 정도는 있지만 합리적이고 분석적 사고에 참여하려는 의지가 필요합니다.\n아래 아이콘에 메모와 추가 정보를 포함하는 것입니다. 세부사항을 관리하는 타법입니다.\n다음에서 설명하는 것처럼 이 책의 각 장은 서로 연결되어 있지만 합리적으로 구성원으로서는 이웃에 거주할 수도 있습니다. 각 항아리와 목표 섹션으로 시작되었습니다. 본문에 있는 질문은 귀하가 이에 따라 확인하는 데 도움이 됩니다. 이 문자에는 전체 R 코드 예제가 포함되어 있습니다. HTML에서 R 코드를 스크랩하거나 책에서 문서로 복사할 필요가 없습니다. 이 웹사이트에 있는 R 파일(확장자 .R)을 사용하세요. 각 부분의 주요 부분을 요약하고 실천하는 데 중점을 둡니다.\n_생성 모델_은 우리의 기본 구성요소입니다. 변환된 데이터에 대한 결론을 내리려면 생성된 데이터에 대한 간단한 모델을 사용하는 것이 유용한 경향이 있습니다. 우리는 1 이산 데이터에 대한 생성 모델에서 소개하는 재미있는 실험과 모델을 사용하여 이를 재현합니다. 면역학 및 DNA 분석의 예를 사용하여 생물학 데이터(이항, 다항 및 포아송 있음)에 유용하게 사용할 수 있는 생성 모델을 설명합니다.\n특정 모델에서 데이터가 어떻게 보이는지 서로 알게 되서 같이 작업할 수 있습니다. 특정 데이터가 특별한 면 어떤 모델이 가장 잘 설명할 수 있는지? 이러한 _상향 방식 접근 방식_은 측정 사고의 핵심이며, 2분석 모델링에서 설명합니다.\nTukey의 계획(그림 2)에서 _graphics_의 주요 역할을 살펴보았으면 3 데이터 수집에서 데이터를 끌어내는 방법을 이해하겠습니다. 그래픽과 ggplot2를 사용하겠습니다.\n실제 생물학 데이터는 1 이산 데이터에 대한 생성 모델에서 독립할 수 있는 것보다 더 먼 곳에서 조종하는 경우에 있습니다. 우리는 4개의 파일을 우리에서 검색하는 _mixtures_를 사용할 것입니다. 이를 통해 우리는 이질적인 생물학 데이터에 대한 본질적인 모델을 구축하고 의도적인 방향 전환을 선택하기 위해 노력할 기반을 제공할 수 있습니다.\n영양의 파티션 매트릭스와 같은 데이터 세트는 _클러스터링_에 적합합니다. 일단 매트릭스 행렬(특징) 사이의 거리 측정 값을 정의하면 경계 패턴의 동일성에 따라 클러스터링하고 그룹화할 수 있도록 열(개인 샘플)과 같은 것입니다. 클러스터링에서 5개의 클러스터링에서 계속하겠습니다. 클러스터링은 거리에만 의존하기 때문에 높이가 아닌 측정에도 불구하고 사이에 있는 거리에 있는 것을 적용할 수 있습니다.\nEDA의 길을 따라 7 다변량 분석에서 캄보디아에 대한 가장 기초적인 비지도 분석 방법인 _주성분 분석_을 다뤘습니다. 9 이종 데이터에 대한 다변량 방법에서는 다양한 데이터 유형을 조사하는 것보다 이질적인 데이터를 분석합니다. 여기서는 단일 셀 데이터의 검토를 계산하는 비선형 비지도 방법을 살펴보겠습니다. 또한 7 다변량 분석에서 일반적인 다변량 접근 방식의 일반화를 시도하는 방법과 비슷한 방식으로 기록된 여러 분석에 사용하는 방법도 더 많은 것입니다.\n그림 1에 설명된 기본 기능 테스트 플로는 6 테스트에 설명되어 있습니다. 우리는 \\(np\\)-데이터 세트에 대한 가장 일반적인 쿼리 중 하나에 적용할 수 있는 기회를 이용합니다. 즉, 약간의 고무(특징)가 샘플의 특정 속성(예: 어린이 유형 또는 결과)과 _관련_포함되어 있습니까? 그러나 원래의 참고 사항으로 특별한 값을 만들 수 없습니다. \\(\\)의 불가능하다고 인정하면 \\(p=20000\\)이 가능합니다. 따라서 우리는 또한 여러 테스트를 처리해야 합니다.\n통계에서 가장 유용하다는 생각 중 하나는 해석하기, 분산 분석(ANOVA)입니다. 8개의 높은 처리량 데이터 및 일반화 라이더 모델에서 라이더와 일반화 라이더의 프레임워크에 대해 살펴보겠습니다. 우리는 RNA-Seq 실험에서 예시 데이터를 훈련했기 때문에 우리는 실험 데이터에 대한 모델과 _강건성_의 개념을 논의할 기회도 제공합니다.\n생물학의 어떤 변화의 소형을 제외하고 의미가 있고, 진화된 관계는 발생수에 유용하게 코드화되어 있습니다. 10개의 네트워크 및 트리에서 네트워크와 트리를 살펴보겠습니다.\n1 테오도시우스 도브잔스키,\nhttps://en.wikipedia.org/wiki/Nothing_in_Biology_Makes_Sense_Except_in_the_Light_of_Evolution\n생물학의 풍부한 데이터 소스는 이미지이며, 11 이미지 데이터에서는 이미지 및 공간 통계의 특징 추출을 탐색하여 모든 종류의 이질적인 데이터 유형에 대해 EDA를 추구하는 의지를 강화합니다.\n12 지도 학습에서 지도 학습을 살펴봅니다. 즉, 각에 대한 다변량 특징이 있기 때문에 다양한 종족을 구별하는 것을 훈련시킵니다. 저차원적인 벡터와 사용하는 방법을 간단하게 설명하는 다음 고차원 설정에서 일부 문제를 살펴보겠습니다. 여기에 (적어도 개념적으로) Translations 구별되는 훈련 세트를 동시에 동시에 배울 수 있는 ‘고전적인’ 지도 학습에 놀라워합니다. 우리는 (아직…) 하나의 에이전트가 연속적으로 작업을 수행하고 그에 대한 반응을 수용하는 환경과 인력 작동하여 학습하는 것보다 유연한 프레임워크인 학습을 계속합니다. 따라서 해당 프레임워크에는 time 및 state 개념이 있습니다.\n실험 설계 및 데이터 분석의 모범 사례에 대한 고려 사항을 포함하여 13 고컴션 실험 설계 및 분석을 도와드립니다. 이를 위해 우리는 이전 장에서 연결 내용을 사용하고 할 것입니다.\n그림 3: 데이터 분석은 1단계 프로세스가 아닙니다. 각 단계에는 데이터의 일부를 수리하고 수리하는 작업이 포함됩니다. Tukey의 반복 데이터 구조는 \\(Total=V_1+V_2+V_3\\)으로 개념화될 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>주제: 이질성</span>"
    ]
  },
  {
    "objectID": "00-chap.html#이-책에는-어떤-내용이-있나요",
    "href": "00-chap.html#이-책에는-어떤-내용이-있나요",
    "title": "2  주제: 이질성",
    "section": "",
    "text": "악마 아이콘 아래에 메모와 추가 정보를 포함할 것입니다. 세부 사항을 포함하는 악마입니다.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>주제: 이질성</span>"
    ]
  },
  {
    "objectID": "00-chap.html#생물학-현대식-그래픽-도구를-사용하는-방법",
    "href": "00-chap.html#생물학-현대식-그래픽-도구를-사용하는-방법",
    "title": "2  주제: 이질성",
    "section": "2.2 생물학 현대식 그래픽 도구를 사용하는 방법",
    "text": "2.2 생물학 현대식 그래픽 도구를 사용하는 방법\n계속해서 살펴보지만, 분석 접근 방식, 도구 및 선택 사항은 다양합니다. 우리 게임은 확장 가능한 파일을 보호하기 위해 보존 형식으로 보관할 수 있습니다. R과 Bioconductor는 이러한 플랫폼을 제공합니다.\n우리는 다양한 유형의 데이터, 질문 및 측정 방법을 직접적으로 관리하지만 모든 노력을 한 지붕 아래에 유지하여 노력하는 방식을 유지합니다. 즉, Bioconductor 프로젝트의 생물학 데이터 수집 및 보호하는 방법을 통합하여 R 프로그래밍 언어 및 통계 환경을 제공합니다. 독자는 책을 사용하기 전에 R에 어느 정도 익숙해지는 것부터 시작해야 합니다. 좋은 책과 온라인 자료가 많이 있습니다. 그 중 하나는 Grolemund와 Wickham(2017)이 온라인으로 http://r4ds.had.co.nz에서 만든 것입니다.\nR 코드는 이 책의 주요 구성 요소입니다. 이것이 우리가 문자적으로 설명하는 방법입니다. 본질적으로 책을 받아들이는 모든 데이터는 코드로 만들 수 있고, 독자는 그 구성원 모두와 다른 결과를 복제할 수 있습니다.\n모든 코드가 HTML 페이지에 표시되는 것은 아닙니다. 특별히 코드가 동작 교육 목표에 직접적으로 도움이 되지 않는 경우에는 설명도 있습니다. 그러나 모든 코드는 이 웹페이지에서 제공되는 .R 파일을 사용할 수 있습니다.\nR에 어느 방향으로 가든지 책의 모든 코드 줄을 즉각적으로 이해할 수 없도록 주의하세요. 코드를 내부적으로 유지하고 제외하도록 부분에 대한 팁과 힌트를 제공하려고 노력했지만\n\n이전에 본인이 훌륭한 일을 할 자격을 얻었습니다.\n이해하지 못하는 것이 R에 연결되어 있습니다(아마도 dplyr패이스의 apply 또는 데이터 조작과 관련됨).\n\n그렇지 않을 말 것. 저작권에 관한 설명 페이지를 찾아보세요. RStudio를 참여시키려는 노력을 기울여 봅니다. 존중을 기본으로 하여 확인합니다.\n1 이산 데이터를 생성하는 모델 및 2 통계 모델링 장에서는 가벼운 내용 및 데이터 처리를 위해 base R 기능을 사용합니다. 점점 더 멀리 갈 필요해짐에 따라 3 분석 요구 그래픽을 만드는 ggplot2 방식을 소개합니다. 정도의 집합을 사용하여 분량을 생성할 수 있을 정도로 그래픽 개념을 유지한다는 것을 제외하고 dplyr 방식의 데이터를 사용하는 것을 의미합니다.\n\n2.2.0.1 왜 R과 바이오컨덕터인가?\n우리가 R(Ihaka and Gentleman 1996) 및 Bioconductor(Huber et al. 2015) 우리 플랫폼에 대한 분석을 제시하기로 선택하는 데는 여러 가지 이유가 있습니다.\n책을 작성하는 방법을 따라 R 및 Rstudio를 다운로드하세요.\n최첨단 솔루션. \\(10,000\\)개 이상 패키지를 사용할 수 있으며 거의 ​​모든 통계 방법을 사용할 수 있습니다. 또한 컴퓨터 과학, 수학, 기계 학습, 데이터 관리, 매력 및 인터넷 기술의 다양한 방법을 구현하거나 그에 대한 인터페이스가 있습니다. 이 곳 전역에서 수많은 전문가의 도움을 받을 수 있습니다.\n오픈 소스 및 커뮤니티 소유권. R과 Bioconductor는 개발자 커뮤니티에 의해 공동으로 구축되었습니다. 수많은 사용자가 시도하고 시도합니다.\n데이터 입력 및 랭글링. 바이오컨덕터 패키지는 현대 생물학에서 사용되는 측정 장비에서 생성된 다양한 데이터 유형 및 형식의 확장과 필요한 기술별 “전처리” 루틴을 지원합니다. 커뮤니티는 경쟁 시장의 경쟁적인 발전에 비해 최신 정보를 공유하고 유지하고 있습니다.\n시뮬레이션. 모든 측정 군에 대한 난수 생성 기와 대수, 최적화 등을 끌어올리는 부품 리듬이 있습니다.\n시각화 및 프리젠테이션. R은 매력적인 품질의 그래픽을 만들 수 있습니다. 이에 대한 내용을 3개의 데이터에 만족시키도록에 정리하고 책에 충실해야 합니다.\n사용하기 쉬운 대화형 개발 환경. RStudio는 사용하기 쉽고 R 프로그래밍의 모든 측면에 도움이 됩니다. 그림 2에 대한 도식화된 데이터 분석에 대한 반복적 접근 방식을 구분하는 부분입니다.\n재현성. 실험실 작업의 표준 모범 사례인 실험실 문제와 동일하게 R 마크다운 또는 4절판 형식으로 하중을 받는 일기의 사용을 옹호합니다. 우리는 quarto 시스템을 사용하여 해당 파일을 읽을 수 있으며 HTML 또는 PDF 문서로 변환할 수 있습니다. 그렇다면 과학 기사나 보충 자료가 있을 수도 있습니다. 버전 제어 시스템과 함께 이 접근 방식은 변경 사항을 추적하는 데도 도움이 됩니다.\n협업 환경. Quarto를 사용하면 처벌의 작업으로 코드, 텍스트, 그림 및 표가 포함된 웹사이트를 만들 수 있습니다.\n풍부한 데이터 구조. 바이오컨덕터 프로젝트는 결합된 생물학 데이터 세트를 통해 사용자 데이터 컨테이너를 정의했습니다. 데이터를 일관되게 안전하게 사용하기 쉽게 유지하는 데 도움이 됩니다.\n상호 조종성 및 분리 개발. 특히 바이오컨덕터에는 광범위한 기능을 포괄하지만 쿼터제 가능 능력이 있는 능력을 갖춘 다양한 작성자의 세대가 있습니다.\n문서화. 많은 R 패키지에는 매뉴얼 페이지와 그림에 훌륭한 문서가 포함되어 있습니다. 비네트는 일반적인 패키지의 기능에 대한 높은 수준의 설명을 제공하는 반면, 매뉴얼 페이지는 각 기능의 입력, 출력 및 내부 작동에 대한 정보를 제공하므로 일반적으로 패키지의 가장 좋은 시작점입니다. R 및 Bioconductor 작업의 다양한 측면에 대한 온라인 안내, 대표 및 메일링 목록이 있습니다.\n고급 언어. R은 해석된 특수 언어입니다. LISP에 오류를 범할 수 있도록 프로그래밍 기능은 코드가 데이터이고 계산될 수 있다는 것을 의미하며 효율적인 프로그래밍을 가능하게 하고 로그합니다. 이러한 기능은 강력한 강력한 언어 구축을 가능하게 해줍니다.2. R은 고정된 언어가 아닙니다. 역사에 있어서 R은 뚜렷하게 발전하고 있으며 개선되고 있습니다.\n2 예에는 R의 수식 인터페이스, ggplot2의 그래픽 기호, dplyr의 데이터 처리 기능 및 R 마크다운이 포함됩니다.\n에프론, 브래들리. 2010. 대규모 정당: 심판, 테스트 및 예측을 위한 경험적 뷰즈 방법. 케임브리지 대학원입니다.\n피셔, 로널드 아일머. 1935. 설계설계. 올리버 앤 보이드.\nGrolemund, Garrett 및 Hadley Wickham. 2017. 데이터사이언스를 R. 오라일리.\n헤이스티, 트레버, 리더 팁시라니, 제롬 프리드먼. 2008. 통계학습의 요소. 2^{} ed. 교통하는 것.\n홈즈 - 준카, 수잔. 1985. “데이터 분석 결과의 관련성을 평가하기 위한 컴퓨터 도구.” 프랑스 몽펠리에 2대학 박사학위논문.\nHuber, Wolfgang, Vincent J Carey, Robert Gentleman, Simon Anders, Marc Carlson, Benilton S Carvalho, Hector Corrada Bravo 등 2015. “바이오컨덕터를 사용하여 처리량이 충분하므로 조정 분석.” Nature Methods 12 (2): 115–21.\n이하카, 포트, 포트먼. 1996. “R: 데이터 분석 및 그래픽을 언어.” 계산 및 그래픽 통계 저널 5 (3): 299–314.\n네이먼, 팔, 에곤 S 피어슨. 1936. 충분한 통계와 통계적 능력에 대한 가장 강력한 검정. 캘리포니아대학교.\nTukey, John W. 1977. “탐색적 데이터 분석.” 매사추세츠: 애디슨-웨슬리.\nR 버전 4.5.1(2025-06-13)을 사용하여 2025-09-01 01:33에 페이지에 작성되었습니다",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>주제: 이질성</span>"
    ]
  },
  {
    "objectID": "01-chap.html",
    "href": "01-chap.html",
    "title": "3  1.1 이 장의 목표",
    "section": "",
    "text": "3.1 1.2 실제 예\n분자생물학에서는 여러 상황에서 카운팅 이벤트가 관련됩니다. 특정 철자를 사용하는 코돈 수, 참조와 일치하는 DNA 판독 수, DNA 서열에서 관찰되는 CG 다이그램 수 등이 있습니다. 이러한 개수는 연속적인 규모로 측정되는 질량 및 강도와 같은 양과 달리 이산 변수를 제공합니다.\n연구 중인 메커니즘이 따르는 규칙을 안다면 결과가 무작위라 하더라도 계산 및 표준 확률 법칙을 통해 관심 있는 모든 사건의 확률을 생성할 수 있습니다. 이는 추론과 확률 조작 방법에 대한 지식을 기반으로 한 하향식 접근 방식입니다. 2장에서는 이를 데이터 기반(bottom-up) 통계 모델링과 결합하는 방법을 살펴보겠습니다.\n이 장에서는 다음을 수행합니다.\n데이터 생성 프로세스에 대한 확률 모델이 있는 예를 살펴보겠습니다. 우리 모델에 따르면 HIV(인간 면역결핍 바이러스) 게놈의 돌연변이는 복제 주기당 뉴클레오티드당 \\(5 ^{-4}\\)의 비율로 발생합니다. 각 뉴클레오티드 위치의 비율은 동일하며 한 위치의 돌연변이는 다른 위치에서 발생하는 것과 독립적으로 발생합니다1. HIV의 게놈 크기는 약 \\(10^4=10,000\\) 뉴클레오티드이므로 한 주기 후에 총 돌연변이 수는 \\(5 ^{-4} ^4 = 5\\) 비율의 포아송 분포2를 따릅니다. 그것은 우리에게 무엇을 말해주는가?\n1 실제로, 그리고 엄밀히 말하면, 충분히 자세히 살펴보면 완전하고 완전한 독립은 현실적으로 거의 유지되지 않습니다. 따라서 모델러가 이러한 주장을 통해 일반적으로 의미하는 바는 가능한 상관 관계나 종속성이 너무 약하고 드물기 때문에 이를 무시해도 충분한 근사치라는 것입니다.\n2 이러한 유형의 확률 분포에 대해서는 나중에 더 자세히 설명하겠습니다.\n이 확률 모델은 한 번의 복제 주기 동안 돌연변이 수가 5에 가까울 것이며 이 추정치의 변동성은 \\(\\)(표준 오류)라고 예측합니다. 이제 우리는 일반적인 HIV 계통에서 볼 것으로 예상되는 돌연변이 수와 그 다양성에 대한 기준선 참조 값을 갖게 되었습니다.\n실제로 우리는 훨씬 더 자세한 정보를 추론할 수 있습니다. 푸아송(5) 모델에서 3개의 돌연변이가 얼마나 자주 발생할 수 있는지 알고 싶다면 R 함수를 사용하여 \\(x=3\\) 이벤트를 볼 확률을 생성할 수 있으며, 람다(\\(\\))라고 하는 푸아송 분포의 속도 매개변수 값을 \\(5\\)로 취합니다.\n\\(\\) 및 \\(\\)와 같은 그리스 문자는 종종 우리가 사용하는 확률 분포를 특징짓는 중요한 매개변수를 나타냅니다.\ndpois(x = 3, 람다 = 5)__\n[1] 0.1403739\n이는 정확히 3개의 사건을 볼 확률이 약 0.14, 즉 7분의 1이라는 것을 의미합니다.\n0부터 12까지의 모든 값의 확률을 생성하려면 루프를 작성할 필요가 없습니다. R의 시퀀스 연산자인 콜론 “:”을 사용하여 첫 번째 인수를 이 13개 값의 벡터로 설정할 수 있습니다. 그래프를 그려서 확률을 확인할 수 있습니다(그림 1.1). 이 그림과 마찬가지로 이 책의 여백에 있는 대부분의 그림은 본문에 표시된 코드에 의해 생성됩니다.\nR의 출력 형식이 어떻게 지정되는지 참고하세요. 첫 번째 줄은 벡터의 첫 번째 항목으로 시작하므로 [1]이 되고, 두 번째 줄은 9번째 항목으로 시작하므로 [9]가 됩니다. 이는 긴 벡터의 요소를 추적하는 데 도움이 됩니다. _벡터_라는 용어는 동일한 유형(이 경우 숫자)의 순서가 지정된 요소 목록에 대한 R 용어입니다.\n0:12 __\n[1] 0 1 2 3 4 5 6 7 8 9 10 11 12\ndpois(x = 0:12, 람다 = 5)__\n[1] 0.0067 0.0337 0.0842 0.1404 0.1755 0.1755 0.1462 0.1044 0.0653 0.0363 [11] 0.0181 0.0082 0.0034\nbarplot(dpois(0:12, 5), names.arg = 0:12, col = “red”)__\n그림 1.1: 포아송(5) 분포에 의해 모델링된 대로 0,1,2,…,12개의 돌연변이가 나타날 확률. 플롯은 종종 4개 또는 5개의 돌연변이를 볼 수 있지만 12개까지는 거의 볼 수 없음을 보여줍니다. 분포는 더 높은 숫자(\\(13,…\\))로 계속되지만 확률은 연속적으로 작아지므로 여기서는 이를 시각화하지 않습니다.\n수학 이론에 따르면 \\(x\\) 값을 볼 수 있는 포아송 확률은 \\(e^{-} ^x / x!\\) 공식으로 제공됩니다. 이 책에서 우리는 때때로 이론을 논의할 것이지만 그림 1.1과 같은 구체적인 숫자 예와 시각화를 표시하는 것을 선호합니다.\n포아송 분포는 돌연변이와 같은 희귀 사건에 대한 좋은 모델입니다. 이산 사건에 대한 다른 유용한 확률 모델은 Bernoulli, 이항 및 다항 분포입니다. 이 장에서는 이러한 모델을 살펴보겠습니다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "01-chap.html#실제-예",
    "href": "01-chap.html#실제-예",
    "title": "3  1.1 이 장의 목표",
    "section": "",
    "text": "R의 출력 형식이 어떻게 지정되는지 참고하세요. 첫 번째 줄은 벡터의 첫 번째 항목으로 시작하므로 [1]이고 두 번째 줄은 9번째 항목으로 시작하므로 [9]입니다. 이는 긴 벡터의 요소를 추적하는 데 도움이 됩니다. 벡터라는 용어는 동일한 유형 (이 경우 숫자)의 순서가 지정된 요소 목록에 대한 R 용어입니다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "01-chap.html#이산-확률-모델-사용",
    "href": "01-chap.html#이산-확률-모델-사용",
    "title": "3  1.1 이 장의 목표",
    "section": "3.2 1.3 이산 확률 모델 사용",
    "text": "3.2 1.3 이산 확률 모델 사용\n\n\n\n범주형 변수가 다른 대체 값을 갖는 것으로 생각하십시오. 이는 유전자 좌위의 다양한 대안과 유사한 수준입니다: 대립유전자.\n\n\n범주형 변수가 다른 대체 값을 갖는 것으로 생각하십시오. 이는 유전자 위치의 다양한 대안인 _대립체_와 유사한 수준입니다.\n점 돌연변이는 발생할 수도 있고 발생하지 않을 수도 있습니다. 바이너리 이벤트입니다. 두 가지 가능한 결과(예, 아니오)를 범주형 변수의 수준이라고 합니다.\n모든 이벤트가 바이너리인 것은 아닙니다. 예를 들어, 이배체 유기체의 유전자형은 세 가지 수준(AA, Aa, aa)을 가질 수 있습니다.\n범주형 변수의 수준 수가 매우 큰 경우도 있습니다. 그 예로는 생물학적 샘플에 있는 다양한 유형의 박테리아 수(수백 또는 수천 개)와 3개의 뉴클레오티드로 구성된 코돈 수(64개 수준)가 포함됩니다.\n표본에서 범주형 변수를 측정할 때 카운트 벡터에서 다양한 수준의 빈도를 집계하려는 경우가 많습니다. R에는 범주형 변수에 대한 특수 인코딩이 있으며 이를 인자 3이라고 부릅니다. 여기서 우리는 표로 작성하는 벡터에 19명의 피험자에 대한 다양한 혈액 유전자형을 캡처합니다.\n3 R은 요인 변수가 다른 “불법” 값을 허용하지 않도록 하며 이는 계산을 안전하게 유지하는 데 유용합니다.\n\n\n\nc()는 가장 기본적인 기능 중 하나입니다. 동일한 유형의 요소를 벡터로 조합합니다. 여기에 표시된 코드에서 유전자형 요소는 문자열입니다.\n\n\nc()는 가장 기본적인 함수 중 하나입니다. 동일한 유형의 요소를 벡터로 조합합니다. 여기에 표시된 코드에서 ’유전자형’의 요소는 문자열입니다.\ngenotype = c(“AA”,“AO”,“BB”,“AO”,“OO”,“AO”,“AA”,“BO”,“BO”, “AO”,“BB”,“AO”,“BO”,“AB”,“OO”,“AB”,“BB”,“AO”,“AO”) table(genotype)__\n유전자형 AA AB AO BB BO OO 2 2 7 3 3 2\nfactor 생성 시 R은 자동으로 수준을 감지합니다. levels 기능을 사용하여 레벨에 액세스할 수 있습니다.\n\n\n\n입력이 요인이라는 것이 테이블 함수의 출력에서 ​​명확하지 않습니다. 그러나 인스턴스가 없는 다른 레벨이 있었다면 테이블에는 개수가 0인 해당 레벨도 포함되었을 것입니다.\n\n\n‘table’ 함수의 출력에서는 입력이 요인이라는 것이 분명하지 않습니다. 그러나 인스턴스가 없는 다른 수준이 있었다면 테이블에는 개수가 0인 해당 수준도 포함되었을 것입니다.\ngenotypeF = factor(genotype) levels(genotypeF)__\n[1] “AA” “AB” “AO” “BB” “BO” “OO”\n테이블(유전자형F)__\n유전자형F AA AB AO BB BO OO 2 2 7 3 3 2\n__\n질문 1.1\n데이터에 아직 없는 일부 수준이 있는 _요인_을 생성하려면 어떻게 해야 합니까?\n__\n해결책\n__\nfactor 함수의 매뉴얼 페이지를 보십시오.\n데이터가 관찰되는 순서가 중요하지 않은 경우 무작위 변수를 교환 가능이라고 부릅니다. 이 경우 요인에서 사용할 수 있는 모든 정보는 요인 수준의 개수로 요약됩니다. 그런 다음 빈도 벡터가 데이터의 모든 관련 정보를 캡처하는 데 충분하여 데이터를 압축하는 효과적인 방법을 제공한다고 말합니다.\n\n3.2.1 1.3.1 베르누이 시행\n\n그림 1.2: 확률이 다른 두 가지 가능한 사건. 우리는 이것을 확률 매개변수 \\(p=2/3\\)를 사용하여 Bernoulli 분포로 모델링합니다.\n동전을 던지면 두 가지 결과가 나올 수 있습니다. 베르누이 시행(Bernoulli trial)이라고 불리는 이 간단한 실험은 소위 베르누이 확률 변수(Bernoulli Random Variable)를 사용하여 모델링되었습니다. 이 빌딩 블록을 이해하면 놀라울 정도로 멀리 갈 수 있습니다. 이를 사용하여 보다 복잡한 모델을 구축할 수 있습니다.\n이러한 무작위 변수 중 일부가 어떻게 보이는지 확인하기 위해 몇 가지 실험을 시도해 보겠습니다. 우리는 각 분포 유형에 대한 결과를 생성하도록 맞춤화된 특수 R 기능을 사용합니다. 그것들은 모두 문자 ’r’로 시작하고 그 뒤에 모델 사양이 옵니다. 여기에서는 ’rbinom’입니다. 여기서 ’binom’은 이항식에 사용되는 약어입니다.\n15번의 공정한 동전 던지기 시퀀스를 시뮬레이션한다고 가정해 보겠습니다. 성공 확률이 0.5(공정한 동전)인 15번의 베르누이 시행 결과를 얻으려면 다음과 같이 작성합니다.\nrbinom(15, prob = 0.5, 크기 = 1)__\n[1] 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0\n특정 매개변수 세트와 함께 rbinom 함수를 사용합니다. 4: 첫 번째 매개변수는 관찰하려는 시행 횟수입니다. 여기서는 15를 선택했습니다. 성공 확률을 ’prob’로 지정합니다. ’size=1’을 통해 각 개별 시행은 단 한 번의 동전 던지기로만 구성됨을 선언합니다.\n4 R 함수의 경우 매개변수를 인수라고도 합니다.\n__\n질문 1.2\n이 함수 호출을 여러 번 반복하십시오. 왜 대답은 항상 같지 않습니까?\n성공과 실패는 확률의 합이 1인 한 베르누이 시행에서 불평등한 확률을 가질 수 있습니다5. 오른쪽 상자 \\(\\)와 왼쪽 상자 \\(\\)에 떨어질 확률을 사용하여 그림 1.2에 표시된 두 상자에 공을 던지는 12번의 시도를 시뮬레이션하기 위해 다음과 같이 작성합니다.\n5 우리는 이러한 이벤트를 보완이라고 부릅니다.\nrbinom(12, 확률 = 2/3, 크기 = 1)__\n[1] 1 1 1 0 0 0 1 0 1 0 1 0\n1은 성공을 나타내며 공이 오른쪽 상자에 떨어졌음을 의미하고, 0은 공이 왼쪽 상자에 떨어졌음을 의미합니다.\n\n\n3.2.2 1.3.2 이항 성공 횟수\n오른쪽 상자에 공이 몇 개 들어가는지에만 관심이 있다면 던진 순서는 중요하지 않습니다.6 출력 벡터에서 셀의 합을 취하면 이 숫자를 얻을 수 있습니다. 따라서 위에서 본 이진 벡터 대신 단일 숫자만 보고하면 됩니다. R에서는 size 매개변수를 12로 설정하고 rbinom 함수를 한 번 호출하여 이를 수행할 수 있습니다.\n결과가 2개이고 크기가 1 이상이면 이항 시험이 됩니다. 크기가 1이면 베르누이 시행의 특별한 경우입니다.\n6 교환성 속성.\nrbinom(1, 확률 = 2/3, 크기 = 12)__\n[1] 9\n이 출력은 12개의 공 중 몇 개가 오른쪽 상자에 떨어졌는지 알려줍니다(확률이 2/3인 결과). 앞면 또는 뒷면, 성공 또는 실패, CpG 또는 비CpG, M 또는 F, Y = 피리미딘 또는 R = 퓨린, 질병이 있거나 건강함, 참 또는 거짓과 같은 두 가지 가능한 결과만 있는 경우 무작위 2박스 모델을 사용합니다. “실패”(보완 이벤트)가 확률 \\(1-p\\)로 발생하기 때문에 “성공” 확률 \\(p\\)만 필요합니다. 그러한 여러 번의 시도의 결과를 볼 때 교환이 가능한 경우7 성공 횟수만 기록합니다. 따라서 SSSSSFSSSSFFFSF는 (#Successes=10, #Failures=5) 또는 \\(x=10\\), \\(n=15\\)로 요약됩니다.\n7 시행이 교환 가능한 한 가지 상황은 서로 독립적인 경우입니다.\n성공 확률이 0.3인 베르누이 시행 15회 성공 횟수를 이항 확률 변수 또는 \\(B(15,0.3)\\) 분포를 따르는 확률 변수라고 합니다. 샘플을 생성하기 위해 시행 횟수를 15로 설정한 rbinom 함수 호출을 사용합니다.\n\n\n\nset.seed는 여기서 무엇을 합니까?\n\n\n여기서 set.seed는 무엇을 합니까?\nset.seed(235569515) rbinom(1, prob = 0.3, 크기 = 15)__\n[1] 5\n__\n질문 1.3\n이 함수 호출을 10번 반복합니다. 가장 일반적인 결과는 무엇입니까?\n__\n해결책\n__\n가장 빈번한 값은 4입니다. 실제로 4가 나타날 것으로 예상되는 이론적 비율은 \\(X\\)가 \\(B(15, 0.3)\\) 다음에 오면 \\(X=4\\)가 나올 확률의 값입니다.\n전체 확률 질량 분포는 다음을 입력하여 확인할 수 있습니다.\n소수점 이하 자릿수를 2로 유지하기 위해 round 함수를 사용합니다.\nprobabilities = dbinom(0:15, prob = 0.3, size = 15) round(probabilities, 2)__\n[1] 0.00 0.03 0.09 0.17 0.22 0.21 0.15 0.08 0.03 0.01 0.00 0.00 0.00 0.00 0.00 [16] 0.00\n그림 1.3과 같이 이 분포의 막대 그래프를 생성할 수 있습니다.\nbarplot(확률, names.arg = 0:15, col = “red”)__\n\n그림 1.3: \\(B(15,0.3)\\) 의 이론적 분포. 가장 높은 막대는 \\(x=4\\)에 있습니다. 우리는 이론적인 값을 전체적으로 빨간색으로 표시하기로 선택했습니다.\n시행 횟수는 R에 size 매개변수로 입력한 숫자이며 종종 \\(n\\)로 표시되는 반면, 성공 확률은 \\(p\\)입니다. 수학 이론에 따르면 \\(X\\) 매개변수 \\((n,p)\\)가 \\(X B(n,p)\\)로 작성된 이항 분포로 분포된 경우 \\(X=k\\)개의 성공을 볼 확률은 다음과 같습니다.\n\n\n\n\\frac{n!}{(n-k)!k!} 대신 특수 표기법 {n \\choose k}를 단축키로 사용할 수 있습니다.\n\n\n\\(\\) 대신 특수 표기법 \\({n k}\\)을 단축키로 사용할 수 있습니다.\n\\[ \\begin{aligned} P(X=k) &=&; p^k, (1-p){n-k}\\ &=&;pk, (1-p)^{n-k}\\ &=&{ n 택}; p^k, (1-p)^{n-k}. \\end{정렬} \\]\n__\n질문 1.4\n\\(k=3\\), \\(p=2/3\\), \\(n=4\\)에 대한 수식의 출력은 무엇입니까?\n\n\n3.2.3 1.3.3 포아송 분포\n\n그림 1.4: Poisson 분포의 이름을 딴 Simeon Poisson(이것이 R 코드를 제외하고 항상 대문자를 사용하는 이유입니다).\n성공 확률 \\(p\\)이 작고 시행 횟수 \\(n\\)가 큰 경우 이항 분포 \\(B(n, p)\\)는 더 간단한 분포인 비율 매개변수 \\(=np\\)를 사용하는 푸아송 분포에 의해 충실하게 근사될 수 있습니다. 우리는 이미 HIV 예(그림 1.1)에서 이 사실과 분포를 사용했습니다.\n__\n질문 1.5\n확률이 뉴클레오티드당 \\(p = 5 ^{-4}\\)일 때 \\(n = 10^4\\) 뉴클레오티드 게놈에서 0:12 돌연변이를 관찰할 확률 질량 분포는 무엇입니까? 이항 \\(B(n,p)\\) 분포와 Poisson\\((=np)\\) 분포로 모델링할 때 유사합니까?\n__\n해결책\n__\n이항 분포와 달리 포아송은 더 이상 두 개의 개별 매개변수 \\(n\\) 및 \\(p\\)에 의존하지 않고 해당 곱 \\(np\\)에만 의존합니다. 이항 분포의 경우와 마찬가지로 포아송 확률을 계산하는 수학 공식도 있습니다.\n\\[ P(X=k)= . \\]\n예를 들어 \\(\\)를 취하고 \\(P(X=3)\\)을 계산해 보겠습니다.\n5^3 * exp(-5) / 계승(3)__\n[1] 0.1403739\n이는 dpois를 사용하여 위에서 계산한 것과 비교할 수 있습니다.\n__\n일\n\\(5^{-4}\\)의 돌연변이율로 10,000개 위치에 대한 돌연변이 프로세스를 시뮬레이션하고 돌연변이 수를 계산합니다. 이를 여러 번 반복하고 ‘barplot’ 기능을 사용하여 분포를 그립니다(그림 1.5 참조).\nrbinom(1, prob = 5e-4, 크기 = 10000)__\n[1] 6\nsimulations = rbinom(n = 300000, prob = 5e-4, size = 10000) barplot(table(simulations), col = “lavender”)__\n\n그림 1.5: 300000번의 시뮬레이션에 대한 B(10000, \\(10^{-4}\\))의 시뮬레이션된 분포.\n이제 사례 연구에서 확률 계산을 사용할 준비가 되었습니다.\n\n\n3.2.4 1.3.4 에피토프 검출을 위한 생성 모델\n특정 제약 화합물을 테스트할 때 알레르기 반응을 유발하는 단백질을 검출하는 것이 중요합니다. 이러한 반응을 담당하는 분자 부위를 에피토프라고 합니다. 에피토프의 기술적 정의는 다음과 같습니다.\n\n항체가 결합하는 거대분자 항원의 특정 부분입니다. T 세포가 인식하는 단백질 항원의 경우 에피토프 또는 결정자는 Major에 결합하는 펩타이드 부분 또는 부위입니다. T 세포의 인식을 위한 조직적합성 복합체(MHC) 분자 수용체 (TCR).\n\n면역학에 대해 잘 모르시는 경우를 대비해 말씀드리자면, 항체(그림 1.6에 개략적으로 설명되어 있음)는 항원이라고 불리는 체내 이물질에 대한 반응으로 특정 백혈구에서 생성되는 단백질 유형입니다.\n\n그림 1.6: 여러 면역글로불린 도메인을 컬러로 보여주는 항체 다이어그램.\n항체는 항원에 (다소 특이적으로) 결합합니다. 결합의 목적은 항원을 파괴하는 것을 돕는 것입니다. 항체는 항원의 성질에 따라 여러 가지 방식으로 작용할 수 있습니다. 일부 항체는 항원을 직접 파괴합니다. 다른 사람들은 항원을 파괴하기 위해 백혈구를 모집하는 데 도움을 줍니다. 항원 결정기라고도 알려진 에피토프는 면역 체계, 특히 항체, B 세포 또는 T 세포에 의해 인식되는 항원의 일부입니다.\n\n3.2.4.1 알려진 매개변수를 사용한 ELISA 오류 모델\nELISA8 분석은 단백질을 따라 다른 위치에서 특정 에피토프를 검출하는 데 사용됩니다. 우리가 사용하는 ELISA 어레이에 대해 다음 사실이 성립한다고 가정합니다.\n8 E nzyme-L 잉크 I mmunoS orbent A ssay(Wikipedia 링크 ELISA).\n\n위치당 기본 소음 수준, 더 정확하게는 거짓양성률은 1%입니다. 이는 에피토프가 없을 때 히트를 선언할 확률입니다. 우리는 에피토프가 있다고 생각합니다. 우리는 이것을 \\(P(|)\\)9라고 씁니다.\n단백질은 독립적인 것으로 추정되는 100개의 다른 위치에서 테스트되었습니다.\n우리는 50명의 환자 샘플을 수집하여 검사할 예정입니다.\n\n9 \\(X|Y\\)와 같은 표현의 수직 막대는 “\\(X\\)는 조건에 따라 \\(Y\\)가 발생함”을 의미합니다.\n\n\n3.2.4.2 한 환자의 데이터\n한 환자의 분석에 대한 데이터는 다음과 같습니다.\n[1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n여기서 1은 충격(따라서 알레르기 반응의 가능성)을 나타내고 0은 해당 위치에서 반응이 없음을 나타냅니다.\n__\n일\n\\(p=0.01\\)인 50개의 독립 베르누이 변수의 합이 –충분히 근사한 – Poisson(\\(0.5\\)) 확률 변수와 동일하다는 것을 시뮬레이션을 통해 검증합니다.\n\n\n3.2.4.3 50가지 분석 결과\n우리는 100개 위치 각각에서 집계된 50명의 환자 전체에 대한 데이터를 연구할 것입니다. 알레르기 반응이 없는 경우 위양성률은 한 명의 환자에 대해 각 개별 위치가 1/100의 확률을 갖는다는 것을 의미합니다. 따라서 50명의 환자를 집계한 후 특정 위치에서 관찰된 50개의 \\((0,1)\\) 변수의 합이 매개변수가 0.5인 포아송 분포를 가질 것으로 예상합니다. 일반적인 결과는 그림 1.7과 같습니다. 이제 그림 1.8과 같이 데이터 파일 e100.RData에서 R 객체 e100으로 로드된 실제 데이터가 있다고 가정합니다.\n\n그림 1.7: 배경에 대한 생성 모델의 일반적인 데이터 플롯(예: 거짓 양성 히트): 단백질을 따라 100개 위치, 각 위치에서 카운트는 포아송(0.5) 무작위 변수에서 추출됩니다.\nload(“../data/e100.RData”) barplot(e100, ylim = c(0, 7), width = 0.7, xlim = c(-0.5, 100.5), names.arg = seq(along = e100), col = “darkolivegreen”)__\n\n그림 1.8: 100개 자세에서 50명의 환자에 대한 ELISA 어레이 결과 출력.\n그림 1.8의 스파이크는 놀랍습니다. 에피토프가 없다면 7만큼 큰 값이 나올 확률은 얼마나 되나요? 하나의 포아송(\\(0.5\\)) 확률변수를 고려할 때 7(또는 그 이상)만큼 큰 숫자가 나올 확률을 구하면 답은 다음과 같이 닫힌 형식으로 계산될 수 있습니다.\n\\[ P(X)= _{k=7}^P(X=k). \\]\n물론 이는 \\(1-P(X)\\)과 동일합니다. 확률 \\(P(X)\\)은 6에서의 소위 누적 분포 함수이고, R에는 이를 계산하기 위한 ‘ppois’ 함수가 있는데, 이를 다음 두 가지 방법 중 하나로 사용할 수 있습니다.\n10 하나에서 뺄셈을 할 필요가 없다는 편리함 외에도 이러한 계산 중 두 번째 계산은 확률이 작을 때 더 정확한 경향이 있습니다. 이는 부동 소수점 연산의 한계와 관련이 있습니다.\n1 - ppois(6, 0.5)__\n[1] 1.00238e-06\nppois(6, 0.5, lower.tail = FALSE)__\n[1] 1.00238e-06\n__\n일\nlower.tail 인수의 의미는 ppois 매뉴얼 페이지를 확인하세요.\n이 숫자는 그리스 문자 엡실론11인 \\(\\)으로 표시됩니다. 우리는 에피토프 반응이 없다고 가정할 때 \\(7\\)만큼 큰 수를 볼 확률은 다음과 같다는 것을 보여주었습니다.\n11 수학자들은 종종 작은 숫자(및 하위 숫자)를 \\(\\)이라고 부릅니다.\n\\[ =P(X)=1-P(X)^{-6}. \\]\n\n\n3.2.4.4 포아송 분포에 대한 극단값 분석\n멈추다! 위의 계산은 이 경우 올바른 계산이 아닙니다.\n__\n질문 1.6\n에피토프가 없을 때 이러한 데이터를 관찰할 확률을 계산하려는 경우 우리 추론에서 결함을 발견할 수 있습니까?\n__\n해결책\n__\n100개의 위치를 ​​모두 살펴보고 가장 큰 값을 찾아보니 7이었습니다. 이러한 선택으로 인해 하나의 위치만 볼 때보다 7이라는 큰 값이 발생할 가능성이 더 높습니다.\n따라서 포아송(0.5)이 7만큼 클 가능성이 무엇인지 묻는 대신, 최대 100번의 포아송(0.5) 시행이 7만큼 클 가능성은 얼마나 되는지 자문해야 합니다. 여기서는 극단가치 분석을 사용합니다12. 데이터 값 \\(x_1,x_2,… ,x_{100}\\)의 순서를 지정하고 이름을 \\(x_{(1)},x_{(2)},x_{(3)},… ,x_{(100)}\\)로 변경하여 \\(x_{(1)}\\)가 가장 작은 값을 나타내고 \\(x_{(100)}\\)가 가장 큰 값을 나타냅니다. 100개의 위치. \\(x_{(1)},… x_{(100)}\\)를 함께 이 100개 값 샘플의 순위 통계라고 합니다.\n12 이는 우리가 무작위 분포의 매우 크거나 작은 값(예: 최대값 또는 최소값)의 동작에 관심이 있음을 의미합니다.\n13 \\(\\)를 사용한 표기법은 합계에 대한 \\(\\)과 유사하게 일련의 용어의 곱을 작성하는 간결한 방법일 뿐입니다.\n7만큼 큰 최대값은 100개 카운트가 모두 6보다 작거나 같은 상보 사건입니다. 두 상보 사건의 합은 1이 될 확률을 갖습니다. 위치가 독립적이라고 가정하므로 이제 계산을 수행할 수 있습니다13.\n\\[ ]\n이러한 100개의 이벤트 각각이 독립적이라고 가정하기 때문에 위의 결과를 사용할 수 있습니다.\n\\[ _{i=1}^{100} P(x_i )= (P(x_i ))^{100}= (1-)^{100}. \\]\n\n\n3.2.4.5 실제로 숫자 계산하기\nR이 이 숫자의 값 \\((1-)^{100}\\)을 계산하도록 할 수 있습니다. 그러한 계산이 근사를 통해 어떻게 단축될 수 있는지에 관심이 있는 사람들을 위해 몇 가지 세부 사항을 제공합니다. 처음 읽을 때는 건너뛸 수 있습니다.\n위에서 \\(^{-6}\\)가 1보다 훨씬 작다는 점을 기억합니다. \\((1-)^{100}\\)의 값을 대략적으로 계산하려면 이항 정리를 사용하고 \\(\\)의 “고차” 항을 모두 삭제할 수 있습니다. 즉, \\(^2, ^3, …\\), 나머지(“선행”) 항에 비해 무시할 정도로 작기 때문입니다.\n\\[ (1-)^n = _{k=0}^n {n 선택 k} , 1^{n-k} , (-)^k = 1-n+{n} ^2 - {n 선택 3} ^3 + … -n - 10^{-4} \\]\n또 다른 동등한 경로는 \\((1-)-\\)과 동일한 근사값 \\(e^{-} -\\)을 사용하는 것입니다. 따라서\n\\[ (1-)^{100} = e{((1-){100})} = e^{ 100 (1-)} e^{-100 } e{-10{-4}} - 10^{-4}. \\]\n따라서 에피토프가 없는 경우 100개 위치에서 7개 이상의 히트 수를 볼 정확한 확률은 이전에 잘못 계산한 확률의 약 100배입니다.\n계산된 확률 \\(10^{-6}\\) 및 \\(10^{-4}\\)은 모두 표준 유의성 임계값(예: \\(0.05, 0.01\\) 또는 \\(0.001\\))보다 작습니다. 에피토프가 없다는 것을 거부하는 결정도 마찬가지였을 것입니다. 그러나 일부 법의학 법원 사건처럼 법정에 서서 p-값을 유효 숫자 8자리까지 방어해야 한다면 이는 또 다른 문제입니다. 검정의 다중성을 고려한 수정된 p-값이 보고되어야 하는 값이며, 이 중요한 문제는 6장에서 다시 다루겠습니다.\n14 이는 OJ 심슨 사건의 법의학 증거 조사에서 발생했습니다.\n\n\n3.2.4.6 시뮬레이션을 통한 확률 계산\n방금 살펴본 경우에는 이론적인 확률 계산이 매우 간단하고 명시적인 계산을 통해 결과를 파악할 수 있었습니다. 실제로는 상황이 더 복잡해지는 경향이 있으므로 관심 있는 사건의 확률을 찾는 생성 모델을 기반으로 하는 컴퓨터 시뮬레이션인 몬테 카를로 방법을 사용하여 확률을 계산하는 것이 더 좋습니다. 아래에서는 100개의 푸아송 분포 수에서 최대값을 선택하는 100,000개의 인스턴스를 생성합니다.\nmaxes = replicate(100000, { max(rpois(100, 0.5)) }) table(maxes)__\n최대 1 2 3 4 5 6 7 9 7 23028 60840 14364 1604 141 15 1\n100,000번의 시도 중 16번의 시도에서 최대값은 7 이상이었습니다. 이는 \\(P(X_{})\\)15에 대해 다음과 같은 근사치를 제공합니다.\n15 R에서 maxes &gt;= 7 표현식은 maxes와 길이는 같지만 값은 TRUE 및 FALSE인 논리 벡터로 평가됩니다. 여기에 mean 함수를 적용하면 해당 벡터는 0과 1로 변환되고, 계산 결과는 TRUE의 분수와 동일한 1의 분수가 됩니다.\n평균(최대값 &gt;= 7 )__\n[1] 0.00016\n이는 우리의 이론적 계산과 어느 정도 일치합니다. 우리는 이미 Monte Carlo 시뮬레이션의 잠재적인 한계 중 하나를 확인했습니다. 시뮬레이션 결과의 “세분성”은 시뮬레이션 수(100000)의 역수에 의해 결정되므로 약 10^{-5}가 됩니다. 추정된 확률은 이 세분성보다 더 정확할 수 없으며 실제로 추정의 정밀도는 그 몇 배입니다. 지금까지 수행한 모든 작업은 위치별 위양성률, 분석된 환자 수 및 단백질 길이를 알고 모델에서 동일하게 분포된 독립적 추첨을 갖고 있으며 알려지지 않은 매개변수가 없기 때문에 가능합니다. 이것은 확률 또는 생성 모델링의 예입니다. 모든 매개변수가 알려져 있으며 수학적 이론을 통해 하향식 방식으로 연역을 통해 작업할 수 있습니다.\n우리는 모든 매개변수를 알고 있고 수학적 추론을 통해 결론을 내릴 수 있는 척하면서 소음에 대한 포아송 분포를 가정했습니다.\n대신에 환자 수와 단백질의 길이를 알 수 있는 보다 현실적인 상황에 있지만 데이터의 분포를 모르는 경우 통계적 모델링을 사용해야 합니다. 이 접근 방식은 2장에서 개발됩니다. 시작할 데이터만 있는 경우 먼저 이를 설명하기 위한 합리적인 분포를 맞춰야 합니다. 하지만 이 더 어려운 문제에 도달하기 전에 이산 분포에 대한 지식을 이분법, 성공 또는 실패 결과 이상으로 확장해 보겠습니다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "01-chap.html#다항-분포-dna의-경우",
    "href": "01-chap.html#다항-분포-dna의-경우",
    "title": "3  1.1 이 장의 목표",
    "section": "3.3 1.4 다항 분포: DNA의 경우",
    "text": "3.3 1.4 다항 분포: DNA의 경우\n\n3.3.0.1 결과가 2개 이상입니다.\n예를 들어 그림 1.9의 상자와 같이 네 가지 가능한 결과를 모델링하거나 네 가지 뉴클레오티드 [A,C,G] 및 [T]의 수를 연구할 때 [이항] 모델을 확장해야 합니다.\n\n그림 1.9: 상자는 개별 범주형 변수의 네 가지 결과 또는 수준을 나타냅니다. 오른쪽 상자는 가능성이 더 높은 결과를 나타냅니다.\n이항식을 사용할 때 결과 1에 확률 \\(p=P(1)=p_1\\)을 결과 0에 확률 \\(1-p=p(0)=p_0\\)을 할당하여 두 결과에 대한 불평등한 확률을 고려할 수 있다는 점을 상기하십시오. [A,C,G] 및 [T]와 같이 두 개 이상의 가능한 결과가 있는 경우 서로 다른 확률에 해당하는 서로 다른 크기의 상자에 공을 던지는 것을 생각할 수 있으며 레이블을 지정할 수 있습니다. 이러한 확률은 \\(p_A,p_C,p_G,p_T\\)입니다. 이항의 경우와 마찬가지로 가능한 모든 결과의 확률의 합은 1입니다. \\(p_A+p_C+p_G+p_T=1\\).\n\n\n\n당신은 여기서 비밀리에 연속 배포판, 균등 배포판: runif를 만나고 있습니다.\n\n\n당신은 여기서 연속 분포, 균일 분포인 ’runif’를 비밀리에 만나고 있습니다.\n__\n일\nrunif라는 함수를 통해 \\(0\\)과 \\(1\\) 사이의 가능한 모든 숫자를 생성하는 난수 생성기를 실험해 보세요. 이를 사용하여 4개 수준(A, C, G, T)의 확률 변수를 생성합니다. 여기서 \\(p_{}=, p_{}=, p_{}=, p_{}=\\)입니다.\n수학적 공식화. 다항 분포는 개수 계산을 위한 가장 중요한 모델이며 R은 일반 공식을 사용하여 확률 \\(p_1,…,p_m\\)이 있는 \\(m\\) 상자에서 \\(n\\) 결과에 대한 개수의 다항 벡터 \\((x_1,…,x_m)\\)의 확률을 계산합니다.\n첫 번째 용어는 다음과 같습니다. 상자 1의 확률 \\(p_1\\), 상자 2의 확률 \\(p_2\\), … 상자 \\(m\\)의 확률 \\(p_m\\)을 가정할 때 상자 1의 개수 \\(x_1\\), 2의 \\(x_2\\) 및 상자 m의 … \\(x_m\\)을 관찰할 결합 확률입니다.\n\\[\\begin{align} P(x_1,x_2,…,x_m) &= _{i=1}^m p_i^{x_i}\\ &={{n}} ; p_1{x_1},p_2{x_2}p_m^{x_m}. \\end{정렬}\\]\n괄호 안의 항은 다항 계수라고 하며 \\[{nx_1,x_2,…,x_m}=.\\]의 약어입니다. 따라서 이것은 이항 계수의 일반화입니다. \\(m=2\\)의 경우 이항 계수와 동일합니다.\n__\n질문 1.7\n확률이 동일한 네 개의 상자가 있다고 가정해 보겠습니다. 공식을 사용하면 첫 번째 상자에서 4가 관찰되고, 두 번째 상자에서 2가 관찰되고, 다른 두 상자에서는 아무것도 관찰되지 않을 확률은 얼마입니까?\n__\n해결책\n__\n\\[ P(4,2,0,0)= =. \\]\ndmultinom(c(4, 2, 0, 0), prob = 담당자(1/4, 4))__\n[1] 0.003662109\n우리는 우리가 보는 데이터가 각 상자의 확률이 1/4인 가장 간단한 4박스 모델과 일치하는지 확인하기 위해 시뮬레이션 실험을 자주 실행합니다. 어떤 의미에서는 그것은 허수아비입니다(흥미로운 일은 일어나지 않습니다). 이에 대한 더 많은 예는 2장에서 살펴보겠습니다. 여기서는 몇 가지 R 명령을 사용하여 이러한 개수 벡터를 생성합니다. 먼저 서로 다른 동일한 유형의 8개 문자가 있다고 가정합니다.\npvec = rep(1/4, 4) t(rmultinom(1, prob = pvec, size = 8))__\n[,1] [,2] [,3] [,4] [1,] 1 3 1 3\n__\n질문 1.8\nt() 함수를 사용하지 않고 코드를 시도해 보세요. ’t’는 무엇을 뜻하나요?\n__\n질문 1.9\nrmultinom(n = 8, prob = pvec, size = 1)과 rmultinom(n = 1, prob = pvec, size = 8)의 차이점을 어떻게 해석합니까? 힌트: 섹션 1.3.1과 1.3.2에서 수행한 작업을 기억하세요.\n\n\n3.3.1 1.4.1 전력 시뮬레이션\n과학자들이 실험을 계획할 때 종종 해결해야 하는 문제인 ’얼마나 큰 표본 크기가 필요한가?’와 관련된 방식으로 다항식에 몬테카를로를 사용하는 예를 살펴보겠습니다.\n\n통계학자에게 표본 크기에 대해 물어보면 그들은 항상 더 많은 데이터가 필요하다고 말할 것입니다. 표본 크기가 클수록 결과가 더 민감해집니다. 그러나 실험실 작업은 비용이 많이 들기 때문에 고려해야 할 까다로운 비용-편익 균형이 있습니다. 이것은 매우 중요한 문제이기 때문에 책 마지막 부분(Chapter 13)에 전체 장을 할애했습니다.\n전력이라는 용어는 통계에서 특별한 의미를 갖습니다. 이는 무언가가 거기에 있다면 이를 탐지할 확률로, 참양성률이라고도 합니다.\n전통적으로 실험주의자들은 실험을 계획할 때 80%(또는 그 이상)의 검정력을 목표로 합니다. 이는 동일한 실험을 여러 번 실행하면 약 20%의 시간 동안 의미 있는 결과를 산출해야 함에도 불구하고 실패할 것이라는 의미입니다.\n우리가 수집한 DNA 데이터가 공정한 프로세스에서 나온다는 귀무 가설을 \\(H_0\\)이라고 부르겠습니다. 여기서 4개의 뉴클레오티드 각각은 동일할 가능성이 \\((p_A,p_C,p_G,p_T)=(0.25,0.25,0.25,0.25)\\)입니다. 여기서 Null은 흥미로운 일이 전혀 일어나지 않는 기준선을 의미합니다. 우리가 반증(또는 통계학자의 용어로 “거부”)하려는 것은 허수아비이므로 귀무 가설은 귀무 가설로부터의 편차가 흥미로워야 합니다16.\n16 생물학을 조금이라도 안다면 살아있는 유기체의 DNA가 귀무 가설을 거의 따르지 않는다는 사실을 알게 될 것입니다. 따라서 이를 반증하는 것은 그다지 흥미롭지 않을 수 있습니다. 여기서 우리는 이 귀무가설을 통해 계산을 설명할 수 있지만 좋은 귀무가설(기각이 흥미로운)을 선택하려면 과학적 입력이 필요하다는 점을 상기시켜 주는 역할도 합니다.\n동일한 크기의 상자로 표시되는 8개의 문자와 4개의 동일한 결과에 대해 R 명령을 실행하여 보았듯이 각 상자에 항상 2개가 표시되는 것은 아닙니다. 단지 8개의 문자만 보면 뉴클레오티드가 공정한 과정에서 나온 것인지 아닌지를 알 수 없습니다.\n길이가 \\(n=20\\)인 시퀀스를 살펴봄으로써 뉴클레오티드의 원래 분포가 공정한지 아니면 다른(“대체”) 프로세스에서 나오는지 여부를 감지할 수 있는지 판단해 보겠습니다.\n‘rmultinom’ 함수를 사용하여 귀무 가설로부터 1000개의 시뮬레이션을 생성합니다. 공간을 절약하기 위해 처음 11개 열만 표시합니다.\nobsunder0 = rmultinom(1000, prob = pvec, size = 20) dim(obsunder0)__\n[1] 4 1000\nobsunder0[, 1:11]__\n[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [1,] 6 5 6 8 4 6 2 7 5 4 4 [2,] 6 6 3 7 3 3 8 4 3 3 5 [3,] 3 3 6 2 8 3 5 7 4 7 6 [4,] 5 6 5 3 5 8 5 2 8 6 5\n\n\n\n모든 열의 상단에는 [,1][,2] 형식의 인덱스가 있습니다… 이것이 열 인덱스입니다. 행에는 [1,][2,]라는 레이블이 지정됩니다…. obsunder0 개체는 이전에 본 것과 같은 단순한 벡터가 아니라 행렬의 숫자 배열입니다.\n\n\n모든 열의 상단에는 ‘[,1][,2]…’ 형식의 인덱스가 있습니다. 이것이 열 인덱스입니다. 행에는 [1,][2,]...라는 레이블이 지정됩니다. ‘obsunder0’ 객체는 이전에 본 것처럼 단순한 벡터가 아니라 행렬의 숫자 배열입니다.\n매트릭스 ’obsunder0’의 각 열은 시뮬레이션된 인스턴스입니다. 상자 안의 숫자가 매우 다양하다는 것을 알 수 있습니다. 일부는 8만큼 큰 반면 예상 값은 5=20/4입니다.\n\n3.3.1.1 테스트 만들기\n기억하세요: 우리는 이러한 가치가 공정한 과정에서 나온다는 것을 알고 있습니다. 분명히 프로세스의 예상 값을 아는 것만으로는 충분하지 않습니다. 또한 얼마나 많은 변동성이 예상되는지, 어느 정도가 너무 큰지 설명할 수 있는 변동성 척도가 필요합니다. 우리는 측정값으로 다음 통계를 사용합니다. 이는 기대값에 대한 관찰값과 기대값 간의 차이의 제곱의 합으로 계산됩니다. 따라서 각 인스턴스에 대해\n이 측정값은 기대값을 기준으로 각 제곱 잔차에 가중치를 부여합니다.\n\\[ {}=+ + + =_i \\]\n생성된 데이터의 처음 세 열은 우리가 기대하는 것과 얼마나 다릅니까? 우리는 다음을 얻습니다:\nexpected0 = pvec * 20 sum((obsunder0[, 1] - expected0)^2 / expected0)__\n[1] 1.2\nsum((obsunder0[, 2] - 예상0)^2 / 예상0)__\n[1] 1.2\nsum((obsunder0[, 3] - 예상0)^2 / 예상0)__\n[1] 1.2\n측정값의 값은 다를 수 있습니다. 3개 이상의 열을 볼 수 있으며, 1,000개 열을 모두 연구하는 방법을 살펴보겠습니다. 반복적인 입력을 피하기 위해 ‘stat’ 공식(수식 1.1)을 함수에 캡슐화합니다.\nstat = function(obsvd, exptd = 20 * pvec) { sum((obsvd - exptd)^2 / exptd) } stat(obsunder0[, 1])__\n[1] 1.2\n이 변형에 대한 보다 완전한 그림을 얻기 위해 모든 1000개 인스턴스에 대한 측정값을 계산하고 이 값을 ‘S0’이라고 하는 벡터에 저장합니다. 여기에는 \\(H_0\\) 아래에 생성된 값이 포함됩니다. 그림 1.10에 표시된 ’S0’ 값의 히스토그램을 영 분포의 추정치로 간주할 수 있습니다.\nS0 = apply(obsunder0, 2, stat) summary(S0)__\n최소 1쿼. 중앙값 평균 3번째 Qu. 최대. 0.000 1.200 2.800 3.126 4.400 17.600\nhist(S0, break = 25, col = “라벤더”, main = ““)__\n\n그림 1.10: 널(공정) 분포에서 통계 ’stat’의 시뮬레이션된 값 ’S0’의 히스토그램은 통계 ’stat’의 샘플링 분포에 대한 근사치를 제공합니다.\n\n\n\n적용 함수는 배열의 행이나 열에 대한 루프를 줄여서 표현한 것입니다. 여기서 두 번째 인수 2는 열에 대한 루프를 나타냅니다.\n\n\n‘apply’ 함수는 배열의 행이나 열에 대한 루프를 줄여서 표현한 것입니다. 여기서 두 번째 인수인 2는 열에 대한 반복을 나타냅니다.\n요약 함수는 ’S0’이 다양한 값을 취한다는 것을 보여줍니다. 예를 들어, 시뮬레이션된 데이터에서 95% 분위수(5% 가장 큰 값에서 작은 95% 값을 구분하는 값)를 근사화할 수 있습니다.\nq95 = quantile(S0, probs = 0.95) q95 __\n95% 7.6\n따라서 ‘S0’ 값의 5%가 7.6보다 크다는 것을 알 수 있습니다. 우리는 이것을 데이터 테스트를 위한 중요한 값으로 제안하고 ‘stat’ 가중 제곱합이 7.6보다 큰 경우 데이터가 동일한 가능성의 뉴클레오티드를 사용하는 공정한 프로세스에서 나온다는 가설을 기각할 것입니다.\n\n\n3.3.1.2 테스트의 검정력 결정\n가중 제곱합 차이를 기반으로 한 테스트에서 실제로 데이터가 귀무 가설에서 나온 것이 아니라는 사실을 탐지할 확률을 계산해야 합니다. 시뮬레이션을 통해 거부 확률을 계산합니다. ’pvecA’로 매개변수화된 대체 프로세스에서 1000개의 시뮬레이션 인스턴스를 생성합니다.\n\npvecA = c(3/8, 1/4, 1/4, 1/8) observed = rmultinom(1000, prob = pvecA, size = 20) dim(observed)__\n[1] 4 1000\n관찰됨[, 1:7]__\n[,1] [,2] [,3] [,4] [,5] [,6] [,7] [1,] 10 4 8 8 4 7 7 [2,] 3 10 5 6 6 7 2 [3,] 5 3 5 6 4 2 6 [4,] 2 3 2 0 6 4 5\n적용(관찰, 1, 평균)__\n[1] 7.469 4.974 5.085 2.472\nexpectedA = pvecA * 20 expectedA __\n[1] 7.5 5.0 5.0 2.5\n귀무가설의 시뮬레이션과 마찬가지로 관측값은 상당히 다양합니다. 문제는 데이터가 null에서 벗어났음을 테스트에서 얼마나 자주(1000개 인스턴스 중) 감지하는 것입니다.\n통계 값이 95번째 백분위수 내에 있기 때문에 테스트는 첫 번째 관측치(10, 3, 5, 2)를 거부하지 않습니다.\n통계(관찰됨[, 1])__\n[1] 7.6\nS1 = apply(observed, 2, stat) q95 __\n95% 7.6\n합계(S1 &gt; q95)__\n[1] 199\npower = mean(S1 &gt; q95) power __\n[1] 0.199\n\n\n\n우리는 주어진 또는 조건에 따라 수직선을 읽습니다.\n\n\n우리는 수직선을 주어진 또는 조건부로 읽습니다.\n1000개의 시뮬레이션을 실행한 결과 199개가 대체 분포에서 나온 것으로 확인되었습니다. 따라서 우리는 확률 \\(P(H_0 ;|; H_A)\\)이 0.199라는 것을 계산했습니다.\n\\(n = 20\\)의 시퀀스 길이를 사용하면 공정한 생성 프로세스와 대안 간의 차이를 감지할 수 있는 약 20%의 _능력_이 있습니다.\n__\n일\n실제로 앞서 언급했듯이 허용되는 전력 값은 \\(0.8\\) 이상입니다. 시뮬레이션 실험을 반복하고 전력이 허용 가능한지 확인하는 새로운 시퀀스 길이 \\(n\\)를 제안합니다.\n\n\n3.3.1.3 고전 데이터에 대한 고전 통계\n95번째 백분위수를 계산하기 위해 Monte Carlo를 사용하여 데이터를 시뮬레이션할 필요가 없었습니다. 계산에 도움이 되는 적절한 이론이 있습니다.\n우리의 통계 stat에는 실제로 카이제곱 분포(자유도 3)라는 잘 알려진 분포가 있으며 \\({}^2_3\\)로 표시됩니다.\n\n그림 1.11: 우리는 확률 모델이 \\(F\\)라고 부르는 분포를 갖는 방식을 연구했습니다. \\(F\\)는 관례에 따라 \\(\\)와 같이 그리스 문자로 표시되는 매개 변수에 따라 달라지는 경우가 많습니다. 관찰된 데이터는 갈색 화살표를 통해 생성되며 \\(x\\)와 같은 라틴 문자로 표시됩니다. 확률 계산의 세로 막대는 가정 또는 조건부를 나타냅니다.\nQ-Q 플롯을 사용하여 분포를 비교하는 방법은 Chapter 2에서 살펴보겠습니다(그림 2.8 참조). 직접 만든 시뮬레이션을 실행하는 대신 보다 표준적인 테스트를 사용할 수도 있었습니다. 그러나 우리가 배운 절차는 카이제곱 분포가 적용되지 않는 많은 상황으로 확장됩니다. 예를 들어, 일부 상자의 확률이 매우 낮고 개수가 대부분 0인 경우입니다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "01-chap.html#이-장의-요약",
    "href": "01-chap.html#이-장의-요약",
    "title": "3  1.1 이 장의 목표",
    "section": "3.4 1.5 이 장의 요약",
    "text": "3.4 1.5 이 장의 요약\n우리는 몇 가지 기본 분포를 사용하여 다양한 이산 _이벤트_의 확률을 계산하기 위해 수학 공식과 R을 사용했습니다.\nBernoulli 분포는 우리의 가장 기본적인 빌딩 블록이었습니다. 이는 동전 던지기와 같은 단일 이진 시행을 나타내는 데 사용됩니다. 결과를 0과 1로 코딩할 수 있습니다. \\(p\\)를 성공 확률(1번째 결과)이라고 부릅니다.\n**_이항 분포**는 \\(n\\) 이진 시행에서 1의 개수에 사용되며, R 함수 dbinom을 사용하여 \\(k\\) 성공 확률을 계산할 수 있습니다. 또한 rbinom 함수를 사용하여 \\(n\\) 시행으로 이항 실험을 시뮬레이션하는 방법도 살펴보았습니다.\n푸아송 분포는 \\(p\\)가 작은 경우(1은 거의 발생하지 않음)에 가장 적합합니다. 여기에는 하나의 매개변수 \\(\\)만 있고 \\(=np\\)에 대한 포아송 분포는 \\(p\\)가 작은 경우 \\((n,p)\\)에 대한 이항 분포와 거의 동일합니다. 우리는 위치당 위양성 비율(\\(p\\))이 작다고 가정하고 시퀀스에 따라 에피토프를 테스트한 분석에서 무작위로 발생하는 위양성 수를 모델링하기 위해 포아송 분포를 사용했습니다. 우리는 모든 매개변수를 알고 있는 한 그러한 매개변수 모델을 통해 어떻게 극단적인 사건의 확률을 계산할 수 있는지 살펴보았습니다.\n다항식 분포는 3개 이상의 가능한 결과 또는 수준이 있는 개별 이벤트에 사용됩니다. 파워 예제에서는 동일한 확률을 갖는 다항 모델이 데이터와 일치하는지 테스트하려는 경우 몬테 카를로 시뮬레이션을 사용하여 수집해야 하는 데이터의 양을 결정하는 방법을 보여주었습니다. 우리는 생성 모델에 대한 가정을 통해 데이터가 어떻게 생성되었는지에 대한 가설을 평가하기 위해 확률 분포와 확률 모델을 사용했습니다. 가설이 주어졌을 때 데이터를 볼 확률을 p-값이라고 부릅니다. 이것은 가설이 참일 확률과는 다릅니다!\n\n\n\nP(H_0\\;|\\;\\text{data})는 p-값 P(\\text{data}\\;|\\;H_0)과 동일하지 않습니다.\n\n\n\\(P(H_0;|;)\\)는 p-값 \\(P(;|;H_0)\\)과 동일하지 않습니다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "01-chap.html#추가-자료",
    "href": "01-chap.html#추가-자료",
    "title": "3  1.1 이 장의 목표",
    "section": "3.5 1.6 추가 자료",
    "text": "3.5 1.6 추가 자료\n\nFreedman, Pisani 및 Purves(1997)의 초등 서적은 여기서 언급하는 상자 모델 유형을 통해 확률에 대한 최고의 소개를 제공합니다.\nDurbin 등의 저서. (1998)은 많은 유용한 확률 분포를 다루고 있으며 부록에서 확률 이론의 이론적 배경과 생물학 시퀀스에 대한 적용에 대한 보다 완전한 보기를 제공합니다.\n몬테카를로 방법은 현대 통계에서 광범위하게 사용됩니다. Robert와 Casella(2009)는 R을 사용하여 이러한 방법을 소개합니다.\n6장에서는 가설 검정에 대한 내용을 다룹니다. 우리는 또한 데이터 분석에 자주 사용하는 고급 확률 분포, 베타, 감마, 지수 유형에 유용한 고급 자료에 대해 Rice(2006)를 제안합니다.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "01-chap.html#연습",
    "href": "01-chap.html#연습",
    "title": "3  1.1 이 장의 목표",
    "section": "3.6 1.7 연습",
    "text": "3.6 1.7 연습\n__\n연습 1.1\nR은 알려진 모든 분포에서 숫자를 생성할 수 있습니다. 이제 우리는 각 분포 유형에 맞춰진 특화된 R 함수를 사용하여 임의의 이산 데이터를 생성하는 방법을 알았습니다. 우리는 rXXXX에서와 같이 r로 시작하는 함수를 사용합니다. 여기서 XXXX는 pois, binom, multinom이 될 수 있습니다. 이러한 모델 중 하나에서 확률에 대한 이론적 계산이 필요한 경우 이산 이항 분포에서 사건의 확률을 계산하는 ‘dbinom’과 연속 정규 분포에 대한 확률 밀도 함수를 계산하는 ’dnorm’과 같은 ’dXXXX’ 함수를 사용합니다. \\(P(X&gt;a)\\)와 같은 꼬리 확률을 계산할 때 ’pXXXX’라고 불리는 누적 분포 함수를 사용하는 것이 편리합니다. 위의 XXXX를 대체할 수 있는 두 개의 다른 이산 분포를 찾으십시오.\n__\n연습 1.2\n이 장에서 우리는 확률이 셀 수 있는 값 세트에 집중되어 있는 이산적 무작위 변수에 집중했습니다. dbinom을 사용하여 이항식 \\(B(10, 0.3)\\)에 대해 \\(X=2\\) 값에서 _확률 질량_을 어떻게 계산합니까? dbinom을 사용하여 \\(P(X)\\)에 해당하는 값 2에서 누적 분포를 계산하고 다른 R 함수로 답을 확인하세요.\n__\n해결책\n__\ndbinom(2, 크기 = 10, 확률 = 0.3)__\n[1] 0.2334744\npbinom(2, 크기 = 10, 확률 = 0.3)__\n[1] 0.3827828\nsum(dbinom(0:2, 크기 = 10, 확률 = 0.3)) __\n[1] 0.3827828\n__\n연습 1.3\n특정 명령 시퀀스가 ​​계속 필요하다는 것을 알 때마다 해당 명령을 함수에 넣는 것이 좋습니다. 함수 본문에는 우리가 반복해서 수행하려는 명령이 포함되어 있으며, 함수 인수는 우리가 변경하려는 항목을 사용합니다. ‘람다’ 비율로 ’n’개의 포아송 변수를 살펴볼 때 최대값이 ’m’일 확률을 계산하는 함수를 작성하세요.\n__\n해결책\n__\npoismax = function(lambda, n, m) { epsilon = 1 - ppois(m - 1, lambda) 1 - exp( -n * epsilon) } poismax(lambda = 0.5, n = 100, m = 7)__\n[1] 0.0001002329\npoismax(람다 = 평균(e100), n = 100, m = 7)__\n[1] 0.0001870183\n__\n연습 1.4\n인수에 대한 기본값(즉, 함수 호출에서 인수가 지정되지 않은 경우 사용되는 값)을 갖도록 함수를 다시 작성합니다.\n__\n해결책\n__\npoismax = function(lambda, n = 100, m = 7) { 1 - exp( -n * (1 - ppois(m - 1, lambda))) } poismax(0.5)__\n[1] 0.0001002329\n포이즈맥스(0.5, m = 9)__\n[1] 3.43549e-07\n__\n연습 1.5\n에피토프 예에서는 시뮬레이션을 사용하여 100번의 시도에서 최대 9 이상일 확률을 찾습니다. “확률이 0.000001보다 작다”는 것을 증명하려면 몇 번의 시뮬레이션이 필요합니까?\n__\n연습 1.6\n사용 가능한 배포판 목록을 얻으려면 R에서 ?Distributions를 사용하세요17. 다양한 분포에 대한 확률 질량 또는 밀도 함수의 도표를 작성하고(’dXXXX’라는 함수 사용) 이산형이 아닌 5개의 분포를 나열하십시오.\n17 이는 기본 R 설치와 함께 제공되는 것들입니다. 추가 패키지에는 더 많은 내용이 있습니다. CRAN 작업 보기: 확률 분포를 참조하세요.\n__\n연습 1.7\nPoisson(3) 확률 변수의 인스턴스 100개를 생성합니다. 무슨 뜻인가요? R 함수 var로 계산된 분산은 무엇입니까?\n__\n연습 1.8\n기음. elegans 게놈 뉴클레오티드 빈도: C의 미토콘드리아 서열입니다. elegans 가능성이 동일한 뉴클레오티드 모델과 일치합니까?\n\nBioconductor의 Biostrings 패키지에 있는 전용 기능을 사용하여 염색체 M의 뉴클레오티드 빈도를 탐색해 보세요.\nC. elegans 데이터는 시뮬레이션을 사용하여 균일 모델(모든 뉴클레오티드 빈도가 동일)과 일치합니다. 힌트: 이번이 처음으로 Bioconductor를 사용해 볼 수 있는 기회입니다. Bioconductor의 패키지 관리는 CRAN보다 더 엄격하게 제어되므로 Bioconductor 패키지를 설치하려면 BiocManager 패키지의 특수 ‘설치’ 기능을 사용해야 합니다.\n\nif (!requireNamespace(“BiocManager”, 조용히 = TRUE)) install.packages(“BiocManager”) BiocManager::install(c(“Biostrings”, “BSgenome.Celegans.UCSC.ce2”))__\n그런 다음 다른 R 패키지를 로드할 때 게놈 시퀀스 패키지를 로드할 수 있습니다.\n__\n해결책\n__\nlibrary(“BSgenome.Celegans.UCSC.ce2”) 셀레간스 __\n웜용 BSgenome 개체 | - 유기체: Caenorhabditis elegans | - 제공자: UCSC | - 게놈: ce2 | - 출시일: 2004년 3월 | - 7개 시퀀스: | chrI chrII chrIII chrIV chrV chrX chrM | | 팁: 모든 시퀀스 이름을 얻으려면 객체에 대해 ‘seqnames()’를 호출하고 | 전체 시퀀스 정보를 얻으려면 ’seqinfo()’, ‘$’ 또는 ‘[[’ 연산자를 사용하여 | 주어진 서열에 접근하세요. 자세한 내용은 ’?BSgenome’을 참조하세요.\n시퀀스 이름(Celegans)__\n[1] “chrI” “chrII” “chrIII” “chrIV” “chrV” “chrX” “chrM”\n셀레간스$chrM __\n13794-문자 DNAString 개체 시퀀스: CAGTAAATAGTTTAATAAAAATATAGCATTTGGGTT…TATTTATAGATATATACTTTGTATATATCTATATTA\n클래스(셀레간스$chrM)__\n[1] “DNAString” attr(,“package”) [1] “바이오스트링”\n길이(셀레간스$chrM)__\n[1] 13794\nlibrary(“바이오스트링”) lfM = letterFrequency(Celegans$chrM, letter=c(“A”, “C”, “G”, “T”)) lfM __\nA C G T 4335 1225 2055 6179\n합(lfM)__\n[1] 13794\nlfM / 합계(lfM)__\nA C G T 0.31426707 0.08880673 0.14897782 0.44794838\n_C와 길이가 동일한 무작위(확률이 동일한 각 문자) 시퀀스를 만듭니다. 엘레강스_염색체 M:\nt(rmultinom(1, length(Celegans$chrM), p = 담당자(1/4, 4)))__\n[,1] [,2] [,3] [,4] [1,] 3409 3486 3476 3423\n예상 빈도는 다음과 같습니다.\n길이(Celegans$chrM) / 4 __\n[1] 3448.5\n두 개의 다항식 출력이 서로 얼마나 가까운지 측정하는 통계를 계산해 보겠습니다. 관측된(o) 개수와 예상되는(e) 개수 사이의 평균 제곱 차이를 ’e’로 스케일링해 보겠습니다. 우리는 oestat 함수를 호출할 것입니다.\noestat = function(o, e) { sum((o-e)^2 / e) } oe = oestat(o = lfM, e = length(Celegans$chrM) / 4) oe __\n[1] 4386.634\n이것이 무작위성이 설명할 수 있는 것보다 더 큰가요? 우리는 이미 null 모델에서 기대할 수 있는 일련의 일반적인 개수를 살펴보았습니다. 그러나 우리에게는 가치의 전체 집합(분포)이 필요합니다. 함수를 여러 번 평가하는 복제 함수를 사용하여 이를 계산합니다. 우리는 다음을 실행합니다:\nB = 10000 n = length(Celegans$chrM) expected = rep(n / 4, 4) oenull = replicate(B, oestat(e = expected, o = rmultinom(1, n, p = rep(1/4, 4))))__\n더빈, 리차드, 션 에디, 앤더스 크로그, 그레이엄 미치슨. 1998. 생물학적 서열 분석. 케임브리지 대학 출판부.\n프리드먼, 데이비드, 로버트 피사니, 로저 퍼브스. 1997. 통계. 뉴욕, 뉴욕: WW 노턴.\n라이스, 존. 2006. 수학적 통계 및 데이터 분석. 센게이지 학습.\n로버트, 크리스찬, 조지 카셀라. 2009. R을 이용한 몬테카를로 방법 소개. Springer 과학 및 비즈니스 미디어.\nR 버전 4.5.1(2025-06-13)을 사용하여 2025-09-01 01:33에 작성된 페이지",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>1.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "02-chap.html",
    "href": "02-chap.html",
    "title": "4  2.1 이 장의 목표",
    "section": "",
    "text": "4.0.0.1 매개변수가 핵심입니다.\n이전 장에서 생성 모델과 매개변수 값에 대한 지식은 의사 결정에 사용할 수 있는 확률(예: 실제로 에피토프를 찾았는지 여부)을 제공했습니다. 많은 실제 상황에서는 생성 모델이나 매개변수가 모두 알려져 있지 않으며 수집된 데이터를 사용하여 이를 추정해야 합니다. 통계 모델링은 데이터를 상향으로 시작하여 데이터를 아마도 그럴듯하게 설명하는 모델1로 작동합니다. 이러한 상향 추론 단계를 통계적 추론이라고 합니다. 이 장에서는 추론을 위한 구성 요소 역할을 하는 분포 및 추정 메커니즘 중 일부를 보여줍니다. 이 장의 예제는 모두 매개변수적이지만(즉, 통계 모델에는 알려지지 않은 소수의 매개변수만 있음) 우리가 논의하는 원리는 일반화됩니다.\n1 현재 데이터를 모두 완벽하게 설명하는 모델을 찾았다고 해도 현실은 항상 더 복잡할 수 있습니다. 새로운 데이터 세트를 통해 다른 모델이 필요하다는 결론을 내릴 수 있으며 현재 모델을 특수 사례 또는 근사치로 포함할 수 있습니다.\n통계 설정에서는 데이터 \\(X\\)로 시작하여 이를 사용하여 매개변수를 _추정_합니다. 이러한 추정치는 \\(\\)와 같이 우리가 모자라고 부르는 그리스 문자로 표시됩니다.\n이 장에서는 다음을 수행합니다.\n매개변수의 예: 단일 매개변수 \\(\\)는 포아송 분포를 정의합니다. 문자 \\(\\)는 정규 평균의 평균으로 자주 사용됩니다. 보다 일반적으로, 우리는 확률 모델을 지정하는 데 필요한 매개변수의 일반적인 튜플을 지정하기 위해 그리스 문자 \\(\\)를 사용합니다. 예를 들어, 이항 분포의 경우 \\(=(n,p)\\)는 양의 정수와 0과 1 사이의 실수라는 두 숫자로 구성됩니다.\n우리는 1장에서 에피토프 예의 모든 매개변수 값에 대한 지식을 통해 확률 모델을 사용하고 우리가 보유한 데이터를 기반으로 귀무 가설을 테스트할 수 있음을 확인했습니다. 몇 가지 실제 사례와 컴퓨터 시뮬레이션을 통해 통계 모델링에 대한 다양한 접근 방식을 살펴보겠지만, 사용 가능한 정보의 양에 따라 두 가지 상황을 구별하는 것부터 시작하겠습니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "02-chap.html#통계모델과-확률모델의-차이점",
    "href": "02-chap.html#통계모델과-확률모델의-차이점",
    "title": "4  2.1 이 장의 목표",
    "section": "4.1 2.2 통계모델과 확률모델의 차이점",
    "text": "4.1 2.2 통계모델과 확률모델의 차이점\n확률적 분석은 데이터의 무작위성에 대한 좋은 생성 모델을 알고 그리고 해당 매개변수의 실제 값을 알 때 가능합니다.\n\n그림 2.1: 1장에서 얻은 확률 모델. 데이터는 녹색으로 \\(x\\)로 표시됩니다. \\(\\)의 실제 값을 안다면 \\(x\\)의 가능한 모든 인스턴스, 특히 우리가 관찰한 \\(x\\)에 대해 \\(x\\)를 관찰할 확률을 계산할 수 있습니다.\n에피토프 예에서 위양성(false positives)이 위치당 베르누이(0.01)로 발생한다는 것을 알고, 분석된 환자 샘플의 수와 단백질의 길이는 _알 수 없는 매개변수가 없음_을 의미합니다.\n그러한 경우, 우리는 수학적 추론을 사용하여 그림 2.1에 도식화된 사건의 확률을 계산할 수 있습니다. 에피토프 예에서는 주어진 매개변수 \\(\\)를 사용하여 포아송 확률을 널 모델로 사용했습니다. 우리는 수학적 추론을 통해 7 이상의 최대값을 볼 가능성이 약 \\(10^{-4}\\)이므로 실제로 관찰된 데이터가 해당 모델(또는 “귀무가설”) 하에서는 거의 발생하지 않는다는 결론을 내릴 수 있었습니다.\n이제 우리가 환자 수와 단백질의 길이(이것은 실험 설계에 의해 제공됨)를 알고 있지만 분포 자체와 위양성률은 알지 못한다고 가정합니다. 데이터를 관찰한 후에는 확률 모델 \\(F\\)(푸아송, 정규, 이항)과 해당 모델에 대한 누락된 매개변수를 모두 추정하기 위해 데이터에서 위로 이동해야 합니다. 이것이 이번 장에서 설명할 통계적 추론 유형입니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "02-chap.html#통계-모델링의-간단한-예",
    "href": "02-chap.html#통계-모델링의-간단한-예",
    "title": "4  2.1 이 장의 목표",
    "section": "4.2 2.3 통계 모델링의 간단한 예",
    "text": "4.2 2.3 통계 모델링의 간단한 예\n\n4.2.0.1 데이터부터 시작하세요\n모델링 절차는 두 부분으로 구성됩니다. 먼저 데이터 생성 프로세스를 모델링하려면 합리적인 확률 _분포_가 필요합니다. 1장에서 본 것처럼 이산형 카운트 데이터는 이항, 다항 또는 포아송 분포와 같은 간단한 확률 분포로 모델링될 수 있습니다. 정규 분포 또는 종 모양의 곡선은 연속 측정에 적합한 모델인 경우가 많습니다. 배포판은 이러한 기본 배포판의 더 복잡한 혼합일 수도 있습니다(자세한 내용은 4장 참조).\n까다로운 이상치 없이 시작하여 이전 장의 에피토프 예를 다시 살펴보겠습니다.\nload(“../data/e100.RData”) e99 = e100[-which.max(e100)]__\n\n\n4.2.0.2 적합도 : 육안평가\n첫 번째 단계는 후보 분포에서 적합성을 찾는 것입니다. 이를 위해서는 그래픽 및 정량적 적합도 플롯에 대한 컨설팅이 필요합니다. 이산형 데이터의 경우 그림 2.2와 같이 빈도 막대 그래프를 그릴 수 있습니다(연속형 데이터의 경우 히스토그램 참조).\nbarplot(table(e99), space = 0.8, col = “chartreuse4”)__\n\n그림 2.2: 이상치가 없는 에피토프 데이터의 관찰된 분포.\n그러나 비교를 사용하지 않고는 어떤 이론적 분포가 데이터에 가장 잘 맞는지 결정하기는 어렵습니다. 하나의 시각적 적합도 다이어그램은 루토그램(Cleveland 1988)으로 알려져 있습니다. 이론적인 빨간색 점에서 관찰된 개수로 막대를 걸어 놓습니다. 개수가 이론적 값과 정확히 일치하는 경우 상자 아래쪽이 가로 축과 정확하게 정렬됩니다.\nlibrary(“vcd”) gf1 = goodfit( e99, “poisson”) rootogram(gf1, xlab = ““, ret_gp = gpar(fill =”chartreuse4”))__\n\n그림 2.3: 이론적 값의 제곱근을 빨간색 점으로 표시하고 관찰된 주파수의 제곱근을 드롭다운 직사각형으로 표시하는 루트그램. (goodfit 함수가 어떤 \\(\\)를 사용할지 결정하는 방법을 아래에서 조금 살펴보겠습니다.)\n__\n질문 2.1\n알려진 포아송 변수를 사용하여 이러한 플롯이 어떻게 보이는지 보정하려면 \\(\\) = 0.05와 함께 rpois를 사용하여 100개의 포아송 분포 숫자를 생성하고 루트그램을 그립니다.\n__\n해결책\n__\nsimp = rpois(100, lambda = 0.05) gf2 = goodfit(simp, “poisson”) rootogram(gf2, xlab = ““)__\n우리는 ’e99’의 루트그램이 푸아송 모델에 합리적으로 잘 맞는 것으로 보입니다. 하지만 이를 실현하기 위해 이상값을 제거했다는 점을 기억하세요. 포아송은 포아송 평균 \\(\\)이라고 불리는 하나의 매개변수에 의해 완전히 결정됩니다. 데이터가 포아송 분포를 따른다고 추측할 수 있는 대부분의 경우 데이터에서 포아송 매개변수를 추정해야 합니다.\n\n\n\n모수는 이론적 분포의 평균이고 표본 평균에 의해 추정되기 때문에 포아송 평균이라고 합니다. 이러한 단어의 오버로드는 모든 사람에게 혼란을 줍니다.\n\n\n이 모수는 이론적 분포의 평균이고 그리고 표본 평균에 의해 추정되기 때문에 포아송 평균이라고 합니다. 이러한 단어의 과부하는 모든 사람에게 혼란을 줍니다.\n\\(\\)를 추정하는 가장 일반적인 방법은 관찰된 데이터를 가장 가능성 있게 만드는 값 \\(\\)을 선택하는 것입니다. 이를 최대 우도 추정기(Rice 2006, chap. 8, 섹션 5)라고 하며, 종종 MLE로 축약됩니다. 우리는 다음 섹션에서 다소 역설적인 이 아이디어를 설명할 것입니다.\n위에서 확률 분포를 추측하기 전에 극단적인 관찰을 수행했지만 나머지 분석을 위해 이를 사용하여 데이터로 돌아갈 것입니다. 실제로 우리는 이상값이 있는지, 그리고 그것이 어떤 데이터 포인트인지 알 수 없습니다. 그것을 그대로 두면 평균의 추정치가 더 높아지는 효과가 있습니다. 결과적으로 이는 널 모델에서 7이라는 값을 관찰할 가능성이 더 높아지며 결과적으로 더 큰 p-값이 생성됩니다. 따라서 결과 p-값이 이상값을 포함하더라도 작다면 우리의 분석은 실제적인 것임을 확신할 수 있습니다. 우리는 이러한 전술을 보수적이라고 부릅니다. 즉, 우리는 무언가를 감지하지 못하는 주의 측면에서 실수를 저지릅니다.\n\n\n4.2.0.3 포아송 분포의 모수 추정\n포아송 평균의 어떤 값이 데이터를 가장 가능성 있게 만드는가? 첫 번째 단계에서는 결과를 집계합니다.\n테이블(e100)__\ne100 0 1 2 7 58 34 7 1\n그런 다음 포아송 평균에 대한 다양한 값을 시험해 보고 어떤 값이 데이터에 가장 적합한지 확인하겠습니다. 푸아송 분포의 평균 \\(\\)가 3인 경우 개수는 다음과 같습니다.\n테이블(rpois(100, 3))__\n0 1 2 3 4 5 6 7 4 12 23 24 14 16 4 3\n데이터에서 볼 수 있는 것보다 2와 3이 더 많습니다. 따라서 개수가 잘 일치하지 않기 때문에 \\(\\)이 데이터를 생성했을 가능성이 낮다는 것을 알 수 있습니다.\n__\n질문 2.2\n\\(\\)의 다른 값을 사용하여 이 시뮬레이션을 반복합니다. 시행착오를 통해 관찰된 것과 가까운 수치를 제공하는 것을 찾을 수 있습니까?\n그래서 우리는 가능한 많은 값을 시험해보고 무차별 대입으로 진행할 수 있었습니다. 그러나 우리는 좀 더 우아한 작업을 수행하고 약간의 수학을 사용하여 어떤 값이 데이터 관찰 확률을 최대화하는지 확인하겠습니다. 포아송 매개변수의 값이 \\(m\\)일 때 데이터를 볼 확률을 계산해 보겠습니다. 데이터가 독립적인 추첨에서 파생된다고 가정하므로 이 확률은 단순히 개별 확률의 곱입니다.\n\\[]\n\\(m=3\\)에 대해 우리는 이것을 계산할 수 있습니다2.\n2 여기서 R의 벡터화를 어떻게 사용하는지 주목하세요. dpois에 대한 호출은 4개의 서로 다른 숫자에 해당하는 4개의 값을 반환합니다. 그런 다음 ‘^’ 연산자를 사용하여 이를 각각 58, 34, 7 및 1의 거듭제곱으로 가져오면 다시 4개의 값이 생성됩니다. 마지막으로 ‘prod’ 함수를 사용하여 이를 하나의 숫자인 product로 축소합니다.\nprod(dpois(c(0, 1, 2, 7), 람다 = 3) ^ (c(58, 34, 7, 1)))__\n[1] 1.392143e-110\n__\n질문 2.3\n\\(m=0,1,2\\)에 대해 위와 같이 확률을 계산합니다. \\(m\\)은 정수여야 합니까? 예를 들어 \\(m=0.4\\)에 대한 확률을 계산해 보세요.\n__\n해결책\n__\nprod(dpois(c(0, 1, 2, 7), 람다 = 0.4) ^ (c(58, 34, 7, 1)))__\n[1] 8.5483e-46\n이 확률은 주어진 데이터에서 \\(\\)의 우도 함수이며 이를 씁니다.\n여기서 \\(L\\)은 우도를 나타내고 \\(f(k)=e^{-} ,^k,/,k!\\)는 앞서 본 포아송 확률을 나타냅니다.\n\\[ L(,,x=(k_1,k_2,k_3,…))=_{i=1}^{100}f(k_i) \\]\n100개의 작은 숫자를 곱하는 대신 로그를 취하는 것이 편리합니다. 로그는 엄격하게 증가하므로 로그가 구간 내에서 최대값에 도달하는 지점이 있으면 확률에 대한 최대값도 됩니다.\n3 이는 일반적으로 연필과 종이, 그리고 컴퓨터 계산 모두에 해당됩니다.\n4 여기서는 데이터 포인트에 대한 명시적인 루프 없이 계산을 작성할 수 있는 R의 벡터 구문을 다시 사용합니다. 위의 코드와 비교하면, 여기서는 고유한 값에 대해서만 ‘dpois’를 호출하기 전에 ’table’ 함수로 ’data’를 표로 만드는 대신 100개의 데이터 포인트 각각에 대해 ’dpois’를 호출합니다. 이는 결과가 동일하지만 코드를 읽는 것이 얼마나 쉬운지 또는 실행하는 데 걸리는 시간이 다를 수 있는 대체 솔루션에 대한 간단한 예입니다.\n계산적인 일러스트레이션부터 시작해 보겠습니다. 우리는 포아송 매개변수의 다양한 값에 대한 가능성을 계산합니다. 이를 위해서는 다양한 값에 대한 데이터의 확률을 계산하는 작은 함수를 작성해야 합니다4.\nloglikelihood = function(lambda, data = e100) { sum(log(dpois(data, lambda))) }__\n이제 우리는 0.05에서 0.95까지 일련의 ‘람다’ 값에 대한 우도를 계산할 수 있습니다(그림 2.4).\nlambdas = seq(0.05, 0.95, length = 100) loglik = vapply(lambdas, loglikelihood, numeric(1)) plot(lambdas, loglik, type = “l”, col = “red”, ylab = ““, lwd = 2, xlab = expression(lambda)) m0 = mean(e100) abline(v = m0, col =”blue”, lwd = 2) abline(h = loglikelihood(m0), col = “purple”, lwd = 2) m0 __\n[1] 0.55\n\n그림 2.4: 빨간색 곡선은 로그 우도 함수입니다. 수직선은 ‘m’(평균) 값을 나타내고 수평선은 ’m’의 로그 우도를 나타냅니다. ’m’이 가능성을 최대화하는 것 같습니다.\n__\nQuestion 2.4\n위 코드에서 vapply 함수는 무엇을 합니까? 힌트: 매뉴얼 페이지를 확인하세요.\n__\n해결책\n__\n‘vapply’는 첫 번째 인수인 이 경우 벡터 ’lambdas’를 취하고 각 벡터 요소에 ’loglikelihood’ 함수(두 번째 인수)를 반복적으로 적용합니다. 결과적으로 결과의 벡터를 반환합니다. 이 함수에는 세 번째 인수인 ’numeric(1)’도 필요합니다. 이 인수는 ’loglikelihood’에 대한 각 개별 호출이 반환할 값 유형(단일 숫자)을 지정합니다. (일반적으로 함수가 때때로 문자열이나 두 개의 숫자와 같은 다른 것을 반환하는 경우가 발생할 수 있습니다. 이 경우 전체 결과를 일관된 벡터로 조합할 수 없으며 vapply가 불평합니다.)\n실제로 ‘goodfit’ 함수라는 지름길이 있습니다.\ngf = goodfit(e100, “poisson”) names(gf)__\n[1] “관찰됨” “계산” “적합” “유형” “방법” “df” “par”\ngf$파 __\n$람다 [1] 0.55\n’goodfit’의 출력은 목록이라는 복합 개체입니다. 해당 구성요소 중 하나는 ’par’라고 하며 연구된 분포에 적합한 매개변수의 값을 포함합니다. 이 경우 추정치는 \\(\\)라는 하나의 숫자뿐입니다.\n__\n질문 2.5\ngoodfit 함수 출력의 다른 구성요소는 무엇입니까?\n__\n일\nm 값을 이전에 \\(\\)에 사용한 값인 0.5와 비교합니다. 0.5 대신 m을 사용하여 1장에서 수행한 모델링을 다시 실행하세요.\n\n\n4.2.1 2.3.1 고전 데이터에 대한 고전 통계\n다음은 표본 평균이 (로그) 가능성을 최대화한다는 계산 결과에 대한 공식적인 증거입니다.\n\\[ \\[\\begin{align} \\log L(\\lambda, x) &= \\sum_{i=1}^{100} - \\lambda + k_i\\log\\lambda - \\log(k_i!) \\\\\\ &= -100\\lambda + \\log\\lambda\\left(\\sum_{i=1}^{100}k_i\\right) + \\text{const.} \\end{align}\\] \\]\n우리는 포괄적인 “const”를 사용합니다. \\(\\)에 의존하지 않는 용어의 경우(비록 \\(x\\), 즉 \\(k_i\\)에 의존하지만) 이를 최대화하는 \\(\\)를 찾기 위해 \\(\\)에서 도함수를 계산하고 이를 0으로 설정합니다.\n\\[ \\[\\begin{align} \\frac{d}{d\\lambda}\\log L &= -100 + \\frac{1}{\\lambda} \\sum_{i=1}^{100}k_i \\stackrel{?}{=}0 \\\\\\ \\lambda &= \\frac{1}{100} \\sum_{i=1}^{100}k_i = \\bar{k} \\end{align}\\] \\]\n방금 (데이터로부터) ‘처음부터’ 시작하여 모델 매개변수를 추론하는 _통계적 접근_의 첫 번째 단계를 살펴보았습니다. 이는 데이터에서 매개변수를 통계적으로 _추정_하는 것입니다. 또 다른 중요한 구성 요소는 데이터를 모델링하는 데 사용할 분포군을 선택하는 것입니다. 그 부분은 _적합도_를 평가하여 수행됩니다. 우리는 나중에 이것을 만날 것입니다.\n전통적인 통계 테스트 프레임워크에서는 데이터에 대해 _null 모델_이라고 하는 하나의 단일 모델을 고려합니다. 널 모델은 모든 관찰이 어떤 그룹이나 치료에 관계없이 동일한 무작위 분포에서 나오는 것과 같이 “흥미롭지 않은” 기준선을 공식화합니다. 그런 다음 데이터가 해당 모델과 호환될 확률을 계산하여 더 흥미로운 일이 진행되고 있는지 테스트합니다. 종종 이것이 우리가 할 수 있는 최선의 방법입니다. 왜냐하면 우리는 “흥미로운”, null이 아닌 모델 또는 대체 모델이 무엇인지 충분히 자세히 알지 못하기 때문입니다. 다른 상황에서는 나중에 살펴보겠지만 비교할 수 있는 두 가지 경쟁 모델이 있습니다.\n__\n질문 2.6\n알려진 분포를 사용한 모델링의 가치는 무엇입니까? 예를 들어, 변수에 포아송 분포가 있다는 것을 아는 것이 왜 흥미로운가요?\n__\n해결책\n__\n모델은 데이터 생성 프로세스를 간결하지만 표현적으로 표현합니다. 예를 들어 푸아송의 경우 하나의 숫자를 알면 앞서 살펴본 것처럼 극단적이거나 희귀한 사건의 확률을 포함하여 분포에 대한 모든 것을 알 수 있습니다.\n또 다른 유용한 방향은 회귀입니다. 우리는 카운트 기반 응답 변수(예: 시퀀싱 판독 카운트 결과)가 연속 공변량(예: 온도 또는 영양분 농도)에 어떻게 의존하는지 알고 싶을 수 있습니다. 우리 모델은 응답 변수 \\(y\\)가 방정식 \\(y = ax+b + e\\)를 통해 공변량 \\(x\\)에 의존하고 매개변수 \\(a\\) 및 \\(b\\)(추정해야 함)와 확률 모델이 정규 분포(일반적으로 분산도 추정해야 함)인 잔차 \\(e\\)를 사용하는 선형 회귀를 접했을 수 있습니다. 카운트 데이터의 경우 동일한 유형의 회귀 모델이 가능하지만 잔차의 확률 분포는 비정규적이어야 합니다. 이 경우 일반화 선형 모델 프레임워크를 사용합니다. Chapter 8에서는 RNA-Seq을 연구할 때의 예시를, Chapter 9에서는 또 다른 차세대 시퀀싱 데이터인 16S rRNA 데이터를 살펴보겠습니다.\n확률 모델에 포아송, 이항, 다항 분포 또는 기타 매개변수 계열이 포함되어 있다는 사실을 알면 모델 매개변수에 대한 질문에 대한 빠른 답변을 얻고 p-값 및 신뢰 구간과 같은 수량을 계산할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "02-chap.html#이항분포와-최대우도",
    "href": "02-chap.html#이항분포와-최대우도",
    "title": "4  2.1 이 장의 목표",
    "section": "4.3 2.4 이항분포와 최대우도",
    "text": "4.3 2.4 이항분포와 최대우도\n이항 분포에는 두 가지 매개변수가 있습니다. 일반적으로 알려진 시행 횟수 \\(n\\)와 시행에서 1이 나올 확률 \\(p\\)입니다. 이 확률은 종종 알려지지 않았습니다.\n\n4.3.1 2.4.1 예\n\\(n=120\\)명의 남성 샘플을 채취하여 적록색맹 여부를 테스트한다고 가정해 보겠습니다. 대상이 색맹이 아닌 경우 데이터를 0으로, 색맹인 경우 1로 코딩할 수 있습니다. 데이터를 표로 요약합니다.\n테이블(CB)__\nCB 0 1 110 10\n__\n질문 2.7\n주어진 데이터에서 가장 가능성이 높은 \\(p\\) 값은 무엇입니까?\n__\n해결책\n__\n\\(=\\).\n평균(cb)__\n[1] 0.08333333\n\n\n\n그러나 주의하세요. 최대 우도 추정치는 추측하고 계산하기 어려울 뿐만 아니라 훨씬 덜 직관적이기도 합니다(연습 imp-models-mlmax 참조).\n\n\n그러나 조심하세요. 최대 우도 추정치는 추측하고 계산하기가 더 어려울 뿐만 아니라 훨씬 덜 직관적이기도 합니다(연습 2.2 참조).\n이 특별한 경우에는 직관으로 추정값 \\(=\\)을 얻을 수 있으며, 이는 최대 우도 추정값으로 판명됩니다. 우리는 이것이 (반드시) 기본 실제 값이 아니라 데이터에서 얻은 추정치임을 상기시키기 위해 편지 위에 모자를 썼습니다.\n포아송의 경우 이전과 마찬가지로 가능한 많은 \\(p\\)에 대한 우도를 계산하면 이를 플롯하고 최대값이 떨어지는 위치를 확인할 수 있습니다(그림 2.5).\nprobs = seq(0, 0.3, by = 0.005) likelihood = dbinom(sum(cb), prob = probs, size = length(cb)) plot(probs, likelihood, pch = 16, xlab = “probability of success”, ylab = “likelihood”, cex=0.6) probs[which.max(likelihood)]__\n[1] 0.085\n\n그림 2.5: 확률의 함수로 우도를 도표로 나타낸 것입니다. 가능성은 \\([0, 1]\\)의 함수입니다. 여기서는 \\([0, 0.3]\\)의 범위를 확대했습니다. \\(p\\)의 더 큰 값에 대한 가능성은 사실상 0이기 때문입니다.\n참고: 0.085는 정확히 우리가 기대한 값 \\(()\\)이 아닙니다. 이는 우리가 시도한 값 집합(probs에서)에 \\(\\)의 정확한 값이 포함되지 않았기 때문에 차선책을 얻었기 때문입니다. 이를 극복하기 위해 수치 최적화 방법을 사용할 수 있습니다.\n\n\n4.3.2 2.4.2 이항분포의 가능성\n\n\n\n최대 우도와는 다른 기준을 제시할 수 있으며, 이는 다른 추정기로 이어집니다. 그들은 모두 모자를 가지고 다닙니다. 초-혼합에서 다른 예를 볼 수 있습니다.\n\n\n최대 우도와는 다른 기준을 제시하여 다른 추정기로 이어질 수 있습니다. 그들은 모두 모자를 가지고 다닙니다. 다른 예시는 4장에서 살펴보겠습니다.\n가능성과 확률은 동일한 수학적 함수이며 다른 방식으로만 해석됩니다. 어떤 경우에는 함수가 매개변수가 주어졌을 때 데이터의 특정 값 집합을 볼 가능성이 얼마나 되는지 알려줍니다. 다른 경우에는 데이터를 주어진 것으로 간주하고 이러한 데이터를 생성했을 가능성이 있는 매개변수 값을 요청합니다. \\(n=300\\)을 가정하고 \\(y=40\\) 성공을 관찰합니다. 그런 다음 이항 분포의 경우:\n\\[ f(p,|,n,y) = f(y,|,n,p)={n 선택} , p^y , (1-p)^{(n-y)}. \\]\n다시 말하지만, 우도의 로그를 사용하여 작업하는 것이 더 편리합니다.\n\\[ f(p |y) = + y(p) + (n-y)(1-p). \\]\n다음은 이를 계산하는 데 사용할 수 있는 함수입니다5,\n5 실제로는 ’choose(n, y)’를 명시적으로 계산하는 것을 피하려고 합니다. 이는 컴퓨터의 부동 소수점 연산의 한계를 테스트하는 매우 큰 숫자일 수 있기 때문입니다(n=300 및 y=40의 경우 약 9.8e+49입니다). 최대화에 영향을 주지 않는 \\(p\\)와 무관한 추가 오프셋일 뿐이므로 스털링 공식을 사용하여 항을 근사화하거나 실제로 무시할 수 있습니다.\nloglikelihood = function(p, n = 300, y = 40) { log(choose(n, y)) + y * log(p) + (n - y) * log(1 - p) }__\n0에서 1까지의 \\(p\\) 범위를 플롯합니다(그림 2.6).\np_seq = seq(0, 1, by = 0.001) plot(p_seq, loglikelihood(p_seq), xlab = “p”, ylab = “log f(p|y)”, type = “l”)__\n\n그림 2.6: \\(n=300\\) 및 \\(y=40\\)에 대한 로그 우도 함수 플롯.\n최대값은 40/300 = 0.1333… 에 있으며 직관과 일치하지만 함수가 최대값 주위에서 매우 균일하므로 \\(p\\)의 다른 값이 거의 동일할 가능성이 있음을 알 수 있습니다. 우리는 베이지안 방법을 사용하여 단일 최대값을 선택하는 대신 \\(p\\)에 대한 값 범위로 작업할 수 있는 방법을 이후 섹션에서 살펴보겠습니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "02-chap.html#추가-상자다항-데이터",
    "href": "02-chap.html#추가-상자다항-데이터",
    "title": "4  2.1 이 장의 목표",
    "section": "4.4 2.5 추가 상자:다항 데이터",
    "text": "4.4 2.5 추가 상자:다항 데이터\n\n4.4.1 2.5.1 DNA 수 모델링: 염기쌍\nDNA에는 네 가지 기본 분자가 있습니다: A - 아데닌, C - 시토신, G - 구아닌, T - 티민. 뉴클레오티드는 퓨린(A와 G)과 피리미딘(C와 T)의 두 그룹으로 분류됩니다. 이항식은 퓨린/피리미딘 그룹화에 대한 모델로 작동하지만 A, C, G, T를 사용하려는 경우에는 작동하지 않습니다. 이를 위해서는 섹션 1.4의 다항 모델이 필요합니다. 이러한 주파수에서 발생하는 눈에 띄는 패턴을 살펴보겠습니다.\n\n\n4.4.2 2.5.2 뉴클레오티드 편향\n이 섹션에서는 실제 사례에서 시뮬레이션을 통한 추정과 테스트를 결합합니다. Staphylococcus aureus 박테리아 유전자에 대한 DNA 한 가닥의 데이터는 fasta 파일 ’staphsequence.ffn.txt’에서 확인할 수 있으며, Bioconductor 패키지 Biostrings의 기능을 사용하여 읽을 수 있습니다.\nlibrary(“바이오스트링”) staph = readDNAStringSet(“../data/staphsequence.ffn.txt”, “fasta”)__\n첫 번째 유전자를 살펴보겠습니다.\n포도상구균[1]__\n길이가 1인 DNAStringSet 개체: 너비 seq 이름 [1] 1362 ATGTCGGAAAAAGAAATTTGGGA…AAAAAGAAATAAGAAATGTATAA lcl|NC_002952.2_c…\nletterFrequency(staph[[1]], 문자 = “ACGT”, OR = 0)__\nA C G T 522 219 229 392\n__\n질문 2.8\n두 번째 줄에 이중 대괄호를 사용한 이유는 무엇입니까?\n__\n해결책\n__\n이중 대괄호 [[i]]는 단일 _DNAString_이 포함된 _DNAStringSet_를 반환하는 단일 괄호 [i] 쌍과 반대로 i 번째 유전자의 시퀀스를 _DNAString_로 추출합니다. staph[1]의 길이를 보면 1인데 staph[[1]]의 길이는 1362이다.\n__\n질문 2.9\n연습 1.8과 유사한 절차에 따라 뉴클레오티드가 이 첫 번째 유전자의 4개 뉴클레오티드에 걸쳐 균등하게 분포되어 있는지 테스트합니다.\n물리적 특성이 다르기 때문에 진화 선택은 뉴클레오티드 빈도에 영향을 미칠 수 있습니다. 따라서 우리는 이 데이터의 처음 10개 유전자가 동일한 다항식에서 나온 것인지 물어볼 수 있습니다. 우리는 사전 참고 자료가 없으며 처음 10개 유전자에서 뉴클레오티드가 동일한 비율로 나타나는지 여부만 결정하고 싶습니다. 그렇지 않다면, 이는 이들 10개 유전자에 대한 선택압이 다양하다는 증거를 제공할 것입니다.\nletterFrq = vapply(staph, letterFrequency, FUN.VALUE = numeric(4), letters = “ACGT”, OR = 0) colnames(letterFrq) = paste0(“gene”, seq(along = staph)) tab10 = letterFrq[, 1:10] computeProportions = function(x) { x/sum(x) } prop10 = apply(tab10, 2, computeProportions) round(prop10, digits = 2)__\n유전자1 유전자2 유전자3 유전자4 유전자5 유전자6 유전자7 유전자8 유전자9 유전자10 A 0.38 0.36 0.35 0.37 0.35 0.33 0.33 0.34 0.38 0.27 C 0.16 0.16 0.13 0.15 0.15 0.15 0.16 0.16 0.14 0.16G 0.17 0.17 0.23 0.19 0.22 0.22 0.20 0.21 0.20 0.20 T 0.29 0.31 0.30 0.29 0.27 0.30 0.30 0.29 0.28 0.36\np0 = rowMeans(prop10) p0 __\nA C G T 0.3470531 0.1518313 0.2011442 0.2999714\n따라서 ’p0’가 10개 유전자 모두에 대한 다항 확률의 벡터라고 가정하고 몬테카를로 시뮬레이션을 사용하여 이 가정 하에서 관찰된 문자 빈도와 예상 값 사이의 차이가 타당한 범위 내에 있는지 여부를 테스트해 보겠습니다.\n우리는 확률 벡터 p0의 ‘외부’ 곱과 10개 열 각각의 뉴클레오티드 개수 합계 ’cs’를 취하여 예상 개수를 계산합니다.\ncs = colSums(tab10) cs __\n유전자1 유전자2 유전자3 유전자4 유전자5 유전자6 유전자7 유전자8 유전자9 유전자10 1362 1134 246 1113 1932 2661 831 1515 1287 696\nexpectedtab10 = outer(p0, cs, FUN = “*“) round(expectedtab10)__\n유전자1 유전자2 유전자3 유전자4 유전자5 유전자6 유전자7 유전자8 유전자9 유전자10 A 473 394 85 386 671 924 288 526 447 242 C 207 172 37 169 293 404 126 230 195 106 G 274 228 49 224 389 535 167 305 259 140 티 409 340 74 334 580 798 249 454 386 209\n이제 rmultinom 함수를 사용하여 올바른 열 합계를 포함하는 무작위 테이블을 만들 수 있습니다. 이 테이블은 실제 비율이 ’p0’에 의해 제공된다는 귀무가설에 따라 생성됩니다.\nrandomtab10 = sapply(cs, function(s) { rmultinom(1, s, p0) } ) all(colSums(randomtab10) == cs)__\n[1] 사실\n이제 이것을 B = 1000번 반복합니다. 각 테이블에 대해 1장의 섹션 1.4.1(stat 함수)에서 테스트 통계를 계산하고 결과를 벡터 simulstat에 저장합니다. 이 값들은 ’p0’가 10개 유전자 각각에 대한 다항 비율의 벡터라는 귀무 가설 하에서 생성되었으므로 함께 귀무 분포를 구성합니다.\nstat = function(obsvd, exptd) { sum((obsvd - exptd)^2 / exptd) } B = 1000 simulstat = replicate(B, { randomtab10 = sapply(cs, function(s) { rmultinom(1, s, p0) }) stat(randomtab10, expectedtab10) }) S1 = stat(tab10, expectedtab10) sum(simulstat &gt;= S1)__\n[1] 0\nhist(simulstat, col = “라벤더”, break = seq(0, 75, length.out=50)) abline(v = S1, col = “빨간색”) abline(v = Quantile(simulstat, probs = c(0.95, 0.99)), col = c(“darkgreen”, “blue”), lty = 2)__\n\n그림 2.7: simulstat의 히스토그램. ’S1’의 값은 빨간색 수직선으로 표시되고, 0.95 및 0.99 분위수(다음 섹션 참조)의 값은 점선으로 표시됩니다.\n히스토그램은 그림 2.7에 나와 있습니다. _null 모델_에서는 S1=70.1만큼 큰 값을 볼 확률이 매우 작다는 것을 알 수 있습니다. 1000번의 시뮬레이션에서 ’S1’만큼 큰 값이 발생한 경우는 0번 발생했습니다. 따라서 10개의 유전자는 동일한 다항 모델에서 나온 것 같지 않습니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "02-chap.html#분포",
    "href": "02-chap.html#분포",
    "title": "4  2.1 이 장의 목표",
    "section": "4.5 2.6 \\(^2\\) 분포",
    "text": "4.5 2.6 \\(^2\\) 분포\n사실, 우리는 이러한 시뮬레이션을 실행하지 않고도 통계 이론을 사용하여 동일한 결론에 도달할 수 있었습니다. simulstat 통계의 이론적 분포는 매개변수가 30(\\(=10(4-1)\\))인 \\(^2\\)(카이제곱) 분포6라고 합니다. 이것을 S1 \\(=\\) 70.1만큼 큰 값을 가질 확률을 계산하는 데 사용할 수 있습니다. 위에서 본 것처럼 작은 확률은 Monte Carlo로 계산하기 어렵습니다. 계산의 세분성은 \\(1/B\\)이므로 이보다 작은 확률은 추정할 수 없으며 실제로 추정의 불확실성은 더 큽니다. 따라서 어떤 이론이라도 적용 가능하다면 그것은 유용한 경향이 있습니다. 또 다른 시각적 적합도 도구인 분위수-분위수(QQ) 플롯을 사용하여 우리 사례에서 이론과 시뮬레이션이 얼마나 잘 일치하는지 확인할 수 있습니다. 두 개의 서로 다른 표본 또는 하나의 표본 대 이론적 모델의 두 분포를 비교할 때 히스토그램만 보는 것만으로는 충분한 정보를 얻을 수 없습니다. 우리는 각 분포의 분위수를 기반으로 하는 방법을 사용합니다.\n6 엄밀히 말하면 simulstat의 분포는 대략 \\(^2\\) 분포로 설명됩니다. 테이블의 개수가 클 경우 근사치는 특히 좋습니다.\n\n4.5.1 2.6.1 Intermezzo: 분위수 및 분위수-분위수 도표\n이전 장에서는 100개의 샘플 값 \\(x_{(1)},x_{(2)},…,x_{(100)}\\)을 주문했습니다. 22번째 백분위수를 원한다고 가정해 보겠습니다. 22번째와 23번째 값 사이의 모든 값을 사용할 수 있습니다. 즉, \\(x_{(22)} c_{0.22} &lt; x_{(23)}\\)를 충족하는 모든 값은 0.22 분위수(\\(c_{0.22}\\))로 허용됩니다. 즉, \\(c_{0.22}\\)는 다음과 같이 정의됩니다.\n\\[ = 0.22. \\]\n섹션 3.6.7에서는 경험적 누적 분포 함수(ECDF) \\(\\)를 소개하고 \\(c_{0.22}\\)의 정의가 \\(n(c{0.22}) =로 작성될 수도 있음을 확인합니다. 0.22\\). 그림 2.7에는 simulstat 분포에 대한 히스토그램, 분위수 \\(c_{0.95}\\) 및 \\(c_{0.99}\\)도 표시되어 있습니다.\n__\n질문 2.10\n\nsimulstat 값과 무작위로 생성된 1000개의 \\(^2_{30}\\) 난수를 각각 50개의 빈이 있는 히스토그램에 표시하여 비교합니다.\nsimulstat 값의 분위수를 계산하고 이를 \\(_{30}^2\\) 분포의 분위수와 비교합니다. 힌트:\n\nqs = ppoints(100) quantile(simulstat, qs) quantile(qchisq(qs, df = 30), qs)__\n\n\n\n여기서 이름 충돌이 발생합니다. 통계학자들은 방금 계산한 요약 통계를 simulstat(가중치 차이의 제곱합), 카이제곱 또는 \\chi^2 통계라고 부릅니다. 이론적 분포 \\chi^2_\\nu는 자유도라는 매개변수 \\nu를 사용하는 그 자체의 분포입니다. 카이 제곱 또는 \\chi^2에 대해 읽을 때 어떤 의미가 적절한지 확인하기 위해 문맥에 주의를 기울여야 합니다.\n\n\n여기서 이름 충돌이 발생합니다. 통계학자들은 방금 계산한 요약 통계를 simulstat(가중치 차이의 제곱합), 카이제곱 또는 \\(^2\\) statistic_이라고 부릅니다. 이론적 분포 \\(^2\\)는 자유도라는 매개변수 \\(\\)를 사용하는 그 자체의 분포입니다. 카이제곱 또는 \\(^2\\)에 대해 읽을 때 어떤 의미가 적절한지 확인하려면 문맥에 주의를 기울여야 합니다.\n__\n질문 2.11\n0.5 분위수의 다른 이름을 알고 있나요?\n__\n해결책\n__\n중앙값.\n__\n질문 2.12\n위의 정의에서 우리는 분위수가 일반적으로 어떻게 정의되는지에 대해 약간 모호했습니다. 즉, 0.22에 대해서만이 아닙니다. \\(1/n\\)의 배수가 아닌 숫자를 포함하여 0과 1 사이의 숫자에 대한 분위수는 어떻게 계산됩니까?\n__\n해결책\n__\n‘Quantile’ 함수의 매뉴얼 페이지와 ’type’이라는 인수를 확인하세요.\n이제 분위수가 무엇인지 알았으므로 분위수-분위수 플롯을 수행할 수 있습니다. 귀무 가설 하에서 시뮬레이션한 ‘simulstat’ 값의 분위수를 이론적 귀무 분포 \\(^2_{30}\\)에 대해 플롯합니다(그림 2.8).\nqqplot(qchisq(ppoints(B), df = 30), simulstat, main = ““, xlab = 표현식(chi[nu==30]^2), asp = 1, cex = 0.5, pch = 16) abline(a = 0, b = 1, col =”red”)__\n\n그림 2.8: QQ(분위수-분위수) 플롯을 사용하여 \\({30}^2\\)와 비교한 시뮬레이션된 통계 분포. 이 분포는 \\(^2{30}\\) 분포에 대한 이론적 분위수를 가로 축에, 샘플링된 분위수를 세로 축에 표시합니다.\nsimulstat가 \\(^2_{30}\\) 분포로 잘 설명되어 있다고 확신하면 이를 사용하여 p-값, 즉 귀무 가설 하에서 확률(카운트는 확률 \\(p_{} = 0.35\\), \\(p_{} = 0.15\\)을 갖는 다항식으로 분포됩니다.) \\(p_{} = 0.2\\), \\(p_{} = 0.3\\)) S1=70.1만큼 높은 값을 관찰합니다.\n1 - pchisq(S1, df = 30)__\n[1] 4.74342e-05\n이렇게 작은 p-값을 사용하면 귀무가설이 불가능해 보입니다. 이 계산에는 어떻게 1000번의 시뮬레이션이 필요하지 않고 더 빠른지 확인하십시오.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "02-chap.html#샤가프의-법칙",
    "href": "02-chap.html#샤가프의-법칙",
    "title": "4  2.1 이 장의 목표",
    "section": "4.6 2.7 샤가프의 법칙",
    "text": "4.6 2.7 샤가프의 법칙\n뉴클레오티드 빈도에서 가장 중요한 패턴은 Chargaff에 의해 발견되었습니다(Elson and Chargaff 1952).\n\n분자의 무게를 사용하여 DNA 서열 분석이 가능해지기 오래 전에 그는 뉴클레오티드가 동일한 빈도로 발생하는지 여부를 물었습니다. 그는 이것을 테트라뉴클레오티드 가설이라고 불렀습니다. 우리는 이를 \\(p_{} = p_{} = p_{} = p_{}\\) 여부를 묻는 것으로 해석합니다.\n불행하게도 Chargaff는 측정 자체가 아닌 각 뉴클레오티드에 대해 서로 다른 유기체에 존재하는 질량의 _백분율_만 발표했습니다.\nload(“../data/ChargaffTable.RData”) ChargaffTable __\nA T C G 인간-흉선 30.9 29.4 19.9 19.8 Mycobac.Tuber 15.1 14.6 34.9 35.4 Chicken-Eryth. 28.8 29.2 20.5 21.5 양간 29.3 29.3 20.5 20.7 성게 32.8 32.1 17.7 17.3 밀 27.3 27.1 22.7 22.8 효모 31.3 32.9 18.7 17.1 대장균 24.7 23.6 26.0 25.7\n\n그림 2.9: ’ChargaffTable’의 다양한 행에 대한 막대 그래프. 패턴을 발견할 수 있나요?\n__\n질문 2.13\n\n이러한 데이터는 가능성이 동일한 다항 범주에서 나온 것처럼 보입니까?\n대체 패턴을 제안해 주실 수 있나요? 시뮬레이션에서 추정됨\n위의 시뮬레이션에서 영감을 받아 패턴을 정량적으로 분석할 수 있습니까?\n\n__\n해결책\n__\n샤가프는 이 질문에 대한 답을 보고 유기체 DNA의 아데닌(A) 양과 티민(T) 양의 완벽한 일치를 보장하는 _염기쌍_이라는 패턴을 가정했습니다. 마찬가지로 구아닌(G)의 양에 관계없이 시토신(C)의 양은 동일합니다. 이것이 이제 샤가프의 법칙(Chargaff’s rule)이라고 불립니다. 반면에, 유기체 내 C/G의 양은 A/T의 양과 상당히 다를 수 있으며 유기체 전반에 걸쳐 뚜렷한 패턴이 없습니다. Chargaff의 규칙을 기반으로 통계를 정의할 수 있습니다.\n\\[ (p_{} - p_{})^2 + (p_{} - p_{})^2, \\]\n테이블의 모든 행에 대해 합산됩니다. 우리는 각 행에서 관찰된 확률이 특별한 순서가 아니어서 As와 Ts의 비율 또는 Cs와 Gs의 비율 사이에 특별한 관계가 없었다는 의미에서 뉴클레오티드가 ’교환 가능’한 경우 데이터와 어떤 일이 발생하는지 비교를 살펴볼 것입니다.\nstatChf = function(x){ sum((x[, “C”] - x[, “G”])^2 + (x[, “A”] - x[, “T”])^2) } chfstat = statChf(ChargaffTable) permstat = replicate(100000, { permuted = t(apply(ChargaffTable, 1, sample)) colnames(permuted) = colnames(ChargaffTable) statChf(permuted) }) pChf = mean(permstat &lt;= chfstat) pChf __\n[1] 0.00014\nhist(permstat, break = 100, main = ““, col =”라벤더”) abline(v = chfstat, lwd = 2, col = “red”)__\n\n그림 2.10: 열의 행별 순열을 사용하여 시뮬레이션을 통해 계산된 통계 ’statChf’의 히스토그램. 관찰된 데이터에 대해 산출된 값은 빨간색 선으로 표시됩니다.\n그림 2.10의 히스토그램은 빨간색 선이 그려진 관측된 11.1만큼 작은 값을 갖는 경우가 매우 드물다는 것을 보여줍니다. 작거나 작은 값을 관찰할 확률은 pChf=1.4^{-4}입니다. 따라서 데이터는 Chargaff의 통찰력을 강력하게 뒷받침합니다.\n__\n질문 2.14\n’pChf’를 계산할 때 관찰된 값보다 작은 널 분포의 값만 살펴보았습니다. 여기서 우리는 왜 일방적인 방식으로 이 일을 했는가?\n\n4.6.1 2.7.1 두 개의 범주형 변수\n지금까지 우리는 예/아니요 이진 상자에 대한 이항 분포와 A, C, G, T와 같은 범주형 변수 또는 aa, aA, AA와 같은 다른 유전자형에 대한 다항 분포 등 다양한 상자로 분류될 수 있는 표본에서 데이터를 가져온 사례를 살펴보았습니다. 그러나 눈 색깔과 머리 색깔과 같은 일련의 대상에 대해 두 개 이상의 범주형 변수를 측정할 수도 있습니다. 그런 다음 눈과 머리 색깔의 모든 조합에 대한 개수를 교차표로 작성할 수 있습니다. 우수표라는 개수 표를 얻습니다. 이 개념은 많은 생물학적 데이터 유형에 매우 유용합니다.\n머리카락눈색상[,, “여성”]__\n눈 머리 갈색 파란색 헤이즐색 녹색 검정색 36 9 5 2 갈색 66 34 29 14 빨간색 16 7 7 7 금발 4 64 5 8\n__\n질문 2.15\nR에서 ‘HairEyeColor’ 개체를 탐색해 보세요. 어떤 데이터 유형, 모양, 크기가 있나요?\n__\n해결책\n__\n이는 3차원의 숫자 배열입니다.\nstr(HairEyeColor)__\n‘테이블’ 번호 [1:4, 1:4, 1:2] 32 53 10 3 11 50 10 30 10 25 …\n\nattr(*, “dimnames”)=3개 목록 ..$ 헤어: chr [1:4] “검은색” “갈색” “빨간색” “금발” ..$ 눈 : chr [1:4] “갈색” “파란색” “개암색” “녹색” ..$ 성별 : chr [1:2] “남성” “여성”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "02-chap.html#머리눈색상-__",
    "href": "02-chap.html#머리눈색상-__",
    "title": "4  2.1 이 장의 목표",
    "section": "4.7 ?머리눈색상 __",
    "text": "4.7 ?머리눈색상 __\n\n4.7.0.1 색맹과 섹스\n녹색맹은 중간 파장에 민감한 원뿔(녹색)이 없기 때문에 발생하는 적록 색맹의 한 형태입니다. 듀테라노프는 2~3가지 색상만 구별할 수 있는 반면, 정상적인 시력을 가진 사람은 7가지 색상을 구분할 수 있습니다. 인간 대상의 이러한 유형의 색맹에 대한 조사에서는 색맹과 성별을 교차하는 양방향 표가 생성되었습니다.\nload(“../data/Deuteranopia.RData”) 중수로맹 __\n남성 여성 Deute 19 2 NonDeute 1981 1998\n성별과 색맹 발생 사이에 관계가 있는지 어떻게 테스트할 수 있나요? 우리는 두 개의 독립적인 이항식(성별과 색맹에 대한 하나)을 사용하여 null 모델을 가정합니다. 이 모델에서 우리는 모든 셀의 다항 확률을 추정할 수 있으며 관찰된 개수와 예상 개수를 비교할 수 있습니다. 이는 R의 chisq.test 함수를 통해 수행됩니다.\nchisq.test(Deuteranopia)__\nYates의 연속성 수정을 사용한 Pearson의 카이제곱 테스트\n데이터: 중맹 X-제곱 = 12.255, df = 1, p-값 = 0.0004641\n작은 p 값은 널 모델 하에서 매우 작은 확률로 그러한 테이블을 볼 것으로 예상해야 함을 알려줍니다. 즉, 여성과 남성의 중수소 색맹 비율이 동일한 경우입니다.\n섹션 10.3.2에서 Fisher의 정확한 테스트(초기하 테스트라고도 함)라는 이러한 유형의 데이터에 대한 또 다른 테스트를 볼 수 있습니다. 이 테스트는 유의하게 발현된 유전자 목록에서 특정 유형의 유전자가 과도하게 나타나는지 테스트하는 데 널리 사용됩니다.\n\n\n4.7.1 2.7.2 특수 다항식: 하디-와인버그 평형\n여기서 우리는 두 대립유전자 M과 N을 결합하여 생성된 세 가지 가능한 수준을 가진 다항식의 사용을 강조합니다. 모집단에서 대립유전자 M의 전체 빈도가 \\(p\\)이고 N의 빈도가 \\(q = 1-p\\)라고 가정합니다. Hardy-Weinberg 모델은 유전자형에서 두 대립 유전자의 빈도가 독립된 경우, 소위 Hardy-Weinberg 평형(HWE)인 경우 \\(p\\)와 \\(q\\) 사이의 관계를 조사합니다. 이는 성별 간에 대립 유전자가 균등하게 분포되어 있는 대규모 집단에서 무작위 교배가 있는 경우에 해당됩니다. 세 가지 유전자형의 확률은 다음과 같습니다.\n\\[ p_{}=p2,p_{}=q2,p_{}=2pq \\]\n우리는 유전자형 MM, MN, NN에 대한 빈도 \\((n_{},,n_{},,n_{})\\)와 총 수 \\(S=n_{}+ n_{}+n_{}\\)만을 관찰합니다. 다항식을 사용하여 가능성, 즉 범주의 확률이 방정식 2.5에 의해 주어질 때 관찰된 데이터의 확률을 쓸 수 있습니다.\n\\[ P(n_{},,n_{},,n_{};|;p) = {S n_{},n_{},n_{}} (p2){n_{}} ,, (2pq)^{n_{}} ,, (q2){n_{}}, \\]\nHWE 하의 로그 우도\n\\[ L(p)=n_{}(p^2)+n_{} (2pq)+n_{}(q^2). \\]\n로그 우도를 최대화하는 \\(p\\) 값은 다음과 같습니다.\n\\[ p = . \\]\n그 증거는 (Rice 2006, chap. 8, 섹션 5)을 참조하세요. 주어진 데이터 \\((n_{},,n_{},,n_{})\\)에서 로그 우도 \\(L\\)은 단 하나의 매개변수 \\(p\\)의 함수입니다. 그림 2.11은 다음 코드에서 계산된 Mourant 데이터7의 216번째 행에 대한 \\(p\\)의 다양한 값에 대한 로그 우도 함수를 보여줍니다.\n7 이는 R 패키지 HardyWeinberg를 통해 제공되는 Mourant, Kopec 및 Domaniewska-Sobczak(1976)의 혈액형 대립유전자의 유전자형 빈도 데이터입니다.\nlibrary(“HardyWeinberg”) data(“Mourant”) Mourant[214:216,]__\n인구 국가 합계 MM MN NN 214 오세아니아 미크로네시아 962 228 436 298 215 오세아니아 미크로네시아 678 36 229 413 216 오세아니아 타히티 580 188 296 96\nnMM = Mourant\\(MM[216] nMN = Mourant\\)MN[216] nNN = Mourant$NN[216] loglik = function(p, q = 1 - p) { 2 * nMM * log(p) + nMN * log(2pq) + 2 * nNN * log(q) } xv = seq(0.01, 0.99, by = 0.01) yv = loglik(xv) plot(x = xv, y = yv, type = “l”, lwd = 2, xlab = “p”, ylab = “log-likelihood”) imax = which.max(yv) abline(v = xv[imax], h = yv[imax], lwd = 1.5, col = “blue”) abline(h = yv[imax], lwd = 1.5, col = “purple”)__\n\nFigure 2.11: 타히티 데이터에 대한 로그 우도 플롯.\n다항식의 확률에 대한 최대 우도 추정치는 이항의 경우와 마찬가지로 관측된 빈도를 사용하여 얻어지지만 추정치는 세 가지 확률 간의 관계를 고려해야 합니다. HardyWeinberg 패키지의 af 함수를 사용하여 \\({}\\), \\({}\\) 및 \\(_{}\\)을 계산할 수 있습니다.\nphat = af(c(nMM, nMN, nNN)) phat __\n0.5793103\npMM = phat^2 qhat = 1 - phat __\nHardy-Weinberg 평형 하에서 기대되는 값은 다음과 같습니다.\npHW = c(MM = phat^2, MN = 2phatqhat, NN = qhat^2) sum(c(nMM, nMN, nNN)) * pHW __\nMM.A MN.A NN.A 194.6483 282.7034 102.6483\n이는 위에서 관찰된 값과 비교할 수 있습니다. 관측된 값과 매우 유사하다는 것을 알 수 있습니다. 시뮬레이션이나 위와 같이 \\(^2\\) 테스트를 수행하여 관찰된 값이 Hardy-Weinberg 모델을 거부할 수 있는지 여부를 추가로 테스트할 수 있습니다. Hardy-Weinberg의 적합도에 대한 시각적 평가는 de Finetti(Finetti 1926; Cannings and Edwards 1968)에 의해 설계되었습니다. 이는 각 대립유전자의 비율로 좌표가 제공되는 지점에 모든 샘플을 배치합니다.\n\n4.7.1.1 Hardy-Weinberg 평형과의 시각적 비교\n우리는 ‘HWTernaryPlot’ 함수를 사용하여 데이터를 표시하고 이를 Hardy-Weinberg 평형과 그래픽으로 비교합니다.\npops = c(1, 69, 128, 148, 192) genotypeFrequencies = as.matrix(Mourant[, c(“MM”, “MN”, “NN”)]) HWTernaryPlot(genotypeFrequencies[pops, ], markerlab = Mourant$Country[pops], alpha = 0.0001, curvecols = c(“red”, rep(“purple”, 4)), mcex = 0.75, vertex.cex = 1)__\n[](02-chap_files/Figure- html/fig-HWtern-1.png “그림 2.12: 이 de Finetti 플롯은 주파수를 삼각형의 각 모서리에 대한 가중치로 사용하여 세 가지 유전자형의 중심점을 보여줍니다. Hardy-Weinberg 모델은 빨간색 곡선입니다. 수용 영역은 두 보라색 선 사이에 있습니다. 미국이 HW 균형 상태에서 가장 멀리 떨어져 있음을 알 수 있습니다.”\n그림 2.12: 이 de Finetti 플롯은 빈도를 삼각형의 각 모서리에 대한 가중치로 사용하여 세 가지 유전자형의 중심점을 보여줍니다. Hardy-Weinberg 모델은 빨간색 곡선이고 수용 영역은 두 개의 보라색 선 사이에 있습니다. 우리는 미국이 HW 균형 상태에서 가장 멀리 떨어져 있음을 알 수 있습니다.\n__\n질문 2.16\n위 코드와 같이 삼원 플롯을 만든 다음 여기에 다른 데이터 포인트를 추가합니다. 무엇을 알 수 있습니까? ‘HWChisq’ 기능을 사용하여 토론을 백업할 수 있습니다.\n__\n해결책\n__\nHWTernaryPlot(genotypeFrequency[-pops, ], newframe = FALSE, alpha = 0.0001, cex = 0.5)__\n__\n질문 2.17\n각 유전자형에 대해 동일한 비율을 유지하면서 모든 총 빈도를 50으로 나누고 삼원 플롯을 다시 만듭니다.\n\n포인트는 어떻게 되나요?\n신뢰 영역은 어떻게 되며 그 이유는 무엇입니까?\n\n__\n해결책\n__\nnewgf = round(genotypeFrequencies / 50) HWTernaryPlot(newgf[pops, ], markerlab = Mourant$Country[pops], curvecols = c(“red”, rep(“purple”, 4)), alpha = 0.0001, mcex = 0.75, vertex.cex = 1)__\n\n\n\n4.7.2 2.7.3 여러 다항식 연결: 시퀀스 모티프 및 로고\nKozak Motif는 코딩 영역의 시작 코돈 ATG 근처에서 발생하는 시퀀스입니다. 시작 코돈 자체는 항상 고정된 철자를 가지고 있지만 왼쪽 5번째 위치에는 문자가 동일할 가능성이 전혀 없는 뉴클레오티드 패턴이 있습니다.\n우리는 모든 포지션에서 다항 확률을 제공하는 포지션 가중치 매트릭스(PWM) 또는 포지션별 점수 매트릭스(PSSM)를 제공하여 이를 요약합니다. 이는 시퀀스 로고(그림 2.13)를 통해 그래픽으로 인코딩됩니다.\nlibrary(“seqLogo”) load(“../data/kozak.RData”) kozak __\n[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] A 0.33 0.25 0.4 0.15 0.20 1 0 0 0.05 C 0.12 0.25 0.1 0.40 0.40 0 0 0 0.05 G 0.33 0.25 0.4 0.20 0.25 0 0 1 0.90 T 0.22 0.25 0.1 0.25 0.15 0 1 0 0.00\npwm = makePWM(kozak) seqLogo(pwm, ic.scale = FALSE)__\n\n그림 2.13: 다음은 Kozak 모티프를 모델링하는 데 사용되는 위치 종속 다항식에 대한 시퀀스 로고라는 다이어그램입니다. 로그 스케일의 각 위치에서 변동량을 코드화합니다. 큰 글자는 어떤 뉴클레오티드가 발생하는지에 대한 불확실성이 없는 위치를 나타냅니다.\n마지막 섹션에서 우리는 다항 분포의 다양한 “상자”가 동일한 확률을 갖는 경우가 거의 없음을 확인했습니다. 즉, 매개변수 \\(p_1, p_2, …\\)는 모델링되는 항목에 따라 달라지는 경우가 많습니다. 빈도가 같지 않은 다항식의 예로는 20개의 서로 다른 아미노산, 혈액형 및 머리 색깔이 있습니다.\n범주형 변수가 여러 개 있는 경우 독립적인 경우가 거의 없습니다(성별 및 색맹, 머리카락 및 눈 색깔 등). 나중에 9장에서 분할표의 다변량 분해를 사용하여 이러한 종속성의 패턴을 탐색할 수 있음을 살펴보겠습니다. 여기에서는 범주형 변수 사이의 종속성의 중요한 특수 사례, 즉 범주형 변수의 시퀀스(또는 “체인”)를 따라 발생하는 종속성(예: 시간이 지남에 따라 또는 생체 고분자를 따라 발생)을 살펴보겠습니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "02-chap.html#순차적-종속성-모델링-마르코프-체인",
    "href": "02-chap.html#순차적-종속성-모델링-마르코프-체인",
    "title": "4  2.1 이 장의 목표",
    "section": "4.8 2.8 순차적 종속성 모델링: 마르코프 체인",
    "text": "4.8 2.8 순차적 종속성 모델링: 마르코프 체인\n내일의 날씨를 예측하고 싶다면 오늘의 날씨와 같을 가능성이 가장 높다는 것이 합리적으로 좋은 추측이며 다양한 종류의 가능한 변화에 대한 확률을 기술할 수도 있습니다^[동일한 추론을 반대로 적용할 수도 있습니다. 우리는 오늘로부터 어제의 날씨를 “예측”할 수 있습니다. 이 일기 예보 방법은 마르코프 가정의 한 예입니다. 내일에 대한 예측은 오늘의 상황에만 의존하지만 어제 또는 3주 전의 상태에는 의존하지 않습니다(우리가 잠재적으로 사용할 수 있는 모든 정보는 오늘 날씨에는 이미 포함되어 있습니다.) 날씨 예는 또한 그러한 가정이 반드시 정확할 필요는 없지만 충분히 좋은 가정이어야 한다는 점을 강조합니다. 이 가정을 이전 \\(k\\)일에 대한 종속성으로 확장하는 것은 매우 간단합니다. 여기서 \\(k\\)는 유한하고 너무 크지 않기를 바랍니다. 마르코프 가정의 핵심은 프로세스에 유한한 “기억”이 있으므로 예측은 유한한 시간 동안만 되돌아보면 된다는 것입니다.\n시간적 순서 대신에 이를 생물학적 순서에 적용할 수도 있습니다. DNA에서 우리는 다이그램(예: [CG, CA, CC] 및 [CT])이라고 하는 뉴클레오티드 쌍이 똑같이 빈번하지 않도록 특정 패턴의 연속을 볼 수 있습니다. 예를 들어, 게놈의 일부에서는 독립 상태에서 예상되는 것보다 [CA]의 사례가 더 자주 나타납니다.\n\\[ P() P() , P(). \\]\n우리는 이 종속성을 Markov 체인으로 시퀀스에서 모델링합니다.\n\\[ P() = P() = P() = P(…) = P() , P(), \\]\n여기서 N은 임의의 뉴클레오티드를 나타내고 \\(P()\\)는 “이전 염기가 \\(\\)인 경우 \\(\\)의 확률”을 나타냅니다. 그림 2.14는 이러한 전환을 그래프로 도식적으로 나타낸 것입니다.\n\n그림 2.14: 4-상태 마르코프 체인의 시각화. 가능한 각 다이그램(예: CA)의 확률은 해당 노드 사이의 에지 가중치로 제공됩니다. 예를 들어 CA 확률은 가장자리 C$ o$ A로 제공됩니다. 11장에서 R 패키지를 사용하여 이러한 유형의 네트워크 그래프를 그리는 방법을 살펴보겠습니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "02-chap.html#베이지안-사고",
    "href": "02-chap.html#베이지안-사고",
    "title": "4  2.1 이 장의 목표",
    "section": "4.9 2.9 베이지안 사고",
    "text": "4.9 2.9 베이지안 사고\n\n그림 2.15: 아래로 내려가는 거북이. 분포 매개변수의 불확실성에 대한 베이지안 모델링은 불확실성이 무작위 변수로 모델링될 수 있는 매개변수에 따라 분포가 달라질 수 있는 무작위 변수를 사용하여 수행됩니다. 이를 계층적 모델이라고 합니다.\n지금까지 우리는 모델의 매개변수와 모델이 사용하는 분포, 즉 가능한 다른 결과의 확률이 장기적인 빈도를 나타내는 고전적인 접근 방식을 따랐습니다. 매개변수는 적어도 개념적으로는 명확하고 알 수 있으며 고정되어 있습니다. 우리는 그것들을 알지 못할 수도 있으므로, 우리는 현재의 데이터로부터 그것들을 추정합니다. 그러나 이러한 접근 방식은 우리가 이미 가지고 있을 수 있는 정보를 고려하지 않으며, 이로 인해 매개변수에 대해 알려주거나 특정 매개변수 값이나 그 조합이 다른 것보다 더 가능성이 높아질 수 있습니다. 심지어 우리가 현재 데이터 세트를 보기 전이라도 말이죠. 이를 위해서는 확률 모델(예: 분포)을 사용하여 매개변수에 대한 사전 지식8을 표현하고 현재 데이터를 사용하여 그러한 지식을 _업데이트_하는(예: 분포를 이동하거나 더 좁게 만드는 등) 다른 접근 방식이 필요합니다. 이러한 접근법은 베이지안 패러다임(그림 2.15)에 의해 제공됩니다.\n8 어떤 사람들은 “우리의 믿음(들)”이라고 말하기를 좋아합니다.\n베이지안 패러다임은 일부 데이터를 수집하고 관찰하기 전 및 _후_에 우리의 지식을 모델링하기 위해 사전 및 사후 분포를 사용하는 실용적인 접근 방식입니다. 무한히 반복될 수 있습니다. 한 라운드의 데이터 생성 후 사후가 다음 라운드의 사전으로 사용될 수 있습니다. 따라서 다양한 소스의 정보를 통합하거나 결합하는 데 특히 유용합니다.\n동일한 아이디어가 가설 테스트에도 적용될 수 있습니다. 여기서 데이터를 사용하여 가설 \\(H\\)이라고 부를 수 있는 특정 진술이 사실인지 여부를 결정합니다. 여기서 “매개변수”는 \\(H\\)가 참일 확률이며, \\(P(H)\\)9로 쓰여진 사전 확률의 형태로 사전 지식을 공식화할 수 있습니다. 데이터를 확인한 후에는 사후 확률을 갖게 됩니다. \\(D\\)를 본 경우 \\(H\\)의 확률인 \\(P(H,|,D)\\)로 씁니다. 이는 데이터 \\(D\\)에 따라 \\(P(H)\\)보다 높거나 낮을 수 있습니다.\n9 소위 빈도주의자에게는 그러한 가능성이 존재하지 않습니다. 그들의 관점은 비록 진실은 알려지지 않았지만 실제로 가설은 참이거나 거짓이라는 것입니다. “70% 진실”이라고 부르는 것은 의미가 없습니다.\n\n4.9.1 2.9.1 예: 일배체형 빈도\n수학적 형식을 최소한으로 유지하기 위해 Y 염색체의 결합된 서명(일배체형)을 사용하는 법의학의 예부터 시작합니다.\n일배체형(haplotype)은 염색체에서 공간적으로 인접한 대립유전자(DNA 서열 변종)의 모음으로, 일반적으로 함께 유전되므로(재조합으로 인해 분리되지 않는 경향이 있음) 유전적으로 연결됩니다. 이 경우 우리는 Y 염색체의 연결된 변이를 살펴봅니다.\n먼저 일배체형 빈도 분석의 동기를 살펴본 다음 가능성에 대한 아이디어를 다시 살펴보겠습니다. 그런 다음, 알려지지 않은 매개변수를 난수 자체로 생각하고 사전 분포를 사용하여 불확실성을 모델링하는 방법을 설명하겠습니다. 그런 다음 관찰된 새로운 데이터를 확률 분포에 통합하고 매개변수에 대한 사후 신뢰도를 계산하는 방법을 살펴보겠습니다.\n\n그림 2.16: DNA의 짧은 직렬 반복(STR)은 두 개 이상의 뉴클레오티드 패턴이 반복되고 반복된 서열이 서로 직접 인접할 때 발생합니다. STR은 미세위성이라고도 알려져 있습니다. 패턴의 길이는 2~13개 뉴클레오티드이며, 반복 횟수는 개인마다 매우 다양합니다. STR 번호는 유전적 서명으로 사용될 수 있습니다.\n\n그림 2.17: 인간 Y 염색체의 짧은 직렬 반복(STR) 위치. 출처 : https://strbase.nist.gov/ystrpos1.htm\n\n그림 2.18: FBI가 사용하는 데이터베이스에서 Y STR 일배체형 조회.\n우리는 다양한 짧은 탠덤 반복(STR) 세트로 구성된 특정 Y-일배체형의 빈도에 관심이 있습니다. DNA 법의학에 사용되는 특정 위치의 STR 번호 조합은 특정 위치의 반복 횟수로 표시됩니다. 다음은 이러한 STR 일배체형 테이블의 짧은 발췌입니다.\nhaplo6 = read.table(“../data/haplotype6.txt”, header = TRUE) haplo6 __\n개별 DYS19 DXYS156Y DYS389m DYS389n DYS389p 1 H1 14 12 4 12 3 2 H3 15 13 4 13 3 3 H4 15 11 5 11 3 4 H5 17 13 4 11 3 5 H7 13 12 5 12 3 6 H8 16 11 5 12 3\n표에 따르면 일배체형 H1은 ‘DYS19’ 위치에 14개의 반복이 있고 ‘DXYS156Y’ 위치에 12개의 반복이 있습니다. \\(n=300\\)명의 남성을 일배체형으로 지정하여 관심 집단에서 특정 일배체형의 기본 비율 \\(p\\)를 찾고 싶다고 가정해 보겠습니다. 그 중 \\(y=40\\)에서 H1을 찾았다고 가정해 보겠습니다. 우리는 이를 모델링하기 위해 이항 분포 \\(B(n,p)\\)를 사용할 것이며, \\(p\\)는 알 수 없습니다.\n이러한 Y-STR 프로필을 사용하여 생성된 일배체형은 동일한 가부장적 혈통의 남성 간에 공유됩니다. 따라서 두 명의 다른 남성이 동일한 프로필을 공유하는 것이 가능합니다.\n\n\n4.9.2 2.9.2 이항식에 대한 베이지안 패러다임의 시뮬레이션 연구\n분포\n매개변수 \\(p\\)가 하나의 단일 값(예: 최대 가능성 추정치 40/300)을 갖는다고 가정하는 대신 베이지안 접근 방식을 사용하면 이를 통계 분포에서 가져온 것으로 볼 수 있습니다. 분포는 매개변수 \\(p\\)의 가능한 값에 대한 우리의 믿음을 표현합니다. 원칙적으로 \\(p\\)에 대해 가능한 값이 허용되는 분포를 사용할 수 있습니다. 여기에서는 비율이나 확률을 표현하고 0과 1 사이의 값을 갖는 매개변수를 보고 있으므로 _베타 분포_를 사용하는 것이 편리합니다. 밀도 공식은 다음과 같습니다.\n\\[ f_{,}(x) = (,)=. \\]\n그림 2.19에서 이 함수가 두 매개변수 \\(\\) 및 \\(\\)에 어떻게 의존하는지 확인할 수 있습니다. 이는 매우 유연한 분포 계열을 만듭니다(다양한 상황에 “적합”할 수 있음). 그리고 그것은 훌륭한 수학적 특성을 가지고 있습니다. 베타 모양의 \\(p\\)에 대한 사전 믿음으로 시작하고 \\(n\\) 이항 시행의 데이터 세트를 관찰한 다음 믿음을 업데이트하면 \\(p\\)에 대한 사후 분포도 업데이트된 매개변수에도 불구하고 베타 분포를 갖게 됩니다. 이것은 수학적 사실입니다. 여기서는 증명하지 않겠지만 시뮬레이션을 통해 보여드리겠습니다.\n\n그림 2.19: \\(,20,50\\) 및 \\(,60,150\\)을 사용한 베타 배포판. 이를 이항 실험의 성공 확률에 대한 사전으로 사용할 수 있습니다. 이 세 가지 분포는 동일한 평균(\\(\\))을 갖지만 평균 주위의 농도는 다릅니다.\n\n\n4.9.3 2.9.3 \\(Y\\)의 분포\n주어진 \\(p\\) 선택에 대해 방정식 2.3을 통해 \\(Y\\)의 분포가 무엇인지 알 수 있습니다. 그러나 \\(p\\) 자체도 일부 분포에 따라 달라지면 \\(Y\\)의 분포는 무엇입니까? 우리는 이것을 \\(Y\\)의 한계 분포라고 부릅니다. 그것을 시뮬레이션해 봅시다. 먼저 100000 \\(p\\)의 무작위 샘플 rp를 생성합니다. 그런 다음 그림 2.20에 표시된 것처럼 각각에 대해 \\(Y\\)의 무작위 샘플을 생성합니다. 아래 코드에서는 데모를 위해 사전에 매개변수 50과 350을 사용합니다. 그러한 사전은 이미 매우 유익하며(“정점”), 예를 들어 이전 연구를 기반으로 한 우리의 믿음을 반영할 수 있습니다. 질문 2.20에서는 “더 부드러운”(덜 유익한) 사전을 시험해 볼 기회가 있습니다. 우리는 다시 vapply를 사용하여 이름이 지정되지 않은 x 함수를 rp의 모든 요소에 적용하여 결과적으로 동일한 길이의 또 다른 벡터 y를 얻습니다.\nrp = rbeta(100000, 50, 350) y = vapply(rp, function(x) rbinom(1, prob = x, size = 300), integer(1)) hist(y, breaks = 50, col = “orange”, main = ““, xlab =”“)__\n\n그림 2.20: \\(Y\\)의 한계 분포.\n__\n질문 2.18\nR의 벡터화 기능을 사용하고 rbinom(length(rp), rp, size = 300)을 작성하여 위의 코드 청크에서와 동일한 결과를 얻을 수 있는지 확인하십시오.\n__\n해결책\n__\nset.seed(0xbebe) y1 = vapply(rp, function(x) rbinom(1, prob = x, 크기 = 300), 정수(1)) set.seed(0xbebe) y2 = rbinom(length(rp), rp, size = 300) stopifnot(identical(y1, y2))__\n\n\n4.9.4 2.9.4 \\(Y=40\\)을 만족하는 모든 \\(p\\)의 히스토그램: 사후\n분포\n이제 \\(Y\\)가 40인 결과를 조건으로 \\(p\\)의 사후 분포를 계산해 보겠습니다. 이를 이론적인 사후 분포 ’densPostTheory’와 비교합니다. 자세한 내용은 아래에서 확인하세요. 결과는 그림 2.21에 나와 있습니다.\npPostEmp = rp[ y == 40 ] hist(pPostEmp, breaks = 40, col = “chartreuse4”, main = ““, probability = TRUE, xlab =”posterior p”)\np_seq = seq(0, 1, by = 0.001) densPostTheory = dbeta(p_seq, 50 + 40, 350 + 260) lines(p_seq, densPostTheory, type = “l”, lwd = 3)__\n\n그림 2.21: \\(Y=40\\)인 분포 값을 선택하면 \\(p\\)의 사후 분포가 제공됩니다. 히스토그램(녹색)은 사후 분포에 대한 시뮬레이션 값, 즉 베타 분포의 밀도와 이론적인 매개변수의 선을 보여줍니다.\n또한 위에서 계산된 두 분포의 평균을 확인하여 유효 숫자 4자리에 가깝다는 것을 확인할 수 있습니다.\n평균(pPostEmp)__\n[1] 0.128726\ndp = p_seq[2] - p_seq[1] sum(p_seq * densPostTheory * dp)__\n[1] 0.1285714\n이론적 밀도 ’densPostTheory’의 평균을 근사화하기 위해 위에서 문자 그대로 적분을 계산했습니다.\n\\[ _0^1 p , f(p) , dp \\]\n수치 적분, 즉 적분에 대한 ‘합’을 사용합니다. 이는 항상 편리한(또는 실행 가능한) 것은 아닙니다. 특히 모델에 단일 스칼라 매개변수 \\(p\\)가 포함되지 않고 많은 매개변수가 있어 고차원 매개변수 벡터와 고차원 적분을 다루는 경우에는 더욱 그렇습니다. 적분을 분석적으로 계산할 수 없는 경우 몬테카를로 적분을 사용할 수 있습니다. 위 코드에서 몬테 카를로 통합의 아주 간단한 사례를 이미 보셨는데, 여기서 우리는 pPostEmp로 사후를 샘플링하고 R의 mean 함수를 호출하여 사후 평균을 계산하기 위해 통합을 수행했습니다. 이 경우 대체 몬테카를로 알고리즘은 올바른 매개변수와 함께 ’rbeta’ 함수를 직접 사용하여 사후 샘플을 생성하는 것입니다.\npPostMC = rbeta(n = 100000, 90, 610) mean(pPostMC)__\n[1] 0.1285718\n분위수-분위수 플롯(QQ-plot, 그림 2.22)을 사용하여 약간 다른 방식으로 생성된 Monte Carlo 샘플 ‘pPostMC’와 ’pPostEmp’ 간의 일치성을 확인할 수 있습니다.\nqqplot(pPostMC, pPostEmp, type = “l”, asp = 1) abline(a = 0, b = 1, col = “blue”)__\n[](02-chap_files/Figure- html/fig-qqplotbeta-1-1.png “그림 2.22: 이론적 분포와 시뮬레이션 샘플 pPostEmp에서 얻은 Monte Carlo 샘플 pPostMC의 Quantile-Quantile (QQ) 플롯. 다음 중 하나를 유사하게 비교할 수도 있습니다. 이 두 가지 분포는 이론적 분포 함수 pbeta(., 90, 610)에 적용됩니다. 곡선이 y=x 선에 있으면 꼬리 부분에 임의의 차이가 있음을 나타냅니다.\n그림 2.22: 이론적 분포와 시뮬레이션 샘플 ’pPostEmp’의 Monte Carlo 샘플 ’pPostMC’의 QQ(Quantile-Quantile) 플롯. 또한 이 두 분포 중 하나를 이론적 분포 함수 ’pbeta(., 90, 610)’와 유사하게 비교할 수도 있습니다. 곡선이 \\(y=x\\) 선에 있으면 이는 좋은 일치를 나타냅니다. 꼬리에는 약간의 무작위 차이가 있습니다.\n__\n질문 2.19\n’pPostEmp’로 이어지는 시뮬레이션과 ’pPostMC’로 이어지는 Monte Carlo 시뮬레이션의 차이점은 무엇인가요?\n\n\n4.9.5 2.9.5 사후 분포도 베타 분포입니다.\n이제 우리는 사후 분포도 베타임을 확인했습니다. 우리의 경우 해당 매개변수 \\(\\) 및 \\(\\)은 이전 매개변수 \\(\\), \\(\\)를 관찰된 성공 \\(y=40\\) 및 관찰된 실패 \\(n-y=260\\)와 합산하여 얻은 것입니다.\n\\[ (90,, 610)=(+y,+(n-y)). \\]\n우리는 사후 분포에 의해 주어진 불확실성을 가지고 \\(p\\)에 대해 우리가 할 수 있는 최선의 추정치를 제공하기 위해 그것을 사용할 수 있습니다.\n10 사후 분포를 최대화하는 값을 최선의 추정치로 취할 수 있습니다. 이를 MAP 추정치라고 하며, 이 경우에는 \\(=\\)가 됩니다.\n\n\n4.9.6 2.9.6 두 번째 데이터 계열이 있다고 가정합니다.\n이전 데이터를 본 후 이제 새로운 이전 데이터인 \\((90, 610)\\)를 갖게 되었습니다. \\(n=150\\)개의 관찰과 \\(y=25\\)개의 성공, 즉 125개의 실패로 새로운 데이터 세트를 수집한다고 가정합니다. 이제 \\(p\\)에 대한 최선의 추측은 무엇입니까?\n이전과 동일한 추론을 사용하면 새로운 사후값은 \\((90+25=115,, 610+125=735)\\)가 됩니다. 이 분포의 평균은 \\(=\\)이므로 \\(p\\)의 한 추정치는 0.135가 됩니다. 최대 사후(MAP) 추정치는 \\((115, 735)\\) 모드, 즉 \\(\\)입니다. 이를 수치로 확인해 보겠습니다.\ndensPost2 = dbeta(p_seq, 115, 735) mcPost2 = rbeta(1e6, 115, 735) sum(p_seq * densPost2 * dp) # mean, by numeric integration __\n[1] 0.1352941\n평균(mcPost2) # MC의 의미 __\n[1] 0.1352655\np_seq[which.max(densPost2)] # MAP 추정 __\n[1] 0.134\n__\n질문 2.20\n원래 사전을 더 부드러운 사전(덜 정점)으로 바꾸는 모든 계산을 다시 실행합니다. 즉, 사전 정보를 덜 사용한다는 의미입니다. 예를 들어 균일 분포인 Beta(1,1)을 시도해 보세요. 이것이 최종 결과를 얼마나 변화시키나요?\n일반적으로 사전 분포는 매우 정점에 있는 경우를 제외하고 사후 분포를 실질적으로 변경하는 경우가 거의 없습니다. 처음에 우리가 이미 무엇을 기대할지 확신하고 있었다면 이런 경우가 될 것입니다. 사전이 영향을 미치는 또 다른 경우는 데이터가 거의 없는 경우입니다.\n가장 좋은 상황은 사전을 압도할 만큼 충분한 데이터를 확보하여 선택이 최종 결과에 큰 영향을 미치지 않도록 하는 것입니다.\n\n\n4.9.7 2.9.7 비율 매개변수에 대한 신뢰 진술\n이제 데이터를 바탕으로 \\(p\\) 비율이 실제로 무엇인지 결론을 내릴 차례입니다. 한 가지 요약은 신뢰 구간의 베이지안 아날로그인 사후 신뢰 구간입니다. 사후 분포의 2.5번째 및 97.5번째 백분위수를 취할 수 있습니다: \\(P(q_{2.5%} p q_{97.5%})=0.95\\).\n분위수(mcPost2, c(0.025, 0.975))__\n2.5% 97.5% 0.1131080 0.1590221\n[](imgs/DESeq2-Prediction-Interval.png “그림 2.23: @LoveDESeq2의 예는 더 높은 분산으로 인해 녹색 및 보라색 유전자와 이전 (검은색 실선)에 대한 우도 (1로 적분되도록 조정된 실선) 및 사후 (점선)의 플롯을 보여줍니다. 보라색 유전자의 가능성은 더 넓고 피크가 적으며(정보가 적음을 나타냄), 사전은 녹색 유전자보다 후방에 더 많은 영향을 미칩니다. 최대 녹색 후방의 더 강한 곡률은 MAP 로그 접기 변화(LFC) 추정(수평 오류 막대)에 대해 보고된 표준 오류가 더 작은 것으로 해석됩니다.\n그림 2.23: Love, Huber 및 Anders(2014)의 예는 녹색 및 보라색 유전자와 사전(검은색 실선)에 대한 우도(1로 적분되도록 스케일링된 실선) 및 사후(점선)의 플롯을 보여줍니다. 보라색 유전자의 더 높은 분산으로 인해 우도는 더 넓고 정점이 덜 나타납니다(더 적은 정보를 나타냄). 사전은 녹색 유전자보다 사후에 더 많은 영향을 미칩니다. 최대 녹색 후방의 더 강한 곡률은 MAP 로그 접기 변경(LFC) 추정치(수평 오류 막대)에 대해 보고된 표준 오류가 더 작은 것으로 해석됩니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "02-chap.html#예-게놈에서-뉴클레오티드-패턴의-발생",
    "href": "02-chap.html#예-게놈에서-뉴클레오티드-패턴의-발생",
    "title": "4  2.1 이 장의 목표",
    "section": "4.10 2.10 예: 게놈에서 뉴클레오티드 패턴의 발생",
    "text": "4.10 2.10 예: 게놈에서 뉴클레오티드 패턴의 발생\n지금까지 우리가 본 예는 이산 개수와 범주형 데이터의 분포에 집중되어 있습니다. 준연속적인 거리 분포의 예를 살펴보겠습니다. 게놈 서열의 특정 모티프 인스턴스 간 거리 분포에 대한 이 사례 연구를 통해 Bioconductor에서 특정 게놈 서열 조작을 탐색할 수도 있습니다.\nBiostrings 패키지는 서열 데이터 작업을 위한 도구를 제공합니다. 필수 데이터 구조 또는 R에서 알려진 _클래스_는 DNAString 및 _DNAStringSet_입니다. 이를 통해 우리는 하나 또는 여러 개의 DNA 서열을 효율적으로 사용할 수 있습니다.\nBiostrings 패키지에는 아미노산 서열과 보다 일반적인 생물학에서 영감을 받은 서열을 나타내는 추가 클래스도 포함되어 있습니다.\nlibrary(“바이오스트링”)__\n__\n질문 2.21\n튜토리얼 비네팅을 탐색하여 Biostrings 패키지에서 제공되는 유용한 데이터와 기능 중 일부를 살펴보세요.\n__\n해결책\n__\n첫 번째 줄은 유전 코드 정보를 인쇄하고, 두 번째 줄은 IUPAC 뉴클레오티드 모호성 코드를 반환합니다. 세 번째 줄에는 Biostrings 패키지에서 사용할 수 있는 모든 비네팅이 나열되고, 네 번째 줄에는 특정 비네팅이 하나 표시됩니다.\nGENETIC_CODE IUPAC_CODE_MAP vignette(package = “Biostrings”) vignette(“BiostringsQuickOverview”, package = “Biostrings”)__\n이 마지막 명령은 브라우저 창에서 설명서에 액세스할 수 있는 목록을 엽니다11. BSgenome 패키지는 많은 게놈에 대한 접근을 제공하며, 전체 게놈 서열이 포함된 데이터 패키지의 이름에 다음을 입력하여 접근할 수 있습니다.\n11 Vignettes는 예제와 사례 연구가 포함된 패키지 매뉴얼입니다.\nlibrary(“BSgenome”) ag = available.genomes() 길이(ag)__\n[1] 113\nag[1:2]__\n[1] “BSgenome.Alyrata.JGI.v1” [2] “BSgenome.Amellifera.BeeBase.assemble4”\n우리는 E.coli의 게놈에서 ‘AGGAGGT’ 모티프12의 발생을 조사할 것입니다. 우리는 특정 균주인 Escherichia coli str의 게놈 서열을 사용합니다. K12 substr.DH10B13, NCBI 접근번호는 NC_010473입니다.\n12 이는 박테리아에서 단백질 합성을 시작하는 데 도움을 주는 Shine-Dalgarno 모티브입니다.\n13 실험실의 일꾼으로 알려져 있으며 실험에 자주 사용됩니다.\nlibrary(“BSgenome.Ecoli.NCBI.20080805”) Ecoli ShineDalgarno = “AGGAGGT” ecoli = Ecoli$NC_010473 __\ncountPattern 함수를 사용하여 너비가 50000인 창에서 패턴의 발생 횟수를 계산할 수 있습니다.\nwindow = 50000 starts = seq(1, length(ecoli) - window, by = window) ends = starts + window - 1 numMatches = vapply(seq_along(starts), function(i) { countPattern(shineDalgarno, ecoli[starts[i]:ends[i]], max.mismatch = 0) }, numeric(1)) table(numMatches)__\n숫자 일치 항목 0 1 2 3 4 48 32 8 3 2\n__\n질문 2.22\n이 표는 어떤 분포에 적합할까요?\n__\n해결책\n__\n푸아송은 이러한 데이터에 대한 정량적 및 그래픽 평가(그림 2.24 참조)가 보여주듯이 좋은 후보입니다.\nlibrary(“vcd”) gf = goodfit(numMatches, “poisson”) summary(gf)__\n포아송 분포에 대한 적합도 검정\nX^2 df P(&gt; X^2) 우도비 4.134932 3 0.2472577\ndistplot(numMatches, type = “poisson”)__\n\n그림 2.24: ‘Ecoli$NC_010473’ 시퀀스에 따른 모티프 개수에 대한 포아송 모델 평가.\n‘matchPattern’ 함수를 사용하여 일치 항목을 검사할 수 있습니다.\nsdMatches = matchPattern(shineDalgarno, ecoli, max.mismatch = 0)__\nR 명령줄에 ’sdMatches’를 입력하면 이 개체에 대한 요약을 얻을 수 있습니다. 여기에는 원본 시퀀스에서 소위 views 집합으로 표시되는 모든 65개 패턴 일치 위치가 포함되어 있습니다. 이제 그들 사이의 거리는 얼마나 됩니까?\nbetweenmotifs = gaps(sdMatches)__\n따라서 이들은 실제로 66개의 보완적인 영역입니다. 이제 모티프 사이의 간격 크기 분포에 대한 모델을 찾아보겠습니다. 모티프가 무작위 위치에서 발생하는 경우 간격 길이는 지수 분포를 따를 것으로 예상됩니다. 아래 코드(출력은 그림 2.25에 표시됨)는 이 가정을 평가합니다. 지수 분포가 적합하다면 점은 대략 직선 위에 있어야 합니다. 지수 분포에는 하나의 매개변수인 비율이 있으며, 데이터의 추정치에 해당하는 기울기가 있는 선도 표시됩니다.\n14 지수가 여기에 딱 들어맞는다고 어떻게 추측할 수 있습니까? 시퀀스를 따라 독립적인 무작위 베르누이 발생이 있을 때마다 간격 길이는 기하급수적으로 늘어납니다. 방출 사이의 대기 시간도 기하급수적으로 분포되는 방사성 붕괴에 대해 잘 알고 계실 것입니다. 이 분포에 익숙하지 않다면 위키피디아에서 자세한 내용을 찾아보는 것이 좋습니다.\nlibrary(“Renext”) expplot(width(betweenmotifs), rate = 1/mean(width(betweenmotifs)), labels = “fit”)__\n\n그림 2.25: 모티프 간 간격의 지수 분포(검은색 선)에 대한 적합성 평가.\n__\n질문 2.23\n그림 2.25의 적합선에서 분포의 오른쪽 꼬리, 즉 가장 큰 값에 약간의 편차가 있는 것으로 보입니다. 이유가 무엇일까요?\n\n4.10.1 2.10.1 종속성의 경우 모델링\n섹션 2.8에서 보았듯이 뉴클레오티드 서열은 종종 의존적입니다. 주어진 위치에서 특정 뉴클레오티드를 발견할 확률은 주변 서열에 의존하는 경향이 있습니다. 여기서는 Markov 체인을 사용하여 종속성 모델링을 실습해 보겠습니다. 우리는 인간 게놈의 8번 염색체 영역을 살펴보고 CpG15 섬이라고 불리는 영역과 나머지 영역 간의 차이점을 발견하려고 노력할 것입니다.\n15 CpG는 5’-C-포스페이트-G-3’을 나타내고; 이는 C가 가닥을 따라 인산염을 통해 G에 연결되어 있음을 의미합니다(이것은 섹션 2.7의 C-G 염기쌍과 관련이 없습니다). CpG 디뉴클레오티드의 시토신은 메틸화되어 유전자 발현 수준을 변화시킬 수 있습니다. 이러한 유형의 유전자 조절은 후생유전학의 일부입니다. 더 많은 정보는 Wikipedia에 있습니다: CpG 사이트 및 epigenetics.\n우리는 섬의 시작점과 끝점이 게놈에서 어디에 있는지 알려주고 뉴클레오티드와 다이그램 ‘CG’, ‘CT’, ‘CA’, ’CC’의 빈도를 살펴보는 데이터(Irizarry, Wu 및 Feinberg(2009)가 생성한 데이터를 사용합니다. 따라서 우리는 뉴클레오티드 발생 사이에 종속성이 있는지 여부와 그렇다면 이를 모델링하는 방법을 물어볼 수 있습니다.\nlibrary(“BSgenome.Hsapiens.UCSC.hg19”) chr8 = Hsapiens$chr8 CpGtab = read.table(“../data/model-based-cpg-islands-hg19.txt”, 헤더 = TRUE) nrow(CpGtab)__\n[1] 65699\n헤드(CpGtab)__\n문자 시작 끝 길이 CpGcount GCcontent pctGC obsExp 1 chr10 93098 93818 721 32 403 0.559 0.572 2 chr10 94002 94165 164 12 97 0.591 0.841 3 chr10 94527 95302 776 65 538 0.693 0.702 4문자10 119652 120193 542 53 369 0.681 0.866 5문자10 122133 122621 489 51 339 0.693 0.880 6 문자10 180265 180720 456 32 256 0.561 0.893\nirCpG = with(dplyr::filter(CpGtab, chr == “chr8”), IRanges(start = start, end = end))__\n\n\n\n저희는 :: 연산자를 사용하여 dplyr 패키지에서 필터 함수를 호출합니다. 로드되고 동일한 이름의 함수를 정의할 수 있는 다른 패키지에서는 호출하지 않습니다. 이 예방 조치는 필터 기능의 경우 특히 권장됩니다. 이 이름은 꽤 많은 다른 패키지에서 사용되기 때문입니다. 사람들의 이름을 (주어진) 이름으로 부르는 것과 같이 R 함수를 호출하는 일반적인 (: 없이) 방법을 생각할 수 있습니다. 반면에 ::가 포함된 정규화된 버전은 누군가의 이름을 부르는 것과 같습니다. 적어도 CRAN 및 Bioconductor 저장소의 범위 내에서 이러한 정규화된 이름은 고유함을 보장합니다.\n\n\n우리는 dplyr 패키지에서 filter 함수를 호출하기 위해 :: 연산자를 사용합니다. 로드되고 동일한 이름의 함수를 정의할 수 있는 다른 패키지에서는 호출되지 않습니다. 이 예방 조치는 filter 함수의 경우 특히 권장됩니다. 이 이름은 꽤 많은 다른 패키지에서 사용되기 때문입니다. 사람들을 이름으로 부르는 것과 같이 R 함수를 호출하는 일반적인(:: 없이) 방법을 생각할 수 있습니다. 반면에 ::가 포함된 정규화된 버전은 누군가를 이름으로 부르는 것에 해당합니다. 최소한 CRAN 및 Bioconductor 저장소의 범위 내에서 이러한 정규화된 이름은 고유함이 보장됩니다.\n위 줄에서 데이터 프레임 CpGtab을 염색체 8로만 부분 집합(필터)한 다음 시작 및 끝 위치가 데이터 프레임의 동일하게 명명된 열에 의해 정의되는 IRanges 개체를 만듭니다. 인수로부터 개체를 구성하는 ‘IRanges’ 함수 호출에서 첫 번째 ’start’는 함수의 인수 이름이고, 두 번째 ’start’는 ’filter’의 출력으로 얻은 데이터 프레임의 열을 나타냅니다. ’end’도 마찬가지입니다. _IRanges_는 수학적 간격에 대한 일반 컨테이너입니다. 우리는 다음 줄을 사용하여 생물학적 맥락을 만듭니다.\n16 _IRanges_의 “I”는 “간격”을 의미합니다. _GRanges_의 “G”는 “게놈”을 의미합니다.\ngrCpG = GRanges(ranges = irCpG, seqnames = “chr8”, strand = “+”) genome(grCpG) = “hg19”__\n이제 시각화해 보겠습니다. 그림 2.26의 출력을 참조하세요.\nlibrary(“Gviz”) ideo = IdeogramTrack(genome = “hg19”, 염색체 = “chr8”)plotTracks( list(GenomeAxisTrack(), AnnotationTrack(grCpG, 이름 = “CpG”), ideo), from = 2200000, to = 5800000, Shape = “box”, fill = “#006400”, stacking = “밀도”)__\n\n그림 2.26: 염색체 8의 선택된 영역에 있는 CpG 위치의 Gviz 플롯.\n이제 우리는 CpG 섬 ‘irCpG’와 그 사이의 영역(’gaps(irCpG)’)에 해당하는 염색체 서열에 대한 소위 뷰를 정의합니다. 결과 개체 CGIview 및 NonCGIview에는 시퀀스 자체가 아닌 좌표만 포함되므로(이들은 큰 개체 Hsapiens$chr8에 유지됨) 저장 측면에서 상당히 가볍습니다.\nCGIview = Views(unmasked(Hsapiens\\(chr8), irCpG) NonCGIview = Views(unmasked(Hsapiens\\)chr8), gaps(irCpG))__\n우리는 데이터를 사용하여 CpG 섬과 비섬의 전환 횟수를 계산합니다.\nseqCGI = as(CGIview, “DNAStringSet”) seqNonCGI = as(NonCGIview, “DNAStringSet”) dinucCpG = sapply(seqCGI, dinucleotideFrequency) dinucNonCpG = sapply(seqNonCGI, dinucleotideFrequency) dinucNonCpG[, 1]__\nAA AC AG AT CA CC CG CT GA GC GG GT TA TC TG TT 389 351 400 436 498 560 112 603 359 336 403 336 330 527 519 485\nNonICounts = rowSums(dinucNonCpG) IslCounts = rowSums(dinucCpG)__\n우리가 가지고 있는 4개 상태 마르코프 체인의 경우 전이 행렬을 행이 ‘from’ 상태이고 열이 ‘to’ 상태인 행렬로 정의합니다.\nTI = matrix( IslCounts, ncol = 4, byrow = TRUE) TnI = matrix(NonICounts, ncol = 4, byrow = TRUE) dimnames(TI) = dimnames(TnI) = list(c(“A”, “C”, “G”, “T”), c(“A”, “C”, “G”, “T”))__\n각 유형의 전환 횟수를 사용하여 빈도를 계산하고 이를 두 개의 행렬에 넣습니다.\n\n\n\n전환 확률은 확률이므로 행의 합이 1이 되어야 합니다.\n\n\n전환 확률은 확률이므로 행의 합이 1이 되어야 합니다.\nMI = TI /rowSums(TI) MI __\nA C G T A 0.20457773 0.2652333 0.3897678 0.1404212 C 0.20128250 0.3442381 0.2371595 0.2173200 G 0.18657245 0.3145299 0.3450223 0.1538754 T 0.09802105 0.3352314 0.3598984 0.2068492\nMN = TnI / rowSums(TnI) MN __\nA C G T A 0.3351380 0.1680007 0.23080886 0.2660524 C 0.3641054 0.2464366 0.04177094 0.3476871 G 0.2976696 0.2029017 0.24655406 0.2528746 T 0.2265813 0.1972407 0.24117528 0.3350027\n__\n질문 2.24\n행마다 전환이 다릅니까? 이는 예를 들어 \\(P(,|,) P(,|,)\\)를 의미합니다.\n__\n해결책\n__\n전환이 다릅니다. 예를 들어, 아일랜드(MI) 전이 행렬에서 C에서 A로, T에서 A로의 전이는 매우 다르게 보입니다(0.201 대 0.098).\n__\n질문 2.25\nCpG 섬에서는 다른 뉴클레오티드의 상대 빈도가 다른 곳과 다른가요?\n__\n해결책\n__\nfreqIsl = alphabetFrequency(seqCGI, baseOnly = TRUE, collapse = TRUE)[1:4] freqIsl / sum(freqIsl)__\nA C G T 0.1781693 0.3201109 0.3206298 0.1810901\nfreqNon = alphabetFrequency(seqNonCGI, baseOnly = TRUE, collapse = TRUE)[1:4] freqNon / sum(freqNon)__\nA C G T 0.3008292 0.1993832 0.1993737 0.3004139\n이는 반대 패턴을 보여줍니다. CpG 섬에서는 C와 G의 주파수가 약 0.32인 반면, CpG가 아닌 섬에서는 A와 T의 주파수가 약 0.30입니다.\n__\n질문 2.26\n주어진 시퀀스가 ​​CpG 섬에서 오는지 여부를 결정하기 위해 이러한 차이점을 어떻게 사용할 수 있습니까?\n__\n해결책\n__\n\\(^2\\) 통계를 사용하여 관찰된 주파수와 freqIsl 및 freqNon 주파수 사이의 주파수를 비교합니다. 더 짧은 시퀀스의 경우 이는 충분히 민감하지 않을 수 있으며 아래에 더 민감한 접근 방식이 제공됩니다.\nCpG 섬에 있는지 아닌지 알 수 없는 시퀀스가 ​​주어지면 다른 곳에 비해 CpG 섬에 속할 확률이 얼마나 되는지 물어볼 수 있습니다. 우리는 승산비(odds ratio)를 기반으로 점수를 계산합니다. 예를 들어 보겠습니다. 시퀀스 \\(x\\)가 ACGTTATACTACG이고 그것이 CpG 섬에서 오는지 여부를 결정하고 싶다고 가정합니다.\n시퀀스를 1차 마르코프 체인으로 모델링하면 시퀀스가 ​​CpG 섬에서 나온다고 가정하여 작성할 수 있습니다.\n\\[ \\[\\begin{align} P_{\\text{i}}(x = \\mathtt{ACGTTATACTACG}) = \\; &P_{\\text{i}}(\\mathtt{A}) \\, P_{\\text{i}}(\\mathtt{AC})\\, P_{\\text{i}}(\\mathtt{CG})\\, P_{\\text{i}}(\\mathtt{GT})\\, P_{\\text{i}}(\\mathtt{TT}) \\times \\\\\\ &P_{\\text{i}}(\\mathtt{TA})\\, P_{\\text{i}}(\\mathtt{AT})\\, P_{\\text{i}}(\\mathtt{TA})\\, P_{\\text{i}}(\\mathtt{AC})\\, P_{\\text{i}}(\\mathtt{CG}). \\end{align}\\] \\]\n우리는 이 확률을 섬이 아닌 곳의 확률과 비교할 것입니다. 위에서 본 것처럼 이러한 확률은 상당히 다른 경향이 있습니다. 우리는 그들의 비율을 취하여 그것이 1보다 크거나 작은지 확인할 것입니다. 이러한 확률은 많은 작은 항의 곱이 될 것이며 매우 작아질 것입니다. 우리는 로그를 취함으로써 이 문제를 해결할 수 있습니다.\n\\[ \\[\\begin{align} \\log&\\frac{P(x\\,|\\, \\text{섬})}{P(x\\,|\\,\\text{비섬})}=\\\\\\ \\log&\\left( \\frac{P_{\\text{i}}(\\mathtt{A})\\, P_{\\text{i}}(\\mathtt{A}\\rightarrow \\mathtt{C})\\, P_{\\text{i}}(\\mathtt{C}\\rightarrow \\mathtt{G})\\, P_{\\text{i}}(\\mathtt{G}\\rightarrow \\mathtt{T})\\, P_{\\text{i}}(\\mathtt{T}\\rightarrow \\mathtt{T})\\, P_{\\text{i}}(\\mathtt{T}\\rightarrow \\mathtt{A})} {P_{\\text{n}}(\\mathtt{A})\\, P_{\\text{n}}(\\mathtt{A}\\rightarrow \\mathtt{C})\\, P_{\\text{n}}(\\mathtt{C}\\rightarrow \\mathtt{G})\\, P_{\\text{n}}(\\mathtt{G}\\rightarrow \\mathtt{T})\\, P_{\\text{n}}( \\mathtt{T}\\rightarrow \\mathtt{T})\\, P_{\\text{n}}( \\mathtt{T}\\rightarrow \\mathtt{A})} \\right.\\times\\\\\\ &\\left.\\mathtt{A}\\rightarrow \\mathtt{T})\\, P_{\\text{i}}(\\mathtt{T}\\rightarrow \\mathtt{A})\\, P_{\\text{i}}(\\mathtt{A}\\rightarrow \\mathtt{C})\\, P_{\\text{i}}(\\mathtt{C}\\rightarrow \\mathtt{G})} {P_{\\text{n}}(\\mathtt{A}\\rightarrow \\mathtt{T})\\, P_{\\text{n}}(\\mathtt{T}\\rightarrow \\mathtt{A})\\, P_{\\text{n}}(\\mathtt{A}\\rightarrow \\mathtt{C})\\, P_{\\text{n}}(\\mathtt{C}\\rightarrow \\mathtt{G})} \\right) \\end{align}\\] \\]\n이것이 로그 우도 비율 점수입니다. 계산 속도를 높이기 위해 로그 비율 \\((P_{}()/P_{}()),…, (P_{}()/P_{}())\\) 한 번에 모두 해당 내용을 합산하여 점수를 얻습니다.\n\n\n\n작업된 예제와 많은 유용한 세부 정보는 Durbin et al.에서 찾을 수 있습니다. (1998).\n\n\n연구된 예제와 많은 유용한 세부 정보는 Durbin et al.에서 찾을 수 있습니다. (1998).\nalpha = log((freqIsl/sum(freqIsl)) / (freqNon/sum(freqNon))) beta = log(MI / MN)__\nx = “ACGTTATACTACG” scorefun = function(x) { s = unlist(strsplit(x, ““)) score = alpha[s[1]] if (length(s) &gt;= 2) for (j in 2:length(s)) score = score + beta[s[j-1], s[j]] score } scorefun(x)__\nA -0.2824623\n아래 코드에서는 seqCGI 개체의 2855개 시퀀스 중에서 len = 100 길이의 시퀀스를 선택한 다음 seqNonCGI 개체의 2854개 시퀀스 중에서 선택합니다(각각은 _DNAStringSet_입니다). generateRandomScores 함수의 처음 세 줄에서는 A, C, T, G 이외의 문자가 포함된 시퀀스를 삭제합니다. 와 같은 “.” (정의되지 않은 뉴클레오티드에 사용되는 문자). 나머지 시퀀스 중에서 길이에서 ’len’을 뺀 값에 비례하는 확률로 샘플링한 다음 길이가 ’len’인 하위 시퀀스를 선택합니다. 하위 시퀀스의 시작점은 하위 시퀀스가 ​​맞아야 하는 제약 조건을 사용하여 균일하게 샘플링됩니다.\ngenerateRandomScores = function(s, len = 100, B = 1000) { alphFreq = alphabetFrequency(s) isGoodSeq = rowSums(alphFreq[, 5:ncol(alphFreq)]) == 0 s = s[isGoodSeq] slen = sapply(s, length) prob = pmax(slen - len, 0) prob = prob / sum(prob) idx = sample(length(s), B, replace = TRUE, prob = prob) ssmp = s[idx] start = sapply(ssmp, function(x) sample(length(x) - len, 1)) scores = sapply(seq_len(B), function(i) scorefun(as.character(ssmp[[i]][start[i]+(1:len)])) ) scores / len } scoresCGI = generateRandomScores(seqCGI) scoresNonCGI = generateRandomScores(seqNonCGI)__\nrgs = range(c(scoresCGI, scoresNonCGI)) br = seq(rgs[1], rgs[2], length.out = 50) h1 = hist(scoresCGI, breaks = br, plot = FALSE) h2 = hist(scoresNonCGI, breaks = br, plot = FALSE) plot(h1, col = rgb(0, 0, 1, 1/4), xlim = c(-0.5, 0.5), ylim=c(0,120)) plot(h2, col = rgb(1, 0, 0, 1/4), add = TRUE)__\n\n그림 2.27: ‘generateRandomScores’ 함수에 의해 생성된 섬 및 비섬 점수. 이것은 우리가 접하는 첫 번째 혼합물 사례입니다. 이에 대해서는 4장에서 다시 살펴보겠습니다.\n우리는 이것을 _훈련 데이터_로 간주할 수 있습니다. 유형을 알고 있는 데이터에서 점수가 식별에 유용한지 여부를 확인할 수 있습니다(그림 2.27 참조).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "02-chap.html#이-장의-요약",
    "href": "02-chap.html#이-장의-요약",
    "title": "4  2.1 이 장의 목표",
    "section": "4.11 2.11 이 장의 요약",
    "text": "4.11 2.11 이 장의 요약\n이 장에서 우리는 통계의 기본 요가, 즉 데이터에서 가능한 생성 분포로 돌아가는 방법과 이러한 분포를 정의하는 매개변수를 추정하는 방법을 경험했습니다.\n통계 모델 범주형 결과(이항 및 다항) 실험에 대한 몇 가지 구체적인 통계 모델을 보여 주었습니다.\n적합도 우리는 다양한 시각화를 사용하고 시뮬레이션 실험을 실행하여 데이터가 공정한 4개 상자 다항 모델에 적합할 수 있는지 테스트하는 방법을 보여주었습니다. 카이제곱 통계를 접하고 qq-plot을 사용하여 시뮬레이션과 이론을 비교하는 방법을 살펴보았습니다.\n추정 최대 우도 및 베이지안 추정 절차에 대해 설명했습니다. 이러한 접근법은 뉴클레오티드 패턴 발견 및 일배체형 추정과 관련된 예에서 설명되었습니다.\n사전 및 사후 분포 일배체형과 같이 이전에 연구된 유형의 데이터를 평가할 때 데이터의 사후 분포를 계산하는 것이 도움이 될 수 있습니다. 이를 통해 간단한 계산을 통해 의사결정에 불확실성을 포함할 수 있습니다. 충분한 데이터가 있는 한 사전 선택은 결과에 거의 영향을 미치지 않습니다.\nCpG 섬 및 마르코프 사슬 우리는 DNA 서열에 따른 의존성이 마르코프 사슬 전이에 의해 어떻게 모델링될 수 있는지 살펴보았습니다. 우리는 이를 사용하여 긴 DNA 서열이 CpG 섬에서 유래했는지 여부를 확인할 수 있는 우도 비율을 기반으로 점수를 구축했습니다. 점수 히스토그램을 만들 때 그림 2.27에서 눈에 띄는 특징을 확인했습니다. 두 조각으로 구성된 것처럼 보였습니다. 이 이중 양식은 혼합을 처음 접한 것이며 4장의 주제입니다.\n이것은 일부 훈련 데이터에 대한 모델을 구축하는 첫 번째 사례입니다. CpG 섬에 있다는 것을 알고 있는 시퀀스는 나중에 새 데이터를 분류하는 데 사용할 수 있습니다. 우리는 12장에서 이 작업을 수행하는 훨씬 더 완전한 방법을 개발할 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "02-chap.html#추가-자료",
    "href": "02-chap.html#추가-자료",
    "title": "4  2.1 이 장의 목표",
    "section": "4.12 2.12 추가 자료",
    "text": "4.12 2.12 추가 자료\n이용 가능한 최고의 통계 입문 서적 중 하나는 Freedman, Pisani 및 Purves(1997)입니다. 중요한 개념을 설명하기 위해 상자 모델을 사용합니다. 통계 수업을 들어본 적이 없거나 재교육이 필요하다고 생각된다면 이 수업을 적극 권장합니다. 많은 입문 통계 수업에서는 이산형 데이터에 대한 깊이 있는 통계를 다루지 않습니다. 주제는 생물학적 응용에 필요한 중요한 부분입니다. 이러한 유형의 분석에 대한 책 한 권의 소개는 (Agresti 2007)에서 찾을 수 있습니다.\n여기서 우리는 간단한 구조화되지 않은 다항식의 예를 제시했습니다. 그러나 다항식의 범주(또는 상자)가 특정 구조를 갖는 경우도 있습니다. 예를 들어, 64개의 가능한 코돈은 20개의 아미노산을 코딩하고 정지 코돈(61+3)을 코딩합니다. 그래서 우리는 아미노산 자체를 자유도 20의 다항식으로 볼 수 있습니다. 각 아미노산 내에는 다양한 범주의 다항식이 있습니다(프롤린에는 ‘CCA, CCG, CCC, CCT’, 연습 2.3 참조). 일부 다변량 방법은 서로 다른 풍부한 아미노산 내에서 코돈 사용 간의 가변성을 분해하기 위해 특별히 고안되었으며(Grantham et al. 1981; Perrière and Thiouluse 2002), 이를 통해 잠재 유전자 전달 및 번역 선택을 발견할 수 있습니다. Chapter 9에서 범주형 데이터의 다변량 탐색을 탐구할 때 해당 논문에서 사용된 구체적인 방법을 다룰 것입니다.\n불확실성을 정량화하기 위해 베이지안 패러다임을 성공적으로 사용한 사례가 많이 있습니다. 최근 몇 년 동안 마르코프 체인, 랜덤 워크 또는 해밀턴 역학을 사용하는 특수한 유형의 몬테카를로에 의해 사후 분포 계산이 혁신되었습니다. 이러한 방법은 몇 번의 반복 후에 올바른 사후 분포로 수렴되는 근사치를 제공합니다. 예와 더 많은 내용은 (Robert and Casella 2009; Marin and Robert 2007; McElreath 2015)을 참조하세요.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "02-chap.html#연습",
    "href": "02-chap.html#연습",
    "title": "4  2.1 이 장의 목표",
    "section": "4.13 2.13 연습",
    "text": "4.13 2.13 연습\n__\n연습 2.1\n1,000개의 긴 유전자 서열을 따라 발생하는 돌연변이를 모델링하는 1,000개의 무작위 0/1 변수를 생성합니다. 이는 각각 \\(10^{-4}\\)의 비율로 독립적으로 발생합니다. 그런 다음 1,000개의 위치를 ​​합산하여 길이가 1,000인 시퀀스에서 얼마나 많은 돌연변이가 있는지 계산합니다.\n적합성 테스트를 사용하여 이러한 돌연변이 합계에 대한 올바른 분포를 찾고 적합성 품질을 시각화하는 플롯을 만듭니다.\n__\n연습 2.2\n\\(0\\)과 \\(7\\) 사이에서 \\(n\\)개의 임의의 균일수를 생성하고 최대값을 반환하는 함수를 만드세요. \\(n=25\\)에 대한 함수를 실행합니다. 이 절차를 \\(B=100\\)번 반복합니다. 이러한 최대값의 분포를 도표화합니다. 크기가 25인 표본의 최대우도 추정치는 얼마입니까(\\(\\))? 이론적 근거와 실제 최대값 \\(\\)를 찾을 수 있습니까?\n__\n연습 2.3\n유전자의 코딩 영역에 포함된 3개의 뉴클레오티드(코돈) 시퀀스는 20개의 가능한 아미노산 중 하나로 전사될 수 있습니다. \\(4^3=64\\)개의 가능한 코돈 서열이 있지만 아미노산은 20개뿐입니다. 우리는 유전자 코드가 중복된다고 말합니다. 각 아미노산을 _철자_하는 방법에는 여러 가지가 있습니다.\n다중도(동일한 아미노산을 코딩하는 코돈의 수)는 2에서 6까지 다양합니다. 각 아미노산의 서로 다른 코돈 철자는 동일한 확률로 발생하지 않습니다. 표준 실험실 결핵균(H37Rv)에 대한 데이터를 살펴보겠습니다.\nmtb = read.table(“../data/M_tuberculosis.txt”, header = TRUE) head(mtb, n = 4)__\nAmAcid 코돈 번호 PerThous 1 Gly GGG 25874 19.25 2 Gly GGA 13306 9.90 3 Gly GGT 25320 18.84 4 Gly GGC 68310 50.82\n아미노산 프롤린에 대한 코돈은 \\(CC*\\) 형식이며 Mycobacterium turberculosis에서 다음과 같은 빈도로 발생합니다.\npro = mtb[ mtb$AmAcid == “Pro”, “Number”] pro/sum(pro)__\n[1] 0.54302025 0.10532985 0.05859765 0.29305225\n\n‘AmAcid’ 및 ‘Codon’ 변수를 표로 만들기 위해 ’table’을 사용하여 데이터 ’mtb’를 탐색합니다.\nPerThous 변수는 어떻게 만들어졌나요?\n가장 강한 코돈 편향, 즉 가능한 철자 중에서 균일 분포에서 가장 강한 이탈을 보이는 아미노산을 찾기 위해 표에 적용할 수 있는 R 함수를 작성하세요.\n\n\\(*\\)는 정규식에 대한 컴퓨터 표기법을 사용하여 4개 문자 중 하나를 나타냅니다.\n__\n연습 2.4\n_Staphylococcus Aureus_의 순서에 따라 실행 창에 GC 콘텐츠를 표시합니다. 파일에서 fasta 파일 시퀀스를 읽습니다.\nstaph = readDNAStringSet(“../data/staphsequence.ffn.txt”, “fasta”)__\n\n전체 staph 개체를 살펴본 다음 세트의 처음 세 시퀀스를 표시합니다.\n너비가 100인 슬라이딩 윈도우의 시퀀스를 따라 GC 콘텐츠를 찾습니다.\nb)의 결과를 표시합니다.\n시퀀스를 따라 이러한 비율의 전반적인 추세를 어떻게 시각화할 수 있습니까?\n\n__\n해결책\n__\n\n데이터는 다음을 사용하여 표시됩니다.\n\n포도상구균[1:3, ]__\n길이가 3인 DNAStringSet 객체: 너비 seq 이름 [1] 1362 ATGTCGGAAAAAGAAATTTGGGA…AAAAAGAAATAAGAAATGTATAA lcl|NC_002952.2_c… [2] 1134 ATGATGGAATTCACTATTAAAAG…TTTTACCAATCAGAACTTACTAA lcl|NC_002952.2_c… [3] 246 GTGATTATTTTGGTTCAAGAAGT…TCATTCATCAAGGTGAACAATGA lcl|NC_002952.2_c…\n포도상 구균 __\n길이가 2650인 DNAStringSet 개체: 너비 시퀀스 이름 [1] 1362 ATGTCGGAAAAAGAAATTTGGG…AAAGAAATAAGAAATGTATAA lcl|NC_002952.2_c… [2] 1134 ATGATGGAATTCACTATTAAAA…TTACCAATCAGAACTTACTAA lcl|NC_002952.2_c… [3] 246 GTGATTATTTTGGTTCAAGAAG…ATTCATCAAGGTGAACAATGA lcl|NC_002952.2_c… [4] 1113 ATGAAGTTAAATACACTCCAAT…CAAGGTGAAATTATAAAGTAA lcl|NC_002952.2_c… [5] 1932 GTGACTGCATTGTCAGATGTAA…TATGCAAACTTAGACTTCTAA lcl|NC_002952.2_c… … … … [2646] 720 ATGACTGTAGAATGGTTAGCAG…ACTCCTTTACTTGAAAAATAA lcl|NC_002952.2_c… [2647] 1878 GTGGTTCAAGAATATGATGTAA…CTCCAAAGGGTGAGTGACTAA lcl|NC_002952.2_c… [2648] 1380 ATGGATTTAGATACAATTACGA…CAATTCTGCTTAGGTAAATAG lcl|NC_002952.2_c… [2649] 348 TTGGAAAAAGCTTACCGAATTA…TTTAATAAAAAGATTAAGTAA lcl|NC_002952.2_c… [2650] 138 ATGGTAAAACGTACTTATCAAC…CGTAAAGTTTTATCTGCATAA lcl|NC_002952.2_c…\n\n‘letterFrequency’ 함수를 사용하여 빈도를 계산할 수 있습니다.\n\nletterFrequency(staph[[1]], 문자 = “ACGT”, OR = 0)__\nA C G T 522 219 229 392\nGCstaph = data.frame( ID = names(staph), GC = rowSums(alphabetFrequency(staph)[, 2:3] / width(staph)) * 100 )__\n\n플롯팅은 다음과 같이 수행될 수 있습니다. 여기서는 시퀀스 364에 대한 예시입니다(그림 2.28).\n\nwindow = 100 gc = rowSums( letterFrequencyInSlidingView(staph[[364]], window, c(“G”,“C”)))/window plot(x = seq(along = gc), y = gc, type = “l”)__\n\n그림 2.28: Staphylococcus Aureus 게놈의 서열 364에 따른 GC 함량.\n\n창을 따라 ‘lowess’ 함수를 사용하여 데이터를 평활화하면 전반적인 추세를 볼 수 있습니다.\n\n줄거리(x = seq(along = gc), y = gc, 유형 = “l”) 행(lowess(x = seq(along = gc), y = gc, f = 0.2), col = 2)__\n\n그림 2.29: 그림 2.28과 유사하며 평활화가 적용되었습니다.\n시퀀스를 따라 이동할 때 항상 여러 가능한 상태 중 하나에 있다는 아이디어를 사용하여 창에 비정상적으로 높은 GC 콘텐츠가 있는지 여부를 결정하는 적절한 방법을 나중에 살펴보겠습니다. 그러나 우리는 상태를 직접 관찰하지 않고 순서만 관찰합니다. 이러한 모델을 숨겨진(상태) 마르코프 모델 또는 줄여서 HMM이라고 합니다(Wikipedia 참조). 이 모델 이름의 _Markov_는 이웃 위치 간의 종속성을 모델링하는 방법에 대한 것이며, hidden ​​부분은 상태가 직접 관찰되지 않음, 즉 숨겨진 것을 나타냅니다.\n__\n연습 2.5\n그림 2.19와 유사한 그림을 다시 실행하되 두 가지 다른 분포, 즉 균일 분포(Beta(1,1))와 Beta(\\(,\\))를 포함합니다. 무엇을 알아차렸나요?\n__\n해결책\n__\ndfbetas = data.frame( p = rep(p_seq, 5), dbeta = c(dbeta(p_seq, 0.5, 0.5), dbeta(p_seq, 1, 1), dbeta(p_seq, 10, 30), dbeta(p_seq, 20, 60), dbeta(p_seq, 50, 150)), pars = rep(c(“Beta(0.5,0.5)”, “U(0,1)=Beta(1,1)”, “Beta(10,30)”, “Beta(20,60)”, “Beta(50,150)”), each = length(p_seq))) ggplot(dfbetas) + geom_line(aes(x = p, y = dbeta, colour = pars)) + theme(legend.title = element_blank()) + geom_vline(aes(xintercept = 0.25), colour = “#990000”, linetype = “dashed”)__\n\n그림 2.30: 다양한 매개변수 선택에 대한 베타 밀도.\n1보다 큰 매개변수를 갖는 베타 분포는 단봉인 반면, Beta(0.5,0.5) 분포는 쌍봉이고 Beta(1,1)은 평탄하며 최빈값이 없습니다.\n__\n연습 2.6\n베타 분포의 매개변수에 대한 사전 설정을 직접 선택하세요. 여기에서 스케치하면 됩니다.\nhttps://jhubiostatistics.shinyapps.io/drawyourprior. Once you have set up a 먼저 \\(n=300\\)번의 시도 중 \\(Y = 40\\)번의 성공을 확인한 섹션 2.9.1의 데이터를 다시 분석하세요. QQ 플롯을 사용하여 해당 섹션에서 얻은 사후 분포를 비교하십시오.\n아그레스티, 앨런. 2007. 범주형 데이터 분석 소개. 존 와일리.\n캐닝스, 크리스, 앤서니 WF 에드워즈. 1968. “자연 선택과 드 피네티 다이어그램.” 인간 유전학 연대기 31 (4): 421–28.\n클리블랜드, 윌리엄 S. 1988. John w.의 수집 작품. 터키: 그래픽 1965-1985. Vol. 5. CRC를 누릅니다.\n더빈, 리차드, 션 에디, 앤더스 크로그, 그레이엄 미치슨. 1998. 생물학적 서열 분석. 케임브리지 대학 출판부.\n엘슨, D, E 샤가프. 1952. “성게 배우자의 Desoxyribonucleic Acid 함량.” 경험 8(4): 143–45.\n피네티, 브루노 데. 1926. “Considerazioni Matematiche Sull’ereditarieta Mendeliana.” 메트론 ​​6:3–41.\n프리드먼, 데이비드, 로버트 피사니, 로저 퍼브스. 1997. 통계. 뉴욕, 뉴욕: WW 노턴.\nGrantham, Richard, Christian Gautier, Manolo Gouy, M Jacobzone 및 R Mercier.\n\n“코돈 카탈로그 사용은 유전자에 맞게 조정된 게놈 전략입니다 표현력.” 핵산연구 9(1): 213–13.\n\nIrizarry, 라파엘 A, 하오 우, 앤드류 P 페인버그. 2009. “CpG 섬의 종-일반화 확률 모델 기반 정의.” 포유류 게놈 20 (9-10): 674–80.\n사랑, 미카엘 1세, 볼프강 후버, 사이먼 앤더스. 2014. “DESeq2를 사용한 RNA-seq 데이터의 접힘 변화 및 분산에 대한 적절한 추정.” 그놈 생물학 15 (12): 1–21.\n마린, 장 미셸, 크리스티앙 로버트. 2007. 베이지안 핵심: 전산 베이지안 통계에 대한 실용적인 접근 방식. Springer 과학 및 비즈니스 미디어.\n맥엘리스, 리처드. 2015. 통계적 재검토: R 및 Stan의 예를 사용한 베이지안 코스. 행상인; 홀/CRC.\nMourant, AE, Ada Kopec 및 K Domaniewska-Sobczak. 1976. “인간 혈액형의 분포 2판.” 옥스포드 대학 출판부 런던.\n페리에르(Perrière), 기(Guy), 장 티울루즈(Jean Thiouluse). 2002. “코돈 사용 연구에서 대응 분석의 사용 및 오용.” 핵산연구 30(20): 4548–55.\n라이스, 존. 2006. 수학적 통계 및 데이터 분석. 센게이지 학습.\n로버트, 크리스찬, 조지 카셀라. 2009. R을 이용한 몬테카를로 방법 소개. Springer 과학 및 비즈니스 미디어.\nR 버전 4.5.1(2025-06-13)을 사용하여 2025-09-01 01:33에 작성된 페이지",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>2.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "03-chap.html",
    "href": "03-chap.html",
    "title": "5  3.1 이 장의 목표",
    "section": "",
    "text": "5.1 3.2 기본 R 플로팅\n데이터 시각화에는 적어도 두 가지 유형이 있습니다. 첫 번째는 과학자가 데이터를 탐색하고 복잡한 프로세스에 대한 발견을 할 수 있도록 합니다. 다른 유형의 시각화는 다른 사람들에게 보여주고 궁극적으로 출판물에 포함할 수 있는 유익하고 명확하며 시각적으로 매력적인 결과의 삽화를 제공합니다.\n이 두 가지 유형의 시각화는 모두 R로 만들 수 있습니다. 실제로 R은 여러 그래픽 시스템을 제공합니다. 이는 R이 확장 가능하기 때문이며, 수년에 걸친 R 그래픽의 발전은 기존 함수를 대체하는 것이 아니라 주로 패키지를 추가함으로써 이루어졌기 때문입니다. 각 그래픽 시스템에는 장점과 한계가 있습니다. 이 장에서는 그 중 두 가지를 알아보겠습니다. 먼저 기본 R 플로팅 함수를 간략히 살펴보겠습니다1. 그 후에 ggplot2로 전환할 것입니다.\n1 이 함수들은 모든 기본 R 설치와 함께 제공되는 graphics 패키지에 있습니다.\n그림 3.1: ZUSE 플로터 Z64 (1961년 발표). 출처: https://en.wikipedia.org/wiki/Plotter.\n기본 R 그래픽은 역사적으로 먼저 나왔습니다. 간단하고 절차적이며 개념적으로 캔버스에 그리는 것에서 동기를 얻었습니다. 다양한 유형의 플롯을 위한 특수 함수가 있습니다. 이 함수들은 호출하기 쉽지만, 더 복잡한 플롯을 만들기 위해 결합하거나 서로 교환하고 싶을 때, 이것은 금방 지저분해지거나 심지어 불가능해집니다. 사용자는 (개념적인) 캔버스에 직접 플롯을 그립니다(이 단어는 최초의 그래픽 장치 중 일부로 거슬러 올라갑니다 - 그림 3.1 참조). 사용자는 여백, 축 레이블, 제목, 범례, 하위 패널에 할당할 공간과 같은 결정을 명시적으로 처리해야 합니다. 일단 무언가가 ’플롯’되면 이동하거나 지울 수 없습니다.\n더 높은 수준의 접근 방식이 있습니다. _그래픽의 문법_에서는 그래픽이 모듈식 논리 조각으로 구성되므로, 인간 언어에서 문장의 일부를 바꾸고 제외할 수 있는 것처럼 직관적이고 쉽게 해독할 수 있는 방식으로 데이터에 대한 다양한 시각화 유형을 쉽게 시도할 수 있습니다. 캔버스나 플로터의 개념이 없습니다. 대신 사용자는 ggplot2에 R 객체 형태로 원하는 플롯에 대한 높은 수준의 설명을 제공하고, 렌더링 엔진은 장면을 전체적으로 파악하여 그래픽을 배치하고 출력 장치에 렌더링합니다.\n이 장에서는 다음을 수행합니다:\n가장 기본적인 함수는 plot입니다. 아래 코드(그림 3.2에 출력이 표시됨)에서는 효소 결합 면역 흡착 분석(ELISA)의 데이터를 플로팅하는 데 사용됩니다. 이 분석은 DNA를 분해하는 효소인 데옥시리보뉴클레아제(DNase)의 활성을 정량화하는 데 사용되었습니다. 데이터는 기본 R과 함께 편리하게 제공되는 R 객체 DNase에 수집됩니다. DNase 객체는 Run(분석 실행), conc(사용된 단백질 농도), density(측정된 광학 밀도) 열이 있는 데이터 프레임입니다.\nFigure 3.2: Plot of concentration vs. density for an ELISA assay of DNase.\nThis basic plot can be customized, for example by changing the plot symbol and axis labels using the parameters xlab, ylab and pch (plot character), as shown in Figure 3.3. Information about the variables is stored in the object DNase, and we can access it with the attr function.\n그림 3.3: 그림 3.2와 동일한 데이터이지만 더 나은 축 레이블과 다른 플롯 기호를 사용했습니다.\n__\n질문 3.1\n더 긴 설명, 물리적 단위, 출처 정보 등과 같은 “메타데이터”로 데이터 프레임 열에 주석을 달아두는 것은 유용한 기능처럼 보입니다. DNase 객체처럼 이러한 정보를 저장하는 방식이 R 생태계 전반에서 표준화되거나 일반적인가요? 이 작업을 수행하는 다른 표준화되거나 일반적인 방법이 있습니까?\n__\n해결책\n__\n일반적인 R _data.frame_이나 tidyverse(data_frame, tibble)에는 이를 위한 훌륭하거나 널리 사용되는 인프라가 없습니다. 하지만 Bioconductor 패키지 S4Vectors의 DataFrame 클래스를 살펴보세요. 무엇보다도 이것은 _SummarizedExperiment_의 행과 열에 주석을 다는 데 사용됩니다.\n산점도 외에도 내장 함수를 사용하여 히스토그램 및 상자 그림을 만들 수도 있습니다(그림 3.4).\nFigure 3.4: (a) Histogram of the density from the ELISA assay, and (b) boxplots of these values stratified by the assay run. The boxes are ordered along the axis in lexicographical order because the runs were stored as text strings. We could use R’s type conversion functions to achieve numerical ordering.\n상자 그림(Boxplot)은 좁은 공간에 여러 분포를 나란히 보여주기에 편리합니다. 3.6절에서 여러 단변량 분포를 그리는 것에 대해 더 자세히 다룰 것입니다.\n기본 R 플로팅 함수는 빠른 대화형 데이터 탐색에 유용하지만, 더 정교한 디스플레이를 만들고 싶다면 곧 그 한계에 부딪히게 됩니다. 우리는 ggplot2 패키지에 구현된 ’그래픽의 문법(grammar of graphics)’이라는 시각화 프레임워크를 사용할 것입니다. 이 프레임워크는 논리적이고 우아한 방식으로 고품질 그래픽을 단계별로 구성할 수 있게 해줍니다. 먼저 예제 데이터 세트를 소개하고 로드해 보겠습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "03-chap.html#기본-r-플로팅",
    "href": "03-chap.html#기본-r-플로팅",
    "title": "5  3.1 이 장의 목표",
    "section": "",
    "text": "head(DNase)__\n\n\n  Run       conc density\n1   1 0.04882812   0.017\n2   1 0.04882812   0.018\n3   1 0.19531250   0.121\n4   1 0.19531250   0.124\n5   1 0.39062500   0.206\n6   1 0.39062500   0.215\n\n\nplot(DNase$conc, DNase$density)__\n\n\n\nplot(DNase$conc, DNase$density,\n  ylab = attr(DNase, \"labels\")$y,\n  xlab = paste(attr(DNase, \"labels\")$x, attr(DNase, \"units\")$x),\n  pch = 3,\n  col = \"blue\")__\n\n\n\n\n\n\n\n\n\n\nhist(DNase$density, breaks=25, main = \"\")\nboxplot(density ~ Run, data = DNase)__",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "03-chap.html#예제-데이터-세트",
    "href": "03-chap.html#예제-데이터-세트",
    "title": "5  3.1 이 장의 목표",
    "section": "5.2 3.3 예제 데이터 세트",
    "text": "5.2 3.3 예제 데이터 세트\n\n그림 3.5: 원시 내배엽(primitive endoderm)의 마커인 Serpinh1(파란색), Gata6(빨간색), Nanog(녹색)으로 염색된 E3.5 마우스 배반포의 단일 절편 면역형광 이미지.\nggplot2 기능을 제대로 테스트하려면, 여러 다른 각도에서 자르고 볼 수 있도록 충분히 크고 복잡한 데이터 세트가 필요합니다. 초기 발달의 다른 시점에서 얻은 약 100개의 개별 마우스 배아 세포의 전사체를 보고하는 유전자 발현 마이크로어레이 데이터 세트를 사용할 것입니다. 포유류 배아는 단일 세포인 수정란에서 시작됩니다. 동기화된 세포 분열의 물결을 통해, 난자는 처음에는 눈에 띄는 차이가 없는 세포 덩어리로 증식합니다. 그러나 어느 시점에서 세포는 다른 계통(lineage)을 선택합니다. 점점 더 세분화되면서 완전한 유기체에 필요한 다양한 세포 유형과 조직이 발생합니다. Ohnishi 등(2014)이 설명한 실험의 목표는 배아에서 첫 번째 대칭성 깨짐 현상과 관련된 유전자 발현 변화를 조사하는 것이었습니다. 진행하면서 데이터를 더 설명하겠습니다. 자세한 내용은 논문과 Bioconductor 데이터 패키지 Hiiragi2013의 문서에서 찾을 수 있습니다. 먼저 데이터를 로드합니다.\n\n데이터 객체가 더 설명적인 이름 대신 다소 일반적인 이름인 x를 갖는 것은 유감입니다. 이름 충돌을 피하기 위해 가장 실용적인 해결책은 esHiiragi = x; rm(list=\"x\")와 같은 코드를 실행하는 것입니다.\nlibrary(\"Hiiragi2013\")__\n\n\nIn chunk 'loadHiiragi': Warning: replacing previous import 'boot::logit' by 'gtools::logit' whenloading 'Hiiragi2013'\n\n\nIn chunk 'loadHiiragi': Warning: replacing previous import 'boot::inv.logit' by 'gtools::inv.logit'when loading 'Hiiragi2013'\n\n\ndata(\"x\")\ndim(Biobase::exprs(x))__\n\n\n[1] 45101   101\nR 프롬프트에 x를 입력하여 ExpressionSet 객체 x의 더 자세한 요약을 출력할 수 있습니다. 데이터 행렬의 101개 열(Biobase 패키지의 exprs 함수를 통해 위에서 접근)은 표본(각각 단일 세포)에 해당하고, 45101개 행은 Affymetrix mouse4302 어레이에 의해 프로브된 유전자에 해당합니다. 데이터는 RMA 방법을 사용하여 정규화되었습니다(Irizarry et al. 2003). 원시 데이터는 패키지(데이터 객체 a)와 EMBL-EBI의 ArrayExpress 데이터베이스(접근 코드 E-MTAB-1681)에서도 사용할 수 있습니다.\n표본에 대해 어떤 정보가 있는지 살펴보겠습니다2.\n2 #CAB2D6 표기법은 색상의 RGB 좌표를 16진수로 표현한 것입니다. 이에 대한 자세한 내용은 3.10.2절을 참조하세요.\nhead(pData(x), n = 2)__\n\n\n        File.name Embryonic.day Total.number.of.cells lineage genotype\n1 E3.25  1_C32_IN         E3.25                    32               WT\n2 E3.25  2_C32_IN         E3.25                    32               WT\n          ScanDate sampleGroup sampleColour\n1 E3.25 2011-03-16       E3.25      #CAB2D6\n2 E3.25 2011-03-16       E3.25      #CAB2D6\n제공된 정보는 세포에 대한 정보(즉, 세포를 얻은 배아의 나이, 크기, 유전자형)와 기술적 정보(스캔 날짜, 원시 데이터 파일 이름)가 혼합된 것입니다. 관례에 따라, 마우스 배아의 발달 시간은 일 단위로 측정되며, 예를 들어 E3.5로 보고됩니다. 또한, 논문에서 저자들은 나이, 유전자형, 계통을 기준으로 세포를 8개의 생물학적 그룹(sampleGroup)으로 나누고, 이 그룹들을 나타내는 색상 구성표(sampleColour3)를 정의했습니다. 다음 코드(설명은 아래 참조)를 사용하여 각 그룹에 대한 요약 정보, 즉 세포 수와 선호하는 색상을 포함하는 작은 데이터 프레임 groups를 정의합니다.\n3 데이터 세트의 이 식별자는 영국식 철자법을 사용합니다. 이 책의 다른 곳에서는 미국식 철자법(color)을 사용합니다. ggplot2 패키지는 일반적으로 두 철자법을 모두 허용합니다.\nlibrary(\"dplyr\")\ngroups = group_by(pData(x), sampleGroup) |&gt;\n  summarise(n = n(), color = unique(sampleColour))\ngroups __\n\n\n# A tibble: 8 × 3\n  sampleGroup         n color  \n  &lt;chr&gt;           &lt;int&gt; &lt;chr&gt;  \n1 E3.25              36 #CAB2D6\n2 E3.25 (FGF4-KO)    17 #FDBF6F\n3 E3.5 (EPI)         11 #A6CEE3\n4 E3.5 (FGF4-KO)      8 #FF7F00\n5 E3.5 (PE)          11 #B2DF8A\n6 E4.5 (EPI)          4 #1F78B4\n7 E4.5 (FGF4-KO)     10 #E31A1C\n8 E4.5 (PE)           4 #33A02C\nFGF4-KO라는 이름이 포함된 그룹의 세포는 세포 분화의 중요한 조절자인 FGF4 유전자가 녹아웃된 배아에서 유래했습니다. E3.5부터, 야생형 세포(FGF4 녹아웃 없음)는 첫 번째 대칭성 깨짐 현상을 겪고, 전분화능 외배엽(pluripotent epiblast, EPI)과 원시 내배엽(primitive endoderm, PE)이라는 다른 세포 계통으로 분화됩니다.\n위의 코드 청크는 dplyr 패키지의 파이프 연산자 |&gt;와 group_by, summarise 함수를 처음 접하는 경우이므로, 코드를 풀어보겠습니다. 먼저 파이프 |&gt;4입니다. 일반적으로 파이프는 중첩된 함수 호출을 사람이 더 쉽게 읽을 수 있도록 만드는 데 유용합니다. 다음 두 줄의 코드는 R에서 동일합니다.\n4 |&gt;는 2021년에 출시된 버전 4.1부터 기본 R과 함께 제공되는 파이프 연산자입니다. magrittr 패키지는 의미는 동일하지 않지만 유사한 %&gt;% 연산자를 오래 전부터 제공해 왔으며, %&lt;&gt;% 및 %T&gt;%와 같은 여러 다른 파이핑 관련 연산자도 제공합니다. 이 책의 대부분은 2021년 이전에 작성되었으므로 magrittr의 %&gt;% 연산자가 여러 곳에서 사용됩니다. 여기에 표시된 코드에서처럼 책을 유지 관리하는 동안 %&gt;%를 |&gt;로 가끔 업데이트하고 있습니다.\nf(x) |&gt; g(y) |&gt; h()\nh(g(f(x), y))__\n이것은 “먼저 f(x)를 평가한 다음, 그 결과를 g 함수의 첫 번째 인수로 전달하고, y는 g의 두 번째 인수로 전달합니다. 그런 다음 g의 출력을 h 함수에 전달합니다.”라는 의미입니다. 이를 무한히 반복할 수 있습니다. 특히 인수 x와 y가 그 자체로 복잡한 표현식이거나, 여러 함수의 연쇄가 관련된 경우 첫 번째 버전이 더 읽기 쉬운 경향이 있습니다.\ngroup_by 함수는 모든 후속 작업이 전체 데이터 프레임에 한 번에 적용되는 것이 아니라 sampleGroup 요인에 의해 정의된 블록에 적용되어야 한다는 메모로 데이터 프레임을 “표시”합니다. 마지막으로, summarise는 요약 통계를 계산합니다. 예를 들어 mean, sum 등이 될 수 있습니다. 이 경우, 우리는 단순히 각 블록의 행 수(n())와 대표 색상을 계산합니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "03-chap.html#ggplot2",
    "href": "03-chap.html#ggplot2",
    "title": "5  3.1 이 장의 목표",
    "section": "5.3 3.4 ggplot2",
    "text": "5.3 3.4 ggplot2\nggplot2 is a package by Hadley Wickham (Wickham 2016) that implements the idea of grammar of graphics – a concept created by Leland Wilkinson in his eponymous book (Wilkinson 2005). We will explore some of its functionality in this chapter, and you will see many examples of how it can be used in the rest of this book. Comprehensive documentation for the package can be found on its website. The online documentation includes example use cases for each of the graphic types that are introduced in this chapter (and many more) and is an invaluable resource when creating figures.\nLet’s start by loading the package and redoing the simple plot of Figure 3.2.\nlibrary(\"ggplot2\")\nggplot(DNase, aes(x = conc, y = density)) + geom_point()__\n\nFigure 3.6: Our first ggplot2 figure, similar to the base graphics Figure 3.2.\nWe just wrote our first “sentence” using the grammar of graphics. Let’s deconstruct this sentence. First, we specified the dataframe that contains the data, DNase. The aes (this stands for aesthetic) argument states which variables we want to see mapped to the \\(x\\)- and \\(y\\)-axes, respectively. Finally, we stated that we want the plot to use points (as opposed to, say, lines or bars), by adding the result of calling the function geom_point.\nNow let’s turn to the mouse single cell data and plot the number of samples for each of the 8 groups using the ggplot function. The result is shown in Figure 3.7.\nggplot(groups, aes(x = sampleGroup, y = n)) +\n  geom_bar(stat = \"identity\")__\n\nFigure 3.7: A barplot, produced with the ggplot function from the table of group sizes in the mouse single cell data.\nWith geom_bar we now told ggplot that we want each data item (each row of groups) to be represented by a bar. Bars are one example of geometric object (geom in the ggplot2 package’s parlance) that ggplot knows about. We have already seen another such object in Figure 3.6: points, indicated by the geom_point function. We will encounter many other geoms later. We used the aes to indicate that we want the groups shown along the \\(x\\)-axis and the sizes along the \\(y\\)-axis. Finally, we provided the argument stat = \"identity\" (in other words, do nothing) to the geom_bar function, since otherwise it would try to compute a histogram of the data (the default value of stat is \"count\"). stat is short for statistic , which is what we call any function of data. Besides the identity and count statistic, there are others, such as smoothing, averaging, binning, or other operations that reduce the data in some way.\nThese concepts –data, geometrical objects, statistics– are some of the ingredients of the grammar of graphics, just as nouns, verbs and adverbs are ingredients of an English sentence.\n__\nTask\nFlip the \\(x\\)- and \\(y\\)-aesthetics to produce a horizontal barplot.\nThe plot in Figure 3.7 is not bad, but there are several potential improvements. We can use color for the bars to help us quickly see which bar corresponds to which group. This is particularly useful if we use the same color scheme in several plots. To this end, let’s define a named vector groupColor that contains our desired colors for each possible value of sampleGroup.5\n5 The information is completely equivalent to that in the sampleGroup and color columns of the dataframe groups; we are just adapting to the fact that ggplot2 expects this information in the form of a named vector.\ngroupColor = setNames(groups$color, groups$sampleGroup)__\nAnother thing that we need to fix in Figure 3.7 is the readability of the bar labels. Right now they are running into each other — a common problem when you have descriptive names.\nggplot(groups, aes(x = sampleGroup, y = n, fill = sampleGroup)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = groupColor, name = \"Groups\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))__\n\nFigure 3.8: Similar to Figure 3.7, but with colored bars and better bar labels.\nThis is now already a longer and more complex sentence. Let’s dissect it. We added an argument, fill to the aes function that states that we want the bars to be colored (filled) based on sampleGroup (which in this case co- incidentally is also the value of the x argument, but that need not always be so). Furthermore we added a call to the scale_fill_manual function, which takes as its input a color map – i.e., the mapping from the possible values of a variable to the associated colors – as a named vector. We also gave this color map a title (note that in more complex plots, there can be several different color maps involved). Had we omitted the call to scale_fill_manual, ggplot2 would have used its choice of default colors. We also added a call to theme stating that we want the \\(x\\)-axis labels rotated by 90 degrees and right-aligned (hjust; the default would be to center).\n\n5.3.1 3.4.1 데이터 흐름\n\nggplot 함수는 데이터를 데이터 프레임 형식으로 기대합니다. 데이터가 행렬이나 별도의 벡터, 또는 다른 유형의 객체에 있다면 변환해야 합니다. dplyr 및 broom 등의 패키지가 이 목적을 위한 기능을 제공합니다. 이에 대해서는 13.10절에서 더 자세히 다룰 것이며, 책 전체에서 이러한 변환의 예시를 보게 될 것입니다.\n여기에는 기본 R의 _data.frame_뿐만 아니라 tidyverse의 tibble 패키지에 있는 tibble(및 동의어 data_frame) 클래스도 포함됩니다.\nggplot 호출의 결과는 ggplot 객체입니다. 앞서 보았던 코드 조각을 다시 상기해 봅시다.\ngg = ggplot(DNase, aes(x = conc, y = density)) + geom_point()__\n여기서는 ggplot의 출력을 콘솔로 직접 보내 그림 3.6을 “출력”하는 대신, gg 객체에 할당했습니다. 이는 R 콘솔에서 작업할 때와 완전히 동일합니다. 1+1 같은 표현식을 입력하고 “Enter”를 누르면 결과가 출력되지만, s = 1+1처럼 할당 표현식인 경우 부작용(이름 \"s\"가 메모리의 객체에 바인딩됨)만 발생하고 아무것도 출력되지 않는 것과 같습니다. 마찬가지로 source로 호출된 스크립트 내에서 표현식이 평가될 때도 출력되지 않습니다. 따라서 위 코드도 print 메서드가 호출되지 않았기 때문에 그래픽 출력을 생성하지 않습니다. gg를 출력하려면 (대화형 세션에서) 그 이름을 입력하거나 print를 호출하면 됩니다.\ngg\nprint(gg)__\n\n\n5.3.2 3.4.2 그림 저장하기\nggplot2에는 ggsave라는 내장 플롯 저장 함수가 있습니다.\nggplot2::ggsave(\"DNAse-histogram-demo.pdf\", plot = gg)__\n플롯을 저장하는 두 가지 주요 방법은 벡터 그래픽과 래스터(픽셀) 그래픽입니다. 벡터 그래픽에서 플롯은 점, 선, 곡선, 도형 및 타이포그래피 문자와 같은 일련의 기하학적 원형(primitives)으로 저장됩니다. R에서 플롯을 벡터 그래픽 형식으로 저장할 때 선호되는 형식은 PDF입니다. 래스터 그래픽에서 플롯은 도트 매트릭스 데이터 구조에 저장됩니다. 래스터 형식의 주요 한계는 사용 가능한 픽셀 수에 따른 제한된 해상도입니다. R에서 래스터 그래픽 출력에 가장 흔히 사용되는 장치는 png입니다. 일반적으로 플롯을 벡터 그래픽 형식으로 저장하는 것이 바람직합니다. 나중에 언제든지 벡터 그래픽 파일을 원하는 해상도의 래스터 형식으로 변환할 수 있는 반면, 그 반대는 매우 어렵기 때문입니다. 발표나 논문의 그림이 픽셀화 아티팩트 때문에 보기 흉해지는 것을 원치 않으실 것입니다!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "03-chap.html#the-grammar-of-graphics",
    "href": "03-chap.html#the-grammar-of-graphics",
    "title": "5  3.1 이 장의 목표",
    "section": "5.4 3.5 The grammar of graphics",
    "text": "5.4 3.5 The grammar of graphics\nThe components of ggplot2 ’s grammar of graphics are\n\none or more datasets,\none or more geometric objects that serve as the visual representations of the data, – for instance, points, lines, rectangles, contours,\ndescriptions of how the variables in the data are mapped to visual properties (aesthetics) of the geometric objects, and an associated scale (e. g., linear, logarithmic, rank),\none or more coordinate systems,\nstatistical summarization rules,\na facet specification, i.e. the use of multiple similar subplots to look at subsets of the same data,\noptional parameters that affect the layout and rendering, such text size, font and alignment, legend positions.\n\nIn the examples above, Figures 3.7 and 3.8, the dataset was groupsize, the variables were the numeric values as well as the names of groupsize, which we mapped to the aesthetics \\(y\\)-axis and \\(x\\)-axis respectively, the scale was linear on the \\(y\\) and rank-based on the \\(x\\)-axis (the bars are ordered alphanumerically and each has the same width), and the geometric object was the rectangular bar.\nItems 4–7 in the above list are optional. If you don’t specify them, then the Cartesian is used as the coordinate system, the statistical summary is the trivial one (i.e., the identity), and no facets or subplots are made (we’ll see examples later on, in Section 3.8). The first three items are required: a valid ggplot2 “sentence” needs to contain at least one of each of them.\nIn fact, ggplot2 ’s implementation of the grammar of graphics allows you to use the same type of component multiple times, in what are called layers (Wickham 2010). For example, the code below uses three types of geometric objects in the same plot, for the same data: points, a line and a confidence band.\ndftx = data.frame(t(Biobase::exprs(x)), pData(x))\nggplot( dftx, aes( x = X1426642_at, y = X1418765_at )) +\n  geom_point( shape = 1 ) +\n  geom_smooth( method = \"loess\" )__\n\nFigure 3.9: A scatterplot with three layers that show different statistics of the same data: points (geom_point), a smooth regression line and a confidence band (the latter two from geom_smooth).\nHere we had to assemble a copy of the expression data (Biobase::exprs(x)) and the sample annotation data (pData(x)) all together into the dataframe dftx – since this is the data format that ggplot2 functions most easily take as input (more on this in Section 13.10).\nWe can further enhance the plot by using colors – since each of the points in Figure 3.9 corresponds to one sample, it makes sense to use the sampleColour information in the object x.\nggplot(dftx, aes(x = X1426642_at, y = X1418765_at))  +\n  geom_point(aes(color = sampleGroup), shape = 19) +\n  scale_color_manual(values = groupColor, guide = \"none\") +\n  geom_smooth(method = \"loess\")__\n\nFigure 3.10: As Figure 3.9, but in addition with points colored by the time point and cell lineage (as defined in Figure 3.8). We can now see that the expression values of the gene Timd2 (targeted by the probe 1418765_at, along the y-axis) are consistently high in the early time points, whereas its expression goes down in the EPI samples at days 3.5 and 4.5. In the FGF4-KO, this decrease is delayed - at E3.5, its expression is still high. Conversely, the gene Fn1 (1426642_at, x-axis) is off in the early timepoints and then goes up at days 3.5 and 4.5. The PE samples (green) show a high degree of cell-to- cell variability.\n__\n질문 3.2\n위의 코드에서 우리는 geom_point 레이어에 대해서만 color 미학(aes)을 정의한 반면, x와 y 미학은 모든 레이어에 대해 정의했습니다. 만약 모든 레이어에 대해 color 미학을 설정한다면, 즉 ggplot의 인수 목록으로 옮긴다면 어떻게 될까요?\n__\n질문 3.3\n그림 3.9와 3.10에서처럼 산점도 데이터를 회귀선과 함께 시각화하는 것이 항상 의미가 있을까요?\n여담으로, 이러한 프로브 식별자가 어떤 유전자를 타겟팅하고 그 유전자들이 어떤 역할을 하는지 알고 싶다면 다음과 같이 호출할 수 있습니다.\n\n여기서는 원래의 피처 식별자(예: 선행 “X”가 없는 “1426642_at”)를 사용해야 합니다. 이는 마이크로어레이 제조업체, Bioconductor 주석 패키지, 그리고 객체 x 내부에서 사용되는 표기법입니다. 위에서 dftx로 작업할 때 사용한 선행 “X”는 data.frame 생성자 함수에 의해 dftx 생성 중에 삽입된 것인데, 이는 check.names 인수가 기본적으로 TRUE로 설정되어 있기 때문입니다. 또는 check.names = FALSE로 설정하여 원래 식별자 표기법을 유지할 수도 있었지만, 그럴 경우 코드(예: aes() 호출)에서 R이 이를 올바르게 해석할 수 있도록 식별자 주위에 역따옴표(backtick)를 사용해야 합니다.\n패키지 이름을 포함한 전체 이름(fully qualified name)으로 select 함수를 호출하기 위해 :: 연산자를 사용한 것에 주목하세요. 우리는 이미 2장에서 이를 접했습니다.\nlibrary(\"mouse4302.db\")__\n\n\nAnnotationDbi::select(mouse4302.db,\n   keys = c(\"1426642_at\", \"1418765_at\"), keytype = \"PROBEID\",\n   columns = c(\"SYMBOL\", \"GENENAME\"))__\n\n\n     PROBEID SYMBOL                                            GENENAME\n1 1426642_at    Fn1                                       fibronectin 1\n2 1418765_at  Timd2 T cell immunoglobulin and mucin domain containing 2\nggplot을 사용할 때 종종 데이터, 미학, 그리고 기하학적 객체만 지정하면 됩니다. 대부분의 기하학적 객체는 데이터에 적합한 기본 통계 요약을 암시적으로 호출합니다. 예를 들어 geom_smooth를 사용하면 ggplot2는 기본적으로 stat = \"smooth\"를 사용하여 선을 표시합니다. geom_histogram을 사용하면 데이터가 구간화(binning)되고 결과가 막대 그림 형식으로 표시됩니다. 다음은 예시입니다.\ndfx = as.data.frame(Biobase::exprs(x))\nggplot(dfx, aes(x = `20 E3.25`)) + geom_histogram(binwidth = 0.2)__\n\n그림 3.11: E3.25일차의 특정 샘플 하나(세포 번호 20)에 대한 프로브 강도의 히스토그램.\n__\n질문 3.4\n객체 dfx와 dftx의 차이점은 무엇인가요? 왜 두 객체를 모두 만들어야 했을까요?\n위의 막대 그림 예제로 돌아가 봅시다.\npb = ggplot(groups, aes(x = sampleGroup, y = n))__\n이것은 플롯 객체 pb를 생성합니다. 이를 표시하려고 하면 기하학적 객체를 지정하지 않았기 때문에 빈 플롯이 생성됩니다. 지금까지 pb 객체에 있는 것은 데이터와 미학뿐입니다(그림 3.12).\nclass(pb)__\n\n\n[1] \"gg\"     \"ggplot\"\n\n\npb __\n\n그림 3.12: pb: 기하학적 객체(geom)가 없으면 플롯 영역이 비어 있습니다. 기본 스타일 매개변수 하에서는 (x)축의 눈금 레이블을 읽을 수 없습니다.\n이제 + 연산자를 사용하여 플롯의 다른 구성 요소들을 간단히 추가할 수 있습니다(그림 3.13).\npb = pb + geom_bar(stat = \"identity\")\npb = pb + aes(fill = sampleGroup)\npb = pb + theme(axis.text.x = element_text(angle = 90, hjust = 1))\npb = pb + scale_fill_manual(values = groupColor, name = \"Groups\")\npb __\n\n그림 3.13: 그래픽 객체 bp의 완전한 모습.\n어떤 방식으로든 이미 생성된 그래픽 객체를 가져와서 더 정제하는 이러한 단계별 구축 방식은, 그래픽을 생성하는 단일 함수 호출에 모든 지침을 미리 제공하는 것보다 더 편리하고 관리하기 쉬울 수 있습니다.\n매번 처음부터 플롯을 다시 만들 필요 없이, 부분적으로 완성된 객체를 저장한 다음 다른 방식으로 수정함으로써 다양한 시각화 아이디어를 빠르게 시도해 볼 수 있습니다. 예를 들어, 막대 그림의 대안적인 시각화를 만들기 위해 플롯을 극좌표계(polar coordinates)로 전환할 수 있습니다.\npb.polar = pb + coord_polar() +\n  theme(axis.text.x = element_text(angle = 0, hjust = 1),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank()) +\n  xlab(\"\") + ylab(\"\")\npb.polar __\n\n그림 3.14: 극좌표계에서의 막대 그림.\n위에서 이전에 설정한 theme 매개변수를 단순히 새 값으로 설정하여 덮어쓸 수 있음에 주목하세요. 원래 설정했던 pb를 다시 생성할 필요가 없습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "03-chap.html#차원-데이터-시각화",
    "href": "03-chap.html#차원-데이터-시각화",
    "title": "5  3.1 이 장의 목표",
    "section": "5.5 3.6 1차원 데이터 시각화",
    "text": "5.5 3.6 1차원 데이터 시각화\n생물학적 데이터 분석에서 일반적인 작업은 단변량 측정값(univariate measurements)의 여러 샘플을 비교하는 것입니다. 이 절에서는 이러한 샘플을 시각화하고 비교하는 몇 가지 가능성을 살펴보겠습니다. 예시로 Fgf4, Gata4, Gata6, Sox2라는 네 유전자의 강도를 사용하겠습니다6. 마이크로어레이에서 이들은 다음과 같이 표시됩니다.\n6 이 유전자들에 대해서는 (Ohnishi et al. 2014)에서 더 자세히 읽을 수 있습니다.\nselectedProbes = c( Fgf4 = \"1420085_at\", Gata4 = \"1418863_at\",\n                   Gata6 = \"1425463_at\",  Sox2 = \"1416967_at\")__\n이 표현에서 데이터를 추출하여 데이터 프레임으로 변환하기 위해 reshape2 패키지의 melt 함수를 사용합니다7.\n7 다양한 데이터 표현 방식의 개념과 원리에 대해서는 13.10절에서 더 자세히 다룰 것입니다.\nlibrary(\"reshape2\")\ngenes = melt(Biobase::exprs(x)[selectedProbes, ],\n             varnames = c(\"probe\", \"sample\"))__\nFor good measure, we also add a column that provides the gene symbol along with the probe identifiers.\ngenes$gene =\n  names(selectedProbes)[match(genes$probe, selectedProbes)]\nhead(genes)__\n\n\n       probe  sample    value  gene\n1 1420085_at 1 E3.25 3.027715  Fgf4\n2 1418863_at 1 E3.25 4.843137 Gata4\n3 1425463_at 1 E3.25 5.500618 Gata6\n4 1416967_at 1 E3.25 1.731217  Sox2\n5 1420085_at 2 E3.25 9.293016  Fgf4\n6 1418863_at 2 E3.25 5.530016 Gata4\n\n5.5.1 3.6.1 막대 그림(Barplots)\n우리 데이터 프레임 genes와 같은 데이터를 표시하는 일반적인 방법은 막대 그림입니다(그림 3.15).\nggplot(genes, aes(x = gene, y = value)) +\n  stat_summary(fun = mean, geom = \"bar\")__\n\nFigure 3.15: Barplots showing the means of the distributions of expression measurements from four probes.\n그림 3.15에서 각 막대는 해당 유전자의 값의 평균을 나타냅니다. 이러한 플롯은 생물 과학뿐만 아니라 대중 매체에서도 흔히 사용됩니다. 하지만 데이터를 평균이라는 단 하나의 숫자로 요약하면 많은 정보가 손실됩니다. 차지하는 공간에 비해 막대 그림은 데이터를 시각화하는 효율적인 방법이 아닙니다8.\n8 실제로 데이터 분포가 심하게 치우치거나(skewed), 다봉 분포(multimodal)이거나, 큰 이상치(outlier)가 있는 경우처럼 평균이 적절한 요약 통계량이 아닐 때는 이러한 종류의 시각화가 완전히 오해를 불러일으킬 수 있습니다.\n가끔은 오차 막대(error bars)를 추가하고 싶을 때가 있는데, ggplot2에서 이를 수행하는 한 가지 방법은 다음과 같습니다.\nlibrary(\"Hmisc\")\nggplot(genes, aes( x = gene, y = value, fill = gene)) +\n  stat_summary(fun = mean, geom = \"bar\") +\n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\",\n               width = 0.25)__\n\nFigure 3.16: Barplots with error bars indicating standard error of the mean.\n여기서 우리는 다시 계층화된 그래픽(layered graphics)의 원칙을 볼 수 있습니다. 두 개의 요약 함수 mean과 mean_cl_normal, 그리고 이와 연관된 두 개의 기하학적 객체 bar와 errorbar를 사용합니다. mean_cl_normal 함수는 Hmisc 패키지에서 제공하며, 평균의 표준 오차(또는 신뢰 한계, confidence l imits)를 계산합니다. 이는 간단한 함수이므로, 원한다면 기본 R 표현식을 사용하여 직접 계산할 수도 있습니다. 또한 플롯을 시각적으로 더 보기 좋게 만들기 위해 막대에 색상을 입혔습니다.\n\n\n5.5.2 3.6.2 상자 그림(Boxplots)\n상자 그림은 막대 그림과 비슷한 공간을 차지하면서도 훨씬 더 많은 정보를 제공합니다.\np = ggplot(genes, aes( x = gene, y = value, fill = gene))\np + geom_boxplot()__\n\nFigure 3.17: Boxplots.\n그림 3.17에서 두 유전자(Gata4, Gata6)는 분포가 비교적 집중되어 있으며, 높은 값 방향으로 몇 개의 데이터 포인트만 벗어나 있음을 알 수 있습니다. Fgf4의 경우 분포가 오른쪽으로 치우쳐(right-skewed) 있습니다. 상자 안의 검은 수평선으로 표시된 중앙값이 상자의 아래쪽(또는 왼쪽)에 더 가깝기 때문입니다. 반대로 Sox2의 분포는 왼쪽으로 치우쳐(left-skewed) 있습니다.\n\n\n5.5.3 3.6.3 점 그림(Dot plots)과 벌떼 그림(Beeswarm plots)\n데이터 포인트의 수가 너무 많지 않다면, 앞서 본 요약 방식 대신 데이터 포인트를 직접 보여주는 것이 가능하며 권장되는 방법입니다. 하지만 데이터를 직접 플로팅하면 점들이 서로 겹치는(overlapping) 경우가 많아 시각적으로 좋지 않거나 데이터를 가릴 수 있습니다. 점들이 겹치지 않으면서 원래 위치에 최대한 가깝게 배치되도록 시도할 수 있습니다(Wilkinson 1999).\np + geom_dotplot(binaxis = \"y\", binwidth = 1/6,\n       stackdir = \"center\", stackratio = 0.75,\n       aes(color = gene))\nlibrary(\"ggbeeswarm\")\np + geom_beeswarm(aes(color = gene))__\n[](03-chap_files/figure-html/fig-graphics-oneddot-1.png “Figure 3.18 (a):”)\n\n\n\n[](03-chap_files/figure-html/fig-graphics-oneddot-2.png “Figure 3.18 (b):”)\n\n\n\nFigure 3.18: (a) ggplot2의 geom_dotplot을 사용해 만든 점 그림. (b) ggbeeswarm 패키지의 geom_beeswarm을 사용해 만든 벌떼 그림.\n그림 3.18의 (a) 패널에 있는 플롯은 점들의 (y) 좌표를 구간(bin)으로 이산화하고(위에서는 구간 크기를 1/6로 선택함), 그 다음 서로 옆으로 쌓아 올린 것입니다.\nggbeeswarm 패키지의 geom_beeswarm을 사용하는 대안도 있습니다. 이 플롯은 그림 3.18의 (b) 패널에 나와 있습니다. 배치 알고리즘은 점들 사이의 겹침을 피하는 것을 목표로 합니다. 점이 기존 점과 겹치게 되면, 겹침을 피할 수 있을 만큼만 (x)축 방향으로 조금 이동시킵니다. 점 그림이나 벌떼 그림을 보기 좋게 만들려면 대개 새로운 데이터 세트마다 배치 매개변수를 약간씩 조정해야 합니다.\n\n\n5.5.4 3.6.4 밀도 그림(Density plots)\n동일한 데이터를 나타내는 또 다른 방법은 밀도 그림입니다. 여기서는 데이터 포인트를 평활화(smoothing)하여 기저의 데이터 생성 밀도를 추정하려고 시도합니다(그림 3.19).\nggplot(genes, aes( x = value, color = gene)) + geom_density()__\n\nFigure 3.19: 밀도 그림.\n그림 3.17—3.19에서 볼 수 있듯이, 상자 그림은 단봉(unimodal) 데이터에는 꽤 잘 작동하지만 데이터 분포가 다봉(multimodal)일 경우에는 오해를 불러일으킬 수 있으며, 긴 꼬리(long-tailed) 분포의 특성을 항상 공정하게 보여주지는 못합니다. 데이터 포인트나 그 밀도를 직접 보여주는 그림 3.18—3.19는 이러한 특징을 더 잘 보여줍니다. 예를 들어 Fgf4와 Sox2의 데이터는 이봉(bimodal) 분포임을 알 수 있고, Gata4와 Gata6은 대부분의 값이 기준선 근처에 모여 있지만 일부 세포들은 넓은 값 범위에 걸쳐 발현 수준이 높아져 있음을 알 수 있습니다.\n그러나 밀도 추정에는 여러 복잡한 문제가 있습니다. 특히 평활 창(smoothing window)의 크기를 선택해야 합니다. 데이터가 밀집된 영역의 봉우리를 포착할 만큼 작은 창 크기를 선택하면 다른 영역에서는 불안정한(“구불구불한”) 추정치가 나올 수 있습니다. 반면 창을 너무 크게 잡으면 뾰족한 봉우리와 같은 밀도의 뚜렷한 특징들이 뭉개질 수 있습니다. 또한 밀도 선은 밀도를 추정하는 데 얼마나 많은 데이터가 사용되었는지에 대한 정보를 전달하지 않으므로, 그림 3.19와 같은 플롯은 곡선들 간의 샘플 크기가 다를 경우 특히 문제가 될 수 있습니다.\n\n\n5.5.5 3.6.5 바이올린 그림(Violin plots)\n그림 3.19와 같은 밀도 그림은 밀도가 여러 개일 경우 복잡해 보일 수 있습니다. 한 가지 아이디어는 상자 그림에서 영감을 얻은 레이아웃으로 밀도를 배치하는 것인데, 이것이 바로 바이올린 그림입니다(그림 3.20). 여기서는 밀도 추정치를 사용하여 대칭적인 모양을 그리는데, 이 모양이 때때로 시각적으로 바이올린을 연상시킵니다.\np + geom_violin()__\n\nFigure 3.20: 바이올린 그림.\n\n\n5.5.6 3.6.6 능선 그림(Ridgeline plots)\n밀도 그림의 또 다른 변형은 능선 그림입니다(그림 3.21).\nlibrary(\"ggridges\")\nggplot(genes, aes(x = value, y = gene, fill = gene)) + \n  geom_density_ridges()__\n\nFigure 3.21: 능선 그림.\n이러한 방식의 표시는 표시해야 할 밀도의 수가 수십 개에 달할 때 가장 적합합니다.\ntop42 = order(rowMeans(Biobase::exprs(x)), decreasing = TRUE)[1:42]\ng42 = melt(Biobase::exprs(x)[rev(top42), ], varnames = c(\"probe\", \"sample\"))\nggplot(g42, aes(x = value, y = probe)) __\n\n그림 3.22: 그림 3.21과 유사하지만 더 많은 유전자를 포함하고 있습니다.\n\n\n5.5.7 3.6.7 ECDF 그림\n1차원 확률 변수 (X)의 분포를 설명하는 수학적으로 가장 편리한 방법은 누적 분포 함수(cumulative distribution function, CDF)입니다. 즉, 다음과 같이 정의되는 함수입니다.\n[ F(x) = P(X x), ]\n여기서 (x)는 실수축 전체의 값을 가집니다. (X)의 밀도는 (F)가 존재할 경우 (F)의 도함수입니다9. 식 3.1의 확률에 대한 유한 표본 버전은 경험적 누적 분포 함수(empirical cumulative distribution function, ECDF)라고 합니다.\n9 정의에 따라 (F)는 (x)가 작아질수록 0에 수렴하고((x -)), (x)가 커질수록 1에 수렴합니다((x +)).\n[ F_{n}(x) = = _{i=1}^n 𝟙({x x_i}), ]\n여기서 (x_1, …, x_n)은 (X)에서 추출한 (n)개의 표본을 나타내고, (𝟙)은 인수의 표현식이 참이면 1, 아니면 0의 값을 갖는 지시 함수(indicator function)입니다. 이 설명이 추상적으로 들린다면 다음 예제(그림 3.23)를 통해 좀 더 직관적으로 이해할 수 있을 것입니다.\nsimdata = rnorm(70)\ntibble(index = seq(along = simdata),\n          sx = sort(simdata)) %&gt;%\nggplot(aes(x = sx, y = index)) + geom_step()__\n\nFigure 3.23: Sorted values of simdata versus their index. This is the empirical cumulative distribution function of simdata.\nPlotting the sorted values against their ranks gives the essential features of the ECDF (Figure 3.23). In practice, we do not need to do the sorting and the other steps in the above code manually and will rather use the stat_ecdf() geometric object. The ECDFs of our data are shown in Figure 3.24.\nggplot(genes, aes( x = value, color = gene)) + stat_ecdf()__\n\nFigure 3.24: Empirical cumulative distribution functions (ECDF).\nECDF는 몇 가지 유용한 속성을 가지고 있습니다:\n\n무손실(Lossless): ECDF \\(F_{n}(x)\\)는 값의 순서(중요하지 않다고 가정함)를 제외하고 원래 표본 \\(x_1,…,x_n\\)에 포함된 모든 정보를 유지합니다.\n\\(n\\)이 커짐에 따라 ECDF \\(F_{n}(x)\\)는 실제 CDF \\(F(x)\\)로 수렴합니다. 제한된 표본 크기 \\(n\\)의 경우에도 두 함수 사이의 차이는 작은 경향이 있습니다. 이는 경험적 밀도(empirical density)의 경우에는 해당되지 않는다는 점에 유의하세요! 평활화(smoothing)가 없으면 유한 표본의 경험적 밀도는 디랙 델타 함수(Dirac delta functions)의 합으로 나타나며, 이는 시각화하기 어렵고 기저의 매끄러운 실제 밀도와는 상당히 다릅니다. 평활화를 사용하면 그 차이가 덜 두드러질 수 있지만, 앞서 논의한 것처럼 이를 제어하기는 어렵습니다.\n\n\n그림 3.25: Lawrence 등 (2013)의 그림 1의 일부. 각 점은 종양-정상 쌍에 대응하며, 수직 위치는 엑솜(exome) 내 체세포 돌연변이의 총 빈도를 나타냅니다. 결과 곡선은 본질적으로 ECDF 그림이며, 개념적으로 이 플롯은 그림 3.24와 유사하지만 그래프가 90도 회전(즉, \\(x\\)-와 \\(y\\)-축의 역할이 바뀜)되어 있고 개별 종양 유형에 대한 곡선이 서로 더 잘 분리되도록 수평으로 이동되어 있다는 점이 다릅니다.\n__\n태스크\n티블(tibbles). 위 코드에서 처음으로 tibble을 접했습니다. tibble 패키지의 비네트(vignette)를 살펴보고 그 기능을 확인해 보세요.\n\n\n5.5.8 3.6.8 밀도에 대한 변환의 영향\n히스토그램이나 밀도 그림을 보고 일부 기저의 생물학적 현상의 지표로서 이봉성(bimodality)(또는 다봉성)의 증거를 조사하고 싶은 유혹이 생길 수 있습니다. 그렇게 하기 전에, 밀도의 최빈값(mode)의 수는 연쇄 법칙(chain rule)을 통해 데이터가 변환되는 척도에 따라 달라진다는 점을 기억하는 것이 중요합니다. 예를 들어, Hiiragi 데이터 세트의 어레이 중 하나에서 얻은 데이터를 살펴보겠습니다(그림 3.26).\nggplot(dfx, aes(x = `64 E4.5 (EPI)`)) + geom_histogram(bins = 100)\nggplot(dfx, aes(x = 2 ^ `64 E4.5 (EPI)`)) + \n  geom_histogram(binwidth = 20) + xlim(0, 1500)__\n[](03-chap_files/figure-html/fig-graphics-onedtrsf-1.png “그림 3.26 (a):”)\n\n\n\n[](03-chap_files/figure-html/fig-graphics-onedtrsf-2.png “그림 3.26 (b):”)\n\n\n\n그림 3.26: 로그 변환 여부에 따른 동일한 데이터의 히스토그램. (a) 데이터는 데이터 객체 x에 저장된 척도로 표시되며, 이는 마이크로어레이 형광 강도를 로그(밑 2) 변환한 결과입니다(Irizarry et al. 2003). (b) 다시 지수화하여 원래의 형광 척도로 되돌린 후의 모습입니다. 공간을 더 잘 활용하기 위해 (x)축의 범위를 1500으로 제한했습니다.\n__\n질문 3.5\n(심화:) 확률 변수 \\(X\\)와, 변환된 확률 변수 \\(Y = f(X)\\)를 정의하는 비선형 1:1 변환 \\(f: x y\\)를 생각해 봅시다. \\(Y\\)의 밀도 함수를 \\(p(y)\\)라고 가정할 때, \\(X\\)의 밀도는 무엇일까요? \\(X\\)의 최빈값(또는 최빈값들)은 \\(Y\\)의 최빈값(들)과 어떤 관련이 있을까요?\n힌트: 함수 \\(p\\)의 최빈값은 그 도함수 \\(p’=dp/dx\\)의 근(root)이라는 점에 유의하세요. \\(x_0\\)이 \\(X\\)의 최빈값이라면 \\(y_0=f(x_0)\\)이 \\(Y\\)의 최빈값이라는 것이 일반적으로 성립할까요?\n__\n해결책\n__\n연쇄 법칙에 따르면 \\(p(y), dy = p(f(x)),f’(x),dx\\)이므로, \\(X\\)의 밀도는 \\((x) = p(f(x)),f’(x)\\)입니다. \\(\\)의 최빈값은 그 도함수 \\(d/dx\\)의 근이며, 즉 이들은 \\(p’(f(x)),f’^2(x) + p(f(x))f”(x) = 0\\)을 따릅니다. 합의 두 번째 항은 \\(f\\)가 아핀 선형(affine linear, \\(f”\\))인 경우 사라지지만, 일반적으로 두 밀도의 근 사이에는, 따라서 \\(X\\)와 \\(Y\\)의 최빈값 사이에는 간단한 관계가 없습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "03-chap.html#차원-데이터-시각화-산점도scatterplots",
    "href": "03-chap.html#차원-데이터-시각화-산점도scatterplots",
    "title": "5  3.1 이 장의 목표",
    "section": "5.6 3.7 2차원 데이터 시각화: 산점도(Scatterplots)",
    "text": "5.6 3.7 2차원 데이터 시각화: 산점도(Scatterplots)\n산점도는 처리-반응 비교(그림 3.3에서처럼), 변수 간의 연관성(그림 3.10에서처럼), 또는 쌍을 이룬 데이터(예: 치료 전후 여러 환자의 질병 바이오마커)를 시각화하는 데 유용합니다. 두 변수를 나타내기 위해 플로팅 종이나 화면의 두 차원을 사용합니다. 야생형과 FGF4-KO 샘플 사이의 차등 발현을 살펴보겠습니다.\nscp = ggplot(dfx, aes(x = `59 E4.5 (PE)` ,\n                      y = `92 E4.5 (FGF4-KO)`))\nscp + geom_point()__\n\n그림 3.27: 두 샘플에 대한 45,101개 발현 측정값의 산점도.\n레이블 59 E4.5 (PE)와 92 E4.5 (FGF4-KO)는 위에서 만든 데이터 프레임 dfx의 열 이름(샘플 이름)을 나타냅니다. 이들은 특수 문자(공백, 괄호, 하이픈)를 포함하고 숫자로 시작하기 때문에, R이 구문적으로 이해할 수 있도록 역따옴표(backtick)로 묶어줄 필요가 있습니다. 플롯은 그림 3.27에 나와 있습니다. 밀집된 점 구름(point cloud)을 얻게 되는데, 구름의 외곽 부분은 해석을 시도해 볼 수 있겠지만, 플롯의 밀집된 영역 내에서 데이터가 어떻게 분포되어 있는지는 시각적으로 알 방법이 없습니다.\n겹쳐그리기(overplotting) 문제를 개선하는 한 가지 쉬운 방법은 geom_point의 alpha 매개변수를 수정하여 점들의 투명도(알파 값)를 조절하는 것입니다(그림 3.28).\nscp  + geom_point(alpha = 0.1)__\n\n그림 3.28: 그림 3.27과 유사하지만, 일부 겹쳐그리기를 해결하기 위해 반투명한 점을 사용했습니다.\n이것은 이미 그림 3.27보다 낫지만, 더 밀집된 영역에서는 반투명한 점들조차 특징 없는 검은 덩어리로 겹쳐지는 반면, 더 고립된 바깥쪽 점들은 희미해집니다. 대안으로는 2D 밀도의 등고선 그림(contour plot)이 있으며, 이는 그림 3.29에서처럼 플롯의 모든 점을 다 그리지 않아도 된다는 장점이 있습니다.\n하지만 그림 3.29를 보면 오른쪽 하단의 점 구름(상대적으로 적은 수의 점을 포함)이 더 이상 표현되지 않는 것을 알 수 있습니다. geom_density2d의 대역폭(bandwidth)과 구간화(binning) 매개변수를 조정하여 이 문제를 어느 정도 극복할 수 있습니다(그림 3.30, 왼쪽 패널).\nscp + geom_density2d()__\n\n그림 3.29: 그림 3.27과 유사하지만, 2D 밀도 추정치의 등고선 그림으로 렌더링되었습니다.\nscp + geom_density2d(h = 0.5, bins = 60)\nlibrary(\"RColorBrewer\")\ncolorscale = scale_fill_gradientn(\n    colors = rev(brewer.pal(9, \"YlGnBu\")),\n    values = c(0, exp(seq(-5, 0, length.out = 100))))\n\nscp + stat_density2d(h = 0.5, bins = 60,\n          aes( fill = after_stat(level)), geom = \"polygon\") +\n  colorscale + coord_fixed()__\n[](03-chap_files/figure-html/fig-graphics-twodsp4-1.png “그림 3.30 (a):”)\n\n\n\n[](03-chap_files/figure-html/fig-graphics-twodsp4-2.png “그림 3.30 (b):”)\n\n\n\n그림 3.30: 2D 밀도 그림. (a) 그림 3.29와 같지만 더 작은 평활 대역폭과 더 촘촘한 등고선 구간을 사용했습니다. (b) 색상 채우기가 추가된 모습입니다.\n그림 3.30의 오른쪽 패널에서처럼, stat_density2d 함수(geom_density2d의 래퍼 함수)를 명시적으로 호출하고 기하학적 객체로 _polygon_을 사용함으로써 등고선 사이의 각 공간을 점들의 상대적 밀도로 채울 수 있습니다.\n우리는 RColorBrewer 패키지의 brewer.pal 함수를 사용해 색상 척도를 정의했고, coord_fixed를 호출하여 플롯의 종횡비(aspect ratio)를 고정했습니다. 이는 두 변수에 대해 데이터 범위에서 (x) 및 (y) 좌표로의 매핑이 동일하게 이루어지도록 보장합니다. 이 두 이슈 모두 더 깊이 살펴볼 가치가 있으며, 3.7.1절에서는 플롯의 모양에 대해, 3.9절에서는 색상에 대해 더 이야기하겠습니다.\n그림 3.30의 밀도 기반 플로팅 방법은 그림 3.27 및 3.28의 겹쳐진 점 구름보다 시각적으로 더 매력적이고 해석하기 쉽지만, 플롯의 희소한 영역에 있는 이상치 점들에 대한 정보를 많이 잃게 되므로 주의해서 사용해야 합니다. 한 가지 방법은 geom_point를 사용하여 그러한 점들을 다시 추가하는 것입니다.\n하지만 평활화의 한계를 피할 수 있는 아마도 가장 좋은 대안은 육각형 구간화(hexagonal binning)일 것입니다(Carr et al. 1987).\nscp + geom_hex() + coord_fixed()\nscp + geom_hex(binwidth = c(0.2, 0.2)) + colorscale +\n  coord_fixed()__\n[](03-chap_files/figure-html/fig-graphics-twodsp6-1.png “그림 3.31 (a):”)\n\n\n\n[](03-chap_files/figure-html/fig-graphics-twodsp6-2.png “그림 3.31 (b):”)\n\n\n\n그림 3.31: 육각형 구간화. (a) 기본 매개변수 사용. (b) 더 미세한 구간 크기와 사용자 정의 색상 척도 사용.\n\n5.6.1 3.7.1 플롯 모양(Plot shapes)\n플롯에 적절한 모양을 선택하는 것은 정보가 잘 전달되도록 하는 데 중요합니다. 기본적으로 그래프의 높이와 너비 사이의 비율인 모양 매개변수(shape parameter)는 현재 플로팅 장치(plotting device)에서 사용 가능한 공간을 기반으로 ggplot2에 의해 선택됩니다. 장치의 너비와 높이는 명시적으로 지정하거나 기본 매개변수10를 통해 R에서 열릴 때 지정됩니다. 더욱이 그래프의 크기는 그림 3.31의 색상 척도 막대와 같은 추가적인 장식 요소의 유무에 따라서도 달라집니다.\n10 See for example the manual pages of the pdf and png functions.\nThere are two simple rules that you can apply for scatterplots:\n\nIf the variables on the two axes are measured in the same units, then make sure that the same mapping of data space to physical space is used – i.e., use coord_fixed. In the scatterplots above, both axes are the logarithm to base 2 of expression level measurements; that is, a change by one unit has the same meaning on both axes (a doubling of the expression level). Another case is principal component analysis (PCA), where the \\(x\\)-axis typically represents component 1, and the \\(y\\)-axis component 2. Since the axes arise from an orthonormal rotation of input data space, we want to make sure their scales match. Since the variance of the data is (by definition) smaller along the second component than along the first component (or at most, equal), well-made PCA plots usually have a width that’s larger than the height.\nIf the variables on the two axes are measured in different units, then we can still relate them to each other by comparing their dimensions. The default in many plotting routines in R, including ggplot2 , is to look at the range of the data and map it to the available plotting region. However, in particular when the data more or less follow a line, looking at the typical slope of the line can be useful. This is called banking (William S. Cleveland, McGill, and McGill 1988).\n\nTo illustrate banking, let’s use the classic sunspot data from Cleveland’s paper.\nlibrary(\"ggthemes\")\nsunsp = tibble(year   = time(sunspot.year),\n               number = as.numeric(sunspot.year))\nsp = ggplot(sunsp, aes(x = year, y = number)) + geom_line()\nsp\nratio = with(sunsp, bank_slopes(year, number))\nsp + coord_fixed(ratio = ratio)__\n[](03-chap_files/figure-html/fig-graphics-banking-1.png “Figure 3.32 (a):”)\n\n\n\n[](03-chap_files/figure-html/fig-graphics-banking-2.png “Figure 3.32 (b):”)\n\n\n\nFigure 3.32: The sunspot data. In (a), the plot shape is roughly quadratic, a frequent default choice. In (b), a technique called banking was used to choose the plot shape. (Note: the placement of the tick labels is not great in this plot and would benefit from customization.)\nThe resulting plot is shown in the upper panel of Figure 3.32. We can clearly see long-term fluctuations in the amplitude of sunspot activity cycles, with particularly low maximum activities in the early 1700s, early 1800s, and around the turn of the 20\\(^\\) century. But now lets try out banking.\nHow does the algorithm work? It aims to make the slopes in the curve be around one. In particular, bank_slopes computes the median absolute slope, and then with the call to coord_fixed we set the aspect ratio of the plot such that this quantity becomes 1. The result is shown in the lower panel of Figure 3.32. Quite counter-intuitively, even though the plot takes much smaller space, we see more on it! In particular, we can see the saw-tooth shape of the sunspot cycles, with sharp rises and more slow declines.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "03-chap.html#차원-이상의-시각화",
    "href": "03-chap.html#차원-이상의-시각화",
    "title": "5  3.1 이 장의 목표",
    "section": "5.7 3.8 2차원 이상의 시각화",
    "text": "5.7 3.8 2차원 이상의 시각화\n때로는 두 개 이상의 변수 간의 관계를 보여주고 싶을 때가 있습니다. 추가적인 차원을 포함하기 위한 당연한 선택은 플롯 기호의 모양과 색상입니다. geom_point 기하학적 객체는 ((x)와 (y) 외에) 다음과 같은 미학(aesthetics)을 제공합니다:\n\nfill\ncolor\nshape\nsize\nalpha\n\n이들은 geom_point 함수의 매뉴얼 페이지에서 살펴볼 수 있습니다. fill과 color는 객체의 채우기 및 테두리 색상을 의미하며, alpha는 투명도를 나타냅니다. 앞서 그림 3.28 등에서 우리는 점의 밀도를 반영하고 겹쳐그리기(overplotting)에 의한 가려짐 효과를 피하기 위해 색상이나 투명도를 사용했습니다. 또한 이러한 속성들을 사용하여 데이터의 다른 차원을 보여줄 수도 있습니다. 원칙적으로는 위에 나열된 5가지 미학을 모두 동시에 사용하여 최대 7차원의 데이터를 보여줄 수 있지만, 그러한 플롯은 해독하기가 매우 어려울 것입니다. 대개는 이 미학들 중 한두 가지만 선택하여 데이터의 추가적인 한두 차원을 보여주는 정도로 제한하는 것이 좋습니다.\n\n5.7.1 3.8.1 패싯(Faceting)\n\n\n\n때로는 이러한 플롯 배열의 모양을 빗대어 트렐리스(trellis) 또는 래티스(lattice) 그래픽이라고도 부릅니다. 패싯을 구현한 첫 번째 주요 R 패키지는 lattice였습니다. 이 책에서는 ggplot2를 통해 제공되는 패싯 기능을 사용할 것입니다.\n\n\n때로는 이러한 플롯 배열의 모양을 빗대어 트렐리스(trellis) 또는 래티스(lattice) 그래픽이라고도 부릅니다. 패싯을 구현한 첫 번째 주요 R 패키지는 lattice였습니다. 이 책에서는 ggplot2를 통해 제공되는 패싯 기능을 사용할 것입니다.\n데이터의 추가적인 차원을 보여주는 또 다른 방법은 하나(또는 그 이상)의 변수를 기반으로 데이터를 반복적으로 하위 집합화(subsetting, 또는 “슬라이싱”)하여 여러 개의 플롯을 보여줌으로써 각 부분을 따로 시각화하는 것입니다. 이를 패싯(faceting)이라고 하며, 이를 통해 최대 4~5차원의 데이터를 시각화할 수 있습니다. 예를 들어, 패싯 변수의 범위에 따라 다른 변수들 간에 관찰된 패턴이 동일한지 아니면 다른지 조사할 수 있습니다. 예제를 살펴보겠습니다:\nlibrary(\"magrittr\")\ndftx$lineage %&lt;&gt;% sub(\"^$\", \"no\", .)\ndftx$lineage %&lt;&gt;% factor(levels = c(\"no\", \"EPI\", \"PE\", \"FGF4-KO\"))\n\nggplot(dftx, aes(x = X1426642_at, y = X1418765_at)) +\n  geom_point() + facet_grid( . ~ lineage )__\n\n그림 3.33: 패싯(faceting)의 예시: 그림 3.9와 동일한 데이터이지만, 이제 범주형 변수 lineage에 의해 분할되었습니다.\n결과는 그림 3.33에 나와 있습니다. 우리는 R의 포뮬러(formula) 언어를 사용하여 데이터를 분할할 변수를 지정했고, 개별 패널이 서로 다른 열에 배치되도록 했습니다: facet_grid( . ~ lineage ). 사실 다음과 같이 두 개의 패싯 변수를 지정할 수도 있습니다. 그 결과는 그림 3.34에 나와 있습니다.\nggplot(dftx,\n  aes(x = X1426642_at, y = X1418765_at)) + geom_point() +\n   facet_grid( Embryonic.day ~ lineage )__\n\n그림 3.34: 패싯(Faceting): 그림 3.9와 동일한 데이터이지만, 범주형 변수 Embryonic.day(행)와 lineage(열)에 의해 분할되었습니다.\n또 다른 유용한 함수는 facet_wrap입니다. 패싯 변수의 수준(level)이 너무 많아 모든 플롯을 한 행이나 한 열에 맞추기 어려울 때, 이 함수를 사용하여 지정된 수의 열이나 행으로 배열할 수 있습니다. 지금까지는 범주형 변수에 의한 패싯을 보았지만, 연속형 변수를 수준별로 이산화하여 사용할 수도 있습니다. 이를 위해 cut 함수가 유용합니다.\nggplot(mutate(dftx, Tdgf1 = cut(X1450989_at, breaks = 4)),\n   aes(x = X1426642_at, y = X1418765_at)) + geom_point() +\n   facet_wrap( ~ Tdgf1, ncol = 2 )__\n\n그림 3.35: 패싯(Faceting): 그림 3.9와 동일한 데이터이지만, 연속형 변수 X1450989_at에 의해 분할되고 facet_wrap에 의해 배열되었습니다.\n그림 3.35에서 네 패널의 점 개수가 서로 다른 것을 볼 수 있습니다. 이는 cut이 점 개수가 아니라 구간의 길이를 동일하게 나누기 때문입니다. 만약 점 개수를 동일하게 맞추고 싶다면 cut과 함께 quantile을 사용하거나, 변수 값의 순위(rank)를 기준으로 나눌 수 있습니다.\n\n5.7.1.1 축 스케일(Axes scales)\n그림 3.33—3.35에서 축의 스케일은 모든 패널에서 동일합니다. 대신 facet_grid와 facet_wrap 함수의 scales 인수를 설정하여 이를 변경할 수 있습니다. 이 인수를 통해 각 패널의 (x)축과 (y)축이 동일한 스케일을 가질지, 아니면 각 패널의 데이터 범위에 맞게 조정될지 제어할 수 있습니다. 여기에는 상충 관계(trade-off)가 있습니다. 축 스케일을 패널별로 조정하면 세부 사항을 더 잘 볼 수 있는 반면, 그룹 간 패널 비교는 더 어려워집니다.\n\n\n5.7.1.2 암시적 패싯(Implicit faceting)\nfacet_grid나 facet_wrap을 명시적으로 호출하지 않고도 미학(aesthetics)을 지정하여 플롯을 패싯할 수 있습니다. 암시적 패싯의 매우 간단한 예는 그림 3.15—3.18에서처럼 요인(factor)을 (x)축으로 사용하는 것입니다.\n\n\n\n5.7.2 3.8.2 대화형 그래픽\n지금까지 생성된 플롯은 정적인 이미지였습니다. 플롯을 대화형(interactive)으로 만들면 엄청난 양의 정보와 표현력을 더할 수 있습니다. 여기서는 대화형 시각화를 깊이 있게 다루지는 않겠지만, 몇 가지 중요한 자료를 소개하겠습니다. 이는 역동적인 분야이므로 독자들은 최신 동향을 파악하기 위해 R 생태계를 탐색해 보아야 합니다.\n\n5.7.2.1 Shiny\nPosit(구 RStudio)의 shiny는 R용 웹 애플리케이션 프레임워크입니다. 대화형 요소가 플롯을 생성하는 R 코드를 직접 호출하므로, 슬라이더, 선택기 및 기타 제어 요소를 사용하여 표시되는 플롯의 모든 측면을 변경할 수 있는 대화형 디스플레이를 쉽게 만들 수 있습니다. 훌륭한 예시들을 Shiny 갤러리에서 확인해 보세요.\nShiny 기반 대화형 시각화를 위한 그래픽 엔진으로 ggplot2를 사용할 수 있으며, 사실 기본 R 그래픽이나 다른 그래픽 패키지도 사용할 수 있습니다. 여기서 약간 어색한 점은 대화형 옵션을 설명하는 데 사용되는 언어가 ggplot2를 통한 그래픽 생성 및 그래픽의 문법과 분리되어 있다는 것입니다. ggvis 패키지는 이러한 한계를 극복하는 것을 목표로 합니다.\n\n\n5.7.2.2 ggvis\nggvis11는 ggplot2의 장점들을 대화형 그래픽 영역으로 확장하려는 시도입니다. ggplot2가 R의 전통적인 그래픽 장치(PDF, PNG 등)에 그래픽을 생성하는 것과 대조적으로, ggvis는 Vega라는 JavaScript 인프라를 기반으로 하며, 그 플롯은 웹 브라우저에서 볼 수 있도록 의도되었습니다. ggplot2와 마찬가지로 ggvis 패키지도 그래픽의 문법 개념에서 영감을 받았지만, 별개의 구문을 사용합니다. 상호 작용에 필요한 계산을 수행하기 위해 R에 연결하는 Shiny의 인프라를 활용합니다. 저자가 말했듯이12, 목표는 R의 최고(예: 상상할 수 있는 모든 모델링 기능)와 웹의 최고(모든 사람이 웹 브라우저를 가지고 있음)를 결합하는 것입니다. 데이터 조작 및 변환은 R에서 수행되며, 그래픽은 Vega를 사용하여 웹 브라우저에서 렌더링됩니다.\n11 이 글을 쓰는 시점(2017년 여름)에는 ggvis 개발의 초기 추진력이 유지될지 불분명하며, 현재 기능과 완성도는 아직 ggplot2에 미치지 못합니다.\n12 https://ggvis.rstudio.com\nShiny와 ggvis의 상호 작용 결과로, 사용자가 그래픽을 보는 동안 사용자의 동작에 응답하기 위해 기본 데이터와 코드를 실행하는 R 인터프리터가 필요합니다. 이 R 인터프리터는 로컬 머신이나 서버에 있을 수 있습니다. 두 경우 모두 뷰어 애플리케이션은 웹 브라우저이며, R과의 상호 작용은 웹 프로토콜(HTTP 또는 HTTPS)을 통해 이루어집니다. 물론 이는 R에 의해 한 번 생성되어 실행 중인 R 인스턴스에 연결되지 않고 PDF 또는 HTML 뷰어에서 볼 수 있는 자체 포함 파일에 저장된 그래픽과는 다릅니다.\n\n\n5.7.2.3 plotly\n대화형 그래픽 생성을 위한 또 다른 훌륭한 웹 기반 도구는 plotly입니다. 온라인에서 대화형 그래픽의 몇 가지 예를 볼 수 있습니다. R에서 자신만의 대화형 플롯을 만들려면 다음과 같은 코드를 사용할 수 있습니다:\nlibrary(\"plotly\")\nplot_ly(economics, x = ~date, y = ~unemploy/pop)__\nShiny 및 ggvis와 마찬가지로, 그래픽은 웹 브라우저에서 볼 수 있습니다. 그러나 실행 중인 R 세션이 반드시 필요한 것은 아닙니다. 그래픽은 로직이 JavaScript 또는 D3.js 시스템으로 코딩된 자체 포함 HTML 문서로 구성될 수 있습니다.\n\n그림 3.36: rgl을 이용한 volcano 데이터 렌더링. 오클랜드 화산 지대의 약 50개 화산 중 하나인 Maunga Whau(Mt Eden)의 지형 정보입니다.\n\n\n5.7.2.4 rgl, webgl\n3D 객체(예: 기하학적 구조)를 시각화하려면 rgl 패키지가 있습니다. 이 패키지는 화면의 특수 그래픽 창이나 웹 브라우저를 통해 장면을 회전시키거나 확대/축소할 수 있는 대화형 뷰어를 생성합니다. 아래 코드에서 생성된 장면의 스크린샷이 그림 3.36에 나와 있습니다. 이러한 스크린샷은 snapshot3d 함수를 사용하여 생성할 수 있습니다.\ndata(\"volcano\")\nvolcanodata = list(\n  x = 10 * seq_len(nrow(volcano)),\n  y = 10 * seq_len(ncol(volcano)),\n  z = volcano,\n  col = terrain.colors(500)[cut(volcano, breaks = 500)]\n)\nlibrary(\"rgl\")\nwith(volcanodata, persp3d(x, y, z, color = col))__\n위의 코드에서 기본 R 함수 cut은 volcano 데이터의 값 범위에서 1부터 500 사이의 정수로의 매핑을 계산하며13, 우리는 이 정수들을 사용하여 terrain.colors(500)에서 색상 스케일을 인덱싱합니다. 자세한 내용은 이 패키지의 훌륭한 비네트(vignettes)를 참조하세요.\n13 더 정확하게는, cut은 500개의 수준(level)을 가진 요인(factor)을 반환하며, 우리는 이를 정수로 강제 변환(coerce)할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "03-chap.html#색상color",
    "href": "03-chap.html#색상color",
    "title": "5  3.1 이 장의 목표",
    "section": "5.8 3.9 색상(Color)",
    "text": "5.8 3.9 색상(Color)\n플롯을 만들 때 중요하게 고려해야 할 사항 중 하나는 플롯에 사용하는 색상입니다. 대부분의 R 사용자는 그림 3.37에 표시된 것과 같이 기본 R 그래픽에서 사용하는 내장 색상 체계에 익숙할 것입니다.\npie(rep(1, 8), col=1:8)__\n\n그림 3.37: 기본 R 색상 팔레트의 처음 8가지 색상.\n이 색상 팔레트의 기원은 1980년대 하드웨어로 거슬러 올라갑니다. 당시 그래픽 카드는 음극선관(CRT)의 세 가지 기본 색상 채널인 빨강, 초록, 파랑(RGB)을 각각 켜거나 끄는 방식으로 색상을 처리했습니다. 이로 인해 RGB 색상 입방체의 여덟 개 모서리에서 \\(2^3=8\\)가지 조합이 만들어졌습니다14.\n14 따라서 그림 3.37의 8번째 색상은 흰색이어야 하지만, 대신 회색으로 표시됩니다.\nThe first eight colors for a categorical variable in ggplot2 are shown in Figure 3.38:\nggplot(tibble(u = factor(1:8), v = 1), \n       aes(x = \"\",  y = v, fill = u)) +\n  geom_bar(stat = \"identity\", width = 1) + \n  coord_polar(\"y\", start = 0) + theme_void()__\n\nFigure 3.38: The first eight colors in the ggplot2 color palette.\nThese defaults are appropriate for simple use cases, but often we will want to make our own choices. In Section 3.7 we already saw the function scale_fill_gradientn, with which we created the smooth-looking color gradient used in Figures 3.30 and 3.31 by interpolating a set of color steps provided by the function brewer.pal in the RColorBrewer package. This package defines a set of purpose-designed color palettes. We can see all of them at a glance with the function display.brewer.all (Figure 3.39).\ndisplay.brewer.all()__\n\nFigure 3.39: RColorBrewer palettes.\nWe can get information about the available color palettes from brewer.pal.info.\nhead(brewer.pal.info)__\n\n\n     maxcolors category colorblind\nBrBG        11      div       TRUE\nPiYG        11      div       TRUE\nPRGn        11      div       TRUE\nPuOr        11      div       TRUE\nRdBu        11      div       TRUE\nRdGy        11      div      FALSE\n\n\ntable(brewer.pal.info$category)__\n\n\n div qual  seq \n   9    8   18 \nThe palettes are divided into three categories:\n\nqualitative: for categorical properties that have no intrinsic ordering. The Paired palette supports up to 6 categories that each fall into two subcategories - such as before and after , with and without treatment, etc.\nsequential: for quantitative properties that go from low to high\ndiverging: for quantitative properties for which there is a natural midpoint or neutral value, and whose value can deviate both up- and down; we’ll see an example in Figure 3.41.\n\nTo obtain the colors from a particular palette we use the function brewer.pal. Its first argument is the number of colors we want (which can be less than the available maximum number in brewer.pal.info).\nbrewer.pal(4, \"RdYlGn\")__\n\n\n[1] \"#D7191C\" \"#FDAE61\" \"#A6D96A\" \"#1A9641\"\nIf we want more than the available number of preset colors (for example so we can plot a heatmap with continuous colors) we can interpolate using the colorRampPalette function15.\n15 colorRampPalette returns a function of one parameter, an integer. In the code shown, we call that function with the argument 100.\nmypalette  = colorRampPalette(\n    c(\"darkorange3\", \"white\",\"darkblue\")\n  )(100)\nhead(mypalette)__\n\n\n[1] \"#CD6600\" \"#CE6905\" \"#CF6C0A\" \"#D06F0F\" \"#D17214\" \"#D27519\"\n\n\nimage(matrix(1:100, nrow = 100, ncol = 10), col = mypalette,\n        xaxt = \"n\", yaxt = \"n\", useRaster = TRUE)__\n\nFigure 3.40: A quasi-continuous color palette derived by interpolating between the colors darkorange3, white and darkblue.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "03-chap.html#heatmaps",
    "href": "03-chap.html#heatmaps",
    "title": "5  3.1 이 장의 목표",
    "section": "5.9 3.10 Heatmaps",
    "text": "5.9 3.10 Heatmaps\nHeatmaps are a powerful way of visualizing large, matrix-like datasets and providing a quick overview of the patterns that might be in the data. There are a number of heatmap drawing functions in R; one that is convenient and produces a good-looking output is the function pheatmap from the eponymous package16. In the code below, we first select the top 500 most variable genes in the dataset x and define a function rowCenter that centers each gene (row) by subtracting the mean across columns. By default, pheatmap uses the RdYlBu color palette from RcolorBrewer in conjuction with the colorRampPalette function to interpolate the 11 colors into a smooth-looking palette (Figure 3.41).\n16 A very versatile and modular alternative is the ComplexHeatmap package.\nlibrary(\"pheatmap\")\ntopGenes = order(rowVars(Biobase::exprs(x)), decreasing = TRUE)[1:500]\nrowCenter = function(x) { x - rowMeans(x) }\npheatmap(rowCenter(dfx[topGenes, ]), \n  show_rownames = FALSE, \n  show_colnames = FALSE, \n  breaks = seq(-5, +5, length = 101),\n  annotation_col = pData(x)[, c(\"sampleGroup\", \"Embryonic.day\", \"ScanDate\", \"genotype\") ],\n  annotation_colors = list(\n    sampleGroup = groupColor,\n    genotype = c(`FGF4-KO` = \"chocolate1\", `WT` = \"azure2\"),\n    Embryonic.day = setNames(brewer.pal(9, \"Blues\")[c(3, 6, 9)], c(\"E3.25\", \"E3.5\", \"E4.5\")),\n    ScanDate = setNames(brewer.pal(nlevels(x$ScanDate), \"YlGn\"), levels(x$ScanDate))\n  )\n)__\n\nFigure 3.41: A heatmap of relative expression values, i.e., logarithmic fold change compared to the average expression of that gene (row) across all samples (columns). The color scale uses a diverging palette whose midpoint is at 0.\nLet’s take a minute to deconstruct this rather massive-looking call to pheatmap. The options show_rownames and show_colnames control whether the row and column names are printed at the sides of the matrix. Because our matrix is large in relation to the available plotting space, the labels would not be readable, so we might as well suppress them. The annotation_col argument takes a data frame that carries additional information about the samples. The information is shown in the colored bars on top of the heatmap. There is also a similar annotation_row argument, which we haven’t used here, to annotate the rows (genes) with colored bars at the side. The argument annotation_colors is a list of named vectors by which we can override the default choice of colors for the annotation bars. The pheatmap function has many further options, and if you want to use it for your own data visualizations, it’s worth studying them.\n\n5.9.1 3.10.1 덴드로그램 순서(Dendrogram ordering)\n그림 3.41에서 왼쪽과 상단의 나무 모양은 계층적 클러스터링 알고리즘의 결과를 나타내며 덴드로그램(dendrograms)이라고도 불립니다. 행과 열의 순서는 덴드로그램을 기반으로 합니다. 이는 히스토그램의 시각적 효과에 엄청난 영향을 미칩니다. 그러나 나타나는 패턴 중 어느 것이 실제이고 어느 것이 임의적인 트리 레이아웃 결정의 결과인지 결정하기 어려울 수 있습니다17. 다음 사항을 염두에 둡시다:\n17 우리는 5장에서 클러스터링과 클러스터의 유의성을 평가하는 방법에 대해 배울 것입니다.\n\n클러스터 덴드로그램에 따라 행과 열을 정렬하는 것(그림 3.41에서처럼)은 임의적인 선택이며, 다른 방식으로 정렬할 수도 있습니다.\n덴드로그램 순서로 결정하더라도, 각 내부 분기(internal branch)에서 어느 쪽을 왼쪽 또는 오른쪽으로 둘지는 본질적으로 임의적인 선택입니다. 트리의 위상(topology)을 바꾸지 않고도 각 분기를 뒤집을 수 있기 때문입니다(그림 5.21 참조).\n\n__\n질문 3.6\npheatmap 함수는 서브트리의 어느 분기를 왼쪽과 오른쪽으로 보낼지 결정하는 문제를 어떻게 처리하나요?\n__\n해결책\n__\n이는 기본적으로 pheatmap에서 사용되는 stats 패키지의 hclust 함수 매뉴얼 페이지에 설명되어 있습니다.\n__\n질문 3.7\n다른 어떤 정렬 방법들을 생각할 수 있나요?\n__\n해결책\n__\n제안된 방법들 중에는 외판원 문제(travelling salesman problem, McCormick Jr, Schweitzer, and White 1972)나 첫 번째 주성분으로의 투영(예를 들어 pheatmap 매뉴얼 페이지의 예제 참조) 등이 있습니다.\n__\n질문 3.8\npheatmap 함수의 clustering_callback 인수를 확인해 보세요.\n\n\n5.9.2 3.10.2 색 공간(Color spaces)\n인간의 색 지각(Helmholtz 1867)은 3차원적입니다18. 이 공간을 매개변수화하는 방법은 여러 가지가 있습니다. 위에서 우리는 이미 3.4절 시작 부분에서 groupColor의 내용을 출력할 때 [0,1] 범위의 세 값을 사용하는 RGB 색상 모델을 접했습니다.\n18 물리적으로는 빛의 파장이 무수히 많고 이를 혼합하는 방법도 무수히 많기 때문에, 다른 종이나 로봇의 경우 색 공간이 더 적거나 더 많은 차원을 가질 수 있습니다.\ngroupColor[1]__\n\n\n    E3.25 \n\"#CAB2D6\" \n여기서 CA는 빨간색 채널의 강도를 나타내는 16진수 표현이며, B2는 녹색, D6은 파란색 채널의 강도를 나타냅니다. 십진수로 변환하면 이 숫자들은 각각 202, 178, 214입니다. 이 값들의 범위는 0에서 255까지이므로, 이를 최대값으로 나누면 RGB 삼중항(triplet)을 3차원 단위 입방체 내의 한 점으로 생각할 수도 있습니다.\n\n\n\n\n\n\n\n\n그림 3.42: HCL 색 공간의 원. (a) 휘도(Luminance) \\(L\\)은 75로 고정된 상태에서 각도 좌표 \\(H\\)(색상, hue)는 0에서 360까지 변하고 반지름 좌표 \\(C\\)는 0, 10, …, 60의 값을 가집니다. (b) 크로마(Chroma) \\(C\\)는 50으로 고정되고 \\(H\\)는 위와 같으며, 반지름 좌표는 휘도 \\(L\\)로 10, 20, …, 90의 값을 가집니다.\nhcl 함수는 다른 좌표계를 사용합니다. 이 역시 색상(hue) \\(H\\)(\\([0, 360]\\) 범위의 각도), 크로마(chroma) \\(C\\)(양수), 휘도(luminance) \\(L\\)(\\([0, 100]\\) 범위의 값)라는 세 가지 좌표로 구성됩니다. \\(C\\)의 상한선은 색상과 휘도에 따라 달라집니다.\nhcl 함수는 CIE-LUV19의 극좌표(polar coordinates)에 대응하며 영역 채우기(area fill)를 위해 설계되었습니다. 크로마와 휘도 좌표를 일정하게 유지하고 색상만 변화시킴으로써, 조화로운 색상 팔레트를 쉽게 만들 수 있으며 밝은 색 영역이 어두운 영역보다 더 커 보이는 조사(irradiation) 착시를 피할 수 있습니다. 또한 우리의 시선은 강렬한 색상에 끌리는 경향이 있는데, 크로마 값을 고정하면 모든 색상이 우리 눈에 동일하게 매력적으로 보이게 됩니다.\n19 CIE: 국제 조명 위원회(Commission Internationale de l’Éclairage) – 자세한 내용은 위키백과 등을 참조하세요.\n색상환(color wheel)에서 색을 선택하는 방법은 여러 가지가 있습니다. _삼합색(Triads)_은 색상환에서 등간격으로 떨어진 세 가지 색상을 선택하는 것입니다. 예를 들어 \\(H=0,120,240\\)은 빨강, 초록, 파랑을 줍니다. _사합색(Tetrads)_은 색상환에서 등간격인 네 가지 색상이며, 일부 그래픽 아티스트들은 그 효과를 “역동적”이라고 설명합니다. _따뜻한 색(Warm colors)_은 노란색에 가까운 등간격 색상 세트이고, _차가운 색(Cool colors)_은 파란색에 가까운 등간격 색상 세트입니다. 유사 색(Analogous color) 세트는 노랑, 주황, 빨강 또는 초록, 청록, 파랑과 같이 색상환의 작은 세그먼트 내의 색상들을 포함합니다. _보색(Complementary colors)_은 색상환에서 서로 정반대편에 위치한 색상들입니다. 사합색은 두 쌍의 보색으로 이루어집니다. _분할 보색(Split complementaries)_은 한 쌍의 보색 중 하나를 양옆으로 등간격인 두 색으로 나눈 세 가지 색상입니다. 예를 들어 \\(H=60,,240-30,,240+30\\)입니다. 이는 두 개의 유사한 범주와 세 번째 다른 범주 사이의 차이를 강조할 때 유용합니다. 참고 문헌(Mollon 1995; Ihaka 2003)에서 더 자세한 논의를 확인할 수 있습니다.\n\n5.9.2.1 선 대 영역(Lines versus areas)\n선과 점의 경우 배경과 강한 대비를 원하므로, 흰색 배경에서는 상대적으로 어둡게(낮은 휘도 \\(L\\)) 만드는 것이 좋습니다. 영역 채우기의 경우, 채도가 낮거나 중간 정도인 더 밝은 파스텔 톤의 색상이 보통 더 보기 좋습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "03-chap.html#데이터-변환data-transformations",
    "href": "03-chap.html#데이터-변환data-transformations",
    "title": "5  3.1 이 장의 목표",
    "section": "5.10 3.11 데이터 변환(Data transformations)",
    "text": "5.10 3.11 데이터 변환(Data transformations)\n대부분의 점이 한 영역에 뭉쳐 있고 가용 공간의 상당 부분이 비어 있는 플롯은 읽기 어렵습니다. 변수의 주변 분포(marginal distribution) 히스토그램이 날카로운 정점을 갖고 한쪽 또는 양쪽으로 긴 꼬리를 가진다면, 데이터를 변환하는 것이 도움이 될 수 있습니다. 이러한 고려 사항은 (x) 및 (y) 미학뿐만 아니라 색상 척도에도 적용됩니다. 마이크로어레이 데이터를 포함하는 이 장의 플롯들에서 우리는 로그 변환20을 사용했습니다. 이는 그림 3.27과 같은 산점도의 (x) 및 (y) 좌표뿐만 아니라, 발현 배수 변화(fold changes)를 나타내는 그림 3.41의 색상 척도에서도 마찬가지입니다. 로그 변환은 확실한 의미를 갖기 때문에 매력적입니다. 로그 변환된 척도에서 동일한 양만큼 위아래로 움직이는 것은 원래 척도에서 동일한 배수 변화에 대응합니다: \\((ax)=a+x\\).\n20 ExpressionSet 객체 x의 데이터가 이미 로그 변환된 상태로 제공되므로 우리는 이를 암시적으로 사용했습니다.\n하지만 때로는 로그 변환만으로는 충분하지 않을 수 있습니다. 예를 들어 데이터에 0이나 음수 값이 포함되어 있거나, 로그 척도에서도 데이터 분포가 매우 불균등한 경우입니다. 그림 3.43의 위쪽 패널을 보면, A가 낮을수록 분산이 더 커 보이는 등 M의 분포가 A에 의존한다는 인상을 받기 쉽습니다. 그러나 이는 전적으로 시각적 착시이며, 아래쪽 패널이 이를 확인해 줍니다. M의 분포는 A와 독립적이며, 위쪽 패널에서 보았던 겉보기 추세는 작은 A에서 점 밀도가 더 높았기 때문에 발생한 것입니다.\ngg = ggplot(tibble(A = Biobase::exprs(x)[, 1], M = rnorm(length(A))),\n            aes(y = M))\ngg + geom_point(aes(x = A), size = 0.2)\ngg + geom_point(aes(x = rank(A)), size = 0.2)__\n\n\n\n\n\n\n\n\n그림 3.43: 순위 변환(rank transformation)이 의존성의 시각적 지각에 미치는 영향.\n__\n질문 3.9\n그림 3.31에서처럼 밀도 기반 또는 구간화 기반 플로팅 방법을 사용하여 시각적 착시를 피할 수 있을까요?\n__\n질문 3.10\n순위 변환을 히트맵 등의 색상 척도를 선택할 때도 적용할 수 있을까요? 이미지 처리에서 _히스토그램 평활화(histogram equalization)_는 무엇을 하나요?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "03-chap.html#수학-기호-및-기타-글꼴",
    "href": "03-chap.html#수학-기호-및-기타-글꼴",
    "title": "5  3.1 이 장의 목표",
    "section": "5.11 3.12 수학 기호 및 기타 글꼴",
    "text": "5.11 3.12 수학 기호 및 기타 글꼴\n우리는 R 구문과 LaTeX 스타일 표기법이 혼합된 방식을 사용하여 플롯 레이블에 수학적 표기법을 사용할 수 있습니다(자세한 내용은 help(\"plotmath\")를 참조하세요):\n\n그림 3.44: 그림 3.24와 유사하지만, “Bauhaus 93” 글꼴을 사용했습니다.\nvolume = function(rho, nu)\n            pi^(nu/2) * rho^nu / gamma(nu/2+1)\n\nggplot(tibble(nu    = 1:15,\n  Omega = volume(1, nu)), aes(x = nu, y = Omega)) +\ngeom_line() +\nxlab(expression(nu)) + ylab(expression(Omega)) +\ngeom_text(label =\n\"Omega(rho,nu)==frac(pi^frac(nu,2)~rho^nu, Gamma(frac(nu,2)+1))\",\n  parse = TRUE, x = 6, y = 1.5)__\n\n그림 3.45: 반지름 \\(\\)인 \\(\\)차원 구의 부피 \\(\\) (\\(,…,15\\)).\n결과는 그림 3.45에 나와 있습니다. 또한 세리프(serif) 글꼴인 Times와 같은 다른 글꼴로 전환하는 것도 쉽습니다(그림 3.46).\nggplot(genes, aes( x = value, color = gene)) + stat_ecdf() +\n  theme(text = element_text(family = \"Times\"))__\n\n그림 3.46: 그림 3.24와 유사하지만, 다른 글꼴을 사용했습니다.\n사실 표준 R 설치에서 사용할 수 있는 글꼴 세트는 제한적이지만, 다행히 R의 표준 PostScript 글꼴 이외의 글꼴을 쉽게 사용할 수 있도록 도와주는 extrafont 패키지가 있습니다. R 외부의 글꼴을 먼저 R에 인식시켜야 하므로 사용하기 전에 추가 작업이 필요합니다. 글꼴은 운영 체제, 워드 프로세서 또는 다른 그래픽 애플리케이션과 함께 설치되었을 수 있습니다. 따라서 사용 가능한 글꼴 세트와 물리적 위치는 표준화되어 있지 않으며, 운영 체제 및 추가 구성에 따라 달라집니다. extrafont 패키지를 불러온 후 첫 번째 세션에서는 글꼴을 가져와 패키지에 알리기 위해 font_import 함수를 실행해야 합니다. 그런 다음 글꼴을 사용하려는 각 세션에서 하나 이상의 R 그래픽 장치에 등록하기 위해 loadfonts 함수를 호출해야 합니다. 마지막으로 fonttable 함수를 사용하여 사용 가능한 글꼴 목록을 확인할 수 있습니다. 자신의 머신에서 이 작업을 수행하는 방법은 extrafonts 패키지의 문서를 참조해야 합니다.\n__\n태스크\nextrafont 패키지를 사용하여 “Bauhaus 93”(또는 시스템에서 사용 가능한 다른 글꼴) 글꼴로 그림 3.46의 버전을 만들어 보세요.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "03-chap.html#genomic-data",
    "href": "03-chap.html#genomic-data",
    "title": "5  3.1 이 장의 목표",
    "section": "5.12 3.13 Genomic data",
    "text": "5.12 3.13 Genomic data\n\nFigure 3.47: Screenshot from Ensembl genome browser, showing gene annotation of a genomic region as well as a read pile-up visualization of an RNA-Seq experiment.\nTo visualize genomic data, in addition to the general principles we have discussed in this chapter, there are some specific considerations. The data are usually associated with genomic coordinates. In fact genomic coordinates offer a great organizing principle for the integration of genomic data. You will probably have seen genome browser displays such as in Figure 3.47. Here we will briefly show how to produce such plots programmatically, using your data as well as public annotation. It will be a short glimpse, and we refer to resources such as Bioconductor for a fuller picture.\nThe main challenge of genomic data visualization is the size of genomes. We need visualizations at multiple scales, from whole genome down to the nucleotide level. It should be easy to zoom in and and out, and we may need different visualization strategies for the different size scales. It can be convenient to visualize biological molecules (genomes, genes, transcripts, proteins) in a linear manner, although their embedding in the 3D physical world can matter (a great deal).\nLet’s start with some fun examples, an ideogram plot of human chromosome 1 (Figure 3.48) and a plot of the genome-wide distribution of RNA editing sites (Figure 3.49).\nlibrary(\"ggbio\")\ndata(\"hg19IdeogramCyto\", package = \"biovizBase\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "03-chap.html#게놈-데이터genomic-data",
    "href": "03-chap.html#게놈-데이터genomic-data",
    "title": "5  3.1 이 장의 목표",
    "section": "5.13 3.13 게놈 데이터(Genomic data)",
    "text": "5.13 3.13 게놈 데이터(Genomic data)\n\n그림 3.47: Ensembl 게놈 브라우저의 스크린샷. 게놈 지역의 유전자 어노테이션과 RNA-Seq 실험의 리드 파일업(read pile-up) 시각화를 보여줍니다.\n게놈 데이터를 시각화하려면 이 장에서 논의한 일반적인 원칙 외에도 몇 가지 구체적인 고려 사항이 있습니다. 데이터는 대개 게놈 좌표와 연관되어 있습니다. 사실 게놈 좌표는 게놈 데이터 통합을 위한 훌륭한 조직화 원칙을 제공합니다. 그림 3.47과 같은 게놈 브라우저 디스플레이를 본 적이 있을 것입니다. 여기서는 공공 어노테이션뿐만 아니라 여러분의 데이터를 사용하여 이러한 플롯을 프로그래밍 방식으로 생성하는 방법을 간단히 보여드리겠습니다. 이는 짧은 맛보기에 불과하며, 더 완전한 그림을 보려면 Bioconductor와 같은 자료를 참조하시기 바랍니다.\n게놈 데이터 시각화의 주요 과제는 게놈의 크기입니다. 전체 게놈에서 뉴클레오타이드 수준까지 다중 척도에서의 시각화가 필요합니다. 확대 및 축소가 쉬워야 하며, 크기 척도에 따라 서로 다른 시각화 전략이 필요할 수 있습니다. 생물학적 분자(게놈, 유전자, 전사체, 단백질)를 선형적으로 시각화하는 것이 편리할 수 있지만, 3D 물리적 세계에서의 배치가 (매우) 중요할 수도 있습니다.\n몇 가지 재미있는 예시부터 시작해 보겠습니다. 인간 1번 염색체의 이데오그램(ideogram) 플롯(그림 3.48)과 RNA 편집 부위의 전 게놈 분포 플롯(그림 3.49)입니다.\nlibrary(\"ggbio\")\ndata(\"hg19IdeogramCyto\", package = \"biovizBase\")\nplotIdeogram(hg19IdeogramCyto, subchr = \"chr1\")__\n\n그림 3.48: 인간 게놈의 1번 염색체: 이데오그램 플롯.\ndarned_hg19_subset500은 인간 게놈에서 선택된 500개의 RNA 편집 부위를 나열합니다. 이는 파리, 생쥐 및 인간의 RNA 편집 데이터베이스(DARNED, https://darned.ucc.ie/)에서 가져온 것입니다. 결과는 그림 3.49에 나와 있습니다.\nlibrary(\"GenomicRanges\")\ndata(\"darned_hg19_subset500\", package = \"biovizBase\")\nautoplot(darned_hg19_subset500, layout = \"karyogram\",\n         aes(color = exReg, fill = exReg))__\n\n그림 3.49: RNA 편집 부위가 포함된 카리오그램(Karyogram). exReg는 해당 부위가 코딩 영역(C), 3’- 또는 5’-UTR에 있는지 여부를 나타냅니다.\n__\n질문 3.11\n그림 3.49에서 염색체의 순서를 어떻게 고치고 염색체 길이에 대한 경고를 어떻게 없앨 수 있을까요?\n__\n해결책\n__\n인간 게놈의 hg19 어셈블리에서의 염색체 길이에 대한 정보는 (예를 들어) ideoCyto 데이터 세트에 저장되어 있습니다. 염색체의 순서를 다시 정하기 위해 keepSeqlevels 함수를 사용합니다. 그림 3.50을 참조하세요.\ndata(\"ideoCyto\", package = \"biovizBase\")\ndn = darned_hg19_subset500\nseqlengths(dn) = seqlengths(ideoCyto$hg19)[names(seqlengths(dn))]\ndn = keepSeqlevels(dn, paste0(\"chr\", c(1:22, \"X\")))\nautoplot(dn, layout = \"karyogram\", aes(color = exReg, fill = exReg))__\n\n그림 3.50: 그림 3.49의 개선된 버전.\n__\n질문 3.12\ndarned_hg19_subset500은 어떤 유형의 객체인가요?\n__\n해결책\n__\ndarned_hg19_subset500[1:2,]__\n\n\nGRanges object with 2 ranges and 10 metadata columns:\n      seqnames    ranges strand |       inchr       inrna         snp\n         &lt;Rle&gt; &lt;IRanges&gt;  &lt;Rle&gt; | &lt;character&gt; &lt;character&gt; &lt;character&gt;\n  [1]     chr5  86618225      - |           A           I        &lt;NA&gt;\n  [2]     chr7  99792382      - |           A           I        &lt;NA&gt;\n             gene      seqReg       exReg      source      ests      esta\n      &lt;character&gt; &lt;character&gt; &lt;character&gt; &lt;character&gt; &lt;integer&gt; &lt;integer&gt;\n  [1]        &lt;NA&gt;           O        &lt;NA&gt;    amygdala         0         0\n  [2]        &lt;NA&gt;           O        &lt;NA&gt;        &lt;NA&gt;         0         0\n           author\n      &lt;character&gt;\n  [1]    15342557\n  [2]    15342557\n  -------\n  seqinfo: 23 sequences from an unspecified genome; no seqlengths\n이는 게놈 좌표와 연관된 데이터를 저장하기 위한 Bioconductor 프로젝트의 특수 클래스인 GRanges 객체입니다. 처음 세 개의 열은 필수 사항입니다: seqnames는 포함된 생체 고분자의 이름(우리의 경우 인간 염색체 이름), ranges는 구간의 게놈 좌표(이 경우 각 구간은 단일 뉴클레오타이드를 나타내므로 길이는 모두 1임), 그리고 RNA가 전사되는 DNA strand입니다. 이 클래스와 관련 인프라를 사용하는 방법에 대한 자세한 내용은 문서(예: GenomicRanges 패키지의 비네트)를 참조하세요. 게놈 연관 데이터 세트로 작업하려면 이를 배울 가치가 있습니다. 왜냐하면 이러한 데이터를 편리하고 효율적이며 안전하게 조작할 수 있게 해주고 많은 강력한 유틸리티를 제공하기 때문입니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "03-chap.html#이-장의-요약",
    "href": "03-chap.html#이-장의-요약",
    "title": "5  3.1 이 장의 목표",
    "section": "5.14 3.14 이 장의 요약",
    "text": "5.14 3.14 이 장의 요약\n데이터를 “가공되지 않은” 상태로 보거나 처리, 요약 및 추론의 다양한 단계에서 시각화하는 것은 응용 통계학뿐만 아니라 과학 전반에서 가장 중요한 활동 중 하나입니다. 연역적 이론이 많지 않기 때문에 교과서에서는 때때로 소홀히 다루어지기도 합니다. 하지만 수많은 좋은(그리고 나쁜) 관행들이 있으며, 일단 주의를 기울이기 시작하면 특정 그래픽이 메시지를 전달하는 데 효과적인지, 또는 강력하고 미적으로 매력적인 데이터 시각화를 만들기 위해 어떤 선택을 할 수 있는지 금방 알게 될 것입니다. 중요한 옵션 중에는 플롯 유형(ggplot2에서 geom이라 부르는 것), 비율(종횡비 포함) 및 색상이 있습니다. _그래픽의 문법(grammar of graphics)_은 그래픽에 대해 추론하고 데이터 시각화에 대한 우리의 의도를 컴퓨터에 전달하기 위한 강력한 개념 세트입니다.\n옵션에 대해 생각하지 않고 소프트웨어의 기본값만 사용하는 _게으름_을 피하고, 플롯을 복잡하게 만들 뿐 실제 메시지는 없는 시각적 장식들을 잔뜩 추가하는 _지나침_도 피하십시오. 자신만의 시각화를 만드는 것은 여러 면에서 좋은 글을 쓰는 것과 비슷합니다. (발표나 논문에서) 여러분의 메시지를 전달하는 것은 매우 중요하지만, 이를 위한 간단한 비법은 없습니다. 다른 사람들이 만든 수많은 시각화 자료를 주의 깊게 살펴보고, 요령을 터득하기 위해 자신만의 시각화를 직접 만드는 실험을 해본 다음, 여러분만의 스타일을 결정하십시오.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "03-chap.html#더-읽을거리",
    "href": "03-chap.html#더-읽을거리",
    "title": "5  3.1 이 장의 목표",
    "section": "5.15 3.15 더 읽을거리",
    "text": "5.15 3.15 더 읽을거리\nggplot2에 관한 가장 유용한 자료는 Wickham (2016)의 2판과 ggplot2 웹사이트입니다. 온라인에는 수많은 ggplot2 코드 스니펫이 있으며, 약간의 연습을 거치면 검색 엔진을 통해 찾을 수 있을 것입니다. 하지만 비판적으로 정보를 확인하십시오. 온라인에서 찾는 코드 예제는 때때로 패키지의 구버전을 참조하거나 품질이 좋지 않을 수 있습니다.\n이 시스템의 기초는 Wilkinson (2005)과 Tukey (1977; William S. Cleveland 1988)의 아이디어에 기반하고 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "03-chap.html#연습-문제",
    "href": "03-chap.html#연습-문제",
    "title": "5  3.1 이 장의 목표",
    "section": "5.16 3.16 연습 문제",
    "text": "5.16 3.16 연습 문제\n__\n연습 문제 3.1\n_테마(themes)_를 사용하여 플롯의 시각적 외관을 변경해 보세요. 다음 예제를 실행하는 것부터 시작하십시오:\nggcars = ggplot(mtcars, aes(x = hp, y = mpg)) + geom_point()\nggcars\nggcars + theme_bw()\nggcars + theme_minimal()__\n어떤 다른 테마들이 있나요? (힌트: ggplot2 온라인 문서의 themes 섹션을 살펴보세요.) 이것들이 테마의 전부인가요? (힌트: ggthemes 패키지를 살펴보세요.) 위의 플롯을 Economist 잡지 스타일로 만들어 보세요. 평활 선(smoothing line)을 추가해 보세요.\n__\n연습 문제 3.2\n기본 R에서 허용되는 _색상 이름(color names)_은 무엇인가요? colors 함수의 매뉴얼 페이지를 살펴보세요. 예제와 데모를 실행해 보세요.\n인터넷 검색 엔진을 사용하여 “R color names”를 검색하고 검색 결과로 나오는 자료들을 탐색해 보세요. 예를 들어, 모든 색상이 표시된 치트 시트(cheat sheets) 등이 있습니다.\n그림 3.41의 히트맵을 R 패키지 beyonce의 색상 팔레트를 사용하는 버전으로 만들어 보세요.\n__\n연습 문제 3.3\nxkcd 웹툰 스타일의 플롯을 만들어 보세요. 다음 자료들을 참고하십시오:\n\nStackoverflow의 How can we make xkcd style graphs?라는 제목의 스레드\n이 장의 시작 부분을 생성하는 코드가 포함된 이 책의 R 소스 코드\nxkcd 패키지의 비네트(vignette)\n\n\n__\n연습 문제 3.4\nRStudio 웹사이트의 shiny 튜토리얼을 확인해 보세요. 이 장의 플롯 중 하나를 표시하지만, 표시되는 유전자 등을 제어할 수 있는 대화형 요소가 포함된 _shiny 앱_을 작성해 보세요(그림 3.33—3.35).\n__\n연습 문제 3.5\n그래픽을 _직렬화(serializing)_하는 방법, 즉 나중에 사용하기 위해 파일로 저장하거나 다른 소프트웨어에서 불러오기 위한 방법에는 어떤 것들이 있나요? 대화형 그래픽은 어떻게 직렬화할 수 있나요?\n__\n연습 문제 3.6\n중요하고 잘 만들어진 그래픽이 반드시 복잡할 필요는 없습니다. 예를 들어, Comirnaty에 대한 유럽 의약품청(EMA)의 공공 평가 보고서21의 그림 9(82페이지)와 XKCD 2400을 확인해 보세요.\n21 https://www.ema.europa.eu/en/medicines/human/EPAR/comirnaty에서도 확인할 수 있습니다.\nCarr, Daniel B, Richard J Littlefield, WL Nicholson, and JS Littlefield. 1987. “Scatterplot Matrix Techniques for Large N.” Journal of the American Statistical Association 82 (398): 424–36.\nCleveland, William S. 1988. The Collected Works of John w. Tukey: Graphics 1965-1985. Vol. 5. CRC Press.\nCleveland, William S., Marylyn E. McGill, and Robert McGill. 1988. “The Shape Parameter of a Two-Variable Graph.” Journal of the American Statistical Association 83: 289–300.\nHelmholtz, H. von. 1867. Handbuch Der Physiologischen Optik. Leipzig: Leopold Voss.\nIhaka, Ross. 2003. “Color for Presentation Graphics.” In Proceedings of the 3rd International Workshop on Distributed Statistical Computing , edited by Kurt Hornik and Friedrich Leisch. Vienna, Austria: http://www.r-project.org/conferences/DSC-2003/Proceedings/; ISSN 1609-395X.\nIrizarry, R. A., B. Hobbs, F. Collin, Y. D. Beazer-Barclay, K. J. Antonellis, U. Scherf, and T. P. Speed. 2003. “Exploration, Normalization, and Summaries of High Density Oligonucleotide Array Probe Level Data.” Biostatistics 4 (2): 249–64.\nLawrence, Michael S., Petar Stojanov, Paz Polak, Gregory V. Kryukov, Kristian Cibulskis, Andrey Sivachenko, Scott L. Carter, et al. 2013. “Mutational Heterogeneity in Cancer and the Search for New Cancer-Associated Genes.” Nature 499 (7457): 214–18. https://doi.org/10.1038/nature12213.\nMcCormick Jr, William T, Paul J Schweitzer, and Thomas W White. 1972. “Problem Decomposition and Data Reorganization by a Clustering Technique.” Operations Research 20 (5): 993–1009.\nMollon, John. 1995. “Seeing Colour.” In Colour: Art and Science , edited by T. Lamb and J. Bourriau. Cambridge Unversity Press.\nOhnishi, Y., W. Huber, A. Tsumura, M. Kang, P. Xenopoulos, K. Kurimoto, A. K. Oles, et al. 2014. “Cell-to-Cell Expression Variability Followed by Signal Reinforcement Progressively Segregates Early Mouse Lineages.” Nature Cell Biology 16 (1): 27–37.\nTukey, John W. 1977. “Exploratory Data Analysis.” Massachusetts: Addison- Wesley.\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28.\n———. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer New York. http://had.co.nz/ggplot2/book.\nWilkinson, Leland. 1999. “Dot Plots.” The American Statistician 53 (3): 276.\n———. 2005. The Grammar of Graphics. Springer.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>3.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "04-chap.html",
    "href": "04-chap.html",
    "title": "6  4.1 이 장의 목표",
    "section": "",
    "text": "6.1 4.2 유한 혼합(Finite mixtures)\n생물학적 데이터 분석의 주요 과제 중 하나는 이질성(heterogeneity)을 다루는 것입니다. 우리가 관심을 갖는 수치들은 종종 단순하고 단봉형(unimodal)인 “교과서적인 분포”를 따르지 않습니다. 예를 들어, 2장의 마지막 부분에서 그림 2.27의 서열 점수 히스토그램이 CpG 섬과 비섬(non-islands)이라는 두 개의 별개 최빈값(mode)을 갖는 것을 보았습니다. 우리는 이 데이터를 몇 가지(이 경우 두 개) 성분의 단순한 혼합으로 볼 수 있습니다. 이를 유한 혼합(finite mixtures)이라고 부릅니다. 다른 혼합물은 관측치 수만큼이나 많은 성분을 포함할 수 있는데, 이를 무한 혼합(infinite mixtures)이라고 부릅니다1.\n1 모델링 선택의 많은 부분이 그러하듯이, 혼합물의 적절한 복잡성은 보는 사람의 관점에 달려 있으며, 종종 데이터의 양과 우리가 달성하고자 하는 해상도 및 매끄러움에 따라 달라집니다.\n1장에서 포아송 분포를 이용한 단순한 생성 모델이 에피토프(epitope) 검출에서 어떻게 유용한 추론으로 이어지는지 보았습니다. 불행히도, 이러한 단순한 모델로 실제 데이터를 만족스럽게 피팅하는 것은 종종 어렵습니다. 그러나 정규 분포나 포아송 분포와 같은 단순한 모델은 이 장에서 다룰 혼합 프레임워크를 사용하여 더 현실적인 모델을 구축하기 위한 구성 요소 역할을 할 수 있습니다. 혼합물은 유세포 분석 데이터, 생체 측정값, RNA-Seq, ChIP-Seq, 마이크로바이옴 및 현대 생명공학 기술을 사용하여 수집된 다른 많은 유형의 데이터에서 자연스럽게 발생합니다. 이 장에서는 단순한 예제들을 통해 혼합물을 사용하여 더 현실적인 분포 모델을 구축하는 방법을 배울 것입니다.\n이 장에서 우리는 다음을 수행할 것입니다:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "04-chap.html#유한-혼합finite-mixtures",
    "href": "04-chap.html#유한-혼합finite-mixtures",
    "title": "6  4.1 이 장의 목표",
    "section": "",
    "text": "6.1.1 4.2.1 단순한 예제와 컴퓨터 실험\n여기에 두 개의 동일한 크기의 성분으로 구성된 혼합 모델의 첫 번째 예제가 있습니다. 생성 과정은 두 단계로 이루어집니다:\n공정한 동전을 던집니다.\n앞면이 나오면: 평균 1, 분산 0.25인 정규 분포에서 난수를 생성합니다.\n뒷면이 나오면: 평균 3, 분산 0.25인 정규 분포에서 난수를 생성합니다. 그림 4.1에 표시된 히스토그램은 다음 코드를 사용하여 이 두 단계를 10,000번 반복하여 생성되었습니다.\ncoinflips = (runif(10000) &gt; 0.5)\ntable(coinflips)__\n\n\ncoinflips\nFALSE  TRUE \n 5003  4997 \n\n\noneFlip = function(fl, mean1 = 1, mean2 = 3, sd1 = 0.5, sd2 = 0.5) {\n  if (fl) {\n   rnorm(1, mean1, sd1)\n  } else {\n   rnorm(1, mean2, sd2)\n  }\n}\nfairmix = vapply(coinflips, oneFlip, numeric(1))\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nggplot(tibble(value = fairmix), aes(x = value)) +\n     geom_histogram(fill = \"purple\", binwidth = 0.1)__\n\n그림 4.1: 두 정규 분포의 공정한 혼합물로부터 얻은 10,000개의 난수 히스토그램. 히스토그램의 왼쪽 부분은 (A)에서 생성된 숫자가 지배적이고, 오른쪽은 (B)에서 생성된 숫자가 지배적입니다.\n__\n질문 4.1\nR의 벡터화된 구문을 사용하여 vapply 루프를 제거하고 fairmix 벡터를 더 효율적으로 생성하려면 어떻게 해야 할까요?\n__\n해결책\n__\nmeans = c(1, 3)\nsds   = c(0.5, 0.5)\nvalues = rnorm(length(coinflips),\n               mean = ifelse(coinflips, means[1], means[2]),\n               sd   = ifelse(coinflips, sds[1],   sds[2]))__\n__\n질문 4.2\n개선된 코드를 사용하여 백만 번의 동전 던지기를 수행하고 200개의 빈(bin)을 가진 히스토그램을 만들어 보세요. 무엇을 알 수 있나요?\n__\n해결책\n__\nfair = tibble(\n  coinflips = (runif(1e6) &gt; 0.5),\n  values = rnorm(length(coinflips),\n                 mean = ifelse(coinflips, means[1], means[2]),\n                 sd   = ifelse(coinflips, sds[1],   sds[2])))\nggplot(fair, aes(x = values)) +\n     geom_histogram(fill = \"purple\", bins = 200)__\n\n그림 4.2: 그림 4.1과 유사하지만 백만 개의 관측치를 사용한 경우.\n그림 4.2는 빈의 수와 빈당 관측치 수를 늘림에 따라 히스토그램이 매끄러운 곡선에 가까워지는 것을 보여줍니다. 이 매끄러운 한계 곡선을 확률 변수 fair$values의 밀도(density) 함수라고 부릅니다.\n정규 분포 \\(N(,)\\) 확률 변수의 밀도 함수는 다음과 같이 명시적으로 쓸 수 있습니다. 우리는 보통 이를 다음과 같이 부릅니다.\n\\[ (x)=e{-()2}. \\]\n__\n질문 4.3\n\ncoinflips가 TRUE인 fair$values 값들에 대한 히스토그램을 그리세요. 힌트: aes 호출 시 y = after_stat(density)를 사용하고(이는 수직축이 비율을 나타냄을 의미함), binwidth를 0.01로 설정하세요.\n\\((z)\\)에 해당하는 선을 겹쳐서 그리세요.\n\n__\n해결책\n__\nggplot(dplyr::filter(fair, coinflips), aes(x = values)) +\n  geom_histogram(aes(y = after_stat(density)), fill = \"purple\", binwidth = 0.01) +\n  stat_function(fun = dnorm, color = \"red\",\n                args = list(mean = means[1], sd = sds[1]))__\n\n그림 4.3: 정규 분포 \\(N(,2=0.52)\\)로부터 얻은 50만 개의 난수 히스토그램. 곡선은 dnorm 함수를 사용하여 계산된 이론적 밀도 \\((x)\\)입니다.\n사실 우리는 fair$values 전체의 밀도(히스토그램이 따라가는 한계 곡선)에 대한 수학적 공식을 두 밀도의 합으로 쓸 수 있습니다.\n\\[ f(x)=_1(x)+_2(x), \\]\n여기서 \\(_1\\)은 정규 분포 \\(N(_1=1,^2=0.25)\\)의 밀도이고, \\(_2\\)는 정규 분포 \\(N(_2=3,^2=0.25)\\)의 밀도입니다. 그림 4.4는 다음 코드를 통해 생성되었습니다.\nfairtheory = tibble(\n  x = seq(-1, 5, length.out = 1000),\n  f = 0.5 * dnorm(x, mean = means[1], sd = sds[1]) +\n      0.5 * dnorm(x, mean = means[2], sd = sds[2]))\nggplot(fairtheory, aes(x = x, y = f)) +\n  geom_line(color = \"red\", linewidth = 1.5) + ylab(\"mixture density\")__\n\n그림 4.4: 혼합물의 이론적 밀도.\n이 경우, 두 성분 분포의 겹침이 거의 없기 때문에 혼합 모델이 매우 뚜렷하게 보입니다. 그림 4.4는 두 개의 뚜렷한 정점을 보여줍니다. 우리는 이를 이봉(bimodal) 분포라고 부릅니다. 실제로는 많은 경우 혼합 성분 사이의 분리가 그렇게 명확하지 않지만, 그럼에도 불구하고 이는 중요합니다.\n\n그림 4.5: 인식하기 더 어려운 두 정규 분포의 혼합물.\n__\n질문 4.4\n그림 4.5는 분산이 동일한 두 정규 분포의 공정한 혼합물 히스토그램입니다. 성분 분포의 두 평균 매개변수를 추측할 수 있나요? 힌트: 시행착오법을 사용하여 다양한 혼합물을 시뮬레이션하여 일치하는 히스토그램을 만들 수 있는지 확인할 수 있습니다. 이 장의 R 코드를 살펴보면 데이터가 정확히 어떻게 생성되었는지 알 수 있습니다.\n__\n해결책\n__\n다음 코드는 동전의 _앞면_에서 생성된 점은 빨간색으로, _뒷면_에서 생성된 점은 파란색으로 표시합니다. 그림 4.6에 표시된 출력 결과는 두 기저 분포를 보여줍니다.\nhead(mystery, 3)__\n\n\n# A tibble: 3 × 2\n  coinflips values\n  &lt;lgl&gt;      &lt;dbl&gt;\n1 FALSE       2.40\n2 FALSE       1.66\n3 TRUE        1.22\n\n\nbr = with(mystery, seq(min(values), max(values), length.out = 30))\nggplot(mystery, aes(x = values)) +\n  geom_histogram(data = dplyr::filter(mystery, coinflips),\n     fill = \"red\", alpha = 0.2, breaks = br) +\n  geom_histogram(data = dplyr::filter(mystery, !coinflips),\n     fill = \"darkblue\", alpha = 0.2, breaks = br) __\n\n그림 4.6: 그림 4.5의 혼합물이지만, 두 성분을 빨간색과 파란색으로 표시한 경우.\n그림 4.6에서 두 성분 분포의 막대는 서로 겹쳐서 표시됩니다. 성분을 표시하는 다른 방법은 아래 코드로 생성된 그림 4.7입니다.\nggplot(mystery, aes(x = values, fill = coinflips)) +\n  geom_histogram(data = dplyr::filter(mystery, coinflips),\n     fill = \"red\", alpha = 0.2, breaks = br) +\n  geom_histogram(data = dplyr::filter(mystery, !coinflips),\n     fill = \"darkblue\", alpha = 0.2, breaks = br) +\n  geom_histogram(fill = \"purple\", breaks = br, alpha = 0.2)__\n\n그림 4.7: 그림 4.6과 유사하지만, 두 혼합 성분에 대해 누적 막대(stacked bars)를 사용한 경우.\n__\n질문 4.5\n왜 그림 4.7의 막대 높이는 그림 4.5와 같지만, 그림 4.6의 막대 높이는 그렇지 않을까요?\n__\nSolution\n__\nIn Figures 4.7 and 4.5, each count occupies a different piece of vertical space in a bin. In Figure 4.6, in the overlapping (darker) region, some of the counts falling within the same bin are overplotted.\nIn Figures 4.6 and 4.7, we were able to use the coinflips column in the data to disentangle the components. In real data, this information is missing.\n\n\n\nA book-long treatment on the subject of finite mixtures (McLachlan and Peel 2004).\n\n\nA book-long treatment on the subject of finite mixtures (McLachlan and Peel 2004).\n\n\n6.1.2 4.2.2 Discovering the hidden class labels\nWe use a method called the expectation-maximization (EM) algorithm to infer the value of the hidden groupings. The EM algorithm is a popular iterative procedure that alternates between pretending we know one part of the solution to compute the other part, and pretending the other part is known and computing the first part, and so on, until convergence. More concretely, it alternates between\n\npretending we know the probabilities with which each observation belongs to the different mixture components and, from this, estimating the parameters of the components, and\npretending we know the parameters of the mixture components and estimating the probabilities with which each observation belongs to the components.\n\nLet’s take a simple example. We measure a variable \\(x\\) on a series of objects that we think come from two groups, although we do not know the group labels. We start by augmenting 2 the data with the unobserved (latent) group label, which we will call \\(U\\).\n2 Adding another variable which was not measured, called a hidden or latent variable.\nWe are interested in finding the values of \\(U\\) and the unknown parameters \\(\\) of the underlying distribution of the groups. We will use the maximum likelihood approach introduced in Chapter 2 to estimate the parameters that make the data \\(x\\) the most likely. We can write the probability densities\n\\[ p(x,u,|,) = p(x,|,u,),p(u,|,). \\]\n\n6.1.2.1 Mixture of normals\nFor instance, we could generalize our previous mixture model with two normal distributions Equation 4.1 by allowing non-equal mixture fractions,\n\\[ f(x)=_1(x)+(1-)_2(x), \\]\nfor \\(\\). Similarly as above, \\(_k\\) is the density of the normal \\(N(_k,_k^2)\\) for \\(k=1\\) and \\(k=2\\), respectively. Then, the parameter vector \\(\\) is a five-tuple of the two means, the two standard deviations, and the mixture parameter \\(\\). In other words, \\(=(_1,_2,_1,_2,)\\). Here is an example of 이러한 모델에 따라 생성된 데이터입니다. 레이블은 \\(u\\)로 표시됩니다.\nmus = c(-0.5, 1.5)\nlambda = 0.5\nu = sample(2, size = 100, replace = TRUE, prob = c(lambda, 1-lambda))\nx = rnorm(length(u), mean = mus[u])\ndux = tibble(u, x)\nhead(dux)__\n\n\n# A tibble: 6 × 2\n      u     x\n  &lt;int&gt; &lt;dbl&gt;\n1     2 0.303\n2     2 2.65 \n3     1 0.484\n4     2 3.04 \n5     2 1.10 \n6     2 1.96 \n만약 레이블 \\(u\\)를 알고 있다면, 각 그룹에 대해 별도의 MLE를 사용하여 매개변수를 추정할 수 있습니다. 전체 가능도(likelihood)는 다음과 같습니다.\n\\[ p(x, u ,|, ) = ( _{\\{i:,u_i=1\\}} 1(x_i) ) ( {\\{i:,u_i=2\\}} _2(x_i) ). \\]\n이 식을 최대화하는 작업은 세 가지 독립적인 부분으로 나눌 수 있습니다: 식 4.4의 우변에 있는 첫 번째 괄호 안의 식을 최대화하여 \\(_1\\)과 \\(_1\\)을 찾고, 두 번째 괄호를 최대화하여 \\(_2\\)와 \\(_2\\)를 찾으며, 레이블의 경험적 빈도로부터 \\(\\)를 찾습니다.\ngroup_by(dux, u) |&gt; summarize(mu = mean(x), sigma = sd(x))__\n\n\n# A tibble: 2 × 3\n      u     mu sigma\n  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     1 -0.558  1.05\n2     2  1.41   1.04\n\n\ntable(dux$u) / nrow(dux)__\n\n\n   1    2 \n0.55 0.45 \n__\n질문 4.6\n혼합 비율 \\(=\\)은 알고 있지만 \\(u_i\\)는 모른다고 가정해 봅시다. 이 경우 밀도는 \\(_1(x)+_2(x)\\)입니다. (로그) 가능도를 직접 써보세요. 여기에서 MLE를 명시적으로 구하는 것을 방해하는 요인은 무엇인가요?\n__\n해결책\n__\n정규 혼합물의 가능도 계산에 대해서는 Shalizi (2017)의 혼합 모델(Mixture Models) 장을 참조하십시오. “혼합 모델을 추정하려고 하면, 사후 레이블 확률(posterior label probabilities)에 의해 주어진 가중치를 사용하여 가중 최대 가능도(weighted maximum likelihood)를 수행하게 됩니다. 거듭 강조하지만, 이 확률들은 우리가 추정하고자 하는 매개변수에 의존하므로, 이는 일종의 악순환처럼 보입니다.”\n대부분의 경우 우리는 \\(u\\) 레이블이나 혼합 비율을 알지 못합니다. 우리가 할 수 있는 일은 레이블에 대한 초기 추측에서 시작하여 이를 알고 있다고 가정하고 위와 같이 매개변수를 추정한 다음, 추정치가 실질적으로 변하지 않을 때까지(즉, 수렴할 때까지) 그리고 가능도가 최적값에 도달할 때까지 매 단계마다 레이블과 매개변수에 대한 현재의 최선의 추측을 업데이트하는 반복적인 사이클을 거치는 것입니다.\n사실 우리는 각 관측치에 대해 “딱딱한(hard)” 레이블 \\(u\\)(그룹 1 아니면 2에 속함)를 부여하는 대신, 합이 1이 되는 멤버십 가중치(membership weights)로 대체하는 더 정교한 작업을 수행할 수 있습니다. 혼합 모델 4.3은 다음 식을 제안합니다.\n\\[ w(x, 1) = \\]\n이 식은 값 \\(x\\)를 갖는 관측치가 첫 번째 혼합 성분에 의해 생성되었을 확률로 해석될 수 있으며, 두 번째 성분에 대해서는 유사하게 \\(w(x, 2) = 1 - w(x, 1)\\)이 됩니다. 즉, \\(\\)가 아직 보지 못한 관측치가 혼합 성분 1에서 나올 사전 확률(prior probability)이라면, \\(w(x,1)\\)은 그 값 \\(x\\)를 관찰한 후의 상응하는 사후 확률(posterior probability)입니다. 이는 다음과 같은 반복 알고리즘을 제안합니다:\nE 단계 : \\(\\)(즉, 평균, 표준 편차 및 \\(\\))가 알려져 있다고 가정하고, 멤버십 가중치 4.5를 평가합니다.\nM 단계 : 각 관측치 \\(x_i\\)의 멤버십 가중치가 주어졌을 때, \\(\\)의 새롭고 개선된 최대 가능도 추정치를 결정합니다.\n\\(\\)와 가능도가 수렴할 때까지 반복합니다. 이 시점에서 코드가 포함된 데모인 연습 문제 4.1을 확인해 보시기 바랍니다. 사실, 이 알고리즘은 여기서 다루는 특정 응용 예제보다 훨씬 더 일반적입니다. (Bishop 2006)은 매우 읽기 쉬운 설명을 제시하고 있으며, 주요 내용은 다음과 같습니다:\n우리의 목표는 관측 변수 \\(x\\), 관측되지 않은 변수 \\(u\\) 및 일부 매개변수 \\(\\)를 포함하는 확률 모델의 주변 가능도(marginal likelihood)를 최대화하는 것입니다. 우리의 단순한 예제에서 \\(u\\)는 두 가지 가능한 값을 갖는 범주형 변수이고, \\(x\\)는 실수입니다. 일반적으로 \\(x\\)와 \\(u\\) 모두 모든 유형의 개별 변수들의 튜플(즉, 다변량)일 수 있습니다. 주변 가능도는 \\(u\\)의 모든 가능한 값에 대해 기댓값(즉, 가중 평균)을 취하여 계산됩니다:\n\\[ L_(; x) = _U p(x, u,|,) , U. \\]\n우리의 구체적인 예제에서, 적분은 가능한 모든 멤버십 구성에 대해 (확률적으로) 평균을 내는 것에 해당하며, 따라서 멤버십 가중치를 고려한 가중 합이 됩니다.\n\\[ L_(; x) = {i=1}^n {u=1}^2 p(x_i, u,|,) , w(x_i, u,|,). \\]\n이 수치를 직접 최대화하는 것은 다루기 힘든(intractable) 일입니다.\n데이터와 현재의 매개변수 추정치 \\(_t\\)가 주어졌을 때 우리가 파악할 수 있는 것은 잠재 변수의 조건부 분포 \\(p(u,|,x, _t)\\)입니다. 우리는 이를 사용하여 일반적인 매개변수 값 \\(\\)에 대해 평가된 완전 데이터 로그 가능도(complete data log likelihood) \\(p(x, u,|,)\\)의 기댓값을 찾을 수 있습니다. 이 기댓값은 종종 다음과 같이 표시됩니다.\n\\[ Q(, _t) = _U p(x, u,|,) , p(u,|, x, _t) , U. \\]\nM 단계에서 우리는 이 함수를 최대화하여 수정된 매개변수 추정치 \\(_{t+1}\\)을 결정합니다.\n\\[ {t+1} = , Q(, _t). \\]\nE 단계는 \\(Q\\)의 핵심 요소인 \\(p(u,|, x, _t)\\)를 계산하는 것으로 구성됩니다.\n이 두 단계(E와 M)는 개선 사항이 작아질 때까지 반복됩니다. 이는 우리가 가능도의 평평한 부분에 가까워졌고 최대값에 도달했음을 나타내는 수치적 지표입니다. 반복 궤적은 시작 지점에 따라 달라지겠지만, 도달하는 지점은 달라지지 않기를 바랍니다. 이는 산 정상에 오르는 것과 비슷합니다. 산 정상에 오르는 길은 시작 지점에 따라 다를 수 있고 경로도 다를 수 있지만, 산봉우리가 하나만 있고 여러 개가 아닌 이상 항상 정상으로 이어집니다. 따라서 예방 조치로서, 이러한 절차를 서로 다른 시작 지점에서 여러 번 반복하여 항상 동일한 답을 얻는지 확인하는 것이 좋습니다.\n__\n질문 4.7\n여러 R 패키지에서 mclust, EMcluster, EMMIXskew를 포함한 EM 구현을 제공합니다. 하나를 선택하여 서로 다른 시작 값으로 EM 함수를 여러 번 실행해 보세요. 그런 다음 mixtools 패키지의 normalmixEM 함수를 사용하여 출력을 비교해 보세요.\n__\n해결책\n__\n여기서는 mixtools의 출력을 보여줍니다.\nlibrary(\"mixtools\")\ny = c(rnorm(100, mean = -0.2, sd = 0.5),\n      rnorm( 50, mean =  0.5, sd =   1))\ngm = normalmixEM(y, k = 2, \n                    lambda = c(0.5, 0.5),\n                    mu = c(-0.01, 0.01), \n                    sigma = c(3, 3))__\n\n\nnumber of iterations= 134 \n\n\nwith(gm, c(lambda, mu, sigma, loglik))__\n\n\n[1]    0.9263    0.0737   -0.1859    1.9216    0.6134    0.4746 -170.5662\nEM 알고리즘은 매우 유익합니다:\n\n우리는 관측치가 한 그룹에 속하는지 다른 그룹에 속하는지 결정하지 않고, 멤버십 확률을 가중치로 사용하여 여러 그룹에 참여할 수 있게 함으로써 더 미묘한 추정치를 얻는 “부드러운(soft)” 평균화의 첫 번째 사례를 보았습니다 (Slonim et al. 2005).\n미지수가 너무 많은 어려운 문제를 더 간단한 문제들을 번갈아 가며 해결함으로써 어떻게 다룰 수 있는지 보여줍니다.\n숨겨진 변수(hidden variables)가 있는 데이터 생성 모델을 고려하면서도 그 매개변수를 추정할 수 있었습니다. 숨겨진 변수의 값을 명시적으로 확정하지 않고도 그렇게 할 수 있었습니다: 식 4.8의 기댓값 단계에서 멤버십 확률로 구체화된 이들에 대해 (가중) 평균을 취했습니다. 이 기본 아이디어는 매우 강력하여 머신러닝의 많은 고급 알고리즘의 출발점이 됩니다 (Bishop 2006).\n이는 모델 평균화(model averaging, Hoeting et al. 1999)의 더 일반적인 경우로 확장될 수 있습니다. 우리 데이터에 어떤 모델이 적합한지 확신할 수 없는 경우 여러 모델을 동시에 고려하는 것이 때때로 유익할 수 있습니다. 우리는 이들을 가중 모델로 결합할 수 있습니다. 가중치는 모델의 가능도에 의해 제공됩니다.\n\n\n\n\n6.1.3 4.2.3 영-과잉 데이터(zero inflated data)를 위한 모델\n생태학적 및 분자적 데이터는 종종 카운트(counts)의 형태로 나타납니다. 예를 들어, 여러 장소 각각에 있는 여러 종 각각의 개체 수일 수 있습니다. 이러한 데이터는 종종 두 가지 시나리오의 혼합으로 볼 수 있습니다: 종이 존재하지 않으면 카운트는 반드시 0이지만, 종이 존재한다면 우리가 관찰하는 개체 수는 무작위 샘플링 분포에 따라 달라지며, 이 분포에도 0이 포함될 수 있습니다. 우리는 이를 다음과 같은 혼합 모델로 모델링합니다:\n\\[ f_{}(x) = , (x) + (1-) , f_{}(x), \\]\n여기서 \\(\\)는 모든 질량이 0에 있는 확률 분포를 나타내는 디랙 델타 함수입니다. 첫 번째 혼합 성분에서 발생하는 0은 “구조적(structural)”이라고 불립니다. 우리의 예제에서 이는 특정 종이 특정 서식지에 살지 않기 때문에 발생합니다. 두 번째 성분인 \\(f_{}\\) 역시 단순히 무작위 샘플링으로 인해 0 및 다른 작은 숫자를 포함할 수 있습니다. R 패키지 pscl (Zeileis, Kleiber, and Jackman 2008)과 zicounts는 영-과잉(zero inflated) 카운트를 다루기 위한 많은 예제와 함수를 제공합니다.\n\n6.1.3.1 예시: ChIP-Seq 데이터\nChIP-Seq 데이터의 예를 들어보겠습니다. 이 데이터는 염색질 면역 침전(chromatin immunoprecipitation, ChIP)을 통해 얻은 DNA 조각의 서열입니다. 이 기술을 사용하면 전사 인자, 뉴클레오솜, 히스톤 수정, 염색질 리모델링 효소, 샤페론, 중합효소 및 기타 단백질의 게놈 DNA 상의 위치를 매핑할 수 있습니다. 이는 DNA 요소 백과사전(ENCODE) 프로젝트에서 사용된 주요 기술이었습니다. 여기서는 mosaicsExample 패키지의 예제(Kuan et al. 2011)를 사용합니다. 이 예제는 GM12878 세포주에 적용된 STAT1 단백질 및 H3K4me3 히스톤 수정에 대한 항체의 ChIP-Seq으로부터 22번 염색체에서 측정된 데이터를 보여줍니다. 여기서는 binTFBS 객체를 생성하는 코드는 보여주지 않지만, 이 장의 소스 코드 파일에 나와 있으며 mosaics 패키지의 비네트를 따릅니다.\nbinTFBS __\n\n\nSummary: bin-level data (class: BinData)\n----------------------------------------\n- # of chromosomes in the data: 1\n- total effective tag counts: 462479\n  (sum of ChIP tag counts of all bins)\n- control sample is incorporated\n- mappability score is NOT incorporated\n- GC content score is NOT incorporated\n- uni-reads are assumed\n----------------------------------------\n이 객체로부터 빈(bin)당 카운트의 히스토그램을 생성할 수 있습니다.\nbincts = print(binTFBS)\nggplot(bincts, aes(x = tagCount)) +\n  geom_histogram(binwidth = 1, fill = \"forestgreen\")__\n\n그림 4.8: ChIP-Seq 데이터 세트에서 22번 염색체를 따라 200nt 창(window)에서 발견된 결합 부위의 수.\n그림 4.8을 보면 0이 아주 많다는 것을 알 수 있는데, 이 플롯만으로는 다른 작은 숫자들(\\(1, 2, …\\))의 빈도를 고려할 때 0의 개수가 정말로 특별한지는 즉각적으로 명확하지 않습니다.\n__\n질문 4.8\n\n(y)축에 로그(밑 10) 스케일을 사용하여 카운트 히스토그램을 다시 그리세요.\n카운트가 0인 빈의 비율인 (_0)을 추정하세요.\n\n__\n해결책\n__\nggplot(bincts, aes(x = tagCount)) + scale_y_log10() +\n   geom_histogram(binwidth = 1, fill = \"forestgreen\")__\n\nFigure 4.9: As Figure 4.8, but using a logarithm base 10 scale on the \\(y\\)-axis. The fraction of zeros seems elevated compared to that of ones, twos, …\n\n\n\n6.1.4 4.2.4 More than two components\nSo far we have looked at mixtures of two components. We can extend our description to cases where there may be more. For instance, when weighing N=7,000 nucleotides obtained from mixtures of deoxyribonucleotide monophosphates (each type has a different weight, measured with the same standard deviation sd=3), we might observe the histogram (shown in Figure 4.10) generated by the following code.\nmasses = c(A =  331, C =  307, G =  347, T =  322)\nprobs  = c(A = 0.12, C = 0.38, G = 0.36, T = 0.14)\nN  = 7000\nsd = 3\nnuclt   = sample(length(probs), N, replace = TRUE, prob = probs)\nquadwts = rnorm(length(nuclt),\n                mean = masses[nuclt],\n                sd   = sd)\nggplot(tibble(quadwts = quadwts), aes(x = quadwts)) +\n  geom_histogram(bins = 100, fill = \"purple\")__\n\nFigure 4.10: Simulation of 7,000 nucleotide mass measurements.\n__\nQuestion 4.9\nRepeat this simulation experiment with \\(N=1000\\) nucleotide measurements. What do you notice in the histogram?\n__\nQuestion 4.10\nWhat happens when \\(N=7000\\) but the standard deviation is 10?\n__\nQuestion 4.11\nPlot the theoretical density curve for the distribution simulated in Figure 4.10.\nIn this case, as we have enough measurements with good enough precision, we are able to distinguish the four nucleotides and decompose the distribution shown in Figure 4.10. With fewer data and/or more noisy measurements, the four modes and the distribution component might be less clear.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "04-chap.html#empirical-distributions-and-the-nonparametric-bootstrap",
    "href": "04-chap.html#empirical-distributions-and-the-nonparametric-bootstrap",
    "title": "6  4.1 이 장의 목표",
    "section": "6.2 4.3 Empirical distributions and the nonparametric bootstrap",
    "text": "6.2 4.3 Empirical distributions and the nonparametric bootstrap\nIn this section, we will consider an extreme case of mixture models, where we model our sample of \\(n\\) data points as a mixture of \\(n\\) point masses. We could use almost any set of data here; to be concrete, we use Darwin’s Zea Mays data3 in which he compared the heights of 15 pairs of Zea Mays plants (15 self-hybridized versus 15 crossed). The data are available in the HistData package, and we plot the distribution of the 15 differences in height:\n3 They were collected by Darwin who asked his cousin, Francis Galton to analyse them. R.A. Fisher re-analysed the same data using a paired t-test (Bulmer 2003). We will get back to this example in Chapter 13.\nlibrary(\"HistData\")\nZeaMays$diff __\n\n\n [1]  6.125 -8.375  1.000  2.000  0.750  2.875  3.500  5.125  1.750  3.625\n[11]  7.000  3.000  9.375  7.500 -6.000\n\n\nggplot(ZeaMays, aes(x = diff, ymax = 1/15, ymin = 0)) +\n  geom_linerange(linewidth = 1, col = \"forestgreen\") + ylim(0, 0.1)__\n\nFigure 4.11: The observed sample can be seen as a mixture of point masses at each of the values (real point masses would be bars without any width whatsoever).\nSection 3.6.7에서 우리는 크기 \\(n\\)인 표본에 대한 경험적 누적 분포 함수(empirical cumulative distribution function, ECDF)가 다음과 같음을 보았습니다.\n\\[ n(x)= {i=1}^n {}_{x x_i}, \\]\n그리고 그림 3.24에서 ECDF 플롯을 보았습니다. 우리는 또한 우리 표본의 _밀도_를 다음과 같이 쓸 수 있습니다.\n\\[ n(x) ={i=1}^n _{x_i}(x) \\]\n일반적으로 확률 분포의 밀도는 (존재한다면) 분포 함수의 도함수입니다. 우리는 여기서 이 원리를 적용했습니다: 식 4.11로 정의된 분포의 밀도는 식 4.12입니다. 함수 \\(a\\)를 계단 함수(step function) \\({}{x a}\\)의 “도함수”로 간주할 수 있기 때문에 이렇게 할 수 있었습니다: 이 함수는 계단이 있는 한 점 \\(a\\)를 제외하고는 거의 모든 곳에서 완전히 평평하며, 그 점에서의 값은 “무한”입니다. 식 4.12는 우리의 데이터 표본이 그림 4.11에서와 같이 관측된 값 \\(x_1, x_2, …, x_n\\)에서의 \\(n\\)개 점 질량(point masses)의 혼합물로 간주될 수 있음을 강조합니다.\n이것이 타당하기 위해서는 (표준 미적분학을 넘어서는) 약간의 고급 수학이 필요하지만, 여기서는 다루지 않겠습니다.\n\n그림 4.12: 통계량 \\(\\)의 값은 기저 분포 \\(F\\)로부터 생성된 데이터(회색 행렬들)로부터 추정됩니다. \\(F\\)로부터 얻은 서로 다른 표본들은 서로 다른 데이터를 생성하고, 따라서 추정치 \\(\\)의 값도 달라집니다: 이를 표집 가변성(sampling variability)이라고 합니다. 모든 \\(\\)들의 분포가 표집 분포(sampling distribution)입니다.\n평균, 최솟값 또는 중앙값과 같은 우리 표본의 통계량은 이제 ECDF의 함수로 쓰여질 수 있습니다. 예를 들어, \\({x} = {x_i}(x),x\\)입니다. 또 다른 예로, 만약 \\(n\\)이 홀수라면 중앙값은 정렬된 리스트의 정중앙에 있는 값인 \\(x{()}\\)입니다.\n통계량 \\(\\)의 실제 표집 분포는 통계량을 계산하기 위한 많은 서로 다른 데이터 표본을 필요로 하기 때문에 알기 어려운 경우가 많습니다. 이는 그림 4.12에 나와 있습니다.\n붓스트랩(bootstrap) 원리는 원래 표본으로부터 구축된 경험적 분포에서 뽑은 새로운 표본들을 생성함으로써 \\(\\)의 실제 표집 분포를 근사합니다(그림 4.13). 우리는 데이터를 (\\(\\)들의 혼합 분포로 간주하여) _재사용_하여 표본을 추출하고 그로부터 계산된 통계량 \\(^*\\)의 표집 분포를 살펴봄으로써 새로운 “데이터 세트”를 만듭니다. 이를 비모수적 붓스트랩(nonparametric bootstrap) 재표본 추출 접근법이라고 하며, 완전한 참고 문헌으로는 Bradley Efron과 Tibshirani (1994)를 참조하십시오. 이는 아무리 복잡하더라도 기본적으로 모든 통계량에 적용할 수 있는 매우 다재다능하고 강력한 방법입니다. 5장에서 이 방법의 응용 예시, 특히 클러스터링에 대한 적용 사례를 살펴볼 것입니다.\n\n그림 4.13: 붓스트랩은 (그림 4.12에서처럼) 기저의 실제 분포 \\(F\\)가 아니라 경험적 분포 함수 \\(_n\\)으로부터 표본을 추출함으로써 표집 가변성을 시뮬레이션합니다.\n이러한 아이디어를 사용하여 그림 4.11에서 보았던 Zea Mays 차이값의 중앙값에 대한 표집 분포를 추정해 봅시다. 이전 섹션들과 유사한 시뮬레이션을 사용합니다: 15개의 값(각각이 15개 성분 혼합물의 한 성분임)으로부터 크기가 15인 표본을 \\(B=1000\\)번 추출합니다. 그런 다음 이 15개 값으로 구성된 1000개 표본 각각의 중앙값을 계산하고 그 분포를 살펴봅니다. 이것이 중앙값의 붓스트랩 표집 분포입니다.\nB = 1000\nmeds = replicate(B, {\n  i = sample(15, 15, replace = TRUE)\n  median(ZeaMays$diff[i])\n})\nggplot(tibble(medians = meds), aes(x = medians)) +\n  geom_histogram(bins = 30, fill = \"purple\")__\n\n그림 4.14: Zea Mays 차이값의 중앙값에 대한 붓스트랩 표집 분포.\n__\n질문 4.12\n이 시뮬레이션을 바탕으로 중앙값에 대한 99% 신뢰 구간을 추정해 보세요. 이 구간과 0 사이의 겹침을 보고 무엇을 결론지을 수 있나요?\n__\n질문 4.13\nbootstrap 패키지의 bootstrap 함수를 사용하여 median과 mean 모두에 대해 동일한 분석을 다시 수행해 보세요. 평균과 중앙값의 표집 분포 사이에서 어떤 차이점을 발견했나요?\n__\n해결책\n__\nlibrary(\"bootstrap\")\nbootstrap(ZeaMays$diff, B, mean)\nbootstrap(ZeaMays$diff, B, median)__\n\n6.2.0.1 왜 비모수적(nonparametric)인가요?\n이론 통계학에서 비모수적 방법이란 무한히 많은 자유도나 알 수 없는 매개변수의 수를 가진 방법을 말합니다.\n\n실제로 우리는 무한대까지 기다리지 않습니다. 매개변수의 수가 가용 데이터의 양만큼 많거나 그보다 많아지면 그 방법을 비모수적이라고 부릅니다. 붓스트랩은 \\(n\\)개의 성분을 가진 혼합물을 사용하므로, 크기가 \\(n\\)인 표본의 경우 비모수적 방법의 자격을 갖춥니다.\n그 이름에도 불구하고, 비모수적 방법이 매개변수를 사용하지 않는 방법은 아닙니다. 모든 통계적 방법은 알 수 없는 수치를 추정합니다.\n__\n질문 4.14\n표본이 \\(n=3\\)개의 서로 다른 값으로 구성되어 있다면, 몇 가지의 서로 다른 붓스트랩 재표본이 가능할까요? \\(n=15\\)인 경우에 대해서도 답해 보세요.\n__\n해결책\n__\n모든 붓스트랩 재표본의 집합은 합이 \\(n\\)인 \\(n\\)개 정수 벡터의 집합과 동일합니다. 관측치 \\(x_1, x_2, …, x_n\\)이 붓스트랩 표본에 나타나는 횟수를 \\( = (k_1, k_2, …, k_n)\\)이라고 합시다. 각 \\(k_i\\)를 (다항 분포에서처럼) 상자로 생각할 수 있고, \\(n\\)개의 공을 떨어뜨릴 \\(n\\)개의 상자가 있습니다. 구성을 세는 방법은 \\(n\\)개의 공을 상자에 나누는 방법의 수를 세는 것입니다. 즉, 공을 나타내는 o를 \\(n\\)번 쓰고 그 사이에 구분선 |를 \\(n-1\\)번 쓰는 것입니다. 따라서 우리는 o(공) 또는 |(구분선) 중 하나를 선택해야 하는 \\(2n-1\\)개의 자리를 채워야 합니다. \\(n=3\\)인 경우, 가능한 배치는 oo||o일 수 있으며, 이는 \\( = (2,0,1)\\)에 해당합니다. 일반적으로 이 숫자는 \\({2n-1} \\)이며, 따라서 \\(n=3\\)과 \\(15\\)에 대한 답은 다음과 같습니다.\nc(N3 = choose(5, 3), N15 = choose(29, 15))__\n\n\n      N3      N15 \n      10 77558760 \n__\n질문 4.15\nbootstrap 패키지에 구현된 붓스트랩을 사용할 때 발생할 수 있는 두 가지 유형의 오류는 무엇인가요? 그중 하나를 개선하기 위해 어떤 매개변수를 수정할 수 있나요?\n__\n해결책\n__\n무작위 재표본 추출을 통한 데이터 하위 집합의 몬테카를로 시뮬레이션은 전수 붓스트랩(exhaustive bootstrap)을 근사합니다(Diaconis and Holmes 1994). bootstrap 함수의 nboot 인수의 크기를 늘리면 몬테카를로 오류를 줄일 수 있지만, 전수 붓스트랩도 여전히 정확하지는 않습니다. 우리는 여전히 실제 분포 대신 데이터의 근사 분포 함수를 사용하고 있기 때문입니다. 표본 크기가 작거나 원래 표본에 편향이 있는 경우, nboot를 아무리 크게 선택하더라도 근사는 여전히 상당히 좋지 않을 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "04-chap.html#infinite-mixtures",
    "href": "04-chap.html#infinite-mixtures",
    "title": "6  4.1 이 장의 목표",
    "section": "6.3 4.4 Infinite mixtures",
    "text": "6.3 4.4 Infinite mixtures\nSometimes mixtures can be useful even if we don’t aim to assign a label to each observation or, to put it differently, if we allow as many `labels’ as there are observations. If the number of mixture components is as big as (or bigger than) the number of observations, we say we have an infinite mixture. Let’s look at some examples.\n\n6.3.1 4.4.1 Infinite mixture of normals\n\nFigure 4.15: Laplace knew already that the probability density \\[f_X(y)=(-),&gt;0\\] has the median as its location parameter \\(\\) and the median absolute deviation (MAD) as its scale parameter \\(\\).\nConsider the following two-level data generating scheme:\nLevel 1 Create a sample of Ws from an exponential distribution.\nw = rexp(10000, rate = 1)__\nLevel 2 The \\(w\\)s serve as the variances of normal variables with mean \\(\\) generated using rnorm.\nmu  = 0.3\nlps = rnorm(length(w), mean = mu, sd = sqrt(w))\nggplot(data.frame(lps), aes(x = lps)) +\n  geom_histogram(fill = \"purple\", binwidth = 0.1)__\n\nFigure 4.16: Data sampled from a Laplace distribution.\nThis turns out to be a rather useful distribution. It has well-understood properties and is named after Laplace, who proved that the median is a good estimator of its location parameter \\(\\) and that the median absolute deviation can be used to estimate its scale parameter \\(\\). From the formula in the caption of Figure 4.15 we see that the \\(L_1\\) distance (absolute value of the difference) holds a similar position in the Laplace density as the \\(L_2\\) (square of the difference) does for the normal density.\nConversely, in Bayesian regression4, having a Laplace distribution as a prior on the coefficients amounts to an \\(L_1\\) penalty, called the lasso (Tibshirani 1996), while a normal distribution as a prior leads to an \\(L_2\\) penalty, called ridge regression.\n4 Don’t worry if you are not familiar with this, in that case just skip this sentence.\n__\nQuestion 4.16\nWrite a random variable whose distribution is the symmetric Laplace as a function of normal and exponential random variables.\n__\nSolution\n__\nWe can write the hierarchical model with variances generated as exponential variables, \\(W\\), as:\n\\[ X = Z, W Exp(1), Z N(0,1). \\]\n\n6.3.1.1 Asymmetric Laplace\nIn the Laplace distribution, the variances of the normal components depend on \\(W\\), while the means are unaffected. A useful extension adds another parameter \\(\\) that controls the locations or centers of the components. We generate data alps from a hierarchical model with \\(W\\) an exponential variable; the output shown in Figure 4.17 is a histogram of normal \\(N(+w,w)\\) random numbers, where the \\(w\\)’s themselves were randomly generated from an exponential distribution with mean \\(1\\) as shown in the code:\nmu = 0.3; sigma = 0.4; theta = -1\nw  = rexp(10000, 1)\nalps = rnorm(length(w), theta + mu * w, sigma * sqrt(w))\nggplot(tibble(alps), aes(x = alps)) +\n  geom_histogram(fill = \"purple\", binwidth = 0.1)__\n\nFigure 4.17: Histogram of data generated from an asymmetric Laplace distribution – a scale mixture of many normals whose means and variances are dependent. We write \\(X AL(, , )\\).\nSuch hierarchical mixture distributions, where every instance of the data has its own mean and variance, are useful models in many biological settings. Examples are shown in Figure 4.18.\n\n\nThe lengths of the promoters shorter than 2000bp from Saccharomyces cerevisiae as studied by Kristiansson et al. (2009).\n\n: The log- ratios of microarray gene expression measurements for 20,000 genes [@Purdom2005].”)\n\nThe log-ratios of microarray gene expression measurements for 20,000 genes (Purdom and Holmes 2005).\n\nFigure 4.18: Histogram of real data. Both distributions can be modeled by asymmetric Laplace distributions.\n__\nQuestion 4.17\nLooking at the log-ratio of gene expression values from a microarray, one gets a distribution as shown on the right of Figure 4.18. How would one explain that the data have a histogram of this form?\nThe Laplace distribution is an example of where the consideration of the generative process indicates how the variance and mean are linked. The expectation value and variance of an asymmetric Laplace distribution \\(AL(, , )\\) are\n\\[ E(X)=+(X)=2+2. \\]\nNote the variance is dependent on the mean, unless \\(= 0\\) (the case of the symmetric Laplace Distribution). This is the feature of the distribution that makes it useful. Having a mean-variance dependence is very common for physical measurements, be they microarray fluorescence intensities, peak heights from a mass spectrometer, or reads counts from high-throughput sequencing, as we’ll see in the next section.\n\n\n\n6.3.2 4.4.2 Infinite mixtures of Poisson variables.\n\nFigure 4.19: How to count the fish in a lake? MC Escher.\nA similar two-level hierarchical model is often also needed to model real- world count data. At the lower level, simple Poisson and binomial distributions serve as the building blocks, but their parameters may depend on some underlying (latent) process. In ecology, for instance, we might be interested in variations of fish species in all the lakes in a region. We sample the fish species in each lake to estimate their true abundances, and that could be modeled by a Poisson. But the true abundances will vary from lake to lake. And if we want to see whether, for instance, changes in climate or altitude play a role, we need to disentangle such systematic effects from random lake-to-lake variation. The different Poisson rate parameters \\(\\) can be modeled as coming from a distribution of rates. Such a hierarchical model also enables us to add supplementary steps in the hierarchy, for instance we could be interested in many different types of fish, model altitude and other environmental factors separately, etc.\nFurther examples of sampling schemes that are well modeled by mixtures of Poisson variables include applications of high-throughput sequencing, such as RNA-Seq, which we will cover in detail in Chapter 8, or 16S rRNA-Seq data used in microbial ecology.\n\n\n6.3.3 4.4.3 Gamma distribution: two parameters (shape and scale)\nNow we are getting to know a new distribution that we haven’t seen before. The gamma distribution is an extension of the (one-parameter) exponential distribution, but it has two parameters, which makes it more flexible. It is often useful as a building block for the upper level of a hierarchical model. The gamma distribution is positive-valued and continuous. While the density of the exponential has its maximum at zero and then simply decreases towards 0 as the value goes to infinity, the density of the gamma distribution has its maximum at some finite value. Let’s explore it by simulation examples. The histograms in Figure 4.20 were generated by the following lines of code:\nggplot(tibble(x = rgamma(10000, shape = 2, rate = 1/3)),\n   aes(x = x)) + geom_histogram(bins = 100, fill= \"purple\")\nggplot(tibble(x = rgamma(10000, shape = 10, rate = 3/2)),\n   aes(x = x)) + geom_histogram(bins = 100, fill= \"purple\")__\n\n\ngamma\\((2,)\\)\n\n\n\ngamma\\((10,)\\)\n\nFigure 4.20: Histograms of random samples of gamma distributions. The gamma is a flexible two parameter distribution: see Wikipedia.\n\n6.3.3.1 Gamma–Poisson mixture: a hierarchical model\n\nGenerate a set of parameters: \\(_1,_2,…\\) from a gamma distribution.\nUse these to generate a set of Poisson(\\(_i\\)) random variables, one for each \\(_1\\).\n\nlambda = rgamma(10000, shape = 10, rate = 3/2)\ngp = rpois(length(lambda), lambda = lambda)\nggplot(tibble(x = gp), aes(x = x)) +\n  geom_histogram(bins = 100, fill= \"purple\")__\n\nFigure 4.21: Histogram of gp, generated via a gamma-Poisson hierachical model.\nThe resulting values are said to come from a gamma–Poisson mixture. Figure 4.21 shows the histogram of gp.\n__\nQuestion 4.18\n\nAre the values generated from such a gamma–Poisson mixture continuous or discrete ?\nWhat is another name for this distribution? Hint: Try the different distributions provided by the goodfit function from the vcd package.\n\n__\nSolution\n__\nlibrary(\"vcd\")\nofit = goodfit(gp, \"nbinomial\")\nplot(ofit, xlab = \"\")\nofit$par __\n\n\n$size\n[1] 9.911829\n\n$prob\n[1] 0.5963857\n\nFigure 4.22: Goodness of fit plot. The rootogram shows the theoretical probabilities of the gamma-Poisson distribution (a.k.a. negative binomial) as red dots and the square roots of the observed frequencies as the height of the rectangular bars. The bars all end close to the horizontal axis, which indicates a good fit to the negative binomial distribution.\nIn R, and in some other places, the gamma-Poisson distribution travels under the alias name of negative binomial distribution. The two names are synonyms; the second one alludes to the fact that Equation 4.15 bears some formal similarities to the probabilities of a binomial distribution. The first name, gamma–Poisson distribution, is more indicative of its generating mechanism, and that’s what we will use in the rest of the book. It is a discrete distribution, that means that it takes values only on the natural numbers (in contrast to the gamma distribution, which covers the whole positive real axis). Its probability distribution is\n\\[ (K=k)=(]\nwhich depends on the two parameters \\(a^+\\) and \\(p\\). Equivalently, the two parameters can be expressed by the mean \\(=pa/(1-p)\\) and a parameter called the dispersion \\(/a\\). The variance of the distribution depends on these parameters, and is \\(+^2\\).\n\nFigure 4.23: Visualization of the hierarchical model that generates the gamma- Poisson distribution. The top panel shows the density of a gamma distribution with mean 50 (vertical black line) and variance 30. Assume that in one particular experimental replicate, the value 60 is realized. This is our latent variable. The observable outcome is distributed according to the Poisson distribution with that rate parameter, shown in the middle panel. In one particular experiment the outcome may be, say, 55, indicated by the dashed green line. Overall, if we repeat these two subsequent random process many times, the outcomes will be distributed as shown in the bottom panel – the gamma-Poisson distribution.\n__\nQuestion 4.19\nIf you are more interested in analytical derivations than illustrative simulations, try writing out the mathematical derivation of the gamma-Poisson probability distribution.\n__\nSolution\n__\nRecall that the final distribution is the result of a two step process:\n\nGenerate a \\((a,b)\\) distributed number, call it \\(x\\), from the density\n\n\\[ f_(x, a, b)=,x{a-1},e{-b x}, \\]\nwhere \\(\\) is the so-called \\(\\)-function, \\((a)=_0x{a-1},e^{-x},x\\) (not to be confused with the gamma distribution, even though there is this incidental relation).\n\nGenerate a number \\(k\\) from the Poisson distribution with rate \\(x\\). The probability distribution is\n\n\\[ f_{}(k, =x)= \\]\nIf \\(x\\) only took on a finite set of values, we could solve the problem simply by summing over all the possible cases, each weighted by their probability according to \\(f_\\). But \\(x\\) is continuous, so we have to write the sum out as an integral instead of a discrete sum. We call the distribution of \\(K\\) the marginal. Its probability mass function is\n\\[ ]\nCollect terms and move terms independent of \\(x\\) outside the integral\n\\[ P(K=k)= _{x=0}^{} x{k+a-1}e{-(b+1)x} dx \\]\nBecause we know the gamma density sums to one: \\(_0^ x{k+a-1}e{-(b+1)x} dx = \\)\n\\[ ]\nwhere in the last line we used that \\((v+1)=v!\\). This is the same as Equation (4.15), the gamma-Poisson with size parameter \\(a\\) and probability \\(p=\\).\n\n\n\n6.3.4 4.4.4 Variance stabilizing transformations\nA key issue we need to control when we analyse experimental data is how much variability there is between repeated measurements of the same underlying true value, i.e., between replicates. This will determine whether and how well we can see any true differences, i.e., between different conditions. Data that arise through the type of hierarchical models we have studied in this chapter often turn out to have very heterogeneous variances, and this can be a challenge. We will see how in such cases variance-stabilizing transformations (Anscombe 1948) can help. Let’s start with a series of Poisson variables with rates from 10 to 100:\nNote how we construct the dataframe (or, more precisely, the tibble) simdat: the output of the lapply loop is a list of tibble s, one for each value of lam. With the pipe operator |&gt; we send it to the function bind_rows (from the dplyr package). The result is a dataframe of all the list elements neatly stacked on top of each other.\nsimdat = lapply(seq(10, 100, by = 10), function(lam)\n    tibble(n = rpois(200, lambda = lam),\n           `sqrt(n)` = sqrt(n),\n       lambda = lam)) |&gt;\n  bind_rows() |&gt;\n  tidyr::pivot_longer(cols = !lambda)\nggplot(simdat, aes(x = factor(lambda), y = value)) + xlab(expression(lambda)) +\n  geom_violin() + facet_grid(rows = vars(name), scales = \"free\")__\n\nFigure 4.24: Poisson distributed measurement data, for eight different choices of the mean lambda. In the upper panel, the \\(y\\)-axis is proportional to the data; in the lower panel, it is on a square-root scale. Note how the distribution widths change in the first case, but less so in the second.\nThe data that we see in the upper panel of Figure 4.24 are an example of what is called heteroscedasticity : the standard deviations (or, equivalently, the variance) of our data is different in different regions of our data space. In particular, it increases along the \\(x\\)-axis, with the mean. For the Poisson distribution, we indeed know that the standard deviation is the square root of the mean; for other types of data, there may be other dependencies. This can be a problem if we want to apply subsequent analysis techniques (for instance, regression, or a statistical test) that are based on assuming that the variances are the same. In Figure 4.24, the numbers of replicates for each value of lambda are quite large. In practice, this is not always the case. Moreover, the data are usually not explicitly stratified by a known mean as in our simulation, so the heteroskedasticity may be harder to see, even though it is there. However, as we see in the lower panel of Figure 4.24, if we simply apply the square root transformation, then the transformed variables will have approximately the same variance. This works even if we do not know the underlying mean for each observation, the square root transformation does not need this information. We can verify this more quantitatively, as in the following code, which shows the standard deviations of the sampled values n and sqrt(n) for the difference choices of lambda.\nThe standard deviation of the square root transformed values is consistently around 0.5, so we would use the transformation 2*sqrt(n) to achieve unit variance.\nsummarise(group_by(simdat, name, lambda), sd(value)) |&gt; tidyr::pivot_wider(values_from = `sd(value)`)__\n\n\n# A tibble: 10 × 3\n   lambda     n `sqrt(n)`\n    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n 1     10  2.95     0.478\n 2     20  4.19     0.470\n 3     30  5.62     0.521\n 4     40  5.99     0.473\n 5     50  7.69     0.546\n 6     60  7.59     0.492\n 7     70  8.69     0.520\n 8     80  8.99     0.505\n 9     90  9.44     0.498\n10    100  9.84     0.495\nAnother example, now using the gamma-Poisson distribution, is shown in Figure 4.25. We generate gamma-Poisson variables u5 and plot the 95% confidence intervals around the mean.\n5 To catch a greater range of values for the mean value mu, without creating too dense a sequence, we use a geometric series: \\(_{i+1} = 2_i\\).\nmuvalues = 2^seq(0, 10, by = 1)\nsimgp = lapply(muvalues, function(mu) {\n  u = rnbinom(n = 1e4, mu = mu, size = 4)\n  tibble(mean = mean(u), sd = sd(u),\n         lower = quantile(u, 0.025),\n         upper = quantile(u, 0.975),\n         mu = mu)\n  } ) |&gt; bind_rows()\nhead(as.data.frame(simgp), 2)__\n\n\n    mean       sd lower upper mu\n1 0.9965 1.106440     0     4  1\n2 2.0233 1.748503     0     6  2\n\n\nggplot(simgp, aes(x = mu, y = mean, ymin = lower, ymax = upper)) +\n  geom_point() + geom_errorbar()__\n\nFigure 4.25: gamma-Poisson distributed measurement data, for a range of \\(\\) from 1 to 1024.\n__\nQuestion 4.20\nHow can we find a transformation for these data that stabilizes the variance, similar to the square root function for the Poisson distributed data?\n__\nSolution\n__\nIf we divide the values that correspond to mu[1] (and which are centered around simgp$mean[1]) by their standard deviation simgp$sd[1], the values that correspond to mu[2] (and which are centered around simgp$mean[2]) by their standard deviation simgp$sd[2], and so on, then the resulting values will have, by construction, a standard deviation (and thus variance) of 1. And rather than defining 11 separate transformations, we can achieve our goal by defining one single piecewise linear and continuous function that has the appropriate slopes at the appropriate values.\nsimgp = mutate(simgp,\n  slopes = 1 / sd,\n  trsf   = cumsum(slopes * mean))\nggplot(simgp, aes(x = mean, y = trsf)) +\n  geom_point() + geom_line() + xlab(\"\")__\n\nFigure 4.26: Piecewise linear function that stabilizes the variance of the data in Figure 4.25.\nWe see in Figure 4.26 that this function has some resemblance to a square root function in particular at its lower end. At the upper end, it seems to look more like a logarithm. The more mathematically inclined will see that an elegant extension of these numerical calculations can be done through a little calculus known as the delta method , as follows.\nCall our transformation function \\(g\\), and assume it’s differentiable (that’s not a very strong assumption: pretty much any function that we might consider reasonable here is differentiable). Also call our random variables \\(X_i\\), with means \\(_i\\) and variances \\(v_i\\), and we assume that \\(v_i\\) and \\(_i\\) are related by a functional relationship \\(v_i = v(_i)\\). Then, for values of \\(X_i\\) in the neighborhood of its mean \\(_i\\),\n\\[ g(X_i) = g(_i) + g’(_i) (X_i-_i) + … \\]\nwhere the dots stand for higher order terms that we can neglect. The variances of the transformed values are then\n\\[ \\[\\begin{align} \\text{Var}(g(X_i)) &\\simeq g'(\\mu_i)^2\\\\\\ \\text{Var}(X_i) &=\ng'(\\mu_i)^2 \\, v(\\mu_i), \\end{align}\\] \\]\nwhere we have used the rules \\((X-c)=(X)\\) and \\((cX)=c^2,(X)\\) that hold whenever \\(c\\) is a constant number. Requiring that this be constant leads to the differential equation\n\\[ g’(x) = . \\]\nFor a given mean-variance relationship \\(v()\\), we can solve this for the function \\(g\\). Let’s check this for some simple cases:\n\nif \\(v()=\\) (Poisson), we recover \\(g(x)=\\), the square root transformation.\nIf \\(v()=,^2\\), solving the differential equation 4.19 gives \\(g(x)=(x)\\). This explains why the logarithm transformation is so popular in many data analysis applications: it acts as a variance stabilizing transformation whenever the data have a constant coefficient of variation, that is, when the standard deviation is proportional to the mean.\n\n__\nQuestion 4.21\nWhat is the variance-stabilizing transformation associated with \\(v() = + ,^2\\)?\n__\nSolution\n__\nTo solve the differential equation 4.19 with this function \\(v()\\), we need to compute the integral\n\\[ . \\]\nA closed form expression can be looked up in a reference table such as (Bronštein and Semendjajew 1979). These authors provide the general solution\n\\[ = (2+2ax+b) + , \\]\ninto which we can plug in our special case \\(a=\\), \\(b=1\\), \\(c=0\\), to obtain the variance-stabilizing transformation\n\\[ \\[\\begin{align} g_\\alpha(x) &= \\frac{1}{2\\sqrt{\\alpha}}\n\\ln\\left(2\\sqrt{\\alpha x (\\alpha x+1)} + 2\\alpha x + 1\\right) \\\\\\ &=\n\\frac{1}{2\\sqrt{\\alpha}} {\\displaystyle \\operatorname {arcosh}} (2\\alpha\nx+1).\\\\\\ \\end{align}\\] \\]\nFor the second line in Equation 4.22, we used the identity \\({ }(z) = (z+)\\). In the limit of \\(\\), we can use the linear approximation \\((1+)=+O(^2)\\) to see that \\(g_0(x)=\\). Note that if \\(g_\\) is a variance-stabilizing transformation, then so is \\(ug_+v\\) for any pair of numbers \\(u\\) and \\(v\\), and we have used this freedom to insert an extra factor \\(\\) for reasons that become apparent in the following. You can verify that the function \\(g_\\) from Equation 4.22 fulfills condition 4.19 by computing its derivative, which is an elementary calculation. We can plot it:\nf = function(x, a) \n  ifelse (a==0, \n    sqrt(x), \n    log(2*sqrt(a) * sqrt(x*(a*x+1)) + 2*a*x+1) / (2*sqrt(a)))\nx  = seq(0, 24, by = 0.1)\ndf = lapply(c(0, 0.05*2^(0:5)), function(a) \n  tibble(x = x, a = a, y = f(x, a))) %&gt;% bind_rows()\nggplot(df, aes(x = x, y = y, col = factor(a))) + \n  geom_line() + labs(col = expression(alpha))__\n\nFigure 4.27: Graph of the function Equation 4.22 for different choices of \\(\\).\nand empirically verify the equivalence of two terms in Equation 4.22:\nf2 = function(x, a) ifelse (a==0, sqrt(x), acosh(2*a*x + 1) / (2*sqrt(a)))  \nwith(df, max(abs(f2(x,a) - y)))__\n\n\n[1] 8.881784e-16\nAs we see in Figure 4.27, for small values of \\(x\\), \\(g_(x) \\) (independently of \\(\\)), whereas for large values (\\(x\\)) and \\(&gt;0\\), it behaves like a logarithm:\n\\[ \\[\\begin{align} &\\frac{1}{2\\sqrt{\\alpha}}\\ln\\left(2\\sqrt{\\alpha\\left(\\alpha\nx^2+x\\right)}+2\\alpha x+1\\right)\\\\\\ \\approx&\\frac{1}{2\\sqrt{\\alpha}}\n\\ln\\left(2\\sqrt{\\alpha^2x^2}+2\\alpha x\\right)\\\\\\\n=&\\frac{1}{2\\sqrt{\\alpha}}\\ln\\left(4\\alpha x\\right)\\\\\\\n=&\\frac{1}{2\\sqrt{\\alpha}}\\ln x+\\text{const.} \\end{align}\\]=&(4x)\\\n=&x+ \\end{align} \\]\nWe can verify this empirically by, say,\n  a = c(0.2, 0.5, 1)\n  f(1e6, a) __\n\n\n[1] 15.196731 10.259171  7.600903\n\n\n  1/(2*sqrt(a)) * (log(1e6) + log(4*a))__\n\n\n[1] 15.196728 10.259170  7.600902",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "04-chap.html#summary-of-this-chapter",
    "href": "04-chap.html#summary-of-this-chapter",
    "title": "6  4.1 이 장의 목표",
    "section": "6.4 4.5 Summary of this chapter",
    "text": "6.4 4.5 Summary of this chapter\nWe have given motivating examples and ways of using mixtures to model biological data. We saw how the EM algorithm is an interesting example of fitting a difficult-to-estimate probabilistic model to data by iterating between partial, simpler problems.\n\n6.4.0.1 Finite mixture models\nWe have seen how to model mixtures of two or more normal distributions with different means and variances. We have seen how to decompose a given sample of data from such a mixture, even without knowing the latent variable, using the EM algorithm. The EM approach requires that we know the parametric form of the distributions and the number of components. In Chapter 5, we will see how we can find groupings in data even without relying on such information – this is then called clustering. We can keep in mind that there is a strong conceptual relationship between clustering and mixture modeling.\n\n\n6.4.0.2 Common infinite mixture models\nInfinite mixture models are good for constructing new distributions (such as the gamma-Poisson or the Laplace) out of more basic ones (such as binomial, normal, Poisson). Common examples are\n\nmixtures of normals (often with a hierarchical model on the means and the variances);\nbeta-binomial mixtures – where the probability \\(p\\) in the binomial is generated according to a \\((a, b)\\) distribution;\ngamma-Poisson for read counts (see Chapter 8);\ngamma-exponential for PCR.\n\n\n\n6.4.0.3 Applications\nMixture models are useful whenever there are several layers of experimental variability. For instance, at the lowest layer, our measurement precision may be limited by basic physical detection limits, and these may be modeled by a Poisson distribution in the case of a counting-based assay, or a normal distribution in the case of the continuous measurement. On top of there may be one (or more) layers of instrument-to-instrument variation, variation in the reagents, operator variaton etc.\nMixture models reflect that there is often heterogeneous amounts of variability (variances) in the data. In such cases, suitable data transformations, i.e., variance stabilizing transformations, are necessary before subsequent visualization or analysis. We’ll study in depth an example for RNA-Seq in Chapter 8, and this also proves useful in the normalization of next generation reads in microbial ecology (McMurdie and Holmes 2014).\nAnother important application of mixture modeling is the two-component model in multiple testing – we will come back to this in Chapter 6.\n\n\n6.4.0.4 The ECDF and bootstrapping\nWe saw that by using the observed sample as a mixture we could generate many simulated samples that inform us about the sampling distribution of an estimate. This method is called the bootstrap and we will return to it several times, as it provides a way of evaluating estimates even when a closed form expression is not available (we say it is non-parametric).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "04-chap.html#further-reading",
    "href": "04-chap.html#further-reading",
    "title": "6  4.1 이 장의 목표",
    "section": "6.5 4.6 Further reading",
    "text": "6.5 4.6 Further reading\nA useful book-long treatment of finite mixture models is by McLachlan and Peel (2004); for the EM algorithm, see also the book by McLachlan and Krishnan (2007). A recent book that presents all EM type algorithms within the Majorize-Minimization (MM) framework is by Lange (2016).\nThere are in fact mathematical reasons why many natural phenomena can be seen as mixtures: this occurs when the observed events are exchangeable (the order in which they occur doesn’t matter). The theory underlying this is quite mathematical, a good way to start is to look at the Wikipedia entry and the paper by Diaconis and Freedman (1980).\nIn particular, we use mixtures for high-throughput data. You will see examples in Chapters 8 and 11.\nThe bootstrap can be used in many situations and is a very useful tool to know about, a friendly treatment is given in (B. Efron and Tibshirani 1993).\nA historically interesting paper is the original article on variance stabilization by Anscombe (1948), who proposed ways of making variance stabilizing transformations for Poisson and gamma-Poisson random variables. Variance stabilization is explained using the delta method in many standard texts in theoretical statistics, e.g., those by Rice (2006, chap. 6) and Kéry and Royle (2015, 35).\nKéry and Royle (2015) provide a nice exploration of using R to build hierarchical models for abundance estimation in niche and spatial ecology.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "04-chap.html#exercises",
    "href": "04-chap.html#exercises",
    "title": "6  4.1 이 장의 목표",
    "section": "6.6 4.7 Exercises",
    "text": "6.6 4.7 Exercises\n__\nExercise 4.1\nThe EM algorithm step by step. As an example dataset, we use the values in the file Myst.rds. As always, it is a good idea to first visualize the data. The histogram is shown in Figure 4.28. We are going to model these data as a mixture of two normal distributions with unknown means and standard deviations, and unknown mixture fraction. We’ll call the two components A and B.\nmx = readRDS(\"../data/Myst.rds\")$yvar\nstr(mx)__\n\n\n num [1:1800] 0.3038 0.0596 -0.0204 0.1849 0.2842 ...\n\n\nggplot(tibble(mx), aes(x = mx)) + geom_histogram(binwidth = 0.025)__\nWe start by randomly assigning the membership weights for each of the values in mx for each of the components\nwA = runif(length(mx))\nwB = 1 - wA __\nWe also need to set up some housekeeping variables: iter counts over the iterations of the EM algorithm; loglik stores the current log-likelihood; delta stores the change in the log-likelihood from the previous iteration to the current one. We also define the parameters tolerance, miniter and maxiter of the algorithm.\niter      = 0\nloglik    = -Inf\ndelta     = +Inf\ntolerance = 1e-12\nminiter   = 50\nmaxiter   = 1000 __\nStudy the code below and answer the following questions:\n\nWhich lines correspond to the E-step, which to the M-step?\nWhat is the role of tolerance, miniter and maxiter?\nCompare the result of what we are doing here to the output of the normalmixEM function from the mixtools package.\n\nwhile((delta &gt; tolerance) && (iter &lt;= maxiter) || (iter &lt; miniter)) {\n  lambda = mean(wA)\n  muA = weighted.mean(mx, wA)\n  muB = weighted.mean(mx, wB)\n  sdA = sqrt(weighted.mean((mx - muA)^2, wA))\n  sdB = sqrt(weighted.mean((mx - muB)^2, wB))\n\n  pA   =    lambda    * dnorm(mx, mean = muA, sd = sdA)\n  pB   = (1 - lambda) * dnorm(mx, mean = muB, sd = sdB)\n  ptot = pA + pB\n  wA   = pA / ptot\n  wB   = pB / ptot\n\n  loglikOld = loglik\n  loglik = sum(log(pA + pB))\n  delta = abs(loglikOld - loglik)\n  iter = iter + 1\n}\niter __\n\n\n[1] 447\n\n\nc(lambda, muA, muB, sdA, sdB)__\n\n\n[1]  0.4756 -0.1694  0.1473  0.0983  0.1498\n\nFigure 4.28: Histogram of mx, our example data for the EM algorithm.\n__\nSolution\n__\nThe first five lines in the while loop implement the Maximization step. Given the current values of wA and wB, we estimate the parameters of the mixture model using the maximum-likelihood estimators: the mixture fraction lambda by the mean of wA, and the parameters of the two normal distribution components (muA, sdA) and (muB, sdB) by the sample means and the sample standard deviations. To take into account the membership weights, we use the weighted mean (function weighted.mean) and standard deviation.\nNext comes the Expectation step. For each of the elements in the data vector mx, we compute the probability densities pA and pB for the generative distribution models A and B, using the normal density function dnorm, weighted by the mixture fractions lambda and (1-lambda), respectively. From this, we compute the updated membership weights wA and wB, according to Equation 4.5.\nGiven the membership weights and the parameters, the logarithmic likelihood loglik is easily computed, and the while loop iterates these steps.\nThe termination criterion for the loop is based on delta, the change in the likelihood. The loop can end if this becomes smaller than tolerance. This is a simple way of checking whether the algorithm has converged. The additional conditions on iter make sure that at least miniter iterations are run, and that the loop always stops after maxiter iterations. The latter is to make sure that the loop terminates in finite time no matter what. (“Professional” implementations of such iterative algorithms typically work a bit harder to decide what is the best time to stop.)\nFinally, let’s compare our estimates to those from the function normalmixEM from the mixtools package.\ngm = mixtools::normalmixEM(mx, k = 2)__\n\n\nnumber of iterations= 215 \n\n\nwith(gm, c(lambda[1], mu, sigma))__\n\n\n[1]  0.4757 -0.1694  0.1473  0.0983  0.1498\n__\nExercise 4.2\nWhy do we often consider the logarithm of the likelihood rather than the likelihood? E.g., in the EM code above, why did we work with the probabilities on the logarithmic scale?\n__\nSolution\n__\nLikelihoods often (whenever the data points are sampled independently) take the form of a product. This is, for instance, the case in Equation 4.4. Calculating the derivative, for likelihood optimisation, would then require application of the product rule. On the logarithmic scale, the product turns into a sum, and the derivative of a sum is simply the sum of the derivatives of the individual summands.\nAn additional reason comes from the way computers implement arithmetic. They commonly use a floating point representation of numbers with a finite number of bits. E.g., the IEEE 754-2008 standard uses 64 bits for a double- precision number: 1 bit for the sign, 52 for the mantissa (also called significand), 11 for the exponent. Multiplication between such numbers implies addition of the exponents, but the range of the exponent is only \\(0\\) to \\(2^{11}-1=2047\\). Even likelihoods that involve only a few hundred data points can lead to arithmetic overflow or other problems with precision. On the logarithmic scale, where the product is a sum, the workload tends to be better distributed between mantissa and exponent, and log-likelihoods even with millions of data points to be handled with reasonable precision.\nSee also Gregory Gundersen’s post on the Log-Sum-Exp Trick for normalizing vectors of log probabilities.\n__\nExercise 4.3\nCompare the theoretical values of the gamma-Poisson distribution with parameters given by the estimates in ofit$par in Section 4.4.3 to the data used for the estimation using a QQ-plot.\n__\nExercise 4.4\nMixture modeling examples for regression. The flexmix package (Grün, Scharl, and Leisch 2012) enables us to cluster and fit regressions to the data at the same time. The standard M-step FLXMRglm of flexmix is an interface to R’s generalized linear modeling facilities (the glm function). Load the package and an example dataset.\nlibrary(\"flexmix\")\ndata(\"NPreg\")__\n\nFirst, plot the data and try to guess how the points were generated.\nFit a two component mixture model using the commands\n\nm1 = flexmix(yn ~ x + I(x^2), data = NPreg, k = 2)__\n\nLook at the estimated parameters of the mixture components and make a truth table that cross-classifies true classes versus cluster memberships. What does the summary of the object m1 show us?\nPlot the data again, this time coloring each point according to its estimated class.\n\n__\nSolution\n__\nggplot(NPreg, aes(x = x, y = yn)) + geom_point()__\n\nFigure 4.29: The points seem to come from two different generative processes, one is linear; the other quadratic.\nThe components are:\nmodeltools::parameters(m1, component = 1)__\n\n\n                      Comp.1\ncoef.(Intercept) -0.20998685\ncoef.x            4.81807854\ncoef.I(x^2)       0.03613061\nsigma             3.47665584\n\n\nmodeltools::parameters(m1, component = 2)__\n\n\n                     Comp.2\ncoef.(Intercept) 14.7167886\ncoef.x            9.8468507\ncoef.I(x^2)      -0.9683734\nsigma             3.4795657\nThe parameter estimates of both components are close to the true values. A cross-tabulation of true classes and cluster memberships can be obtained by\ntable(NPreg$class, modeltools::clusters(m1))__\n\n\n   \n     1  2\n  1 95  5\n  2  5 95\nFor our example data, the ratios of both components are approximately 0.7, indicating the overlap of the classes at the cross-section of line and parabola.\nsummary(m1)__\nThe summary shows the estimated prior probabilities \\(k\\), the number of observations assigned to the two clusters, the number of observations where \\(p{nk}&gt;\\) (with a default of \\(^{-4}\\)), and the ratio of the latter two numbers. For well- separated components, a large proportion of observations with non-vanishing posteriors \\(p_{nk}\\) should be assigned to their cluster, giving a ratio close to 1.\nNPreg = mutate(NPreg, gr = factor(class))\nggplot(NPreg, aes(x = x, y = yn, group = gr)) +\n   geom_point(aes(colour = gr, shape = gr)) +\n   scale_colour_hue(l = 40, c = 180)__\n\nFigure 4.30: Regression example using flexmix with the points colored according to their estimated class. You can see that at the intersection we have an `identifiability’ problem: we cannot distinguish points that belong to the straight line from ones that belong to the parabole.\n__\nExercise 4.5\nOther hierarchical noise models:\nFind two papers that explore the use of other infinite mixtures for modeling molecular biology technological variation.\n__\nSolution\n__\nThe paper by Chen, Xie, and Story (2011) explores an exponential-Poisson model for modeling background noise in bead arrays. Wills et al. (2013) compares several Poisson mixture models.\nAnscombe, Francis J. 1948. “The Transformation of Poisson, Binomial and Negative-Binomial Data.” Biometrika , 246–54.\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\nBronštein, Il’ja N., and Konstantin A Semendjajew. 1979. Taschenbuch Der Mathematik. B.G. Teubner Verlagsgesellschaft, Leipzig; Verlag Nauka, Moscow.\nBulmer, Michael George. 2003. Francis Galton: Pioneer of Heredity and Biometry. JHU Press.\nChen, Min, Yang Xie, and Michael Story. 2011. “An Exponential-Gamma Convolution Model for Background Correction of Illumina BeadArray Data.” Communications in Statistics-Theory and Methods 40 (17): 3055–69.\nDiaconis, Persi, and David Freedman. 1980. “Finite Exchangeable Sequences.” The Annals of Probability , 745–64.\nDiaconis, Persi, and Susan Holmes. 1994. “Gray Codes for Randomization Procedures.” Statistics and Computing 4 (4): 287–302.\nEfron, Bradley, and Robert J Tibshirani. 1994. An Introduction to the Bootstrap. CRC press.\nEfron, B., and R. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman & Hall/CRC.\nGrün, Bettina, Theresa Scharl, and Friedrich Leisch. 2012. “Modelling Time Course Gene Expression Data with Finite Mixtures of Linear Additive Models.” Bioinformatics 28 (2): 222–28. https://doi.org/10.1093/bioinformatics/btr653.\nHoeting, Jennifer A, David Madigan, Adrian E Raftery, and Chris T Volinsky. 1999. “Bayesian Model Averaging: A Tutorial.” Statistical Science , 382–401.\nKéry, Marc, and J Andrew Royle. 2015. Applied Hierarchical Modeling in Ecology: Analysis of Distribution, Abundance and Species Richness in r and BUGS: Volume 1: Prelude and Static Models. Academic Press.\nKristiansson, Erik, Michael Thorsen, Markus J Tamás, and Olle Nerman. 2009. “Evolutionary Forces Act on Promoter Length: Identification of Enriched Cis- Regulatory Elements.” Molecular Biology and Evolution 26 (6): 1299–1307.\nKuan, Pei Fen, Dongjun Chung, Guangjin Pan, James A Thomson, Ron Stewart, and Sündüz Keleş. 2011. “A Statistical Framework for the Analysis of ChIP-Seq Data.” Journal of the American Statistical Association 106 (495): 891–903.\nLange, Kenneth. 2016. MM Optimization Algorithms. SIAM.\nMcLachlan, Geoffrey, and Thriyambakam Krishnan. 2007. The EM Algorithm and Extensions. Vol. 382. John Wiley & Sons.\nMcLachlan, Geoffrey, and David Peel. 2004. Finite Mixture Models. John Wiley & Sons.\nMcMurdie, Paul J, and Susan Holmes. 2014. “Waste Not, Want Not: Why Rarefying Microbiome Data Is Inadmissible.” PLoS Computational Biology 10 (4): e1003531.\nPurdom, Elizabeth, and Susan P Holmes. 2005. “Error Distribution for Gene Expression Data.” Statistical Applications in Genetics and Molecular Biology 4 (1).\nRice, John. 2006. Mathematical Statistics and Data Analysis. Cengage Learning.\nShalizi, Cosma. 2017. Advanced Data Analysis from an Elementary Point of View. Cambridge University Press. https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf.\nSlonim, Noam, Gurinder Singh Atwal, Gašper Tkačik, and William Bialek. 2005. “Information-Based Clustering.” PNAS 102 (51): 18297–302.\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society. Series B (Methodological) , 267–88.\nWills, Quin F, Kenneth J Livak, Alex J Tipping, Tariq Enver, Andrew J Goldson, Darren W Sexton, and Chris Holmes. 2013. “Single-Cell Gene Expression Analysis Reveals Genetic Associations Masked in Whole-Tissue Experiments.” Nature Biotechnology 31 (8): 748–52.\nZeileis, Achim, Christian Kleiber, and Simon Jackman. 2008. “Regression Models for Count Data in R.” Journal of Statistical Software 27 (8). http://www.jstatsoft.org/v27/i08/.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>4.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "05-chap.html",
    "href": "05-chap.html",
    "title": "7  5.1 이 장의 목표",
    "section": "",
    "text": "7.1 5.2 데이터란 무엇이며 왜 군집화하는가?\n세포, 질병, 유기체의 범주를 찾고 그 이름을 짓는 것은 자연과학의 핵심 활동입니다. 4장에서 우리는 일부 데이터를 명확한 모수적 생성 모델을 가진 서로 다른 그룹이나 모집단의 혼합물로 모델링할 수 있음을 보았습니다. 우리는 그러한 예제들에서 EM 알고리즘을 사용하여 성분들을 어떻게 분리할 수 있는지 보았습니다. 이제 우리는 클러스터(clusters)가 반드시 예쁜 타원형1 모양을 가질 필요는 없는 경우로 그룹들을 풀어내는 아이디어를 확장해 보려 합니다.\n1 다변량 정규 분포를 이용한 혼합 모델링은 타원형 클러스터 경계를 함축합니다.\n군집화(Clustering)는 데이터(연속형 또는 준연속형)를 가져와서 새로운 범주형 그룹 변수를 추가하며, 이는 때때로 중간 상태를 무시하는 대가를 치르더라도 의사 결정을 단순화할 수 있습니다. 예를 들어, 공복 혈당, 당화혈색소, 섭취 2시간 후 혈장 포도당 수치와 관련된 복잡한 고차원 진단 수치들을 단순히 환자를 당뇨병 “그룹”에 할당함으로써 의료 결정을 단순화합니다.\n이 장에서 우리는 저차원 및 고차원 비모수적(nonparametric) 환경 모두에서 의미 있는 클러스터나 그룹을 찾는 방법을 공부할 것입니다. 그러나 주의할 점이 있습니다: 군집화 알고리즘은 클러스터를 찾도록 설계되었으므로, 클러스터가 없는 곳에서도 클러스터를 찾아낼 것입니다2. 따라서 클러스터의 존재를 뒷받침하는 사전 지식이 없는 경우, 클러스터 _검증(validation)_은 우리 프로세스의 필수적인 구성 요소입니다.\n2 이는 인간을 연상시킵니다: 우리는 무작위성 속에서도 패턴을 보기를 좋아합니다.\n이 장에서 우리는 다음을 수행할 것입니다:\n그림 5.1: 존 스노우의 콜레라 사례 지도: 각 집의 작은 막대 그래프는 진단된 사례들의 군집을 나타냅니다.\nDavid Freedman은 이 발견으로 이어진 모든 단계에 대해 훌륭하고 상세한 설명을 남겼습니다 (Freedman 1991).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "05-chap.html#데이터란-무엇이며-왜-군집화하는가",
    "href": "05-chap.html#데이터란-무엇이며-왜-군집화하는가",
    "title": "7  5.1 이 장의 목표",
    "section": "",
    "text": "7.1.1 5.2.1 군집화는 때때로 발견으로 이어질 수 있습니다.\n존 스노우(John Snow)는 콜레라 사례 지도를 만들고 사례들의 _클러스터_를 식별했습니다. 그런 다음 그는 펌프의 위치에 대한 추가 정보를 수집했습니다. 밀집된 사례 클러스터들이 브로드 스트리트(Broadstreet) 펌프와 가깝다는 사실은 물이 범인일 가능성을 지목했습니다. 그는 콜레라 발생의 원인을 추론할 수 있게 해주는 별도의 정보원들을 수집했습니다.\n이제 그림 5.2에 표시된 또 다른 런던 지도를 살펴봅시다. 빨간색 점들은 제2차 세계대전 중 폭격을 받은 위치를 나타냅니다. 전쟁 중에 분석 팀들은 많은 이론을 내놓았습니다. 그들은 폭격 패턴(유틸리티 공장, 병기창과의 근접성, \\(…\\))에 대한 합리적인 설명을 찾으려 노력했습니다. 사실, 전쟁 후에 폭격은 특정 목표물을 타격하려는 시도 없이 무작위로 분포되었다는 것이 밝혀졌습니다.\n\n그림 5.2: 1940년 9월 7일 런던에 떨어진 폭탄의 위치 지도. 영국 국립 보존 기록관 웹사이트 http://bombsight.org에서 가져온 것입니다.\n군집화는 복잡한 다변량 데이터를 이해하는 데 유용한 기법이며, 이는 비지도(unsupervised)3 학습입니다. 탐색적 기법은 데이터를 해석하는 데 중요할 수 있는 그룹화된 모습을 보여줍니다.\n3 모든 변수가 동일한 상태를 가지며, 설명 변수의 정보를 바탕으로 한 변수(감독 반응)의 값을 예측하거나 학습하려고 하지 않기 때문에 이렇게 불립니다.\n예를 들어, 군집화를 통해 연구자들은 암 생물학에 대한 이해를 높일 수 있었습니다. 해부학적 위치와 조직 병리학적 소견으로는 동일해 보였던 종양들이 유전자 발현 데이터와 같은 분자적 특성에 따라 여러 클러스터로 나뉘었습니다 (Hallett et al. 2012). 결국 이러한 군집화는 새롭고 더 적절한 질병 유형의 정의로 이어질 수 있습니다. 적절성은 예를 들어 서로 다른 환자 예후와 연관되어 있다는 사실로 입증됩니다. 이 장에서 우리가 하고자 하는 것은 그림 5.3과 같은 그림들이 어떻게 구성되는지, 그리고 어떻게 해석해야 하는지를 이해하는 것입니다.\n\n그림 5.3: 유방암 샘플들(The Cancer Genome Atlas (TCGA) 및 Molecular Taxonomy of Breast Cancer International Consortium (METABRIC)에서 가져옴)은 그들의 miRNA 발현을 사용하여 그룹으로 나뉠 수 있습니다 (Aure et al. 2017). 저자들은 하단 플롯에서 서로 다른 그룹의 생존 기간이 다르다는 것을 보여줍니다. 따라서 이러한 클러스터들은 생물학적 및 임상적으로 의미가 있었습니다. 이러한 분석의 약속은 그룹들을 사용하여 더 구체적이고 최적화된 치료를 제공할 수 있다는 것입니다.\n4장에서 우리는 이미 그룹을 찾아내기 위한 한 가지 기법인 EM 알고리즘을 공부했습니다. 이 장에서 우리가 탐구하는 기법들은 더 일반적이며 더 복잡한 데이터에 적용될 수 있습니다. 이들 중 상당수는 관측치 쌍 사이의 거리에 기초하며(이는 전체 대 전체일 수도 있고, 때로는 전체 대 일부일 수도 있음), 정규 분포, 감마-포아송 등과 같은 특정 분포군을 포함하는 데이터의 생성 메커니즘에 대해 명시적인 가정을 하지 않습니다. 문헌과 과학 소프트웨어 분야에는 군집화 알고리즘이 넘쳐나며, 이는 위협적으로 느껴질 수 있습니다. 사실 이는 데이터 유형의 다양성과 각 분야에서 추구하는 목표의 다양성과 연결되어 있습니다.\n__\n태스크\nBiocViews Clustering 또는 CRAN의 Cluster view를 찾아보고 군집화 도구를 제공하는 패키지 수를 세어 보세요.\n\n그림 5.4: 우리는 군집화 알고리즘에서 이루어지는 선택들을 취해진 단계에 따라 분해합니다: 관측치-특성 직사각형 테이블 \\(X\\)에서 시작하여, 관측치 간 거리 척도를 선택하고 거리 행렬을 계산합니다(여기서는 삼각형으로 도식화됨). 거리는 클러스터를 구성하는 데 사용됩니다. 왼쪽에는 계층적 군집 트리를 구축하는 응집형(agglomerative) 방법을 도식화했습니다. 오른쪽에는 데이터를 하위 집합으로 분리하는 분할형(partitioning) 방법을 도식화했습니다. 두 유형의 방법 모두 클러스터 수 \\(k\\)의 선택을 필요로 합니다. \\(k\\)-평균과 같은 분할 접근법의 경우 이 선택은 처음에 이루어져야 합니다. 계층적 군집화의 경우 분석이 끝날 때까지 이를 미룰 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "05-chap.html#how-do-we-measure-similarity",
    "href": "05-chap.html#how-do-we-measure-similarity",
    "title": "7  5.1 이 장의 목표",
    "section": "7.2 5.3 How do we measure similarity?",
    "text": "7.2 5.3 How do we measure similarity?\n\n\n\nOf a feather: how the distances are measured and similarities between observations defined has a strong impact on the clustering result.\n\n\nOf a feather : how the distances are measured and similarities between observations defined has a strong impact on the clustering result.\nOur first step is to decide what we mean by similar. There are multiple ways of comparing birds: for instance, a distance using size and weight will give a different clustering than one using diet or habitat. Once we have chosen the relevant features, we have to decide how we combine differences between the multiple features into a single number. Here is a selection of choices, some of them are illustrated in Figure 5.5.\n\n\n\nFigure 5.5 (a):\n\n\n\n\n\n\n\n\nFigure 5.5 (b):\n\n\n\n\n\n\n\n\nFigure 5.5 (c):\n\n\n\n\n\n\n\n\nFigure 5.5 (d):\n\n\n\n\n\nFigure 5.5: Equal-distance contour plots according to four different distances: points on any one curve are all the same distance from the center point.\nEuclidean The Euclidean distance between two points \\(A=(a_1,…,a_p)\\) and \\(B= (b_1,…,b_p)\\) in a \\(p\\)-dimensional space (for the \\(p\\) features) is the square root of the sum of squares of the differences in all \\(p\\) coordinate directions:\n\\[ d(A,B)=. \\]\nManhattan The Manhattan, City Block, Taxicab or \\(L_1\\) distance takes the sum of the absolute differences in all coordinates.\n\\[ d(A,B)=|a_1-b_1|+|a_2-b_2|+… +|a_p-b_p|. \\]\nMaximum The maximum of the absolute differences between coordinates is also called the \\(L_\\) distance:\n\\[ d_(A,B)= _{i}|a_i-b_i|. \\]\nWeighted Euclidean distance is a generalization of the ordinary Euclidean distance, by giving different directions in feature space different weights. We have already encountered one example of a weighted Euclidean distance in Chapter 2, the \\(^2\\) distance. It is used to compare rows in contingency tables, and the weight of each feature is the inverse of the expected value. The Mahalanobis distance is another weighted Euclidean distance that takes into account the fact that different features may have a different dynamic range, and that some features may be positively or negatively correlated with each other. The weights in this case are derived from the covariance matrix of the features. See also Question 5.1.\nMinkowski Allowing the exponent to be \\(m\\) instead of \\(2\\), as in the Euclidean distance, gives the Minkowski distance\n\\[ d(A,B) = ( (a_1-b_1)m+(a_2-b_2)m+… +(a_p-b_p)^m )^. \\]\nEdit, Hamming This distance is the simplest way to compare character sequences. It simply counts the number of differences between two character strings. This could be applied to nucleotide or amino acid sequences – although in that case, the different character substitutions are usually associated with different contributions to the distance (to account for physical or evolutionary similarity), and deletions and insertions may also be allowed.\nBinary When the two vectors have binary bits as coordinates, we can think of the non-zero elements as ‘on’ and the zero elements as ‘off’. The binary distance is the proportion of features having only one bit on amongst those features that have at least one bit on.\nJaccard Distance Occurrence of traits or features in ecological or mutation data can be translated into presence and absence and encoded as 1’s and 0’s. In such situations, co-occurence is often more informative than co- absence. For instance, when comparing mutation patterns in HIV, the co- existence in two different strains of a mutation tends to be a more important observation than its co-absence. For this reason, biologists use the Jaccard index. Let’s call our two observation vectors \\(S\\) and \\(T\\), \\(f_{11}\\) the number of times a feature co-occurs in \\(S\\) and \\(T\\), \\(f_{10}\\) (and \\(f_{01}\\)) the number of times a feature occurs in \\(S\\) but not in \\(T\\) (and vice versa), and \\(f_{00}\\) the number of times a feature is co-absent. The Jaccard index is\n\\[ J(S,T) = , \\]\n(i.e., it ignores \\(f_{00}\\)), and the Jaccard dissimilarity is\n\\[ d_J(S,T) = 1-J(S,T) = . \\]\nCorrelation based distance\n\\[ d(A,B)=. \\]\n\nFigure 5.6: An example for the use of Mahalanobis distances to measure the distance of a new data point (red) from two cluster centers.\n__\nQuestion 5.1\nWhich of the two cluster centers in Figure 5.6 is the red point closest to?\n__\nSolution\n__\nA naïve answer would use the Euclidean metric and decide that the point is closer to the left cluster. However, as we see that the features have different ranges and correlations, and that these even differ between the two clusters, it makes sense to use cluster-specific Mahalanobis distances. The figure shows contour lines for both clusters. These were obtained from a density estimate; the Mahalanobis distance approximates these contours with ellipses. The distance between the red point and each of the cluster centers corresponds to the number of contour lines crossed. We see that as the group on the right is more spread out, the red point is in fact closer to it.\n\nFigure 5.7: The lower triangle of distances can be computed by any of a hundred different functions in various R packages (vegdist in vegan , daisy in cluster , genetic_distance in gstudio , dist.dna in ape , Dist in amap , distance in ecodist , dist.multiPhylo in distory , shortestPath in gdistance , % dudi.dist and dist.genet in ade4).\n\n7.2.1 5.3.1 Computations related to distances in R\nThe dist function in R is designed to use less space than the full \\(n^2\\) positions a complete \\(n n\\) distance matrix between \\(n\\) objects would require. The function computes one of six choices of distance (euclidean, maximum, manhattan, canberra, binary, minkowski) and outputs a vector of values sufficient to reconstruct the complete distance matrix. The function returns a special object of class dist that encodes the relevant vector of size \\(n(n-1)/2\\). Here is the output for a \\(3\\) by \\(3\\) matrix:\nmx  = c(0, 0, 0, 1, 1, 1)\nmy  = c(1, 0, 1, 1, 0, 1)\nmz  = c(1, 1, 1, 0, 1, 1)\nmat = rbind(mx, my, mz)\ndist(mat)__\n\n\n         mx       my\nmy 1.732051         \nmz 2.000000 1.732051\n\n\ndist(mat, method = \"binary\")__\n\n\n          mx        my\nmy 0.6000000          \nmz 0.6666667 0.5000000\nIn order to access a particular distance (for example the distance between observations 1 and 2), one has to turn the dist class object back into a matrix.\nload(\"../data/Morder.RData\")\nsqrt(sum((Morder[1, ] - Morder[2, ])^2))__\n\n\n[1] 5.593667\n\n\nas.matrix(dist(Morder))[2, 1]__\n\n\n[1] 5.593667\nLet’s look at how we would compute the Jaccard distance we defined above between HIV strains.\nmut = read.csv(\"../data/HIVmutations.csv\")\nmut[1:3, 10:16]__\n\n\n  p32I p33F p34Q p35G p43T p46I p46L\n1    0    1    0    0    0    0    0\n2    0    1    0    0    0    1    0\n3    0    1    0    0    0    0    0\n__\nQuestion 5.2\nCompare the Jaccard distance (available as the function vegdist in the R package vegan) between mutations in the HIV data mut to the correlation based distance.\n__\nSolution\n__\nlibrary(\"vegan\")\nmutJ = vegdist(mut, \"jaccard\")\nmutC = sqrt(2 * (1 - cor(t(mut))))\nmutJ __\n\n\n      1     2     3     4\n2 0.800                  \n3 0.750 0.889            \n4 0.900 0.778 0.846      \n5 1.000 0.800 0.889 0.900\n\n\nas.dist(mutC)__\n\n\n     1    2    3    4\n2 1.19               \n3 1.10 1.30          \n4 1.32 1.13 1.30     \n5 1.45 1.19 1.30 1.32\n\nFigure 5.8: An example of computing the cophenetic distance (xkcd).\nIt can also be interesting to compare complex objects that are not traditional vectors or real numbers using dissimilarities or distances. Gower’s distance for data of mixed modalities (both categorical factors and continuous variables) can be computed with the daisy function. In fact, distances can be defined between any pairs of objects, not just points in \\({ R}^p\\) or character sequences. For instance, the shortest.paths function from the igraph package that we will see in Chapter 10 computes the distance between vertices on a graph and the function cophenetic computes the distance between leaves of a tree as illustrated in Figure 5.8. We can compute the distance between trees using dist.multiPhylo in the distory package.\nThe Jaccard index between graphs can be computed by looking at two graphs built on the same nodes and counting the number of co-occurring edges. This is implemented in the function similarity in the igraph package. Distances and dissimilarities are also used to compare images, sounds, maps and documents. A distance can usefully encompass domain knowledge and, if carefully chosen, can lead to the solution of many hard problems involving heterogeneous data. Asking yourself what is the relevant notion of “closeness” or similarity for your data can provide useful ways of representing them, as we will explore in Chapter 9.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "05-chap.html#nonparametric-mixture-detection",
    "href": "05-chap.html#nonparametric-mixture-detection",
    "title": "7  5.1 이 장의 목표",
    "section": "7.3 5.4 Nonparametric mixture detection",
    "text": "7.3 5.4 Nonparametric mixture detection\n\n7.3.1 5.4.1 \\(k\\)-methods: \\(k\\)-means, \\(k\\)-medoids and PAM\n\n\n\nThe centers of the groups are sometimes called medoids, thus the name PAM (partitioning around medoids).\n\n\nThe centers of the groups are sometimes called medoids, thus the name PAM (partitioning around medoids).\nPartitioning or iterative relocation methods work well in high-dimensional settings, where we cannot4 easily use probability densities, the EM algorithm and parametric mixture modeling in the way we did in Chapter 4. Besides the distance measure, the main choice to be made is the number of clusters \\(k\\). The PAM (partitioning around medoids, Kaufman and Rousseeuw (2009)) method is as follows:\n4 This is due to the so-called curse of dimensionality. We will discuss this in more detail in Chapter 12.\n\nStarts from a matrix of \\(p\\) features measured on a set of \\(n\\) observations.\nRandomly pick \\(k\\) distinct cluster centers out of the \\(n\\) observations (“seeds”).\nAssign each of the remaining observation to the group to whose center it is the closest.\nFor each group, choose a new center from the observations in the group, such that the sum of the distances of group members to the center is minimal; this is called the medoid.\nRepeat Steps 3 and 4 until the groups stabilize.\n\nEach time the algorithm is run, different initial seeds will be picked in Step 2, and in general, this can lead to different final results. A popular implementation is the pam function in the package cluster.\nA slight variation of the method replaces the medoids by the arithmetic means (centers of gravity) of the clusters and is called \\(k\\)-means. While in PAM, the centers are observations, this is not, in general, the case with \\(k\\)-means. The function kmeans comes with every installation of R in the stats package; an example run is shown in Figure 5.9.\n[](05-chap_files/figure-html/fig-clust-kmeansastep1-1.png “Figure 5.9 (a):”)\n\n\n\n[](05-chap_files/figure-html/fig-clust-kmeansastep1-2.png “Figure 5.9 (b):”)\n\n\n\n[](05-chap_files/figure-html/fig-clust-kmeansastep1-3.png “Figure 5.9 (c):”)\n\n\n\nFigure 5.9: An example run of the \\(k\\)-means algorithm. The initial, randomly chosen centers (black circles) and groups (colors) are shown in (a). The group memberships are assigned based on their distance to centers. At each iteration (b) and (c), the group centers are redefined, and the points reassigned to the cluster centers.\nThese so-called \\(k\\)-methods are the most common off-the-shelf methods for clustering; they work particularly well when the clusters are of comparable size and convex (blob-shaped). On the other hand, if the true clusters are very different in size, the larger ones will tend to be broken up; the same is true for groups that have pronounced non-spherical or non-elliptic shapes.\n__\nQuestion 5.3\nThe \\(k\\)-means algorithm alternates between computing the average point and assigning the points to clusters. How does this alternating, iterative method differ from an EM-algorithm?\n__\nSolution\n__\nIn the EM algorithm, each point participates in the computation of the mean of all the groups through a probabilistic weight assigned to it. In the \\(k\\)-means method, the points are either attributed to a cluster or not, so each point participates only, and entirely, in the computation of the center of one cluster.\n\n\n7.3.2 5.4.2 Tight clusters with resampling\nThere are clever schemes that repeat the process many times using different initial centers or resampled datasets. Repeating a clustering procedure multiple times on the same data, but with different starting points creates strong forms according to Diday and Brito (1989). Repeated subsampling of the dataset and applying a clustering method will result in groups of observations that are “almost always” grouped together; these are called tight clusters (Tseng and Wong 2005). The study of strong forms or tight clusters facilitates the choice of the number of clusters. A recent package developed to combine and compare the output from many different clusterings is clusterExperiment. Here we give an example from its vignette. Single-cell RNA-Seq experiments provide counts of reads, representing gene transcripts, from individual cells. The single cell resolution enables scientists, among other things, to follow cell lineage dynamics. Clustering has proved very useful for analysing such data.\n__\nQuestion 5.4\nFollow the vignette of the package clusterExperiment. Call the ensemble clustering function clusterMany, using pam for the individual clustering efforts. Set the choice of genes to include at either the 60, 100 or 150 most variable genes. Plot the clustering results for \\(k\\) varying between 4 and 9. What do you notice?\n__\nSolution\n__\nThe following code produces Figure 5.10.\nlibrary(\"clusterExperiment\")\nfluidigm = scRNAseq::ReprocessedFluidigmData()\nse = fluidigm[, fluidigm$Coverage_Type == \"High\"]\nassays(se) = list(normalized_counts = as.matrix(limma::normalizeQuantiles(assay(se))))\nce = clusterMany(se, clusterFunction = \"pam\", ks = c(5, 7, 9), run = TRUE,\n                 isCount = TRUE, reduceMethod = \"var\", nFilterDims = c(60, 100, 150))__\n\n\n9 parameter combinations, 0 use sequential method, 0 use subsampling method\nRunning Clustering on Parameter Combinations...\ndone.\n\n\nclusterLabels(ce) = sub(\"FilterDims\", \"\", clusterLabels(ce))\nplotClusters(ce, whichClusters = \"workflow\", axisLine = -1)__\n\nFigure 5.10: Comparison of clustering results (rows), for different numbers of included genes and for varying numbers of clusters, \\(k\\). Each column of the heatmap corresponds to a cell, and the colors represent the cluster assignments.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "05-chap.html#군집화-예시-유세포-분석-및-질량-분석",
    "href": "05-chap.html#군집화-예시-유세포-분석-및-질량-분석",
    "title": "7  5.1 이 장의 목표",
    "section": "7.4 5.5 군집화 예시: 유세포 분석 및 질량 분석",
    "text": "7.4 5.5 군집화 예시: 유세포 분석 및 질량 분석\n\n\n\n유세포 분석을 위한 생물정보학적 방법들에 대한 리뷰는 (O’Neill et al. 2013)와 잘 관리된 위키백과 문서에서 찾을 수 있습니다.\n\n\n유세포 분석을 위한 생물정보학적 방법들에 대한 리뷰는 (O’Neill et al. 2013)와 잘 관리된 위키백과 문서에서 찾을 수 있습니다.\n단일 세포에 대한 측정값을 연구하면 세포 유형과 역학을 분석할 수 있는 초점과 해상도가 모두 향상됩니다. 유세포 분석(flow cytometry)은 약 10가지의 서로 다른 세포 마커를 동시에 측정할 수 있게 해줍니다. 질량 분석(mass cytometry)은 측정 컬렉션을 세포당 최대 80개의 단백질로 확장합니다. 이 기술의 특히 유망한 응용 분야는 면역 세포 역학 연구입니다.\n\n7.4.1 5.5.1 유세포 분석 및 질량 분석\n발달의 서로 다른 단계에서 면역 세포는 표면에 고유한 단백질 조합을 발현합니다. 이러한 단백질 마커는 CD (clusters of differentiation, 분화 클러스터)라고 불리며 유세포 분석(형광 사용, Hulett et al. (1969) 참조) 또는 질량 분석(중원소 리포터의 단일 세포 원자 질량 분석법 사용, Bendall et al. (2012) 참조)에 의해 수집됩니다. 흔히 사용되는 CD의 예로 CD4가 있는데, 이 단백질은 “CD4+”라고 불리는 보조 T 세포(helper T cells)에 의해 발현됩니다. 그러나 일부 세포는 CD4를 발현하지만(따라서 CD4+임), 실제로는 보조 T 세포가 아니라는 점에 유의하세요. 먼저 세포 분석(cytometry) 데이터를 위한 유용한 Bioconductor 패키지인 flowCore와 flowViz를 불러오고, 다음과 같이 예시 데이터 객체 fcsB를 읽어들입니다:\nlibrary(\"flowCore\")\nlibrary(\"flowViz\")\nfcsB = read.FCS(\"../data/Bendall_2011.fcs\", truncate_max_range = FALSE)\nslotNames(fcsB)__\n\n\n[1] \"exprs\"       \"parameters\"  \"description\"\n그림 5.11은 fcsB 데이터에서 사용 가능한 두 변수의 산점도를 보여줍니다. (이러한 플롯을 만드는 방법은 아래에서 살펴보겠습니다.) 이 두 차원에서 명확한 이봉성과 군집화를 볼 수 있습니다.\n__\n질문 5.5\n\nfcsB 객체의 구조를 살펴보세요(힌트: colnames 함수). 얼마나 많은 변수가 측정되었나요?\n처음 몇 행을 보기 위해 데이터를 하위 집합화해 보세요(힌트: Biobase::exprs(fcsB) 사용). 얼마나 많은 세포가 측정되었나요?\n\n\n\n7.4.2 5.5.2 데이터 전처리\n먼저 동위원소(isotopes)와 마커(항체) 사이의 매핑을 보고하는 테이블 데이터를 불러온 다음, fcsB의 열 이름에 있는 동위원소 이름을 마커 이름으로 바꿉니다. 이렇게 하면 후속 분석 및 플로팅 코드가 더 읽기 쉬워집니다:\nmarkersB = readr::read_csv(\"../data/Bendall_2011_markers.csv\")\nmt = match(markersB$isotope, colnames(fcsB))\nstopifnot(!any(is.na(mt)))\ncolnames(fcsB)[mt] = markersB$marker __\n이제 그림 5.11을 생성할 준비가 되었습니다.\nflowPlot(fcsB, plotParameters = colnames(fcsB)[2:3], logy = TRUE)__\n\nFigure 5.11: Cell measurements that show clear clustering in two dimensions.\nPlotting the data in two dimensions as in Figure 5.11 already shows that the cells can be grouped into subpopulations. Sometimes just one of the markers can be used to define populations on their own; in that case simple rectangular gating is used to separate the populations; for instance, CD4+ cells can be gated by taking the subpopulation with high values for the CD4 marker. Cell clustering can be improved by carefully choosing transformations of the data. The left part of Figure 5.12 shows a simple one dimensional histogram before transformation; on the right of Figure 5.12 we see the distribution after transformation. It reveals a bimodality and the existence of two cell populations.\nData Transformation: hyperbolic arcsin (asinh). It is standard to transform both flow and mass cytometry data using one of several special functions. We take the example of the inverse hyperbolic sine (asinh):\n\\[ (x) = . \\]\nFrom this we can see that for large values of \\(x\\), \\((x)\\)는 로그 함수처럼 행동하며 실제로는 \\((x)+(2)\\)와 거의 같습니다. (x)가 작을 때 이 함수는 (x)에 대해 거의 선형적입니다.\n__\n태스크\n변환의 두 가지 주요 영역인 작은 값과 큰 값을 확인하기 위해 다음 코드를 실행해 보세요.\nv1 = seq(0, 1, length.out = 100)\nplot(log(v1), asinh(v1), type = 'l')__\n\n\n plot(v1, asinh(v1), type = 'l')__\n\n\nv3 = seq(30, 3000, length = 100)\nplot(log(v3), asinh(v3), type= 'l')__\n이것은 4장과 8장에서도 언급된 분산 안정화 변환의 또 다른 예입니다. 그림 5.12는 flowCore 패키지의 arcsinhTransform 함수를 사용하는 다음 코드로 생성되었습니다.\nasinhtrsf = arcsinhTransform(a = 0.1, b = 1)\nfcsBT = transform(fcsB, transformList(colnames(fcsB)[-c(1, 2, 41)], asinhtrsf))\ndensityplot(~`CD3all`, fcsB)\ndensityplot(~`CD3all`, fcsBT)__\n\n\n\n\n\n\n\n\n그림 5.12: 패널 (a)는 CD3all 변수의 히스토그램을 보여줍니다: 세포들이 0 근처에 군집해 있고 몇 개의 큰 값들이 있습니다. (b)에서는 asinh 변환 후 세포들이 군집을 이루어 두 그룹 또는 유형으로 나뉘는 것을 볼 수 있습니다.\n__\n질문 5.6\n다음 코드는 \\(k\\)-평균을 사용하여 데이터를 2개의 그룹으로 나누기 위해 몇 개의 차원을 사용하나요?\nkf = kmeansFilter(\"CD3all\" = c(\"Pop1\",\"Pop2\"), filterId=\"myKmFilter\")\nfres = flowCore::filter(fcsBT, kf)\nsummary(fres)__\n\n\nPop1: 33434 of 91392 events (36.58%)\nPop2: 57958 of 91392 events (63.42%)\n\n\nfcsBT1 = flowCore::split(fcsBT, fres, population = \"Pop1\")\nfcsBT2 = flowCore::split(fcsBT, fres, population = \"Pop2\")__\n다음 코드로 생성된 그림 5.13은 CD3와 CD56 마커에 의해 확장된 두 차원으로 데이터를 단순 투영한 것을 보여줍니다:\nlibrary(\"flowPeaks\")\nfp = flowPeaks(Biobase::exprs(fcsBT)[, c(\"CD3all\", \"CD56\")])\nplot(fp)__\n\n그림 5.13: 변환 후 이 세포들은 kmeans를 사용하여 군집화되었습니다.\n어느 영역에 밀집된 점들을 플롯할 때는 겹쳐그리기(overplotting)를 피해야 합니다. 3장에서 선호되는 기법들 중 일부를 보았습니다. 여기서는 등고선(contours)과 음영(shading)을 사용합니다. 다음과 같이 수행합니다:\nflowPlot(fcsBT, plotParameters = c(\"CD3all\", \"CD56\"), logy = FALSE)\ncontour(fcsBT[, c(40, 19)], add = TRUE)__\n\n그림 5.14: 그림 5.13과 유사하지만 등고선을 사용했습니다.\n이 코드는 그림 5.13보다 더 많은 정보를 담고 있는 그림 5.14를 생성합니다.\n__\n태스크\nBioconductor 패키지 ggcyto는 ggplot을 사용하여 각 환자의 데이터를 서로 다른 패싯(facet)에 그릴 수 있게 해줍니다. 다음과 같은 방식으로 이 접근법을 사용한 출력과 위에서 수행한 작업을 비교해 보세요:\nlibrary(\"ggcyto\")\nlibrary(\"labeling\")\n\np1 = ggcyto(fcsB, aes(x = CD4)) + geom_histogram(bins = 60)\np2 = ggcyto(fcsB, aes(x = CD8)) + geom_histogram(bins = 60)\np3 = ggcyto(fcsB, aes(x = CD4, y = CD8)) + geom_density2d(colour = \"black\")\n\nfcsBT = transform(fcsB, transformList(colnames(fcsB)[-c(1, 2, 41)], \n                                      arcsinhTransform(a = 0, b = 1)))\n                                      \np1t = ggcyto(fcsBT, aes(x = CD4))            + geom_histogram(bins = 90)\np2t = ggcyto(fcsBT, aes(x = CD4,y = CD8))    + geom_density2d(colour = \"black\")\np3t = ggcyto(fcsBT, aes(x = CD45RA,y = CD20))+ geom_density2d(colour = \"black\")__\n\n\n7.4.3 5.5.3 밀도 기반 군집화(Density-based clustering)\n마커 수가 적고 세포 수가 많은 유세포 분석과 같은 데이터 세트는 밀도 기반 군집화에 적합합니다. 이 방법은 희소한 영역에 의해 분리된 고밀도 영역을 찾습니다. 이 방법은 클러스터가 반드시 볼록할 필요가 없는 경우에도 대처할 수 있다는 장점이 있습니다. 이러한 방법의 한 구현체로 dbscan이 있습니다. 다음 코드를 실행하여 예시를 살펴보겠습니다.\nlibrary(\"dbscan\")\nmc5 = Biobase::exprs(fcsBT)[, c(15,16,19,40,33)]\nres5 = dbscan::dbscan(mc5, eps = 0.65, minPts = 30)\nmc5df = data.frame(mc5, cluster = as.factor(res5$cluster))\ntable(mc5df$cluster)__\n\n\n    0     1     2     3     4     5     6     7     8 \n76053  4031  5450  5310   257   160    63    25    43 \n\n\nggplot(mc5df, aes(x=CD4,    y=CD8,  col=cluster))+geom_density2d()\nggplot(mc5df, aes(x=CD3all, y=CD20, col=cluster))+geom_density2d()__\n\n\n\n\n\n\n\n\n그림 5.15: 이 두 플롯은 5개의 마커를 사용하여 dbscan으로 군집화한 결과를 보여줍니다. 여기서는 데이터를 CD4-CD8 및 C3all-CD20 평면에 투영한 것만 보여줍니다.\n출력 결과는 그림 5.15에 나와 있습니다. 2D 투영에서의 클러스터 중첩을 통해 군집화의 다차원적 특성을 이해할 수 있습니다.\n__\n질문 5.7\n입력 데이터에서 CD 마커 변수 하나를 추가하여 차원을 6으로 늘려보세요.\n그런 다음 eps를 변화시키면서, 적어도 두 개가 100개 이상의 점을 가진 4개의 클러스터를 찾아보세요.\n7개의 CD 마커 변수로 이 작업을 반복해 보세요. 무엇을 알 수 있나요?\n__\n해결책\n__\n다음 6개 마커를 사용한 예시입니다.\nmc6 = Biobase::exprs(fcsBT)[, c(15, 16, 19, 33, 25, 40)]\nres = dbscan::dbscan(mc6, eps = 0.65, minPts = 20)\nmc6df = data.frame(mc6, cluster = as.factor(res$cluster))\ntable(mc6df$cluster)__\n\n\n    0     1     2     3     4     5     6 \n91068    34    61    20    67   121    21 \n우리는 eps=0.75일 때 eps=0.65일 때보다 충분히 큰 클러스터를 찾기가 더 쉽다는 것을 알 수 있으며, eps=0.55일 때는 불가능합니다. 차원수를 7로 늘리면 eps를 훨씬 더 크게 만들어야 합니다.\nmc7 = Biobase::exprs(fcsBT)[, c(11, 15, 16, 19, 25, 33, 40)]\nres = dbscan::dbscan(mc7, eps = 0.95, minPts = 20)\nmc7df = data.frame(mc7, cluster = as.factor(res$cluster))\ntable(mc7df$cluster)__\n\n\n    0     1     2     3     4     5     6     7     8     9    10 \n90249    21   102   445   158   119    19   224    17    20    18 \n이는 소위 차원의 저주(curse of dimensionality)가 실제로 작동하는 것을 보여주며, 이에 대해서는 12장에서 더 자세히 다룹니다.\n\n7.4.3.1 밀도 기반 군집화(dbscan)는 어떻게 작동하나요?\ndbscan 방법은 밀도 연결성(density-connectedness) 기준에 따라 고밀도 영역의 점들을 군집화합니다. 이 방법은 점들이 연결되어 있는지 확인하기 위해 반지름 \\(\\)인 작은 이웃 구(neighborhood spheres)를 살펴봅니다.\ndbscan의 기본 구성 요소는 밀도 도달 가능성(density-reachability) 개념입니다: 점 \\(q\\)가 점 \\(p\\)로부터 주어진 임계값 \\(\\)보다 멀리 있지 않고, \\(p\\)가 충분히 많은 점들에 둘러싸여 있어 \\(p\\)(및 \\(q\\))를 밀집 영역의 일부로 간주할 수 있다면, 점 \\(q\\)는 점 \\(p\\)로부터 직접 밀도 도달 가능(density-reachable)합니다. \\(p_1 = p\\)이고 \\(p_n = q\\)인 일련의 점 \\(p_1, …, p_n\\)이 있어서 각 \\(p_{i+1}\\)이 \\(p_i\\)로부터 직접 밀도 도달 가능하다면, \\(q\\)는 \\(p\\)로부터 _밀도 도달 가능_하다고 합니다.\n\n\n\n방법이 이웃에서 높은 점 밀도를 찾는 것이 중요합니다. 클러스터 사이의 공백(void) 또는 “누락된 점”에 의해 클러스터를 정의하려는 다른 방법들이 존재합니다. 그러나 이러한 방법들은 차원의 저주에 취약하며, 가짜 “공백”을 만들어낼 수 있습니다.\n\n\n방법이 이웃에서 높은 점 밀도를 찾는 것이 중요합니다. 클러스터 사이의 공백(void) 또는 “누락된 점”에 의해 클러스터를 정의하려는 다른 방법들이 존재합니다. 그러나 이러한 방법들은 차원의 저주에 취약하며, 가짜 “공백”을 만들어낼 수 있습니다.\n그러면 _클러스터_는 다음 속성들을 만족하는 점들의 하위 집합입니다:\n\n클러스터 내의 모든 점은 서로 밀도 연결되어 있습니다.\n어떤 점이 클러스터의 임의의 점에 밀도 연결되어 있다면, 그 점 역시 클러스터의 일부입니다.\n점들의 그룹이 클러스터로 간주되려면 적어도 MinPts개의 점을 가져야 합니다.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "05-chap.html#계층적-군집화hierarchical-clustering",
    "href": "05-chap.html#계층적-군집화hierarchical-clustering",
    "title": "7  5.1 이 장의 목표",
    "section": "7.5 5.6 계층적 군집화(Hierarchical clustering)",
    "text": "7.5 5.6 계층적 군집화(Hierarchical clustering)\n\n그림 5.16: 특성 유사성에 따라 유기체들을 군집화한 린네의 분류 체계의 일부.\n계층적 군집화는 유사한 관측치와 하위 클래스를 반복적으로 조립하는 상향식(bottom-up) 접근 방식입니다. 그림 5.16은 린네가 특정 특성에 따라 유기체들의 중첩된 클러스터를 어떻게 만들었는지 보여줍니다. 이러한 계층적 조직은 많은 분야에서 유용하게 사용되어 왔으며, _자연의 사다리(ladder of nature)_를 상정한 아리스토텔레스까지 거슬러 올라갑니다.\n덴드로그램 순서(Dendrogram ordering). 그림 5.17의 예에서 볼 수 있듯이, 레이블의 순서는 형제 쌍(sibling pairs) 내에서는 중요하지 않습니다. 수평 거리는 대개 무의미한 반면, 수직 거리는 어떤 정보를 인코딩합니다. 이러한 속성들은 계통발생학적(monophyletic)이지 않지만(즉, 동일한 서브트리나 클레이드(clade)에 속하지 않음) 플롯에서는 이웃으로 나타나는 대상(예를 들어 오른쪽 나무의 B와 D)에 대해 해석을 내릴 때 기억해야 할 중요한 사항입니다.\n\n그림 5.17: 동일한 계층적 군집 트리의 세 가지 표현 방식.\n하향식 계층 구조(Top-down hierarchies). 대안적인 하향식 접근 방식은 모든 객체를 가져와서 선택된 기준에 따라 순차적으로 분할합니다. 이러한 소위 재귀적 분할(recursive partitioning) 방법은 종종 의사 결정 나무(decision trees)를 만드는 데 사용됩니다. 이들은 예측(예를 들어 의료 진단이 주어졌을 때의 생존 기간)에 유용할 수 있습니다: 우리는 그러한 사례들에서 분할을 통해 불균질한 모집단을 더 균질한 하위 그룹으로 나누기를 희망합니다. 이 장에서 우리는 상향식 접근 방식에 집중합니다. 12장에서 비지도 학습과 분류에 대해 이야기할 때 분할 방식으로 다시 돌아올 것입니다.\n\n7.5.1 5.6.1 집계된 클러스터 간의 (비)유사성을 어떻게 계산하는가?\n\nFigure 5.18: In the single linkage method, the distance between groups \\(C_1\\) and \\(C_2\\) is defined as the distance between the closest two points from the groups.\nA hierarchical clustering algorithm, which works by aggregation, is easy enough to get started, by grouping the most similar observations together. But we will need more than just the distances between all pairs of individual objects. Once an aggregation is made, one is required to say how the distance between the newly formed cluster and all other points (or existing clusters) is computed. There are different choices, all based on the object-object distances, and each choice results in a different type of hierarchical clustering.\nThe minimal jump method, also called single linkage or nearest neighbor method computes the distance between clusters as the smallest distance between any two points in the two clusters (as shown in Figure 5.18):\n\\[ d_{12} = {i C_1, j C_2 } d{ij}. \\]\nThis method tends to create clusters that look like contiguous strings of points. The cluster tree often looks like a comb.\n\nFigure 5.19: In the complete linkage method, the distance between groups \\(C_1\\) and \\(C_2\\) is defined as the maximum distance between pairs of points from the two groups.\nThe maximum jump (or complete linkage) method defines the distance between clusters as the largest distance between any two objects in the two clusters, as represented in Figure 5.19:\n\\[ d_{12} = {i C_1, j C_2 } d{ij}. \\]\nThe average linkage method is half way between the two above (here, \\(|C_k|\\) denotes the number of elements of cluster \\(k\\)):\n\\[ d_{12} = {i C_1, j C_2 } d{ij} \\]\n\nFigure 5.20: The Ward method maximizes the between group sum of squares (red edges), while minimizing the sums of squares within groups (black edges).\nWard’s method takes an analysis of variance approach, where the goal is to minimize the variance within clusters. This method is very efficient, however, it tends to create clusters of smaller sizes.\n\n\n\nAdvantages and disadvantages of various choices of defining distances between aggregates (Chakerian and Holmes 2012). Method\nPros\nCons\n\n\n\n\nSingle linkage\nnumber of clusters\ncomblike trees\n\n\nComplete linkage\ncompact classes\none observation can alter groups\n\n\nAverage linkage\nsimilar size and variance\nnot robust\n\n\nCentroid\nrobust to outliers\nsmaller number of clusters\n\n\nWard\nminimising an inertia\nclasses small if high variability\n\n\n\n\nFigure 5.21: Hierarchical clustering output has similar properties to a mobile: the branches can rotate freely from their suspension points.\nThese are the choices we have to make building hierarchical clustering trees. An advantage of hierarchical clustering compared to the partitioning methods 좋은 점은 그룹화의 강도를 그래픽으로 진단할 수 있다는 것입니다: 트리의 내부 에지(inner edges)의 길이입니다.\n클러스터들의 크기가 거의 같다는 사전 지식이 있다면, 그룹 내 분산을 최소화하는 평균 연결법(average linkage)이나 와드 방법(Ward’s method)을 사용하는 것이 가장 좋은 전술입니다.\n__\n질문 5.8\n세포군에 대한 계층적 군집화 Morder 데이터는 10명의 환자로부터 얻은 3가지 유형(naïve, effector, memory)의 T 세포에 대한 156개 유전자의 발현 측정값입니다(Holmes et al. 2005). pheatmap 패키지를 사용하여, 이 데이터의 유클리드 거리와 맨해튼 거리에 대해 덴드로그램이나 재정렬 없이 두 개의 단순한 히트맵을 만드세요.\n__\n질문 5.9\n이제 이 두 거리를 사용한 계층적 군집 트리에서의 순서 차이를 살펴보세요. 어떤 차이점이 눈에 띄나요?\n: “)\n\n\n\n: “)\n\n\n\n[](imgs/complete14heatmap.png “그림 5.22 (c):”)\n\n\n\n그림 5.22: 서로 다른 응집(agglomeration) 선택으로 만들어진 세 개의 계층적 군집 플롯. (a)의 단일 연결법(single linkage)에 대한 빗 모양 구조에 주목하세요. 평균 연결법 (b)와 완전 연결법 (c) 트리는 내부 분기의 길이에 의해서만 다릅니다.\n\n그림 5.23: 이 트리는 여러 가지 다른 방식으로 그려질 수 있습니다. 여기에 나타난 잎들의 순서는 \\((8,11,9,10,7,5,6,1,4,2,3)\\)입니다.\n__\n질문 5.10\n계층적 군집 트리는 그림 5.21의 콜더 모빌(Calder mobile)과 같아서 많은 내부 피벗 점들을 중심으로 회전할 수 있으며, 주어진 트리와 일치하는 많은 팁(tips) 순서를 제공합니다. 그림 5.23의 트리를 보세요. 이 트리와 일관성을 유지하면서 팁 레이블을 정렬할 수 있는 방법은 몇 가지가 있을까요?\n행 및/또는 열이 계층적 군집 트리에 기반하여 정렬된 히트맵을 흔히 볼 수 있습니다. 때때로 이것은 일부 클러스터를 매우 강력해 보이게 만듭니다 – 트리가 실제로 암시하는 것보다 더 강력하게 말이죠. 히트맵에서 행과 열을 정렬하는 대안적인 방법들이 있는데, 예를 들어 서열화 방법(ordination methods)5을 사용하여 순서를 찾는 NeatMap 패키지가 있습니다.\n5 이들은 9장에서 설명될 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "05-chap.html#클러스터-수-검증-및-선택",
    "href": "05-chap.html#클러스터-수-검증-및-선택",
    "title": "7  5.1 이 장의 목표",
    "section": "7.6 5.7 클러스터 수 검증 및 선택",
    "text": "7.6 5.7 클러스터 수 검증 및 선택\n우리가 설명한 군집화 방법들은 다양한 제약 조건 하에서 데이터의 좋은 그룹화를 제공하도록 맞춤화되어 있습니다. 그러나 군집화 방법은 클러스터가 없더라도 항상 그룹을 제공한다는 점을 명심하세요. 만약 데이터에 실제 클러스터가 없다면, 계층적 군집 트리는 상대적으로 짧은 내부 분기를 보여줄 수 있지만, 이를 정량화하기는 어렵습니다. 일반적으로 보다 객관적인 기준으로 클러스터 선택을 검증하는 것이 중요합니다.\n군집화 결과의 품질을 평가하는 한 가지 기준은 그룹 내 거리를 작게 유지하면서 그룹 간 차이를 어느 정도까지 최대화하느냐 하는 것입니다(그림 5.20에서 빨간색 선의 길이를 최대화하고 검은색 선의 길이를 최소화하는 것). 우리는 이를 그룹 내 제곱 거리 합(within-groups sum of squared distances, WSS)으로 공식화합니다:\n\\[ k={}^k {x_i C} d^2(x_i, {x}_{}) \\]\n여기서 \\(k\\)는 클러스터의 수, \\(C_\\)은 \\(\\)번째 클러스터에 있는 객체들의 집합, 그리고 \\({x}_\\)은 \\(\\)번째 클러스터의 질량 중심(평균점)입니다. 우리는 동일한 클러스터 알고리즘에 대해 서로 다른 \\(k\\) 값에 걸쳐 이 수치를 비교하는 데 관심이 있으므로 식 5.4에서 WSS의 \\(k\\)에 대한 의존성을 명시합니다. 하지만 WSS 그 자체로는 충분한 기준이 되지 못합니다: WSS의 최솟값은 단순히 각 점을 개별 클러스터로 만듦으로써 얻어질 수 있기 때문입니다. WSS는 유용한 구성 요소이지만, 이 숫자만 보는 것보다 더 정교한 아이디어가 필요합니다.\n한 가지 아이디어는 \\(k\\)의 함수로서 \\(_k\\)를 살펴보는 것입니다. 이는 항상 감소 함수이겠지만, 급격히 감소하다가 완만해지는 뚜렷한 영역이 있다면, 우리는 이를 _엘보우(elbow)_라고 부르며 이를 클러스터 수의 잠재적인 최적 지점으로 간주할 수 있습니다.\n__\n질문 5.11\n**\\(_k\\)에 대한 대안적 표현**. R을 사용하여 클러스터 내의 모든 점 쌍 사이의 거리 합을 계산하고 이를 \\(_k\\)와 비교해 보세요. \\(_k\\)가 다음과 같이 쓰여질 수 있음을 알 수 있나요?\n\\[ k={}^k {x_i C} {x_j C} d^2(x_i,x_j), \\]\n여기서 \\(n_\\)은 \\(\\)번째 클러스터의 크기입니다.\n질문 5.11은 클러스터 내 제곱합 \\(_k\\)가 클러스터 내의 모든 점과 중심 사이의 거리뿐만 아니라, 클러스터 내의 모든 점 쌍 사이의 평균 거리도 측정한다는 것을 보여줍니다.\n데이터에 적합한 클러스터 수를 결정하는 데 도움이 되는 다양한 지수와 통계량의 거동을 살펴볼 때, 정답을 실제로 알고 있는 경우를 살펴보는 것이 유용할 수 있습니다.\n시작하기 위해, 네 개의 그룹에서 나오는 데이터를 시뮬레이션합니다. 우리는 파이프(%&gt;%) 연산자와 dplyr의 bind_rows 함수를 사용하여 각 클러스터에 해당하는 네 개의 _tibble_을 하나의 큰 _tibble_로 연결합니다.6\n6 파이프 연산자는 왼쪽에 있는 값을 오른쪽 함수로 전달합니다. 이는 코드에서 데이터의 흐름을 더 쉽게 따라갈 수 있게 해줍니다: f(x) %&gt;% g(y)는 g(f(x), y)와 동일합니다.\nlibrary(\"dplyr\")\nsimdat = lapply(c(0, 8), function(mx) {\n  lapply(c(0,8), function(my) {\n    tibble(x = rnorm(100, mean = mx, sd = 2),\n           y = rnorm(100, mean = my, sd = 2),\n           class = paste(mx, my, sep = \":\"))\n   }) %&gt;% bind_rows\n}) %&gt;% bind_rows\nsimdat __\n\n\n# A tibble: 400 × 3\n        x      y class\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;\n 1 -2.42  -4.59  0:0  \n 2  1.89  -1.56  0:0  \n 3  0.558  2.17  0:0  \n 4  2.51  -0.873 0:0  \n 5 -2.52  -0.766 0:0  \n 6  3.62   0.953 0:0  \n 7  0.774  2.43  0:0  \n 8 -1.71  -2.63  0:0  \n 9  2.01   1.28  0:0  \n10  2.03  -1.25  0:0  \n# ℹ 390 more rows\n\n\nsimdatxy = simdat[, c(\"x\", \"y\")] # class 레이블 제외 __\n\n\nggplot(simdat, aes(x = x, y = y, col = class)) + geom_point() +\n  coord_fixed()__\n\n그림 5.24: 클래스 레이블로 색상이 입혀진 simdat 데이터. 여기서는 우리가 데이터를 생성했으므로 레이블을 알고 있습니다 – 보통은 이를 알지 못합니다.\n우리는 \\(k\\)-평균 방법으로 얻은 클러스터들에 대해 그룹 내 제곱합을 계산합니다:\nwss = tibble(k = 1:8, value = NA_real_)\nwss$value[1] = sum(scale(simdatxy, scale = FALSE)^2)\nfor (i in 2:nrow(wss)) {\n  km  = kmeans(simdatxy, centers = wss$k[i])\n  wss$value[i] = sum(km$withinss)\n}\nggplot(wss, aes(x = k, y = value)) + geom_col()__\n\n그림 5.25: \\(k\\)의 함수로서 WSS 통계량의 막대 그래프는 마지막 실질적인 도약이 \\(k=4\\) 직전에 있음을 보여줍니다. 이는 이 데이터에 대한 최선의 선택이 \\(k=4\\)임을 나타냅니다.\n__\n질문 5.12\n\n위의 코드를 여러 번 실행하고 서로 다른 실행에서의 wss 값을 비교해 보세요. 왜 이들은 서로 다를까요?\nsimdat와 동일한 범위와 차원을 가진, 정규 분포 대신 균등 분포(uniform distributions)로부터 오는 데이터 세트를 만드세요. 이 데이터에 대해 WSS 값을 계산해 보세요. 무엇을 결론지을 수 있나요?\n\n__\n질문 5.13\n이른바 칼린스키-하라바츠(Calinski-Harabasz) 지수는 WSS와 BSS(between group sums of squares, 그룹 간 제곱합)를 사용합니다. 이는 분산 분석에서 사용되는 \\(F\\) 통계량 — 인자에 의해 설명되는 평균 제곱합 대 평균 잔차 제곱합의 비율 — 에서 영감을 받았습니다:\n\\[ (k)= k = {}^k n_({x}_{}-{x})^2, \\]\n여기서 \\({x}\\)는 전체 질량 중심(평균점)입니다. simdat 데이터에 대한 칼린스키-하라바츠 지수를 플롯해 보세요.\n__\n해결책\n__\n그림 5.26을 생성하는 코드는 다음과 같습니다.\nlibrary(\"fpc\")\nlibrary(\"cluster\")\nCH = tibble(\n  k = 2:8,\n  value = sapply(k, function(i) {\n    p = pam(simdatxy, i)\n    calinhara(simdatxy, p$cluster)\n  })\n)\nggplot(CH, aes(x = k, y = value)) + geom_line() + geom_point() +\n  ylab(\"CH index\")__\n\n그림 5.26: simdat 데이터에 대해 계산된, 서로 다른 \\(k\\) 선택에 대한 그룹 간 분산과 그룹 내 분산의 비율인 칼린스키-하라바츠 지수.\n\n7.6.1 5.7.1 갭 통계량(gap statistic) 사용하기\n그룹 내 제곱합의 로그(\\((_k)\\))를 취하고 이를 구조가 덜한 시뮬레이션 데이터의 평균과 비교하는 것은 \\(k\\)를 선택하는 좋은 방법이 될 수 있습니다. 이것이 Tibshirani, Walther, Hastie (2001)에 의해 도입된 갭 통계량(gap statistic)의 기본 아이디어입니다. 우리는 클러스터 수인 여러 \\(k\\) 값에 대해 \\((_k)\\)를 계산하고, 이를 다양한 가능한 ‘군집되지 않은’ 분포를 가진 유사한 차원의 참조 데이터에서 얻은 것과 비교합니다. 위에서 했던 것처럼 균등하게 분포된 데이터를 사용하거나, 원래 데이터와 동일한 공분산 구조를 가진 시뮬레이션 데이터를 사용할 수 있습니다.\n\n이 알고리즘은 관측된 데이터에 대한 갭 통계량을 유사한 구조를 가진 시뮬레이션 데이터의 평균과 비교하는 몬테카를로 방법입니다.\n갭 통계량 계산 알고리즘 (Tibshirani, Walther, and Hastie 2001):\n\n데이터를 \\(k\\)개의 클러스터로 군집화하고 다양한 \\(k\\) 선택에 대해 \\(_k\\)를 계산합니다.\n균질한 분포로부터 몬테카를로 샘플링을 사용하여 \\(B\\)개의 그럴듯한 참조 데이터 세트를 생성하고, 이 새로운 시뮬레이션 데이터에 대해 위의 1단계를 다시 수행합니다. 그 결과 시뮬레이션 데이터에 대한 \\(B\\)개의 새로운 그룹 내 제곱합 \\(W_{kb}^*\\)(\\(b=1,…,B\\))을 얻습니다.\n\\((k)\\)-통계량을 계산합니다:\n\n\\[ (k) = _k - _k k ={b=1}^B W^*_{kb} \\]\n군집화가 잘 되었다면(즉, WSS가 더 작다면) 첫 번째 항이 두 번째 항보다 클 것으로 예상됩니다. 따라서 갭 통계량은 대부분 양수일 것이며 우리는 그 최댓값을 찾습니다.\n\n표준 편차\n\n\\[ k^2 = {b=1}B((W*_{kb})-_k)^2 \\]\n를 사용하여 최적의 \\(k\\)를 선택하는 데 도움을 줄 수 있습니다. 여러 선택지가 있는데, 예를 들어 다음과 같은 최소의 \\(k\\)를 선택하는 것입니다.\n\\[ (k) (k+1) - s’{k+1} s’{k+1}=_{k+1}. \\]\nThe packages cluster and clusterCrit provide implementations.\n__\nQuestion 5.14\nMake a function that plots the gap statistic as in Figure 5.27. Show the output for the simdat example dataset clustered with the pam function.\n__\nSolution\n__\nlibrary(\"cluster\")\nlibrary(\"ggplot2\")\npamfun = function(x, k)\n  list(cluster = pam(x, k, cluster.only = TRUE))\n\ngss = clusGap(simdatxy, FUN = pamfun, K.max = 8, B = 50,\n              verbose = FALSE)\nplot_gap = function(x) {\n  gstab = data.frame(x$Tab, k = seq_len(nrow(x$Tab)))\n  ggplot(gstab, aes(k, gap)) + geom_line() +\n    geom_errorbar(aes(ymax = gap + SE.sim,\n                      ymin = gap - SE.sim), width=0.1) +\n    geom_point(size = 3, col=  \"red\")\n}\nplot_gap(gss)__\n\nFigure 5.27: The gap statistic, see Question 5.14.\nLet’s now use the method on a real example. We load the Hiiragi data that we already explored in Chapter 3 and will see how the cells cluster.\nlibrary(\"Hiiragi2013\")__\n\n\nIn chunk 'Hiiragi': Warning: replacing previous import 'boot::logit' by 'gtools::logit' whenloading 'Hiiragi2013'\n\n\nIn chunk 'Hiiragi': Warning: replacing previous import 'boot::inv.logit' by 'gtools::inv.logit'when loading 'Hiiragi2013'\n\n\ndata(\"x\")__\nWe start by choosing the 50 most variable genes (features)7.\n7 The intention behind this step is to reduce the influence of technical (or batch) effects. Although individually small, when accumulated over all the 45101 features in x, many of which match genes that are weakly or not expressed, without this feature selection step, such effects are prone to suppress the biological signal.\nselFeats = order(rowVars(Biobase::exprs(x)), decreasing = TRUE)[1:50]\nembmat = t(Biobase::exprs(x)[selFeats, ])\nembgap = clusGap(embmat, FUN = pamfun, K.max = 24, verbose = FALSE)\nk1 = maxSE(embgap$Tab[, \"gap\"], embgap$Tab[, \"SE.sim\"])\nk2 = maxSE(embgap$Tab[, \"gap\"], embgap$Tab[, \"SE.sim\"],\n           method = \"Tibs2001SEmax\")\nc(k1, k2)__\n\n\n[1] 9 7\nThe default choice for the number of clusters, k1, is the first value of \\(k\\) for which the gap is not larger than the first local maximum minus a standard error \\(s\\) (see the manual page of the clusGap function). This gives a number of clusters \\(k = 9\\), whereas the choice recommended by Tibshirani, Walther, and Hastie (2001) is the smallest \\(k\\) such that \\((k) (k+1) - s_{k+1}’\\), this gives \\(k = 7\\). Let’s plot the gap statistic (Figure 5.28).\nplot(embgap, main = \"\")\ncl = pamfun(embmat, k = k1)$cluster\ntable(pData(x)[names(cl), \"sampleGroup\"], cl)__\n\n\n                 cl\n                   1  2  3  4  5  6  7  8  9\n  E3.25           23 11  1  1  0  0  0  0  0\n  E3.25 (FGF4-KO)  0  0  1 16  0  0  0  0  0\n  E3.5 (EPI)       2  1  0  0  0  8  0  0  0\n  E3.5 (FGF4-KO)   0  0  8  0  0  0  0  0  0\n  E3.5 (PE)        0  0  0  0  9  2  0  0  0\n  E4.5 (EPI)       0  0  0  0  0  0  0  4  0\n  E4.5 (FGF4-KO)   0  0  0  0  0  0  0  0 10\n  E4.5 (PE)        0  0  0  0  0  0  4  0  0\n\n그림 5.28: Hiiragi2013 데이터에 대한 갭 통계량.\n위에서 우리는 pamfun으로부터 얻은 군집화 결과와 데이터의 어노테이션에 포함된 샘플 레이블을 비교한 것을 볼 수 있습니다.\n__\n질문 5.15\n가장 가변적인 상위 50개 유전자만 사용하는 대신 x의 모든 특성을 사용하면 결과가 어떻게 달라질까요?\n\n\n7.6.2 5.7.2 붓스트랩을 이용한 클러스터 검증\n[](imgs/BootstrapClusterNew.png “그림 5.29 (a):”)\n\n\n\n[](imgs/BootstrapCluster2New.png “그림 5.29 (b):”)\n\n\n\n그림 5.29: 동일한 분포 \\(F\\)로부터 얻은 서로 다른 표본들은 서로 다른 군집화 결과로 이어집니다. (a)에서 우리는 실제 표집 가변성을 봅니다. 붓스트랩은 (b)에서 보듯이 경험적 분포 함수 \\(_n\\)을 사용하여 하위 표본(subsamples)을 추출함으로써 이러한 표집 가변성을 시뮬레이션합니다.\n우리는 4장에서 붓스트랩 원리를 보았습니다: 이상적으로는 기저의 데이터 생성 프로세스로부터 많은 새로운 표본(데이터 세트)을 얻어 각각에 군집화 방법을 적용한 다음, 군집화를 비교하기 위해 위에서 사용했던 것과 같은 지수를 사용하여 클러스터가 얼마나 안정적인지 또는 얼마나 변하는지 보고 싶을 것입니다. 물론 우리에게는 이러한 추가 표본이 없습니다. 따라서 우리는 단순히 데이터의 서로 다른 무작위 하위 표본을 취하여 매번 얻는 서로 다른 군집화 결과를 비교함으로써 새로운 데이터 세트를 만들 것입니다. Tibshirani, Walther, Hastie (2001)는 갭 통계량을 사용하여 클러스터 수를 추론할 때 붓스트랩 재표본 추출을 사용할 것을 권장합니다.\n우리는 계속해서 Hiiragi2013 데이터를 사용하겠습니다. 여기서는 생쥐 배아의 배아기 3.5일(E3.5) 배반포의 내세포집단(ICM)이 다분화능 에피블라스트(EPI)와 원시 내배엽(PE)에 해당하는 두 개의 클러스터로 “자연스럽게” 나뉘는 반면, 배아기 3.25일(E3.25)의 데이터는 아직 이러한 대칭성 붕괴(symmetry breaking)를 보여주지 않는다는 가설에 대한 조사를 따라갑니다.\n우리는 군집화 과정에서 실제 그룹 레이블을 사용하지 않고, 결과의 최종 해석에서만 이를 사용할 것입니다. 우리는 (E3.5)와 (E3.25)라는 두 가지 서로 다른 데이터 세트에 각각 붓스트랩을 적용할 것입니다. 붓스트랩의 각 단계는 데이터의 무작위 하위 집합에 대한 군집화 결과를 생성할 것이며, 우리는 이를 클러스터 앙상블의 합의(consensus)를 통해 비교해야 할 것입니다. clue 패키지(Hornik 2005)에 이를 위한 유용한 프레임워크가 있습니다. Ohnishi 등 (2014)의 부록에서 가져온 clusterResampling 함수가 이 접근 방식을 구현합니다:\nclusterResampling = function(x, ngenes = 50, k = 2, B = 250,\n                             prob = 0.67) {\n  mat = Biobase::exprs(x)\n  ce = cl_ensemble(list = lapply(seq_len(B), function(b) {\n    selSamps = sample(ncol(mat), size = round(prob * ncol(mat)),\n                      replace = FALSE)\n    submat = mat[, selSamps, drop = FALSE]\n    sel = order(rowVars(submat), decreasing = TRUE)[seq_len(ngenes)]\n    submat = submat[sel,, drop = FALSE]\n    pamres = pam(t(submat), k = k)\n    pred = cl_predict(pamres, t(mat[sel, ]), \"memberships\")\n    as.cl_partition(pred)\n  }))\n  cons = cl_consensus(ce)\n  ag = sapply(ce, cl_agreement, y = cons)\n  list(agreements = ag, consensus = cons)\n}__\nclusterResampling 함수는 다음 단계들을 수행합니다:\n\n복원 추출 없이 샘플의 67%를 선택하여 데이터(모든 E3.25 샘플 또는 모든 E3.5 샘플)의 무작위 하위 집합을 추출합니다.\n(하위 집합에서) 전체 분산 기준 상위 ngenes개의 특성을 선택합니다.\n\\(k\\)-평균 군집화를 적용하고, clue 패키지의 cl_predict 메서드를 사용하여 하위 집합에 포함되지 않았던 샘플들이 어느 클러스터 중심과 가장 가까운지에 따라 그들의 클러스터 멤버십을 예측합니다.\n1-3단계를 B번 반복합니다.\n합의 군집화(consensus clustering, cl_consensus)를 적용합니다.\nB개의 군집화 결과 각각에 대해 cl_agreement 함수를 통해 합의 결과와의 일치도를 측정합니다. 여기서 일치도가 높으면 1에 가까운 값을, 낮으면 더 작은 값을 갖습니다. 일치도가 전반적으로 높다면 \\(k\\)개 클래스로의 군집화는 안정적이고 재현 가능한 것으로 간주될 수 있습니다. 반대로 낮다면 샘플들을 \\(k\\)개 클러스터로 나누는 안정적인 분할이 뚜렷하지 않음을 의미합니다.\n\n합의 군집화를 위한 클러스터 간 거리 척도로 멤버십의 유클리드 비유사성이 사용됩니다. 즉, \\(\\)와 \\(\\)의 모든 열 순열 사이의 최소 제곱 차이합의 제곱근입니다(여기서 \\(\\)와 \\(\\)는 클러스터 멤버십 행렬임). 일치도 측정값으로는 \\(1 - d/m\\) 수치가 사용되는데, 여기서 \\(d\\)는 유클리드 비유사성이고 \\(m\\)은 최대 유클리드 비유사성의 상한선입니다.\niswt = (x$genotype == \"WT\")\ncr1 = clusterResampling(x[, x$Embryonic.day == \"E3.25\" & iswt])\ncr2 = clusterResampling(x[, x$Embryonic.day == \"E3.5\"  & iswt])__\n결과는 그림 5.30에 나와 있습니다. 이들은 E3.5 데이터가 두 개의 클러스터로 나뉜다는 가설을 확인해 줍니다.\nag1 = tibble(agreements = cr1$agreements, day = \"E3.25\")\nag2 = tibble(agreements = cr2$agreements, day = \"E3.5\")\np1 &lt;- ggplot(bind_rows(ag1, ag2), aes(x = day, y = agreements)) +\n  geom_boxplot() +\n  ggbeeswarm::geom_beeswarm(cex = 1.5, col = \"#0000ff40\")\nmem1 = tibble(y = sort(cl_membership(cr1$consensus)[, 1]),\n              x = seq(along = y), day = \"E3.25\")\nmem2 = tibble(y = sort(cl_membership(cr2$consensus)[, 1]),\n              x = seq(along = y), day = \"E3.5\")\np2 &lt;- ggplot(bind_rows(mem1, mem2), aes(x = x, y = y, col = day)) +\n  geom_point() + facet_grid(~ day, scales = \"free_x\")\ngridExtra::grid.arrange(p1, p2, widths = c(2.4,4.0))__\n\n그림 5.30: E3.25 및 E3.5 샘플을 이용한 클러스터 안정성 분석. 왼쪽: B개의 군집화 결과와 합의 결과 사이의 일치도에 대한 beeswarm 플롯; \\(1\\)은 완벽한 일치를 나타내며, 낮은 값은 낮은 일치도를 나타냅니다. 오른쪽: 합의 군집화의 멤버십 확률. E3.25의 경우 확률이 분산되어 있어 개별 군집화 결과들이 자주 일치하지 않음을 나타내는 반면, E3.5의 경우 분포가 이봉형(bimodal)이며 모호한 샘플은 하나뿐입니다.\n\n7.6.2.1 계산 및 메모리 문제\n\n\n\n계산 복잡도. 어떤 알고리즘이 O(n^k)라고 불리는 것은, n이 커짐에 따라 자원 소비(CPU 시간 또는 메모리)가 n^k에 비례하여 증가함을 의미합니다. 다른(때로는 상당한) 기본 비용이나 n의 더 낮은 거듭제곱에 비례하여 증가하는 비용이 있을 수 있지만, n이 무한대로 감에 따라 이들은 항상 지배적인 항에 비해 무시할 수 있는 수준이 됩니다.\n\n\n계산 복잡도(Computational complexity). 어떤 알고리즘이 \\(O(n^k)\\)라고 불리는 것은, \\(n\\)이 커짐에 따라 자원 소비(CPU 시간 또는 메모리)가 \\(n^k\\)에 비례하여 증가함을 의미합니다. 다른(때로는 상당한) 기본 비용이나 \\(n\\)의 더 낮은 거듭제곱에 비례하여 증가하는 비용이 있을 수 있지만, \\(n\\)임에 따라 이들은 항상 지배적인 항에 비해 무시할 수 있는 수준이 됩니다.\n\\(n\\)개 객체의 전체 대 전체 거리 계산이 (시간과 메모리 측면에서) \\(O(n^2)\\) 작업임을 기억하는 것이 중요합니다. 전통적인 계층적 군집화 접근 방식(stats 패키지의 hclust 등)은 시간 측면에서 심지어 \\(O(n^3)\\)입니다. \\(n\\)이 크다면 이는 비실용적일 수 있습니다8. 우리는 전체 대 전체 거리 행렬의 완전한 계산을 피할 수 있습니다. 예를 들어, \\(k\\)-평균은 각 객체와 클러스터 중심 사이의 거리만 추적하면 되므로 \\(O(n)\\)의 계산만 필요하다는 장점이 있으며, 클러스터 중심의 수는 \\(n\\)이 증가하더라도 일정하게 유지됩니다.\n8 예를 들어 백만 개 객체에 대한 거리 행렬을 8바이트 부동 소수점으로 저장하면 약 4테라바이트를 차지하며, hclust와 같은 알고리즘은 각 반복이 1나노초만 걸린다는 낙관적인 가정 하에서도 30년 동안 실행될 것입니다.\nfastclust (Müllner 2013) 및 dbscan과 같은 빠른 구현체들은 많은 수의 관측치를 처리하기 위해 신중하게 최적화되었습니다.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "05-chap.html#노이즈-제거-수단으로서의-군집화",
    "href": "05-chap.html#노이즈-제거-수단으로서의-군집화",
    "title": "7  5.1 이 장의 목표",
    "section": "7.7 5.8 노이즈 제거 수단으로서의 군집화",
    "text": "7.7 5.8 노이즈 제거 수단으로서의 군집화\n어떤 기저의 실제 값(예를 들어 게놈의 DNA 서열로 표현되는 종)을 반영하지만 기술적 노이즈에 의해 변질된 측정값 세트를 생각해 봅시다. 군집화는 이러한 노이즈를 제거하는 데 사용될 수 있습니다.\n\n7.7.1 5.8.1 서로 다른 베이스라인 빈도를 가진 노이즈 섞인 관측치\n동일한 오차 분산으로 만들어진 관측치들의 이변량 분포(bivariate distribution)가 있다고 가정해 봅시다. 그러나 샘플링은 베이스라인 빈도가 매우 다른 두 그룹으로부터 이루어집니다. 더 나아가, 오차는 연속적이고 독립적인 이변량 정규 분포를 따른다고 가정합시다. 다음 코드에서 생성된 것처럼 우리는 seq1을 \\(10^{3}\\)개, seq2를 \\(10^{5}\\)개 가지고 있습니다:\nlibrary(\"mixtools\")\nseq1 = rmvnorm(n = 1e3, mu = -c(1, 1), sigma = 0.5 * diag(c(1, 1)))\nseq2 = rmvnorm(n = 1e5, mu =  c(1, 1), sigma = 0.5 * diag(c(1, 1)))\ntwogr = data.frame(\n  rbind(seq1, seq2),\n  seq = factor(c(rep(1, nrow(seq1)),\n                 rep(2, nrow(seq2))))\n)\ncolnames(twogr)[1:2] = c(\"x\", \"y\")\nlibrary(\"ggplot2\")\nggplot(twogr, aes(x = x, y = y, colour = seq,fill = seq)) +\n  geom_hex(alpha = 0.5, bins = 50) + coord_fixed()__\n\n그림 5.31: 두 그룹 모두 동일한 분산의 노이즈 분포를 가지고 있음에도 불구하고, 그룹의 겉보기 반지름은 매우 다릅니다. seq2의 \\(10^{5}\\)개 인스턴스는 \\(10^{3}\\)개뿐인 seq1보다 오차가 발생할 기회가 훨씬 더 많습니다. 따라서 우리는 데이터를 군집화할 때 빈도가 중요하다는 것을 알 수 있습니다.\n관측된 값들은 그림 5.31과 같이 보일 것입니다.\n__\n질문 5.16\n데이터 seq1과 seq2를 가져와서 그룹 중심으로부터의 거리에 따라 두 그룹으로 군집화해 보세요. 결과가 두 서열 유형 각각의 빈도에 의존해야 한다고 생각하시나요?\n__\n해결책\n__\n분류학적 군집화(taxonomic clustering), 즉 OTU(operational taxonomic unit) 군집화(Caporaso et al. 2010; P. D. Schloss et al. 2009) 방법들에서 종종 사용되는 이러한 접근 방식은 최적이 아닙니다.\n오직 유사성에만 기반한 방법들은 대표성(representativeness) 휴리스틱에 내재된 편향으로 인해 고통받습니다. 군집화 및 분류학적 할당에서 대표성과 거리 기반 휴리스틱만을 사용하려는 우리의 자연스러운 경향이 어떻게 편향된 결과로 이어질 수 있는지 설명하는 데 도움이 되는 인지 심리학의 세계로 잠시 외도해 봅시다.\n1970년대에 Tversky와 Kahneman (1975)은 우리가 일반적으로 가장 유사한 _대표자(representatives)_를 살펴봄으로써 그룹을 할당한다고 지적했습니다. 군집화와 그룹 할당에서 이는 새로운 서열을 그 중심까지의 거리에 따라 그룹에 할당하는 것을 의미할 것입니다. 사실 이는 서로 다른 그룹의 보급률(prevalence) 차이를 무시하고 동일한 반지름을 가진 공을 취하는 것과 같습니다. 이러한 심리학적 오류는 많은 다양한 휴리스틱과 편향을 다루는 중요한 Science 논문(Tversky and Kahneman 1974)에서 처음 논의되었습니다.\n\n\n\n우리의 자연스러운 휴리스틱과 우리가 확률 계산을 할 때 그것들이 우리를 오도할 수 있는 방식에 대한 단행본 규모의 논의는 Kahneman (2011)을 참조하십시오(특히 14장과 15장을 추천합니다).\n\n\n우리의 자연스러운 휴리스틱과 우리가 확률 계산을 할 때 그것들이 우리를 오도할 수 있는 방식에 대한 단행본 규모의 논의는 Kahneman (2011)을 참조하십시오. (특히 14장과 15장을 추천합니다.) when we make probability calculations (we recommend especially Chapters 14 and 15).\n__\nTask\nSimulate n=2000 binary variables of length len=200 that indicate the quality of n sequencing reads of length len. For simplicity, let us assume that sequencing errors occur independently and uniformly with probability perr=0.001. That is, we only care whether a base was called correctly (TRUE) or not (FALSE).\nn    = 2000\nlen  = 200\nperr = 0.001\nseqs = matrix(runif(n * len) &gt;= perr, nrow = n, ncol = len)__\nNow, compute all pairwise distances between reads.\ndists = as.matrix(dist(seqs, method = \"manhattan\"))__\nFor various values of number of reads k (from 2 to n), the maximum distance within this set of reads is computed by the code below and shown in Figure 5.32.\nlibrary(\"tibble\")\ndfseqs = tibble(\n  k = 10 ^ seq(log10(2), log10(n), length.out = 20),\n  diameter = vapply(k, function(i) {\n    s = sample(n, i)\n    max(dists[s, s])\n    }, numeric(1)))\nggplot(dfseqs, aes(x = k, y = diameter)) + geom_point()+geom_smooth()__\n\nFigure 5.32: The diameter of a set of sequences as a function of the number of sequences.\nWe will now improve the 16SrRNA-read clustering using a denoising mechanism that incorporates error probabilities.\n\n\n7.7.2 5.8.2 Denoising 16S rRNA sequences\nWhat are the data? In the bacterial 16SrRNA gene there are so-called variable regions that are taxa-specific. These provide fingerprints that enables taxon 9 identification. The raw data are FASTQ-files with quality scored sequences of PCR-amplified DNA regions10. We use an iterative alternating approach11 to build a probabilistic noise model from the data. We call this a de novo method, because we use clustering, and we use the cluster centers as our denoised sequence variants (a.k.a. Amplicon Sequence Variants, ASVs, see (Benjamin J. Callahan, McMurdie, and Holmes 2017)). After finding all the denoised variants, we create contingency tables of their counts across the different samples. We will show in Chapter 10 how these tables can be used to infer properties of the underlying bacterial communities using networks and graphs.\n9 Calling different groups of bacteria taxa rather than species highlights the approximate nature of the concept, as the notion of species is more fluid in bacteria than, say, in animals.\n10 The FASTQ format is described here.\n11 Similar to the EM algorithm we saw in Chapter 4.\nIn order to improve data quality, one often has to start with the raw data and model all the sources of variation carefully. One can think of this as an example of cooking from scratch (see the gruesome details in Ben J. Callahan et al. (2016) and Exercise 5.5).\n__\nQuestion 5.17\nSuppose that we have two sequences of length 200 (seq1 and seq2) present in our sample at very different abundances. We are told that the technological sequencing errors occur as independent Bernoulli(0.0005) random events for each nucleotide.\nWhat is the distribution of the number of errors per sequence?\n__\nSolution\n__\nProbability theory tells us that the sum of 200 independent Poisson(0.0005) will be Poisson(0.1).\nWe can also verify this by Monte Carlo simulation:\nsimseq10K = replicate(1e5, sum(rpois(200, 0.0005)))\nmean(simseq10K)__\n\n\n[1] 0.10143\n\n\nvcd::distplot(simseq10K, \"poisson\")__\n\nFigure 5.33: distplot for the simseq10K data.\nFigure 5.33 shows us how close the distribution is to being Poisson distributed.\n\n\n7.7.3 5.8.3 Infer sequence variants\nThe DADA method (Divisive Amplicon Denoising Algorithm, Rosen et al. (2012)) uses a parameterized model of substitution errors that distinguishes sequencing errors from real biological variation. The model computes the probabilities of base substitutions, such as seeing an \\({}\\) instead of a \\({}\\). It assumes that these probabilities are independent of the position along the sequence. Because error rates vary substantially between sequencing runs and PCR protocols, the model parameters are estimated from the data themselves using an EM-type approach. A read is classified as noisy or exact given the current parameters, and the noise model parameters are updated accordingly12.\n12 In the case of a large data set, the noise model estimation step does not have to be done on the complete set. See https://benjjneb.github.io/dada2/bigdata.html for tricks and tools when dealing with large data sets.\n13 F stands for forward strand and R for reverse strand.\nThe dereplicated sequences13 are read in and then divisive denoising and estimation is run with the dada function as in the following code:\nderepFs = readRDS(file=\"../data/derepFs.rds\")\nderepRs = readRDS(file=\"../data/derepRs.rds\")\nlibrary(\"dada2\")\nddF = dada(derepFs, err = NULL, selfConsist = TRUE)\nddR = dada(derepRs, err = NULL, selfConsist = TRUE)__\nIn order to verify that the error transition rates have been reasonably well estimated, we inspect the fit between the observed error rates (black points) and the fitted error rates (black lines) (Figure 5.34).\nplotErrors(ddF)__\n\n\nIn chunk 'fig-rerrorprofile1': Warning: Removed 82 rows containing missing values or values outside the scale range(`geom_line()`).\n\nFigure 5.34: Forward transition error rates as provided by plotErrors(ddF). This shows the frequencies of each type of nucleotide transition as a function of quality.\nOnce the errors have been estimated, the algorithm is rerun on the data to find the sequence variants:\ndadaFs = dada(derepFs, err=ddF[[1]]$err_out, pool = TRUE)\ndadaRs = dada(derepRs, err=ddR[[1]]$err_out, pool = TRUE)__\nNote: The sequence inference function can run in two different modes: Independent inference by sample (pool = FALSE), and pooled inference from the sequencing reads combined from all samples. Independent inference has two advantages: as a functions of the number of samples, computation time is linear and memory requirements are constant. Pooled inference is more computationally taxing, however it can improve the detection of rare variants that occur just once or twice in an individual sample but more often across all samples. As this dataset is not particularly large, we performed pooled inference.\nSequence inference removes nearly all substitution and indel 14 errors from the data. We merge the inferred forward and reverse sequences, while removing paired sequences that do not perfectly overlap as a final control against residual errors.\n14 The term indel stands for insertion-deletion; when comparing two sequences that differ by a small stretch of characters, it is a matter of viewpoint whether this is an insertion or a deletion, thus the name.\nmergers = mergePairs(dadaFs, derepFs, dadaRs, derepRs)__\nWe produce a contingency table of counts of ASVs. This is a higher-resolution analogue of the “OTU15 table”, i.e., a samples by features table whose cells contain the number of times each sequence variant was observed in each sample.\n15 operational taxonomic units\nseqtab.all = makeSequenceTable(mergers[!grepl(\"Mock\",names(mergers))])__\n__\nQuestion 5.18\nExplore the components of the objects dadaRs and mergers.\n__\nSolution\n__\ndadaRs is a list of length 20. Its elements are objects class dada that contain the denoised reads. We will see in Chapter 10 how to align the sequences, assign their taxonomies and combine them with the sample information for downstream analyses.\n[1] 20\n\n\n[1] 20\n\n\n[1] \"list\"\n\n\n [1] \"F3D0\"   \"F3D1\"   \"F3D141\" \"F3D142\" \"F3D143\" \"F3D144\" \"F3D145\" \"F3D146\"\n [9] \"F3D147\" \"F3D148\" \"F3D149\" \"F3D150\" \"F3D2\"   \"F3D3\"   \"F3D5\"   \"F3D6\"  \n[17] \"F3D7\"   \"F3D8\"   \"F3D9\"   \"Mock\"  \n\n\n[1] \"list\"\n\n\n[1] 20\nChimera are sequences that are artificially created during PCR amplification by the melding of two (in rare cases, more) of the original sequences. To complete our denoising workflow, we remove them with a call to the function removeBimeraDenovo, leaving us with a clean contingency table we will use later on.\nseqtab = removeBimeraDenovo(seqtab.all)__\n__\nQuestion 5.19\nWhy do you think the chimera are quite easy to recognize?\nWhat proportion of the reads were chimeric in the seqtab.all data?\nWhat proportion of unique sequence variants are chimeric?\n__\nSolution\n__\nHere we observed some sequence variants as chimeric, but these only represent 7% of all reads.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "05-chap.html#summary-of-this-chapter",
    "href": "05-chap.html#summary-of-this-chapter",
    "title": "7  5.1 이 장의 목표",
    "section": "7.8 5.9 Summary of this chapter",
    "text": "7.8 5.9 Summary of this chapter\nOf a feather: how to compare observations We saw at the start of the chapter how finding the right distance is an essential first step in a clustering analysis; this is a case where the garbage in, garbage out motto is in full force. Always choose a distance that is scientifically meaningful and compare output from as many distances as possible; sometimes the same data require different distances when different scientific objectives are pursued.\nTwo ways of clustering We saw there are two approaches to clustering:\n\niterative partitioning approaches such as \\(k\\)-means and \\(k\\)-medoids (PAM) that alternated between estimating the cluster centers and assigning points to them;\nhierarchical clustering approaches that first agglomerate points, and subsequently the growing clusters, into nested sequences of sets that can be represented by hierarchical clustering trees.\n\nBiological examples Clustering is important tool for finding latent classes in single cell measurements, especially in immunology and single cell data analyses. We saw how density-based clustering is useful for lower dimensional data where sparsity is not an issue.\nValidating Clustering algorithms always deliver clusters, so we need to assess their quality and the number of clusters to choose carefully. Such validation steps are performed using visualization tools and repeating the clustering on many resamples of the data. We saw how statistics such as WSS/BSS or \\(()\\) can be calibrated using simulations on data where we understand the group structure and can provide useful benchmarks for choosing the number of clusters on new data. Of course, the use of biologically relevant information to inform and confirm the meaning of clusters is always the best validation approach.\nThere is arguably no ground truth to compare a clustering result against, in general. The old adage of “all models are wrong, some are useful” also applies here. A good clustering is one that turns out to be useful.\nDistances and probabilities Finally: distances are not everything. We showed how important it was to take into account baseline frequencies and local densities when clustering. This is essential in a cases such as clustering to denoise 16S rRNA sequence reads where the true class or taxa group occur at very different frequencies.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "05-chap.html#further-reading",
    "href": "05-chap.html#further-reading",
    "title": "7  5.1 이 장의 목표",
    "section": "7.9 5.10 Further reading",
    "text": "7.9 5.10 Further reading\nFor a complete book on Finding groups in data , see Kaufman and Rousseeuw (2009). The vignette of the clusterExperiment package contains a complete workflow for generating clusters using many different techniques, including preliminary dimension reduction (PCA) that we will cover in Chapter 7. There is no consensus on methods for deciding how many clusters are needed to describe data in the absence of contiguous biological information. However, making hierarchical clusters of the strong forms is a method that has the advantage of allowing the user to decide how far down to cut the hierarchical tree and be careful not to cut in places where these inner branches are short. See the vignette of clusterExperiment for an application to single cell RNA experimental data.\nIn analyzing the Hiiragi data, we used cluster probabilities, a concept already mentioned in Chapter 4, where the EM algorithm used them as weights to compute expected value statistics. The notion of probabilistic clustering is well-developed in the Bayesian nonparametric mixture framework, which enriches the mixture models we covered in Chapter 4 to more general settings. See Dundar et al. (2014) for a real example using this framework for flow cytometry. In the denoising and assignment of high-throughput sequencing reads to specific strains of bacteria or viruses, clustering is essential. In the presence of noise, clustering into groups of true strains of very unequal sizes can be challenging. Using the data to create a noise model enables both denoising and cluster assignment concurrently. Denoising algorithms such as those by Rosen et al. (2012) or Benjamin J. Callahan et al. (2016) use an iterative workflow inspired by the EM method (McLachlan and Krishnan 2007).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "05-chap.html#exercises",
    "href": "05-chap.html#exercises",
    "title": "7  5.1 이 장의 목표",
    "section": "7.10 5.11 Exercises",
    "text": "7.10 5.11 Exercises\n__\nExercise 5.1\nWe can define the average dissimilarity of a point \\(x_i\\) to a cluster \\(C_k\\) as the average of the distances from \\(x_i\\) to all points in \\(C_k\\). Let \\(A(i)\\) be the average dissimilarity of all points in the cluster that \\(x_i\\) belongs to. Let \\(B(i)\\) be the lowest average dissimilarity of \\(x_i\\) to any other cluster of which \\(x_i\\) is not a member. The cluster with this lowest average dissimilarity is said to be the neighboring cluster of \\(x_i\\), because it is the next best fit cluster for point \\(x_i\\). The silhouette index is\n\\[ S(i)=. \\]\nCompute the silhouette index for the simdat data we simulated in Section 5.7.\nlibrary(\"cluster\")\npam4 = pam(simdatxy, 4)\nsil = silhouette(pam4, 4)\nplot(sil, col=c(\"red\",\"green\",\"blue\",\"purple\"), main=\"Silhouette\")__\nChange the number of clusters \\(k\\) and assess which \\(k\\) gives the best silhouette index.\nNow, repeat this for groups that have uniform (unclustered) data distributions over a whole range of values.\n__\nExercise 5.2\nMake a “character” representation of the distance between the 20 locations in the dune data from the vegan package using the function symnum.\nMake a heatmap plot of these distances.\n__\nExercise 5.3\nLoad the spirals data from the kernlab package. Plot the results of using \\(k\\)-means on the data. This should give you something similar to Figure 5.35.\n\n\n\n\n\n\n\n\nFigure 5.35: An example of non-convex clusters. In (a), we show the result of \\(k\\)-means clustering with \\(k=2\\). In (b), we have the output from dbscan. The colors represent the three clusters found by the algorithm for the settings .\nYou’ll notice that the clustering in Figure 5.35 seems unsatisfactory. Show how a different method, such as specc or dbscan, could cluster spirals data in a more useful manner.\nRepeat the dbscan clustering with different parameters. How robust is the number of groups?\n__\nExercise 5.4\nLooking at graphical representations in simple two-dimensional maps can often reveal important clumping patterns. We saw an example for this with the map that enabled Snow to discover the source of the London cholera outbreak. Such clusterings can often indicate important information about hidden variables acting on the observations. Look at a map for breast cancer incidence in the US at:\nhttp://www.huffingtonpost.com/bill-davenhall/post_1663_b_817254.html (Mandal et al. 2009); the areas of high incidence seem spatially clustered. Can you guess the reason(s) for this clustering and high incidence rates on the West and East coasts and around Chicago?\n__\nExercise 5.5\nAmplicon bioinformatics: from raw reads to dereplicated sequences. As a supplementary exercise, we provide the intermediate steps necessary to a full data preprocessing workflow for denoising 16S rRNA sequences. We start by setting the directories and loading the downloaded data:\nbase_dir = \"../data\"\nmiseq_path = file.path(base_dir, \"MiSeq_SOP\")\nfilt_path = file.path(miseq_path, \"filtered\")\nfnFs = sort(list.files(miseq_path, pattern=\"_R1_001.fastq\"))\nfnRs = sort(list.files(miseq_path, pattern=\"_R2_001.fastq\"))\nsampleNames = sapply(strsplit(fnFs, \"_\"), `[`, 1)\nif (!file_test(\"-d\", filt_path)) dir.create(filt_path)\nfiltFs = file.path(filt_path, paste0(sampleNames, \"_F_filt.fastq.gz\"))\nfiltRs = file.path(filt_path, paste0(sampleNames, \"_R_filt.fastq.gz\"))\nfnFs = file.path(miseq_path, fnFs)\nfnRs = file.path(miseq_path, fnRs)\nprint(length(fnFs))__\n\n\n[1] 20\nThe data are highly-overlapping Illumina Miseq \\(2\\) amplicon sequences from the V4 region of the 16S rRNA gene (Kozich et al. 2013). There were originally 360 fecal samples collected longitudinally from 12 mice over the first year of life. These were collected by P. D. Schloss et al. (2012) to investigate the development and stabilization of the murine microbiome. We have selected 20 samples to illustrate how to preprocess the data.\nWe will need to filter out low-quality reads and trim them to a consistent length. While generally recommended filtering and trimming parameters serve as a starting point, no two datasets are identical and therefore it is always worth inspecting the quality of the data before proceeding. We show the sequence quality plots for the two first samples in Figure 5.36. They are generated by:\nplotQualityProfile(fnFs[1:2]) + ggtitle(\"Forward\")\nplotQualityProfile(fnRs[1:2]) + ggtitle(\"Reverse\")__\n\n\n\n\n\n\n\n\nFigure 5.36: Quality scores. The lines show positional summary statistics: green is the mean, orange is the median, and the dashed orange lines are the 25th and 75th quantiles.\nNote that we also see the background distribution of quality scores at each position in Figure 5.36 as a grey-scale heat map. The dark colors correspond to higher frequency.\n__\nExercise 5.6\nGenerate similar plots for four randomly selected sets of forward and reverse reads. Compare forward and reverse read qualities; what do you notice?\n__\nSolution\n__\nii = sample(length(fnFs), 4)\nplotQualityProfile(fnFs[ii]) + ggtitle(\"Forward\")__\n\n\n plotQualityProfile(fnRs[ii]) + ggtitle(\"Reverse\")__\n__\nExercise 5.7\nHere, the forward reads maintain high quality throughout, while the quality of the reverse reads drops significantly at about position 160. Therefore, we truncate the forward reads at position 240, and trimm the first 10 nucleotides as these positions are of lower quality. The reverse reads are trimmed at position 160. Combine these trimming parameters with standard filtering parameters remember to enforce a maximum of 2 expected errors per-read. (Hint: Trim and filter on paired reads jointly, i.e., both reads must pass the filter for the pair to pass. The input arguments should be chosen following the dada2 vignette carefully. We recommend filtering out all reads with any ambiguous nucleotides.)\n__\nSolution\n__\nMost Illumina sequencing data show a trend of decreasing quality towards the end of the reads.\nout = filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),\n        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,  trimLeft=10,\n        compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE\nhead(out)__\n\n\n                              reads.in reads.out\nF3D0_S188_L001_R1_001.fastq       7793      7139\nF3D1_S189_L001_R1_001.fastq       5869      5314\nF3D141_S207_L001_R1_001.fastq     5958      5478\nF3D142_S208_L001_R1_001.fastq     3183      2926\nF3D143_S209_L001_R1_001.fastq     3178      2955\nF3D144_S210_L001_R1_001.fastq     4827      4323\nThe maxN parameter omits all reads with more than maxN = 0 ambiguous nucleotides and maxEE at 2 excludes reads with more than 2 expected errors.\nThe sequence data was imported into R from demultiplexed fastq files (i.e. one fastq for each sample) and simultaneously dereplicated to remove redundancy. Name the resulting objects by their sample provenance; they will have derep as their class.\nderepFs = derepFastq(filtFs, verbose = FALSE)\nderepRs = derepFastq(filtRs, verbose = FALSE)\nnames(derepFs) = sampleNames\nnames(derepRs) = sampleNames __\n__\nExercise 5.8\nUse R to create a map like the one shown in Figure 5.2. Hint: go to the website of the British National Archives and download street addresses of hits, use an address resolution service to convert these into geographic coordinates, and display these as points on a map of London.\n__\nSolution\n__\nSee the Gist https://gist.github.com/wolfganghuber/523622aff6a156d26e77a87ccbe7bd0d by Andrzej Oles.\nAure, Miriam Ragle, Valeria Vitelli, Sandra Jernström, Surendra Kumar, Marit Krohn, Eldri U Due, Tonje Husby Haukaas, et al. 2017. “Integrative Clustering Reveals a Novel Split in the Luminal A Subtype of Breast Cancer with Impact on Outcome.” Breast Cancer Research 19 (1): 44.\nBendall, Sean C, Garry P Nolan, Mario Roederer, and Pratip K Chattopadhyay. 2012. “A Deep Profiler’s Guide to Cytometry.” Trends in Immunology 33 (7): 323–32.\nCallahan, Ben J, Kris Sankaran, Julia A Fukuyama, Paul J McMurdie, and Susan P Holmes. 2016. “Bioconductor Workflow for Microbiome Data Analysis: From Raw Reads to Community Analyses.” F1000Research 5.\nCallahan, Benjamin J, Paul J McMurdie, and Susan P Holmes. 2017. “Exact Sequence Variants Should Replace Operational Taxonomic Units in Marker Gene Data Analysis.” ISME Journal , 1–5.\nCallahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy J Johnson, and Susan P Holmes. 2016. “DADA2: High Resolution Sample Inference from Amplicon Data.” Nature Methods , 1–4.\nCaporaso, J. G., J. Kuczynski, J. Stombaugh, K. Bittinger, F. D. Bushman, E. K. Costello, N. Fierer, et al. 2010. “QIIME Allows Analysis of High-Throughput Community Sequencing Data.” Nature Methods 7 (5): 335–36.\nChakerian, John, and Susan Holmes. 2012. “Computational Tools for Evaluating Phylogenetic and Hierarchical Clustering Trees.” Journal of Computational and Graphical Statistics 21 (3): 581–99.\nDiday, Edwin, and M Paula Brito. 1989. “Symbolic Cluster Analysis.” In Conceptual and Numerical Analysis of Data , 45–84. Springer.\nDundar, Murat, Ferit Akova, Halid Z. Yerebakan, and Bartek Rajwa. 2014. “A Non-Parametric Bayesian Model for Joint Cell Clustering and Cluster Matching: Identification of Anomalous Sample Phenotypes with Random Effects.” BMC Bioinformatics 15 (1): 1–15. https://doi.org/10.1186/1471-2105-15-314.\nFreedman, David A. 1991. “Statistical Models and Shoe Leather.” Sociological Methodology 21 (2): 291–313.\nHallett, Robin M, Anna Dvorkin-Gheva, Anita Bane, and John A Hassell. 2012. “A Gene Signature for Predicting Outcome in Patients with Basal-Like Breast Cancer.” Scientific Reports 2.\nHolmes, Susan, Michael He, Tong Xu, and Peter P Lee. 2005. “Memory t Cells Have Gene Expression Patterns Intermediate Between Naive and Effector.” PNAS 102 (15): 5519–23.\nHornik, Kurt. 2005. “A CLUE for CLUster Ensembles.” Journal of Statistical Software 14 (12).\nHulett, Henry R, William A Bonner, Janet Barrett, and Leonard A Herzenberg. 1969. “Cell Sorting: Automated Separation of Mammalian Cells as a Function of Intracellular Fluorescence.” Science 166 (3906): 747–49.\nKahneman, Daniel. 2011. Thinking, Fast and Slow. Macmillan.\nKaufman, Leonard, and Peter J Rousseeuw. 2009. Finding Groups in Data: An Introduction to Cluster Analysis. Vol. 344. John Wiley & Sons.\nKozich, James J, Sarah L Westcott, Nielson T Baxter, Sarah K Highlander, and Patrick D Schloss. 2013. “Development of a Dual-Index Sequencing Strategy and Curation Pipeline for Analyzing Amplicon Sequence Data on the MiSeq Illumina Sequencing Platform.” Applied and Environmental Microbiology 79 (17): 5112–20.\nMandal, Rakesh, Sophie St-Hilaire, John G Kie, and DeWayne Derryberry. 2009. “Spatial Trends of Breast and Prostate Cancers in the United States Between 2000 and 2005.” International Journal of Health Geographics 8 (1): 53.\nMcLachlan, Geoffrey, and Thriyambakam Krishnan. 2007. The EM Algorithm and Extensions. Vol. 382. John Wiley & Sons.\nMüllner, Daniel. 2013. “Fastcluster: Fast Hierarchical, Agglomerative Clustering Routines for r and Python.” Journal of Statistical Software 53 (9): 1–18.\nO’Neill, Kieran, Nima Aghaeepour, Josef Špidlen, and Ryan Brinkman. 2013. “Flow Cytometry Bioinformatics.” PLoS Computational Biology 9 (12): e1003365.\nOhnishi, Y., W. Huber, A. Tsumura, M. Kang, P. Xenopoulos, K. Kurimoto, A. K. Oles, et al. 2014. “Cell-to-Cell Expression Variability Followed by Signal Reinforcement Progressively Segregates Early Mouse Lineages.” Nature Cell Biology 16 (1): 27–37.\nRosen, Michael J, Benjamin J Callahan, Daniel S Fisher, and Susan P Holmes. 2012. “Denoising PCR-Amplified Metagenome Data.” BMC Bioinformatics 13 (1): 283.\nSchloss, P D, S L Westcott, T Ryabin, J R Hall, M Hartmann, E B Hollister, R A Lesniewski, et al. 2009. “Introducing mothur: Open-Source, Platform- Independent, Community-Supported Software for Describing and Comparing Microbial Communities.” Applied and Environmental Microbiology 75 (23): 7537–41.\nSchloss, P. D., A. M. Schuber, J. P. Zackular, K. D. Iverson, Young V. B., and Petrosino J. F. 2012. “Stabilization of the Murine Gut Microbiome Following Weaning.” Gut Microbes 3 (4): 383–93.\nTibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. “Estimating the Number of Clusters in a Data Set via the Gap Statistic.” JRSSB 63 (2): 411–23.\nTseng, George C, and Wing H Wong. 2005. “Tight Clustering: A Resampling-Based Approach for Identifying Stable and Tight Patterns in Data.” Biometrics 61 (1): 10–16.\nTversky, Amos, and Daniel Kahneman. 1974. “Heuristics and Biases: Judgement Under Uncertainty.” Science 185: 1124–30.\n———. 1975. “Judgment Under Uncertainty: Heuristics and Biases.” In Utility, Probability, and Human Decision Making , 141–62. Springer.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>5.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "06-chap.html",
    "href": "06-chap.html",
    "title": "8  6.1 이 장의 목표",
    "section": "",
    "text": "8.0.1 6.1.1 쏟아지는 데이터 속에서 정보 찾기\n가설 검정(Hypothesis testing)은 과학의 핵심적인 도구 중 하나입니다. 이는 유한한 데이터 표본을 바탕으로 결론을 내리거나 결정을 내리는 방법입니다. 예를 들어, 질병에 대한 새로운 치료법은 대개 임상 시험을 바탕으로 승인됩니다. 임상 시험은 해당 치료법이 다른 가용한 선택지에 비해 더 나은 효능을 보이는지, 그리고 부작용과의 상충 관계(trade-off)가 수용 가능한지를 결정하는 것을 목표로 합니다. 이러한 시험은 비용이 많이 들고 오랜 시간이 걸릴 수 있습니다. 따라서 모집할 수 있는 환자 수는 제한적이며, 우리는 관찰된 제한된 환자 반응 표본을 바탕으로 추론을 해야 합니다. 환자의 반응은 치료뿐만 아니라 우리가 통제할 수 없는 많은 다른 요인들에 의존하기 때문에 데이터에는 노이즈가 섞여 있습니다. 신뢰할 수 있는 결론을 내리기 위해서는 표본 크기가 충분히 커야 합니다. 반면, 소중한 자원이나 시간을 낭비하지 않도록 표본 크기가 너무 커서도 안 됩니다. 예를 들어, 약값을 필요 이상으로 비싸게 만들거나 새로운 약의 혜택을 볼 수 있는 환자들에게 접근 기회를 박탈해서는 안 되기 때문입니다. 가설 검정의 메커니즘은 오늘날 훨씬 더 널리 사용되고 있긴 하지만, 주로 이러한 응용 분야를 염두에 두고 개발되었습니다.\n생물학적 데이터 분석(및 다른 많은 분야1)에서 가설 검정은 수천 또는 수백만 개의 가능한 가설들을 스크리닝하여 후속 연구를 진행할 가치가 있는 가설을 찾는 데 적용됩니다. 예를 들어, 연구자들은 유전적 변이와 표현형 간의 연관성이나, 유전자 발현 수준과 질병 간의 연관성을 스크리닝합니다. 여기서 “가치 있는” 것은 종종 “통계적으로 유의미한” 것으로 해석되지만, 이 두 개념이 분명히 같은 것은 아닙니다. 통계적 유의성은 흥미로운 것을 찾기 위한 데이터 기반 의사결정의 필요조건이긴 하지만, 충분조건은 아니라고 말하는 것이 타당할 것입니다. 어쨌든, 이러한 대규모 연관성 스크리닝은 다중 가설 검정(multiple hypothesis testing)과 밀접하게 관련되어 있습니다.\n1 신용카드 사기 탐지, 이메일 스팸 탐지, \\(…\\)\n이 장에서 우리는 다음을 수행할 것입니다:\n그림 6.1: 현대 생물학의 고처리량 데이터는 수백만 개의 가설 검정을 통해 연관성을 스크리닝합니다. (출처: 바이엘)\n만약 통계적 검정(불확실성을 동반한 의사결정)이 단 한 번의 결정을 내릴 때도 어려운 과제처럼 느껴진다면, 마음을 단단히 먹으십시오: 유전체학이나 더 일반적으로 “빅데이터” 분야에서는 이를 한 번이 아니라 수천 번 또는 수백만 번 수행해야 합니다. 2장에서 우리는 에피토프 검출의 예와 단 한 곳이 아닌 여러 위치를 고려할 때의 어려움을 보았습니다. 유사하게, 전유전체 시퀀싱(whole genome sequencing)에서는 손에 든 DNA 시퀀싱 데이터와 참조 서열(또는 다른 시퀀싱 데이터 세트) 간의 차이에 대한 증거를 찾기 위해 게놈의 모든 위치를 스캔합니다. 인간 데이터를 보고 있다면 이는 약 60억 번의 검정에 해당합니다! 유전자 또는 화학 화합물 스크리닝에서는 대조군과 비교하여 각 시약이 어세이(assay)에서 효과를 나타내는지 테스트하는데, 이 역시 수만 번에서 수백만 번의 검정이 이루어집니다. 8장에서 우리는 측정된 수천 개의 유전자 각각에 대해 가설 검정을 적용하여 RNA-Seq 데이터의 차등 발현을 분석할 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "06-chap.html#예시-동전-던지기",
    "href": "06-chap.html#예시-동전-던지기",
    "title": "8  6.1 이 장의 목표",
    "section": "8.1 6.2 예시: 동전 던지기",
    "text": "8.1 6.2 예시: 동전 던지기\n이제 단일 검정부터 시작하여 가설 검정에 대해 깊이 파고들어 봅시다. 메커니즘을 제대로 이해하기 위해 가장 간단한 예제 중 하나를 사용합니다: 어떤 동전이 공정한지4 확인하기 위해 동전을 던진다고 가정해 봅시다. 동전을 100번 던지고 매번 앞면이 나왔는지 뒷면이 나왔는지 기록합니다. 그러면 다음과 같은 기록을 갖게 될 것입니다:\n4 동전 던지기가 본질적으로 중요해서가 아니라, (생물학에서 모델 시스템을 사용하는 것처럼) 쉬운 “모델 시스템”이기 때문에 살펴보는 것입니다: 모든 것을 쉽게 계산할 수 있고, 동전 던지기가 무엇인지 이해하는 데 많은 도메인 지식이 필요하지 않습니다. 모든 중요한 개념들이 등장하며, 우리는 이를 더 많은 추가 세부 사항과 함께 다른 응용 분야에 적용할 수 있습니다.\nHHTTHTHTT…\n이를 R에서 시뮬레이션할 수 있습니다. 편향된 동전을 던진다고 가정하고, probHead를 1/2이 아닌 값으로 설정해 보겠습니다:\nset.seed(0xdada)\nnumFlips = 100\nprobHead = 0.6\ncoinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n  replace = TRUE, prob = c(probHead, 1 - probHead))\nhead(coinFlips)__\n\n\n[1] \"T\" \"T\" \"H\" \"T\" \"H\" \"H\"\n이제 동전이 공정하다면 앞면이 절반 정도 나올 것으로 기대할 것입니다. 확인해 봅시다.\ntable(coinFlips)__\n\n\ncoinFlips\n H  T \n59 41 \n결과가 50/50과는 다릅니다. 친구에게 동전이 공정한지 알려주지 않고 이 데이터를 보여주었다고 가정해 봅시다. 그 친구의 사전 가정, 즉 귀무 가설은 동전이 대체로 공정하다는 것입니다. 이 데이터가 동전이 공정하지 않다고 결론 내리기에 충분히 강력할까요? 친구는 무작위 표집에 따른 차이가 발생할 수 있다는 것을 알고 있습니다. 이를 결정하기 위해 공정한 동전에 대한5 우리의 검정 통계량(100번의 동전 던지기에서 나타난 앞면의 총 횟수)의 표집 분포를 살펴봅시다. 1장에서 보았듯이, 동전을 독립적으로 \\(n\\)번 던졌을 때 앞면의 횟수 \\(k\\)는 다음과 같습니다.\n5 공정하다는 것이 무엇을 의미하는지 아직 제대로 정의하지 않았습니다 – 앞면과 뒷면이 나올 확률이 같고, 각 동전 던지기의 결과가 이전 결과에 의존하지 않는다는 것이 합리적인 정의일 것입니다. 더 복잡한 응용 분야의 경우,… nailing down the most suitable null hypothesis can take some thought.\n\\[ P(K=k,|,n, p) = (]\nwhere \\(p\\) is the probability of heads (0.5 if we assume a fair coin). We read the left hand side of the above equation as “the probability that the observed value for \\(K\\) is \\(k\\), given the values of \\(n\\) and \\(p\\)”. Statisticians like to make a difference between all the possible values of a statistic and the one that was observed6, and we use the upper case \\(K\\) for the possible values (so \\(K\\) can be anything between 0 and 100), and the lower case \\(k\\) for the observed value.\n6 In other words, \\(K\\) is the abstract random variable in our probabilistic model, whereas \\(k\\) is its realization, that is, a specific data point.\nWe plot Equation 6.3 in Figure 6.5; for good measure, we also mark the observed value numHeads with a vertical blue line.\nlibrary(\"dplyr\")\nk = 0:numFlips\nnumHeads = sum(coinFlips == \"H\")\nbinomDensity = tibble(k = k,\n     p = dbinom(k, size = numFlips, prob = 0.5))__\n\n\nlibrary(\"ggplot2\")\nggplot(binomDensity) +\n  geom_bar(aes(x = k, y = p), stat = \"identity\") +\n  geom_vline(xintercept = numHeads, col = \"blue\")__\n\nFigure 6.5: The binomial distribution for the parameters \\(n=100\\) and \\(p=0.5\\), according to Equation 6.3.\nSuppose we didn’t know about Equation 6.3. We can still use Monte Carlo simulation to give us something to compare with:\nnumSimulations = 10000\noutcome = replicate(numSimulations, {\n  coinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n                     replace = TRUE, prob = c(0.5, 0.5))\n  sum(coinFlips == \"H\")\n})\nggplot(tibble(outcome)) + xlim(-0.5, 100.5) +\n  geom_histogram(aes(x = outcome), binwidth = 1, center = 50) +\n  geom_vline(xintercept = numHeads, col = \"blue\")__\n\nFigure 6.6: An approximation of the binomial distribution from \\(10000\\) simulations (same parameters as Figure 6.5).\nAs expected, the most likely number of heads is 50, that is, half the number of coin flips. But we see that other numbers near 50 are also quite likely. How do we quantify whether the observed value, 59, is among those values that we are likely to see from a fair coin, or whether its deviation from the expected value is already large enough for us to conclude with enough confidence that the coin is biased? We divide the set of all possible \\(k\\) (0 to 100) in two complementary subsets, the rejection region and the region of no rejection. Our choice here7 is to fill up the rejection region with as many \\(k\\) as possible while keeping their total probability, assuming the null hypothesis, below some threshold \\(\\) (say, 0.05).\n7 More on this in Section 6.3.1.\nIn the code below, we use the function arrange from the dplyr package to sort the p-values from lowest to highest, then pass the result to mutate, which adds another dataframe column reject that is defined by computing the cumulative sum (cumsum) of the p-values and thresholding it against alpha. The logical vector reject therefore marks with TRUE a set of ks whose total probability is less than alpha. These are marked in Figure 6.7, and we can see that our rejection region is not contiguous – it comprises both the very large and the very small values of k.\nlibrary(\"dplyr\")\nalpha = 0.05\nbinomDensity = arrange(binomDensity, p) |&gt;\n        mutate(reject = (cumsum(p) &lt;= alpha))\n\nggplot(binomDensity) +\n  geom_bar(aes(x = k, y = p, col = reject), stat = \"identity\") +\n  scale_colour_manual(\n    values = c(`TRUE` = \"red\", `FALSE` = \"darkgrey\")) +\n  geom_vline(xintercept = numHeads, col = \"blue\") +\n  theme(legend.position = \"none\")__\n\nFigure 6.7: As Figure 6.5, with rejection region (red) that has been chosen such that it contains the maximum number of bins whose total area is at most \\(\\).\nThe explicit summation over the probabilities is clumsy, we did it here for pedagogic value. For one-dimensional distributions, R provides not only functions for the densities (e.g., dbinom) but also for the cumulative distribution functions (pbinom), which are more precise and faster than cumsum over the probabilities. These should be used in practice.\n__\nTask\nDo the computations for the rejection region and produce a plot like Figure 6.7 without using dbinom and cumsum, and with using pbinom instead.\nWe see in Figure 6.7 that the observed value, 59, lies in the grey shaded area, so we would not reject the null hypothesis of a fair coin from these data at a significance level of \\(\\).\n__\nQuestion 6.1\nDoes the fact that we don’t reject the null hypothesis mean that the coin is fair?\n__\nQuestion 6.2\nWould we have a better chance of detecting that the coin is not fair if we did more coin tosses? How many?\n__\nQuestion 6.3\nIf we repeated the whole procedure and again tossed the coin 100 times, might we then reject the null hypothesis?\n__\nQuestion 6.4\nThe rejection region in Figure 6.7 is asymmetric – its left part ends with \\(k=40\\), while its right part starts with \\(k=61\\). Why is that? Which other ways of defining the rejection region might be useful?\nWe have just gone through the steps of a binomial test. In fact, this is such a frequent activity in R that it has been wrapped into a single function, and we can compare its output to our results.\nbinom.test(x = numHeads, n = numFlips, p = 0.5)__\n\n\n    Exact binomial test\n\ndata:  numHeads and numFlips\nnumber of successes = 59, number of trials = 100, p-value = 0.08863\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4871442 0.6873800\nsample estimates:\nprobability of success \n                  0.59",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "06-chap.html#가설-검정의-5단계",
    "href": "06-chap.html#가설-검정의-5단계",
    "title": "8  6.1 이 장의 목표",
    "section": "8.2 6.3 가설 검정의 5단계",
    "text": "8.2 6.3 가설 검정의 5단계\n가설 검정의 일반적인 원칙을 요약해 보겠습니다:\n\n관심 있는 효과를 결정하고, 적절한 실험이나 연구를 설계하며, 데이터 요약 함수와 검정 통계량(test statistic)을 선택합니다.\n귀무 가설(null hypothesis)을 설정합니다. 이는 귀무 가설이 참이라는 가정하에 검정 통계량의 가능한 결과와 그 확률인 귀무 분포(null distribution)를 계산할 수 있게 해주는, 단순하고 계산 가능한 현실 모델입니다.\n기각역(rejection region)을 결정합니다. 즉, 전체 확률이 매우 작은8 가능한 결과들의 하위 집합을 선택합니다.\n실험을 수행하고 데이터를 수집합니다9. 검정 통계량을 계산합니다.\n결정을 내립니다: 검정 통계량이 기각역에 있으면 귀무 가설을 기각합니다10.\n\n8 이에 대한 자세한 내용은 6.3.1절을 참조하십시오.\n9 또는 다른 누군가가 이미 수행했다면 그들의 데이터를 다운로드하십시오.\n10 즉, 그것이 참일 가능성이 낮다고 결론 내립니다.\n이 이상적인 워크플로에서 우리는 데이터를 보기 전인 1~3단계에서 모든 중요한 결정을 내린다는 점에 주목하십시오. 서론(그림 1 및 2)에서 이미 언급했듯이, 이는 종종 현실적이지 않습니다. 우리는 6.6절에서 이 문제로 다시 돌아올 것입니다.\n위의 예에서 사용한 귀무 가설에도 이상화가 있었습니다: 우리는 공정한 동전이 정확히 0.5의 확률을 가져야 하며(예를 들어 0.500001이 아니라), 던지기 사이에 절대적으로 의존성이 없어야 한다고 가정했습니다. 공기 저항, 동전이 떨어지는 재료의 탄성 등 발생 가능한 효과에 대해서는 걱정하지 않았습니다. 이는 귀무 가설이 이항 분포를 통해 계산 가능하다는 이점을 주었습니다. 여기서 이러한 이상화는 그리 논란의 여지가 없어 보일 수 있지만, 다른 상황에서는 귀무 가설이 얼마나 다루기 쉬운가와 얼마나 현실적인가 사이의 상충 관계(trade-off)가 더 클 수 있습니다. 문제는 귀무 가설이 처음부터 너무 이상화되어 있다면, 그것을 기각하는 것이 그리 흥미롭지 않다는 것입니다. 결과가 오도될 수 있으며, 확실히 우리는 시간을 낭비하고 있는 것입니다.\n우리 예에서의 검정 통계량은 앞면의 총 횟수였습니다. 만약 우리가 뒷면이 연속으로 50번 나오고, 그 다음 앞면이 연속으로 50번 나오는 것을 관찰했다고 가정해 봅시다. 우리의 검정 통계량은 결과의 순서를 무시하므로, 우리는 이것이 완벽하게 공정한 동전이라고 결론 내릴 것입니다. 그러나 만약 우리가 뒷면이 두 번 연속으로 나오는 횟수와 같은 다른 검정 통계량을 사용했다면, 이 동전에 뭔가 이상한 점이 있다는 것을 알아챘을지도 모릅니다.\n__\n질문 6.5\n이 다른 검정 통계량의 귀무 분포는 무엇일까요?\n__\n질문 6.6\n그 통계량에 기반한 검정이 일반적으로 더 선호될까요?\n__\n해결책\n__\n아니요, 동전 던지기 사이의 그러한 상관관계를 감지하는 데는 더 큰 검정력을 갖지만, 결과의 편향(bias)을 감지하는 데는 검정력이 더 낮습니다.\n우리가 방금 한 일은 두 가지 서로 다른 종류의 대립 가설(alternative hypotheses)을 살펴본 것입니다. 첫 번째 종류의 대립 가설은 연속적인 동전 던지기가 여전히 서로 독립적이지만 앞면의 확률이 0.5가 아니라는 것이었습니다. 두 번째는 앞면의 전체 확률은 여전히 0.5일 수 있지만 연속적인 동전 던지기가 상관관계가 있다는 것이었습니다.\n__\n질문 6.7\n1장의 충분 통계량(sufficient statistics) 개념을 상기해 보세요. 앞면의 총 횟수는 이항 분포에 대한 충분 통계량인가요? 왜 그것이 첫 번째 종류의 대립 가설에 대해서는 좋은 검정 통계량이 될 수 있지만, 두 번째 종류에 대해서는 그렇지 않을까요?\n그러므로 우리는 일반적으로 여러 가지 가능한 검정 통계량 선택지가 있음을 기억합시다 (원칙적으로는 데이터의 어떤 수치적 요약도 될 수 있습니다). 좋은 검정력11을 가진 검정을 얻기 위해서는 올바른 선택을 하는 것이 중요합니다. 올바른 선택이 무엇인지는 우리가 어떤 종류의 대립 가설을 예상하느냐에 달려 있습니다. 이를 미리 아는 것이 항상 쉬운 것은 아닙니다.\n11 1.4.1절 및 6.4절을 참조하십시오.\n12 가정이 정확히 참일 필요는 없습니다 – 이론의 예측이 진실에 대한 수용 가능한 근사치라면 충분합니다.\n일단 검정 통계량을 선택했다면 그것의 귀무 분포를 계산해야 합니다. 이는 종이와 연필로 할 수도 있고 컴퓨터 시뮬레이션으로 할 수도 있습니다. 종이와 연필을 이용한 해결책은 모수적(parametric)이며 (식 6.3과 같은) 닫힌 형태의 수학적 표현으로 이어지는데, 이는 귀무 가설의 모델 매개변수(\\(n, p\\) 등) 범위에 대해 유효하다는 장점이 있습니다. 또한 특정 매개변수 세트에 대해 빠르게 계산될 수 있습니다. 하지만 항상 동전 던지기 예제처럼 쉬운 것은 아닙니다. 때로는 종이와 연필을 이용한 해결책을 계산하는 것이 불가능할 정도로 어려울 때도 있습니다. 다른 경우에는 단순화 가정이 필요할 수 있습니다. 한 예로 (이 장의 뒷부분에서 살펴볼) \\(t\\)-통계량에 대한 귀무 분포가 있습니다. 데이터가 독립적이고 정규 분포를 따른다고 가정하면 이를 계산할 수 있는데, 그 결과를 \\(t\\)-분포라고 부릅니다. 이러한 모델링 가정은 현실적일 수도 있고 그렇지 않을 수도 있습니다. 귀무 분포를 시뮬레이션하는 것은 잠재적으로 더 정확하고 현실적이며 아마도 더 직관적인 접근 방식을 제공합니다. 시뮬레이션의 단점은 시간이 꽤 오래 걸릴 수 있고, 매개변수 변화가 결과에 어떤 영향을 미치는지 체계적으로 이해하기 위해 추가적인 작업이 필요하다는 점입니다. 일반적으로 모수적 이론이 적용될 때는 이를 사용하는 것이 더 우아합니다12. 의심스러울 때는 시뮬레이션을 하거나, 둘 다 하십시오.\n\n8.2.1 6.3.1 기각역\n여러분의 검정에 적합한 기각역을 어떻게 선택할까요? 먼저, 그 크기는 얼마여야 할까요? 그것은 여러분이 선택한 유의 수준(significance level) 또는 위양성률(false positive rate) \\(\\)이며, 이는 귀무 가설이 참이더라도 검정 통계량이 이 영역에 떨어질 전체 확률입니다13.\n13 어떤 시점에 어떤 사람들이 특정 질문 세트에 대해 \\(\\)를 “작다”고 공모했습니다. 하지만 이 숫자에는 특별한 것이 없으며, 어떤 경우에도 결정 임계값에 대한 최선의 선택은 문맥에 따라 크게 달라질 수 있습니다 (Wasserstein and Lazar 2016; Altman and Krzywinski 2017).\n크기가 주어졌을 때, 다음 질문은 형태에 관한 것입니다. 주어진 크기에 대해 대개 여러 가지 가능한 형태가 존재합니다. 대립 가설이 참일 때 검정 통계량이 기각역에 떨어질 확률이 가능한 한 커야 한다는 요구 조건은 타당합니다. 다시 말해, 우리는 우리의 검정이 높은 검정력(power), 즉 진양성률(true positive rate)을 갖기를 원합니다.\n그림 6.7의 기각역을 계산하는 코드에서 사용한 기준은 그 영역이 가능한 한 많은 k를 포함하도록 하는 것이었습니다. 이는 대립 분포에 대한 정보가 전혀 없는 상황에서는 어떤 k도 다른 k만큼이나 좋기 때문에, 그들의 총 개수를 최대화하는 것입니다.\n이것의 결과로 그림 6.7에서 기각역은 분포의 두 꼬리(tails) 사이로 나뉩니다. 이는 우리가 불공정한 동전이 앞면이나 뒷면 어느 쪽으로든 편향될 수 있다고 예상하기 때문입니다. 만약 우리가 알고 있다면, 예를 들어 편향이 앞면 쪽일 것이라고 생각한다면 기각역을 적절한 쪽(예: 오른쪽 꼬리)에 모두 집중시킬 것입니다. 이러한 선택은 각각 양측(two-sided) 및 단측(one-sided) 검정이라고 불립니다. 보다 일반적으로, 대립 분포에 대한 가정이 있다면 이는 기각역 형태의 선택에 영향을 줄 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "06-chap.html#오류의-유형",
    "href": "06-chap.html#오류의-유형",
    "title": "8  6.1 이 장의 목표",
    "section": "8.3 6.4 오류의 유형",
    "text": "8.3 6.4 오류의 유형\n검정의 메커니즘을 설정했으므로, 우리가 얼마나 잘하고 있는지 평가할 수 있습니다. 표 6.1은 실제 상황(귀무 가설이 실제로 참인지 여부)과 데이터를 본 후 귀무 가설을 기각할지 여부에 대한 우리의 결정을 비교합니다.\n\n\n\n검정 대 실제\n귀무 가설이 참\n\\(…\\)이 거짓\n\n\n\n\n귀무 가설 기각\n제1종 오류 (위양성)\n진양성\n\n\n기각하지 않음\n진음성\n제2종 오류 (위음성)\n\n\n\n표 6.1: 통계적 검정에서의 오류 유형.\n다른 유형의 오류를 증가시키는 대가로 두 가지 오류 유형 중 하나를 줄이는 것은 항상 가능합니다. 진짜 과제는 두 가지 사이에서 수용 가능한 상충 관계를 찾는 것입니다. 이것이 그림 6.2에 예시되어 있습니다. 우리는 임계값을 오른쪽으로 이동시켜 위양성률(false positive rate, FPR)을 항상 낮출 수 있습니다. 우리는 더 “보수적”이 될 수 있습니다. 하지만 이는 더 높은 위음성률(false negative rate, FNR)이라는 대가를 치러야 합니다. 유사하게, 임계값을 왼쪽으로 이동시켜 FNR을 낮출 수 있습니다. 하지만 이 역시 더 높은 FPR이라는 대가를 치러야 합니다. 용어에 대해 조금 설명하자면: FPR은 앞서 언급한 확률 \\(\\)와 같습니다. \\(1 - \\)는 검정의 특이도(specificity)라고도 불립니다. FNR은 때때로 \\(\\)라고도 불리며, \\(1 - \\)는 검정의 검정력(power), 민감도(sensitivity) 또는 진양성률(true positive rate)이라고 불립니다.\n__\n질문 6.8\n6.3절의 끝에서 단측 및 양측 검정에 대해 배웠습니다. 왜 이러한 구분이 존재할까요? 왜 더 넓은 범위의 대립 가설에 민감한 양측 검정을 항상 사용하지 않을까요?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "06-chap.html#t-검정",
    "href": "06-chap.html#t-검정",
    "title": "8  6.1 이 장의 목표",
    "section": "8.4 6.5 t-검정",
    "text": "8.4 6.5 t-검정\n많은 실험 측정값은 유기수(rational numbers)로 보고되며, 우리가 할 수 있는 가장 간단한 비교는 두 그룹 사이의 비교입니다. 예를 들어, 어떤 물질로 처리된 세포와 그렇지 않은 세포를 비교하는 것입니다. 이러한 상황을 위한 기본적인 검정이 \\(t\\)-검정입니다. 검정 통계량은 다음과 같이 정의됩니다.\n\\[ t = c ; , \\]\n여기서 \\(m_1\\)과 \\(m_2\\)는 두 그룹 값들의 평균이고, \\(s\\)는 통합 표준 편차(pooled standard deviation)이며, \\(c\\)는 표본 크기, 즉 두 그룹의 관측치 수 \\(n_1\\)과 \\(n_2\\)에 의존하는 상수입니다. 공식으로는14,\n14 모든 사람이 식 6.4는 기억하려고 노력해야 하지만, 많은 사람들은 필요할 때 식 6.5를 찾아보는 것으로 충분합니다.\n\\[ \\[\\begin{align} m_g &= \\frac{1}{n_g} \\sum_{i=1}^{n_g} x_{g, i}\n\\quad\\quad\\quad g=1,2\\\\\\ s^2 &= \\frac{1}{n_1+n_2-2} \\left( \\sum_{i=1}^{n_1}\n\\left(x_{1,i} - m_1\\right)^2 + \\sum_{j=1}^{n_2} \\left(x_{2,j} - m_2\\right)^2\n\\right)\\\\\\ c &= \\sqrt{\\frac{n_1n_2}{n_1+n_2}} \\end{align}\\] \\]\n여기서 \\(x_{g, i}\\)는 \\(g\\)번째 그룹의 \\(i\\)번째 데이터 포인트입니다. R의 datasets 패키지에 있는 PlantGrowth 데이터를 사용하여 이를 시도해 봅시다.\nlibrary(\"ggbeeswarm\")\ndata(\"PlantGrowth\")\nggplot(PlantGrowth, aes(y = weight, x = group, col = group)) +\n  geom_beeswarm() + theme(legend.position = \"none\")\ntt = with(PlantGrowth, \n       t.test(weight[group ==\"ctrl\"],\n              weight[group ==\"trt2\"],\n              var.equal = TRUE))\ntt __\n\n\n    Two Sample t-test\n\ndata:  weight[group == \"ctrl\"] and weight[group == \"trt2\"]\nt = -2.134, df = 18, p-value = 0.04685\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.980338117 -0.007661883\nsample estimates:\nmean of x mean of y \n    5.032     5.526 \n\n그림 6.8: PlantGrowth 데이터.\n__\n질문 6.9\ntrt1과의 비교에서는 무엇을 얻나요? trt1 대 trt2는요?\np-값을 계산하기 위해 t.test 함수는 \\(t\\)-통계량(식 6.4)에 대한 점근적 이론을 사용합니다. 이 이론은 두 그룹의 평균이 같다는 귀무 가설하에서 통계량이 알려진 수학적 분포인 자유도가 \\(n_1+n_2-2\\)인 \\(t\\)-분포를 따른다고 명시합니다. 이 이론은 추가적인 기술적 가정, 즉 데이터가 독립적이고 동일한 표준 편차를 가진 정규 분포에서 나온다는 가정을 사용합니다. 우리는 이러한 가정들에 대해 걱정할 수 있습니다. 분명히 이 가정들은 성립하지 않습니다: 무게는 항상 양수이지만, 정규 분포는 전체 실수 축에 걸쳐 있습니다. 문제는 이론적 가정으로부터의 이러한 이탈이 실제로 차이를 만드느냐 하는 것입니다. 우리는 이를 알아내기 위해 순열 검정(permutation test)을 사용할 수 있습니다(6.5.1절에서 순열 검정의 이면에 있는 아이디어를 좀 더 자세히 논의할 것입니다).\n__\n질문 6.10\n위의 \\(t\\)-검정에 대해 동전 던지기의 그림 6.5 및 그림 6.6과 유사한 방식으로 이론적 및 시뮬레이션된 귀무 분포를 플롯해 보세요.\n__\n해결책\n__\nplgr = dplyr::filter(PlantGrowth, group %in% c(\"ctrl\", \"trt2\"))\n\nalpha  = 0.05\nxrange = 5 * c(-1, 1)\ndeckel = function(x) ifelse(x &lt; xrange[1], xrange[1], ifelse(x &gt; xrange[2], xrange[2], x))\n\nsim_null = tibble(\n  t = replicate(10000, t.test(weight ~ sample(group), var.equal = TRUE, data = plgr)$statistic)\n)\nsim_thresh = quantile(sim_null$t, c(alpha/2, 1-alpha/2))\nsim_null = mutate(sim_null, \n  t = deckel(t),        # 범위를 벗어난 데이터에 대한 경고 피하기\n  reject = ifelse(t &lt;= sim_thresh[1], \"low\", ifelse(t &gt; sim_thresh[2], \"high\", \"none\"))\n) \n\ntheo_thresh = qt(c(alpha/2, 1-alpha/2), df =  nrow(plgr) - 2)\ntheo_null = tibble(\n  t = seq(-5, 5, by = 0.05),\n  density = dt (x = t, df = nrow(plgr)  - 2),\n  reject = ifelse(t &lt;= theo_thresh[1], \"low\", ifelse(t &gt; theo_thresh[2], \"high\", \"none\"))\n)\n\np1 = ggplot(sim_null, aes(x = t, col = reject, fill = reject)) +\n       geom_bar(stat = \"bin\", breaks = seq(-5, 5, by = 0.2)) \np2 = ggplot(theo_null, aes(x = t, y = density, col = reject, fill = reject)) +\n       geom_area() \n\nfor (p in list(p1, p2))\n  print(p + \n        geom_vline(xintercept = tt$statistic, col = \"#101010\") +\n        scale_colour_manual(values = c(low = \"blue\", high = \"red\", none = \"darkgrey\")) +\n        scale_fill_manual(values = c(low = \"blue\", high = \"red\", none = \"darkgrey\")) + \n        xlim(xrange) + theme(legend.position = \"none\"))__\n\nFigure 6.9\n\nFigure 6.10\n__\nQuestion 6.11\nWould the solution to the preceding question get simpler if we considered the absolute value of \\(t\\) instead of \\(t\\) itself?\n__\nSolution\n__\nYes, if we are not interested in the sign of the difference, we can directly work on \\((t)\\), and have only a single rejection region instead of a lower and an upper one.\nThe \\(t\\)-test comes in multiple flavors, all of which can be chosen through parameters of the t.test function. What we did above is called a two-sided two-sample unpaired test with equal variance. Two-sided refers to the fact that we were open to reject the null hypothesis if the weight of the treated plants was either larger or smaller than that of the untreated ones.\nTwo-sample 15 indicates that we compared the means of two groups to each other; another option is to compare the mean of one group against a given, fixed number.\n15 It can be confusing that the term sample has a different meaning in statistics than in biology. In biology, a sample is a single specimen on which an assay is performed; in statistics, it is a set of measurements, e.g., the \\(n_1\\)-tuple \\((x_{1,1},…,x_{1,n_1})\\) in Equation 6.5, which can comprise several biological samples. In contexts where this double meaning might create confusion, we refer to the data from a single biological sample as an observation.\nUnpaired means that there was no direct 1:1 mapping between the measurements in the two groups. If, on the other hand, the data had been measured on the same plants before and after treatment, then a paired test would be more appropriate, as it looks at the change of weight within each plant, rather than their absolute weights.\nEqual variance refers to the way the statistic Equation 6.4 is calculated. That expression is most appropriate if the variances within each group are about the same. If they are very different, an alternative form (Welch’s \\(t\\)-test) and associated asymptotic theory exist.\nThe independence assumption. Now let’s try something peculiar: duplicate the data.\nwith(rbind(PlantGrowth, PlantGrowth),\n       t.test(weight[group == \"ctrl\"],\n              weight[group == \"trt2\"],\n              var.equal = TRUE))__\n\n\n    Two Sample t-test\n\ndata:  weight[group == \"ctrl\"] and weight[group == \"trt2\"]\nt = -3.1007, df = 38, p-value = 0.003629\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.8165284 -0.1714716\nsample estimates:\nmean of x mean of y \n    5.032     5.526 \nNote how the estimates of the group means (and thus, of the difference) are unchanged, but the p-value is now much smaller! We can conclude two things from this:\n\nThe power of the \\(t\\)-test depends on the sample size. Even if the underlying biological differences are the same, a dataset with more observations tends to give more significant results16.\nThe assumption of independence between the measurements is really important. Blatant duplication of the same data is an extreme form of dependence, but to some extent the same thing happens if you mix up different levels of replication. For instance, suppose you had data from 8 plants, but measured the same thing twice on each plant (technical replicates), then pretending that these are now 16 independent measurements is wrong.\n\n16 You can also see this from the way the numbers \\(n_1\\) and \\(n_2\\) appear in Equation 6.5.\n\n8.4.1 6.5.1 Permutation tests\nWhat happened above when we contrasted the outcome of the parametric \\(t\\)-test with that of the permutation test applied to the \\(t\\)-statistic? It’s important to realize that these are two different tests, and the similarity of their outcomes is desirable, but coincidental. In 모수적 검정의 경우, \\(t\\)-통계량의 귀무 분포는 데이터의 가정된 귀무 분포(단위 공분산을 가진 \\((n_1+n_2)\\)차원 공간 \\(^{n_1+n_2}\\)에서의 다변량 정규 분포)로부터 도출되며 연속적입니다: 즉, \\(t\\)-분포입니다. 이와 대조적으로, 우리 검정 통계량의 순열 분포는 단일 데이터 인스턴스(\\(n_1+n_2\\)개의 관측치)로부터 관측치 레이블의 \\((n_1+n_2)!\\)개 순열17의 유한 집합으로부터 얻어지므로 이산적입니다. 여기서 우리가 가정하는 전부는 귀무 가설하에서 변수 \\(X_{1,1},…,X_{1,n_1},X_{2,1},…,X_{2,n_2}\\)가 교환 가능하다(exchangeable)는 것입니다. 논리적으로 이 가정은 모수적 검정의 가정에 의해 함축되지만, 더 약한 가정입니다. 순열 검정은 \\(t\\)-통계량을 사용하지만, \\(t\\)-분포(또는 정규 분포)는 사용하지 않습니다. 두 검정이 매우 유사한 결과를 주었다는 사실은 중심 극한 정리(Central Limit Theorem)의 결과입니다.\n17 계산 시간을 절약하고 싶은 경우에는 무작위 하위 집합을 사용합니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "06-chap.html#p-값-해킹p-value-hacking",
    "href": "06-chap.html#p-값-해킹p-value-hacking",
    "title": "8  6.1 이 장의 목표",
    "section": "8.5 6.6 P-값 해킹(P-value hacking)",
    "text": "8.5 6.6 P-값 해킹(P-value hacking)\n동전 던지기 예제로 돌아가 봅시다. 우리는 동전이 공정하지 않다는 것을 “알고” 있었음에도 불구하고(6.2절에서 probHead를 0.6으로 선택했으므로), 5% 수준에서 귀무 가설(동전이 공정하다는 가설)을 기각하지 않았습니다. 이제 우리가 서로 다른 검정 통계량들을 살펴보기 시작한다고 가정해 봅시다. 아마도 3번 이상 연속으로 앞면이 나온 횟수라든지, 처음 50번의 던지기에서 앞면의 횟수 등등 말이죠. 그러다 보면 어느 시점에는 단지 우연히라도 작은 p-값이 나오는 검정을 발견하게 될 것입니다(결국 귀무 가설—공정한 동전—하에서 p-값이 0.05보다 작을 확률은 20분의 1입니다). 우리는 방금 p-값 해킹(p-value hacking)18이라 불리는 일을 저지른 것입니다 (Head et al. 2015). 무엇이 문제인지 아시겠지요: 우리 주장을 증명하려는 열의에 차서 데이터가 우리가 원하는 대로 해줄 때까지 데이터를 고문한 것입니다. 이와 관련된 전술로는 가설 전환(hypothesis switching) 또는 HARKing(결과가 알려진 후에 가설을 세우는 것)이 있습니다: 데이터 세트가 있고, 아마도 그것을 모으기 위해 많은 시간과 돈을 투자했으므로 결과가 필요합니다. 우리는 수많은 다양한 귀무 가설과 검정 통계량을 생각해 내고, 이를 테스트하고, 무언가 보고할 만한 것이 나올 때까지 반복합니다.\n18 http://fivethirtyeight.com/features/science-isnt-broken\n이러한 전술들은 6.3절에서 설명한 가설 검정의 규칙을 위반하는 것입니다. 거기서는 가설과 검정 방법을 먼저 선택하고 그 다음에 데이터를 수집하는 순차적인 절차를 제시했습니다. 그러나 2장에서 보았듯이, 실제로는 그러한 전술이 유혹적일 수 있습니다. 생물학적 데이터의 경우 데이터를 “정규화”하고, 변환하고, 배치 효과(batch effects)를 보정하고, 이상치를 제거하는 등 너무나 많은 다양한 선택지가 존재합니다. 주제는 복잡하고 정답이 정해져 있지 않습니다. Wasserstein과 Lazar (2016)는 과학계에서 p-값이 어떻게 사용되는지에 대한 문제점들과 몇 가지 오해들에 대해 읽기 쉬운 짧은 요약을 제공했습니다. 그들은 또한 p-값이 어떻게 유익하게 사용될 수 있는지도 강조했습니다. 핵심 메시지는 다음과 같습니다: 여러분의 데이터, 어떤 분석을 시도했는지, 그리고 어떻게 수행했는지에 대해 완전히 투명하게 공개하십시오. 분석 코드를 제공하십시오. 그러한 맥락 정보가 있어야만 p-값은 유용할 수 있습니다.\n착각을 피하십시오. 우리의 통계적 검정은 결코 귀무 가설이 참임을 증명하려는 시도가 아니라는 점을 명심하십시오 - 우리는 단순히 그것이 거짓이라는 증거가 있는지 없는지를 말하는 것뿐입니다. 만약 높은 p-값이 귀무 가설이 참이라는 것을 나타내는 것이라면, 우리는 완전히 미친 귀무 가설을 세우고, 전혀 무관한 실험을 하고, 결론을 내릴 수 없는 소량의 데이터를 수집하여, 0과 1 사이의 랜덤한 숫자인 (따라서 우리의 임계값 \\(\\)보다 클 확률이 높은) p-값을 찾음으로써 우리의 가설을 증명해 버릴 수 있을 것입니다!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "06-chap.html#다중-검정multiple-testing",
    "href": "06-chap.html#다중-검정multiple-testing",
    "title": "8  6.1 이 장의 목표",
    "section": "8.6 6.7 다중 검정(Multiple testing)",
    "text": "8.6 6.7 다중 검정(Multiple testing)\n__\n질문 6.12\nxkcd 만화 882를 찾아보세요. 왜 신문은 다른 색상들에 대한 결과는 보도하지 않았을까요?\n만화에서 묘사된 곤경은 생물학의 고처리량 데이터에서도 발생합니다. 그것도 아주 강력하게 말이죠! 여러분은 단지 20가지 색상의 젤리빈뿐만 아니라, 예를 들어 두 조건 사이의 차등 발현을 위해 테스트된 20,000개의 유전자나, DNA 돌연변이가 일어났을지도 모르는 게놈 상의 60억 개의 위치를 다루게 될 것입니다. 그렇다면 우리는 이를 어떻게 처리해야 할까요? 통계적 검정 결과와 실제 상황을 연관시킨 표(표 6.1)를 다시 살펴봅시다. 이번에는 모든 것을 많은 가설의 관점에서 구성해 보겠습니다.\n\n\n\n검정 대 실제\n귀무 가설이 참\n\\(…\\)이 거짓\n합계\n\n\n\n\n기각됨\n\\(V\\)\n\\(S\\)\n\\(R\\)\n\n\n기각되지 않음\n\\(U\\)\n\\(T\\)\n\\(m-R\\)\n\n\n합계\n\\(m_0\\)\n\\(m-m_0\\)\n\\(m\\)\n\n\n\n표 6.2: 다중 검정에서의 오류 유형. 문자는 각 유형의 오류가 발생한 횟수를 나타냅니다.\n\n\\(m\\): 전체 검정(및 귀무 가설)의 수\n\\(m_0\\): 참인 귀무 가설의 수\n\\(m-m_0\\): 거짓인 귀무 가설의 수\n\\(V\\): 위양성(false positives)의 수 (제1종 오류의 척도)\n\\(T\\): 위음성(false negatives)의 수 (제2종 오류의 척도)\n\\(S\\), \\(U\\): 진양성 및 진음성의 수\n\\(R\\): 기각 횟수\n\n이 장의 나머지 부분에서 우리는 제1종 및 제2종 오류를 관리하는 다양한 방법들을 살펴봅니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "06-chap.html#가족오류율family-wise-error-rate",
    "href": "06-chap.html#가족오류율family-wise-error-rate",
    "title": "8  6.1 이 장의 목표",
    "section": "8.7 6.8 가족오류율(Family wise error rate)",
    "text": "8.7 6.8 가족오류율(Family wise error rate)\n가족오류율(family wise error rate, FWER)은 \\(V&gt;0\\)일 확률, 즉 하나 이상의 위양성 오류를 저지를 확률입니다. 우리는 이를 위양성 오류를 전혀 저지르지 않을 확률의 여사건으로 계산할 수 있습니다19.\n19 독립을 가정함.\n\\[ \\[\\begin{align} P(V&gt;0) &= 1 - P(\\text{$m_0$개의 귀무 가설 중 어느 것도 기각되지 않음})\n\\\\\\ &= 1 - (1 - \\alpha)^{m_0} \\to 1 \\quad\\text{as } m_0\\to\\infty. \\end{align}\\] \\]\n어떤 고정된 \\(\\)에 대해, 이 확률은 \\(m_0\\)가 \\(1/\\)의 정도가 되자마자 무시할 수 없을 정도로 커지며, \\(m_0\\)가 커짐에 따라 1을 향해 갑니다. 이러한 관계는 방대한 잠재적 일치 데이터베이스를 검색하는 DNA 매칭과 같은 실험에서 심각한 결과를 초래할 수 있습니다. 예를 들어, 두 사람의 DNA 프로필이 무작위 오차에 의해 일치할 확률이 100만 분의 1이고, 여러분의 DNA를 80만 개의 프로필이 담긴 데이터베이스와 대조한다면, 데이터베이스 내에서 (즉, 여러분이 그 안에 없더라도) 무작위로 일치할 확률은 다음과 같습니다:\n1 - (1 - 1/1e6)^8e5 __\n\n\n[1] 0.5506712\n상당히 높습니다. 그리고 데이터베이스에 몇 백만 개의 프로필이 더 포함된다면, 잘못된 일치는 사실상 피할 수 없습니다.\n__\n질문 6.13\n\\(m_0\\)가 클 때 식 6.6의 확률이 실제로 1에 매우 가까워진다는 것을 증명해 보세요.\n\n8.7.1 6.8.1 본페로니 방법(Bonferroni method)\nFWER을 제어하고 싶다면 가설별 \\(\\)를 어떻게 선택해야 할까요? 위의 계산들은 \\(\\)와 \\(m_0\\)의 곱이 합리적인 어림값이 될 수 있음을 시사합니다. 대개 우리는 \\(m_0\\)를 모르지만, \\(m_0 m\\)이므로 \\(m_0\\)의 상한선인 \\(m\\)은 알고 있습니다. 본페로니 방법은 간단히 말해, 우리가 \\({}\\) 수준에서 FWER 제어를 원한다면 가설별 임계값을 \\(= {}/m\\)으로 선택해야 한다는 것입니다. 예제를 통해 이를 확인해 봅시다.\nm = 10000\nggplot(tibble(\n  alpha = seq(0, 7e-6, length.out = 100),\n  p     = 1 - (1 - alpha)^m),\n  aes(x = alpha, y = p)) +  geom_line() +\n  xlab(expression(alpha)) +\n  ylab(\"Prob( no false rejection )\") +\n  geom_hline(yintercept = 0.05, col = \"red\")__\n\n그림 6.11: 본페로니 방법. 플롯은 \\(m=10000\\)에 대해 \\(\\)의 함수로서 식 6.6의 그래프를 보여줍니다.\n그림 6.11에서 검은색 선은 (0.05 값에 해당하는) 빨간색 선과 \\(^{-6}\\)에서 교차하는데, 이는 본페로니 방법에 의해 함축된 \\(0.05/m\\) 값보다 아주 약간 큽니다.\n__\n질문 6.14\n왜 두 값이 정확히 같지 않을까요?\n하지만 이 방법의 잠재적인 단점은 \\(m_0\\)가 크면 기각 임계값이 매우 작아진다는 것입니다. 이는 무언가를 감지할 기회를 가지려면 개별 검정의 검정력이 매우 높아야 함을 의미합니다. 종종 FWER 제어는 너무 엄격하여, 데이터를 생성하고 모으는 데 들인 시간과 비용을 비효율적으로 사용하게 만들 수 있습니다. 이제 우리는 제1종 오류를 제어하는 더 미묘한 방법들이 있다는 것을 보게 될 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "06-chap.html#허위-발견율false-discovery-rate",
    "href": "06-chap.html#허위-발견율false-discovery-rate",
    "title": "8  6.1 이 장의 목표",
    "section": "8.8 6.9 허위 발견율(False discovery rate)",
    "text": "8.8 6.9 허위 발견율(False discovery rate)\n데이터를 살펴봅시다. 합성 글루코코르티코이드인 덱사메타손(dexamethasone)을 처리하거나 처리하지 않은 4개의 1차 인간 기도 평활근 세포주의 유전자 발현 측정값(유전자 수준 카운트)이 포함된 RNA-Seq 데이터 세트 airway를 불러옵니다. 우리는 8장에서 더 자세히 다룰 DESeq2 방법을 사용할 것입니다. 지금은 이 방법이 각 유전자에 대해 차등 발현 검정을 수행한다고만 말해두면 충분합니다. 개념적으로 검정되는 귀무 가설은 \\(t\\)-검정의 그것과 유사하지만, 카운트 데이터를 다루기 때문에 세부 사항은 약간 더 복잡합니다.\nlibrary(\"DESeq2\")\nlibrary(\"airway\")\ndata(\"airway\")\naw   = DESeqDataSet(se = airway, design = ~ cell + dex)\naw   = DESeq(aw)\nawde = as.data.frame(results(aw)) |&gt; dplyr::filter(!is.na(pvalue))__\n__\n태스크\nawde의 내용을 살펴보세요.\n__\n태스크\n(선택 사항) 위의 코드 청크가 무엇을 하는지에 대한 더 많은 정보를 얻으려면 DESeq2 비네트 및/또는 8장을 참고하십시오.\n\n8.8.1 6.9.1 p-값 히스토그램\np-값 히스토그램은 다중 검정을 포함하는 모든 분석에서 중요한 무결성 검사(sanity check)입니다. 이는 두 가지 성분으로 구성된 혼합물입니다:\nnull: 귀무 가설이 참인 검정에서 얻은 p-값들.\nalt: 귀무 가설이 거짓인 검정에서 얻은 p-값들. 이 두 성분의 상대적 크기는 참인 귀무 가설과 참인 대립 가설의 비율(즉, \\(m_0\\)와 \\(m\\))에 달려 있으며, 종종 히스토그램에서 시각적으로 추정할 수 있습니다. 분석의 통계적 검정력이 높다면 두 번째 성분(“alt”)은 대부분 작은 p-값들로 구성되며, 즉 히스토그램에서 0 근처의 정점으로 나타납니다. 일부 대립 가설에 대해 검정력이 높지 않다면 이 정점이 오른쪽으로 뻗어 나가는, 즉 “어깨(shoulder)” 모양을 가질 것으로 예상합니다. “null” 성분의 경우 (연속형 데이터와 검정 통계량에 대한 p-값의 정의에 따라) \\([0,1]\\)에서의 균등 분포를 예상합니다. airway 데이터에 대한 p-값 히스토그램을 그려봅시다.\nggplot(awde, aes(x = pvalue)) +\n  geom_histogram(binwidth = 0.025, boundary = 0)__\n\n그림 6.12: airway 데이터에 대한 p-값 히스토그램.\n그림 6.12에서 우리는 예상된 혼합물을 봅니다. 또한 null 성분이 정확히 평평(균등)하지 않음을 볼 수 있는데, 이는 데이터가 카운트(counts)이기 때문입니다. 카운트가 높을 때는 준연속(quasi-continuous)으로 보이지만, 카운트가 낮은 검정의 경우 데이터의 이산성(discreteness)과 그 결과로 나타나는 p-값이 히스토그램 오른쪽의 뾰족한 부분(spikes)으로 나타납니다.\n이제 우리가 p-값이 \\(\\)보다 작은 모든 검정을 기각한다고 가정해 봅시다. 우리는 다음 코드로 생성된 그림 6.13과 같은 플롯을 통해 허위 발견 비율(false discovery proportion)의 추정치를 시각적으로 결정할 수 있습니다.\nalpha = binw = 0.025\npi0 = 2 * mean(awde$pvalue &gt; 0.5)\nggplot(awde,\n  aes(x = pvalue)) + geom_histogram(binwidth = binw, boundary = 0) +\n  geom_hline(yintercept = pi0 * binw * nrow(awde), col = \"blue\") +\n  geom_vline(xintercept = alpha, col = \"red\")__\n\n그림 6.13: p-값 히스토그램을 통한 FDR의 시각적 추정.\n우리는 첫 번째 빈 \\([0, ]\\)에 4772개의 p-값이 있음을 알 수 있는데, 이 중 약 945개는 null일 것으로 예상됩니다(파란색 선으로 표시됨). 따라서 우리는 잘못된 기각의 비율을 다음과 같이 추정할 수 있습니다.\npi0 * alpha / mean(awde$pvalue &lt;= alpha)__\n\n\n[1] 0.1980092\n허위 발견율(False discovery rate, FDR)은 다음과 같이 정의됩니다.\n\\[ = \\!, \\]\n여기서 \\(R\\)과 \\(V\\)는 표 6.2에서와 같습니다. 분모의 식은 \\(R=0\\)인 경우에도 FDR이 잘 정의되도록 보장합니다(그 경우 함축적으로 \\(V=0\\)이 됨). 모든 귀무 가설이 참인 경우, 즉 \\(V=R\\)인 경우 FDR은 FWER과 동일해진다는 점에 유의하세요. \\(\\)는 기대값(expected value)을 의미합니다. 즉, FDR은 수치가 아니라… associated with a specific outcome of \\(V\\) and \\(R\\) for one particular experiment. Rather, given our choice of tests and associated rejection rules for them, it is the average20 proportion of type I errors out of the rejections made, where the average is taken (at least conceptually) over many replicate instances of the experiment.\n20 Since the FDR is an expectation value, it does not provide worst case control: in any single experiment, the so-called false discovery proportion (FDP), that is the realized value \\(v/r\\) (without the \\(\\)), could be much higher or lower.\n\n\n8.8.2 6.9.2 The Benjamini-Hochberg algorithm for controlling the FDR\nThere is a more elegant alternative to the “visual FDR” method of the last section. The procedure, introduced by Benjamini and Hochberg (1995) has these steps:\n\nFirst, order the p-values in increasing order, \\(p_{(1)} … p_{(m)}\\)\nThen for some choice of \\(\\) (our target FDR), find the largest value of \\(k\\) that satisfies: \\(p_{(k)} , k / m\\)\nFinally reject the hypotheses \\(1, …, k\\)\n\nWe can see how this procedure works when applied to our RNA-Seq p-values through a simple graphical illustration:\nphi  = 0.10\nawde = mutate(awde, rank = rank(pvalue))\nm    = nrow(awde)\n\nggplot(dplyr::filter(awde, rank &lt;= 7000), aes(x = rank, y = pvalue)) +\n  geom_line() + geom_abline(slope = phi / m, col = \"red\")__\n\nFigure 6.14: Visualization of the Benjamini-Hochberg procedure. Shown is a zoom-in to the 7000 lowest p-values.\nThe method finds the rightmost point where the black (our p-values) and red lines (slope \\(/ m\\)) intersect. Then it rejects all tests to the left.\nkmax = with(arrange(awde, rank),\n         last(which(pvalue &lt;= phi * rank / m)))\nkmax __\n\n\n[1] 4099\n__\nQuestion 6.15\nCompare the value of kmax with the number of 4772 from above (Figure 6.13). Why are they different?\n__\nQuestion 6.16\nLook at the code associated with the option method=\"BH\" of the p.adjust function that comes with R. How does it compare to what we did above?\n__\nQuestion 6.17\nSchweder and Spj øtvoll plot: check out Figures 1–3 in Schweder and Spjøtvoll (1982). Make a similar plot for the data in awde. How does it relate to Figures 6.14 and 6.13?\n__\nSolution\n__\nThirteen years before Benjamini and Hochberg (1995), Schweder and Spjøtvoll (1982) suggested a diagnostic plot of the observed \\(p\\)-values that permits estimation of the fraction of true null hypotheses. For a series of hypothesis tests \\(H_1, …, H_m\\) with \\(p\\)-values \\(p_i\\), they suggested plotting\n\\[ ( 1-p_i, N(p_i) ) i , …, m, \\]\nwhere \\(N(p)\\) is the number of \\(p\\)-values greater than \\(p\\). An application of this diagnostic plot to awde$pvalue is shown in Figure 6.15. When all null hypotheses are true, each of the \\(p\\)-values is uniformly distributed in \\([0,1]\\), Consequently, the empirical cumulative distribution of the sample \\((p_1, …, p_m)\\) is expected to be close to the line \\(F(t)=t\\). By symmetry, the same applies to \\((1 - p_1, …, 1 - p_m)\\). When (without loss of generality) the first \\(m_0\\) null hypotheses are true and the other \\(m-m_0\\) are false, the empirical cumulative distribution of \\((1-p_1, …, 1-p_{m_0})\\) is again expected to be close to the line \\(F_0(t)=t\\). The empirical cumulative distribution of \\((1-p_{m_0+1}, …, 1-p_{m})\\), on the other hand, is expected to be close to a function \\(F_1(t)\\) which stays below \\(F_0\\) but shows a steep increase towards 1 as \\(t\\) approaches \\(1\\). In practice, we do not know which of the null hypotheses are true, so we only observe a mixture whose empirical cumulative distribution is expected to be close to\n\\[ F(t) = F_0(t) + F_1(t). \\]\nSuch a situation is shown in Figure 6.15. If \\(F_1(t)/F_0(t)\\) is small for small \\(t\\) (i.e., the tests have reasonable power), then the mixture fraction \\(\\) can be estimated by fitting a line to the left- hand portion of the plot, and then noting its height on the right. Such a fit is shown by the red line. Here, we focus on those tests for which the count data are not all very small numbers (baseMean&gt;=1), since for these the p-value null distribution is sufficiently close to uniform (i.e., does not 위의 genes 들에 대해서만 위와 같은 현상이 발생함)을 보여주기 위해 baseMean &gt;= 1인 유전자들만 필터링했습니다. 여러분은 모든 유전자에 대해 동일한 플롯을 만들어 볼 수도 있습니다.\nawdef = awde |&gt;\n  dplyr::filter(baseMean &gt;=1) |&gt; \n  arrange(pvalue) |&gt;\n  mutate(oneminusp = 1 - pvalue,\n         N = n() - row_number())\njj = round(nrow(awdef) * c(1, 0.5))\nslope = with(awdef, diff(N[jj]) / diff(oneminusp[jj]))\nggplot(awdef) +\n  geom_point(aes(x = oneminusp, y = N), size = 0.15) + \n  xlab(expression(1-p[i])) +\n  ylab(expression(N(p[i]))) +\n  geom_abline(intercept = 0, slope = slope, col = \"red3\") +\n  geom_hline(yintercept = slope, linetype = \"dotted\") +\n  geom_vline(xintercept = 1, linetype = \"dotted\") +\n  geom_text(x = 0, y = slope, label = paste(round(slope)), \n            hjust = -0.1, vjust = -0.25) __\n\n그림 6.15: Schweder와 Spjotvoll 플롯.\nawdef에는 22,853개의 행이 있으며, 따라서 이 단순한 추정치에 따르면 22853-17302=5551개의 대립 가설이 존재합니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "06-chap.html#국소-fdrlocal-fdr",
    "href": "06-chap.html#국소-fdrlocal-fdr",
    "title": "8  6.1 이 장의 목표",
    "section": "8.9 6.10 국소 FDR(Local FDR)",
    "text": "8.9 6.10 국소 FDR(Local FDR)\n\n그림 6.16: http://xkcd.com/1132 – 빈도론자는 현재 가용한 데이터만 가질 수 있는 반면, 베이지안은 세계에 대한 이해나 이전 경험을 활용할 수 있습니다. 베이지안으로서 그녀는 우리 태양의 질량이 노바(nova)가 되기에는 너무 작다는 물리학 지식을 충분히 알고 있을 것입니다. 설령 물리학을 모르더라도, 그녀는 경험적 베이지안(empirical Bayesian)이 되어 태양이 폭발하지 않았던 수많은 이전 날들로부터 사전 확률을 가져올 수 있습니다.\n이 장의 첫머리에 있는 xkcd 만화는 다중 검정 문제를 오차를 축적하는 방식이라는 다소 불길한 해석으로 끝을 맺었지만, 그림 6.16은 다중 검정의 기회를 강조합니다: 우리가 많은 검정을 수행할 때, 우리는 다중성을 사용하여 단일 검정으로 가능한 것 이상의 이해를 높일 수 있습니다.\n\n그림 6.17: 국소 허위 발견율과 두 그룹 모델. \\(f_{}(p)\\)의 일부 선택 및 \\(_0=0.6\\)을 사용함; 밀도(상단) 및 분포 함수(하단).\n그림 6.13의 히트맵으로 돌아가 봅시다. 개념적으로 우리는 이를 이른바 두 그룹 모델(two-groups model, Efron 2010)의 관점에서 생각할 수 있습니다:\n\\[ f(p)= _0 + (1-0) f{}(p), \\]\n여기서 \\(f(p)\\)는 분포의 밀도(데이터가 무한하고 빈이 무한히 작을 때 히스토그램의 모습)이고, \\(0\\)는 균등 성분의 크기를 나타내는 0과 1 사이의 숫자이며, \\(f{}\\)는 대립 성분입니다. 이는 우리가 이미 4장에서 보았던 혼합 모델입니다. 혼합 밀도와 주변 밀도 \\(f(p)\\)는 그림 6.17의 상단 패널에 시각화되어 있습니다: 파란색 영역들은 함께 \\(f_{}(p)\\)의 그래프에 해당하고, 회색 영역들은 \\(f_{}(p) = _0\\)에 해당합니다. 이제 특정 컷오프 \\(p\\)(예를 들어 그림 6.17에서처럼 \\(p=0.1\\))를 고려한다면, 이 컷오프에서 기각하는 가설이 위양성일 확률을 다음과 같이 계산할 수 있습니다. 우리는 컷오프에서의 \\(f\\) 값(빨간색 선)을 귀무 가설의 기여분(연한 빨간색, \\(_0\\))과 대립 가설의 기여분(더 짙은 빨간색, \\((1-0) f{}(p)\\))으로 분해합니다. 그러면 국소 허위 발견율(local false discovery rate)은 다음과 같습니다.\n\\[ (p) = . \\]\n정의에 따라 이 수치는 0과 1 사이입니다. 그림 6.17에서 \\(\\)이 \\(p\\)의 단조 증가 함수라는 점에 주목하세요. 이는 fdr이 가장 작은 \\(p\\)에 대해 가장 낮아야 하고, 그 다음 점차 커져서 가장 오른쪽 끝에서 1에 도달해야 한다는 우리의 직관과 일치합니다. 우리는 빨간색 선뿐만 아니라 곡선 아래의 면적에 대해서도 유사한 분해를 할 수 있습니다. 이는 다음과 같습니다.\n\\[ F(p) = _0^p f(t),dt, \\]\n그리고 전체 면적 \\(F(p)\\)에 대한 짙은 회색 영역(\\(_0\\)에 \\(p\\)를 곱한 값)의 비율은 꼬리 영역 허위 발견율(tail area false discovery rate)(Fdr21)입니다.\n21 관습적으로 소문자 약어 fdr은 국소(local)를, 대문자 약어 Fdr은 식 6.10의 두 그룹 모델 문맥에서의 꼬리 영역 허위 발견율을 위해 사용합니다. 약어 FDR은 원래의 정의인 식 6.7을 위해 사용되는데, 이는 좀 더 일반적입니다. 즉, 식 6.10의 모델링 가정에 의존하지 않습니다.\n\\[ (p) = . \\]\n우리는 그림 6.21의 진단을 위해 \\(F\\)의 데이터 버전을 사용할 것입니다.\nqvalue와 fdrtool 패키지는 이러한 모델을 데이터에 적합시키는 기능을 제공합니다.\nlibrary(\"fdrtool\")\nft = fdrtool(awde$pvalue, statistic = \"pvalue\")__\nfdrtool에서는 위에서 우리가 \\(_0\\)라고 불렀던 것을 eta0라고 부릅니다:\nft$param[,\"eta0\"]__\n\n\n     eta0 \n0.8822922 \n__\n질문 6.18\n위의 fdrtool 호출로 생성된 플롯들은 무엇을 보여주나요?\n__\n태스크\n리스트 ft의 다른 요소들을 탐색해 보세요.\n__\n질문 6.19\n경험적 베이즈(empirical Bayes) 방법에서 “경험적”은 무엇을 의미하나요?\n\n8.9.1 6.10.1 국소 대 전체\nFDR(또는 Fdr)은 집합의 속성입니다. 이는 다중 검정 분석 과정에서 이루어진 기각 전체의 집합에 적용되는 단일 숫자입니다. 대조적으로 fdr은 국소적인 속성입니다. 이는 개별 가설에 적용됩니다. fdr이 밀도 플롯의 (x)축을 따른 각 지점에 대해 계산되었던 반면, Fdr은 빨간색 선의 왼쪽 면적들에 의존했던 그림 6.17을 상기해 보십시오.\n__\n질문 6.20\n경제학에서의 _총비용(total cost)_과 한계비용(marginal cost) 개념을 확인해 보세요. Fdr 및 fdr과의 유사성을 찾을 수 있나요?\n__\n해결책\n__\n\\(m\\)개의 제품 세트를 생산하는 생산 공정에서 총비용은 관련된 모든 비용의 합입니다. 제품의 평균 비용은 총비용을 \\(m\\)으로 나눈 가상의 수치입니다. 한계비용은 제품 하나를 추가로 만드는 데 드는 비용이며, 종종 평균 비용과는 매우 다릅니다. 예를 들어, 피아노로 베토벤 소나타 한 곡을 연주하는 법을 배우는 데는 초보자에게 상당한 시간이 걸릴 수 있지만, 그것을 한 번 더 연주하는 데는 비교적 적은 추가 노력이 필요합니다: 즉, 한계비용이 고정비용(따라서 총비용)보다 훨씬 적습니다. 한계비용이 평균 비용보다 높은 경우의 예로는 달리기가 있습니다: 운동화를 신고 나가서 10km를 달리는 것은 대부분의 사람들에게 꽤 견딜 만한(아마도 즐거운) 일이겠지만, 추가로 10km를 더 달릴 때마다 불균형적으로 큰 불쾌감이 더해질 수 있습니다.\n\n\n8.9.2 6.10.2 용어\n역사적으로 _다중 검정 보정(multiple testing correction)_과 _조정된 p-값(adjusted p-value)_이라는 용어가 프로세스와 출력물에 사용되어 왔습니다. 허위 발견율의 문맥에서 이러한 용어들은 도움이 되지 않을 뿐만 아니라 혼란을 줄 수 있습니다. 우리는 이 용어들의 사용을 피할 것을 권장합니다. 이 용어들은 우리가 p-값 세트 \\((p_1, …, p_m)\\)로 시작하여 어떤 전형적인 절차를 적용하고, “보정된” 또는 “조정된” p-값 세트 \\((p_1^{}, …, p_m^{})\\)를 얻는다는 것을 암시합니다. 그러나 벤자미니-호크버그 방법의 결과물은 p-값이 아니며, FDR, Fdr, fdr 역시 p-값이 아닙니다. FDR과 Fdr은 집합의 속성임을 기억하십시오. 이를 개별 검정과 연관 짓는 것은 평균 비용과 한계 비용을 혼동하는 것만큼이나 말이 되지 않습니다. Fdr과 fdr 역시 상당한 양의 모델링 가정에 의존합니다. 다음 섹션에서 여러분은 벤자미니-호크버그 방법이 유일한 방법이 아니라는 것과, 다중 검정 절차에 입력되는 가설 및 p-값 세트와 그 출력물 사이의 추정되는 직접적인 대응 관계를 더욱 멀어지게 만드는 중요하고 유용한 확장들이 있다는 것을 보게 될 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "06-chap.html#독립-가설-가중치-부여independent-hypothesis-weighting",
    "href": "06-chap.html#독립-가설-가중치-부여independent-hypothesis-weighting",
    "title": "8  6.1 이 장의 목표",
    "section": "8.10 6.11 독립 가설 가중치 부여(Independent hypothesis weighting)",
    "text": "8.10 6.11 독립 가설 가중치 부여(Independent hypothesis weighting)\n지금까지 살펴본 벤자미니-호크버그 방법과 두 그룹 모델은 가설의 _교환 가능성(exchangeability)_을 암시적으로 가정합니다: 우리가 사용하는 전부는 p-값뿐입니다. 이들 외에 어떠한 추가 정보도 고려하지 않습니다. 이것이 항상 최적인 것은 아니며, 여기서는 이를 개선하는 방법들을 공부할 것입니다.\n예를 들어봅시다. 직관적으로, 더 많은 수의 리드(reads)가 매핑된 유전자의 신호 대 잡음비(signal-to-noise ratio)는 리드가 적은 유전자보다 더 좋을 것이며, 이는 우리 검정의 검정력에 영향을 줄 것입니다. 관측치 전반에 걸친 정규화된 카운트의 평균을 살펴봅시다. DESeq2 패키지에서 이 수치는 baseMean이라 불립니다.\nawde$baseMean[1]__\n\n\n[1] 708.6022\n\n\ncts = counts(aw, normalized = TRUE)[1, ]\ncts __\n\n\nSRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 \n  663.3142   499.9070   740.1528   608.9063   966.3137   748.3722   836.2487 \nSRR1039521 \n  605.6024 \n\n\nmean(cts)__\n\n\n[1] 708.6022\n다음으로 유전자 전체에 대한 이 수치의 히스토그램을 생성하고, p-값에 대해 플롯합니다(그림 6.18 및 6.19).\nggplot(awde, aes(x = asinh(baseMean))) +\n  geom_histogram(bins = 60)__\n\n그림 6.18: baseMean의 히스토그램. 0에 가까운 값부터 약 330,000까지 넓은 동적 범위를 포괄하고 있음을 알 수 있습니다.\nggplot(awde, aes(x = rank(baseMean), y = -log10(pvalue))) +\n  geom_hex(bins = 60) +\n  theme(legend.position = \"none\")__\n\n그림 6.19: baseMean의 순위(rank) 대 p-값의 상용로그 역수(negative logarithm)의 산점도. baseMean 값이 작을 때는 작은 p-값이 나타나지 않습니다. 모든 관측치에 걸친 리드 카운트가 일정 크기 이상인 유전자에 대해서만 차등 발현 검정이 작은 p-값을 내놓을 수 있는 검정력을 갖습니다.\n__\n질문 6.21\n히스토그램에 왜 \\(\\) 변환을 사용했을까요? 변환을 하지 않았을 때, 로그 변환을 했을 때, 이동된 로그 변환(즉, \\((x+)\\))을 했을 때는 어떤 모습일까요?\n__\n질문 6.22\n산점도에서 왜 p-값에 \\(-_{10}\\)을 사용했을까요? baseMean에는 왜 순위 변환을 사용했을까요?\n편의상, 우리는 baseMean을 동일한 크기의 6개 그룹에 해당하는 요인 변수 group으로 이산화(discretize)합니다.\nawde = mutate(awde, stratum = cut(baseMean, include.lowest = TRUE,\n  breaks = signif(quantile(baseMean,probs=seq(0,1,length.out=7)),2)))__\n그림 6.20과 6.21에서 우리는 stratum별로 계층화된 p-값의 히스토그램과 ECDF를 봅니다.\nggplot(awde, aes(x = pvalue)) + facet_wrap( ~ stratum, nrow = 4) +\n  geom_histogram(binwidth = 0.025, boundary = 0)__\n\nFigure 6.20: p-value histograms of the airway data, stratified into equally sized groups defined by increasing value of baseMean.\nggplot(awde, aes(x = pvalue, col = stratum)) +\n  stat_ecdf(geom = \"step\") + theme(legend.position = \"bottom\")__\n\nFigure 6.21: Same data as in Figure 6.20, shown with ECDFs.\n이러한 계층(strata)들에 대해 두 그룹 모델을 따로 적합시킨다면, \\(0\\)와 \\(f{}\\).에 대해 상당히 다른 추정치를 얻게 될 것입니다. 가장 낮게 발현된 유전자들의 경우 DESeq2 검정의 검정력이 낮으며, p-값들은 본질적으로 모두 귀무(null) 성분에서 나옵니다. 평균 발현량이 높아질수록 히스토그램에서 작은 p-값 정점의 높이가 높아지는데, 이는 검정의 검정력이 증가함을 반영합니다.\n이를 다중 검정 처리를 개선하는 데 사용할 수 있을까요? 그것이 가능하다는 것이 밝혀졌습니다. 한 가지 접근 방식은 독립 가설 가중치 부여(independent hypothesis weighting, IHW)입니다 (Ignatiadis et al. 2016; Ignatiadis and Huber 2021)22.\n22 이 외에도 여러 접근 방식이 있습니다. 예를 들어 Korthauer 등 (2019)의 벤치마크 연구나 Ignatiadis와 Huber (2021) 논문의 인용 문헌들을 참조하십시오.\nlibrary(\"IHW\")\nihw_res = ihw(awde$pvalue, awde$baseMean, alpha = 0.1)\nrejections(ihw_res)__\n\n\n[1] 4892\n이를 일반적인 (가중치를 부여하지 않은) 벤자미니-호크버그 방법과 비교해 봅시다:\npadj_BH = p.adjust(awde$pvalue, method = \"BH\")\nsum(padj_BH &lt; 0.1)__\n\n\n[1] 4099\n가설 가중치를 사용하면 더 많은 기각(rejections)을 얻습니다. 이 데이터의 경우 차이가 두드러지긴 하지만 극적이진 않은데, 이는 이미 신호 대 잡음비가 꽤 높기 때문입니다. 처음부터 검정력이 낮은 다른 상황(예: 반복 수가 적거나, 데이터에 노이즈가 더 많거나, 처리 효과가 덜 급격한 경우)에서는 IHW를 사용하는 효과가 더 뚜렷할 수 있습니다.\nihw 함수에 의해 결정된 가중치들을 살펴볼 수 있습니다(그림 6.22).\nplot(ihw_res)__\n\n그림 6.22: ihw 함수에 의해 결정된 가설 가중치. 여기서 함수의 기본 설정은 22개의 계층(strata)을 선택한 반면, 위에서의 수동 탐색(그림 6.20, 6.21)에서는 6개를 사용했습니다. 실제로는 이는 사소한 세부 사항입니다.\n직관적으로 여기서 일어나는 일은, IHW가 baseMean이 높은 가설 계층에 더 많은 가중치를 부여하고, 카운트가 매우 낮은 계층에는 낮은 가중치를 부여하기로 선택한 것입니다. 벤자미니-호크버그 방법은 특정한 제1종 오류 예산을 가지고 있는데, 이를 모든 가설에 똑같이 분배하는 대신, 여기서는 어차피 작은 fdr을 가질 가능성이 거의 없는 계층에서 예산을 가져와 많은 가설이 작은 fdr에서 기각될 수 있는 계층에 “투자”하는 것입니다.\n__\n질문 6.23\n왜 그림 6.22는 하나의 곡선이 아니라 5개의 곡선을 보여주나요?\np-값 외에 추가적인 요약 통계량(우리의 경우 baseMean)에 의한 계층화 가능성은 많은 다중 검정 상황에서 존재합니다. 비공식적으로 말하자면, 우리는 그러한 이른바 _공변량(covariate)_이 다음과 같아야 합니다:\n\n귀무 가설하에서 우리의 p-값과 통계적으로 독립적이어야 하지만,\n두 그룹 모델에서 사전 확률 \\(0\\) 및/또는 검정력(대립 가설 밀도 \\(f{}\\)의 형태)에 대한 정보를 제공해야 합니다.\n\n이러한 요구 사항은 그림 6.18—6.21과 같은 진단 플롯을 통해 평가할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "06-chap.html#이-장의-요약",
    "href": "06-chap.html#이-장의-요약",
    "title": "8  6.1 이 장의 목표",
    "section": "8.11 6.12 이 장의 요약",
    "text": "8.11 6.12 이 장의 요약\n우리는 단일 가설 검정 이면의 개념들을 탐구한 다음 _다중 검정_으로 넘어갔습니다. 우리는 수많은 검정으로부터 나온 결과의 전체 분포를 고려할 수 있게 되면 단일 검정에서 얻은 단일 p-값을 해석할 때의 몇 가지 한계들을 어떻게 극복할 수 있는지 보았습니다. 또한 p-값 외에도 우리 데이터에 종종 추가적인 요약 통계량이 있다는 것도 보았습니다. 우리는 이를 정보성 공변량(informative covariates)이라 불렀으며, 이를 사용하여 p-값에 가중치를 부여하고 전반적으로 더 많거나 더 나은 발견을 얻는 방법을 살펴보았습니다.\n다중 검정 시나리오에서의 가설 검정 사용은 단일 검정 사례와는 상당히 다릅니다: 후자의 경우, 가설 검정은 말 그대로 (이상적으로는 사전에 지정된 가설과 데이터 분석 계획을 가진) 길고 비용이 많이 드는 데이터 획득 캠페인의 최종 결과이자 정점일 수 있습니다. 다중 검정의 경우, 그 결과는 종종 중간 단계일 뿐입니다: 방대한 초기 집합을 스크리닝하여 선택된, 가장 가치 있는 가설들의 하위 집합입니다. 이 하위 집합은 이후 더 세심한 분석을 통해 후속 연구가 진행됩니다.\n우리는 허위 발견율(FDR)의 개념을 보았습니다. 이것이 선택된 가설들의 하위 집합에 대한 평균적인 속성이라는 점을 명심하는 것이 중요합니다. 다른 평균들과 마찬가지로, 개별 가설에 대해서는 아무것도 말해주지 않습니다. 그리고 개별 가설에 실제로 적용되는 국소 허위 발견율(fdr)이라는 개념이 있습니다. 그러나 두 그룹 모델이 보여주었듯이, 국소 허위 발견율은 p-값과는 상당히 무관합니다. p-값에 대한 많은 혼란과 좌절은 사람들이 fdr이 담당하는 목적을 위해 p-값을 사용하고 싶어 한다는 사실에서 비롯되는 것 같습니다. 응용 과학의 아주 많은 부분이 국소 허위 발견율이 아닌 p-값에 집중하고 있다는 것은 아마도 역사적 일탈일지 모릅니다. 반면에 실무적인 이유도 있는데, p-값은 즉시 계산되는 반면, fdr은 강력한 모델링 가정 없이는 데이터로부터 추정하거나 제어하기 어렵기 때문입니다.\n우리는 진단 플롯의 중요성을 확인했습니다. 특히 다중 검정 분석을 마주할 때는 항상 p-값 히스토그램을 살펴보아야 합니다.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "06-chap.html#더-읽을거리",
    "href": "06-chap.html#더-읽을거리",
    "title": "8  6.1 이 장의 목표",
    "section": "8.12 6.13 더 읽을거리",
    "text": "8.12 6.13 더 읽을거리\n\n다중 검정에 대한 포괄적인 교과서적 처리는 Efron (2010)에 의해 제공됩니다.\n임상 시험에서의 결과 전환(Outcome switching): http://compare-trials.org\n가설 가중치에 대해서는 IHW 비네트, IHW 논문 (Ignatiadis et al. 2016) 및 그 안의 참고 문헌들을 참조하십시오.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "06-chap.html#연습-문제",
    "href": "06-chap.html#연습-문제",
    "title": "8  6.1 이 장의 목표",
    "section": "8.13 6.14 연습 문제",
    "text": "8.13 6.14 연습 문제\n__\n연습 문제 6.1\n여러분의 과학적 전문 분야에서 다중 검정에 의존하는 응용 사례를 하나 찾아보십시오. 전형적인 데이터 세트를 찾아 p-값 히스토그램을 그려보세요. 가설들이 모두 교환 가능한가요, 아니면 하나 이상의 정보성 공변량이 있나요? 계층화된 히스토그램을 그려보세요.\n__\n연습 문제\n수리 통계학자들은 대립 가설에 비해 왜 검정의 귀무 가설에 그렇게 많이 집중할까요?\n__\n연습 문제 6.2\n우리는 어떻게 귀무 가설이 참임을 증명할 수 있을까요? 아니면 대립 가설이 참임을요?\n__\n연습 문제 6.3\n6.5절 끝부분의 데이터 복제보다 덜 극단적인 상관된 검정 통계량의 예를 만들어 보세요. 오직 참인 귀무 가설들로만 데이터를 시뮬레이션하고, 어떤 연속형 제어 매개변수의 함수로서 데이터가 완전히 독립적인 반복(열)에서 고도로 상관된 상태로 변하도록 만드십시오. 이 제어 매개변수의 함수로서 (예를 들어 p-값 히스토그램을 통해) 제1종 오류 제어를 확인해 보세요.\n__\n연습 문제 6.4\n발표된 문헌 중에서 p-값 해킹, 결과 전환, HARKing이 작용한 것처럼 보이는 사례를 하나 찾아보십시오.\n__\n연습 문제 6.5\nFDR은 기대값(expectation value)입니다. 즉, 절차의 평균적인 거동을 제어하고 싶을 때 사용됩니다. 최악의 경우(worst case)를 제어하기 위한 방법들이 있을까요?\n__\n연습 문제 6.6\n벤자미니-호크버그 알고리즘의 메모리 및 시간 복잡도는 얼마인가요? IHW 방법은 어떤가요? 검정의 수 \\(m\\)의 함수로서 다항 함수를 적합시킬 수 있나요? 힌트: 가설 검정의 수를 늘려가며 데이터를 시뮬레이션하고, pryr::object_size나 동명의 패키지에 있는 microbenchmark와 같은 함수를 사용하여 시간 및 메모리 소비를 측정하고, 이들을 이중 로그 플롯(double-logarithmic plot)에서 \\(m\\)에 대해 플롯해 보세요.\nAltman, Naomi, and Martin Krzywinski. 2017. “Points of Significance: Interpreting p Values.” Nature Methods 14 (3): 213–14. https://doi.org/10.1038/nmeth.4210.\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society B 57: 289–300.\nEfron, Bradley. 2010. Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction. Cambridge University Press.\nHead, Megan L, Luke Holman, Rob Lanfear, Andrew T Kahn, and Michael D Jennions. 2015. “The Extent and Consequences of p-Hacking in Science.” PLoS Biology 13 (3): e1002106.\nIgnatiadis, Nikolaos, and Wolfgang Huber. 2021. “Covariate Powered Cross- Weighted Multiple Testing.” Journal of the Royal Statistical Society: Series B 83: 720–51. https://doi.org/10.1111/rssb.12411.\nIgnatiadis, Nikolaos, Bernd Klaus, Judith Zaugg, and Wolfgang Huber. 2016. “Data-Driven Hypothesis Weighting Increases Detection Power in Genome-Scale Multiple Testing.” Nature Methods 13: 577–80.\nKorthauer, K., P. K. Kimes, C. Duvallet, A. Reyes, A. Subramanian, M. Teng, C. Shukla, E. J. Alm, and S. C. Hicks. 2019. “A practical guide to methods controlling false discoveries in computational biology.” Genome Biology 20 (1): 118.\nSchweder, T., and E. Spjøtvoll. 1982. “Plots of P-values to Evaluate Many Tests Simultaneously.” Biometrika 69: 493–502. https://doi.org/10.1093/biomet/69.3.493.\nStorey, John D. 2003. “The Positive False Discovery Rate: A Bayesian Interpretation and the q-Value.” The Annals of Statistics 31 (6). https://doi.org/10.1214/aos/1074290335.\nWasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA’s Statement on p-Values: Context, Process, and Purpose.” The American Statistician.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>6.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "07-chap.html",
    "href": "07-chap.html",
    "title": "9  7.1 이 장의 목표",
    "section": "",
    "text": "9.1 7.2 데이터란 무엇인가? 행렬과 그 동기\n많은 데이터 세트는 동일한 대상(환자, 샘플 또는 유기체)에 대해 측정된 여러 변수로 구성됩니다. 예를 들어, 우리는 천 명의 환자에 대해 키, 몸무게, 나이와 같은 생체 정보뿐만 아니라 혈압, 혈당, 심박수와 같은 임상 변수 및 유전 데이터를 가질 수 있습니다. 다변량 분석(multivariate analysis)의 존재 이유(raison d’être)는 측정된 서로 다른 변수들 간의 연결 또는 연관성을 조사하는 것입니다. 대개 데이터는 각 대상에 대해 하나의 행, 각 변수가 하나의 열을 가진 표 형식의 데이터 구조로 보고됩니다. 이하에서는 각 변수가 수치형인 특수한 경우에 초점을 맞출 것이며, 따라서 R에서 데이터 구조를 _행렬(matrix)_로 나타낼 수 있습니다.\n행렬의 열들이 모두 서로 독립적(무관)이라면, 우리는 단순히 각 열을 별도로 공부하고 표준 “단변량(univariate)” 통계 처리를 하나씩 수행할 수 있습니다. 이들을 행렬로 공부해서 얻는 이점은 없을 것입니다.\n더 자주, 패턴과 의존성이 존재할 것입니다. 예를 들어 세포 생물학에서 우리는 증식률(proliferation rate)이 많은 유전자의 발현에 동시에 영향을 미친다는 것을 알고 있습니다. 환자 유래 세포의 많은 샘플(행)에 대해 25,000개의 유전자(열) 발현을 공부하면서, 우리는 많은 유전자가 함께 작용하여 양(+)의 상관관계를 갖거나 음(-)의 상관관계를 갖는다는 것을 알아차립니다. 각 유전자를 별도로만 공부한다면 우리는 많은 중요한 정보를 놓칠 것입니다. 유전자들 사이의 중요한 연결은 우리가 데이터를 전체적으로 고려할 때만 감지 가능합니다: 각 행은 동일한 관찰 단위에 대해 이루어진 많은 측정값들을 나타냅니다. 그러나 한꺼번에 고려해야 할 25,000차원의 변동을 갖는 것은 벅찬 일입니다. 우리는 너무 많은 정보를 잃지 않으면서 우리 데이터를 더 적은 수의 가장 중요한 차원1으로 줄이는 방법을 보여줄 것입니다.\n1 우리는 아래에서 이러한 차원 축소(dimension reduction)의 아이디어를 훨씬 더 자세히 설명할 것입니다. 당분간은 우리가 4차원 세계에 살고 있다는 것을 기억합시다.\n이 장에서는 고처리량 실험에서 마주치는 다변량 데이터 행렬의 많은 예시뿐만 아니라, 여러분의 직관을 높여줄 좀 더 기초적인 예시들을 제시합니다. 우리는 이 장에서 차원 축소 방법인 주성분 분석(Principal Component Analysis), 약어로 PCA에 초점을 맞출 것입니다. 우리는 알고리즘에 대한 기하학적 설명뿐만 아니라 PCA 분석의 출력을 해석하는 데 도움이 되는 시각화 자료들을 제공할 것입니다.\n이 장에서 우리는 다음을 수행할 것입니다:\n먼저, 측정값 표를 나타내는 데 사용되는 직사각형 행렬(matrices)의 예시들을 살펴보겠습니다. 각 행렬에서 행과 열은 특정 개체를 나타냅니다.\n거북이(Turtles): 기본 원리를 이해하는 데 도움이 될 간단한 데이터 세트는 거북이(painted turtles)에 대한 세 가지 차원의 생체 측정값 행렬입니다 (Jolicoeur and Mosimann 1960).\n마지막 세 열은 길이 측정값(밀리미터 단위)인 반면, 첫 번째 열은 각 동물의 성별을 알려주는 요인 변수입니다.\n운동선수(Athletes): 이 행렬은 스포츠 세계의 흥미로운 예시입니다. 10종 경기(decathlon)의 10개 종목에 대한 33명 선수의 성적을 보고합니다: m100, m400 및 m1500은 각각 100미터, 400미터, 1500미터 달리기 시간(초)입니다. m110은 110미터 허들 완주 시간입니다. pole은 장대높이뛰기 높이, high와 long은 각각 높이뛰기와 멀리뛰기 결과이며 모두 미터 단위입니다. weight, disc, javel은 선수들이 던진 포환, 원반, 창의 거리(미터)입니다. 처음 세 명의 선수에 대한 변수들은 다음과 같습니다:\nCell Types: Holmes et al. (2005) studied gene expression profiles of sorted T-cell populations from different subjects. The columns are a subset of gene expression measurements, they correspond to 156 genes that show differential expression between cell types.\nBacterial Species Abundances: Matrices of counts are used in microbial ecology studies (as we saw in Chapter 4). Here the columns represent different species (or operational taxonomic units, OTUs) of bacteria, which are identified by numerical tags. The rows are labeled according to the samples in which they were measured, and the (integer) numbers represent the number of times of each of the OTUs was observed in each of the samples.\nNotice the propensity of the matrix entries to be zero; we call such data sparse.\nmRNA reads: RNA-Seq transcriptome data report the number of sequence reads matching each gene2 in each of several biological samples. We will study this type of data in detail in Chapter 8\n2 Or sub-gene structures, such as exons.\nIt is customary in the RNA-Seq field—and so it is for the airway data above—to report the genes in the rows and the samples in the columns. Compared to the other matrices we look at here, this is transposed : rows and columns are swapped. Such different conventions easily lead to errors, so they are worthwhile paying attention to3. Proteomic profiles: Here, the columns are aligned mass spectroscopy peaks or molecules identified through their \\(m/z\\)-ratios; the entries in the matrix are the measured intensities4.\n3 The Bioconductor project tries to help users and developers to avoid such ambiguities by defining data containers in which such conventions are explicitly fixed. In Chapter 8, we will see the example of the SummarizedExperiment class.\n4 More details can be found, e.g., on Wikipedia.\nIn many of the matrices we have seen here, important information about the samples (subjects) and the measured features is stored in the row or column names, often through some ad hoc string concatenation. This is not the best place to store all available information, and quickly becomes limiting and error-prone. A much better approach is the Bioconductor SummarizedExperiment class.\n__\nTask\nWhen a peak was not detected for a particular \\(m/z\\) score in the mass spectrometry run, a zero was recorded in metab. Similarly, zeros in GPOTUs or in the airway object occur when there were no matching sequence reads detected. Tabulate the frequencies of zeros in these data matrices.\n__\nQuestion 7.1",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "07-chap.html#데이터란-무엇인가-행렬과-그-동기",
    "href": "07-chap.html#데이터란-무엇인가-행렬과-그-동기",
    "title": "9  7.1 이 장의 목표",
    "section": "",
    "text": "turtles = read.table(\"../data/PaintedTurtles.txt\", header = TRUE)\nturtles[1:4, ]__\n\n\n  sex length width height\n1   f     98    81     38\n2   f    103    84     38\n3   f    103    86     42\n4   f    105    86     40\n\n\ndata(\"olympic\", package = \"ade4\")\nathletes = setNames(olympic$tab, \n  c(\"m100\", \"long\", \"weight\", \"high\", \"m400\", \"m110\", \"disc\", pole, \"javel\", \"m1500\"))\nathletes[1:3, ]__\n\n\n   m100 long weight high  m400  m110  disc pole javel  m1500\n1 11.25 7.43  15.48 2.27 48.90 15.13 49.28  4.7 61.32 268.95\n2 10.87 7.45  14.97 1.97 47.71 14.46 44.36  5.1 61.76 273.02\n3 11.18 7.44  14.20 1.97 48.29 14.81 43.66  5.2 64.16 263.20\n\nload(\"../data/Msig3transp.RData\")\nround(Msig3transp,2)[1:5, 1:6]__\n\n\n             X3968 X14831 X13492 X5108 X16348  X585\nHEA26_EFFE_1 -2.61  -1.19  -0.06 -0.15   0.52 -0.02\nHEA26_MEM_1  -2.26  -0.47   0.28  0.54  -0.37  0.11\nHEA26_NAI_1  -0.27   0.82   0.81  0.72  -0.90  0.75\nMEL36_EFFE_1 -2.24  -1.08  -0.24 -0.18   0.64  0.01\nMEL36_MEM_1  -2.68  -0.15   0.25  0.95  -0.20  0.17\n\ndata(\"GlobalPatterns\", package = \"phyloseq\")\nGPOTUs = as.matrix(t(phyloseq::otu_table(GlobalPatterns)))\nGPOTUs[1:4, 6:13]__\n\n\nOTU Table:          [4 taxa and 8 samples]\n                     taxa are rows\n        246140 143239 244960 255340 144887 141782 215972 31759\nCL3          0      7      0    153      3      9      0     0\nCC1          0      1      0    194      5     35      3     1\nSV1          0      0      0      0      0      0      0     0\nM31Fcsw      0      0      0      0      0      0      0     0\n\n\n\nNotice the propensity of the matrix entries to be zero; we call such data sparse.\n\n\n\n\n\nlibrary(\"SummarizedExperiment\")\ndata(\"airway\", package = \"airway\")\nassay(airway)[1:3, 1:4]__\n\n\n                SRR1039508 SRR1039509 SRR1039512 SRR1039513\nENSG00000000003        679        448        873        408\nENSG00000000005          0          0          0          0\nENSG00000000419        467        515        621        365\n\n\n\nmetab = t(as.matrix(read.csv(\"../data/metabolites.csv\", row.names = 1)))\nmetab[1:4, 1:4]__\n\n\n         146.0985388 148.7053275 310.1505057 132.4512963\nKOGCHUM1    29932.36    17055.70     1132.82    785.5129\nKOGCHUM2    94067.61    74631.69    28240.85   5232.0499\nKOGCHUM3   146411.33   147788.71    64950.49  10283.0037\nWTGCHUM1   229912.57   384932.56   220730.39  26115.2007\n\n\n\nIn many of the matrices we have seen here, important information about the samples (subjects) and the measured features is stored in the row or column names, often through some ad hoc string concatenation. This is not the best place to store all available information, and quickly becomes limiting and error-prone. A much better approach is the Bioconductor SummarizedExperiment class.\n\n\n\n\n\n\n\n\n\nWhat are the columns of these data matrices usually called?\nIn each of these examples, what are the rows of the matrix?\nWhat does a cell in a matrix represent?\nIf the data matrix is called athletes and you want to see the value of the third variable for the fifth athlete, what do you type into R?\n\n\n9.1.1 7.2.1 Low-dimensional data summaries and preparation\n\nFigure 7.1: xkcd: What do we mean by low-dimensional? We live in 3 dimensions, or 4 if you count time, a plane has 2 dimensions, a line has one dimension. A point is said to be zero-dimensional. For the amusing novel referenced in the cartoon see Abbott (1884).\nIf we are studying only one variable, i.e., just the third column of the turtles matrix5, we say we are looking at one-dimensional data. Such a vector, say all the turtle weights, can be visualized by plots such as those that we saw in Section 3.6, e.g., a histogram. If we compute a one number summary, say mean or median, we have made a zero- dimensional summary of our one-dimensional data. This is already an example of dimension reduction.\n5 The third column of a matrix \\(X\\) is denoted mathematically by \\({x}_{}\\) or accessed in R using X[, 3].\nIn Chapter 3 we studied two-dimensional scatterplots. We saw that if there are too many observations, it can be beneficial to group the data into (hexagonal) bins: these are two-dimensional histograms. When considering two variables (\\(x\\) and \\(y\\)) measured together on a set of observations, the correlation coefficient measures how the variables co- vary. This is a single number summary of two-dimensional data. Its formula involves the summaries \\({x}\\) and \\({y}\\):\n\\[ = { } \\]\nIn R, we use the cor function to calculate its value. Applied to a matrix this function computes all the two way correlations between continuous variables. In Chapter 9 we will see how to analyse multivariate categorical data.\n__\nQuestion 7.2\nCompute the matrix of all correlations between the measurements from the turtles data. What do you notice ?\n__\nSolution\n__\nWe take out the categorical variable and compute the matrix.\ncor(turtles[, -1])__\n\n\n          length     width    height\nlength 1.0000000 0.9783116 0.9646946\nwidth  0.9783116 1.0000000 0.9605705\nheight 0.9646946 0.9605705 1.0000000\nWe see that this square matrix is symmetric and the values are all close to 1. The diagonal values are always 1.\nIt is always beneficial to start a multidimensional analysis by checking these simple one-dimensional and two-dimensional summary statistics using visual displays such as those we look at in the next two questions.\n__\nQuestion 7.3\n\nProduce all pairwise scatterplots, as well as the one-dimensional histograms on the diagonal, for the turtles data. Use the package GGally.\nGuess the underlying or “true dimension” of these data?\n\n__\nSolution\n__\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"GGally\")\nggpairs(turtles[, -1], axisLabels = \"none\")__\n\nFigure 7.2: All pairs of bivariate scatterplots for the three biometric 그림 7.2를 보면, 세 변수 모두 높은 상관관계를 가지고 있으며 대부분 거북이의 _크기_로 해석될 수 있는 동일한 “기저” 변수를 반영하는 것으로 보입니다.\n__\n질문 7.4\nathletes 데이터의 변수들 사이의 모든 쌍별 상관관계를 계산하고 그 행렬을 히트맵으로 표시해 보세요. 무엇을 알 수 있나요?\n__\n해결책\n__\nlibrary(\"pheatmap\")\npheatmap(cor(athletes), cell.width = 10, cell.height = 10)__\n\n그림 7.3: athletes 데이터의 변수들 사이의 상관관계 히트맵. 높은 수치는 빨강-주황색으로 색상 코딩되어 있습니다. 계층적 군집화는 서로 관련된 종목들의 그룹화를 보여줍니다.\n그림 7.3은 10개의 변수가 달리기, 던지기, 점프라는 그룹으로 어떻게 군집화되는지 보여줍니다.\n\n\n9.1.2 7.2.2 데이터 전처리\n많은 경우, 서로 다른 변수들은 서로 다른 단위로 측정되므로, 서로 다른 기준선과 서로 다른 스케일(scales)6을 가집니다. 이들은 원래의 형태로는 직접적으로 비교할 수 없습니다.\n6 스케일의 일반적인 척도는 범위와 표준 편차입니다. 예를 들어, 110미터 허들 시간은 표준 편차 0.51과 함께 14.18에서 16.2 사이인 반면, 1500미터 완주 시간은 표준 편차 13.66과 함께 256.64에서 303.17 사이입니다; 이는 10배 이상 더 큽니다. 더욱이 athletes 데이터는 서로 다른 단위(초, 미터)의 측정값도 포함하고 있는데, 이들의 선택은 임의적입니다(길이는 센티미터나 피트로 기록될 수도 있고, 시간은 밀리초로 기록될 수도 있습니다).\n따라서 PCA 및 다른 많은 방법들의 경우, 비교를 의미 있게 만들기 위해 수치들을 어떤 공통적인 스케일로 변환해야 합니다. 중앙화(Centering)는 평균을 빼는 것을 의미하며, 중앙화된 데이터의 평균은 원점에 위치하게 됩니다. 스케일링(Scaling) 또는 표준화(Standardizing)는 표준 편차로 나누는 것을 의미하며, 새로운 표준 편차는 \\(1\\)이 됩니다. 사실 우리는 상관 계수를 계산할 때(식 7.1) 이미 이러한 연산들을 접했습니다: 상관 계수는 단순히 중앙화되고 스케일링된 변수들의 벡터 곱입니다. 이러한 연산을 수행하기 위해 R에는 scale 함수가 있으며, 행렬이나 데이터 프레임이 주어졌을 때의 기본 거동은 모든 열의 평균을 0으로, 표준 편차를 1로 만드는 것입니다.\n__\n질문 7.5\n\nturtle 데이터의 평균과 표준 편차를 계산한 다음, scale 함수를 사용하여 연속형 변수들을 중앙화하고 표준화하세요. 이를 scaledTurtles라고 부르고, scaledTurtles의 평균과 표준 편차의 새로운 값을 확인해 보세요.\n거북이 데이터의 스케일링 및 중앙화된 너비(width)와 높이(height) 변수의 산점도를 만들고 점들에 성별에 따른 색상을 입히세요.\n\n__\n해결책\n__\napply(turtles[,-1], 2, sd)__\n\n\n   length     width    height \n20.481602 12.675838  8.392837 \n\n\napply(turtles[,-1], 2, mean)__\n\n\n   length     width    height \n124.68750  95.43750  46.33333 \n\n\nscaledTurtles = scale(turtles[, -1])\napply(scaledTurtles, 2, mean)__\n\n\n       length         width        height \n-1.432050e-18  1.940383e-17 -2.870967e-16 \n\n\napply(scaledTurtles, 2, sd)__\n\n\nlength  width height \n     1      1      1 \n\n\ndata.frame(scaledTurtles, sex = turtles[, 1]) %&gt;%\n  ggplot(aes(x = width, y = height, group = sex)) +\n    geom_point(aes(color = sex)) + coord_fixed()__\n\n그림 7.4: 너비(width)와 높이(height) 변수에 의해 정의된 평면에 투영된 거북이 데이터: 각 점은 성별에 따라 색상이 입혀져 있습니다.\n우리는 이미 4장과 5장에서 log와 asinh 함수를 사용한 다른 데이터 변환 선택지들을 접했습니다. 이러한 변환의 목적은 (대개) 분산 안정화(variance stabilization)입니다. 즉, 동적 범위의 서로 다른 부분에서 _동일한 변수_의 반복 측정값 분산을 더 비슷하게 만드는 것입니다. 이와 대조적으로 위에서 설명한 표준화 변환은 _서로 다른 변수_의 스케일(평균과 표준 편차로 측정됨)을 동일하게 만드는 것을 목표로 합니다.\n때로는 변수들이 진정으로 중요도가 다르기 때문에 서로 다른 스케일로 남겨두는 것이 더 나을 수도 있습니다. 만약 원래의 스케일이 유의미하다면, 데이터를 있는 그대로 두어야 합니다. 다른 경우에, 변수들은 사전적으로 알려진 서로 다른 정밀도를 가집니다. 9장에서 우리는 그러한 변수들에 가중치를 부여하는 여러 방법들을 살펴볼 것입니다.\n데이터를 전처리한 후, 우리는 차원 축소를 통한 데이터 _단순화_를 수행할 준비가 되었습니다.\n\n관련 장들이 포함된 유용한 책으로는 입문용 설명이 담긴 Flury (1997)와 상세한 수학적 접근 방식이 담긴 Mardia, Kent, Bibby (1979)가 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "07-chap.html#차원-축소dimension-reduction",
    "href": "07-chap.html#차원-축소dimension-reduction",
    "title": "9  7.1 이 장의 목표",
    "section": "9.2 7.3 차원 축소(Dimension reduction)",
    "text": "9.2 7.3 차원 축소(Dimension reduction)\n우리는 여러 가지 서로 다른 관점에서 차원 축소를 설명할 것입니다. 이는 1901년에 Karl Pearson (Pearson 1901)에 의해 두 변수 산점도를 단일 좌표로 줄이는 방법으로 발명되었습니다. 1930년대에는 통계학자들에 의해 동일한 대상에 대해 수행된 일련의 심리 테스트 결과를 요약하는 데 사용되었습니다 (Hotelling 1933); 이로써 한꺼번에 테스트된 많은 변수들을 요약하는 종합 점수(overall scores)를 제공하게 되었습니다.\n\n\n\nPrincipal과 principle은 서로 다른 뜻을 가진 두 개의 다른 단어입니다. 따라서 이들을 혼동하지 마세요. PCA의 경우, 그것은 항상 principal입니다.\n\n\n_Principal_과 _principle_은 서로 다른 뜻을 가진 두 개의 다른 단어입니다. 따라서 이들을 혼동하지 마세요. PCA의 경우, 그것은 항상 _principal_입니다.\n이러한 주요(principal) 점수라는 아이디어가 주성분 분석(Principal Component Analysis, 약어로 PCA)이라는 이름에 영감을 주었습니다. PCA는 군집화에서와 마찬가지로 모든 변수를 동일한 상태(status)를 가진 것으로 취급하기 때문에 비지도 학습(unsupervised learning) 기법이라 불립니다. 우리는 한 특정한 변수의 값을 다른 변수들로부터 예측하거나 설명하려는 것이 아니라, 오히려 모든 변수들에 대한 기저 구조의 수학적 모델을 찾으려는 것입니다. PCA는 기본적으로 변수들 사이의 관계와 관측치들 사이의 관계를 유용한 방식으로 보여주는 맵(maps)을 생성하는 탐색적 기법입니다.\n우리는 먼저 이 다변량 분석이 데이터에 무엇을 하는지에 대한 맛보기를 제공하겠습니다. 선형 대수학을 통한 이러한 방법들의 우아한 수학적 공식화가 존재하지만, 여기서는 그 사용을 최소화하고 시각화와 데이터 예시에 집중할 것입니다.\n우리는 고차원 공간의 점들을 낮은 차원으로 투영하는 기하학적 투영(projections)을 사용합니다. 그림 7.5는 벡터 \\({v}\\)에 의해 생성된 선 위로 점 \\(A\\)를 투영하는 것을 보여줍니다.\n\n그림 7.5: 점 \\(A\\)가 벡터 \\(v\\)에 의해 생성된 빨간색 선 위로 투영됩니다. 점선 투영선은 빨간색 선에 수직(또는 직교(orthogonal))합니다. 투영선과 빨간색 선의 교차점을 점 \\(A\\)의 벡터 \\(v\\)에 의해 생성된 빨간색 선 위로의 직교 투영이라고 부릅니다.\nPCA는 선형(linear) 기법으로, 변수들 사이의 선형 관계를 찾고 원래 변수들의 선형 함수인 새로운 변수들을 사용한다는 것을 의미합니다 (\\(f(ax+by)=af(x)+b(y)\\)). 선형성 제약 조건은 계산을 특히 쉽게 만듭니다. 9장에서 우리는 비선형 기법들을 살펴볼 것입니다.\n\n9.2.1 7.3.1 저차원 투영\n여기에 athletes 데이터를 사용하여 2차원 데이터를 선 위로 투영하는 한 가지 방법을 보여줍니다. 아래 코드는 그림 7.6을 생성하는 데 사용된 전처리 및 플로팅 단계를 제공합니다:\nathletes = data.frame(scale(athletes))\nath_gg = ggplot(athletes, aes(x = weight, y = disc)) +\n  geom_point(size = 2, shape = 21)\nath_gg + geom_point(aes(y = 0), colour = \"red\") +\n  geom_segment(aes(xend = weight, yend = 0), linetype = \"dashed\")__\n\n그림 7.6: 가로 (x)축((y=0)으로 정의됨)으로의 투영을 빨간색으로 보여주는 두 변수의 산점도이며 투영선은 점선으로 나타납니다.\n__\n태스크\n\n그림 7.6에서 빨간색 점들의 분산을 계산해 보세요.\n(y)축으로의 투영선과 투영된 점들을 보여주는 플롯을 만들어 보세요.\n수직 (y)축으로 투영된 점들의 분산을 계산해 보세요.\n\n\n\n9.2.2 7.3.2 2차원 데이터를 어떻게 선으로 요약할까요?\n일반적으로 우리가 2차원(평면)에서 1차원(선)으로 투영할 때 점들에 대한 정보를 잃게 됩니다. 그림 7.6에서 weight 변수에 대해 했던 것처럼 원래의 좌표만 사용한다면, 우리는 disc 변수에 대한 모든 정보를 잃게 됩니다. 우리의 목표는 두 변수 모두에 대해 가능한 한 많은 정보를 유지하는 것입니다. 사실 점 구름을 선 위로 투영하는 방법은 여러 가지가 있습니다. 하나는 회귀선(regression lines)이라 알려진 것을 사용하는 것입니다. R에서 이러한 선들이 어떻게 구축되는지 살펴보겠습니다.\n\n9.2.2.1 한 변수를 다른 변수에 대해 회귀 분석하기\n선형 회귀(linear regression)를 보셨다면, 산점도를 요약하는 선을 계산하는 방법을 이미 알고 계실 것입니다. 선형 회귀는 한 방향, 즉 반응 변수 방향의 잔차 제곱합을 최소화하는 것을 우선시하는 지도(supervised) 방법입니다.\n\n\n9.2.2.2 weight에 대한 disc 변수의 회귀 분석.\n그림 7.7에서는 회귀선을 찾기 위해 lm(선형 모델) 함수를 사용합니다. 그 기울기와 절편은 결과 객체 reg1의 coefficients 슬롯에 있는 값들에 의해 주어집니다.\nreg1 = lm(disc ~ weight, data = athletes)\na1 = reg1$coefficients[1] # 절편\nb1 = reg1$coefficients[2] # 기울기\npline1 = ath_gg + geom_abline(intercept = a1, slope = b1,\n    col = \"blue\", linewidth = 1.5)\npline1 + geom_segment(aes(xend = weight, yend = reg1$fitted),\n    colour = \"red\", arrow = arrow(length = unit(0.15, \"cm\")))__\n\n그림 7.7: 파란색 선은 (빨간색으로 표시된) 수직 잔차의 제곱합을 최소화합니다.\n\n\n9.2.2.3 discus에 대한 weight의 회귀 분석.\n그림 7.8은 두 변수의 역할을 바꾸었을 때 생성되는 선을 보여줍니다. weight가 반응 변수가 됩니다.\nreg2 = lm(weight ~ disc, data = athletes)\na2 = reg2$coefficients[1] # 절편\nb2 = reg2$coefficients[2] # 기울기\npline2 = ath_gg + geom_abline(intercept = -a2/b2, slope = 1/b2,\n    col = \"darkgreen\", linewidth = 1.5)\npline2 + geom_segment(aes(xend=reg2$fitted, yend=disc),\n    colour = \"orange\", arrow = arrow(length = unit(0.15, \"cm\")))__\n\n그림 7.8: 녹색 선은 (주황색으로 표시된) 수평 잔차의 제곱합을 최소화합니다.\n그림 7.7과 7.8의 각 회귀선은 disc와 weight 사이의 대략적인 선형 관계를 제공합니다. 그러나 그 관계는 우리가 어느 변수를 예측 변수로 선택하느냐에 따라 달라집니다…. which the response.\n__\nQuestion 7.6\nHow large is the variance of the projected points that lie on the blue regression line of Figure 7.7? Compare this to the variance of the data when projected on the original axes, weight and disc.\n__\nSolution\n__\nPythagoras’ theorem tells us that the squared length of the hypotenuse of a right-angled triangle is equal to the sum of the squared lengths of the other two sides, which we apply as follows:\nvar(athletes$weight) + var(reg1$fitted)__\n\n\n[1] 1.650204\nThe variances of the points along the original axes weight and disc are 1, since we scaled the variables.\n\n\n9.2.2.4 A line that minimizes distances in both directions\nFigure 7.9 shows the line chosen to minimize the sum of squares of the orthogonal (perpendicular) projections of data points onto it; we call this the principal component line. All our three ways of fitting a line (Figures 7.7–7.9) together in one plot are shown in Figure 7.10.\nxy = cbind(athletes$disc, athletes$weight)\nsvda = svd(xy)\npc = xy %*% svda$v[, 1] %*% t(svda$v[, 1])\nbp = svda$v[2, 1] / svda$v[1, 1]\nap = mean(pc[, 2]) - bp * mean(pc[, 1])\nath_gg + geom_segment(xend = pc[, 1], yend = pc[, 2]) +\n  geom_abline(intercept = ap, slope = bp, col = \"purple\", linewidth = 1.5)__\n\nFigure 7.9: The purple principal component line minimizes the sums of squares of the orthogonal projections.\n\nFigure 7.10: The blue line minimizes the sum of squares of the vertical residuals, the green line minimizes the horizontal residuals, the purple line, called the principal component , minimizes the orthogonal projections. Notice the ordering of the slopes of the three lines.\n__\nQuestion 7.7\n\nWhat is particular about the slope of the purple line?\nRedo the plots on the original (unscaled) variables. What happens?\n\n__\nSolution\n__\nThe lines computed here depend on the choice of units. Because we have made the standard deviations equal to one for both variables, the PCA line is the diagonal that cuts exactly in the middle of both regression lines. Since the data were centered by subtracting their means, the line passes through the origin \\((0,0)\\).\n__\nQuestion 7.8\nCompute the variance of the points on the purple line.\n__\nSolution\n__\nWe have computed the coordinates of the points when we made the plot, these are in the pc vector:\napply(pc, 2, var)__\n\n\n[1] 0.9031761 0.9031761\n\n\nsum(apply(pc, 2, var))__\n\n\n[1] 1.806352\n우리는 이 축을 따른 분산이 질문 7.6에서 계산한 다른 분산들보다 크다는 것을 알 수 있습니다.\n피타고라스 정리는 여기서 두 가지 흥미로운 사실을 알려줍니다:\n\n수평 및 수직 방향 모두에서 최소화하고 있다면, 사실 우리는 각 점으로부터 선으로의 직교 투영을 최소화하고 있는 것입니다.\n점들의 전체 가변성은 점들을 무게 중심(데이터가 중앙화되어 있다면 원점(0,0))으로 투영한 제곱합으로 측정됩니다. 이를 점 구름의 전체 분산 또는 관성(inertia)이라고 합니다. 이 관성은 선 위로의 투영 제곱합과 그 선을 따른 분산의 합으로 분해될 수 있습니다. 고정된 분산에 대해, 투영 거리를 최소화하는 것은 또한 그 선을 따른 분산을 최대화합니다. 종종 우리는 첫 번째 주성분을 분산이 최대인 선으로 정의합니다.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "07-chap.html#새로운-선형-결합",
    "href": "07-chap.html#새로운-선형-결합",
    "title": "9  7.1 이 장의 목표",
    "section": "9.3 7.4 새로운 선형 결합",
    "text": "9.3 7.4 새로운 선형 결합\n\n이전 섹션에서 찾은 PC 선은 다음과 같이 쓸 수 있습니다.\n이미지 출처: Sara Holmes\n\\[ PC = + . \\]\n주성분은 원래 측정된 변수들의 _선형 결합(linear combinations)_이며, _새로운 좌표계_를 제공합니다. 선형 결합이 실제로 무엇인지 이해하기 위해 비유를 들어보겠습니다. 건강한 주스 믹스를 만들 때 다음과 같은 레시피를 따를 것입니다.\n\\[ \\[\\begin{align} V &= 2 \\times \\text{Beet} + 1 \\times \\text{Carrot} \\\\\\ &\\+\n\\tfrac{1}{2} \\text{Gala} + \\tfrac{1}{2} \\text{GrannySmith} \\\\\\ &\\+ 0.02 \\times\n\\text{Ginger} + 0.25 \\times \\text{Lemon}. \\end{align}\\] \\]\n이 레시피는 개별 주스 유형(원래 변수들)의 선형 결합입니다. 결과는 새로운 변수 \\(V\\)이며, 계수 \\((2,1,,,0.02,0.25)\\)는 로딩(loadings)이라고 불립니다.\n__\n질문 7.9\n주스 한 잔의 칼로리를 어떻게 계산하시겠습니까?\n\n9.3.1 7.4.1 최적의 선\n변수들의 선형 결합은 우리가 2차원 산점도 평면에서 선을 구축했던 것과 같은 방식으로 고차원에서의 선을 정의합니다. 그 사례에서 보았듯이, 데이터를 투영할 선을 선택하는 방법은 여러 가지가 있지만, 우리의 목적에 부합하는 ‘최선의’ 선이 존재합니다.\n모든 변수에 걸친 모든 점의 전체 분산은 분해될 수 있습니다. PCA에서 우리는 점들과 임의의 선 사이의 거리 제곱합이 선까지의 거리와 선을 따른 분산으로 분해될 수 있다는 사실을 이용합니다.\n우리는 주성분이 선까지의 거리를 최소화하며, 또한 선을 따른 투영의 분산을 최대화한다는 것을 보았습니다.\n왜 선을 따른 분산을 최대화하는 것이 좋은 아이디어일까요? 3차원에서 2차원으로의 투영에 대한 또 다른 예시를 살펴봅시다. 사실, 인간의 시각은 그러한 차원 축소에 의존합니다:\n\n\n\n그림 7.11: 수수께끼의 실루엣.\n\n\n그림 7.11: 수수께끼의 실루엣.\n__\n질문 7.10\n그림 7.11에는 3차원 물체의 2차원 투영이 있습니다. 이 물체는 무엇일까요?\n__\n질문 7.11\n그림 7.11과 7.13 중 어느 투영이 더 정보가 많다고 생각하시나요? 그 이유는 무엇인가요?\n__\n해결책\n__\n그림자의 면적을 최대화하는 투영이 더 많은 ’정보’를 보여준다고 주장할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "07-chap.html#pca-워크플로",
    "href": "07-chap.html#pca-워크플로",
    "title": "9  7.1 이 장의 목표",
    "section": "9.4 7.5 PCA 워크플로",
    "text": "9.4 7.5 PCA 워크플로\n\n그림 7.12: PCA 처리 과정 중 많은 선택이 이루어져야 합니다.\nPCA는 가장 큰 관성/가변성을 보여주는 축을 찾고, 그 방향의 가변성을 제거한 다음, 그 다음으로 좋은 직교 축을 찾는 과정을 반복하는 원리에 기반합니다. 사실 반복해서 실행할 필요 없이, 특잇값 분해(Singular Value Decomposition, SVD)라고 불리는 하나의 선형 대수 연산으로 모든 축을 찾을 수 있습니다(이에 대한 자세한 내용은 아래에서 다룰 것입니다).\n그림 7.12의 다이어그램에서, 먼저 평균과 분산이 계산되고 공분산 행렬로 직접 작업할지 아니면 상관 행렬로 작업할지 선택해야 함을 알 수 있습니다. 다음 단계는 데이터에 유의미하다고 간주되는 성분의 수인 \\(k\\)를 선택하는 것입니다. 우리는 \\(k\\)를 근사(approximation)의 계수(rank)라고 말합니다. 최적의 \\(k\\)를 선택하는 것은 어려운 문제이며, 아래에서 어떻게 접근해야 할지 논의할 것입니다. \\(k\\)를 선택하려면 연속적인 주성분들에 의해 설명되는 분산의 플롯을 살펴보아야 합니다. 일단 \\(k\\)를 선택했다면, 새로운 \\(k\\)차원 하위 공간으로의 데이터 투영을 진행할 수 있습니다.\nPCA 워크플로의 최종 결과는 변수와 샘플 모두에 대한 유용한 맵(maps)입니다. 이러한 맵들이 어떻게 구축되는지 이해하면 그로부터 얻을 수 있는 정보를 극대화할 수 있을 것입니다.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "07-chap.html#pca의-내부-구조-계수-축소rank-reduction",
    "href": "07-chap.html#pca의-내부-구조-계수-축소rank-reduction",
    "title": "9  7.1 이 장의 목표",
    "section": "9.5 7.6 PCA의 내부 구조: 계수 축소(Rank reduction)",
    "text": "9.5 7.6 PCA의 내부 구조: 계수 축소(Rank reduction)\n이 섹션은 선형 대수학 배경 지식이 희미한 기억으로만 남아 있는 분들을 위한 짧은 섹션입니다. 너무 많은 수식 없이 PCA의 기초가 되는 특잇값 분해 방법에 대한 직관을 제공하고자 합니다.\n\n그림 7.13: 그림 7.11에서 보여준 것과 동일한 물체의 또 다른 2차원 투영. 여기서의 관점이 더 정보가 많습니다. 일반적으로 점들의 퍼짐(다시 말해 분산)이 최대가 되도록 관점을 선택하는 것이 가장 많은 정보를 제공합니다. 우리는 가능한 한 많은 변동을 보고 싶어 하며, 그것이 바로 PCA가 하는 일입니다.\n행렬의 특잇값 분해는 수평 및 수직 벡터(특잇값 벡터라 불림)와 정규화 값(특잇값이라 불림)을 찾습니다. 이전과 마찬가지로, 분해를 생성하는 데 사용되는 실제 역설계(reverse engineering)를 수행하기 전에 순방향 생성(forward-generative) 설명을 먼저 제공하겠습니다. 각 단계의 의미를 보정하기 위해, 실제 데이터의 복잡함으로 넘어가기 전에 인위적인 예시부터 시작하겠습니다.\n\n9.5.1 7.6.1 계수 1 행렬(Rank-one matrices)\n간단한 생성 모델은 행렬의 계수(rank of a matrix)의 의미를 보여주고 우리가 이를 실제로 어떻게 찾는지 설명해 줍니다. 두 개의 벡터 \\(u\\)(1열 행렬)와 \\(v^t=t(v)\\)(1행 행렬 — 1열 행렬 \\(v\\)의 전치)가 있다고 가정해 봅시다. 예를 들어, \\(u =(\n\\[\\begin{smallmatrix} 1\\\\\\2\\\\\\3\\\\\\4 \\end{smallmatrix}\\]\n)\\)이고 \\(v =(\n\\[\\begin{smallmatrix} 2\\\\\\4\\\\\\8 \\end{smallmatrix}\\]\n)\\)입니다. \\(v\\)의 전치는 \\(v^t = t(v) = (2; 4; 8)\\)로 쓰여집니다. 우리는 다음과 같이 \\(u\\)의 복사본에 \\(v^t\\)의 각 원소를 차례로 곱합니다:\n단계 0:\n\n\n\nX\n2\n4\n8\n\n\n\n\n1\n\n\n\n\n\n2\n\n\n\n\n\n3\n\n\n\n\n\n4\n\n\n\n\n\n\n단계 1:\n\n\n\nX\n2\n4\n8\n\n\n\n\n1\n2\n\n\n\n\n2\n4\n\n\n\n\n3\n6\n\n\n\n\n4\n8\n\n\n\n\n\n단계 2:\n\n\n\nX\n2\n4\n8\n\n\n\n\n1\n2\n4\n\n\n\n2\n4\n8\n\n\n\n3\n6\n12\n\n\n\n4\n8\n16\n\n\n\n\n단계 3:\n\n\n\nX\n2\n4\n8\n\n\n\n\n1\n2\n4\n8\n\n\n2\n4\n8\n16\n\n\n3\n6\n12\n24\n\n\n4\n8\n16\n32\n\n\n\n따라서 행렬 \\(X\\)의 \\((2,3)\\) 항목(entry), 즉 \\(x_{2,3}\\)은 \\(u_2\\)에 \\(v_3\\)를 곱하여 얻어집니다. 우리는 이를 다음과 같이 쓸 수 있습니다.\n\\[ X=]\n여기서 우리가 얻은 행렬 \\(X\\)는 \\(u\\)와 \\(v\\) 모두 하나의 열을 가지므로 계수(rank)가 1이라고 합니다.\n__\n질문 7.12\n왜 \\(X = u*v^t\\)라고 쓰는 것이 전체 행렬 \\(X\\)를 모두 적는 것보다 더 경제적이라고 말할 수 있을까요?\n__\n해결책\n__\n\\(X\\)는 \\(4=12\\)개의 원소를 가지는 반면, \\(u\\)와 \\(v\\)를 통해서는 단지 \\(4+3=7\\)개의 숫자로 표현될 수 있습니다. 이러한 압축은 \\(u\\)나 \\(v\\)가 더 길어질수록 더욱 인상적입니다.\n반면에, 아래에 주어진 3행 4열(12개 숫자)의 또 다른 행렬 \\(X\\)를 단순화하기 위해 그 과정을 거꾸로 수행하고 싶다고 가정해 봅시다. 정보의 손실 없이 항상 이를 벡터들의 곱으로 비슷하게 표현할 수 있을까요? 그림 7.14와 7.15에 표시된 다이어그램에서, 색상이 칠해진 상자들은 행렬 (7.4)의 셀에 있는 숫자들에 비례하는 면적을 가집니다.\n\n그림 7.14: 일부 특수한 행렬들은 분해하기 쉬운 숫자들을 포함하고 있습니다. 이 다이어그램의 각 색상 사각형은 그 안의 숫자에 해당하는 면적을 가집니다.\n__\n질문 7.13\n여기에 우리가 분해하고 싶은 행렬 \\(X\\)가 있습니다.\n\\[ ]\n\\(X\\)는 그림 7.14에서 일련의 사각형들로 다시 그려졌습니다. 사각형의 변의 값들을 곱했을 때 해당 숫자들이 나오도록 하려면 흰색 \\(u\\)와 \\(v\\) 상자에 어떤 숫자들을 넣을 수 있을까요?\n\\(X\\)처럼 완벽하게 “직사각형”인 특수한 성질을 가진 행렬을 계수가 1이라고 합니다. 우리는 \\(X\\)의 숫자들을 사각형의 면적으로 나타낼 수 있으며, 여기서 사각형의 변들은 측면 벡터(\\(u\\)와 \\(v\\))에 있는 값들에 의해 주어집니다.\n: “)\n\n\n\n: “)\n\n\n\n: “)\n\n\n\n그림 7.15: 셀 안의 숫자들은 (a), (b), (c)에서 상응하는 여백(margins)의 곱과 같습니다. 우리는 여러 가지 방식으로 곱을 통해 셀을 만들 수 있습니다. (c)에서는 여백의 노름(norm)이 1이 되도록 강제합니다.\n그림 7.15에서 우리는 \\(X\\)의 분해가 유일하지 않다는 것을 알 수 있습니다: 벡터 \\(u\\)와 \\(v\\)에 대해 여러 가지 후보 선택지가 있습니다. 우리는 각 벡터 원소의 제곱합이 1이 되도록 요구함으로써 선택을 유일하게 만들 것입니다 (벡터 \\(v\\)와 \\(u\\)가 노름 1을 갖는다고 말합니다). 그러면 우리는 각 곱에 곱해줄, \\(X\\)의 “전체적인 스케일”을 나타내는 추가적인 숫자 하나를 추적해야 합니다. 이것이 우리가 왼쪽 상단 모서리에 넣은 값입니다. 이를 첫 번째 특잇값 \\(s_1\\)이라고 부릅니다. 아래 R 코드에서, 우리는 먼저 u, v, s1의 값을 알고 있다고 가정하고 시작하겠습니다. 나중에 우리를 위해 이들을 찾아주는 함수를 볼 것입니다. R에서 곱셈과 노름 속성을 확인해 봅시다:\nX = matrix(c(780,  75, 540,\n             936,  90, 648,\n            1300, 125, 900,\n             728,  70, 504), nrow = 3)\nu = c(0.8196, 0.0788, 0.5674)\nv = c(0.4053, 0.4863, 0.6754, 0.3782)\ns1 = 2348.2\nsum(u^2)__\n\n\n[1] 1\n\n\nsum(v^2)__\n\n\n[1] 1\n\n\ns1 * u %*% t(v)__\n\n\n     [,1] [,2] [,3] [,4]\n[1,]  780  936 1300  728\n[2,]   75   90  125   70\n[3,]  540  648  900  504\n\n\nX - s1 * u %*% t(v)__\n\n\n         [,1]   [,2]   [,3]   [,4]\n[1,] -0.03419 0.0745 0.1355 0.1221\n[2,]  0.00403 0.0159 0.0252 0.0186\n[3,] -0.00903 0.0691 0.1182 0.0982\n__\nQuestion 7.14\nTry svd(X) in R. Look at the components of the output of the svd function carefully. Check the norm of the columns of the matrices that result from this call. Where did the above value of s1 = 2348.2 come from?\n__\nSolution\n__\nsvd(X)$u[, 1]\nsvd(X)$v[, 1]\nsum(svd(X)$u[, 1]^2)\nsum(svd(X)$v[, 1]^2)\nsvd(X)$d __\nIn fact, in this particular case we were lucky: we see that the second and third singular values are 0 (up to the numeric precision we care about). That is why we say that \\(X\\) is of rank 1. For a more general matrix \\(X\\), it is rare to be able to write \\(X\\) exactly as this type of two- vector product. The next subsection shows how we can decompose \\(X\\) when it is not of rank 1: we will just need more pieces.\n\n\n9.5.2 7.6.2 How do we find such a decomposition in a unique way?\nIn the above decomposition, there were three elements: the horizontal and vertical singular vectors, and the diagonal corner, called the singular value. These can be found using the singular value decomposition function (svd). For instance:\nXtwo = matrix(c(12.5, 35.0, 25.0, 25, 9, 14, 26, 18, 16, 21, 49, 32,\n       18, 28, 52, 36, 18, 10.5, 64.5, 36), ncol = 4, byrow = TRUE)\nUSV = svd(Xtwo)__\n__\nQuestion 7.15\nLook at the USV object, the result of calling the svd function. What are its components?\n__\nSolution\n__\nnames(USV)__\n\n\n[1] \"d\" \"u\" \"v\"\n\n\nUSV$d __\n따라서 135.1이 첫 번째 특잇값 USV$d[1]입니다.\n__\n질문 7.16\n각각의 연속적인 특잇값 벡터 쌍이 Xtwo에 대한 근사를 어떻게 개선하는지 확인해 보세요. 세 번째와 네 번째 특잇값에 대해 무엇을 알 수 있나요?\n__\n해결책\n__\nXtwo - USV$d[1] * USV$u[, 1] %*% t(USV$v[, 1])\nXtwo - USV$d[1] * USV$u[, 1] %*% t(USV$v[, 1]) -\n       USV$d[2] * USV$u[, 2] %*% t(USV$v[, 2])__\n세 번째와 네 번째 특잇값은 매우 작아서 (반올림 오차 범위 내에서) 근사를 개선하지 못하므로, 우리는 Xtwo의 계수가 2라고 결론 내릴 수 있습니다.\n다시 말하지만, Xtwo와 같은 계수가 2인 행렬을 계수 1인 행렬들의 합으로 쓰는 방법은 여러 가지가 있습니다: 유일성을 보장하기 위해 우리는 특잇값 벡터에 또 다른7 조건을 부과합니다. 특잇값 분해의 출력 벡터들은 노름이 1일 뿐만 아니라, \\(U\\) 행렬의 각 열 벡터는 이전의 모든 열 벡터들과 직교(orthogonal)합니다. 우리는 이를 \\(u_{} u_{}\\)라고 쓰며, 이는 동일한 위치에 있는 값들의 곱의 합이 0임을 의미합니다: \\(i u{i1} u_{i2} = 0\\). \\(V\\) 행렬에 대해서도 마찬가지입니다.\n7 위에서 우리는 벡터의 노름을 1로 선택했었습니다.\n__\n태스크\n\\(U\\)와 \\(V\\) 행렬의 교차 곱(cross product)을 계산하여 정규 직교성(orthonormality)을 확인해 보세요:\nt(USV$u) %*% USV$u\nt(USV$v) %*% USV$v __\n우리의 scaledTurtles 행렬을 특잇값 분해해 봅시다.\nturtles.svd = svd(scaledTurtles)\nturtles.svd$d __\n\n\n[1] 11.746475  1.419035  1.003329\n\n\nturtles.svd$v __\n\n\n          [,1]       [,2]        [,3]\n[1,] 0.5787981  0.3250273  0.74789704\n[2,] 0.5779840  0.4834699 -0.65741263\n[3,] 0.5752628 -0.8127817 -0.09197088\n\n\ndim(turtles.svd$u)__\n\n\n[1] 48  3\n__\n질문 7.17\nsvd 출력으로부터 거북이 행렬에 대해 무엇을 결론지을 수 있나요?\n__\n해결책\n__\nturtles.svd$v의 첫 번째 열은 세 변수에 대한 계수가 거의 동일함을 보여줍니다. 다른 눈에 띄는 “우연”들은 다음과 같습니다:\nsum(turtles.svd$v[,1]^2)__\n\n\n[1] 1\n\n\nsum(turtles.svd$d^2) / 47 __\n\n\n[1] 3\n계수들이 실제로는 \\(\\)이며 특잇값의 제곱합이 \\((n-1)p\\)와 같음을 알 수 있습니다.\n\n\n9.5.3 7.6.3 특잇값 분해(Singular value decomposition)\n\n\\(X\\)는 계수 1인 조각들로 가산적으로(additively) 분해됩니다. 각 \\(u\\) 벡터는 \\(U\\) 행렬로 결합되고, 각 \\(v\\) 벡터는 \\(V\\) 행렬로 결합됩니다. 특잇값 분해는 다음과 같습니다.\n\\[ {X} = U S V^t, V^t V={I}, U^t U={I}, \\]\n여기서 \\(S\\)는 특잇값들의 대각 행렬(diagonal matrix)이고, \\(V^t\\)는 \\(V\\)의 전치 행렬이며, \\({I}\\)는 단위 행렬(Identity matrix)입니다. 식 7.5는 원소별로 다음과 같이 쓰여질 수 있습니다.\n\\[ X_{ij} = u_{i1}s_1v_{1j} + u_{i2}s_2v_{2j} + u_{i3}s_3v_{3j} +… + u_{ir}s_rv_{rj}, \\]\n\\(U\\)와 \\(V\\)는 그들 자신의 교차 곱이 단위 행렬이므로 정규 직교(orthonormal)8라고 합니다.\n8 정규 분포(normal distribution)와는 아무런 관련이 없으며, 직교(orthogonal)하면서 노름이 1임을 나타냅니다.\n\n\n9.5.4 7.6.4 주성분(Principal components)\n특잇값 분해(R의 svd 함수에서 제공)에서 얻은 특잇값 벡터는 원래의 변수 앞에 붙여서 우리가 주성분이라고 부르는 더 정보가 많은 변수들을 만드는 계수들을 포함하고 있습니다. 우리는 이를 다음과 같이 씁니다:\n\\[ Z_1=v_{11} X_{} +v_{21} X_{} + v_{31} X_{}+ + v_{p1} X_{p}. \\]\n만약 usv = svd(X)라면, \\((v_{11},v_{21},v_{31},…)\\)은 usv$v의 첫 번째 열에 의해 주어집니다; 유사하게 \\(Z_2\\)는 usv$v의 두 번째 열에 의해 주어지는 식입니다. \\(p\\)는 \\(X\\)의 열의 수이자 \\(V\\)의 행의 수입니다. 이러한 새로운 변수 \\(Z_1, Z_2, Z_3, …\\)는 크기가 감소하는 분산을 가집니다: \\(s_1^2 s_2^2 s_3^2 …\\).\n__\n질문 7.18\n첫 번째 특잇값 d[1]과 u[,1]을 곱하여 거북이 데이터의 첫 번째 주성분을 계산해 보세요. 이를 계산하는 또 다른 방법은 무엇일까요?\n__\n해결책\n__\n다음 코드를 사용하여 이를 보여줍니다:\nUS = turtles.svd$u[,1, drop = FALSE] %*% turtles.svd$d[1]\nXV = scaledTurtles %*% turtles.svd$v[, 1, drop = FALSE]\nmax(abs(US-XV))__\n우리는 또한 행렬 대수를 사용하여 \\(XV\\)와 \\(US\\)가 같음을 알 수 있습니다. \\(V\\)가 직교하므로 \\(V^t V={I}\\)이고 \\(XV = USV^tV=US,{I}\\)임을 기억하세요.\n참고: 아래 코드의 첫 번째 줄에 있는 drop = FALSE 인수는 선택된 행렬 열이 matrix / array 클래스 속성을 유지하도록 하여 행렬 곱셈 연산이 가능하도록 보장합니다. 대안으로 일반 곱셈 연산자 *를 사용할 수도 있습니다. 두 번째 줄에서 drop = FALSE는 엄밀히 말해 필요하지 않지만 대칭을 위해 넣었습니다.\n여기에 두 가지 유용한 사실이 있습니다. 먼저 말로 설명하고, 그 다음 수학적 기호로 나타내겠습니다.\n주성분의 수 \\(k\\)는 항상 원래 변수의 수나 관측치 수보다 적게 선택됩니다. 우리는 문제의 차원을 “낮추고” 있는 것입니다:\n\\[ k(n,p). \\]\n주성분 변환은 첫 번째 주성분이 가능한 가장 큰 분산을 갖도록(즉, 데이터의 가변성을 가능한 한 많이 설명하도록) 정의되며, 각 후속 성분은 이전 성분들과 직교해야 한다는 제약 조건 하에서 순차적으로 가장 높은 분산을 갖습니다:\n\\[ {aX bX}({aX} (X)), bX= \\]",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "07-chap.html#주평면에-관측치-플롯하기",
    "href": "07-chap.html#주평면에-관측치-플롯하기",
    "title": "9  7.1 이 장의 목표",
    "section": "9.6 7.7 주평면에 관측치 플롯하기",
    "text": "9.6 7.7 주평면에 관측치 플롯하기\n우리는 discus와 weight 변수가 있는 두 변수 athletes 데이터를 다시 살펴봅니다. 7.3.2절에서 우리는 첫 번째 주성분을 계산했고 이를 그림 7.10에서 보라색 선으로 나타냈습니다. 우리는 \\(Z_1\\)이 대각선에 의해 주어진 선형 결합임을 보여주었습니다. 계수들의 제곱합이 1이 되어야 하므로, 우리는 다음과 같은 식을 얻습니다. \\[Z_1=-0.707- 0.707.\\]\n이는 두 좌표가 \\(c_1=0.7071\\) 및 \\(c_2=0.7071\\)인 것과 같습니다.\n__\n질문 7.19\nsvd 함수의 출력 중 어떤 부분이 PC 로딩(loadings)이라고도 알려진 첫 번째 PC 계수로 우리를 인도하나요?\n우리가 두 변수 discus와 weight에 적용했던 svda를 사용한다는 점에 유의하세요.\n__\n해결책\n__\nsvda$v[,1]__\n\n\n[1] -0.7071068 -0.7071068\n보라색 선을 가로 \\(x\\)축으로 만들어 (discus, weight) 평면을 회전시키면, 첫 번째 주평면(principal plane)이라고 알려진 것을 얻게 됩니다.\nppdf = tibble(PC1n = -svda$u[, 1] * svda$d[1],\n              PC2n =  svda$u[, 2] * svda$d[2])\ngg = ggplot(ppdf, aes(x = PC1n, y = PC2n)) + \n    geom_point() + \n    geom_hline(yintercept = 0, color = \"purple\", linewidth = 1.5, alpha = 0.5) +\n    xlab(\"PC1 \")+ ylab(\"PC2\") + xlim(-3.5, 2.7) + ylim(-2, 2) + coord_fixed()\ngg + geom_point(aes(x = PC1n, y = 0), color = \"red\") +\n     geom_segment(aes(xend = PC1n, yend = 0), color = \"red\") \ngg + geom_point(aes(x = 0, y = PC2n), color = \"blue\") +\n     geom_segment(aes(yend = PC2n, xend = 0), color = \"blue\") +\n     geom_vline(xintercept = 0, color = \"skyblue\", linewidth = 1.5, alpha = 0.5) __\n\n\n\n\n\n\n\n\n그림 7.16: 원래 변수가 두 개뿐인 경우, PCA 변환은 단순한 회전입니다. 새로운 좌표는 항상 가로 및 세로 축으로 선택됩니다.\n__\n질문 7.20\n\n그림 7.16에서 빨간색 세그먼트의 제곱합의 평균은 무엇과 같습니까?\n이것이 빨간색 점들의 분산과 비교하여 어떤가요?\n그림 7.16에서 파란색 세그먼트와 빨간색 세그먼트의 표준 편차 비율을 계산해 보세요. 이를 첫 번째와 두 번째 특잇값의 비율과 비교해 보세요.\n\n__\n해결책\n__\n\n빨간색 세그먼트의 제곱합은 두 번째 특잇값의 제곱에 해당합니다:\n\nsum(ppdf$PC2n^2) __\n\n\n[1] 6.196729\n\n\nsvda$d[2]^2 __\n\n\n[1] 6.196729\n빨간색 세그먼트의 평균이 0이므로, 위의 수치들은 또한 분산에 비례합니다:\nmean(ppdf$PC2n) __\n\n\n[1] 5.451106e-16\n\n\nvar(ppdf$PC2n) * (nrow(ppdf)-1)__\n\n\n[1] 6.196729\n\nThe variance of the red points is var(ppdf$PC1n), which is larger than what we calculated in a) by design of the first PC.\n\nvar(ppdf$PC1n) __\n\n\n[1] 1.806352\n\n\nvar(ppdf$PC2n) __\n\n\n[1] 0.1936478\n\nWe take the ratios of the standard deviations explained by the points on the vertical and horizontal axes by computing:\n\nsd(ppdf$PC1n) / sd(ppdf$PC2n)__\n\n\n[1] 3.054182\n\n\nsvda$d[1] / svda$d[2]__\n\n\n[1] 3.054182\n__\nTask\nUse prcomp to compute the PCA of the first two columns of the athletes data, look at the output. Compare to the singular value decomposition.\n\n9.6.1 7.7.1 PCA of the turtles data\nWe now want to do a complete PCA analysis on the turtles data. Remember, we already looked at the summary statistics for the one- and two-dimensional data. Now we are going to answer the question about the “true” dimensionality of these rescaled data.\nIn the following code, we use the function princomp. Its return value is a list of all the important pieces of information needed to plot and interpret a PCA.\n\n\n\n사실 PCA는 매우 기초적인 기법이라서 다양한 R 패키지에 많은 서로 다른 구현체들이 존재합니다. 불행하게도 입력 인수와 출력의 형식 및 명칭이 표준화되어 있지 않으며, 일부는 출력의 스케일링에 대해 서로 다른 관례를 사용하기도 합니다. 우리는 이러한 선택들에 익숙해지기 위해 몇 가지 서로 다른 것들을 실험해 볼 것입니다.\n\n\n사실 PCA는 매우 기초적인 기법이라서 다양한 R 패키지에 많은 서로 다른 구현체들이 존재합니다. 불행하게도 입력 인수와 출력의 형식 및 명칭이 표준화되어 있지 않으며, 일부는 출력의 스케일링에 대해 서로 다른 관례를 사용하기도 합니다. 우리는 이러한 선택들에 익숙해지기 위해 몇 가지 서로 다른 것들을 실험해 볼 것입니다.\ncor(scaledTurtles)__\n\n\n          length     width    height\nlength 1.0000000 0.9783116 0.9646946\nwidth  0.9783116 1.0000000 0.9605705\nheight 0.9646946 0.9605705 1.0000000\n\n\npcaturtles = princomp(scaledTurtles)\npcaturtles __\n\n\nCall:\nprincomp(x = scaledTurtles)\n\nStandard deviations:\n   Comp.1    Comp.2    Comp.3 \n1.6954576 0.2048201 0.1448180 \n\n 3  variables and  48 observations.\n\n\nlibrary(\"factoextra\")\nfviz_eig(pcaturtles, geom = \"bar\", bar_width = 0.4) + ggtitle(\"\")__\n\n그림 7.17: 스크리 플롯(screeplot)은 표준화된 거북이 데이터(scaledTurtles)에 대한 고윳값을 보여줍니다: 하나의 큰 값과 두 개의 작은 값이 있습니다. 데이터는 (거의) 1차원적입니다. 우리는 왜 이 차원을 크기의 축(axis of size)이라고 부르는지 보게 될 것인데, 이는 생체 데이터에서 흔히 나타나는 현상입니다 (Jolicoeur and Mosimann 1960).\n__\n질문 7.21\n많은 PCA 함수들이 서로 다른 시기에 서로 다른 분야에서 일했던 다양한 팀들에 의해 만들어졌습니다. 이는 특히 명명 규칙이 다르기 때문에 혼란을 줄 수 있습니다. 세 가지를 비교해 봅시다. 다음 코드 라인들을 실행하고 결과 객체들을 살펴보세요:\nsvd(scaledTurtles)$v[, 1]\nprcomp(turtles[, -1])$rotation[, 1]\nprincomp(scaledTurtles)$loadings[, 1]\nlibrary(\"ade4\")\ndudi.pca(turtles[, -1], nf = 2, scannf = FALSE)$c1[, 1]__\nprcomp와 princomp 함수에서 스케일링(scaling)을 비활성화하면 어떤 일이 일어나나요?\n이후 내용에서는 항상 행렬 \\(X\\)가 중앙화되고 스케일링된 행렬을 나타낸다고 가정합니다.\n__\n질문 7.22\nprcomp 함수(결과를 res라고 합시다)에서 새로운 변수들에 대한 관측치의 좌표는 결과의 scores 슬롯에 있습니다. 거북이 데이터에 대한 PC1을 살펴보고 이를 res$scores와 비교해 보세요. 표준 편차 sd1을 res 객체에 있는 값 및 점수(scores)들의 표준 편차와 비교해 보세요.\n__\n해결책\n__\nres = princomp(scaledTurtles)\nPC1 = scaledTurtles %*% res$loadings[,1]\nsd1 = sqrt(mean(res$scores[, 1]^2))__\n__\n질문 7.23\nres$scores 행렬의 직교성(orthogonality)을 확인해 보세요. 왜 이것이 정규 직교(orthonormal)한다고 말할 수 없을까요?\n이제 우리는 PC 점수(\\(US\\))와 로딩 계수(\\(V\\))를 결합할 것입니다. 샘플과 변수가 모두 표시된 플롯을 바이플롯(biplots)이라고 합니다. 이는 다음 factoextra 패키지 함수를 사용하여 한 줄로 수행할 수 있습니다.\nfviz_pca_biplot(pcaturtles, label = \"var\", habillage = turtles[, 1]) +\n  ggtitle(\"\")__\n\n그림 7.18: 변수와 관측치를 모두 보여주는 처음 두 차원의 바이플롯. 화살표는 변수를 나타냅니다. 거북이들은 성별에 따라 레이블이 붙어 있습니다. 가로 방향으로 길게 뻗은 것은 두 번째 고윳값보다 훨씬 큰 첫 번째 고윳값의 크기 때문입니다.\n\n\n\nPCA를 플로팅할 때 종횡비(aspect ratio)에 주의하십시오. 두 성분의 노름이 비슷한 경우는 드물기 때문에, 정사각형 형태의 플롯은 예외적일 것입니다. 가로(첫 번째) 주성분이 두 번째보다 더 중요하다는 것을 보여주는 길쭉한 형태의 플롯이 더 일반적입니다. 이는 예를 들어 플롯에서 점들 사이의 거리를 해석할 때 중요합니다.\n\n\nPCA를 플로팅할 때 종횡비(aspect ratio)에 주의하십시오. 두 성분의 노름이 비슷한 경우는 드물기 때문에, 정사각형 형태의 플롯은 예외적일 것입니다. 가로(첫 번째) 주성분이 두 번째보다 더 중요하다는 것을 보여주는 길쭉한 형태의 플롯이 더 일반적입니다. 이는 예를 들어 플롯에서 점들 사이의 거리를 해석할 때 중요합니다.\n__\n질문 7.24\nPC1이 가로축인 PCA 플롯에서 너비보다 높이가 더 길게 나타나는 것이 가능할까요?\n__\n해결책\n__\nPC1 방향으로의 점들의 분산은 \\(_1=s_1^2\\)이며 이는 항상 \\(_2=s_2^2\\)보다 큽니다. 따라서 PCA 플롯은 항상 높이보다 너비가 더 넓을 것입니다.\n__\n질문 7.25\n그림 7.18을 보고 답해 보세요: a) 수컷과 암컷 거북이 중 어느 쪽이 더 큰 경향이 있나요?\nb) 화살표들은 상관관계에 대해 무엇을 말해주나요?\n__\n질문 7.26\n각 새로운 좌표의 분산을 PCA dudi.pca 함수가 반환하는 고윳값(eigenvalues)과 비교해 보세요.\n__\n해결책\n__\npcadudit = dudi.pca(scaledTurtles, nf = 2, scannf = FALSE)\napply(pcadudit$li, 2, function(x) sum(x^2)/48)__\n\n\n     Axis1      Axis2 \n2.93573765 0.04284387 \n\n\npcadudit$eig __\n\n\n[1] 2.93573765 0.04284387 0.02141848\n이제 이른바 상관관계 원(correlation circle)을 그려서 기존 변수와 새로운 변수 사이의 관계를 살펴봅니다. 여기서는 종횡비가 1이며 변수들은 그림 7.19에서 보듯이 화살표로 표현됩니다. 화살표의 길이는 첫 번째 주평면으로의 투영 품질을 나타냅니다:\nfviz_pca_var(pcaturtles, col.circle = \"black\") + ggtitle(\"\") +\n  xlim(c(-1.2, 1.2)) + ylim(c(-1.2, 1.2))__\n\n그림 7.19: 원래 변수들을 보여주는 “상관관계 원”의 일부. 서로 간의 상관관계 및 새로운 주성분과의 상관관계는 벡터들 사이의 각도와 축과 벡터 사이의 각도로 주어집니다.\n__\n질문 7.27\n우리 거북이 데이터 행렬의 행 수와 다음 숫자들 사이의 관계를 설명해 보세요:\nsvd(scaledTurtles)$d/pcaturtles$sdev\nsqrt(47)__\n__\n해결책\n__\n분산-공분산 행렬을 계산할 때, 많은 구현체들은 분모로 \\(1/(n-1)\\)을 사용합니다. 여기서 \\(n=48\\)이므로 분산의 합은 48/47의 인자만큼 차이가 납니다.\n이 데이터는 때때로 데이터의 거의 모든 변동이 더 낮은 차원의 공간에 캡처될 수 있다는 좋은 예입니다: 여기서는 3차원 데이터가 본질적으로 하나의 선으로 대체될 수 있습니다. 다음을 명심하세요: \\(XtC=VSUtUS=VS^2.\\) 주성분은 행렬 \\(C=US\\)의 열들입니다. \\(U\\)(\\(\\) 함수의 출력에서 USV$u로 주어지는 행렬)의 \\(p\\)개 열들은 노름 \\((s_1^2, s_2^2, …, s_p^2)\\)을 갖도록 스케일이 조정됩니다. 각 열은 자신이 설명해야 할 _책임_이 있는 서로 다른 분산을 가집니다. 이들은 감소하는 숫자들이 될 것임에 주목하세요.\n만약 첫 번째 것만 원한다면 그것은 단지 \\(c_1=s_1 u_1\\)입니다. \\(||c_1||2=s_1tu_1 u_1^t s_1= s_1^2 u_1tu_1=s_12=_1\\)임을 주목하세요.\n만약 행렬 \\(X\\)가 \\(n\\)개의 서로 다른 샘플이나 표본에 대한 연구로부터 나왔다면, 주성분은 그림 7.16에서처럼 이 \\(n\\)개 점들에 대한 새로운 좌표를 제공합니다. 이들은 때때로 PCA 함수의 결과에서 _점수(scores)_라고 불립니다.\n\n그림 7.20: 또 다른 훌륭한 xkcd의 시각: 이번에는 고유벡터입니다.\n더 자세한 예제로 들어가기 전에, SVD와 PCA가 제공하는 것을 요약해 봅시다:\n\n각 주성분은 해당 고윳값(상응하는 특잇값의 제곱)으로 측정되는 분산을 가집니다.\n새로운 변수들은 서로 직교하도록 만들어집니다. 이들은 또한 중앙화되어 있으므로, 이는 서로 상관관계가 없음을 의미합니다. 정규 분포 데이터의 경우, 이는 또한 서로 독립임을 의미합니다.\n변수들이 스케일링되었을 때, 모든 변수의 분산 합은 변수의 수(\\( = p\\))와 같습니다. 분산의 합은 교차 곱 행렬의 대각선을 더하여 계산됩니다9.\n주성분들은 고윳값의 크기 순서대로 정렬됩니다. 우리는 몇 개의 성분을 유지할지 결정하기 전에 항상 스크리 플롯을 확인합니다. 또한 그림 7.18에서 했던 것처럼 각 PC 축에 그것이 설명하는 분산의 비율을 주석으로 다는 것이 가장 좋은 관행입니다.\n\n고유 분해(Eigen Decomposition): X와 그 자신과의 교차 곱은 \\[XtX=VSUtUSVt=VS2Vt=VVt\\]를 만족하며, 여기서 \\(V\\)는 대칭 행렬 \\(X^tX\\)의 고유벡터 행렬이라 불리고 \\(\\)는 \\(X^tX\\)의 고윳값들의 대각 행렬입니다.\n9 이 대각 원소들의 합을 행렬의 대각합(trace)이라고 부릅니다.\n__\n태스크\n위키백과에서 고윳값(eigenvalue)을 찾아보세요. 공식을 사용하지 않고 이를 정의하는 문장을 찾아보세요. 왜 고유벡터가 신데렐라에서 사용될 수 있을까요(약간 억지스럽게 말이죠)? (그림 7.20의 xkcd 만화를 참조하세요.)\n\nFor help with the basics of linear algebra, a motivated student pressed for time may consult Khan’s Academy. If you have more time and would like in depth coverage, Gil Strang’s MIT course is a classic, and some of the book is available online (Strang 2009).\n\n\n9.6.2 7.7.2 A complete analysis: the decathlon athletes\nWe started looking at these data earlier in this chapter. Here, we will follow step by step a complete multivariate analysis. First, let us have another look at the correlation matrix (rounded to 2 digits after the decimal point), which captures the bivariate associations. We already plotted it as a colored heatmap in Figure 7.3.\ncor(athletes) |&gt; round(2)__\n\n\n        m100  long weight  high  m400  m110  disc  pole javel m1500\nm100    1.00 -0.54  -0.21 -0.15  0.61  0.64 -0.05 -0.39 -0.06  0.26\nlong   -0.54  1.00   0.14  0.27 -0.52 -0.48  0.04  0.35  0.18 -0.40\nweight -0.21  0.14   1.00  0.12  0.09 -0.30  0.81  0.48  0.60  0.27\nhigh   -0.15  0.27   0.12  1.00 -0.09 -0.31  0.15  0.21  0.12 -0.11\nm400    0.61 -0.52   0.09 -0.09  1.00  0.55  0.14 -0.32  0.12  0.59\nm110    0.64 -0.48  -0.30 -0.31  0.55  1.00 -0.11 -0.52 -0.06  0.14\ndisc   -0.05  0.04   0.81  0.15  0.14 -0.11  1.00  0.34  0.44  0.40\npole   -0.39  0.35   0.48  0.21 -0.32 -0.52  0.34  1.00  0.27 -0.03\njavel  -0.06  0.18   0.60  0.12  0.12 -0.06  0.44  0.27  1.00  0.10\nm1500   0.26 -0.40   0.27 -0.11  0.59  0.14  0.40 -0.03  0.10  1.00\nThen we look at the screeplot, which will help us choose a rank \\(k\\) for representing the essence of these data.\npca.ath = dudi.pca(athletes, scannf = FALSE)\npca.ath$eig __\n\n\n [1] 3.4182381 2.6063931 0.9432964 0.8780212 0.5566267 0.4912275 0.4305952\n [8] 0.3067981 0.2669494 0.1018542\n\n\nfviz_eig(pca.ath, geom = \"bar\", bar_width = 0.3) + ggtitle(\"\")__\n\nFigure 7.21: The screeplot of the athletes data indicates that most of the variation in the data can be captured in a two-dimensional plane (spanned by the first two principal components).\nThe screeplot in Figure 7.21 shows a clear drop in the eigenvalues after the second one. This indicates that a good approximation will be obtained at rank 2. Let’s look at an interpretation of the first two axes by projecting the loadings of the original variables onto the two new ones, the principal components.\nfviz_pca_var(pca.ath, col.var = \"blue\", repel = TRUE) + ggtitle(\"\")__\n\nFigure 7.22: Correlation circle of the original variables.\nThe correlation circle Figure 7.22 displays the projection of the original variables onto the two first new principal axes. The angles between vectors are interpreted as correlations. On the right side of the plane, we have the track and field events (m110, m100, m400, m1500), and on the left, we have the throwing and jumping events. Maybe there is an opposition of skills as characterized in the correlation matrix. We did see the correlations were negative between variables from these two groups. How can we interpret this?\nIt seems that those who throw the best have lower scores in the track competitions. In fact, if we look at the original measurements, we can see what is happening. The athletes who run in short times are the stronger ones, as are the ones who throw or jump longer distances. We should probably change the scores of the track variables and redo the analysis.\n__\n질문 7.28\n최고의 운동 성적이 동일한 방향으로 변하도록, 즉 대부분 양(+)의 상관관계를 갖도록 하려면 변수들을 어떻게 변환해야 할까요?\n__\n해결책\n__\n달리기 성적의 부호를 바꾸면 거의 모든 변수가 양의 상관관계를 갖게 됩니다.\nrunningvars = grep(\"^m\", colnames(athletes), value = TRUE)\nrunningvars __\n\n\n[1] \"m100\"  \"m400\"  \"m110\"  \"m1500\"\n\n\nathletes[, runningvars] = -athletes[, runningvars]\ncor(athletes) |&gt; round(2)__\n\n\n       m100 long weight high  m400 m110  disc pole javel m1500\nm100   1.00 0.54   0.21 0.15  0.61 0.64  0.05 0.39  0.06  0.26\nlong   0.54 1.00   0.14 0.27  0.52 0.48  0.04 0.35  0.18  0.40\nweight 0.21 0.14   1.00 0.12 -0.09 0.30  0.81 0.48  0.60 -0.27\nhigh   0.15 0.27   0.12 1.00  0.09 0.31  0.15 0.21  0.12  0.11\nm400   0.61 0.52  -0.09 0.09  1.00 0.55 -0.14 0.32 -0.12  0.59\nm110   0.64 0.48   0.30 0.31  0.55 1.00  0.11 0.52  0.06  0.14\ndisc   0.05 0.04   0.81 0.15 -0.14 0.11  1.00 0.34  0.44 -0.40\npole   0.39 0.35   0.48 0.21  0.32 0.52  0.34 1.00  0.27  0.03\njavel  0.06 0.18   0.60 0.12 -0.12 0.06  0.44 0.27  1.00 -0.10\nm1500  0.26 0.40  -0.27 0.11  0.59 0.14 -0.40 0.03 -0.10  1.00\n\n\npcan.ath = dudi.pca(athletes, nf = 2, scannf = FALSE)\npcan.ath$eig __\n\n\n [1] 3.4182381 2.6063931 0.9432964 0.8780212 0.5566267 0.4912275 0.4305952\n [8] 0.3067981 0.2669494 0.1018542\n이제 모든 음의 상관관계는 상당히 작아졌습니다. 행렬의 고윳값은 위의 부호 반전에 의해 영향을 받지 않으므로 스크리 플롯은 변하지 않을 것입니다. 바뀌는 출력값은 부호를 반전시킨 변수들에 대한 주성분 로딩(loadings) 계수의 부호뿐입니다.\nfviz_pca_var(pcan.ath, col.var = \"blue\", repel = TRUE) + ggtitle(\"\")__\n\n그림 7.23: 달리기 변수들의 부호를 바꾼 후의 상관관계 원.\n그림 7.23은 변환된 변수들의 상관관계 원을 보여줍니다. 이제 우리는 넓은 공통의 전체 축을 가지고 있음을 알 수 있습니다: 모든 화살표가 대략 같은 방향을 가리키고 있습니다.\n이제 다음 코드를 사용하여 주평면에 투영된 운동선수들을 플롯합니다:\nfviz_pca_ind(pcan.ath, repel = TRUE) + ggtitle(\"\") __\n\n그림 7.24: 운동선수들의 투영을 보여주는 첫 번째 주평면. 숫자들의 조직 방식에서 무언가 주목할 점이 있나요?\n__\n질문 7.29\n그림 7.24에 표시된 운동선수들 자체를 살펴보면, 약간의 순서 효과를 알 수 있습니다. 그림 7.24에서 운동선수들의 성적과 그들의 번호 매기기 사이의 관계가 보이나요?\n__\n해결책\n__\n번호 순서대로 점들을 연결해 보면, 번호가 무작위로 매겨졌을 때보다 플롯의 한쪽 면에서 더 많은 시간을 보낸다는 것을 깨닫게 될 것입니다.\nolympic 데이터 세트에는 보완적인 정보가 포함되어 있음이 밝혀졌습니다. score라고 불리는 추가 벡터 변수는 1988년 올림픽 남자 10종 경기의 최종 점수를 보고합니다.\nolympic$score __\n\n\n [1] 8488 8399 8328 8306 8286 8272 8216 8189 8180 8167 8143 8114 8093 8083 8036\n[16] 8021 7869 7860 7859 7781 7753 7745 7743 7623 7579 7517 7505 7422 7310 7237\n[31] 7231 7016 6907\n그러면 운동선수들의 첫 번째 주성분 좌표와 이 점수를 비교하는 산점도를 살펴봅시다. 이는 그림 7.25에 나와 있습니다. 두 변수 사이의 강한 상관관계를 볼 수 있습니다. 1번 선수(실제로 올림픽 10종 경기 금메달을 딴 선수)가 가장 높은 점수를 얻었지만, PC1에서 가장 높은 값을 가진 것은 아님을 알 수 있습니다. 왜 그렇다고 생각하시나요?\nggplot(data = tibble(pc1 = pcan.ath$li[, 1], score = olympic$score, label = rownames(athletes)),\n       mapping = aes(y = score, x = pc1)) + \n   geom_text(aes(label = label)) + stat_smooth(method = \"lm\", se = FALSE)__\n\n그림 7.25: olympic$score와 첫 번째 주성분 사이의 산점도. 점들은 데이터 세트에서의 순서로 레이블이 붙어 있습니다. 강한 상관관계를 볼 수 있습니다. 왜 완벽한 선형 적합이 아닐까요?\n\n\n9.6.3 7.7.3 차원 수 k를 어떻게 선택할까요?\n\n그림 7.26: ‘위험할 정도로’ 비슷한 분산들을 보여주는 스크리 플롯. 80%의 분산을 엄격한 임계값으로 끊기로 선택한다면 불안정한 PC 플롯을 얻게 될 것입니다. 그러한 임계값이 없다면, 3개의 유사한 고윳값을 가진 3D 하위 공간에 대응하는 축들은 불안정하며 개별적으로 해석될 수 없습니다.\n우리는 예제들에서 PCA의 첫 번째 단계가 새로운 변수들의 분산(고윳값과 같음)에 대한 스크리 플롯을 만드는 것임을 보았습니다. 이 플롯을 보기 전까지는 얼마나 많은 차원이 필요한지 결정할 수 없습니다. 그 이유는 주성분이 명확하게 정의되지 않는 상황이 있기 때문입니다: 그림 7.26과 같은 스크리 플롯을 주면서 두세 개의 연속된 PC가 매우 유사한 분산을 가질 때, 유사한 고윳값 그룹에 대응하는 하위 공간이 존재합니다. 이 경우 이는 \\(u_2, u_3, u_4\\)에 의해 생성된 3D 공간이 될 것입니다. 이 벡터들은 개별적으로는 의미가 없으며 그 로딩을 해석할 수 없습니다. 이는 하나의 관측치에서 아주 약간의 변화만 있어도 완전히 다른 세 개의 벡터 세트를 얻을 수 있기 때문입니다. 이들은 동일한 3D 공간을 생성하겠지만, 매우 다른 로딩을 가질 수 있습니다. 우리는 이러한 PC들이 불안정하다고 말합니다.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "07-chap.html#탐색적-도구로서의-pca-추가-정보-사용하기",
    "href": "07-chap.html#탐색적-도구로서의-pca-추가-정보-사용하기",
    "title": "9  7.1 이 장의 목표",
    "section": "9.7 7.8 탐색적 도구로서의 PCA: 추가 정보 사용하기",
    "text": "9.7 7.8 탐색적 도구로서의 PCA: 추가 정보 사용하기\n우리는 회귀 분석과 달리 PCA가 모든 변수를 동일하게 취급한다는 것을 보았습니다(동등한 표준 편차를 갖도록 전처리된 범위 내에서). 그러나 결과를 해석하는 데 도움을 주기 위해 다른 연속형 변수나 범주형 요인을 플롯에 매핑하는 것은 여전히 가능합니다. 종종 우리는 샘플에 대한 보완적인 정보를 가지고 있습니다. 예를 들어 당뇨병 데이터의 진단 레이블이나 T 세포 유전자 발현 데이터의 세포 유형 등이 있습니다.\n여기서는 우리의 해석에 정보를 주기 위해 그러한 추가 변수들을 어떻게 사용할 수 있는지 살펴봅니다. 그러한 이른바 _메타데이터(metadata)_를 저장하기에 가장 좋은 장소는 데이터 객체의 적절한 슬롯(Bioconductor SummarizedExperiment 클래스 등)입니다. 두 번째로 좋은 장소는 수치 데이터도 포함하는 데이터 프레임의 추가 열입니다. 실제로 그러한 정보는 종종 행렬의 행 이름에 어느 정도 암호 같은 방식으로 저장됩니다. 아래에서는 후자의 시나리오에 직면해야 하며, 우리는 substr 기교를 사용하여 세포 유형을 추출하고 그림 7.27의 스크리 플롯과 그림 7.28의 PCA를 보여줍니다.\npcaMsig3 = dudi.pca(Msig3transp, center = TRUE, scale = TRUE,\n                    scannf = FALSE, nf = 4)\nfviz_screeplot(pcaMsig3) + ggtitle(\"\")__\n\n그림 7.27: T 세포 발현 PCA 스크리 플롯.\nids = rownames(Msig3transp)\ncelltypes = factor(substr(ids, 7, 9))\nstatus = factor(substr(ids, 1, 3))\ntable(celltypes)__\n\n\ncelltypes\nEFF MEM NAI \n 10   9  11 \n\n\ncbind(pcaMsig3$li, tibble(Cluster = celltypes, sample = ids)) %&gt;%\nggplot(aes(x = Axis1, y = Axis2)) +\n  geom_point(aes(color = Cluster), size = 5) +\n  geom_hline(yintercept = 0, linetype = 2) +\n  geom_vline(xintercept = 0, linetype = 2) +\n  scale_color_discrete(name = \"Cluster\") + coord_fixed()__\n\n그림 7.28: 세 가지 별도의 T 세포 유형(effector, naïve, memory) 각각의 특이성에 관여하는 156개 유전자 하위 집합에 대한 유전자 발현 PCA. 다시 한 번, 첫 번째 축이 분산의 많은 부분을 설명하므로 플롯이 첫 번째 축을 따라 길게 뻗어 있음을 알 수 있습니다. T 세포 중 하나가 레이블이 잘못 붙은 것으로 보인다는 점에 주목하세요.\n\n9.7.1 7.8.1 질량 분석 데이터 분석\n이러한 데이터는 관련 특성을 열로, 샘플을 행으로 하는 원하는 행렬을 얻기 전에 섬세한 전처리가 필요합니다. 가공되지 않은 질량 분석 측정값에서 시작하여, 단계에는 관련 특성의 피크 추출, 여러 샘플에 걸친 정렬 및 피크 높이 추정이 포함됩니다. 끔찍한 세부 사항에 대해서는 Bioconductor xcms 패키지의 비네트를 참고하시기 바랍니다. 우리는 그러한 방식으로 생성된 데이터 행렬을 mat1xcms.RData 파일로부터 불러옵니다. 아래 코드의 출력은 그림 7.29와 7.30에 있습니다.\nload(\"../data/mat1xcms.RData\")\ndim(mat1)__\n\n\n[1] 399  12\n\n\npcamat1 = dudi.pca(t(mat1), scannf = FALSE, nf = 3)\nfviz_eig(pcamat1, geom = \"bar\", bar_width = 0.7) + ggtitle(\"\")__\n\n그림 7.29: 생쥐 데이터에 대한 고윳값을 보여주는 스크리 플롯.\ndfmat1 = cbind(pcamat1$li, tibble(\n    label = rownames(pcamat1$li),\n    number = substr(label, 3, 4),\n    type = factor(substr(label, 1, 2))))\npcsplot = ggplot(dfmat1,\n  aes(x=Axis1, y=Axis2, label=label, group=number, colour=type)) +\n geom_text(size = 4, vjust = -0.5)+ geom_point(size = 3)+ylim(c(-18,19))\npcsplot + geom_hline(yintercept = 0, linetype = 2) +\n  geom_vline(xintercept = 0, linetype = 2)__\n\n그림 7.30: mat1 데이터에 대한 첫 번째 주평면. 분산의 59%를 설명합니다.\n__\n질문 7.30\n그림 7.30을 보면, 샘플들이 평면에 무작위로 배치된 것처럼 보이나요? 레이블에 의해 설명되는 어떤 구조를 알아차릴 수 있나요?\n__\n해결책\n__\n이 플롯을 만들어 보면 답이 (더욱) 명확해집니다. 녹아웃(Knockouts)은 항상 쌍을 이루는 야생형(wildtype) 샘플 아래에 있습니다. 우리는 다음 장에서 지도 방식의 다변량 방법(supervised multivariate methods)을 살펴볼 때 이 예시를 다시 방문할 것입니다.\npcsplot + geom_line(colour = \"red\")__\n\n\n9.7.2 7.8.2 바이플롯 및 스케일링\n이전 예제에서는 측정된 변수의 수가 너무 많아서 변수와 샘플을 유용하게 동시에 플로팅할 수 없었습니다. 이 예제에서는 범주형 wine.class 변수도 있는 서로 다른 와인에 대해 화학적 측정이 이루어진 간단한 데이터 세트의 PCA 바이플롯을 그립니다. 우리는 2차원…을 살펴보는 것으로 분석을 시작합니다. correlations and a heatmap of the variables.\nlibrary(\"pheatmap\")\nload(\"../data/wine.RData\")\nload(\"../data/wineClass.RData\")\nwine[1:2, 1:7]__\n\n\n  Alcohol MalicAcid  Ash AlcAsh  Mg Phenols Flav\n1   14.23      1.71 2.43   15.6 127    2.80 3.06\n2   13.20      1.78 2.14   11.2 100    2.65 2.76\n\n\npheatmap(1 - cor(wine), treeheight_row = 0.2)__\n\nFigure 7.31: The difference between 1 and the correlation can be used as a distance between variables and is used to make a heatmap of the associations between the variables.\nA biplot is a simultaneous representation of both the space of observations and the space of variables. In the case of a PCA biplot like Figure 7.32 the arrows represent the directions of the old variables as they project onto the plane defined by the first two new axes. Here the observations are just colored dots, the color has been chosen according to which type of wine is being plotted. We can interpret the variables’ directions with regards to the sample points, for instance the blue points are from the barbera group and show higher Malic Acid content than the other wines.\nwinePCAd = dudi.pca(wine, scannf=FALSE)\ntable(wine.class)__\n\n\nwine.class\n    barolo grignolino    barbera \n        59         71         48 \n\n\nfviz_pca_biplot(winePCAd, geom = \"point\", habillage = wine.class,\n   col.var = \"violet\", addEllipses = TRUE, ellipse.level = 0.69) +\n   ggtitle(\"\") + coord_fixed()__\n\nFigure 7.32: PCA biplot including ellipses for the three types of wine: barolo, grignolino and barbera. For each ellipsis, the axis lengths are given by one standard deviation. Small angles between the vectors Phenols, Flav and Proa indicate that they are strongly correlated, whereas Hue and Alcohol are uncorrelated.\nInterpretation of multivariate plots requires the use of as much of the available information as possible; here we have used the samples and their groups as well as the variables to understand the main differences between the wines.\n\n\n9.7.3 7.8.3 An example of weighted PCA\nSometimes we want to see variability between different groups or observations, but want to weight them. This can be the case if, e.g., the groups have very different sizes. Let’s re-examine the Hiiragi data we already saw in Chapter 3. In the code below, we select the wildtype (WT) samples and the top 100 features with the highest overall variance.\ndata(\"x\", package = \"Hiiragi2013\")\nxwt = x[, x$genotype == \"WT\"]\nsel = order(rowVars(Biobase::exprs(xwt)), decreasing = TRUE)[1:100]\nxwt = xwt[sel, ]\ntab = table(xwt$sampleGroup)\ntab __\n\n\n     E3.25 E3.5 (EPI)  E3.5 (PE) E4.5 (EPI)  E4.5 (PE) \n        36         11         11          4          4 \n\n\nxwt$weight = 1 / as.numeric(tab[xwt$sampleGroup])\npcaMouse = dudi.pca(as.data.frame(t(Biobase::exprs(xwt))),\n  row.w = xwt$weight,\n  center = TRUE, scale = TRUE, nf = 2, scannf = FALSE)\nfviz_eig(pcaMouse) + ggtitle(\"\")__\n\n그림 7.33: Hiiragi 데이터의 가중 PCA에서 얻은 스크리 플롯. 두 번째 고윳값 이후의 급격한 하락은 2차원 PCA가 적절함을 시사합니다.\nfviz_pca_ind(pcaMouse, geom = \"point\", col.ind = xwt$sampleGroup) +\n  ggtitle(\"\") + coord_fixed()__\n우리는 tab으로부터 그룹들이 다소 불균등하게 대표되어 있음을 알 수 있습니다. 이를 설명하기 위해, 우리는 각 샘플에 해당 그룹 크기의 역수로 가중치를 다시 부여합니다. ade4 패키지의 dudi.pca 함수에는 가중치를 입력할 수 있는 row.w 인수가 있습니다. 코드의 출력은 그림 7.33 및 7.34에 있습니다.\n\n그림 7.34: Hiiragi 데이터에 대한 가중 PCA 출력. 샘플들은 그룹에 따라 색상이 입혀져 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "07-chap.html#이-장의-요약",
    "href": "07-chap.html#이-장의-요약",
    "title": "9  7.1 이 장의 목표",
    "section": "9.8 7.9 이 장의 요약",
    "text": "9.8 7.9 이 장의 요약\n행렬 전처리 다변량 데이터 분석은 “의식적인” 전처리를 필요로 합니다. 모든 평균, 분산 및 1차원 히스토그램을 확인한 후, 우리는 데이터를 어떻게 재스케일링하고 중앙화하는지 보았습니다.\n새로운 변수로의 투영 우리는 고차원 데이터를 너무 많은 정보를 잃지 않으면서 낮은 차원(2D 평면과 3D가 가장 자주 사용됨)으로 투영하는 방법을 보았습니다. PCA는 원래의(기존) 변수들의 선형 결합인, 새로운 “더 정보가 많은” 변수들을 찾습니다.\n행렬 분해 PCA는 SVD라 불리는 행렬 \\(X\\)의 분해를 찾는 것에 기반합니다. 이 분해는 낮은 계수의 근사를 제공하며 \\(X^tX\\)의 고유 분해와 동일합니다. 특잇값의 제곱은 새로운 변수들의 고윳값 및 분산과 같습니다. 우리는 데이터의 신호를 재현하는 데 얼마나 많은 축이 필요한지 결정하기 전에 이러한 값들을 체계적으로 플롯했습니다.\n매우 가까운 두 고윳값은 매우 불안정한 점수(scores) 또는 PC 점수를 발생시킬 수 있습니다. 고윳값의 스크리 플롯을 살펴보고 이러한 가까운 고윳값에 해당하는 축들을 분리하는 것을 피하는 것이 항상 필요합니다. 이를 위해 여러 R 패키지에서 사용 가능한 대화형 3D 또는 4D 투영을 사용해야 할 수도 있습니다.\n바이플롯(Biplot) 표현 관측치의 공간은 자연스럽게 \\(p\\)차원 공간입니다 (\\(p\\)개의 원래 변수가 좌표를 제공합니다). 변수의 공간은 \\(n\\)차원입니다. 우리가 공부한 두 분해(특잇값/고윳값 및 특잇값 벡터/고유벡터)는 이 두 공간 모두에 대해 새로운 좌표를 제공하며, 때때로 우리는 하나를 다른 하나의 쌍대(dual)라고 부릅니다. 우리는 동일한 고유벡터 상에 관측치와 변수 모두의 투영을 플롯할 수 있습니다. 이는 PCA 출력을 해석하는 데 유용한 바이플롯을 제공합니다.\n다른 그룹 변수의 투영 PCA의 해석은 관측치에 대한 중복되거나 인접한 데이터를 통해서도 용이해질 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "07-chap.html#더-읽을거리",
    "href": "07-chap.html#더-읽을거리",
    "title": "9  7.1 이 장의 목표",
    "section": "9.9 7.10 더 읽을거리",
    "text": "9.9 7.10 더 읽을거리\n특잇값 분해에 대한 이해를 깊게 하는 가장 좋은 방법은 Strang (2009)의 7장을 읽는 것입니다. 책 전체가 행렬의 계수의 의미와 행 공간과 열 공간 사이의 쌍대성(Holmes 2006)을 이해하는 데 필요한 선형 대수학의 기초를 다집니다.\nPCA 및 관련 방법들에 대해 완전한 교과서들이 쓰여졌습니다. Mardia, Kent, Bibby (1979)는 선형 대수와 행렬을 사용하여 고전적인 방식으로 모든 다변량 방법을 다루는 표준 텍스트입니다. 데이터가 다변량 정규 분포에서 나온다는 모수적 가정을 함으로써, Mardia, Kent, Bibby (1979)는 성분 수에 대한 추론적 검정과 주성분에 대한 한계 속성도 제공합니다. Jolliffe (2002)는 광범위한 예제와 함께 PCA와 관련된 모든 것을 다루는 단행본입니다.\n우리는 관측치와 변수에 대한 가중치에 보충 정보를 통합할 수 있습니다. 이는 1970년대 프랑스 데이터 과학자들에 의해 도입되었습니다. 리뷰는 Holmes (2006)를, 추가 예제는 9장을 참조하십시오.\nPCA의 해석과 안정성에 대한 개선은 선형 결합에 나타나는 0이 아닌 계수의 수를 최소화하는 페널티를 추가함으로써 얻어질 수 있습니다. Zou, Hastie, Tibshirani (2006)와 Witten, Tibshirani, Hastie (2009)는 희소(sparse) 버전의 주성분 분석을 개발했으며, 그들의 패키지 elasticnet 및 PMA는 R에서의 구현체를 제공합니다.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "07-chap.html#연습-문제",
    "href": "07-chap.html#연습-문제",
    "title": "9  7.1 이 장의 목표",
    "section": "9.10 7.11 연습 문제",
    "text": "9.10 7.11 연습 문제\n__\n연습 문제 7.1\nSVD에 관한 위키백과 문서의 섹션 1, 2, 3을 읽어 svd에 관한 내용을 복습하세요. 또한 행렬의 고유 분해에 관한 위키백과 문서의 섹션 1, 2, 2.1을 읽어 관련 고유 분해에 대해 읽어보는 것도 유익할 것입니다. 우리는 \\(n\\)행 \\(p\\)열의 계수 1인 행렬 \\(X\\)를 다음과 같이 분해할 수 있음을 알고 있습니다:\n\\[ X = ]\n\n\\(X\\)에 모든 값이 0인 행이나 열이 없다면, 이 분해는 유일할까요?\n계수가 1인 행렬을 생성해 보세요. 먼저 2부터 30까지 2씩 증가하는 길이 15의 벡터와, 3, 6, 9, 12의 값을 가진 길이 4의 벡터를 만든 다음, 이들의 외적(outer product)을 구하세요.\n\nu = seq(2, 30, by = 2)\nv = seq(3, 12, by = 3)\nX1 = u %*% t(v)__\n왜 t(v)를 취해야 할까요?\n\n이제 Materr라고 부르는 행렬 형태의 노이즈를 추가하여 “거의 계수가 1인” 행렬을 만듭니다.\n\nMaterr = matrix(rnorm(60,1),nrow=15,ncol=4)\nX = X1+Materr __\nggplot을 사용하여 \\(X\\)를 시각화하세요.\n\n계수가 2인 행렬에 대해서도 동일한 분석을 수행해 보세요.\n\n__\n해결책\n__\nX1은 다음과 같이 계산될 수도 있음에 주목하세요.\nouter(u, v)__\n\n\nggplot(data=data.frame(X), aes(x=X1, y=X2, col=X3, size=X4)) + geom_point()__\n여기서 우리는 데이터가 모든 4개 차원에서 선형으로 보임을 알 수 있습니다. 이것이 계수가 1이라는 것이 의미하는 바입니다. 이제 계수가 2인 행렬을 고려해 봅시다.\nn = 100\np = 4\nY2 = outer(rnorm(n), rnorm(p)) + outer(rnorm(n), rnorm(p))\nhead(Y2)__\n\n\n            [,1]       [,2]         [,3]        [,4]\n[1,] -0.44143871  2.3213197  0.433215525 -1.35523790\n[2,]  0.79620920 -1.0748037  1.217052906 -1.13096295\n[3,]  0.16787281  0.2259296  0.547203332 -0.75836031\n[4,]  0.87269426 -1.9208649  0.856966180 -0.38621340\n[5,]  0.03751521 -0.1480678 -0.005217966  0.05864122\n[6,]  0.50195482 -2.0409896 -0.108241027  0.85336630\n\n\nggplot(data=data.frame(Y2), aes(x=X1, y=X2, col=X3, size=X4)) + geom_point()__\n데이터를 처음 두 좌표(R에서 행렬을 데이터 프레임으로 변환할 때 기본적으로 X1 및 X2라고 불림)로 투영하면 데이터가 두 차원 모두에서 변하므로, 이제 분명히 적어도 두 개의 차원이 존재합니다. 따라서 다음 단계는 두 개 이상의 차원이 있는지 결정하는 것입니다. 오른쪽 상단의 점들이 여러분과 가장 가깝고(가장 큼), 플롯에서 아래쪽과 왼쪽으로 갈수록 그 점들은 더 멀어집니다. 왼쪽에는 가장 파란 점들이 있고 오른쪽으로 갈수록 선형적으로 더 어두워지는 것 같습니다. 아마 짐작하시겠지만, “고차원”이 단지 4차원을 의미할 때조차 고차원에서 저차원 공간을 시각적으로 발견하는 것은 매우 어렵습니다. 이것이 우리가 특잇값 분해에 의존하는 한 이유입니다.\nsvd(Y2)$d # 0이 아닌 2개의 고윳값 __\n\n\n[1] 2.637465e+01 1.266346e+01 3.144564e-15 1.023131e-15\n\n\nY = Y2 + matrix(rnorm(n*p, sd=0.01),n,p) # Y2에 노이즈 추가\nsvd(Y)$d # 4개의 0이 아닌 고윳값 (하지만 큰 것은 2개뿐)__\n\n\n[1] 26.39673712 12.68547439  0.10735103  0.09104741\n여기서 우리는 0이 아닌 2개의 차원과 거의 0인 2개의 차원(“Y2”의 경우, 컴퓨터 허용 오차의 제곱근 내에서 0임)을 갖습니다.\n__\n연습 문제 7.2\n\n먼저 그림 7.35에 표시된 것과 같은 상관관계가 높은 이변량 데이터 행렬을 생성하세요.\n힌트: mvrnorm 함수를 사용하세요.\n\n특잇값을 살펴보고 행렬의 계수를 확인하세요.\n\nPCA를 수행하고 회전된 주성분 축을 보여주세요.\n\n__\n해결책\n__\n\n다음을 사용하여 상관관계가 있는 이변량 정규 데이터를 생성합니다:\n\nlibrary(\"MASS\")\nmu1 = 1; mu2 = 2; s1=2.5; s2=0.8; rho=0.9;\nsigma = matrix(c(s1^2, s1*s2*rho, s1*s2*rho, s2^2),2)\nsim2d = data.frame(mvrnorm(50, mu = c(mu1,mu2), Sigma = sigma))\nsvd(scale(sim2d))$d __\n\n\n[1] 9.647686 2.218592\n\n\nsvd(scale(sim2d))$v[,1]__\n\n\n[1] 0.7071068 0.7071068\n\nprcomp를 사용하여 PCA를 수행하고 점수(scores)를 통해 원하는 회전을 제공합니다.\n\nrespc = princomp(sim2d)\ndfpc  = data.frame(pc1=respc$scores[,1], \n                   pc2=respc$scores[,2])\n\nggplot(data.frame(sim2d), aes(x=X1,y=X2)) + geom_point()\nggplot(dfpc, aes(x=pc1, y=pc2)) + geom_point() + coord_fixed(2)__\n\n\n\\(\\)\n\n\n\n\\(\\)\n\n그림 7.35: 산점도 (A)에 표시된 원래 데이터와 주성분 회전 (B)을 사용하여 얻은 플롯.\n__\n연습 문제 7.3\n그림 7.35의 (a) 부분은 매우 길쭉한 플로팅 영역을 보여주는데, 그 이유는 무엇인가요?\ncoord_fixed() 옵션을 사용하지 않고 정사각형 플로팅 존을 사용하면 어떤 일이 발생하나요? 왜 이것이 오해를 불러일으킬 수 있을까요?\n__\n연습 문제 7.4\nHiiragi 데이터를 다시 살펴보고 가중 및 비가중 접근 방식을 비교해 봅시다.\n\n가중치를 부여하지 않은 Hiiragi 데이터 xwt에 대해 상관관계 원을 만드세요. 어떤 유전자들이 첫 번째 주평면에 가장 잘 투영(가장 좋은 근사)되나요?\n첫 번째 평면에서 대부분의 분산을 설명하는 극단적인 유전자 변수들의 레이블을 보여주는 바이플롯을 만드세요. 샘플 포인트들도 추가하십시오.\n\nAbbott, Edwin A. 1884. Flatland: A Romance of Many Dimensions. OUP Oxford.\nFlury, Bernard. 1997. A First Course in Multivariate Statistics. Springer.\nHolmes, Susan. 2006. “Multivariate Analysis: The French way.” In Probability and Statistics: Essays in Honor of David a. Freedman , edited by D. Nolan and T. P. Speed. Vol. 56. IMS Lecture Notes–Monograph Series. Beachwood, OH: IMS. http://www.imstat.org/publications/lecnotes.htm.\nHolmes, Susan, Michael He, Tong Xu, and Peter P Lee. 2005. “Memory t Cells Have Gene Expression Patterns Intermediate Between Naive and Effector.” PNAS 102 (15): 5519–23.\nHotelling, Harold. 1933. “Analysis of a Complex of Statistical Variables into Principal Components.” Journal of Educational Psychology 24 (6): 417–41.\nJolicoeur, Pierre, and James E Mosimann. 1960. “Size and Shape Variation in the Painted Turtle. A Principal Component Analysis.” Growth 24: 339–54.\nJolliffe, Ian. 2002. Principal Component Analysis. Wiley Online Library.\nMardia, Kanti, John T Kent, and John M Bibby. 1979. Multiariate Analysis. New York: Academic Press.\nPearson, Karl. 1901. “LIII. On Lines and Planes of Closest Fit to Systems of Points in Space.” The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 2 (11): 559–72.\nStrang, Gilbert. 2009. Introduction to Linear Algebra. Fourth. Wellesley- Cambridge Press.\nWitten, Daniela M, Robert Tibshirani, and Trevor Hastie. 2009. “A Penalized Matrix Decomposition, with Applications to Sparse Principal Components and Canonical Correlation Analysis.” Biostatistics , kxp008.\nZou, Hui, Trevor Hastie, and Robert Tibshirani. 2006. “Sparse Principal Component Analysis.” Journal of Computational and Graphical Statistics 15 (2): 265–86.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>7.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "08-chap.html",
    "href": "08-chap.html",
    "title": "10  8.1 이 장의 목표",
    "section": "",
    "text": "10.1 8.2 몇 가지 핵심 개념\n생명공학의 많은 측정 장치들은 분자의 대규모 병렬 샘플링과 카운팅에 기초합니다. 한 가지 예로 고처리량 DNA 시퀀싱(high-throughput DNA sequencing)이 있습니다. 그 응용 분야는 데이터 출력 방식에 따라 크게 두 가지 주요 클래스로 나뉩니다: 첫 번째 경우, 관심 있는 출력은 서열 그 자체이며, 아마도 그들의 다형성(polymorphisms)이나 이전에 보았던 다른 서열들과의 차이일 것입니다. 두 번째 경우, 서열 그 자체는 어느 정도 잘 이해되어 있으며(예를 들어, 잘 조립되고 어노테이션된 게놈을 가지고 있음), 우리의 관심은 샘플 내에서 서로 다른 서열 영역들이 얼마나 풍부하게 존재하는지에 있습니다.\n예를 들어, RNA-Seq (Ozsolak and Milos 2011)에서는 세포 집단이나 조직에서 발견되는 RNA 분자들의 서열을 분석합니다.\n엄밀히 말하면, 우리는 RNA의 서열을 분석하는 것이 아니라 역전사를 통해 얻은 상보적 DNA(cDNA)의 서열을 분석합니다. 전체 RNA 풀은 폴리-A 선택(poly-A selection)이나 리보솜 RNA 고갈(ribosomal RNA depletion)과 같은 생화학적 수단을 통해 관심 있는 하위 집합(예: 메신저 RNA)으로 줄어들 수 있습니다. 단일 세포와 대량의 세포를 분석할 수 있게 해주는 RNA-Seq의 민감한 변체들이 존재합니다.\nChIP-Seq에서는 특정 DNA 결합 단백질에 결합된 DNA 영역(면역 침전을 통해 선택됨)의 서열을 분석합니다. RIP-Seq에서는 특정 RNA 결합 단백질에 결합된 RNA 분자나 그 영역을, DNA-Seq에서는 게놈 DNA의 서열을 분석하며 이질적인 세포 집단에서의 유전적 변이의 유병률(예: 종양의 클론 구성)에 관심을 가집니다. 고처리량 염색질 구조 포착(HiC)에서는 DNA의 3차원 공간적 배치를 매핑하는 것을 목표로 하며, 유전적 스크리닝(변동을 위해 RNAi나 CRISPR-Cas9 라이브러리를 사용하고 리드아웃을 위해 고처리량 시퀀싱을 사용하는 경우)에서는 유전자 넉다운(knockdown), 넉아웃(knockout) 또는 수정에 따른 세포의 증식이나 생존에 관심을 가집니다. 마이크로바이옴 분석에서는 복잡한 미생물 서식지에 있는 서로 다른 미생물 종의 풍부도를 연구합니다.\n이상적으로는 샘플 내의 관심 있는 모든 분자를 시퀀싱하고 세고 싶을 것입니다. 일반적으로 이는 불가능합니다: 생화학적 프로토콜은 100% 효율적이지 않으며, 일부 분자나 중간 생성물은 과정 중에 소실됩니다. 게다가 대개 그럴 필요조차 없습니다. 대신, 우리는 _통계적 표본(statistical sample)_을 시퀀싱하고 셉니다. 표본 크기는 분석되는 서열 풀의 복잡성에 따라 달라질 것이며, 수만 개에서 수십억 개에 이를 수 있습니다. 이러한 데이터의 샘플링 특성은 이를 분석할 때 중요합니다. 우리는 샘플링이 충분히 대표성이 있어 흥미로운 경향과 패턴을 식별할 수 있기를 희망합니다.\n이 장에서 우리는 RNA-Seq과 같은 고처리량 시퀀싱 응용 분야의 카운트 데이터(count data)에 익숙해질 것입니다. 데이터를 해석하기 위해 데이터의 근간이 되는 샘플링 프로세스를 이해하고 모델링할 것입니다. 우리의 주요 목표는 서로 다른 조건(예: 처리되지 않은 군 대 처리된 군)의 샘플들 사이에서 체계적인 변화를 감지하고 정량화하는 것이며, 이때의 과제는 그러한 체계적인 변화를 동일한 조건 내에서의 샘플링 변동 및 실험적 가변성과 구별하는 것입니다. 이를 위해 다음과 같은 필요한 통계적 개념과 도구들을 갖출 것입니다:\n사실 이러한 개념들은 훨씬 더 넓은 범위의 응용 분야를 가집니다: 이들은 어떤 실험적 공변량의 함수로서 노이즈가 섞인 데이터의 차이를 감지하고자 하는 다른 유형의 데이터에도 적용될 수 있습니다. 특히 일반화 선형 모델의 프레임워크는 상당히 추상적이고 일반적이지만, 이는 많은 서로 다른 데이터 유형에 맞춰 조정될 수 있다는 장점이 있습니다. 따라서 우리는 바퀴를 새로 발명할 필요 없이 관련된 광범위한 도구와 진단 기능을 즉시 즐길 수 있습니다.\n보너스로, 5장과 7장에서 보았던 비지도 학습 방법들에 데이터를 적합하게 만들고 데이터 시각화를 더 쉽게 만들어주는 데이터 변환에 대해서도 살펴볼 것입니다.\n시작하기 전에 몇 가지 주요 용어를 정리해 봅시다.\n1 https://www.illumina.com/techniques/sequencing.html를 참조하십시오.\n2 특정 응용 분야의 경우, 가장 적절한 접근 방식과 선택에 대해 최신 문헌을 확인하는 것이 가장 좋습니다.\n3 예: RNA-Seq의 경우, 게놈과 그 전사체에 대한 어노테이션.\n시퀀싱과 카운팅 사이에는 함께 속하는 서열들을 모으는 중요한 집계(aggregation) 또는 군집화 단계가 포함됩니다: 예를 들어, (RNA-Seq에서) 동일한 유전자에 속하는 모든 리드나, (ChIP-Seq에서) 동일한 결합 영역에 속하는 모든 리드를 모으는 것입니다. 실험의 목적에 따라 이에 대한 여러 접근 방식과 선택 사항이 있습니다2. 방법에는 참조 서열(reference sequence)에 대한 명시적인 정렬(alignment) 또는 해시 기반 매핑3, 그리고 리드들의 참조 독립적인 서열 유사성 기반 군집화가 포함됩니다 — 특히 메타제노믹스나 메타전사체학에서와 같이 명확한 참조 서열이 없는 경우에 그렇습니다. 우리는 서로 다른 대립유전자(alleles)나 이소형(isoforms)을 별도로 고려할지, 아니면 이들을 하나의 동등 클래스(equivalence class)로 병합할지 선택해야 합니다. 단순함을 위해 이 장에서는 특정 응용 분야에 따라 다양한 대상이 될 수 있음에도 불구하고 이러한 운영상의 집계 단위에 대해 _유전자(gene)_라는 용어를 사용하겠습니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "08-chap.html#몇-가지-핵심-개념",
    "href": "08-chap.html#몇-가지-핵심-개념",
    "title": "10  8.1 이 장의 목표",
    "section": "",
    "text": "_시퀀싱 라이브러리(sequencing library)_는 시퀀싱 장비의 입력으로 사용되는 DNA 분자들의 집합입니다.\n_단편(Fragments)_은 시퀀싱되는 분자들입니다. 현재 가장 널리 사용되는 기술1은 길이가 약 300~1000 뉴클레오타이드인 분자만 처리할 수 있기 때문에, 이들은 관심 있는 (일반적으로 더 긴) DNA나 cDNA 분자를 단편화하여 얻어집니다.\n리드(read)는 단편으로부터 얻은 서열입니다. 현재 기술로 리드는 단편 전체가 아니라 그 단편의 한쪽 끝 또는 양쪽 끝만을 포괄하며, 양쪽의 리드 길이는 최대 약 150 뉴클레오타이드입니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "08-chap.html#카운트-데이터count-data",
    "href": "08-chap.html#카운트-데이터count-data",
    "title": "10  8.1 이 장의 목표",
    "section": "10.2 8.3 카운트 데이터(Count data)",
    "text": "10.2 8.3 카운트 데이터(Count data)\n예시 데이터 세트를 불러와 봅시다. 이 데이터는 실험 데이터 패키지인 pasilla에 들어 있습니다.\nfn = system.file(\"extdata\", \"pasilla_gene_counts.tsv\",\n                  package = \"pasilla\", mustWork = TRUE)\ncounts = as.matrix(read.csv(fn, sep = \"\\t\", row.names = \"gene_id\"))__\n\n\n\n여기서 보여주는 코드에서는 pasilla 패키지와 함께 제공되는 파일을 찾기 위해 system.file 함수를 사용합니다. 여러분 자신의 데이터로 작업할 때는 counts 행렬을 직접 준비해야 할 것입니다.\n\n\n여기서 보여주는 코드에서는 system.file 함수를 사용하여 pasilla 패키지와 함께 제공되는 파일을 찾습니다. 여러분 자신의 데이터로 작업할 때는 counts 행렬을 직접 준비해야 할 것입니다.\n데이터는 탭으로 구분된 파일에 직사각형 표 형태로 저장되어 있으며, 이를 counts 행렬로 읽어 들였습니다.\ndim(counts)__\n\n\n[1] 14599     7\n\n\ncounts[ 2000+(0:3), ]__\n\n\n            untreated1 untreated2 untreated3 untreated4 treated1 treated2\nFBgn0020369       3387       4295       1315       1853     4884     2133\nFBgn0020370       3186       4305       1824       2094     3525     1973\nFBgn0020371          1          0          1          1        1        0\nFBgn0020372         38         84         29         28       63       28\n            treated3\nFBgn0020369     2165\nFBgn0020370     2120\nFBgn0020371        0\nFBgn0020372       27\n이 행렬은 각 샘플에서 각 유전자에 대해 관찰된 리드의 수를 집계합니다. 우리는 이를 카운트 테이블(count table)이라고 부릅니다. 유전자에 해당하는 14,599개의 행과 샘플에 해당하는 7개의 열로 이루어져 있습니다. 파일에서 데이터를 불러올 때 좋은 타당성 검사는 데이터의 일부를 출력해 보는 것인데, 위에서 했던 것처럼 아주 앞부분뿐만 아니라 중간의 무작위 지점도 확인해 보는 것이 좋습니다.\n이 표는 정수 값들의 행렬입니다: 행렬의 \\(i\\)번째 행과 \\(j\\)번째 열의 값은 샘플 \\(j\\)에서 유전자 \\(i\\)에 매핑된 리드가 몇 개인지를 나타냅니다. 이 장에서 논의할 통계적 샘플링 모델은 이 값들이 시퀀싱 리드의 직접적이고 “가공되지 않은(raw)” 카운트라는 사실에 의존합니다 — 정규화된 카운트나 포괄된 염기쌍의 수와 같은 파생된 수치가 아닙니다. 그러한 수치들은 무의미한 결과로 이어질 뿐입니다.\n\n10.2.1 8.3.1 카운트 데이터의 과제들\n우리가 이러한 카운트 데이터로 극복해야 할 과제들은 무엇일까요?\n\n데이터는 0부터 수백만까지 넓은 동적 범위를 가집니다. 동적 범위의 서로 다른 부분에서 데이터의 분산, 그리고 더 일반적으로 분포의 형태가 매우 다릅니다. 우리는 이분산성(heteroskedasticity)이라고 불리는 이러한 현상을 고려해야 합니다.\n데이터는 음수가 아닌 정수이며, 그 분포는 대칭적이지 않습니다 — 따라서 정규 분포나 로그-정규 분포 모델은 적합도가 낮을 수 있습니다.\n우리는 체계적인 샘플링 편향을 이해하고 이를 보정해야 합니다. 혼란스럽게도 이를 흔히 정규화(normalization)라고 부릅니다. 예로는 실험의 전체 시퀀싱 깊이(두 라이브러리에서 한 유전자의 실제 풍부도가 같더라도, 시퀀싱된 전체 리드 수에 따라 해당 유전자에 대해 서로 다른 리드 수를 예상함), 또는 서로 다른 샘플링 확률(생물학적 샘플 내에서 두 유전자의 실제 풍부도가 같더라도 길이, GC 함량, 2차 구조, 결합 파트너와 같은 생물물리학적 특성이 다르면 서로 다른 리드 수를 예상함) 등이 있습니다.\n우리는 샘플링의 확률적 특성뿐만 아니라 다른 확률적 실험 변동의 원인들도 이해해야 합니다. 생물학적 샘플 수가 많은 연구의 경우 이는 대개 간단하며, 재표본 추출이나 순열 기반 방법에 의존할 수도 있습니다. 그러나 설계된 실험(designed experiments)의 경우 표본 크기가 제한적인 경향이 있습니다.\n\n\n\n\n실험(experiments)과 연구(studies) 사이에는 중요한 개념적 및 실무적 차이가 있습니다 – 13장을 참조하십시오.\n\n\n실험(experiments)과 연구(studies) 사이에는 중요한 개념적 및 실무적 차이가 있습니다 — 13장을 참조하십시오.\n예를 들어, pasilla 데이터에는 처리되지 않은 군에서 4개의 반복(replicates)이, 처리된 군에서 3개의 반복이 있습니다. 이는 재표본 추출이나 순열 기반 방법이 충분한 검정력을 갖지 못함을 의미합니다. 진행하기 위해 우리는 분포에 대한 가정을 해야 합니다. 본질적으로 그러한 가정들이 하는 일은 소수의 분포 매개변수로부터 분포의 꼬리 부분에 있는 드문 사건의 확률 — 즉, 비정상적으로 높거나 낮은 카운트 — 을 계산할 수 있게 해주는 것입니다.\n\n하지만 그것만으로도 부족한 경우가 많은데, 특히 분산(dispersion) 매개변수4의 추정은 작은 표본 크기에서 어렵습니다. 이 경우, 비슷한 위치에 있는 유전자들은 비슷한 분산을 가진다는 것과 같은 추가적인 가정을 해야 합니다. 이를 유전자 간 정보 공유(sharing of information across genes)라고 하며, 8.10.1절에서 다시 다룰 것입니다.\n\n4 분포는 다양한 방식으로 매개변수화될 수 있습니다. 종종 매개변수들은 위치(location)의 척도와 분산(dispersion)의 척도에 대응합니다. 친숙한 위치의 척도는 평균이고, 친숙한 분산의 척도는 분산(또는 표준 편차)이지만, 일부 분포의 경우 다른 척도들이 사용되기도 합니다.\n\n\n10.2.2 8.3.2 RNA-Seq: 유전자 구조, 스플라이싱, 이소형은 어떠한가요?\n진핵생물의 유전자는 복잡합니다: 대부분의 유전자는 여러 개의 엑손으로 구성되며, mRNA는 스플라이싱(splicing)이라는 과정을 통해 엑손들이 연결되어 만들어집니다. 대안적 스플라이싱(alternative splicing)과 전사 시작 및 종료 지점의 다양한 선택은 동일한 유전자 좌위(locus)로부터 여러 개의 대안적 이소형(alternative isoforms) 생성을 가능하게 합니다. 고처리량 시퀀싱을 사용하여 전사체의 이소형 구조를 탐지하는 것이 가능합니다. 특정 이소형에 특징적인 단편들로부터 이소형 특이적 풍부도를 탐지하는 것도 가능합니다. 전체 길이 이소형의 비교적 짧은 단편들만을 제공하는 현재의 RNA-Seq 데이터로는 전체 길이 이소형 구조와 풍부도를 조립하고 분리해내는 것이 어려운 경향이 있습니다 (Steijger et al. 2013). 이 때문에 국소적인 판단(예: 개별 엑손의 포함 또는 제외)만을 내리는 좀 더 겸손한 목표를 가진 절차들이 공식화되었으며 (Anders, Reyes, and Huber 2012), 이들이 더 강건(robust)할 수 있습니다. 미래의 기술들은 전체 길이 전사체의 서열을 분석할 것으로 기대할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "08-chap.html#카운트-데이터-모델링",
    "href": "08-chap.html#카운트-데이터-모델링",
    "title": "10  8.1 이 장의 목표",
    "section": "10.3 8.4 카운트 데이터 모델링",
    "text": "10.3 8.4 카운트 데이터 모델링\n\n10.3.1 8.4.1 분산(Dispersion)\n유전자 1에 해당하는 단편 \\(n_1\\)개, 유전자 2에 해당하는 단편 \\(n_2\\)개 등을 포함하며 전체 라이브러리 크기가 \\(n = n_1+n_2+\\)인 시퀀싱 라이브러리를 생각해 봅시다. 우리는 라이브러리를 시퀀싱하여 무작위로 샘플링된 \\(r\\)개 단편의 정체를 결정합니다. 이 숫자들의 크기 정도를 살펴보면 반가운 단순화가 가능해집니다:\n\n유전자의 수는 수만 개입니다.\n\\(n\\)의 값은 준비에 사용된 세포의 양에 따라 달라지지만, 벌크(bulk) RNA-Seq의 경우 수십억 또는 수조 개에 달할 것입니다.\n리드 수 \\(r\\)은 보통 수천만 개이며, 따라서 \\(n\\)보다 훨씬 작습니다.\n\n이로부터 우리는 주어진 리드가 \\(i\\)번째 유전자에 매핑될 확률이 \\(p_i=n_i/n\\)이며, 이것이 다른 모든 리드에 대한 결과와 거의 독립적이라는 결론을 내릴 수 있습니다. 따라서 우리는 \\(i\\)번째 유전자에 대한 리드 수를 포아송 분포로 모델링할 수 있으며, 이때 포아송 프로세스의 _비율(rate)_은 \\(i\\)번째 유전자에 대한 단편의 초기 비율인 \\(p_i\\)와 \\(r\\)의 곱, 즉 \\(_i=rp_i\\)가 됩니다.\n\n\n\n원칙적으로 우리는 여기서 비복원 추출과 다항 분포를 고려해야 합니다: i번째 유전자에 대한 리드를 샘플링할 확률은 동일한 유전자와 다른 유전자들이 이미 몇 번 샘플링되었는지에 달려 있습니다. 그러나 이러한 의존성은 무시할 수 있을 정도로 작아서 무시할 것입니다. 이는 n이 r보다 훨씬 크고, 유전자의 수가 많으며, 각 n_i가 n에 비해 작기 때문입니다.\n\n\n원칙적으로 우리는 여기서 비복원 추출(sampling without replacement)과 다항 분포를 고려해야 합니다: \\(i\\)번째 유전자에 대한 리드를 샘플링할 확률은 동일한 유전자와 다른 유전자들이 이미 몇 번 샘플링되었는지에 달려 있습니다. 그러나 이러한 의존성은 무시할 수 있을 정도로 작아서 무시할 것입니다. 이는 \\(n\\)이 \\(r\\)보다 훨씬 크고, 유전자의 수가 많으며, 각 \\(n_i\\)가 \\(n\\)에 비해 작기 때문입니다.\n실제로 우리는 대개 단일 라이브러리 내의 리드 카운트를 모델링하는 데 관심이 있는 것이 아니라, 라이브러리 간의 카운트를 비교하는 데 관심이 있습니다. 즉, 우리는 서로 다른 생물학적 조건 사이에서 — 예를 들어 약물 처리를 한 세포주와 하지 않은 동일한 세포주 사이에서 — 보이는 차이가 “우연히” 예상되는 것보다 큰지, 즉 생물학적 반복 사이에서도 예상할 수 있는 것보다 큰지 알고 싶어 합니다. 경험적으로 반복 실험들은 포아송 분포가 예측하는 것보다 더 많이 변하는 것으로 나타납니다. 직관적으로 일어나는 일은 \\(p_i\\)와 따라서 \\(_i\\)가 생물학적 반복 사이에서도 변한다는 것입니다. 아마도 세포가 자란 온도가 약간 달랐거나, 첨가된 약물의 양이 몇 퍼센트 차이 났거나, 배양 시간이 약간 더 길었을 수 있습니다. 이를 설명하기 위해 우리는 그 위에 또 다른 모델링 층을 추가해야 합니다. 우리는 이미 4장에서 계층 모델과 혼합물을 보았습니다. 감마-포아송(일명 음이항) 분포가 우리의 모델링 요구에 적합하다는 것이 밝혀졌습니다. 평균과… 모두를 나타내는 단일 \\(\\) 대신… variance –, this distribution has two parameters. In principle, these can be different for each gene, and we will come back to the question of how to estimate them from the data.\n\n\n10.3.2 8.4.2 Normalization\nOften, there are systematic biases that have affected the data generation and are worth taking into account. Unfortunately, the term normalization is commonly used for that aspect of the analysis, even though it is misleading: it has nothing to do with the normal distribution, norms in a vector space, or normal vectors. Rather, what we aim for is identifying the nature and estimating the magnitude of systematic biases, and take them into account in our model-based analysis of the data.\nThe most important systematic bias stems from variations in the total number of reads in each sample. If we have more reads for one library than in another, then we might assume that, everything else being equal, the counts are proportional to each other with some proportionality factor \\(s\\). Naively, we could propose that a decent estimate of \\(s\\) for each sample is simply given by the sum of the counts of all genes. However, it turns out that we can do better. To understand this, a toy example helps.\n\nFigure 8.1: Size factor estimation. The points correspond to hypothetical genes whose counts in two samples are indicated by their \\(x\\)- and \\(y\\)-coordinates. The lines indicate two different ways of size factor estimation explained in the text.\nConsider a dataset with 5 genes and two samples as displayed in Figure 8.1. If we estimate \\(s\\) for each of the two samples by its sum of counts, then the slope of the blue line represents their ratio. According to this, gene C is down-regulated in sample 2 compared to sample 1, while the other genes are all somewhat up-regulated. If we now instead estimate \\(s\\) such that their ratios correspond to the red line, then we will still conclude that gene C is down-regulated, while the other genes are unchanged. The second version is more parsimonious and is often preferred by scientists. The slope of the red line can be obtained by robust regression. This is what the DESeq2 method does.\n__\nQuestion 8.1\nFor the example dataset count of Section 8.3, how does the output of DESeq2 ’s estimateSizeFactorsForMatrix compare to what you get by simply taking the column sums?\n__\nSolution\n__\nSee Figure 8.2, produced by the code below. In this case, there is not much difference, the results are nearly proportional.\nlibrary(\"tibble\")\nlibrary(\"ggplot2\")\nlibrary(\"DESeq2\")\nggplot(tibble(\n  `size factor` = estimateSizeFactorsForMatrix(counts),\n  `sum` = colSums(counts)), aes(x = `size factor`, y = `sum`)) +\n  geom_point()__\n\nFigure 8.2: Size factors versus sums for the pasilla data.\n__\nTask\nLocate the R sources for this book and have a look at the code that produces Figure 8.1.\n__\nQuestion 8.2\nPlot the mean-variance relationship for the biological replicates in the pasilla dataset.\n__\nSolution\n__\nSee Figure 8.3, produced by the following code.\nlibrary(\"matrixStats\")\nsf = estimateSizeFactorsForMatrix(counts)\nncounts  = counts / matrix(sf,\n   byrow = TRUE, ncol = ncol(counts), nrow = nrow(counts))\nuncounts = ncounts[, grep(\"^untreated\", colnames(ncounts)),\n                     drop = FALSE]\nggplot(tibble(\n        mean = rowMeans(uncounts),\n        var  = rowVars( uncounts)),\n     aes(x = log(mean), y = log(var))) +\n  geom_hex() + coord_fixed() + theme(legend.position = \"none\") +\n  geom_abline(slope = 1:2, color = c(\"forestgreen\", \"red\"))__\n\nFigure 8.3: Variance versus mean for the (size factor adjusted) counts data. The axes are logarithmic. Also shown are lines through the origin with slopes 1 (green) and 2 (red).\nThe green line (slope 1) is what we expect if the variance (\\(v\\)) equals the mean (\\(m\\)), as is the case for a Poisson-distributed random variable: \\(v=m\\). We see that this approximately fits the data in the lower range. The red line (slope 2) corresponds to the quadratic mean-variance relationship \\(v=m^2\\); lines parallel to it (not shown) would represent \\(v = cm^2\\) for various values of \\(c\\). We can see that in the upper range of the data, the quadratic relationship approximately fits the data, for some value of \\(c&lt;1\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "08-chap.html#a-basic-analysis",
    "href": "08-chap.html#a-basic-analysis",
    "title": "10  8.1 이 장의 목표",
    "section": "10.4 8.5 A basic analysis",
    "text": "10.4 8.5 A basic analysis\n\n10.4.1 8.5.1 Example dataset: the pasilla data\nLet’s return to the pasilla data from Section 8.3. These data are from an experiment on Drosophila melanogaster cell cultures that investigated the effect of RNAi knock-down of the splicing factor pasilla (Brooks et al. 2011) on the cells’ transcriptome. There were two experimental conditions, termed untreated and treated in the header of the count table that we loaded. They correspond to negative control and to siRNA against pasilla. The experimental metadata of the 7 samples in this dataset are provided in a spreadsheet-like table, which we load.\n\n\n\nIn the code shown here, we load the file pasilla_sample_annotation.csv that comes with the pasilla package. We locate it with the function system.file. When you work with your own data, you will need to prepare an analogous file, or directly a dataframe like pasillaSampleAnno.\n\n\nIn the code shown here, we load the file pasilla_sample_annotation.csv that comes with the pasilla package. We locate it with the function system.file. When you work with your own data, you will need to prepare an analogous file, or directly a dataframe like pasillaSampleAnno.\nannotationFile = system.file(\"extdata\",\n  \"pasilla_sample_annotation.csv\",\n  package = \"pasilla\", mustWork = TRUE)\npasillaSampleAnno = readr::read_csv(annotationFile)\npasillaSampleAnno __\n\n\n# A tibble: 7 × 6\n  file    condition type  `number of lanes` total number of read…¹ `exon counts`\n  &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                          &lt;dbl&gt;\n1 treate… treated   sing…                 5 35158667                    15679615\n2 treate… treated   pair…                 2 12242535 (x2)               15620018\n3 treate… treated   pair…                 2 12443664 (x2)               12733865\n4 untrea… untreated sing…                 2 17812866                    14924838\n5 untrea… untreated sing…                 6 34284521                    20764558\n6 untrea… untreated pair…                 2 10542625 (x2)               10283129\n7 untrea… untreated pair…                 2 12214974 (x2)               11653031\n# ℹ abbreviated name: ¹​`total number of reads`\n여기서 보듯이, 전체 데이터 세트는 두 개의 배치(batch)로 생성되었습니다. 첫 번째 배치는 단일 리드(single-read) 시퀀싱을 거친 3개의 시퀀싱 라이브러리로 구성되었고, 두 번째 배치는 쌍말단(paired-end) 시퀀싱이 사용된 4개의 라이브러리로 구성되었습니다. 종종 그렇듯이, 우리는 약간의 데이터 가공(wrangling)이 필요합니다: type 열의 하이픈(-)을 언더스코어(_)로 바꿉니다. 왜냐하면 DESeq2에서는 요인 수준(factor levels)에 산술 연산자를 사용하는 것을 권장하지 않기 때문입니다. 그리고 type과 condition 열을 요인(factors)으로 변환하면서, 우리가 선호하는 수준의 순서를 명시적으로 지정합니다(기본값은 알파벳순입니다).\nlibrary(\"dplyr\")\npasillaSampleAnno = mutate(pasillaSampleAnno,\ncondition = factor(condition, levels = c(\"untreated\", \"treated\")),\ntype = factor(sub(\"-.*\", \"\", type), levels = c(\"single\", \"paired\")))__\n우리는 설계(design)가 관심 요인인 condition과 “성가신 요인(nuisance factor)”인 type 사이에서 대략적으로 균형을 이루고 있음에 주목합니다:\nwith(pasillaSampleAnno,\n       table(condition, type))__\n\n\n           type\ncondition   single paired\n  untreated      2      2\n  treated        1      2\nDESeq2는 작업하는 데이터 세트를 저장하기 위해 DESeqDataSet 이라 불리는 특수 데이터 컨테이너를 사용합니다. 이러한 특수 컨테이너 — 또는 R 용어로 클래스(classes) — 의 사용은 관련 데이터를 함께 묶어두는 데 도움이 되기 때문에 Bioconductor 프로젝트의 공통적인 원칙입니다. 이러한 방식은 _matrix_나 dataframe과 같은 기본 R 데이터 유형만을 사용하는 것에 비해 사용자가 클래스를 이해하기 위해 초기에 약간 더 많은 시간을 투자해야 하지만, 데이터의 관련 부분들 사이의 동기화 손실로 인한 버그를 피하는 데 도움이 됩니다. 또한 기본 용어로 항상 표현한다면 상당히 장황해질 수 있는 일반적인 연산들의 추상화와 캡슐화를 가능하게 합니다5. _DESeqDataSet_은 Bioconductor의 SummarizedExperiment 클래스의 확장입니다. SummarizedExperiment 클래스는 다른 많은 패키지에서도 사용되므로, 이를 다루는 법을 익히면 상당히 다양한 도구들을 사용할 수 있게 될 것입니다.\n5 또 다른 장점은 클래스가 유효성(validity) 메서드를 포함할 수 있다는 것인데, 이는 데이터가 항상 특정 기대치(예: 카운트는 양의 정수여야 함, 카운트 행렬의 열이 샘플 어노테이션 데이터 프레임의 행과 일치해야 함 등)를 충족하는지 확인해 줍니다.\n6 아래 코드에서 counts 객체의 열 이름과 pasillaSampleAnno 데이터 프레임의 file 열을 일치시키기 위해 추가적인 작업이 필요함에 주목하세요. 특히 file 열에서 왠지 모르게 사용된 \"fb\"를 제거해야 합니다. 이러한 데이터 가공은 매우 흔한 일입니다. 데이터를 DESeqDataSet 객체에 저장하는 이유 중 하나는 일단 저장하고 나면 더 이상 그런 걱정을 할 필요가 없기 때문입니다.\n우리는 생성자 함수 DESeqDataSetFromMatrix를 사용하여 카운트 데이터 행렬 counts와 샘플 어노테이션 데이터 프레임 pasillaSampleAnno로부터 _DESeqDataSet_을 만듭니다6.\nmt = match(colnames(counts), sub(\"fb$\", \"\", pasillaSampleAnno$file))\nstopifnot(!any(is.na(mt)))\n\npasilla = DESeqDataSetFromMatrix(\n  countData = counts,\n  colData   = pasillaSampleAnno[mt, ],\n  design    = ~ condition)\nclass(pasilla)__\n\n\n[1] \"DESeqDataSet\"\nattr(,\"package\")\n[1] \"DESeq2\"\n\n\nis(pasilla, \"SummarizedExperiment\")__\n\n\n[1] TRUE\nSummarizedExperiment 클래스 — 따라서 DESeqDataSet — 는 카운트 행렬 행(rows)의 어노테이션을 저장하기 위한 기능도 포함하고 있습니다. 지금은 counts 표의 행 이름에 있는 유전자 식별자로 만족하겠습니다.\n__\n질문 8.3\n우리는 어떻게 SummarizedExperiment 객체의 행 메타데이터에 접근할 수 있을까요? 즉, 어떻게 읽어내고, 어떻게 변경할 수 있을까요?\n__\n해결책\n__\nSummarizedExperiment 클래스와 rowData 및 rowData&lt;- 메서드의 매뉴얼 페이지를 확인해 보세요.\n\n\n10.4.2 8.5.2 DESeq2 방법\n이러한 준비를 마친 후, 우리는 이제 곧바로 차등 발현 분석으로 뛰어들 준비가 되었습니다. 우리의 목표는 처리된 세포와 처리되지 않은 세포 사이에서 풍부도가 차이 나는 유전자를 식별하는 것입니다. 이를 위해 우리는 6.5절에서 접했던 \\(t\\)-검정과 개념적으로 유사하지만 수학적으로는 좀 더 복잡한 검정을 적용할 것입니다. 이러한 세부 사항은 일단 미뤄두고 8.7절에서 다시 다룰 것입니다. 일련의 표준 분석 단계들이 DESeq라는 단일 함수로 묶여 있습니다.\npasilla = DESeq(pasilla)__\nDESeq 함수는 단순히 estimateSizeFactors(8.4.2절에서 논의한 정규화를 위해), estimateDispersions(분산 추정) 및 nbinomWaldTest(차등 풍부도에 대한 가설 검정) 함수를 순서대로 호출하는 래퍼(wrapper)입니다. 검정은 요인 condition의 두 수준인 untreated와 treated 사이에서 이루어지는데, 이는 우리가 design=~condition 인수를 통해 pasilla 객체를 구성할 때 지정한 것이기 때문입니다. 그들의 거동을 수정하거나 사용자 정의 단계를 삽입하고 싶다면 언제든지 이 세 함수를 개별적으로 호출할 수 있습니다. 결과를 살펴봅시다.\nres = results(pasilla)\nres[order(res$padj), ] |&gt; head()__\n\n\nlog2 fold change (MLE): condition treated vs untreated \nWald test p-value: condition treated vs untreated \nDataFrame with 6 rows and 6 columns\n             baseMean log2FoldChange     lfcSE      stat       pvalue\n            &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;    &lt;numeric&gt;\nFBgn0039155   730.596       -4.61901 0.1687068  -27.3789 4.88599e-165\nFBgn0025111  1501.411        2.89986 0.1269205   22.8479 1.53430e-115\nFBgn0029167  3706.117       -2.19700 0.0969888  -22.6521 1.33042e-113\nFBgn0003360  4343.035       -3.17967 0.1435264  -22.1539 9.56283e-109\nFBgn0035085   638.233       -2.56041 0.1372952  -18.6490  1.28772e-77\nFBgn0039827   261.916       -4.16252 0.2325888  -17.8965  1.25663e-71\n                    padj\n               &lt;numeric&gt;\nFBgn0039155 4.06661e-161\nFBgn0025111 6.38497e-112\nFBgn0029167 3.69104e-110\nFBgn0003360 1.98979e-105\nFBgn0035085  2.14354e-74\nFBgn0039827  1.74316e-68\n\n\n10.4.3 8.5.3 결과 탐색하기\n차등 발현 분석 후의 첫 번째 단계는 다음의 서너 가지 기본 플롯을 시각화하는 것입니다:\n\np-값의 히스토그램 (그림 8.4),\nMA 플롯 (그림 8.5), 그리고\n서열화 플롯(ordination plot) (그림 8.6).\n추가로, 히트맵 (그림 8.7)도 유익할 수 있습니다.\n\n이들은 필수적인 데이터 품질 평가 척도입니다 — 13.6절에서 제공된 품질 평가 및 제어에 대한 일반적인 조언도 여기에서 똑같이 적용됩니다.\np-값 히스토그램은 직관적입니다 (그림 8.4).\nggplot(as(res, \"data.frame\"), aes(x = pvalue)) +\n  geom_histogram(binwidth = 0.01, fill = \"Royalblue\", boundary = 0)__\n\n그림 8.4: 차등 발현 분석의 p-값 히스토그램.\n분포는 두 가지 주요 성분을 보여줍니다: 0과 1 사이의 값을 갖는 균등한 배경(background)과, 왼쪽의 작은 p-값들의 정점입니다. 균등한 배경은 차등 발현되지 않는 유전자들에 해당합니다. 대개 이것이 유전자의 대다수입니다. 왼쪽의 정점은 차등 발현되는 유전자들에 해당합니다7. 우리가 이미 6장에서 보았듯이, 배경의 수준과 정점의 높이 사이의 비율은 가장 왼쪽 빈(bin)에 있는 유전자들을 차등 발현된 것으로 판정할 때 수반될 허위 발견율(FDR)에 대한 대략적인 지표를 제공합니다. 우리의 경우, 가장 왼쪽 빈은 0과 0.01 사이의 모든 p-값을 포함하며 이는 993개 유전자에 해당합니다. 배경 수준은 약 100이므로, 가장 왼쪽 빈의 모든 유전자를 판정하는 것과 관련된 FDR은 약 10%가 될 것입니다.\n7 여기서 보여주는 데이터의 경우, 히스토그램은 중간이나 오른쪽에 몇 개의 고립된 정점들도 포함하고 있습니다. 이들은 카운트가 적은 유전자들로부터 유래하며 데이터의 이산성을 반영합니다.\n때때로 배경 분포가 균등하지 않고 오른쪽으로 갈수록 증가하는 기울어진 형태를 보이는 경우가 있습니다. 이는 대개 배치 효과의 징후입니다. 연습 문제 8.1에서 이를 더 탐구해 볼 수 있습니다.\nMA 플롯을 생성하기 위해 우리는 DESeq2 패키지의 plotMA 함수를 사용할 수 있습니다 (그림 8.5).\nplotMA(pasilla, ylim = c( -2, 2))__\n\n그림 8.5: MA 플롯: 크기 인자(size-factor)로 정규화된 카운트의 평균 대 폴드 변화(fold change). 두 축 모두 로그 스케일이 사용되었습니다. 기본적으로 조정된 p-값이 0.1보다 작으면 점들은 빨간색으로 표시됩니다. \\(y\\)축 범위를 벗어나는 점들은 삼각형으로 표시됩니다.\n7장에서 보았던 것과 유사한 PCA 플롯을 생성하기 위해 우리는 DESeq2의 plotPCA 함수를 사용할 수 있습니다 (그림 8.6).\npas_rlog = rlogTransformation(pasilla)\nplotPCA(pas_rlog, intgroup=c(\"condition\", \"type\")) + coord_fixed()__\n\n그림 8.6: PCA 플롯. 7개의 샘플이 처음 두 주성분에 의해 확장된 2D 평면에 표시되어 있습니다.\n이전 장에서 보았듯이, 이 유형의 플롯은 실험 공변량의 전체적인 효과를 시각화하거나 배치 효과를 탐지하는 데 유용합니다. 여기서 첫 번째 주축인 PC1은 주로 관심 있는 실험 공변량(untreated / treated)과 일치하며, 두 번째 축은 대략 시퀀싱 프로토콜(single / paired)과 일치합니다.\n우리는 데이터 변환의 일종인 정규화 로그(regularized logarithm) 또는 rlog를 사용했는데, 이에 대해서는 8.10.2절에서 더 자세히 살펴볼 것입니다.\n__\n질문 8.4\nPCA 플롯의 축들이 항상 알려진 실험 공변량과 일치해야 하나요?\n히트맵은 카운트 테이블을 포함한 행렬 형태의 데이터 세트에 대한 개요를 빠르게 얻을 수 있는 강력한 방법이 될 수 있습니다. 아래에서 rlog 변환된 데이터로부터 히트맵을 만드는 방법을 볼 수 있습니다. counts(pasilla)만큼 큰 행렬의 경우 전체를 플롯하는 것은 실용적이지 않으므로, 평균 발현량이 가장 높은 상위 30개 유전자의 하위 행렬을 플롯합니다.\nlibrary(\"pheatmap\")\nselect = order(rowMeans(assay(pas_rlog)), decreasing = TRUE)[1:30]\npheatmap( assay(pas_rlog)[select, ],\n     scale = \"row\",\n     annotation_col = as.data.frame(\n        colData(pas_rlog)[, c(\"condition\", \"type\")] ))__\n\n그림 8.7: 상위 30개 유전자의 정규화 로그 변환된 데이터 히트맵.\n그림 8.7에서 pheatmap은 비지도 군집화(unsupervised clustering)를 통한 덴드로그램에 따라 행과 열을 정렬했습니다. 열의 군집화 결과는… (samples) is dominated by the type factor. This highlights that our differential expression analysis above was probably too naive, and that we should adjust for this strong “nuisance” factor when we are interested in testing for differentially expressed genes between conditions. We will do this in Section 8.9.\n__\nTask\nProduce a plot similar to Figure 8.7, but selecting the 30 most highly variable genes instead. What is different? How do the genes with very high mean and those with very high variance relate? How does their data look?\n\n\n10.4.4 8.5.4 Exporting the results\nAn HTML report of the results with plots and sortable/filterable columns can be exported using the ReportingTools package on a DESeqDataSet that has been processed by the DESeq function. For a code example, see the RNA-Seq differential expression vignette of the ReportingTools package or the manual page for the publish method for the DESeqDataSet class.\nA CSV file of the results can be exported using write.csv (or its counterpart from the readr package).\nwrite.csv(as.data.frame(res), file = \"treated_vs_untreated.csv\")__",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "08-chap.html#critique-of-default-choices-and-possible-modifications",
    "href": "08-chap.html#critique-of-default-choices-and-possible-modifications",
    "title": "10  8.1 이 장의 목표",
    "section": "10.5 8.6 Critique of default choices and possible modifications",
    "text": "10.5 8.6 Critique of default choices and possible modifications\n\n10.5.1 8.6.1 The few changes assumption\nUnderlying the default normalization and the dispersion estimation in DESeq2 (and many other differential expression methods) is that most genes are not differentially expressed.\n\n\n\nFor the normalization, although not for the dispersion estimation, one can slightly relax this assumption: it is still valid if many genes are changing, but in a way that is balanced between up- and downward directions.\n\n\nFor the normalization, although not for the dispersion estimation, one can slightly relax this assumption: it is still valid if many genes are changing, but in a way that is balanced between up- and downward directions.\nThis assumption is often reasonable (well-designed experiments usually ask specific questions, so that not everything changes all at once), but what should we do if it does not hold? Instead of applying these operations on the data from all genes, we will then need to identify a subset of (“negative control”) genes for which we believe the assumption is tenable, either because of prior biological knowledge, or because we explicitly controlled their abundance as external “spiked in” features.\n__\nTask\nRun the DESeq2 workflow with size factors and dispersion parameters estimated only from a predefined subset of genes.\n\n\n10.5.2 8.6.2 Point-like null hypothesis\nAs a default, the DESeq function tests against the null hypothesis that each gene has the same abundance across conditions; this is a simple and pragmatic choice. Indeed, if the sample size is limited, what is statistically significant also tends to be strong enough to be biologically interesting. But as sample size increases, statistical significance in these tests may be present without much biological relevance. For instance, many genes may be slightly perturbed by downstream, indirect effects. We can modify the test to use a more permissive, interval-based null hypothesis; we will further explore this in Section 8.10.4.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "08-chap.html#multi-factor-designs-and-linear-models",
    "href": "08-chap.html#multi-factor-designs-and-linear-models",
    "title": "10  8.1 이 장의 목표",
    "section": "10.6 8.7 Multi-factor designs and linear models",
    "text": "10.6 8.7 Multi-factor designs and linear models\n\n10.6.1 8.7.1 What is a multifactorial design?\nLet’s assume that in addition to the siRNA knockdown of the pasilla gene, we also want to test the effect of a certain drug. We could then envisage an experiment in which the experimenter treats the cells either with negative control, with the siRNA against pasilla, with the drug, or with both. To analyse this experiment, we can use the notation\n\\[ y = _0 + x_1 _1 + x_2 2 + x_1x_2{12}. \\]",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "08-chap.html#차등-풍부도differential-abundance",
    "href": "08-chap.html#차등-풍부도differential-abundance",
    "title": "10  8.1 이 장의 목표",
    "section": "10.7 8.6 차등 풍부도(Differential abundance)",
    "text": "10.7 8.6 차등 풍부도(Differential abundance)\n우리의 주요 목표는 서로 다른 생물학적 조건(예: 약물 처리 대 대조군) 사이에서 통계적으로 유의미한 방식으로 풍부도가 변하는 유전자를 찾는 것입니다. 이를 위해 우리는 각 유전자에 대해 다음 가설들을 검정합니다:\n귀무 가설 \\(H_0\\): 유전자의 풍부도가 두 조건 사이에서 동일하다.\n대립 가설 \\(H_1\\): 두 조건 사이에서 풍부도가 다르다.\n이것은 우리가 6장에서 논의한 가설 검정 프레임워크와 정확히 일치합니다. 유일한 차이점은 우리가 수천 개의 유전자에 대해 이 검정을 동시에 수행한다는 것이며, 따라서 다중 검정(multiple testing) 보정이 필요하다는 점입니다.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "08-chap.html#선형-모델",
    "href": "08-chap.html#선형-모델",
    "title": "10  8.1 이 장의 목표",
    "section": "10.8 8.7 선형 모델",
    "text": "10.8 8.7 선형 모델\n여러 실험적 요인들이 동시에 작용하는 더 복잡한 실험 설계를 분석하기 위해, 우리는 선형 모델(linear models)이라는 매우 강력한 프레임워크를 사용합니다.\n\n10.8.1 8.7.1 요인 설계 및 상호작용\n두 가지 실험적 요인이 있는 실험을 생각해 봅시다: siRNA를 이용한 유전자 넉다운과 약물 처리입니다. 우리는 네 가지 가능한 조건을 가집니다:\n\n처리되지 않음 (negative control)\nsiRNA만 처리됨\n약물만 처리됨\nsiRNA와 약물 모두 처리됨\n\n이러한 실험의 결과를 다음과 같은 선형 방정식으로 모델링할 수 있습니다:\n\\[ y = _0 + x_1 _1 + x_2 2 + x_1 x_2 {12} + . \\]\n이 방정식은 다음과 같이 해석될 수 있습니다. 좌변인 \\(y\\)는 관심 있는 실험 측정값입니다. 우리의 경우, 이는 적절하게 변환된 유전자의 발현 수준입니다(이에 대해서는 8.8.3절에서 논의할 것입니다). RNA-Seq 실험에는 수많은 유전자가 있으므로, 각 유전자에 대해 하나씩 식 8.1과 같은 수많은 방정식을 갖게 될 것입니다. 계수 \\(_0\\)는 대조군에서의 측정 기본 수준이며, 종종 절편(intercept)이라고 불립니다.\n\n\n\n때때로 식 8.1은 _0와 곱해지는 추가 항 x_0를 포함하여 쓰여지기도 하는데, 이때 x_0는 항상 1로 이해됩니다. 이렇게 하면 절편을 별개의 사례로 다루는 대신 다른 함께 일관되게 처리할 수 있어 이후의 표기법과 장부 관리가 더 쉬워집니다.\n\n\n때때로 식 8.1은 \\(_0\\)와 곱해지는 추가 항 \\(x_0\\)를 포함하여 쓰여지기도 하는데, 이때 \\(x_0=1\\)은 항상 성립하는 것으로 이해됩니다. 이렇게 하면 절편을 별개의 사례로 다루는 대신 다른 \\(\\)들과 함께 일관되게 처리할 수 있어 이후의 표기법과 장부 관리가 더 쉬워진다는 점이 밝혀졌습니다.\n설계 요인 \\(x_1\\)과 \\(x_2\\)는 이진 지시 변수(binary indicator variables)입니다: siRNA가 형질감염(transfected)되었으면 \\(x_1\\)은 1, 아니면 0의 값을 가지며, 유사하게 \\(x_2\\)는 약물이 투여되었는지 여부를 나타냅니다. siRNA만 사용된 실험에서는 \\(x_1=1\\)이고 \\(x_2=0\\)이므로, 식 8.1의 세 번째와 네 번째 항은 사라집니다. 그러면 방정식은 \\(y=_0+_1\\)로 단순화됩니다. 이는 \\(_1\\)이 처리군과 대조군 사이의 차이를 나타냄을 의미합니다. 만약 우리의 측정값이 로그 스케일이라면,\n\\[ \\[\\begin{align} \\beta_1 = y-\\beta_0\n&=\\log_2(\\text{발현}_{\\text{처리군}})\n-\\log_2(\\text{발현}_{\\text{대조군}})\\\\\\ &=\\log_2\\frac\n{\\text{발현}_{\\text{처리군}}} {\\text{발현}_{\\text{대조군}}}\n\\end{align}\\] \\]\n은 siRNA 처리에 의한 로그 폴드 변화(logarithmic fold change)입니다. 정확히 같은 방식으로, \\(_2\\)는 약물 처리에 의한 로그 폴드 변화입니다. 세포에 siRNA와 약물을 모두 처리하면 어떻게 될까요? 그 경우 \\(x_1=x_2=1\\)이며, 식 8.1은 다음과 같이 다시 쓰여질 수 있습니다.\n\\[ _{12} = y - (_0 + _1 + _2). \\]\n이는 \\(_{12}\\)가 관찰된 결과인 \\(y\\)와, 기본 수준에 siRNA 단독 효과 \\(_1\\) 및 약물 단독 효과 \\(_2\\)를 더하여 얻은 개별 처리들로부터 예상되는 결과 사이의 차이임을 의미합니다.\n우리는 \\(_{12}\\)를 siRNA와 약물의 상호작용(interaction) 효과라고 부릅니다. 이는 물리적 상호작용과는 무관하며, 이 용어는 이 두 가지 서로 다른 실험 요인의 효과가 단순히 더해지는 것이 아니라 더 복잡한 방식으로 결합됨을 나타냅니다.\n\n\n\n덧셈은 로그 스케일에서 이루어지며, 이는 원래 스케일에서의 곱셈에 해당함에 유의하세요.\n\n\n덧셈은 로그 스케일에서 이루어지며, 이는 원래 스케일에서의 곱셈에 해당함에 유의하세요.\n예를 들어, 만약 약물의 타겟과 siRNA의 타겟이 동등하여 세포에 동일한 효과를 준다면, 생물학적으로 \\(_1=2\\)일 것으로 예상합니다. 또한 우리는 그들의 조합이 추가적인 효과를 주지 않을 것으로 예상하므로, \\({12}=-_1\\)이 됩니다. 반면에, 약물의 타겟과 siRNA의 타겟이 서로 완충 작용을 할 수 있는 평행한 경로에 있다면, \\(_1\\)과 \\(2\\)는 둘 다 비교적 작겠지만 결합된 효과는 시너지 작용을 하여 \\({12}\\)가 클 것으로 예상할 것입니다.\n우리가 항상 상호작용에 관심을 갖는 것은 아닙니다. 많은 실험들이 여러 요인을 가지고 설계되지만 각각의 개별 효과에 가장 큰 관심을 가집니다. 그 경우, 조합 처리는 실험 설계에 포함되지 않을 수 있으며, 분석에 사용할 모델은 식 8.1에서 가장 오른쪽 항을 제거한 버전입니다.\n우리는 실험 설계를 _설계 행렬(design matrix)_로 간결하게 인코딩할 수 있습니다. 예를 들어, 위에서 설명한 조합 실험의 경우 설계 행렬은 다음과 같습니다.\n\\[ ]\n설계 행렬의 열들은 실험 요인들에 대응하고, 행들은 서로 다른 실험 조건들(우리의 경우 4가지)을 나타냅니다. 만약 조합 처리가 수행되지 않는다면, 설계 행렬은 8.4의 처음 세 행으로만 축소됩니다.\n\n\n10.8.2 8.7.2 노이즈와 반복 실험(Replicates)은 어떠한가요?\n식 8.1은 관측된 데이터를 서로 다른 실험 변수들에 의해 발생한 효과들로 개념적으로 분해하는 방법을 제공합니다. 만약 우리의 데이터(\\(y\\)들)가 절대적으로 정확하다면, 우리는 \\(x\\)들로 표현되는 네 가지 가능한 실험 조건 각각에 대해 하나씩 선형 방정식 시스템을 세우고 \\(\\)들을 풀 수 있을 것입니다.\n물론, 우리는 대개 노이즈의 영향을 받는 실제 데이터를 분석하고자 합니다. 그러면 노이즈 수준을 추정하고 추정된 \\(\\)들의 불확실성을 평가하기 위해 반복 실험이 필요합니다. 그래야만 조건 사이에서 관찰된 변화가 단지 실험적 또는 자연적 변동에 의해 발생하는 것보다 유의미하게 큰지 경험적으로 평가할 수 있습니다. 우리는 방정식을 약간 확장해야 합니다.\n\\[ y_{j} = x_{j0} ; 0 + x{j1} ; 1 + x{j2} ; 2 + x{j1},x_{j2};_{12} + _j. \\]\n우리는 인덱스 \\(j\\)와 새로운 항 \\(j\\)를 추가했습니다. 인덱스 \\(j\\)는 이제 우리의 개별 반복 실험들을 명시적으로 셉니다; 예를 들어, 네 가지 조건 각각에 대해 세 번의 반복 실험을 수행한다면 \\(j\\)는 1부터 12까지 셉니다. 설계 행렬은 이제 12개의 행을 가지며, \\(x{jk}\\)는 행렬의 \\(j\\)번째 행과 \\(k\\)번째 열의 값입니다.\n\n\n\n0가 절편이므로 모든 j에 대해 x{j0}=1임을 기억하세요.\n\n\n\\(0\\)가 절편이므로 모든 \\(j\\)에 대해 \\(x{j0}=1\\)임을 기억하세요.\n우리가 잔차(residuals)라고 부르는 추가 항 \\(_j\\)는 반복 실험들 사이의 차이를 흡수하기 위해 존재합니다. 그러나 한 가지 추가적인 모델링 구성 요소가 필요합니다: 12개의 방정식 시스템 8.5는 추가 정보 없이는 과소 결정(underdetermined)될 것인데, 왜냐하면 이제 방정식의 수(12개, 각 \\(j\\)에 대해 하나씩)보다 변수의 수(12개의 입실론과 4개의 베타)가 더 많기 때문입니다. 이를 해결하기 위해 우리는 \\(_j\\)가 작을 것을 요구합니다. 이를 극복하기 위해 널리 쓰이는 한 가지 방법은 — 다른 방법들도 만나게 되겠지만 — 잔차 제곱합을 최소화하는 것입니다.\n\\[ _j _j^2 . \\]\n이 요구 조건이 충족되면, \\(\\)들은 각 실험 요인의 평균적 효과를 나타내고, 잔차 \\(_j\\)는 반복 실험들 사이의 평균 주변의 실험적 변동을 반영하게 됩니다. 최소제곱법 적합(least sum of squares fitting)이라 불리는 이 접근 방식은 간단한 행렬 대수로 달성될 수 있기 때문에 수학적으로 편리합니다. 이것이 R 함수 lm이 하는 일입니다.\n__\n질문 8.5\n식 8.5를 쓰는 대안적인 방법은 다음과 같습니다.\n\\[ y_{j} = k x{jk} ; _k + _j. \\]\n이것이 어떻게 식 8.5와 매핑될 수 있을까요? 즉, 상호작용 항 \\(x_{j1}x_{j2}_{12}\\)는 어떻게 된 것인가요?\n__\n해결책\n__\n이것은 정말 사소한 표기법의 문제입니다: 합은 \\(k=0,…,3\\)에 대해 확장되며, \\(k=0,1,2\\)에 대한 항들은 우리가 이미 알고 있는 것과 정확히 같습니다. 우리는 \\({12}\\) 대신 \\({3}\\)라고 쓰고, \\(x_{j3}\\)는 \\(x_{j1}x_{j2}\\)로 정의됩니다. 일반적인 표기법 8.7은 선형 모델을 구현하는 컴퓨터 소프트웨어와 수학적 증명에서 사용하기에 실용적입니다. 또한 선형 모델의 “과학적 내용”이 그 설계 행렬에 응축되어 있음을 강조합니다.\n__\n태스크\n목적 함수 8.6이 성립하도록 식 8.5를 데이터에 적합시켰다면, 적합 잔차 \\(_j\\)의 평균이 0임을 보이세요.\n\n\n10.8.3 8.7.3 분산 분석(Analysis of variance)\n8.5와 같은 모델을 선형 모델(linear model)이라고 부르며, 종종 기준 8.6이 데이터를 적합시키는 데 사용됨을 암시합니다. 이 접근 방식은 우아하고 강력하지만, 초보자가 그 모든 측면을 이해하는 데는 시간이 좀 걸릴 수 있습니다. 각기 다른 실험 조건에 대해 단순히 반복 실험들의 평균을 취하고 이 값들을 조건들 사이에서 비교하는 것에 비해 어떤 이점이 있을까요? 단순한 경우에 후자의 접근 방식은 직관적이고 효과적일 수 있습니다. 그러나 서로 다른 그룹에서 반복 실험의 수가 모두 같지 않거나, 하나 이상의 \\(x\\) 변수가 연속형 값을 가질 때 한계에 부딪히게 됩니다. 이러한 경우, 결국 데이터에 8.5와 같은 것을 적합시키는 것으로 귀결될 것입니다. 8.5를 생각하는 유용한 방법은 분산 분석(analysis of variance), 약어로 ANOVA라는 용어에 담겨 있습니다. 사실 식 8.5가 하는 일은 우리가 실험 과정에서 관찰한 \\(y\\)의 가변성을 기초적인 성분들로 분해하는 것입니다: 기본 수준 값 \\(_0\\), 첫 번째 변수의 효과에 의해 발생하는 가변성 \\(_1\\), 두 번째 변수의 효과에 의해 발생하는 가변성 \\(2\\), 상호작용의 효과에 의해 발생하는 가변성 \\({12}\\), 그리고 설명되지 않는 가변성입니다. 이들 중 마지막 것을 우리는 흔히 _노이즈(noise)_라고 부르고, 다른 것들은 _체계적 가변성(systematic variability)_이라고 부릅니다.\n\n\n\n노이즈와 체계적 가변성의 구분은 보는 사람의 관점에 달려 있으며, 현실이 아니라 우리의 모델에 달려 있습니다.\n\n\n노이즈와 체계적 가변성의 구분은 보는 사람의 관점에 달려 있으며, 현실이 아니라 우리의 모델에 달려 있습니다.\n\n\n10.8.4 8.7.4 강건성(Robustness)\n합 8.6은 데이터의 이상치(outliers)에 민감합니다. 이상치를 가진 단 하나의 측정값 \\(y_{j}\\)가 \\(\\) 추정치를 다른 반복 실험들에 의해 함축된 값들로부터 멀리 끌어당길 수 있습니다. 이는 최소제곱법에 기반한 방법들이 낮은 붕괴점(breakdown point)을 갖는다는 잘 알려진 사실입니다: 단 하나의 데이터 포인트만 이상치여도 전체 통계 결과가 강력하게 영향을 받을 수 있습니다. 예를 들어, \\(n\\)개 숫자 세트의 평균은 \\(\\)의 붕괴점을 갖는데, 이는 숫자들 중 단 하나만 바꿈으로써 평균을 임의로 바꿀 수 있음을 의미합니다. 반면에 중앙값(median)은 훨씬 더 높은 붕괴점을 갖습니다. 숫자 하나를 바꾸는 것은 종종 아무런 영향을 주지 않으며, 영향이 있더라도 그 효과는 순위의 중간에 있는 데이터 포인트들의 범위(즉, \\(\\) 순위에 인접한 것들)로 제한됩니다. 중앙값을 임의로 높게 바꾸려면 관측치의 절반을 바꾸어야 합니다. 우리는 중앙값을 강건(robust)하다고 부르며, 그 붕괴점은 \\(\\)입니다. 숫자 세트 \\(y_1, y_2, …\\)의 중앙값이 합 \\(_j|y_j-_0|\\)을 최소화한다는 점을 기억하세요.\n이상치에 대해 더 높은 수준의 강건성을 달성하기 위해, 최소화의 목적 함수로 제곱합 8.6 대신 다른 선택지들이 사용될 수 있습니다. 그중에는 다음과 같은 것들이 있습니다:\n\\[ \\[\\begin{align} R &= \\sum_j |\\varepsilon_j| & \\text{최소 절대 편차 (Least absolute deviations)} \\\\\\ R &= \\sum_j \\rho_s(\\varepsilon_j) & \\text{M-추정 (M-estimation)} \\\\\\ R &= Q_{\\theta}\\left( \\\\{\\varepsilon_1^2, \\varepsilon_2^2,... \\\\} \\right) & \\text{LTS, LQS} \\\\\\ R &= \\sum_j w_j \\varepsilon_j^2 & \\text{일반화 가중 회귀 (general weighted regression)} \\end{align}\\] \\]\n여기서 \\(R\\)은 최소화되어야 할 양입니다. 식 8.8의 첫 번째 선택지는 최소 절대 편차(least absolute deviations) 회귀라고 불립니다. 이는 중앙값의 일반화로 볼 수 있습니다. 개념적으로는 단순하고 언뜻 보기에 매력적이지만, 제곱합보다 최소화하기가 더 어렵고, 특히 데이터가 제한적이거나 모델에 잘 맞지 않을 때 덜 안정적이고 덜 효율적일 수 있습니다8. 식 8.8의 두 번째 선택지인 M-추정(M-estimation)은 제한된 범위의 \\(\\)에 대해서는 이차 함수(quadratic function)처럼 보이지만, 절대값 \\(||\\)이 스케일 매개변수 \\(s\\)보다 큰 경우에는 기울기가 더 작아지거나, 평평해지거나, 심지어 다시 0으로 떨어지는 페널티 함수 \\(_s\\)를 사용합니다 (최소제곱 회귀는 \\(_s()=^2\\)인 특수한 경우입니다). 이 이면의 의도는 이상치, 즉 큰 잔차를 가진 데이터 포인트의 효과를 낮게 가중하는 것입니다 (Huber 1964). \\(s\\)의 선택이 이루어져야 하며 이것이 무엇을 이상치로 부를지 결정합니다. 심지어 0 근처에서 \\(_s\\)가 이차 함수여야 한다는 요구 조건을 버릴 수도 있으며(그의 이계 도함수가 양수이기만 하다면), 문헌에서는 다양한 \\(_s\\) 함수 선택지가 제안되었습니다. 그 목표는 데이터가 모델에 잘 맞을 때와 그곳에서 추정기에 바람직한 통계적 속성(예: 편향과 효율성)을 부여하면서도, 그렇지 않은 데이터 포인트의 영향력을 제한하거나 무효화하고 계산을 다루기 쉽게 유지하는 것입니다.\n8 위키백과 문서에서 개요를 제공합니다.\n__\n질문 8.6\nM-추정기를 위해 Huber (1964)가 제안한 함수 \\(_s()\\)의 그래프를 그려보세요.\n__\n해결책\n__\nHuber의 논문은 75페이지에서 다음과 같이 정의합니다:\n\\[ _s() = \\{ ]\n아래 코드로 생성된 그래프는 그림 8.8에 나와 있습니다.\nrho = function(x, s)\n  ifelse(abs(x) &lt; s, x^2 / 2,  s * abs(x) - s^2 / 2)\n\ndf = tibble(\n  x        = seq(-7, 7, length.out = 100),\n  parabola = x ^ 2 / 2,\n  Huber    = rho(x, s = 2))\n\nggplot(reshape2::melt(df, id.vars = \"x\"),\n  aes(x = x, y = value, col = variable)) + geom_line()__\n\n그림 8.8: \\(s=2\\)를 선택했을 때의 \\(_s()\\) 그래프.\nChoice three in 8.8 generalises the least sum of squares method in yet another way. In least quantile of squares (LQS) regression, the the sum over the squared residuals is replaced with a quantile, for instance, \\(Q_{50}\\), the median, or \\(Q_{90}\\), the 90%-quantile (Peter J. Rousseeuw 1987). In a variation thereof, least trimmed sum of squares (LTS) regression, a sum of squared residuals is used, but the sum extends not over all residuals, but only over the fraction \\(0\\) of smallest residuals. The motivation in either case is that outlying data points lead to large residuals, and as long as they are rare, they do not affect the quantile or the trimmed sum.\nHowever, there is a price: while the least sum of squares optimization 8.6 can be done through straightforward linear algebra, more complicated iterative optimization algorithms are needed for M-estimation, LQS and LTS regression.\nThe final approach in 8.8 represents an even more complex way of weighting down outliers. It assumes that we have some way of deciding what weight \\(w_j\\) we want to give to each observation, presumably down-weighting outliers. For instance, in Section 8.10.3, we will encounter the approach used by the DESeq2 package, in which the leverage of each data point on the estimated \\(\\)s is assessed using a measure called Cook’s distance. For those data whose Cook’s distance is deemed too large, the weight \\(w_j\\) is set to zero, whereas the other data points get \\(w_j=1\\). In effect, this means that the outlying data points are discarded and that ordinary regression is performed on the others. The extra computational effort of carrying the weights along is negligible, and the optimization is still straightforward linear algebra.\nAll of these approaches to outlier robustness introduce a degree of subjectiveness and rely on sufficient replication. The subjectiveness is reflected by the parameter choices that need to be made: \\(s\\) in 8.8 (2), \\(\\) in 8.8 (3), the weights in 8.8 (4). One scientist’s outlier may be the Nobel prize of another. On the other hand, outlier removal is no remedy for sloppy experiments and no justification for wishful thinking.\n__\nTask\nSearch the documentation of R and CRAN packages for implementations of the above robust regression methods. A good place to start is the CRAN task view on robust statistical methods.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "08-chap.html#generalized-linear-models",
    "href": "08-chap.html#generalized-linear-models",
    "title": "10  8.1 이 장의 목표",
    "section": "10.9 8.8 Generalized linear models",
    "text": "10.9 8.8 Generalized linear models\nWe need to explore two more theoretical concepts before we can proceed to our next application example. Equations of the form 8.5 model the expected value of the outcome variable, \\(y\\), as a linear function of the design matrix, and they are fit to data according to the least sum of squares criterion 8.6; or a robust variant thereof. We now want to generalize these assumptions.\n\n10.9.1 8.8.1 Modeling the data on a transformed scale\nWe already saw that it can be fruitful to consider the data not on the scale that we obtained them, but after some transformation, for instance, the logarithm. This idea can be generalized, since depending on the context, other transformations are useful. For instance, the linear model 8.5 would not directly be useful for modeling outcomes that are bounded within an interval, say, \\([0,1]\\) as an indicator of disease risk. In a linear model, the values of \\(y\\) cover, in principle, the whole real axis. However, if we transform the expression on the right hand with a sigmoid function, for instance, \\(f(y) = 1/(1+e^{-y})\\), then the range of this function9, is bounded between 0 and 1 and can be used to model such an outcome.\n9 It is called the logistic function (Verhulst 1845), and the associated regression model is called logistic regression.\n\n\n10.9.2 8.8.2 Other error distributions\nThe other generalization regards the minimization criterion 8.6. In fact, this criterion can be derived from a specific probabilistic model and the maximum likelihood principle (we already encountered this in Chapter 2). To see this, consider the probabilistic model\n\\[ p(_j) = (-), \\]\nthat is, we believe that the residuals follow a normal distribution with mean 0 and standard deviation \\(\\). Then it is plausible to demand from a good model (i.e., from a good set of \\(\\)s) that these probabilities are large. Formally,\n\\[ _j p(_j) . \\]\n__\nQuestion 8.7\nShow that the maximizing the likelihood 8.10 is equivalent to minimizing the sum of squared residuals 8.6.\n__\nSolution\n__\nInsert 8.9 into 8.10 and take the logarithm.\nLet’s revise some core concepts: the left hand side of Equation 8.10, i.e., the product of the probabilities of the residuals, is a function of both the model parameters \\(_1, _2, …\\) and the data \\(y_1, y_2, …\\); call it \\(f(,y)\\). If we think of the model parameters \\(\\) as given and fixed, then the collapsed function \\(f(y)\\) simply indicates the probability of the data. We could use it, for instance, to simulate data. If, on the other hand, we consider the data as given, then \\(f()\\) is a function of the model parameters, and it is called the likelihood. The second view is the one we take when we optimise 8.6 (and thus 8.10), and hence the \\(\\)s obtained this way are what is called maximum-likelihood estimates.\n\n\n\nIt is good to remember that, while we can use the normal distribution as a convenient argument to motivate least sum of squares regression through the maximum likelihood principle, the data do not have to be distributed according to the normal for least sum of squares regression to provide a useful result. In fact, least sum of squares fitting often provides useful estimates for the \\betas even when the data are non-normal, although that depends on the specific circumstances.\n\n\nIt is good to remember that, while we can use the normal distribution as a convenient argument to motivate least sum of squares regression through the maximum likelihood principle, the data do not have to be distributed according to the normal for least sum of squares regression to provide a useful result. In fact, least sum of squares fitting often provides useful estimates for the \\(\\)s even when the data are non-normal, although that depends on the specific circumstances.\nThe generalization that we can now make is to use a different probabilistic model. We can use the densities of other distributions than the normal instead of Equation 8.9. For instance, to be able to deal with count data, we will use the gamma-Poisson distribution.\n\n\n10.9.3 8.8.3 A generalized linear model for count data\nThe differential expression analysis in DESeq2 uses a generalized linear model of the form:\n\\[ \\[\\begin{align} K_{ij} & \\sim \\text{GP}(\\mu_{ij}, \\alpha_i) \\\\\\ \\mu_{ij} &=\ns_j\\, q_{ij} \\\\\\ \\log_2(q_{ij}) &= \\sum_k x_{jk} \\beta_{ik}. \\end{align}\\] \\]\nLet us unpack this step by step. The counts \\(K_{ij}\\) for gene \\(i\\), sample \\(j\\) are modeled using a gamma-Poisson (GP) distribution with two parameters, the mean \\({ij}\\) and the dispersion \\(i\\). By default, the dispersion is different for each gene \\(i\\), but the same across all samples, therefore it has no index \\(j\\). The second line in Equation 8.11 states that the mean is composed of a sample-specific size factor \\(s_j\\)10 and \\(q{ij}\\), which is proportional to the true expected concentration of fragments for gene \\(i\\) in sample \\(j\\). The value of \\(q{ij}\\) is given by the linear model in the third line via the link function , \\(2\\). The design matrix \\((x{jk})\\) is the same for all genes (and therefore does not depend on \\(i\\)). Its rows \\(j\\) correspond to the samples, its columns \\(k\\) to the experimental factors. In the simplest case, for a pairwise comparison, the design matrix has only two columns, one of them everywhere filled with 1 (corresponding to \\(0\\) of Section 8.7.1) and the other one containing 0 or 1 depending on whether the sample belongs to one or the other group. The coefficients \\({ik}\\) give the \\(_2\\) fold changes for gene \\(i\\) for each column of the design matrix \\(X\\).\n10 The model can be generalized to use sample- and gene-dependent normalization factors \\(s_{ij}\\). This is explained in the documentation of the DESeq2 package.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "08-chap.html#two-factor-analysis-of-the-pasilla-data",
    "href": "08-chap.html#two-factor-analysis-of-the-pasilla-data",
    "title": "10  8.1 이 장의 목표",
    "section": "10.10 8.9 Two-factor analysis of the pasilla data",
    "text": "10.10 8.9 Two-factor analysis of the pasilla data\nBesides the treatment with siRNA, which we have already considered in Section 8.5, the pasilla data have another covariate, type, which indicates the type of sequencing that was performed.\nWe saw in the exploratory data analysis (EDA) plots in Section 8.5.3 that the latter had a considerable systematic effect on the data. Our basic analysis of Section 8.5 did not take this account, but we will do so now. This should help us get a more correct picture of which differences in the data are attributable to the treatment, and which are confounded – or masked – by the sequencing type.\npasillaTwoFactor = pasilla\ndesign(pasillaTwoFactor) = formula(~ type + condition)\npasillaTwoFactor = DESeq(pasillaTwoFactor)__\nOf the two variables type and condition, the one of primary interest is condition, and in DESeq2 , the convention is to put it at the end of the formula. This convention has no effect on the model fitting, but it helps simplify some of the subsequent results reporting. Again, we access the results using the results function, which returns a dataframe with the statistics of each gene.\nres2 = results(pasillaTwoFactor)\nhead(res2, n = 3)__\n\n\nlog2 fold change (MLE): condition treated vs untreated \nWald test p-value: condition treated vs untreated \nDataFrame with 3 rows and 6 columns\n             baseMean log2FoldChange     lfcSE       stat    pvalue      padj\n            &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt;  &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\nFBgn0000003  0.171569      0.6745518  3.871091  0.1742537  0.861666        NA\nFBgn0000008 95.144079     -0.0406731  0.222215 -0.1830351  0.854770  0.951975\nFBgn0000014  1.056572     -0.0849880  2.111821 -0.0402439  0.967899        NA\nIt is also possible to retrieve the \\(_2\\) fold changes, p-values and adjusted p-values associated with the type variable. The function results takes an argument contrast that lets users specify the name of the variable, the level that corresponds to the numerator of the fold change and the level that corresponds to the denominator of the fold change.\nresType = results(pasillaTwoFactor,\n  contrast = c(\"type\", \"single\", \"paired\"))\nhead(resType, n = 3)__\n\n\nlog2 fold change (MLE): type single vs paired \nWald test p-value: type single vs paired \nDataFrame with 3 rows and 6 columns\n             baseMean log2FoldChange     lfcSE      stat    pvalue      padj\n            &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;\nFBgn0000003  0.171569      -1.611546  3.871083 -0.416304  0.677188        NA\nFBgn0000008 95.144079      -0.262255  0.220686 -1.188362  0.234691  0.543822\nFBgn0000014  1.056572       3.290586  2.087243  1.576522  0.114905        NA\nSo what did we gain from this analysis that took into account type as a nuisance factor (sometimes also called, more politely, a blocking factor), compared to the simple comparison between two groups of Section 8.5? Let us plot the p-values from both analyses against each other.\ntrsf = function(x) ifelse(is.na(x), 0, (-log10(x)) ^ (1/6))\nggplot(tibble(pOne = res$pvalue,\n              pTwo = res2$pvalue),\n    aes(x = trsf(pOne), y = trsf(pTwo))) +\n    geom_hex(bins = 75) + coord_fixed() +\n    xlab(\"Single factor analysis (condition)\") +\n    ylab(\"Two factor analysis (type + condition)\") +\n    geom_abline(col = \"orange\")__\n\nFigure 8.9: Comparison of p-values from the models with a single factor (condition) and with two factors (type + condition). The axes correspond to \\((-_{10}p)^{}\\), an arbitrarily chosen monotonically decreasing transformation that compresses the dynamic range of the p-values for the purpose of visualization. We can see a trend for the joint distribution to lie above the bisector, indicating that the small p-values in the two-factor analysis are generally smaller than those in the one-factor analysis.\nAs we can see in Figure 8.9, the p-values in the two-factor analysis are similar to those from the one-factor analysis, but are generally smaller. The more sophisticated analysis has led to an, albeit modest, increase in power. We can also see this by counting the number of genes that pass a certain significance threshold in each case:\ncompareRes = table(\n   `simple analysis` = res$padj &lt; 0.1,\n   `two factor` = res2$padj &lt; 0.1 )\naddmargins( compareRes )__\n\n\n               two factor\nsimple analysis FALSE TRUE  Sum\n          FALSE  6973  289 7262\n          TRUE     25 1036 1061\n          Sum    6998 1325 8323\nThe two-factor analysis found 1325 genes differentially expressed at an FDR threshold of 10%, while the one-factor analysis found 1061. The two-factor analysis has increased detection power. In general, the gain can be even much larger, or also smaller, depending on the data. The proper choice of the model requires informed adaptation to the experimental design and data quality.\n__\nQuestion 8.8\nWhy do we detect fewer significant genes when we do not take into account the type variable? More generally, what does this mean about the benefit of taking into account (or not) blocking factors?\n__\nSolution\n__\nWithout modeling the blocking factor, the variability in the data that is due to it has to be absorbed by the \\(\\)s. This means that they are generally larger than in the model with the blocking factor. The higher level of noise leads to higher uncertainty in the \\(\\)-estimates. On the other hand, the model with the blocking factor has more parameters that need to be estimated. In statistical parlance, the fit has fewer “degrees of freedom”. Both of these effects are counteracting, and which of them prevails, and which of the modeling choices yields more or fewer significant results depends on the data.\n__\nQuestion 8.9\nWhat is confounding? Can not taking into account a blocking factor also lead to the detection of more genes?\n__\nSolution\n__\nYes. Imagine the variables condition and type were not as nicely balanced as they are, but partially or fully confounded. In that case, differences in the data due to type could be attributed to condition if a model is fit that does not make it possible to absorb them in the type-effect. Scientifically, such an experiment (and analysis) can be quite an embarrassment.\n__\nQuestion 8.10\nConsider a paired experimenal design, say, 10 different cell lines each with and without drug treatment. How should this be analyzed?\n__\nSolution\n__\nIf we just did a simple two-group comparison (treated versus untreated) many of the treatment effects would probably go under in the strong cell line to cell line variation. However, we can set up a paired analysis simply by adding cell line identity as a blocking factor. (Cell line is then really an R factor with 10 different levels, rather than just a 0 vs 1 indicator variable as with the variables that we looked at so far; R’s linear modeling facilities, and also DESeq2 , have no problem dealing with that.)\n__\nQuestion 8.11\nWhat can you do if you suspect there are “hidden” factors that affect your data, but they are not documented? (Sometimes, such undocumented covariates are also called batch effects.)\n__\nSolution\n__\nThere are methods that try to identify blocking factors in an unsupervised fashion, see e.g., Leek and Storey (2007; Stegle et al. 2010).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "08-chap.html#further-statistical-concepts",
    "href": "08-chap.html#further-statistical-concepts",
    "title": "10  8.1 이 장의 목표",
    "section": "10.11 8.10 Further statistical concepts",
    "text": "10.11 8.10 Further statistical concepts\n\n10.11.1 8.10.1 Sharing of dispersion information across genes\nWe already saw an explanation of Bayesian (or empirical Bayes) analysis in Figure 6.16. The idea is to use additional information to improve our estimates (information that we either known a priori, or have from analysis of other, but similar data). This idea is particularly useful if the data per se are relatively noisy. DESeq2 uses an empirical Bayes approach for the estimation of the dispersion parameters (the \\(\\)s in the third line of Equation 8.11) and, optionally, the logarithmic fold changes (the \\(\\)s). The priors are, in both cases, taken from the distributions of the maximum-likelihood estimates (MLEs) across all genes. It turns out that both of these distributions are uni-modal; in the case of the \\(\\)s, with a peak at around 0, in the case of the \\(\\), at a particular value, the typical dispersion. The empirical Bayes machinery then shrinks each per-gene MLE towards that peak, by an amount that depends on the sharpness of the empirical prior distribution and the precision of the ML estimate (the better the latter, the less shrinkage will be done). The mathematics are explained in (Michael I. Love, Huber, and Anders 2014), and Figure 8.10 visualizes the approach for the \\(\\)s.\n__\nTask\nAdvanced: check the R code that produces Figure 8.10.\n[](08-chap_files/figure-html/fig-countdata-posterior-1.png “Figure 8.10 (a):”)\n\n\n\n[](08-chap_files/figure-html/fig-countdata-posterior-2.png “Figure 8.10 (b):”)\n\n\n\nFigure 8.10: Shrinkage estimation of logarithmic fold change estimates by use of an empirical prior in DESeq2. Two genes with similar mean count and MLE logarithmic fold change are highlighted in green and blue. The normalized counts for these genes (a) reveal low dispersion for the gene in blue and high dispersion for the gene in green. In (b), the density plots are shown of the normalized likelihoods (solid lines) and of the posteriors (dashed lines) for the green and blue gene. In addition, the solid black line shows the prior estimated from the MLEs of all genes. Due to the higher dispersion of the green gene, its likelihood is wider and less sharp (indicating less information), and the prior has more influence on its posterior than in the case of the blue gene.\n\n\n10.11.2 8.10.2 Count data transformations\nFor testing for differential expression we operate on raw counts and use discrete distributions. For other downstream analyses – e.g., for visualization or clustering – it might however be useful to work with transformed versions of the count data.\nMaybe the most obvious choice of transformation is the logarithm. However, since count values for a gene can become zero, some advocate the use of pseudocounts , i.e., transformations of the form\n\\[ y = _2(n + 1)y = _2(n + n_0), \\]\nwhere \\(n\\) represents the count values and \\(n_0\\) is a somehow chosen positive constant.\nLet’s look at two alternative approaches that offer more theoretical justification, and a rational way of choosing the parameter equivalent to \\(n_0\\) above. One method incorporates priors on the sample differences, and the other uses the concept of variance-stabilizing transformations.\n\n10.11.2.1 Variance-stabilizing transformation\nWe already explored variance-stabilizing transformations in Section 4.4.4. There we computed a piece-wise linear transformation for a discrete set of random variables (Figure 4.26) and also saw how to use calculus to derive a smooth variance-stabilizing transformation for a gamma-Poisson mixture. These computations are implemented in the DESeq2 package (Anders and Huber 2010):\nvsp = varianceStabilizingTransformation(pasilla)__\nLet us explore the effect of this on the data, using the first sample as an example, and comparing it to the \\(_2\\) transformation; the plot is shown in Figure 8.11 and is made with the following:\nj = 1\nggplot(\n  tibble(\n    counts = rep(assay(pasilla)[, j], 2),\n    transformed = c(\n      assay(vsp)[, j],\n      log2(assay(pasilla)[, j])\n      ),\n    transformation = rep(c(\"VST\", \"log2\"), each = nrow(pasilla))\n  ),\n  aes(x = counts, y = transformed, col = transformation)) +\n  geom_line() + xlim(c(0, 600)) + ylim(c(0, 9))__\n\nFigure 8.11: Graph of variance-stabilizing transformation for the data of one of the samples, and for comparison also of the \\(_2\\) transformation. The variance-stabilizing transformation has finite values and finite slope even for counts close to zero, whereas the slope of \\(_2\\) becomes very steep for small counts and is undefined for counts of zero. For large counts, the two transformation are essentially the same.\n\n\n10.11.2.2 Regularized logarithm (rlog) transformation\nThere is a second way to come up with a data transformation. It is conceptually distinct from variance stabilization. Instead, it builds upon the shrinkage estimation that we already explored in Section 8.10.1. It works by transforming the original count data to a \\(2\\)-like scale by fitting a “trivial” model with a separate term for each sample and a prior distribution on the coefficients which is estimated from the data. The fitting employs the same regularization as what we discussed in Section 8.10.1. The transformed data \\(q{ij}\\) are defined by the third line of Equation 8.11, where the design matrix \\((x_{jk})\\) is of size \\(K (K+1)\\) – here \\(K\\) is the number of samples– and has the form\n\\[ X=(]\nWithout priors, this design matrix would lead to a non-unique solution, however the addition of a prior on non-intercept \\(\\)s allows for a unique solution to be found.\nIn DESeq2 , this functionality is implemented in the function rlogTransformation. It turns out in practice that the rlog transformation is also approximately variance- stabilizing, but in contrast to the variance-stabilizing transformation of Section 8.10.2 it deals better with data in which the size factors of the different samples are very distinct.\n__\nQuestion 8.12\nPlot mean against standard deviation between replicates for the shifted logarithm 8.12, the regularized log transformation and the variance- stabilizing transformation.\n__\nSolution\n__\nSee Figure 8.12.\nlibrary(\"vsn\")\nrlp = rlogTransformation(pasilla)\n\nmsd = function(x)\n  meanSdPlot(x, plot = FALSE)$gg + ylim(c(0, 1)) +\n     theme(legend.position = \"none\")\n\ngridExtra::grid.arrange(\n  msd(log2(counts(pasilla, normalized = TRUE) + 1)) +\n    ylab(\"sd(log2)\"),\n  msd(assay(vsp)) + ylab(\"sd(vst)\"),\n  msd(assay(rlp)) + ylab(\"sd(rlog)\"),\n  ncol = 3\n)__\n\nFigure 8.12: Per-gene standard deviation (sd, taken across samples) against the rank of the mean, for the shifted logarithm \\(_2(n+1)\\), the variance-stabilizing transformation (vst) and the rlog. Note that for the leftmost \\(\\) 2,500 genes, the counts are all zero, and hence their standard deviation is zero. The mean-sd dependence becomes more interesting for genes with non-zero counts. Note also the high value of the standard deviation for genes that are weakly detected (but not with all zero counts) when the shifted logarithm is used, and compare to the relatively flat shape of the mean-sd relationship for the variance-stabilizing transformation.\n\n\n\n10.11.3 8.10.3 Dealing with outliers\nThe data sometimes contain isolated instances of very large counts that are apparently unrelated to the experimental or study design, and which may be considered outliers. There are many reasons why outliers can arise, including rare technical or experimental artifacts, read mapping problems in the case of genetically differing samples, and genuine, but rare biological events. In many cases, users appear primarily interested in genes that show a consistent behaviour, and this is the reason why by default, genes that are affected by such outliers are set aside by DESeq. The function calculates, for every gene and for every sample, a diagnostic test for outliers called Cook’s distance(Cook 1977). Cook’s distance is a measure of how much a single sample is influencing the fitted coefficients for a gene, and a large value of Cook’s distance is intended to indicate an outlier count. DESeq2 automatically flags genes with Cook’s distance above a cutoff and sets their p-values and adjusted p-values to NA.\nThe default cutoff depends on the sample size and number of parameters to be estimated; DESeq2 uses the \\(99%\\) quantile of the \\(F(p,m-p)\\) distribution (with \\(p\\) the number of parameters including the intercept and \\(m\\) number of samples).\n__\nQuestion 8.13\nCheck the documentation to see how the default cutoff can be changed, and how the outlier removal functionality can be disabled altogether. How can the computed Cook’s distances be accessed?\nWith many degrees of freedom – i.e., many more samples than number of parameters to be estimated – it might be undesirable to remove entire genes from the analysis just because their data include a single count outlier. An alternate strategy is to replace the outlier counts with the trimmed mean over all samples, adjusted by the size factor for that sample. This approach is conservative: it will not lead to false positives, as it replaces the outlier value with the value predicted by the null hypothesis.\n\n\n10.11.4 8.10.4 Tests of \\(_2\\) fold change above or below a threshold\nLet’s come back to the point we raised in Section 8.6: how to build into the tests our requirement that we want to detect effects that have a strong enough size, as opposed to ones that are statistically significant, but very small. Two arguments to the results function allow for threshold-based Wald tests: lfcThreshold, which takes a numeric of a non-negative threshold value, and altHypothesis, which specifies the kind of test. It can take one of the following four values, where \\(\\) is the \\(_2\\) fold change specified by the name argument, and \\(\\) represents lfcThreshold:\n\ngreater: \\(&gt; \\)\nless: \\(&lt; (-)\\)\ngreaterAbs: \\(|| &gt; \\) (two-tailed test)\nlessAbs: \\(|| &lt; \\) (p-values are the maximum of the upper and lower tests)\n\nThese are demonstrated in the following code and visually by MA-plots in Figure 8.13. (Note that the plotMA method, which is defined in the DESeq2 package, uses base graphics.)\npar(mfrow = c(4, 1), mar = c(2, 2, 1, 1))\nmyMA = function(h, v, theta = 0.5) {\n  plotMA(pasilla, lfcThreshold = theta, altHypothesis = h,\n         ylim = c(-2.5, 2.5))\n  abline(h = v * theta, col = \"dodgerblue\", lwd = 2)\n}\nmyMA(\"greaterAbs\", c(-1, 1))\nmyMA(\"lessAbs\",    c(-1, 1))\nmyMA(\"greater\",          1)\nmyMA(\"less\",         -1   )__\n[](08-chap_files/figure-html/fig-countdata-lfcThresh-1.png “Figure 8.13: MA-plots of tests of \\log_2 fold change with respect to a threshold value. From top to bottom, the tests are for altHypothesis =”greaterAbs”, “lessAbs”, “greater”, and “less”.”)\nFigure 8.13: MA-plots of tests of \\(_2\\) fold change with respect to a threshold value. From top to bottom, the tests are for altHypothesis = \"greaterAbs\", \"lessAbs\", \"greater\", and \"less\".\nTo produce the results tables instead of MA plots, the same arguments as to plotMA (except ylim) would be provided to the results function.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "08-chap.html#summary-of-this-chapter",
    "href": "08-chap.html#summary-of-this-chapter",
    "title": "10  8.1 이 장의 목표",
    "section": "10.12 8.11 Summary of this chapter",
    "text": "10.12 8.11 Summary of this chapter\nWe have seen how to analyze count tables from high-throughput sequencing (and analagous data types) for differential abundance. We built upon the powerful and elegant framework of linear models. In this framework, we can analyze a basic two-groups comparison as well as more complex multifactorial designs, or experiments with covariates that have more than two levels or are continuous. In ordinary linear models, the sampling distribution of the data around the expected value is assumed to be independent and normal, with zero mean and the same variances. For count data, the distributions are discrete and tend to be skewed (asymmetric) with highly different variances across the dynamic range. We therefore employed a generalization of ordinary linear models, called generalized linear models (GLMs), and in particular considered gamma-Poisson distributed data with dispersion parameters that we needed to estimate from the data.\nSince the sampling depth is typically different for different sequencing runs (replicates), we need to estimate the effect of this variable parameter and take it into account in our model. We did this through the size factors \\(s_i\\). Often this part of the analysis is called normalization (the term is not particularly descriptive, but unfortunately it is now well-settled in the literature).\nFor designed experiments, the number of replicates is (and should be) usually too small to estimate the dispersion parameter (and perhaps even the model coefficients) from the data for each gene alone. Therefore we use shrinkage or empirical Bayes techniques, which promise large gains in precision for relatively small costs of bias.\nWhile GLMs let us model the data on their original scale, sometimes it is useful to transform the data to a scale where the data are more homoskedastic and fill out the range more uniformly – for instance, for plotting the data, or for subjecting them to general purpose clustering, dimension reduction or learning methods. To this end, we saw the variance stabilizing transformation.\nA major, and quite valid critique of differential expression testing such as exercised here is that the null hypothesis – the effect size is exactly zero – is almost never true, and therefore our approach does not provide consistent estimates of what the differentially expressed gene are. In practice, this may be overcome by considering effect size as well as statistical significance. Moreover, we saw how to use “banded” null hypotheses.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "08-chap.html#further-reading",
    "href": "08-chap.html#further-reading",
    "title": "10  8.1 이 장의 목표",
    "section": "10.13 8.12 Further reading",
    "text": "10.13 8.12 Further reading\n\nThe DESeq2 method is explained in the paper by Michael I. Love, Huber, and Anders (2014), and practical aspects of the software in the package vignette. See also the edgeR package and paper (Robinson, McCarthy, and Smyth 2009) for a related approach.\nA classic textbook on robust regression and outlier detection is the book by Peter J. Rousseeuw and Leroy (1987). For more recent developments the CRAN task view on Robust Statistical Methods is a good starting point.\nThe Bioconductor RNA-Seq workflow at https://www.bioconductor.org/help/workflows/rnaseqGene (Michael I. Love et al. 2015) covers a number of issues related specifically to RNA-Seq that we have sidestepped here.\nAn extension of the generalized linear model that we saw to detecting alternative exon usage from RNA-Seq data is presented in the DEXSeq paper (Anders, Reyes, and Huber 2012), and applications of these ideas to biological discovery were described by Reyes et al. (2013) and Reyes and Huber (2017).\nFor some sequencing-based assays, such as RIP-Seq, CLIP-Seq, the biological analysis goal boils down to testing whether the ratio of input and immunoprecipitate (IP) has changed between conditions. Mike Love’s post on the Bioconductor forum provides a clear and quick how-to: https://support.bioconductor.org/p/61509.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "08-chap.html#exercises",
    "href": "08-chap.html#exercises",
    "title": "10  8.1 이 장의 목표",
    "section": "10.14 8.13 Exercises",
    "text": "10.14 8.13 Exercises\n__\nExercise 8.1\nDepletion of small p-values. Consider the following simple generative model for a histogram of p-values that shows a depletion of small p-values. In Figure 8.14, p-values are shown from a differential expression analysis (in this case, simple \\(t\\)-tests) in the absence of an association with the tested two-level categorical variable groups. While the histogram is approximately uniform for x1, small p-values are depleted for x2. This is because the batch (encoded by the eponymous variable), which is orthogonal to groups and balanced, introduces additional variability that inflates the denominator of the test statistic.\nlibrary(\"magrittr\")\nng = 10000\nns = 12\nx1 = x2 = matrix(rnorm(ns * ng), ncol = ns, nrow= ng)\ngroup = factor(letters[1 + seq_len(ns) %% 2])  %T&gt;% print __\n\n\n [1] b a b a b a b a b a b a\nLevels: a b\n\n\nbatch = factor(ifelse(seq_len(ns) &lt;= ns/2, \"B1\", \"B2\")) %T&gt;% print __\n\n\n [1] B1 B1 B1 B1 B1 B1 B2 B2 B2 B2 B2 B2\nLevels: B1 B2\n\n\ntable(group, batch)__\n\n\n     batch\ngroup B1 B2\n    a  3  3\n    b  3  3\n\n\nx2[, batch==\"B2\"] = x2[, batch==\"B2\"] + 2 * rnorm(ng)\npvals = rbind(\n  cbind(type = \"x1\", genefilter::rowttests(x1, fac = group)),\n  cbind(type = \"x2\", genefilter::rowttests(x2, fac = group)))\nggplot(pvals, aes(x = p.value)) + \n  geom_histogram(binwidth = 0.02, boundary = 0) +\n  facet_grid(type ~ .)__\nReplace the \\(t\\)-test by a linear model, first, one with only group as a factor, second, one with group + batch (in R’s formula language). Show that the histogram of p-values for the coefficient of group is uniform in both cases, x1 and x2.\n\nFigure 8.14: p-values for the tests performed on x1 and x2 (see code).\n__\nExercise 8.2\nedgeR. Do the analyses of Section 8.5 with the edgeR package and compare the results: make a scatterplot of the \\(_{10}\\) p-values, pick some genes where there are large differences, and visualize the raw data to see what is going on. Based on this can you explain the differences?\n__\nExercise 8.3\nRobustness. Write a shiny app that performs linear regression on an example \\((x, y)\\) dataset (for instance, from the mtcars data) and displays the data as well as the fitted line. Add a widget that lets you move one of the points in \\(x\\)- and/or \\(y\\)- direction in a wide range (extending a few times outside the original data range). Add a radio buttons widget that lets you choose between lm, rlm and lqs with its different choices of method (the latter two are in the MASS package). Bonus: add functions from the robustbase package.\n__\nSolution\n__\nCode for the file ui.R in the app:\nlibrary(\"shiny\")\nshinyUI(fluidPage(\n  titlePanel(\"Breakdown\"),\n  sidebarLayout(\n    sidebarPanel(     # select oulier shift\n      sliderInput(\"shift\", \"Outlier:\", min = 0, max = 100, value = 0),\n      radioButtons(\"method\", \"Method:\",\n                   c(\"Non-robust least squares\" = \"lm\",\n                     \"M-estimation\" = \"rlm\"))\n    ),\n    mainPanel(       # show fit\n      plotOutput(\"regPlot\")\n    )\n  )\n))__\nCode for the file server.R in the app:\nlibrary(\"shiny\")\nlibrary(\"ggplot2\")\nlibrary(\"MASS\")\nshinyServer(function(input, output) {\n  output$regPlot = renderPlot({\n    whpt = 15\n    mtcars_new = mtcars\n    mtcars_new$mpg[whpt] = mtcars_new$mpg[whpt] + input$shift\n    reg = switch(input$method,\n      lm = lm(mpg ~ disp, data = mtcars_new),\n      rlm = rlm(mpg ~ disp, data = mtcars_new),\n      stop(\"Unimplemented method:\", input$method)\n    )\n    ggplot(mtcars_new, aes(x = disp, y = mpg)) + geom_point() +\n      geom_abline(intercept = reg$coefficients[\"(Intercept)\"],\n                  slope = reg$coefficients[\"disp\"], col = \"blue\")\n  })\n})__\n물론 더 많은 기능을 추가할 수 있습니다.\nAnders, Simon, and Wolfgang Huber. 2010. “Differential Expression Analysis for Sequence Count Data.” Genome Biology 11: R106. http://genomebiology.com/2010/11/10/R106.\nAnders, Simon, Alejandro Reyes, and Wolfgang Huber. 2012. “Detecting differential usage of exons from RNA-Seq data.” Genome Research 22 (10): 2008–17.\nBrooks, Angela N, Li Yang, Michael O Duff, Kasper D Hansen, Jung W Park, Sandrine Dudoit, Steven E Brenner, and Brenton R Graveley. 2011. “Conservation of an RNA Regulatory Map Between Drosophila and Mammals.” Genome Research , 193–202. https://doi.org/10.1101/gr.108662.110.\nCook, R. Dennis. 1977. “Detection of Influential Observation in Linear Regression.” Technometrics.\nHuber, Peter J. 1964. “Robust Estimation of a Location Parameter.” The Annals of Mathematical Statistics 35: 73–101.\nLeek, Jeffrey T., and John D. Storey. 2007. “Capturing heterogeneity in gene expression studies by surrogate variable analysis.” PLoS Genetics 3 (9): 1724–35.\nLove, Michael I., Simon Anders, Vladislav Kim, and Wolfgang Huber. 2015. “RNA- Seq Workflow: Gene-Level Exploratory Analysis and Differential Expression.” F1000Research 4 (1070). https://doi.org/10.12688/f1000research.7035.1.\nLove, Michael I, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation of Fold Change and Dispersion for RNA-seq Data with DESeq2.” Gnome Biology 15 (12): 1–21.\nOzsolak, Fatih, and Patrice M Milos. 2011. “RNA sequencing: advances, challenges and opportunities.” Nature Reviews Genetics 12: 87–98.\nReyes, Alejandro, Simon Anders, Robert J. Weatheritt, Toby J. Gibson, Lars M. Steinmetz, and Wolfgang Huber. 2013. “Drift and Conservation of Differential Exon Usage Across Tissues in Primate Species.” Proceedings of the National Academy of Sciences 110 (38): 15377–82. https://doi.org/10.1073/pnas.1307202110.\nReyes, Alejandro, and Wolfgang Huber. 2017. “Alternative Start and Termination Sites of Transcription Drive Most Transcript Isoform Differences Across Human Tissues.” Nucleic Acids Research 46 (2): 582–92. https://doi.org/10.1093/nar/gkx1165.\nRobinson, M. D., D. J. McCarthy, and G. K. Smyth. 2009. “edgeR: A Bioconductor Package for Differential Expression Analysis of Digital Gene Expression Data.” Bioinformatics 26 (1): 139–40. https://doi.org/10.1093/bioinformatics/btp616.\nRousseeuw, Peter J. 1987. “Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis.” Journal of Computational and Applied Mathematics 20: 53–65.\nRousseeuw, Peter J., and Annick M. Leroy. 1987. Robust Regression and Outlier Detection. Wiley. https://doi.org/10.1002/0471725382.\nStegle, O., L. Parts, R. Durbin, and J. Winn. 2010. “A Bayesian framework to account for some complex non-genetic factors in gene expression levels greatly increases power in eQTL studies.” PLoS Computational Biology 6 (5): e1000770.\nSteijger, T., J. F. Abril, P. G. Engstrom, F. Kokocinski, T. J. Hubbard, R. Guigo, J. Harrow, et al. 2013. “Assessment of transcript reconstruction methods for RNA-seq.” Nature Methods 10 (12): 1177–84.\nVerhulst, Pierre-François. 1845. “Recherches mathématiques Sur La Loi d’accroissement de La Population.” Nouveaux Mémoires de l’Académie Royale Des Sciences Et Belles-Lettres de Bruxelles 18: 1–42.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>8.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "09-chap.html",
    "href": "09-chap.html",
    "title": "11  9.1 이 장의 목표",
    "section": "",
    "text": "11.1 9.2 다차원 척도법과 서열화\n7장에서 우리는 열이 연속 변수인 직사각형 행렬을 요약하는 방법을 보았습니다. 우리가 만든 지도는 모든 열이 의미 있는 분산을 가질 때 행렬 \\(X\\)에서 가장 중요한 신호 성분을 분리하는 것을 목표로 하는 주성분 분석과 같은 비지도 차원 축소 기법을 사용했습니다.\n여기서 우리는 이러한 아이디어를 연속 변수와 범주형 변수가 결합된 더 복잡한 이종 데이터로 확장합니다. 실제로 때때로 우리의 관측치는 개별 변수나 좌표 세트로 쉽게 설명될 수 없지만, 그들 사이의 거리나 (비)유사성을 결정하거나 그래프나 트리를 사용하여 그들 사이의 관계를 설명하는 것이 가능합니다. 예로는 종 계통수의 종이나 생물학적 서열이 있습니다. 생물학 이외의 예로는 텍스트 문서나 영화 파일이 있는데, 여기서 우리는 그들 사이의 (비)유사성을 결정하는 합리적인 방법을 가질 수 있지만 명백한 변수나 좌표는 없습니다.\n이 장에는 더 고급 기법들이 포함되어 있으며, 이를 위해 종종 기술적인 세부 사항은 생략합니다. 여기까지 온 만큼, 예제를 통한 실습 경험과 광범위한 참고 문헌을 제공하여 여러분이 비선형 다변량 분석에서 더 ’최첨단’인 기법 중 일부를 이해하고 사용할 수 있기를 바랍니다.\n이 장에서 우리는 다음을 수행할 것입니다:\n때때로 데이터는 특징 공간의 점으로 표현되지 않습니다. 이는 \\({R}^n\\)에 명백한 좌표가 없는 약물, 이미지, 트리 또는 기타 복잡한 객체와 같은 객체 사이의 (비)유사성 행렬이 제공될 때 발생할 수 있습니다.\n5장에서 우리는 거리로부터 _클러스터_를 생성하는 방법을 보았습니다. 여기서 우리의 목표는 PCA의 처음 몇몇 주축으로 만든 것과 유사하게, 저차원 공간(예: 평면)의 맵에 데이터를 시각화하는 것입니다.\n우리는 지리 데이터를 사용하는 직관적인 예제로 시작합니다. 그림 9.1에는 우크라이나의 도시와 장소 사이의 거리에 대한 열도(heatmap)와 클러스터링이 나와 있습니다1.\n1 이 데이터의 출처는 data 폴더의 ukraine-dists.R 스크립트에 설명되어 있습니다.\n그림 9.1: ukraine_dists 거리 행렬의 열도. 거리는 킬로미터 단위로 측정됩니다. 함수가 도시의 순서를 재배열하고 가장 가까운 것들을 그룹화했습니다.\n쌍별 거리를 포함하는 ukraine_dists 외에도, 위에서 로드한 RData 파일에는 경도와 위도가 포함된 ukraine_coords 데이터 프레임도 포함되어 있습니다. 우리는 이를 나중에 실측 자료(ground truth)로 사용할 것입니다. 거리가 주어지면 다차원 척도법(MDS)은 그들의 상대적 위치에 대한 “지도”를 제공합니다. 도시들이 2D 평면상에서 갖는 유클리드 거리가 주어진 거리 행렬을 정확하게 재현하도록 배치하는 것은 불가능할 것입니다. 도시들은 평면이 아니라 지구의 곡면 위에 있기 때문입니다. 그럼에도 불구하고, 우리는 데이터를 잘 표현하는 2차원 임베딩을 찾을 수 있을 것으로 기대할 수 있습니다. 생물학적 데이터의 경우, 우리의 2차원 임베딩은 이보다 훨씬 덜 명확할 가능성이 높습니다. 다음과 같이 함수를 호출합니다:\n이 장에서 여러 번 재사용하여 cmdscale 함수의 호출 결과로부터 스크리 플롯(screeplot)을 만드는 함수를 만듭니다:\n그림 9.2: 처음 네 개의 고윳값에 대한 스크리 플롯. 처음 두 개의 고윳값 이후에 뚜렷한 감소가 있으며, 이는 데이터가 2차원 임베딩으로 잘 설명됨을 나타냅니다.\n__\n질문 9.1\ncmdscale 함수에 의해 출력된 모든 고윳값들을 살펴보세요: 무엇을 발견했나요?\n__\n해결책\n__\n다음을 실행하면:\n그림 9.3: 모든 고윳값에 대한 스크리 플롯.\nPCA와 달리 일부 음의 고윳값이 있음을 알 수 있습니다. 이는 cmdscale이 작동하는 방식 때문입니다.\ncmdscale 함수의 주요 출력은 2차원 임베딩의 좌표이며, 이를 그림 9.4에 나타냈습니다(알고리즘의 작동 방식은 다음 섹션에서 논의할 것입니다).\n그림 9.4: 거리 기반의 MDS 맵.\n상대적인 위치는 정확하지만 지도의 방향이 관습적이지 않다는 점에 유의하세요: 크림 반도가 상단에 있습니다. 이는 거리로부터 평면 임베딩을 재구성하는 방법들에서 흔히 나타나는 현상입니다. 점들 사이의 거리는 회전과 반사(축 뒤집기)에 대해 불변이기 때문에, 회전이나 반사를 통해 연관되는 어떠한 솔루션도 다른 솔루션만큼 좋습니다. cmdscale과 같은 함수들은 똑같이 최적인 솔루션들 중 하나를 선택할 것이며, 그 구체적인 선택은 데이터의 미세한 세부 사항이나 사용 중인 컴퓨팅 플랫폼에 따라 달라질 수 있습니다. 여기서 우리는 \\(y\\)축의 부호를 반전시켜 결과를 더 관습적인 방향으로 변환할 수 있습니다. 그림 9.5에서 지도를 다시 그리고, 이를 ukraine_coords 데이터 프레임의 실제 경도 및 위도와 비교합니다(그림 9.6).\n그림 9.5: 그림 9.4와 같지만, y축이 뒤집힌 모습.\n그림 9.6: ukraine_coords 데이터 프레임에서 가져온 실제 위도와 경도.\n__\n질문 9.2\n우리는 종횡비에 주의를 기울이지 않고 그림 9.6의 오른쪽 패널에 경도와 위도를 그렸습니다. 이 플롯에 적절한 종횡비는 얼마일까요?\n__\n해결책\n__\n경도의 1도 변화에 해당하는 거리와 위도의 1도 변화에 해당하는 거리 사이에는 단순한 관계가 없으므로 선택하기가 어렵습니다. 지구가 구형이고 반지름이 6371km라는 단순화된 가정 하에서도 복잡합니다: 위도의 1도는 항상 111km 거리(\\(6371/360\\))에 해당하며, 적도에서의 경도 1도 마찬가지입니다. 그러나 적도에서 멀어지면 경도 1도는 점점 더 짧은 거리에 해당하게 됩니다(극점에서는 거리가 아예 없습니다). 실용적으로 그림 9.6과 같은 디스플레이의 경우, 가장 북쪽 지점과 남쪽 지점의 중간 정도인 48도에 대한 코사인 값을 종횡비로 선택할 수 있습니다.\n__\n질문 9.3\n그림 9.6에 국제 경계와 강 같은 지리적 특징을 추가해 보세요.\n__\n해결책\n__\n국제 경계를 다각형으로 추가하는 아래 코드가 시작점이 될 수 있습니다(그림 9.7).\n벡터 및 래스터 데이터 유형을 포함하여 지리 공간 데이터를 위해 R에서 사용할 수 있는 추가 인프라가 많이 있습니다.\n그림 9.7: 그림 9.6에 국제 경계가 추가된 모습.\n참고: MDS는 PCA와 유사한 출력을 생성하지만, 데이터(샘플 포인트)에 대한 한 종류의 ‘차원’만 가집니다. ’쌍대(dual)’ 차원이 없으며, 바이플롯이나 로딩 벡터가 없다는 점입니다. 이는 맵을 해석할 때 단점이 됩니다. 극단적인 점들과 그들 사이의 차이점을 주의 깊게 살펴봄으로써 해석을 용이하게 할 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "09-chap.html#다차원-척도법과-서열화",
    "href": "09-chap.html#다차원-척도법과-서열화",
    "title": "11  9.1 이 장의 목표",
    "section": "",
    "text": "library(\"pheatmap\")\ndata(\"ukraine_dists\", package = \"MSMB\")\nas.matrix(ukraine_dists)[1:4, 1:4]__\n\n\n               Kyiv    Odesa Sevastopol Chernihiv\nKyiv         0.0000 441.2548   687.7551  128.1287\nOdesa      441.2548   0.0000   301.7482  558.6483\nSevastopol 687.7551 301.7482     0.0000  783.6561\nChernihiv  128.1287 558.6483   783.6561    0.0000\n\n\npheatmap(as.matrix(ukraine_dists), \n  color = colorRampPalette(c(\"#0057b7\", \"#ffd700\"))(50),\n  breaks = seq(0, max(ukraine_dists)^(1/2), length.out = 51)^2,\n  treeheight_row = 10, treeheight_col = 10)__\n\n\n\nukraine_mds = cmdscale(ukraine_dists, eig = TRUE)__\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nplotscree = function(x, m = length(x$eig)) {\n  ggplot(tibble(eig = x$eig[seq_len(m)], k = seq(along = eig)),\n    aes(x = k, y = eig)) + theme_minimal() +\n    scale_x_discrete(\"k\", limits = as.factor(seq_len(m))) + \n    geom_bar(stat = \"identity\", width = 0.5, fill = \"#ffd700\", col = \"#0057b7\")\n}__\n\n\nplotscree(ukraine_mds, m = 4)__\n\n\n\n\n\n\n\n\n\nukraine_mds$eig |&gt; signif(3)__\n\n\n [1]  3.91e+06  1.08e+06  3.42e+02  4.84e-01  2.13e-01  3.83e-05  5.90e-06\n [8]  5.82e-07  8.79e-08  4.94e-08  6.52e-10  2.84e-10  1.84e-10  5.22e-11\n[15]  4.89e-11  4.57e-11 -3.26e-12 -2.55e-11 -5.90e-11 -6.55e-11 -1.40e-10\n[22] -1.51e-10 -3.46e-10 -3.76e-10 -4.69e-10 -2.24e-09 -1.51e-08 -9.60e-05\n[29] -2.51e-04 -1.41e-02 -1.19e-01 -3.58e+02 -8.85e+02\n\n\nplotscree(ukraine_mds)__\n\n\n\n\nukraine_mds_df = tibble(\n  PCo1 = ukraine_mds$points[, 1],\n  PCo2 = ukraine_mds$points[, 2],\n  labs = rownames(ukraine_mds$points)\n)\nlibrary(\"ggrepel\")\ng = ggplot(ukraine_mds_df, aes(x = PCo1, y = PCo2, label = labs)) +\n  geom_point() + geom_text_repel(col = \"#0057b7\") + coord_fixed() \ng __\n\n\n\ng %+% mutate(ukraine_mds_df, PCo1 = PCo1, PCo2 = -PCo2)__\n\n\ndata(\"ukraine_coords\", package = \"MSMB\")\nprint.data.frame(ukraine_coords[1:4,  c(\"city\", \"lat\", \"lon\")])__\n\n\n        city      lat      lon\n1       Kyiv 50.45003 30.52414\n2      Odesa 46.48430 30.73229\n3 Sevastopol 44.60544 33.52208\n4  Chernihiv 51.49410 31.29433\n\n\nggplot(ukraine_coords, aes(x = lon, y = lat, label = city)) +\n  geom_point() + geom_text_repel(col = \"#0057b7\")__\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(\"maps\")\nua_borders = dplyr::filter(map_data(\"world\"), region == \"Ukraine\")\nggplot(ukraine_coords, aes(x = lon, y = lat)) + \n  geom_polygon(data = ua_borders, aes(x = long, y = lat, group = subregion), fill = \"#ffd700\", color = \"#0057b7\") +\n  geom_point() + \n  geom_text_repel(aes(label = city)) +\n  coord_fixed(1/cos(48/180*pi))__\n\n\n\n\n\n11.1.1 9.2.1 방법은 어떻게 작동하나요?\n좌표가 알려진 점들로부터 실제로 시작했다면 어떤 일이 일어날지 살펴봅시다2. 우리는 이 좌표들을 점의 수만큼의 행을 가진 행렬의 두 열에 넣습니다. 이제 이 좌표들을 바탕으로 점들 사이의 거리를 계산합니다. 좌표 \\(X\\)에서 거리로 가려면 다음과 같이 씁니다: \\[d^2_{i,j} = (x_i^1 - x_j1)2 + + (x_i^p - x_jp)2.\\] 우리는 제곱 거리 행렬을 R에서 DdotD라고 부르고 텍스트에서는 \\(DD\\)라고 부를 것입니다. 우리는 그 거리의 제곱이 관측된 \\(DD\\)에 가능한 한 가까운 점들을 찾고 싶습니다.3\n이는 \\(D\\)와 그 자신과의 행렬 곱인 \\(DD\\) 또는 \\(D^2\\)과는 다릅니다.\n3 여기서는 도시의 경도와 위도를 데카르트 좌표로 사용하고, 그것들이 구형 표면 위의 곡선 좌표라는 사실을 무시함으로써 약간의 ’남용’을 저지르고 있습니다.\n2 여기서는 우리 도시의 경도와 위도를 데카르트 좌표로 사용하고 지구 표면의 곡률을 무시함으로써 약간의 ’남용’을 저지르고 있습니다. 하버사인(Haversine) 공식에 대한 정보를 인터넷에서 찾아보세요.\nX = with(ukraine_coords, cbind(lon, lat * cos(48)))\nDdotD = as.matrix(dist(X)^2)__\n상대적 거리는 데이터의 원점에 의존하지 않습니다. 우리는 \\(H=I-^t\\)로 정의된 중앙화 행렬(centering matrix) \\(H\\)를 사용하여 데이터를 중앙에 맞춥니다. 다음을 사용하여 \\(H\\)의 중앙화 속성을 확인해 봅시다:\nn = nrow(X)\nH = diag(rep(1,n))-(1/n) * matrix(1, nrow = n, ncol = n)\nXc = sweep(X,2,apply(X,2,mean))\nXc[1:2, ]__\n\n\n            lon          \n[1,] -1.1722946 -1.184705\n[2,] -0.9641429  1.353935\n\n\nHX = H %*% X\nHX[1:2, ]__\n\n\n            lon          \n[1,] -1.1722946 -1.184705\n[2,] -0.9641429  1.353935\n\n\napply(HX, 2, mean)__\n\n\n          lon               \n-1.618057e-15  1.747077e-16 \n__\n질문 9.4\nDdotD의 오른쪽과 왼쪽에 모두 중앙화 행렬을 적용하여 얻은 행렬을 B0라고 합시다. \\(HX\\) 행렬에 의해 주어지는 원점에 중앙화된 점들을 고려하고 그 교차 곱(cross product)을 계산하여 이를 B2라고 부릅시다. B0를 B2와 같게 만들려면 B0에 무엇을 해야 할까요?\n__\n해결책\n__\nB0 = H  %*% DdotD %*% H\nB2 = HX %*% t(HX)\nB2[1:3, 1:3] / B0[1:3, 1:3]__\n\n\n     [,1] [,2] [,3]\n[1,] -0.5 -0.5 -0.5\n[2,] -0.5 -0.5 -0.5\n[3,] -0.5 -0.5 -0.5\n\n\nmax(abs(-0.5 * B0 - B2))__\n\n\n[1] 9.237056e-14\n따라서, 행들 사이의 제곱 거리(\\(DD\\))와 중앙화된 행렬의 교차 곱 \\(B=(HX)(HX)^t\\)가 주어졌을 때, 우리는 다음을 보였습니다:\n\\[ - H(DD) H=B \\]\n이는 항상 참이며, 처음에 \\(DD\\)가 주어졌을 때 식 9.1을 만족하는 \\(X\\)를 역설계하는 데 이를 사용합니다.\n\n11.1.1.1 특잇값 벡터를 사용하여 \\(DD\\)에서 \\(X\\) 구하기.\n식 9.1에서 정의된 \\(B\\)의 고유 분해(eigen-decomposition)를 취함으로써 행렬 \\(DD\\)에서 \\(X\\)로 거꾸로 갈 수 있습니다. 이는 또한 우리가 \\(X\\) 행렬에 대해 몇 개의 좌표 또는 열을 원하는지 선택할 수 있게 해줍니다. 이는 PCA가 최선의 계수 \\(r\\) 근사를 제공하는 방식과 매우 유사합니다.\n참고 : PCA에서와 마찬가지로, 이를 \\(HX\\)의 특잇값 분해(또는 \\(HX(HX)^t\\)의 고유 분해)를 사용하여 다음과 같이 쓸 수 있습니다:\n\\[ HX^{(r)} = US{(r)}Vt S^{(r)} r , \\]\n이는 \\(r\\)차원 유클리드 공간에서의 최선의 근사 표현을 제공합니다. 이 알고리즘은 \\(D\\) 행렬에 의해 제공된 것과 대략 동일한 거리를 가진 점들의 좌표를 우리에게 제공합니다.\n\\[S^{(r)} = ] 이 방법은 종종 주좌표 분석(Principal Coordinates Analysis) 또는 PCoA라고 불리며, 이는 PCA와의 연결성을 강조합니다.\n\n\n11.1.1.2 고전적 MDS 알고리즘.\n요약하자면, 점 간 제곱 거리의 \\(n n\\) 행렬 \\(DD\\)가 주어졌을 때, 다음 연산들을 통해 점들과 그들의 좌표 \\(\\)를 찾을 수 있습니다:\n\n점 간 제곱 거리를 이중 중앙화(double center)하고 \\(-\\)을 곱합니다:\n\\(B = -H DD H\\).\n\\(B\\)를 대각화합니다: \\(B = U U^t\\).\n\\(\\)를 추출합니다: \\( = U ^{1/2}\\).\n\n\n\n11.1.1.3 적절한 기저 차원 찾기.\n예시로서, 유사성(거리를 대신함)은 있지만 자연적인 기저 유클리드 공간은 없는 객체들을 살펴보겠습니다.\n1950년대의 한 심리학 실험에서, Ekman (1954)은 31명의 피험자에게 14가지 서로 다른 색상의 유사성을 평가하도록 요청했습니다. 그의 목표는 색상 인지의 기저 차원을 이해하는 것이었습니다. 유사성 또는 혼동 행렬(confusion matrix)은 0과 1 사이의 값을 갖도록 스케일이 조정되었습니다. 자주 혼동되는 색상들은 1에 가까운 유사성을 가졌습니다. 우리는 값들을 1에서 뺌으로써 데이터를 비유사성(dissimilarity)으로 변환합니다:\nekm = read.table(\"../data/ekman.txt\", header=TRUE)\nrownames(ekm) = colnames(ekm)\ndisekm = 1 - ekm - diag(1, ncol(ekm))\ndisekm[1:5, 1:5]__\n\n\n     w434 w445 w465 w472 w490\nw434 0.00 0.14 0.58 0.58 0.82\nw445 0.14 0.00 0.50 0.56 0.78\nw465 0.58 0.50 0.00 0.19 0.53\nw472 0.58 0.56 0.19 0.00 0.46\nw490 0.82 0.78 0.53 0.46 0.00\n\n\ndisekm = as.dist(disekm)__\n우리는 MDS 좌표와 고윳값을 계산합니다. 우리는 그림 9.8에 표시된 스크리 플롯에 고윳값들을 결합합니다:\nmdsekm = cmdscale(disekm, eig = TRUE)\nplotscree(mdsekm)__\n\n그림 9.8: 스크리 플롯은 현상이 주로 2차원적임을 보여줍니다.\n우리는 다음과 같이 처음 두 개의 주좌표를 사용하여 서로 다른 색상들을 플롯합니다:\ndfekm = mdsekm$points[, 1:2] |&gt;\n  `colnames&lt;-`(paste0(\"MDS\", 1:2)) |&gt;\n  as_tibble() |&gt;\n  mutate(\n    name = rownames(ekm),\n    rgb = photobiology::w_length2rgb(as.numeric(sub(\"w\", \"\", name))))\nggplot(dfekm, aes(x = MDS1, y = MDS2)) +\n  geom_point(col = dfekm$rgb, size = 4) +\n  geom_text_repel(aes(label = name)) + coord_fixed()__\n\n그림 9.9: 처음 두 차원에서의 산점도 배치는 말편자(horseshoe) 모양을 하고 있습니다. 레이블과 색상은 아치형 구조가 파장에 대응함을 보여줍니다.\n그림 9.9는 새로운 좌표계에서의 Ekman 데이터를 보여줍니다. 설명을 요약하는 눈에 띄는 패턴이 있습니다. 점들의 이러한 말편자 또는 아치 구조는 종종 데이터에 순차적인 잠재적 순서나 그래디언트가 존재함을 나타내는 지표입니다 (Diaconis, Goel, and Holmes 2008). 우리는 9.5절에서 이를 다시 살펴볼 것입니다.\n\n\n\n11.1.2 9.2.2 MDS의 강건한 버전들\n다차원 척도법은 \\(DD\\)에 의해 주어지는 제곱 거리와 새로운 좌표를 가진 점들 사이의 제곱 거리 간의 차이를 최소화하는 것을 목표로 합니다. 불행히도, 이 목적 함수는 이상치에 민감한 경향이 있습니다: 다른 모든 점들과의 거리가 큰 단 하나의 데이터 포인트가 분석 전체를 지배하고 왜곡할 수 있습니다. 종종 우리는 좀 더 강건한(robust) 무언가를 사용하고 싶어 하며, 이를 달성하는 한 가지 방법은 거리의 실제 값을 무시하고 원래 거리와 새로운 거리의 상대적 순위(rankings)가 가능한 한 비슷할 것만을 요구하는 것입니다. 이러한 순위 기반 접근 방식은 강건합니다: 이상치에 대한 민감도가 줄어듭니다.\n강건성(Robustness): 소수의 이상치에 의해 너무 크게 영향을 받지 않는다면 그 방법은 강건합니다. 예를 들어, \\(n\\)개 숫자 세트의 중앙값은 우리가 20개 숫자를 임의로 크게 바꾸더라도 크게 변하지 않습니다; 중앙값을 급격하게 이동시키려면 숫자들의 절반 이상을 바꾸어야 합니다. 대조적으로, 평균은 단 하나의 숫자만 조작해도 크게 바꿀 수 있습니다. 우리는 중앙값의 _붕괴점(breakdown point)_이 1/2인 반면, 평균의 붕괴점은 오직 \\(1/n\\)이라고 말합니다. 평균과 중앙값 모두 분포의 위치(location) (즉, 숫자들의 “전형적인” 값이 무엇인지)에 대한 추정량이지만, 중앙값이 더 강건합니다. 중앙값은 순위에 기반합니다; 더 일반적으로, 순위에 기반한 방법들은 종종 실제 값에 기반한 방법들보다 더 강건합니다. 많은 비모수적 검정들은 데이터를 그들의 순위로 축소하는 것에 기반합니다.\n우리는 측정값의 ’스케일’에 대해 확신이 없을 때 강건한 방법들이 얼마나 유용한지 보여주기 위해 Ekman 데이터를 사용할 것입니다. 비계량 다차원 척도법(non metric multidimensional scaling, 줄여서 NMDS)이라 불리는 강건한 서열화는, 오직 새로운 맵에서 재구성된 거리의 순서가 원래 거리 행렬의 순서와 동일하도록 점들을 새로운 공간에 임베딩하려고 시도합니다.\n비계량 MDS는 행렬 \\(d\\)에 주어진 비유사성의 변환 \\(f\\)와, 이 새로운 맵에서의 거리 \\(\\)가 \\(f(d)\\)가 되도록 하는 낮은 차원 공간(맵)에서의 좌표 세트를 찾습니다. 근사의 품질은 표준화된 잔차 제곱합(stress) 함수로 측정될 수 있습니다:\n\\[ ^2=. \\]\nNMDS는 처음에 기저 차원수를 지정해야 하고 그 숫자에 따라 거리의 재구성을 최대화하도록 최적화가 실행된다는 점에서 순차적(sequential)이지 않습니다. PCA에서 제공되는 것처럼 개별 축에 의해 설명되는 변동의 백분율이라는 개념은 없습니다. 그러나 모든 연속적인 \\(k\\) 값(\\(k=1, 2, 3, …\\))에 대해 프로그램을 실행하고 스트레스(stress)가 얼마나 잘 떨어지는지 살펴봄으로써 유사-스크리 플롯(simili-screeplot)을 만들 수 있습니다. 여기 이러한 연속적인 근사치들과 그들의 적합도를 살펴보는 예시가 있습니다. 군집화를 위한 진단 사례에서와 마찬가지로, 우리는 스트레스가 급격히 떨어진 후의 축의 개수를 택할 것입니다.\nNMDS 결과의 각 계산은 무작위적이면서 \\(k\\) 값에 의존하는 새로운 최적화를 필요로 하므로, 우리는 4장에서 군집화를 위해 했던 것과 유사한 절차를 사용합니다. 우리는 metaMDS 함수를 각 네 가지 가능한 \\(k\\) 값에 대해 예를 들어 100번씩 실행하고 스트레스 값들을 기록합니다.\nlibrary(\"vegan\")\nnmds.stress = function(x, sim = 100, kmax = 4) {\n  sapply(seq_len(kmax), function(k)\n    replicate(sim, metaMDS(x, k = k, autotransform = FALSE)$stress))\n}\nstress = nmds.stress(disekm, sim = 100)\ndim(stress)__\n결과의 박스플롯을 살펴봅시다. 이는 \\(k\\)를 선택하기 위한 유용한 진단 플롯이 될 수 있습니다 (그림 9.10).\ndfstr = reshape2::melt(stress, varnames = c(\"replicate\",\"dimensions\"))\nggplot(dfstr, aes(y = value, x = dimensions, group = dimensions)) +\n  geom_boxplot()__\n\n그림 9.10: 스트레스의 안정성을 평가하기 위해 각 차원에서 여러 번의 반복 실험이 수행되었습니다. 우리는 2차원 이상에서 스트레스가 급격히 떨어지는 것을 볼 수 있으며, 이는 여기서 2차원 솔루션이 적절함을 나타냅니다.\n또한 예를 들어 \\(k=2\\)에 대해 셰퍼드 플롯(Shepard plot)이라고 알려진 것을 사용하여 거리와 그 근사치를 비교할 수 있습니다:\nnmdsk2 = metaMDS(disekm, k = 2, autotransform = FALSE)\nstressplot(nmdsk2, pch = 20)__\n\n그림 9.11: 셰퍼드 플롯은 원래의 거리 또는 비유사성(가로축)을 재구성된 거리(세로축, 이 경우 \\(k=2\\))와 비교합니다.\n그림 9.11의 셰퍼드 플롯과 그림 9.10의 스크리 플롯 모두 Ekman의 색상 혼동 연구에 대해 2차원 솔루션을 가리킵니다. 두 가지 서로 다른 MDS 프로그램인 고전적 계량 최소제곱 근사법과 비계량 순위 근사법의 출력을 비교해 봅시다. 그림 9.12의 오른쪽 패널은 비계량 순위 근사의 결과를 보여주며, 왼쪽 패널은 그림 9.9와 동일합니다. 두 경우 모두 투영이 거의 동일합니다. 이 데이터의 경우, 유클리드 다차원 척도법을 사용하든 비계량 다차원 척도법을 사용하든 거의 차이가 없습니다.\nnmdsk2$points[, 1:2] |&gt; \n  `colnames&lt;-`(paste0(\"NmMDS\", 1:2)) |&gt;\n  as_tibble() |&gt; \n  bind_cols(dplyr::select(dfekm, rgb, name)) |&gt;\n  ggplot(aes(x = NmMDS1, y = NmMDS2)) +\n    geom_point(col = dfekm$rgb, size = 4) +\n    geom_text_repel(aes(label = name))__\n\n\n\n\n\n\n\n\n그림 9.12: (a) 고전적 다차원 척도법(그림 9.9와 동일)과 (b) 비계량 버전의 출력 비교.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "09-chap.html#연속-정보-또는-보조-정보",
    "href": "09-chap.html#연속-정보-또는-보조-정보",
    "title": "11  9.1 이 장의 목표",
    "section": "11.2 9.3 연속 정보 또는 보조 정보",
    "text": "11.2 9.3 연속 정보 또는 보조 정보\n\n\n\nMetadata: Many programs and workflows for biological sequence analysis or assays separate the environmental and contextual information, which they call metadata, from the assay data or sequence reads. We discourage such practice as the exact connections between the samples and covariates are important. A lost connection between the assays and covariates makes later analyses impossible. Covariates such as clinical history, time, batch or location are important and should be considered components of the data.\n\n\nMetadata: Many programs and workflows for biological sequence analysis or assays separate the environmental and contextual information, which they call metadata , from the assay data or sequence reads. We discourage such practice as the exact connections between the samples and covariates are important. A lost connection between the assays and covariates makes later analyses impossible. Covariates such as clinical history, time, batch or location are important and should be considered components of the data.\nIn Chapter 3 we introduced the R data.frame class that enables us to combine heterogeneous data types: categorical factors, text and continuous values. Each row of a dataframe corresponds to an object, or a record, and the columns are the different variables, or features.\nExtra information about sample batches, dates of measurement, different protocols are often named metadata ; this can be misnomer if it is implied that metadata are somehow less important. Such information is real data that need to be integrated into the analyses. We typically store it in a data.frame or a similar R class and tightly link it to the primary assay data.\n\n11.2.1 9.3.1 Known batches in data\nHere we show an example of an analysis that was done by Holmes et al. (2011) on bacterial abundance data from Phylochip (Brodie et al. 2006) microarrays. The experiment was designed to detect differences between a group of healthy ## 9.3 불균질 데이터 연구: 배치를 보조 정보로 사용하기\n우리는 종종 관심 있는 실험 변수 외에도, 그 효과를 가릴 수 있는 보조 정보(성가신 공변량)를 가지고 있습니다. 예를 들어, 샘플이 수집된 날짜나 시퀀싱 프로토콜 등이 이에 해당합니다. 이러한 “배치 효과(batch effects)”를 시각화하고 보정하는 것은 데이터 분석의 필수적인 단계입니다.\n\n\n11.2.2 9.3.1 사례 연구: 쥐 마이크로바이옴 데이터\n건강한 쥐 그룹과 과민성 대장 증후군(IBS)을 앓고 있는 쥐 그룹으로부터 얻은 마이크로바이옴 칩 데이터 세트를 고려해 봅시다 (Nelson et al. 2010). 이 예시는 실험 데이터 분석에서 성가신 배치 효과가 어떻게 나타나는지 보여줍니다. 이는 데이터 분석의 모범 사례가 순차적이어야 한다는 것과, 실험 설계상의 심각한 문제가 발생할 때마다 이를 조정하기 위해 데이터를 수집되는 대로 분석하는 것이 사후에 처리하는 것보다 더 낫다는 사실을 잘 보여줍니다4.\n4 피셔(Fisher)의 용어, 13장을 참조하십시오.\n이 프로젝트에서 데이터 수집이 시작되었을 때, 1일째와 2일째의 데이터가 전달되었고 우리는 그림 9.14에 나타난 플롯을 만들었습니다. 이는 분명한 날짜(day) 효과를 보여주었습니다. 이 효과의 원인을 조사한 결과, 1일째와 2일째에 사용된 프로토콜과 칩(array)이 모두 달랐음을 발견했습니다. 이는 변동의 원인에 대한 불확실성을 초래하며, 우리는 이를 효과의 혼동(confounding)이라고 부릅니다.\n\n\n\nBioconductor 컨테이너: 이 데이터는 배치 정보를 실제 데이터와 결합하는 다소 어색한 방식의 예입니다. 날짜 정보가 어레이 데이터와 결합되어 숫자로 인코딩되었으며, 이는 연속형 변수와 혼동될 수 있습니다. 다음 섹션에서는 Bioconductor의 SummarizedExperiment라는 컨테이너를 사용하여 불균질한 데이터를 저장하고 조작하는 더 나은 실무 방식을 살펴볼 것입니다.\n\n\nBioconductor 컨테이너: 이 데이터는 배치 정보를 실제 데이터와 결합하는 다소 어색한 방식의 예입니다. day 정보가 어레이 데이터와 결합되어 숫자로 인코딩되었으며, 이는 연속형 변수와 혼동될 수 있습니다. 다음 섹션에서는 Bioconductor의 _SummarizedExperiment_라는 컨테이너를 사용하여 불균질한 데이터를 저장하고 조작하는 더 나은 실무 방식을 살펴볼 것입니다.\n우리는 데이터를 불러오고 이 섹션에서 사용할 패키지들을 로드합니다:\nIBDchip = readRDS(\"../data/vsn28Exprd.rds\")\nlibrary(\"ade4\")\nlibrary(\"factoextra\")\nlibrary(\"sva\")__\n__\n질문 9.5\nIBDchip의 클래스는 무엇인가요? 행렬의 마지막 행을 살펴보세요. 무엇을 알 수 있나요?\n__\n해결책\n__\nclass(IBDchip)__\n\n\n[1] \"matrix\" \"array\" \n\n\ndim(IBDchip)__\n\n\n[1] 8635   28\n\n\ntail(IBDchip[,1:3])__\n\n\n                                 20CF     20DF     20MF\nbm-026.1.sig_st              7.299308 7.275802 7.383103\nbm-125.1.sig_st              8.538857 8.998562 9.296096\nbru.tab.d.HIII.Con32.sig_st  6.802736 6.777566 6.859950\nbru.tab.d.HIII.Con323.sig_st 6.463604 6.501139 6.611851\nbru.tab.d.HIII.Con5.sig_st   5.739235 5.666060 5.831079\nday                          2.000000 2.000000 2.000000\n\n\ntable(IBDchip[nrow(IBDchip), ])__\n\n\n 1  2  3 \n 8 16  4 \n데이터는 28개 샘플에서 측정된 8634개 분류군(taxa)의 정규화된 풍부도 측정값입니다. 우리는 순위-임계값(rank-threshold) 변환을 사용하여, 가장 풍부한 상위 3000개 분류군에는 3000에서 1까지의 점수를 부여하고 나머지(낮은 풍부도) 분류군은 모두 1의 점수를 갖게 합니다. 또한 요인(factor)으로 간주되어야 할 (어색하게 배치된) day 변수를 적절한 분석 데이터와 분리합니다5:\n5 아래에서 우리는 이러한 데이터를 Bioconductor의 SummarizedExperiment로 정리하는 방법을 보여줄 것인데, 이는 이러한 데이터를 저장하는 훨씬 더 합리적인 방식입니다.\nassayIBD = IBDchip[-nrow(IBDchip), ]\nday      = factor(IBDchip[nrow(IBDchip), ])__\n연속형의 정규화된 데이터를 사용하는 대신, 값을 순위로 대체하는 강건한 분석을 사용합니다. 낮은 풍부도 값들은 실제 존재하는 것으로 생각되는 예상 분류군 수를 반영하도록 선택된 임계값으로 인코딩된 동점(ties)으로 간주됩니다:\nrankthreshPCA = function(x, threshold = 3000) {\n  ranksM = apply(x, 2, rank)\n  ranksM[ranksM &lt; threshold] = threshold\n  ranksM = threshold - ranksM\n  dudi.pca(t(ranksM), scannf = FALSE, nf = 2)\n}\npcaDay12 = rankthreshPCA(assayIBD[, day != 3])\nfviz_eig(pcaDay12, bar_width = 0.6) + ggtitle(\"\")__\n\n그림 9.13: 스크리 플롯은 샘플들이 2차원 임베딩으로 유용하게 표현될 수 있음을 보여줍니다.\nday12 = day[ day!=3 ]\nrtPCA1 = fviz(pcaDay12, element = \"ind\", axes = c(1, 2), geom = c(\"point\", \"text\"),\n  habillage = day12, repel = TRUE, palette = \"Dark2\",\n  addEllipses = TRUE, ellipse.type = \"convex\") + ggtitle(\"\") +\n  coord_fixed()\nrtPCA1 __\n\n그림 9.14: 우리는 색상을 사용하여 서로 다른 날짜를 식별하고 샘플 레이블도 유지했습니다. 또한 각 날짜에 대해 볼록 껍질(convex hulls)을 추가했습니다. 그룹 평균은 더 큰 기호(원, 삼각형 또는 사각형)를 가진 점으로 표시됩니다.\n__\n질문 9.6\n왜 순위에 임계값(threshold)을 사용하나요?\n__\n해결책\n__\n실제로 존재하지 않는 종들의 경우 노이즈 수준의 낮은 풍부도가 나타나는데, 이들이 전체의 절반 이상입니다. 이러한 관측치들에 대해 어떠한 유의미한 이유 없이 순위에서 큰 도약이 쉽게 일어날 수 있습니다. 따라서 우리는 낮은 풍부도에 대해 많은 수의 동점(ties)을 만듭니다.\n그림 9.14는 샘플들이 샘플링 날짜에 따라 자연스럽게 두 개의 서로 다른 그룹으로 나뉘는 것을 보여줍니다. 이 효과를 발견한 후, 우리는 이러한 뚜렷한 클러스터들을 설명할 수 있는 차이점들을 조사했습니다. 두 가지 서로 다른 프로토콜이 사용되었고(1일째는 프로토콜 1, 2일째는 프로토콜 2), 불행히도 그 이틀 동안 사용된 어레이의 출처도 달랐습니다(1일째는 어레이 1, 2일째는 어레이 2).\n혼동 효과를 풀어내기 위해 4개 샘플로 구성된 세 번째 데이터 세트를 수집해야 했습니다. 3일째에는 어레이 2가 프로토콜 2와 함께 사용되었습니다. 그림 9.15는 다음 코드로 생성된 모든 샘플을 포함하는 새로운 PCA 플롯을 보여줍니다:\npcaDay123 = rankthreshPCA(assayIBD)\nfviz(pcaDay123, element = \"ind\", axes = c(1, 2), geom = c(\"point\", \"text\"),\n  habillage = day, repel = TRUE, palette = \"Dark2\",\n  addEllipses = TRUE, ellipse.type = \"convex\") + \n  ggtitle(\"\") + coord_fixed()__\n[](09-chap_files/figure-html/fig-Threesetspca123-1.png “그림 9.15 (a):”)\n\n\n\n[](09-chap_files/figure-html/fig-Threesetspca123-2.png “그림 9.15 (b):”)\n\n\n\n그림 9.15: 3일간의 분석을 처음 이틀간의 분석과 비교해 보면, 두 번째 축 좌표의 부호가 반전된 것을 알 수 있습니다: 이는 생물학적 관련성이 없습니다. 중요한 발견은 그룹 3이 그룹 1과 크게 겹친다는 것인데, 이는 가변성을 만든 것이 2일째의 프로토콜 변경이었음을 나타냅니다.\n__\n질문 9.7\n어떤 상황에서 다음 코드를 사용하여 그룹 평균 주변에 신뢰 타원(confidence ellipses)을 만드는 것이 더 선호될까요?\nfviz_pca_ind(pcaDay123, habillage = day, labelsize = 3,\n  palette = \"Dark2\", addEllipses = TRUE, ellipse.level = 0.69)__\n\n그림 9.16: 3개 그룹의 경우 고윳값 스크리 플롯은 그림 9.13에 표시된 2개 그룹의 경우와 매우 유사합니다.\n이 시각화를 통해 우리는 원래의 실험 설계에 있었던 결함을 발견할 수 있었습니다. 녹색과 갈색으로 표시된 처음 두 배치는 모두 IBS 쥐와 건강한 쥐에 대해 균형이 잡혀 있었습니다. 하지만 그들은 매우 다른 수준의 가변성과 전반적인 다변량 좌표를 보여줍니다. 사실, 두 개의 혼동된 효과가 있습니다. 그 이틀 동안 어레이와 프로토콜이 모두 달랐습니다. 우리는 3일째에 세 번째 배치 실험을 실행해야 했으며(보라색으로 표시됨), 이는 1일째의 프로토콜과 2일째의 어레이를 사용했습니다. 세 번째 그룹은 배치 1과 충실하게 겹치며, 이는 프로토콜의 변경이 가변성의 원인이었음을 말해줍니다.\n\n\n11.2.3 9.3.2 배치 효과 제거하기\nassayIBD의 연속형 측정값과 보충(supplementary) 배치 번호를 요인으로 결합함으로써, PCA 맵은 귀중한 조사 도구를 제공했습니다. 이것은 보충 점(supplementary points)6을 사용한 좋은 예입니다. 평균-중심 점들은 세 그룹 각각에 있는 점들의 그룹 평균을 사용하여 만들어지며 플롯에서 추가 마커 역할을 합니다.\n6 새로운 관측 점이 행렬 분해에 사용되지 않기 때문에 이를 보충 점이라고 부릅니다.\n우리는 모든 배치가 원점에 중심을 맞추도록 그룹 평균을 뺌으로써 세 그룹을 재정렬하기로 결정할 수 있습니다. 약간 더 효과적인 방법은 sva 패키지에서 제공하는 ComBat 함수를 사용하는 것입니다. 이 함수는 유사하지만 약간 더 정교한 방법(경험적 베이즈 혼합 접근법, Leek et al. 2010)을 사용합니다. 우리는 강건한 PCA를 다시 수행하여 데이터에 미치는 효과를 확인할 수 있습니다 (그림 9.17의 결과 참조):\nmodel0 = model.matrix(~1, day)\ncombatIBD = ComBat(dat = assayIBD, batch = day, mod = model0)\npcaDayBatRM = rankthreshPCA(combatIBD)\nfviz(pcaDayBatRM, element = \"ind\", geom = c(\"point\", \"text\"),\n  habillage = day, repel=TRUE, palette = \"Dark2\", addEllipses = TRUE,\n  ellipse.type = \"convex\", axes =c(1,2)) + coord_fixed() + ggtitle(\"\")__\n\n그림 9.17: 배치 효과가 제거된 수정된 데이터는 이제 거의 원점에 중심을 두고 심하게 겹쳐진 세 개의 배치 그룹을 보여줍니다.\n\n\n11.2.4 9.3.3 하이브리드 데이터 및 Bioconductor 컨테이너\n배치와 처리 정보를 복합 객체의 구획으로 결합하는 더 합리적인 방법은 SummarizedExperiment 클래스를 사용하는 것입니다. 이는 행이 관심 있는 특성(예: 유전자, 전사체, 엑손 등)을 나타내고 열이 샘플을 나타내는 분석(assay)을 위한 특수 _슬롯(slots)_을 포함합니다. 특성에 대한 보충 정보는 rowData 함수를 사용하여 액세스할 수 있는 DataFrame 객체에 저장될 수 있습니다. _DataFrame_의 각 행은 SummarizedExperiment 객체의 상응하는 행에 있는 특성에 대한 정보를 제공합니다.\n\n\n\n여기서 혼동스러운 표기법상의 유사성이 발생하는데, SummarizedExperiment 프레임워크에서 DataFrame은 data.frame과 다릅니다.\n\n\n여기서 혼동스러운 표기법상의 유사성이 발생하는데, SummarizedExperiment 프레임워크에서 DataFrame은 data.frame 과 다릅니다.\n여기서 우리는 날짜와 처리라는 두 공변량을 colData 객체에 삽입하고, 이를 분석 데이터와 결합하여 새로운 SummarizedExperiment 객체를 만듭니다.\nlibrary(\"SummarizedExperiment\")\ntreatment  = factor(ifelse(grepl(\"Cntr|^C\", colnames(IBDchip)), \"CTL\", \"IBS\"))\nsampledata = DataFrame(day = day, treatment = treatment)\nchipse = SummarizedExperiment(assays  = list(abundance = assayIBD),\n                              colData = sampledata)__\nThis is the best way to keep all the relevant data together, it will also enable you to quickly filter the data while keeping all the information aligned properly.\n\n\n\nYou can explore composite objects using the Environment pane in RStudio. You will see that in chipse, some of the slots are empty.\n\n\nYou can explore composite objects using the Environment pane in RStudio. You will see that in chipse, some of the slots are empty.\n__\nQuestion 9.8\nMake a new SummarizedExperiment object by choosing the subset of the samples that were created on day 2.\n__\nSolution\n__\nchipse[, day == 2]__\n\n\nclass: SummarizedExperiment \ndim: 8634 16 \nmetadata(0):\nassays(1): abundance\nrownames(8634): 01010101000000.2104_gPM_GC 01010101000000.2141_gPM_GC\n  ... bru.tab.d.HIII.Con323.sig_st bru.tab.d.HIII.Con5.sig_st\nrowData names(0):\ncolnames(16): 20CF 20DF ... IBSM IBSP\ncolData names(2): day treatment\nColumns of the DataFrame represent different attributes of the features of interest, e.g., gene or transcript IDs, etc. Here is an example of hybrid data container from single cell experiments (see Bioconductor workflow in Perraudeau et al. (2017) for more details).\ncorese = readRDS(\"../data/normse.rds\")\nnorm = assays(corese)$normalizedValues __\nAfter the pre-processing and normalization steps prescribed in the workflow, we retain the 1000 most variable genes measured on 747 cells.\n__\nQuestion 9.9\nHow many different batches do the cells belong to ?\n__\nSolution\n__\nlength(unique(colData(corese)$Batch))__\n\n\n[1] 18\nWe can look at a PCA of the normalized values and check graphically that the batch effect has been removed:\nrespca = dudi.pca(t(norm), nf = 3, scannf = FALSE)\nplotscree(respca, 15)\nPCS = respca$li[, 1:3]__\n\nFigure 9.18: Screeplot of the PCA of the normalized data.\n\n\n\nWe have set up colors for the clusters as in the workflow, (the code is not shown here).\n\n\nWe have set up colors for the clusters as in the workflow, (the code is not shown here).\nSince the screeplot in Figure 9.18 shows us that we must not dissociate axes 2 and 3, we will make a three dimensional plot with the rgl package. We use the following interactive code:\nlibrary(\"rgl\")\nbatch = colData(corese)$Batch\nplot3d(PCS,aspect=sqrt(c(84,24,20)),col=col_clus[batch])\nplot3d(PCS,aspect=sqrt(c(84,24,20)),\ncol = col_clus[as.character(publishedClusters)])__\n[](imgs/plotnormpcabatch1.png “Figure 9.19 (a):”)\n\n\n\n: “)\n\n\n\nFigure 9.19: Two-dimensional screenshots of three-dimensional rgl plots. The points are colored according to batch numbers in (a), and according to the original clustering in (b). We can see that the batch effect has been effectively removed and that the cells show the original clustering.\nNote: Of course, the book medium is limiting here, as we are showing two static projections that do not do justice to the depth available when looking at the interactive dynamic plots as they appear using the plot3d function. We encourage the reader to experiment extensively with these and other interactive packages and they provide a much more intuitive experience of the data.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "09-chap.html#correspondence-analysis-for-contingency-tables",
    "href": "09-chap.html#correspondence-analysis-for-contingency-tables",
    "title": "11  9.1 이 장의 목표",
    "section": "11.3 9.4 Correspondence analysis for contingency tables",
    "text": "11.3 9.4 Correspondence analysis for contingency tables\n\n11.3.1 9.4.1 Cross-tabulation and contingency tables\nCategorical data abound in biological settings: sequence status (CpG/non-CpG), phenotypes, taxa are often coded as factors as we saw in Chapter 2. Cross- tabulation of two such variables gives us a contingency table ; the result of counting the co-occurrence of two phenotypes (sex and colorblindness was such an example). We saw that the first step is to look at the independence of the two categorical variables; the standard statistical measure of independence uses the chisquare distance. This quantity will replace the variance we used for continuous measurements.\nThe columns and rows of the table have the same `status’ and we are not in supervised/regression type setting. We won’t see a sample/variable divide; as a consequence the rows and columns will have the same status and we will ‘center’ both the rows and the columns. This symmetry will also translate in our use of biplots where both dimensions appear on the same plot.\nTable 9.1: Sample by mutation matrix.\n\n\n\nPatient\nMut1\nMut2\nMut3\n…\n\n\n\n\nAHX112\n0\n0\n0\n\n\n\nAHX717\n1\n0\n1\n\n\n\nAHX543\n1\n0\n0\n\n\n\n\n\n11.3.1.1 Transforming the data to tabular form.\nIf the data are collected as long lists with each subject (or sample) associated to its levels of the categorical variables, we may want to transform them into a contingency table. Here is an example. In Table 9.1 HIV mutations are tabulated as indicator (0/1) binary variables. These data are then transformed into a mutation co-occurrence matrix shown in Table 9.2.\nTable 9.2: Cross-tabulation of the HIV mutations showing two-way co- occurrences.\n\n\n\nPatient\nMut1\nMut2\nMut3\n…\n\n\n\n\nMut1\n853\n29\n10\n\n\n\nMut2\n29\n853\n52\n\n\n\nMut3\n10\n52\n853\n\n\n\n\n__\nQuestion 9.10\nWhat information is lost in this cross-tabulation ?\nWhen will this matter?\nHere are some co-occurrence data from the HIV database (Rhee et al. 2003). Some of these mutations have a tendency to co- occur.\n__\nQuestion 9.11\nTest the hypothesis of independence of the mutations.\nBefore explaining the details of how correspondence analysis works, let’s look at the output of one of many correspondence analysis functions. We use dudi.coa from the ade4 package to plot the mutations in a lower dimensional projection; the procedure follows what we did for PCA.\ncooc = read.delim2(\"../data/coccurHIV.txt\", header = TRUE, sep = \",\")\ncooc[1:4, 1:11]__\n\n\n    X4S X6D X6K X11R X20R X21I X35I X35L X35M X35T X39A\n4S    0  28   8    0   99    0   22    5   15    3   45\n6D   26   0   0   34  131    0  108    4   30   13   84\n6K    7   0   0    6   45    0    5   13   38   35   12\n11R   0  35   7    0  127   12   60   17   15    6   42\n\n\nHIVca = dudi.coa(cooc, nf = 4, scannf = FALSE)\nfviz_eig(HIVca, geom = \"bar\", bar_width = 0.6) + ggtitle(\"\")__\n\nFigure 9.20: The dependencies between HIV mutations is clearly a three dimensional phenomenon, the three first eigenvalues show a clear signal in the data.\n\nFigure 9.21: A screenshot of the output from an interactive 3d plotting function (plot3d).\nAfter looking at a screeplot, we see that dimensionality of the underlying variation is definitely three dimensional, we plot these three dimensions. Ideally this would be done with an interactive three-dimensional plotting function such as that provided through the package rgl as shown in Figure 9.21.\n__\nQuestion 9.12\nUsing the car and rgl packages make 3d scatterplot similar to Figure 9.21.\nCompare to the plot obtained using aspect=FALSE with the plot3d function from rgl.\nWhat structure do you notice by rotating the cloud of points?\n__\nSolution\n__\nlibrary(\"rgl\")\nCA1=HIVca$li[,1];CA2=HIVca$li[,2];CA3=HIVca$li[,3]\nplot3d(CA1,CA2,CA3,aspect=FALSE,col=\"purple\")__\n\n\nfviz_ca_row(HIVca,axes = c(1, 2),geom=\"text\", col.row=\"purple\",\n  labelsize=3)+ggtitle(\"\") + xlim(-0.55, 1.7) + ylim(-0.53,1.1) +\n  theme_bw() +  coord_fixed()\nfviz_ca_row(HIVca,axes = c(1, 3), geom=\"text\",col.row=\"purple\",\n    labelsize=3)+ggtitle(\"\")+ xlim(-0.55, 1.7)+ylim(-0.5,0.6) +\n    theme_bw() + coord_fixed()__\n\n\n\n\n\n\n\n\nFigure 9.22: Two planar maps of the mutations defined with the horizontal axis corresponding to the first eigenvector of the CA and the vertical axis being the second axis in (a), and the third in (b); notice the difference in heights.\n__\nQuestion 9.13\nShow the code for plotting the plane defined by axes 1 and 3 of the correspondence analysis respecting the scaling of the vertical axis as shown in the bottom figure of Figure 9.22.\n__\nSolution\n__\nfviz_ca_row(HIVca, axes=c(1, 3), geom=\"text\", col.row=\"purple\", labelsize=3) +\n  ggtitle(\"\") + theme_minimal() + coord_fixed()__\n이 첫 번째 예제는 PCA가 연속형 변수를 투영하는 것과 유사한 방식으로 하나의 범주형 변수(돌연변이)의 모든 서로 다른 수준을 매핑하는 방법을 보여주었습니다. 이제 이를 두 개 이상의 범주형 변수로 확장하는 방법을 살펴보겠습니다.\n\n\n\n11.3.2 9.4.2 머리카락 색깔, 눈 색깔 및 표현형 공동 발생\n분석을 자세히 따라갈 수 있도록 작은 표를 고려하겠습니다. 데이터는 표 9.3에 표시된 학생들의 머리카락 색깔과 눈 색깔 표현형 공동 발생에 대한 분할표입니다. 2장에서 우리는 가능한 의존성을 탐지하기 위해 \\(^2\\) 검정을 사용했습니다.\nHairColor = HairEyeColor[,,2]\nchisq.test(HairColor)__\n\n\n    Pearson's Chi-squared test\n\ndata:  HairColor\nX-squared = 106.66, df = 9, p-value &lt; 2.2e-16\n표 9.3: 학생들의 머리카락 색깔과 눈 색깔 분할표.\n\n\n\n갈색\n파란색\n헤이즐\n초록색\n\n\n\n\n\n검은색\n36\n9\n5\n2\n\n\n갈색\n66\n34\n29\n14\n\n\n빨간색\n16\n7\n7\n7\n\n\n금발\n4\n64\n5\n8\n\n\n\n그러나 머리카락 색깔과 눈 색깔 사이의 _비독립성_을 진술하는 것만으로는 충분하지 않습니다. 우리는 의존성이 어디에서 발생하는지에 대한 더 자세한 설명이 필요합니다: 어떤 머리카락 색깔이 초록색 눈과 더 자주 발생하나요? 일부 변수 수준은 독립적인가요? 사실 우리는 SVD의 특수한 가중치 버전을 사용하여 독립성으로부터의 이탈을 연구할 수 있습니다. 이 방법은 PCA와 MDS를 분할표로 확장한 단순한 것으로 이해될 수 있습니다.\n\n11.3.2.1 독립성: 계산 및 시각화.\n우리는 행과 열의 합을 계산하는 것으로 시작합니다. 우리는 이를 사용하여 두 표현형이 독립적일 경우 예상되는 표를 만듭니다. 이 예상 표를 HCexp라고 부릅니다.\nrowsums = as.matrix(apply(HairColor, 1, sum))\nrowsums __\n\n\n      [,1]\nBlack   52\nBrown  143\nRed     37\nBlond   81\n\n\ncolsums = as.matrix(apply(HairColor, 2, sum))\nt(colsums)__\n\n\n     Brown Blue Hazel Green\n[1,]   122  114    46    31\n\n\nHCexp = rowsums %*%t (colsums) / sum(colsums)__\n이제 표의 각 셀에 대한 척도 조정된 잔차의 합인 \\(^2\\)(카이제곱) 통계량을 계산합니다.\nsum((HairColor  - HCexp)^2/HCexp)__\n\n\n[1] 106.6637\n우리는 이 잔차들을 예상 표로부터 연구할 수 있으며, 먼저 수치적으로 확인한 다음 그림 9.23에서 살펴보겠습니다.\nround(t(HairColor-HCexp))__\n\n\n       Hair\nEye     Black Brown Red Blond\n  Brown    16    10   2   -28\n  Blue    -10   -18  -6    34\n  Hazel    -3     8   2    -7\n  Green    -3     0   3     0\n\n\nlibrary(\"vcd\")\nmosaicplot(HairColor, shade=TRUE, las=1, type=\"pearson\", cex.axis=0.7, main=\"\")__\n\n그림 9.23: 독립성으로부터의 이탈 시각화. 이제 상자들의 크기는 실제 관측된 카운트에 비례하며, 더 이상 ‘직사각형’ 속성을 갖지 않습니다. 독립성으로부터의 이탈은 각 상자에 대해 카이제곱 거리로 측정되며, 잔차가 크고 양(+)수인지 여부에 따라 색상이 입혀집니다. 짙은 파란색은 양의 연관성을 나타내며(예: 파란색 눈과 금발), 빨간색은 음의 연관성을 나타냅니다(예: 금발과 갈색 눈).\n\n\n11.3.2.2 수학적 공식화.\n우리가 방금 R에서 수행한 계산들을 좀 더 수학적인 형식으로 정리하면 다음과 같습니다. \\(I\\)개의 행과 \\(J\\)개의 열을 가지고 전체 표본 크기가 \\(n={i=1}^I {j=1}^J n_{ij}= n_{}\\)인 일반적인 분할표 \\({N}\\)에 대해, 만약 두 범주형 변수가 독립적이라면 각 셀의 빈도는 대략 다음과 같을 것입니다.\n\\[ n_{ij} = n \\]\n이는 다음과 같이 쓰여질 수도 있습니다:\n\\[ {N} = {c r’} n, c= {{N}} {}_m ;; r’= {N}’ {}_p \\]\n독립성으로부터의 이탈은 \\(^2\\) 통계량으로 측정됩니다.\n\\[ {X}^2=_{i,j} {n} \\]\n두 변수가 독립적이지 않다는 것을 확인하고 나면, 연관성을 시각화하기 위해 \\(^2\\) 거리를 사용하는 가중 다차원 척도법을 사용합니다.\n대응 분석 함수들 vegan의 CCA, FactoMineR의 CA, phyloseq의 ordinate, ade4의 dudi.coa.\n이 방법은 대응 분석(Correspondence Analysis, CA) 또는 쌍대 척도법(Dual Scaling)이라고 불리며, 이를 구현하는 여러 R 패키지가 있습니다.\n여기서는 머리카락 색깔과 눈 색깔에 대한 단순한 바이플롯을 만듭니다.\nHC = as.data.frame.matrix(HairColor)\ncoaHC = dudi.coa(HC,scannf=FALSE,nf=2)\nround(coaHC$eig[1:3]/sum(coaHC$eig)*100)__\n\n\n[1] 89 10  2\n\n\nfviz_ca_biplot(coaHC, repel=TRUE, col.col=\"brown\", col.row=\"purple\") +\n  ggtitle(\"\") + ylim(c(-0.5,0.5))__\n\n그림 9.24: CA 플롯은 데이터와 독립성 가정하의 예상 값 사이의 카이제곱 거리의 상당 부분을 나타냅니다. 첫 번째 축은 검은 머리와 금발 학생 사이의 대비를 보여주며, 이는 갈색 눈과 파란 눈의 대립에 의해 반영됩니다. CA에서 두 범주는 대칭적인 역할을 하며, 우리는 파란 눈과 금발의 근접성을 이들 범주의 강력한 공동 발생을 의미하는 것으로 해석할 수 있습니다.\n__\n질문 9.14\n카이제곱 통계량의 몇 퍼센트가 대응 분석의 처음 두 축에 의해 설명되나요?\n__\n질문 9.15\n적절한 scaling 매개변수 값을 사용하여 vegan 패키지의 CCA로 얻은 결과와 비교해 보세요.\n__\n해결책\n__\nlibrary(\"vegan\")\nres.ca = vegan::cca(HairColor)\nplot(res.ca, scaling=3)__\n\n\n11.3.2.3 바이플롯 해석하기\nCA는 특별한 무게 중심(barycentric) 속성을 가집니다: 바이플롯 스케일링은 행 지점들이 각자의 가중치를 가진 채 열 수준들의 무게 중심에 놓이도록 선택됩니다. 예를 들어, 파란 눈(Blue eyes) 열 지점은 (검정, 갈색, 빨강, 금발)의 무게 중심에 위치하며 그 가중치는 (9, 34, 7, 64)에 비례합니다. 금발(Blond) 행 지점은 매우 무겁게 가중치가 부여되며, 이것이 그림 9.24에서 금발과 파란 눈이 매우 가깝게 나타나는 이유입니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "09-chap.html#시간-및-기타-중요한-그래디언트-찾기.",
    "href": "09-chap.html#시간-및-기타-중요한-그래디언트-찾기.",
    "title": "11  9.1 이 장의 목표",
    "section": "11.4 9.5 시간… 및 기타 중요한 그래디언트 찾기.",
    "text": "11.4 9.5 시간… 및 기타 중요한 그래디언트 찾기.\n지난 섹션에서 공부한 모든 방법들은 흔히 서열화(ordination) 방법으로 알려져 있습니다. 군집화가 우리에게 숨겨진 요인/범주형 변수를 탐지하고 해석할 수 있게 해준 것과 같은 방식으로, 서열화는 데이터에서 숨겨진 순서, 그래디언트(gradient) 또는 잠재 변수를 탐지하고 해석할 수 있게 해줍니다.\n\n생태학자들은 대응 분석과 주성분 분석에서 관측 점들에 의해 형성되는 아치 구조를 생태학적 그래디언트로 해석해 온 오랜 역사를 가지고 있습니다 (Prentice 1977). 먼저 대응 분석을 수행하는 매우 간단한 데이터 세트를 통해 이를 설명해 보겠습니다.\n서열화(seriation) 또는 연대 측정의 첫 번째 예시는 Kendall (1969)에 의한 고고학 유물 연구였습니다. 그는 도자기의 특징 유무를 사용하여 도자기의 연대를 측정했습니다. 이러한 소위 서열화 방법은 오늘날 예를 들어 단일 세포 데이터의 발달 궤적을 추적할 때도 여전히 유효합니다.\nload(\"../data/lakes.RData\")\nlakelike[1:3,1:8]__\n\n\n     plant1 plant2 plant3 plant4 plant5 plant6 plant7 plant8\nloc1      6      4      0      3      0      0      0      0\nloc2      4      5      5      3      4      2      0      0\nloc3      3      4      7      4      5      2      1      1\n\n\nreslake=dudi.coa(lakelike,scannf=FALSE,nf=2)\nround(reslake$eig[1:8]/sum(reslake$eig),2)__\n\n\n[1] 0.56 0.25 0.09 0.03 0.03 0.02 0.01 0.00\n우리는 행-위치 점들(그림 9.25 (a))과 위치 및 식물 종 모두의 바이플롯을 그림 9.25 (b)의 하단에 플롯합니다; 이 플롯은 다음과 같이 만들어졌습니다:\nfviz_ca_row(reslake,repel=TRUE)+ggtitle(\"\")+ylim(c(-0.55,1.7))\nfviz_ca_biplot(reslake,repel=TRUE)+ggtitle(\"\")+ylim(c(-0.55,1.7))__\n\n\n\n\n\n\n\n\n그림 9.25: 호수 근처의 위치들은 (a)에서 보듯이 아치를 따라 정렬되어 있습니다. 바이플롯 (b)에서는 파란색 점들에 가장 가까운 빨간색 삼각형들을 살펴봄으로써 어떤 위치에서 어떤 식물이 가장 빈번한지 알 수 있습니다.\n__\n질문 9.16\n나타난 그대로의 원시 행렬 lakes를 다시 살펴보면, 그 항목들에서 어떤 패턴이 보이나요?\n만약 식물들이 실제 분류군 이름에 따라 정렬되었다면 어떤 일이 일어났을까요?\n\n11.4.1 9.5.1 세포 발발의 역학\n이제 Moignard 등 (2015)에 의해 발표된 더 흥미로운 데이터 세트를 분석해 보겠습니다. 이 논문은 혈액 세포 발달의 역학을 설명합니다. 데이터는 배아기 7.0일(E7.0)과 8.25일 사이의 5개 모집단으로부터 얻은, 혈액 및 내피 세포 잠재력을 가진 3,934개 세포의 단일 세포 유전자 발현 측정값입니다.\n\n그림 9.26: 여기서 연구된 네 가지 세포 모집단은 세 가지 순차적 상태(PS, NP, HF)와 두 가지 가능한 최종 분기(4SG 및 4SFG\\(^{-}\\))를 대표합니다.\n세포들을 비교하기 위해 여러 가지 서로 다른 거리를 사용할 수 있음을 4장에서 상기해 보세요. 여기서는 3,934개 세포 사이의 \\(L_2\\) 거리와 \\(_1\\) 거리를 모두 계산하는 것부터 시작합니다.\nMoignard = readRDS(\"../data/Moignard.rds\")\ncellt = rowData(Moignard)$celltypes\ncolsn = c(\"red\", \"purple\", \"orange\", \"green\", \"blue\")\nblom = assay(Moignard)\ndist2n.euclid = dist(blom)\ndist1n.l1     = dist(blom, \"manhattan\")__\n이 두 거리 행렬에 대한 고전적 다차원 척도법은 다음과 같이 수행될 수 있습니다:\nce1Mds = cmdscale(dist1n.l1,     k = 20, eig = TRUE)\nce2Mds = cmdscale(dist2n.euclid, k = 20, eig = TRUE)\nperc1  = round(100*sum(ce1Mds$eig[1:2])/sum(ce1Mds$eig))\nperc2  = round(100*sum(ce2Mds$eig[1:2])/sum(ce2Mds$eig))__\n우리는 기저 차원을 살펴보고, 그림 9.27에서 두 개 차원이 분산의 상당 부분을 제공할 수 있음을 확인합니다.\nplotscree(ce1Mds, m = 4)\nplotscree(ce2Mds, m = 4)__\n\n\n\n\n\n\n\n\n그림 9.27: (_1) (a) 및 (L_2) (b) 거리에 대한 MDS의 스크리 플롯. 우리는 고윳값들이 매우 유사하며 둘 다 2차원 현상을 가리키고 있음을 알 수 있습니다.\n처음 2개 좌표는 세포 간에 \\(_1\\) 거리가 사용되었을 때 가변성의 78%를 차지하고, \\(L_2\\) 거리가 사용되었을 때는 57%를 차지합니다. 그림 9.28 (a)에서 세포 간의 \\(_1\\) 거리에 대한 MDS의 첫 번째 평면을 봅니다:\nc1mds = ce1Mds$points[, 1:2] |&gt;\n        `colnames&lt;-`(paste0(\"L1_PCo\", 1:2)) |&gt;\n        as_tibble()\nggplot(c1mds, aes(x = L1_PCo1, y = L1_PCo2, color = cellt)) +\n  geom_point(aes(color = cellt), alpha = 0.6) +\n  scale_colour_manual(values = colsn) + guides(color = \"none\")\nc2mds = ce2Mds$points[, 1:2] |&gt;\n        `colnames&lt;-`(paste0(\"L2_PCo\", 1:2)) |&gt;\n        as_tibble()\nggplot(c2mds, aes(x = L2_PCo1, y = L2_PCo2, color = cellt)) +\n  geom_point(aes(color = cellt), alpha = 0.6) +\n   scale_colour_manual(values = colsn) + guides(color = \"none\")__\n\n\n\n\n\n\n\n\n그림 9.28: 생성된 2차원 MDS 플롯에서 세포 유형(파란색: PS, 초록색: NP, 노란색: HF, 빨간색: 4SG, 보라색: 4SFG^{-})에 따라 색상이 입혀진 Moignard 세포 데이터. (a)는 (_1) 거리를, (b)는 (L_2) 거리를 사용한 경우입니다.\n그림 9.28 (b)는 같은 방식으로 만들어졌으며, \\(L_2\\) 거리에 대한 MDS를 사용하여 생성된 2차원 투영을 보여줍니다.\nFigure 9.28 shows that both distances (L1 and L2) give the same first plane for the MDS with very similar representations of the underlying gradient followed by the cells.\nWe can see from Figure 9.28 that the cells are not distributed uniformly in the lower dimensions we have been looking at, we see a definite organization of the points. All the cells of type 4SG represented in red form an elongated cluster who are much less mixed with the other cell types.\n\n\n11.4.2 9.5.2 Local, nonlinear methods\nMultidimensional scaling and non metric multidimensional scaling aims to represent all distances as precisely as possible and the large distances between far away points skew the representations. It can be beneficial when looking for gradients or low dimensional manifolds to restrict ourselves to approximations of points that are close together. This calls for methods that try to represent local (small) distances well and do not try to approximate distances between faraway points with too much accuracy.\nThere has been substantial progress in such methods in recent years. The use of kernels computed using the calculated interpoint distances allows us to decrease the importance of points that are far apart. A radial basis kernel is of the form\n\\[ 1-(-), ^2 \\]\nIt has the effect of heavily discounting large distances. This can be very useful as the precision of interpoint distances is often better at smaller ranges; several examples of such methods are covered in Exercise 9.6 at the end of this chapter.\n__\nQuestion 9.17\nWhy do we take the difference between the 1 and the exponential?\nWhat happens when the distance between \\(x\\) and \\(y\\) is very big?\n\n11.4.2.1 t-SNE.\nThis widely used method adds flexibility to the kernel defined above and allows the \\(^2\\) parameter to vary locally (there is a normalization step so that it averages to one). The t-SNE method starts out from the positions of the points in the high dimensional space and derives a probability distribution on the set of pairs of points, such that the probabilities are proportional to the points’ proximities or similarities. It then uses this distribution to construct a representation of the dataset in low dimensions. The method is not robust and has the property of separating clusters of points artificially; however, this property can also help clarify a complex situation. One can think of it as a method akin to graph (or network) layout algorithms. They stretch the data to clarify relations between the very close (in the network: connected) points, but the distances between more distal (in the network: unconnected) points cannot be interpreted as being on the same scales in different regions of the plot. In particular, these distances will depend on the local point densities. Here is an example of the output of t-SNE on the cell data:\nlibrary(\"Rtsne\")\nrestsne = Rtsne(blom, dims = 2, perplexity = 30, verbose = FALSE,\n                max_iter = 900)\ndftsne = restsne$Y[, 1:2] |&gt;\n         `colnames&lt;-`(paste0(\"axis\", 1:2)) |&gt;\n         as_tibble()\nggplot(dftsne,aes(x = axis1, y = axis2, color = cellt)) +\n  geom_point(aes(color = cellt), alpha = 0.6) +\n   scale_color_manual(values = colsn) + guides(color = \"none\")\nrestsne3 = Rtsne(blom, dims = 3, perplexity = 30, verbose = FALSE,\n                 max_iter = 900)\ndftsne3 = restsne3$Y[, 1:3] |&gt;\n          `colnames&lt;-`(paste0(\"axis\", 1:3)) |&gt; \n          as_tibble()\nggplot(dftsne3,aes(x = axis3, y = axis2, group = cellt)) +\n      geom_point(aes(color = cellt), alpha = 0.6) +\n      scale_colour_manual(values = colsn) + guides(color = \"none\")__\n\n\n\n\n\n\n\n\n그림 9.29: 여기서 연구된 네 가지 세포 모집단은 세 가지 순차적 상태(PS, NP, HF)와 두 가지 가능한 최종 분기(4SG 및 4SFG\\(^{-}\\))를 대표합니다. 왼쪽 플롯은 당혹감(perplexity)을 30으로 설정하고 t-SNE에 대해 2개 차원을 선택하여 얻은 것입니다. 하단 플롯은 3개 차원을 선택하여 얻은 것인데, 여기서 수평축으로 표시된 것이 세 번째 t-SNE 축임을 알 수 있습니다.\n이 경우 MDS와 t-SNE 사이의 미세한 차이를 확인하려면 3D 플로팅을 사용하는 것이 정말 필요합니다.\n__\n태스크\nrgl 패키지를 사용하여 3개의 t-SNE 차원을 살펴보고 디스플레이에 올바른 세포 유형 색상을 추가해 보세요.\n이러한 3D 스냅샷 중 두 개가 그림 9.30에 나와 있습니다. MDS 플롯에서보다 보라색 점들이 훨씬 더 강력하게 그룹화된 것을 볼 수 있습니다.\n참고: 복잡성(complexity) 및 \\(\\) 매개변수에 대한 t-SNE 방법의 민감도에 대해 더 자세히 알아보려면 방문할 가치가 있는 사이트가 http://distill.pub/2016/misread-tsne에 있습니다.\n[](imgs/tsnemoignard3scrop.png “그림 9.30 (a):”)\n\n\n\n[](imgs/tsnemoignard3crop.png “그림 9.30 (b):”)\n\n\n\n그림 9.30: 3차원 t-SNE 레이아웃에서 세포 유형(파란색: PS, 초록색: NP, 노란색: HF, 빨간색: 4SG, 보라색: 4SFG\\(^-\\))에 따라 색상이 입혀진 Moignard 세포 데이터. 보라색 세포(4SFG\\(^-\\))가 점 구름 상단의 외곽 껍질에 분리되어 있는 것을 볼 수 있습니다.\n__\n질문 9.18\n9.2절의 우크라이나 거리 데이터에 대해 2차원 t-SNE 임베딩을 시각화해 보세요.\n__\n해결책\n__\nukraine_tsne = Rtsne(ukraine_dists, is_distance = TRUE, perplexity = 8)\nukraine_tsne_df = tibble(\n  PCo1 = ukraine_tsne$Y[, 1],\n  PCo2 = ukraine_tsne$Y[, 2],\n  labs = attr(ukraine_dists, \"Labels\")\n)\nggplot(ukraine_tsne_df, aes(x = PCo1, y = PCo2, label = labs)) +\n  geom_point() + geom_text_repel(col = \"#0057b7\") + coord_fixed() __\n\n그림 9.31: 우크라이나의 t-SNE 맵.\n관련 상태 공간에서 점들이 따르는 비선형 궤적을 추정하기 위한 다른 여러 비선형 방법들이 있습니다. 여기 몇 가지 예시가 있습니다.\nRDRToolbox 국소 선형 임베딩(LLE) 및 isomap\ndiffusionMap 이 패키지는 점들 사이의 연결을 마르코프 커널(Markovian kernel)로 모델링합니다.\nkernlab 커널 방법들\nLPCM-package 국소 주성분 곡선(Local principal curves)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "09-chap.html#다중-표multitable-기법",
    "href": "09-chap.html#다중-표multitable-기법",
    "title": "11  9.1 이 장의 목표",
    "section": "11.5 9.6 다중 표(Multitable) 기법",
    "text": "11.5 9.6 다중 표(Multitable) 기법\n현재의 연구들은 종종 서로 다른 실험 조건에 따른 미생물, 게놈 및 대사 측정값의 변동을 정량화하려고 시도합니다. 그 결과, 동일한 생물학적 샘플에 대해 여러 어세이(assays)를 수행하고 미생물, 유전자 또는 대사산물 중 어떤 특성이 서로 다른 샘플 조건과 연관되어 있는지 묻는 것이 일반적입니다. 이러한 질문들에 접근하는 방법은 여러 가지가 있습니다. 어떤 것을 적용할지는 연구의 초점에 달려 있습니다.\n\n11.5.1 9.6.1 공변동, 관성, 공동 관성 및 RV 계수\n물리학에서와 마찬가지로, 우리는 관성(inertia)을 ‘가중치가 부여된’ 점들 사이의 거리 합으로 정의합니다. 이를 통해 우리는 분할표의 카운트에 대한 관성을 (카이제곱 통계량에서처럼) 관측 빈도와 예상 빈도 사이의 거리 제곱의 가중 합으로 계산할 수 있습니다.\n분산-관성의 또 다른 일반화는 유용한 계통 발생 다양성 지수(Phylogenetic diversity index)입니다. (트리를 통해 분류군 하위 집합 사이의 거리 합을 계산함). 다른 유용한 일반화에는 표준 공간 통계학에서 가져온 그래프 상의 점들의 가변성을 사용하는 것이 포함됩니다.\n만약 우리가 동일한 10개 위치에서 측정된 두 개의 표준화된 변수를 함께 연구하고 싶다면, 그들의 공분산(covariance)을 사용합니다. \\(x\\)가 표준화된 pH를 나타내고 \\(y\\)가 표준화된 습도를 나타낸다면, 우리는 다음과 같이 평균을 사용하여 그들의 공변동(covariation)을 측정합니다.\n\\[ (x,y) = (x1y1 + x2y2 + x3y3 + + x10y10). \\]\n만약 \\(x\\)와 \\(y\\)가 같은 방향으로 함께 변한다면, 이 수치는 커질 것입니다. 우리는 8장에서 정의한 상관 계수가 다변량 분석에 얼마나 유용했는지 보았습니다. 다중 표(multitable) 일반화도 그만큼 유용할 것입니다.\n\n\n11.5.2 9.6.2 만텔(Mantel) 계수 및 거리 상관관계 검정\n\n\n\n만텔 계수를 사용할 때는 몇 가지 주의사항이 있습니다. Guillot 및 Rousset (2013)의 비판적 리뷰를 참조하십시오.\n\n\n만텔 계수를 사용할 때는 몇 가지 주의사항이 있습니다. Guillot 및 Rousset (2013)의 비판적 리뷰를 참조하십시오.\n연관성 척도의 가장 초기 버전 중 하나인 만텔 계수는 아마도 현재 특히 생태학 분야에서 가장 인기 있는 방법이기도 할 것입니다 (Josse and Holmes 2016). \\(X\\) 및 \\(Y\\)와 관련된 두 비유사성 행렬 \\(D^X\\)와 \\(D^Y\\)가 주어졌을 때, R의 dist 함수가 하는 것처럼 이 행렬들을 벡터로 만들고 그들의 선형 상관관계를 계산합니다. 전형적인 응용 사례는 예를 들어 17개 서로 다른 위치의 토양 화학 성분으로부터 \\(D^X\\)를 계산하고, 동일한 17개 위치 사이에서 자카드 지수로 측정된 식물 발생의 비유사성을 나타내기 위해 \\(D^Y\\)를 사용하는 것입니다. 만텔 계수는 수학적으로 다음과 같이 정의됩니다:\n\\[ r_m(X, Y) = { }, \\]\n여기서 \\({d}^X\\) (각각 \\({d}^Y\\))는 \\(d^X\\) (각각 \\(d^Y\\))와 관련된 비유사성 행렬의 상삼각 원소들의 평균입니다. 만텔 계수와 RV 또는 dCov와 같은 다른 계수들의 주요 차이점은 이중 중앙화(double centering)가 없다는 것입니다. 거리 행렬 내부의 의존성 때문에, 만텔 상관관계의 귀무 분포와 통계적 유의성은 일반적인 상관 계수처럼 간단하게 평가될 수 없습니다. 대신, 대개 순열 검정(permutation testing)을 통해 평가됩니다. 역사적 배경과 현대적 구현에 대한 리뷰는 Josse와 Holmes (2016)를 참조하십시오. 이 계수와 관련 검정들은 ade4 (Chessel, Dufour, and Thioulouse 2004), vegan 및 ecodist (Goslee, Urban, et al. 2007)와 같은 여러 R 패키지에 구현되어 있습니다.\n\n\n11.5.3 9.6.3 RV 계수\n두 벡터가 아닌 두 데이터 표의 유사성에 대한 전역적인 측정은, 표들 사이의 내적(inner product)에 의해 제공되는 공분산의 일반화를 통해 수행될 수 있습니다. 이는 상관 계수와 비슷하지만 표를 위한 수치인 0과 1 사이의 숫자인 RV 계수를 제공합니다.\n\\[ RV(A,B)= \\]\nMatrixCorrelation 패키지에서 행렬 상관관계에 대한 다른 여러 척도들을 사용할 수 있습니다.\n만약 두 행렬 사이의 연결을 확인했다면, 우리는 그 연결을 이해하기 위한 방법을 찾아야 합니다. 그러한 방법 중 하나가 다음 섹션에서 설명됩니다.\n\n\n11.5.4 9.6.4 정전분석(Canonical correlation analysis, CCA)\nCCA는 1930년대에 Hotelling에 의해 두 개의 연속형 변수 세트 \\(X\\)와 \\(Y\\) 사이의 연관성을 찾기 위해 개발된 PCA와 유사한 방법입니다. 그 목표는 두 번째 변수 세트의 선형 투영과 최대의 상관관계를 갖는 첫 번째 변수 세트의 선형 투영을 찾는 것입니다.\n표현 특이적인 세부 사항(노이즈)을 버리고 동일한 현상에 대한 두 가지 관점의 상관된 함수(공변량)를 찾음으로써, 상관관계의 원인이 되는 기저의 숨겨져 있지만 영향력 있는 요인들을 밝혀낼 것으로 기대됩니다.\n두 개의 행렬을 고려해 봅시다:\n\n\\(np\\) 행렬 \\(X\\), 그리고\n\\(nq\\) 행렬 \\(Y\\).\n\n\\(X\\)의 \\(p\\)개 열과 \\(Y\\)의 \\(q\\)개 열은 변수들에 대응하며, 행들은 동일한 \\(n\\)개의 실험 단위에 대응합니다. 행렬 \\(X\\)의 \\(j\\)번째 열을 \\(X_j\\), 마찬가지로 행렬 \\(Y\\)의 \\(k\\)번째 열을 \\(Y_k\\)로 표시합니다. 일반성을 잃지 않고 \\(X\\)와 \\(Y\\)의 열들은 표준화(평균 0, 분산 1)되었다고 가정합니다.\n고전적 CCA는 \\(p n\\) 및 \\(q n\\)이고, 행렬 \\(X\\)와 \\(Y\\)가 각각 최대 열 계수(full column rank) \\(p\\)와 \\(q\\)를 갖는다고 가정합니다. 이하에서 CCA는 반복 알고리즘을 통해 해결되는 문제로 제시됩니다. CCA의 첫 번째 단계는 다음과 같이 정의된 선형 결합 \\(U\\)와 \\(V\\) 사이의 상관관계를 최대화하는 두 벡터 \\(a =(a_1,…,a_p)^t\\)와 \\(b =(b_1,…,b_q)^t\\)를 찾는 것으로 구성됩니다.\n\\[ ]\n이때 벡터 \\(a\\)와 \\(b\\)는 \\((U) = (V) = 1\\)이 되도록 정규화되었다고 가정합니다. 다시 말해, 문제는 다음을 만족하는 \\(a\\)와 \\(b\\)를 찾는 것입니다.\n\\[ 1 = (U, V) = {a,b} (Xa, Yb)(Xa)=(Yb) = 1. \\]\n결과로 나오는 변수 \\(U\\)와 \\(V\\)를 첫 번째 정전 변수(canonical variates)라고 부르며 \\(_1\\)을 첫 번째 정전 상관관계(canonical correlation)라고 부릅니다.\n참고: 고차원의 정전 변수와 정전 상관관계는 단계별 문제로 찾아낼 수 있습니다. \\(s = 1,…,p\\)에 대해, 우리는 다음을 최대화함으로써 상응하는 벡터 \\((a^1, b^1), …, (a^p, b^p)\\)와 함께 양의 상관관계 \\(_1 _2 … _p\\)를 순차적으로 찾을 수 있습니다.\n\\[ s = (Us,Vs) = {as,bs} (Xas,Ybs)(Xa^s) = (Yb^s)=1 \\]\n이때 다음과 같은 추가 제약 조건이 따릅니다.\n\\[ (Us,Ut) = (V^s, V^t)=0 t &lt; s p. \\]\n우리는 CCA를 우리가 최대화하는 분산이 두 행렬 사이의 ’공분산’인 PCA의 일반화로 생각할 수 있습니다 (더 자세한 내용은 Holmes (2006)를 참조하십시오).\n\n\n11.5.5 9.6.5 희소 정전분석(Sparse canonical correlation analysis, sCCA)\n각 표의 변수 수가 매우 많을 때 상관관계가 매우 높은 두 벡터를 찾는 것은 너무 쉬우며 불안정할 수 있습니다: 자유도가 너무 많기 때문입니다.\n\n\n\n우리는 12장에서 정규화의 많은 예시와 과적합(overfitting)의 위험을 보게 될 것입니다.\n\n\n우리는 12장에서 정규화의 많은 예시와 과적합(overfitting)의 위험을 보게 될 것입니다.\n그럴 때는 0이 아닌 계수의 수를 최소한으로 유지하는 페널티를 추가하는 것이 유익합니다. 이 접근 방식은 희소 정전분석(sparse CCA 또는 sCCA)이라고 불리며, 샘플 간의 탐색적 비교와 흥미로운 공변동(co-variation)을 가진 특성을 식별하는 데 모두 잘 어울리는 방법입니다. 우리는 PMA 패키지의 구현체를 사용할 것입니다.\n여기서는 Kashyap 등 (2013)에 의해 수집된 두 개의 표가 있는 데이터 세트를 연구합니다. 하나는 박테리아 풍부도의 분할표이고 다른 하나는 대사산물의 풍부도 표입니다. 12개의 샘플이 있으므로 \\(n = 12\\)입니다. 대사산물 표는 \\(p = 637\\)개 특성에 대한 측정값을 가지고 있고 박테리아 풍부도는 총 \\(q = 20,609\\)개의 OTU를 가졌는데, 이를 약 200개로 필터링할 것입니다. 먼저 데이터를 불러오는 것부터 시작합니다.\nlibrary(\"genefilter\")\nload(\"../data/microbe.rda\")\nmetab = read.csv(\"../data/metabolites.csv\", row.names = 1) |&gt; as.matrix()__\n우리는 먼저 관심 있는 박테리아와 대사산물로 필터링하여, 많은 샘플에서 0인 것들을 (수동으로) 제거하고 큰 값들에 대해서는 50이라는 상한 임계값을 부여합니다. 꼬리가 긴(heavy tails) 분포를 완화하기 위해 데이터를 변환합니다.\nlibrary(\"phyloseq\")\nmetab   = metab[rowSums(metab == 0) &lt;= 3, ]\nmicrobe = prune_taxa(taxa_sums(microbe) &gt; 4, microbe)\nmicrobe = filter_taxa(microbe, filterfun(kOverA(3, 2)), TRUE)\nmetab   = log(1 + metab, base = 10)\nX       = log(1 + as.matrix(otu_table(microbe)), base = 10)__\n예비 분석의 두 번째 단계는 ade4 패키지의 RV.test를 사용하여 두 행렬 사이에 어떤 연관성이 있는지 살펴보는 것입니다:\ncolnames(metab) = colnames(X)\npca1 = dudi.pca(t(metab), scal = TRUE, scann = FALSE)\npca2 = dudi.pca(t(X), scal = TRUE, scann = FALSE)\nrv1 = RV.rtest(pca1$tab, pca2$tab, 999)\nrv1 __\n\n\nMonte-Carlo test\nCall: RV.rtest(df1 = pca1$tab, df2 = pca2$tab, nrepet = 999)\n\nObservation: 0.8400429 \n\nBased on 999 replicates\nSimulated p-value: 0.002 \nAlternative hypothesis: greater \n\n    Std.Obs Expectation    Variance \n6.231661953 0.314166070 0.007121318 \n우리는 이제 희소 CCA를 적용할 수 있습니다. 이 방법은 샘플보다 측정된 특성이 더 많을 수 있는 고차원 데이터 표 전반에서 특성 세트들을 비교합니다. 이 과정에서 가장 많은 공분산을 포착하는 가용한 특성들의 하위 집합을 선택합니다 — 이들은 여러 표 전반에 존재하는 신호를 반영하는 특성들입니다. 그런 다음 선택된 특성들의 하위 집합에 PCA를 적용합니다. 이런 의미에서, 우리는 희소 CCA를 서열화 방법이라기보다는 스크리닝 절차로 사용합니다.\n구현은 아래와 같습니다. penaltyx와 penaltyz 매개변수는 희소성 페널티입니다. penaltyx 값이 작을수록 더 적은 수의 미생물이 선택되며, 마찬가지로 penaltyz는 선택되는 대사산물의 수를 조절합니다. 우리는 후속 해석을 용이하게 하기 위해 이를 수동으로 조정합니다 — 우리는 일반적으로 기본 매개변수가 제공하는 것보다 더 높은 희소성을 선호합니다.\nlibrary(\"PMA\")\nccaRes = CCA(t(X), t(metab), penaltyx = 0.15, penaltyz = 0.15, \n             typex = \"standard\", typez = \"standard\")__\n\n\n123456789\n\n\nccaRes __\n\n\nCall: CCA(x = t(X), z = t(metab), typex = \"standard\", typez = \"standard\", \n    penaltyx = 0.15, penaltyz = 0.15)\n\n\nNum non-zeros u's:  5 \nNum non-zeros v's:  16 \nType of x:  standard \nType of z:  standard \nPenalty for x: L1 bound is  0.15 \nPenalty for z: L1 bound is  0.15 \nCor(Xu,Zv):  0.9904707\n이 매개변수들을 사용하면, 표들 사이의 공변동을 설명하는 능력을 바탕으로 5개의 박테리아와 16개의 대사산물이 선택되었습니다. 또한, 이 특성들은 두 표 사이에서 0.99의 상관관계를 나타냅니다. 우리는 이를 미생물 데이터와 대사체 데이터가 유사한 기저 신호를 반영하며, 이러한 신호들이 선택된 특성들에 의해 잘 근사될 수 있음을 의미하는 것으로 해석합니다. 그러나 상관관계 값에 대해서는 주의해야 합니다. 점수들이 일반적인 이변량 정규 분포 구름과는 거리가 멀기 때문입니다. 또한, 다른 특성 하위 집합들도 데이터를 그만큼 잘 설명할 수 있다는 점에 유의하세요 — 희소 CCA는 특성들 간의 중복성을 최소화했지만, 이들이 어떤 의미에서든 “진정한” 특성임을 보장하지는 않습니다.\n그럼에도 불구하고, 우리는 여전히 이 21개 특성을 사용하여 큰 손실 없이 두 표의 정보를 압축할 수 있습니다. 회복된 대사산물과 OTU를 그것들이 측정된 샘플의 특성과 연관시키기 위해, 우리는 그것들을 일반적인 PCA의 입력으로 사용합니다. 그림 9.32를 생성하는 데 사용한 코드는 생략했습니다. 독자들께서는 책과 함께 제공되는 온라인 자료나 Callahan 등 (2016)에 발표된 워크플로를 참조하시기 바랍니다.\n그림 9.32는 PCA _트리플롯(triplot)_을 보여주는데, 여기에는 서로 다른 유형의 샘플들과 다중 도메인 특성(대사산물 및 OTU)이 표시됩니다. 이를 통해 측정된 샘플들 간의 비교 — 결손(knockout)은 삼각형으로, 야생형(wild type)은 원으로 표시 — 가 가능하며, 서로 다른 특성들의 영향 — 텍스트 레이블이 있는 다이아몬드 — 을 특징짓습니다. 예를 들어, 데이터의 주요 변동은 서로 다른 식단에 해당하는 PD 및 ST 샘플들 사이에서 나타나는 것을 볼 수 있습니다. 또한, 특성들 중 15개의 큰 값은 ST 상태와 연관되어 있고, 5개의 작은 값은 PD 상태를 나타냅니다.\n\n그림 9.32: 여러 데이터 유형(대사산물 및 OTU)으로부터 CCA로 선택된 특성들로 생성된 PCA 트리플롯.\n희소 CCA 스크리닝의 장점은 이제 분명합니다 — 우리는 상대적으로 단순한 플롯을 사용하여 샘플들 전반의 변동 대부분을 표시할 수 있으며, 모든 특성을 표시하는 데 필요했을 수백 개의 추가 점들을 플롯하는 것을 피할 수 있습니다.\n\n\n11.5.6 9.6.6 정전(또는 제약된) 대응 분석(CCpnA)\n\n\n\nNotational overload for CCA: Originally invented by Braak (1985) and called Canonical Correspondence analysis, we will call this method Constrained Correspondence Analysis and abbreviate it CCpnA to avoid confusion with Canonical Correlation Analysis (CCA). However several R packages, such as ade4 and vegan use the name cca for their correspondence analyses function.\n\n\nNotational overload for CCA : Originally invented by Braak (1985) and called Canonical Correspondence analysis, we will call this method Constrained Correspondence Analysis and abbreviate it CCpnA to avoid confusion with Canonical Correlation Analysis (CCA). However several R packages, such as ade4 and vegan use the name cca for their correspondence analyses function.\nThe term constrained correspondence analysis translates the fact that this 이 방법은 제약된 회귀(constrained regression)와 유사합니다. 이 방법은 잠재 변수들이 ’설명 변수’로 제공된 환경 변수들과 상관관계를 갖도록 강제합니다.\nCCpnA는 샘플의 위치가 종의 시그니처와 환경적 특성 모두의 유사성에 의해 결정되는 바이플롯을 생성합니다. 대조적으로, 주성분 분석이나 대응 분석은 오직 종의 시그니처만을 살펴봅니다. 더 형식적으로 말하자면, 결과로 나오는 CCpnA 방향들이 환경 변수들이 생성하는 공간(span) 내에 놓이도록 보장합니다. 철저한 설명은 Braak (1985; Greenacre 2007)을 참조하십시오.\n이 방법은 phyloseq 패키지의 ordinate 함수를 사용하여 실행할 수 있습니다. 샘플 데이터의 공변량을 사용하기 위해, 어떤 특성을 고려할지 지정하는 추가 인수를 제공합니다.\n여기서는 4장에서 dada2를 사용하여 노이즈를 제거했던 데이터를 사용합니다. phyloseq 객체를 생성하는 자세한 방법은 10장에서 다룰 것입니다. 당분간은 서로 다른 분류군에 대한 카운트의 분할표를 포함하는 otu_table 구성 요소를 사용합니다. 우리는 나이와 가족 관계(두 변수 모두 ps1 객체의 sample_data 슬롯에 포함됨)로 분류군 풍부도를 설명하는 제약된 대응 분석을 수행하고자 합니다.\n우리는 가장 풍부한 상위 4개 분류군만을 사용하여 (바이플롯을 읽기 쉽게 만들기 위해) 2차원 플롯을 만들고자 합니다:\nps1=readRDS(\"../data/ps1.rds\")\nps1p=filter_taxa(ps1, function(x) sum(x) &gt; 0, TRUE)\npsCCpnA = ordinate(ps1p, \"CCA\",\n                 formula = ps1p ~ ageBin + family_relationship)__\n바이플롯을 위한 위치 정보에 접근하기 위해 vegan 패키지의 scores 함수를 사용할 수 있습니다. 또한 그림 주석을 용이하게 하기 위해 사이트 점수(site scores)를 sample_data 슬롯에 있는 환경 데이터와 결합합니다. 총 23개의 목(orders) 중에서 가장 풍부한 4개만 명시적으로 주석을 답니다 — 이는 바이플롯을 읽기 쉽게 만듭니다.\nevalProp = 100 * psCCpnA$CCA$eig[1:2] / sum(psCCpnA$CA$eig)\nggplot() +\n geom_point(data = sites,aes(x =CCA2, y =CCA1),shape =2,alpha=0.5) +\n geom_point(data = species,aes(x =CCA2,y =CCA1,col = Order),size=1)+\n geom_text_repel(data = dplyr::filter(species, CCA2 &lt; (-2)),\n                   aes(x = CCA2, y = CCA1, label = otu_id),\n                   size = 2, segment.size = 0.1) +\n facet_grid(. ~ ageBin) +\n guides(col = guide_legend(override.aes = list(size = 2))) +\n labs(x = sprintf(\"Axis2 [%s%% variance]\", round(evalProp[2])),\n      y = sprintf(\"Axis1 [%s%% variance]\", round(evalProp[1]))) +\n scale_color_brewer(palette = \"Set1\") + theme(legend.position=\"bottom\")__\n\n그림 9.33: CCpnA에 의해 생성된 생쥐 및 분류군 점수. 사이트(생쥐 샘플)는 삼각형이고, 종은 원으로 각각 표시됩니다. 별도의 패널은 서로 다른 연령 그룹을 나타냅니다.\n__\n질문 9.19\n이 책의 온라인 리소스에서 tax 및 species 객체를 생성하는 추가 코드를 찾아보세요. 그런 다음 나이 대신 집단(litter)을 패싯 변수로 사용하여 그림 9.33의 대응물을 만들어 보세요.\n__\n해결책\n__\n\n그림 9.34: 나이가 아닌 집단 멤버십에 따라 패싯을 나눈, 그림 9.33의 대응물.\n그림 9.33과 9.34는 각각 나이와 집단 멤버십에 따라 사이트를 나누어 주석을 단 점수들의 플롯을 보여줍니다. 패싯이 있는 상황에서 적절한 종횡비를 유지하기 위해 수직축을 첫 번째 정전 성분(canonical component)으로 잡았습니다. 두 번째 CCpnA 방향을 따라 이상치인 개별 박테리아들에 레이블을 달았습니다.\n분명히 첫 번째 CCpnA 방향은 두 주요 연령대의 생쥐를 구분합니다. 바이플롯의 왼쪽과 오른쪽에 있는 원들은 각각 어린 생쥐와 나이 든 생쥐의 특징인 박테리아를 나타냅니다. 두 번째 CCpnA 방향은 가장 나이 많은 그룹의 소수 생쥐들을 분리하며, 또한 두 집단을 부분적으로 구분합니다. 두 번째 CCpnA 방향에서 낮은 값을 가진 이러한 샘플들은 다른 샘플들보다 이상치 박테리아를 더 많이 가지고 있습니다.\n이 CCpnA 분석은 서로 다른 생쥐 마이크로바이옴 커뮤니티 간의 주요 차이가 나이 축을 따라 존재한다는 결론을 뒷받침합니다. 그러나 환경 변수의 영향이 그렇게 강하지 않은 상황에서는 CCA가 그러한 연관성을 탐지하는 데 더 큰 검정력을 가질 수 있습니다. 일반적으로 보충 데이터를 통합하는 것이 바람직할 때마다 이를 적용할 수 있지만, (1) 지도 방식보다 덜 공격적이고, (2) 여러 환경 변수를 한꺼번에 사용할 수 있는 방식으로 적용됩니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "09-chap.html#이-장의-요약",
    "href": "09-chap.html#이-장의-요약",
    "title": "11  9.1 이 장의 목표",
    "section": "11.6 9.7 이 장의 요약",
    "text": "11.6 9.7 이 장의 요약\n불균질 데이터 많은 연속형 변수와 소수의 범주형 변수가 섞여 있는 경우, 범주형 변수를 PCA에 보조 정보로 추가하여 처리할 수 있습니다. 이는 그룹 내 모든 점의 평균을 맵에 투영함으로써 수행됩니다.\n거리 사용 데이터 객체 간의 관계는 종종 점 간 거리(트리, 이미지, 그래프 또는 기타 복잡한 객체 사이의 거리)로 요약될 수 있습니다.\n서열화(Ordination) 이러한 거리에 대한 유용한 표현은 PCA와 유사한 다차원 척도법(MDS), 즉 PCoA(주좌표 분석)라 불리는 방법을 통해 가능합니다. 이러한 분석의 결과를 잠재 변수를 찾아내는 것으로 생각하면 도움이 될 수 있습니다. 군집화의 경우 잠재 변수가 범주형인 반면, 서열화에서는 시간이나 물까지의 거리와 같은 환경적 그래디언트와 같은 잠재 변수입니다. 이것이 이러한 방법들을 흔히 서열화라고 부르는 이유입니다.\n강건한 버전들은 점 간 거리가 매우 다를 때 사용될 수 있습니다. NMDS(비계량 다차원 척도법)는 점 간 거리의 순서가 가능한 한 가깝게 유지되도록 하는 좌표를 생성하는 것을 목표로 합니다.\n대응 분석 : 범주형 데이터의 의존성을 설명하는 낮은 차원의 투영을 계산하는 방법입니다. PCA가 분산을 분해하는 것과 거의 같은 방식으로 카이제곱 거리를 분해합니다. 대응 분석은 대개 유의미한 카이제곱 검정에 대한 후속 조치를 취하는 가장 좋은 방법입니다. 카테고리의 서로 다른 수준들 사이에 유의미한 의존성이 있음을 확인했다면, 이를 매핑하고 플롯과 바이플롯을 사용하여 이 맵 상의 근접성을 해석할 수 있습니다.\n거리에 대한 순열 검정 동일한 점들 사이의 두 가지 거리 세트가 주어졌을 때, 만텔(Mantel) 순열 검정을 사용하여 그들이 서로 관련이 있는지 측정할 수 있습니다.\n분산과 공분산의 일반화 동일한 데이터에 대해 두 개 이상의 측정 행렬을 다룰 때, 공분산과 상관관계의 개념을 공동 관성(co-inertia)의 벡터 측정으로 일반화할 수 있습니다.\n정전 상관(Canonical correlation)은 각 표에서 가능한 한 상관관계가 높은 소수의 선형 변수 결합을 찾는 방법입니다. 변수 수가 많은 행렬에 이 방법을 사용할 때는 0이 아닌 계수의 수를 줄이는 L1 페널티를 사용하는 정규화된 버전을 사용합니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "09-chap.html#더-읽을거리",
    "href": "09-chap.html#더-읽을거리",
    "title": "11  9.1 이 장의 목표",
    "section": "11.7 9.8 더 읽을거리",
    "text": "11.7 9.8 더 읽을거리\nPCoA 맵과 비선형 임베딩의 해석은 보충 점 방법의 일반화를 사용하여 PCA에서 했던 것과 같은 방식으로 강화될 수 있습니다. Trosset과 Priebe (2008) 또는 Bengio 등 (2004)을 참조하십시오. 우리는 7장에서 하나의 범주형 변수를 PCA에 투영하는 방법을 보았습니다. 대응 분석 프레임워크는 사실 여러 범주형 변수를 임의의 수의 연속형 변수와 혼합할 수 있게 해줍니다. 이는 다중 대응 분석(MCA)이라는 확장을 통해 수행되는데, 이를 통해 수많은 이진 범주형 변수에 대해 동일한 분석을 수행하고 유용한 맵을 얻을 수 있습니다. 여기서의 비결은 연속형 변수를 먼저 범주형 변수로 바꾸는 것입니다. R을 사용한 광범위한 예제는 예를 들어 Pagès (2016)의 책을 참조하십시오.\n고유벡터에 의해 정의된 주방향 대신 비선형 주성분 곡선 추정을 가능하게 하는 PCA의 단순한 확장이 Hastie와 Stuetzle (1989)에 의해 제안되었으며, princurve 패키지에서 사용할 수 있습니다.\n1보다 높은 차원에서 고밀도 데이터를 포함하는 곡면 하위 공간을 찾는 것은 현재 매니폴드 임베딩(manifold embedding)이라고 불리며, 라플라시안 고유맵(Laplacian eigenmaps, Belkin and Niyogi 2003), Roweis와 Saul (2000)의 국소 선형 임베딩, 또는 isomap 방법(Tenenbaum, De Silva, and Langford 2000)을 통해 수행될 수 있습니다. 비선형 비지도 학습 방법을 다루는 교과서로는 Hastie, Tibshirani, Friedman (2008, 14장) 또는 Izenman (2008)을 참조하십시오.\n많은 다중 표 상관 계수에 대한 리뷰와 응용 분석은 Josse와 Holmes (2016)에서 찾아볼 수 있습니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "09-chap.html#연습-문제",
    "href": "09-chap.html#연습-문제",
    "title": "11  9.1 이 장의 목표",
    "section": "11.8 9.9 연습 문제",
    "text": "11.8 9.9 연습 문제\n__\n연습 문제 9.1\nPhylochip 데이터를 다시 한 번 살펴보겠습니다. 원래의 발현 값을 존재/부재(presence/absence)로 대체합니다. 최소 8개 샘플에서 적어도 8.633의 값을 가진 것들만 남기도록 데이터를 임계값 처리합니다7.\nibd.pres = ifelse(assayIBD[, 1:28] &gt; 8.633, 1, 0)__\n이 이진 데이터에 대해 대응 분석을 수행하고 그림 9.15에서 보았던 것과 얻은 플롯을 비교해 보세요.\n7 이 값들은 이전의 임계값 선택과 유사하게 약 3,000개의 분류군을 유지하도록 선택되었습니다.\n__\n해결책\n__\n그림 9.35를 참조하세요.\nIBDca = dudi.coa(ibd.pres, scannf = FALSE, nf = 4)\nfviz_eig(IBDca, geom = \"bar\", bar_width = 0.7) +\n    ylab(\"Percentage of chisquare\") + ggtitle(\"\")\nfviz(IBDca, element = \"col\", axes = c(1, 2), geom = \"point\",\n     habillage = day, palette = \"Dark2\", addEllipses = TRUE, color = day,\n     ellipse.type = \"convex\", alpha = 1, col.row.sup =  \"blue\",\n     select = list(name = NULL, cos2 = NULL, contrib = NULL),\n     repel = TRUE)__\n\n\n\\(\\)\n\n\n\n\\(\\)\n\n그림 9.35: 이진 데이터에 대한 대응 분석.\n__\n연습 문제 9.2\n색상 연관성 표에 대한 대응 분석:\n여기에 단어 쌍의 쿼리로 인한 Google 히트 수를 조사하여 수집된 데이터 예시가 있습니다. 표 9.4의 수치에는 1000을 곱해야 합니다. 예를 들어, “quiet”와 “blue” 단어의 조합은 2,150,000개의 히트를 반환했습니다.\n표 9.4: 검색 엔진 결과로부터 얻은 공동 발생 용어들의 분할표.\n\n\n\nblack\nblue\ngreen\ngrey\norange\npurple\nwhite\n\n\n\n\n\nquiet\n2770\n2150\n2140\n875\n1220\n821\n2510\n\n\nangry\n2970\n1530\n1740\n752\n1040\n710\n1730\n\n\nclever\n1650\n1270\n1320\n495\n693\n416\n1420\n\n\ndepressed\n1480\n957\n983\n147\n330\n102\n1270\n\n\nhappy\n19300\n8310\n8730\n1920\n4220\n2610\n9150\n\n\nlively\n1840\n1250\n1350\n659\n621\n488\n1480\n\n\nperplexed\n110\n71\n80\n19\n23\n15\n109\n\n\nvirtuous\n179\n80\n102\n20\n25\n17\n165\n\n\n\n이 데이터에 대해 대응 분석을 수행하세요. 2차원 바이플롯을 보았을 때 무엇을 알 수 있나요?\n__\n해결책\n__\n그림 9.36을 참조하세요. 코드는 여기에는 렌더링되지 않았지만, 문서의 소스 파일에 나와 있습니다.\n\n그림 9.36: 대응 분석은 표 9.4와 같은 공동 발생 분할표에 대해 색상과 감정이라는 두 범주형 변수의 대칭적인 그래픽 표현을 가능하게 합니다.\n\n__\n연습 문제 9.3\n플라톤이 그의 다양한 책들을 쓴 날짜는 알려져 있지 않습니다. 우리는 문장 끝맺음을 가져와 그 패턴 빈도를 데이터로 사용합니다.\nplatof = read.table(\"../data/platof.txt\", header = TRUE)\nplatof[1:4, ]__\n\n\n      Rep Laws Crit Phil Pol Soph Tim\nuuuuu  42   91    5   24  13   26  18\n-uuuu  60  144    3   27  19   33  30\nu-uuu  64   72    3   20  24   31  46\nuu-uu  72   98    2   25  20   24  14\n\n\nresPlato = dudi.coa(platof, scannf = FALSE, nf = 2)\nfviz_ca_biplot(resPlato, axes=c(2, 1)) + ggtitle(\"\")\nfviz_eig(resPlato, geom = \"bar\", width = 0.6) + ggtitle(\"\")__\nFigure 9.37: Biplot of Plato’s sentence endings.\n\nFrom the biplot in Figure 9.37 can you guess at the chronological order of Plato’s works?\nHint: the first (earliest) is known to be Republica. The last (latest) is known to be Laws.\nWhich sentence ending did Plato use more frequently early in his life?\nWhat percentage of the inertia (\\(^2\\)-distance) is explained by the map in Figure 9.37?\n\n\n\n\n\n\n\n\n\n__\nSolution\n__\nTo compute the percentage of inertia explained by the first two axes we take the cumulative sum of the eigenvalues at the value 2:\nnames(resPlato)__\n\n\n [1] \"tab\"  \"cw\"   \"lw\"   \"eig\"  \"rank\" \"nf\"   \"c1\"   \"li\"   \"co\"   \"l1\"  \n[11] \"call\" \"N\"   \n\n\nsum(resPlato$eig)__\n\n\n[1] 0.132618\n\n\npercentageInertia=round(100*cumsum(resPlato$eig)/sum(resPlato$eig))\npercentageInertia __\n\n\n[1]  69  85  92  96  98 100\n\n\npercentageInertia[2]__\n\n\n[1] 85\n__\n연습 문제 9.4\n우리는 두 개의 데이터 세트를 살펴볼 것인데, 하나는 다른 하나의 섭동된(perturbed) 버전이며 둘 다 생태학적 데이터에서 흔히 볼 수 있는 그래디언트를 보여줍니다. lakes.RData 객체로 저장된 두 개의 종 카운트 행렬 lakelike와 lakelikeh를 읽어 들이세요. 두 데이터 세트 각각에 대해 대응 분석과 주성분 분석의 출력을 비교해 보세요. 두 개 차원으로 제한하세요. 플롯과 고윳값에서 무엇을 발견했나요?\n__\n해결책\n__\nload(\"../data/lakes.RData\")\nlakelike[ 1:3, 1:8]__\n\n\n     plant1 plant2 plant3 plant4 plant5 plant6 plant7 plant8\nloc1      6      4      0      3      0      0      0      0\nloc2      4      5      5      3      4      2      0      0\nloc3      3      4      7      4      5      2      1      1\n\n\nlakelikeh[1:3, 1:8]__\n\n\n     plant1 plant2 plant3 plant4 plant5 plant6 plant7 plant8\nloc1      6      4      0      3      0      0      0      0\nloc2      4      5      5      3      4      2      0      0\nloc3      3      4      7      4      5      2      1      1\n\n\ne_coa  = dudi.coa(lakelike,  scannf = FALSE, nf = 2)\ne_pca  = dudi.pca(lakelike,  scannf = FALSE, nf = 2)\neh_coa = dudi.coa(lakelikeh, scannf = FALSE, nf = 2)\neh_pca = dudi.pca(lakelikeh, scannf = FALSE, nf = 2)__\n비교 (출력은 표시되지 않음):\nscatter(e_pca)__\n\n\n scatter(e_coa)__\n\n\n s.label(e_pca$li)__\n\n\n s.label(e_coa$li)__\n\n\n s.label(eh_pca$co)__\n\n\n s.label(eh_pca$li)__\n\n\n s.label(eh_coa$li)__\n\n\n s.label(eh_coa$co)__\n__\n연습 문제 9.5\n우리는 9.5.1절에서 정규화된 Moignard 데이터를 분석했습니다. 이제 원시(raw) 데이터(nbt.3154-S3-raw.csv 파일에 있음)로 분석을 다시 수행하고 그 출력을 정규화된 값을 사용했을 때 얻은 것과 비교해 보세요.\n__\n해결책\n__\nmoignard_raw = as.matrix(read.csv(\"../data/nbt.3154-S3-raw.csv\", row.names = 1))\ndist2r.euclid = dist(moignard_raw)\ndist1r.l1     = dist(moignard_raw, \"manhattan\")\ncells1.cmds = cmdscale(dist1r.l1,     k = 20, eig = TRUE)\ncells2.cmds = cmdscale(dist2r.euclid, k = 20, eig = TRUE)\nsum(cells1.cmds$eig[1:2]) / sum(cells1.cmds$eig)__\n\n\n[1] 0.776075\n\n\nsum(cells2.cmds$eig[1:2]) / sum(cells2.cmds$eig)__\n\n\n[1] 0.6297133\n__\n연습 문제 9.6\n우리는 커널(kernel) 방법의 사용을 탐구할 것입니다.\n\nkernlab을 사용하여 Moignard 데이터에 대해 커널 정의의 시그마(sigma) 튜닝 매개변수의 다양한 값을 사용하여 커널화된 거리를 계산하세요. 그런 다음 이 커널화된 거리에 대해 MDS를 수행하세요. 커널 다차원 척도법의 처음 네 성분에 의해 설명되는 가변성에는 어떤 차이가 있나요?\n성분들의 대화형 3차원 표현을 만드세요: 보라색 점들에 대한 분기(branch)가 보이는 투영이 있나요?\n\n__\n해결책\n__\n\n커널화된 거리\n\nlibrary(\"kernlab\")\nlaplacedot1 = laplacedot(sigma = 1/3934)\nrbfdot1     = rbfdot(sigma = (1/3934)^2 )\nKlaplace_cellsn   = kernelMatrix(laplacedot1, blom)\nKGauss_cellsn     = kernelMatrix(rbfdot1, blom)\nKlaplace_rawcells = kernelMatrix(laplacedot1, moignard_raw)\nKGauss_rawcells   = kernelMatrix(rbfdot1, moignard_raw)__\n이상치로부터 보호하고 비선형 성분의 발견을 가능하게 하기 위해 커널화된 거리를 사용합니다.\ndist1kr = 1 - Klaplace_rawcells\ndist2kr = 1 - KGauss_rawcells\ndist1kn = 1 - Klaplace_cellsn\ndist2kn = 1 - KGauss_cellsn\n\ncells1.kcmds = cmdscale(dist1kr, k = 20, eig = TRUE) \ncells2.kcmds = cmdscale(dist2kr, k = 20, eig = TRUE) \n\npercentage = function(x, n = 4) round(100 * sum(x[seq_len(n)]) / sum(x[x&gt;0]))\nkperc1 = percentage(cells1.kcmds$eig)\nkperc2 = percentage(cells2.kcmds$eig)\n\ncellsn1.kcmds = cmdscale(dist1kn, k = 20, eig = TRUE) \ncellsn2.kcmds = cmdscale(dist2kn, k = 20, eig = TRUE)__\n\n대화형 3D 산점도 사용:\n\ncolc = rowData(Moignard)$cellcol\nlibrary(\"scatterplot3d\")\nscatterplot3d(cellsn2.kcmds$points[, 1:3], color=colc, pch = 20,\n   xlab = \"Axis k1\", ylab = \"Axis k2\", zlab = \"Axis k3\", angle=15)\nscatterplot3d(cellsn2.kcmds$points[, 1:3], color=colc, pch = 20,\n   xlab = \"Axis k1\", ylab = \"Axis k2\", zlab = \"Axis k3\", angle = -70)__\n\n\n\\(\\)\n\n\n\n\\(\\)\n\n그림 9.38: 커널 다차원 척도법.\n__\n연습 문제 9.7\n세포 데이터의 고해상도 연구.\n9.5.1절에서 생성한 원래의 발현 데이터 blom을 가져오세요. 상위 10개의 가장 가변적인 유전자 각각의 발현 강도를 확산 매핑(diffusion mapping)으로 만든 3D 플롯 위에 매핑하세요. 어떤 차원, 또는 어떤 주좌표(1,2,3,4)가 4SG(빨간색) 점들을 가장 많이 클러스터링하는 것으로 보이나요?\n__\n해결책\n__\nlibrary(\"rgl\")\nplot3d(cellsn2.kcmds$points[, 1:3], col = colc, size = 3,\n       xlab = \"Axis1\", ylab = \"Axis2\", zlab = \"Axis3\")\nplot3d(cellsn2.kcmds$points[, c(1,2,4)], col = colc, size = 3,\n       xlab = \"Axis1\", ylab = \"Axis2\", zlab = \"Axis4\")\n# L1 거리를 대신 사용.\nplot3d(cellsn1.kcmds$points[, 1:3], col = colc, size = 3,\n       xlab = \"Axis1\", ylab = \"Axis2\", zlab = \"Axis3\")\nplot3d(cellsn1.kcmds$points[, c(1,2,4)], col = colc, size = 3,\n       xlab = \"Axis1\", ylab = \"Axis2\", zlab = \"Axis4\")__\nLPCM 패키지의 구현체는 주성분 곡선을 추정하는 lpc 함수를 제공합니다. 여기서는 확산 맵의 출력으로부터 선택된 3개 차원으로 제한하고 평활화된 곡선을 만듭니다.\nlibrary(\"LPCM\")\nlibrary(\"diffusionMap\")\ndmap1 = diffuse(dist1n.l1, neigen = 10)__\n\n\nPerforming eigendecomposition\nComputing Diffusion Coordinates\nElapsed time: 5.014 seconds\n\n\ncombs = combn(4, 3)\nlpcplots = apply(combs, 2, function(j) lpc(dmap1$X[, j], scale = FALSE))__\n평활화된 데이터가 우리에게 무엇을 보여주는지 감을 잡기 위해, rgl 패키지의 plot3d 함수를 사용하여 대화형 그래픽을 살펴봅니다.\nlibrary(\"rgl\")\nfor (i in seq_along(lpcplots))\n  plot(lpcplots[[i]], type = \"l\", lwd = 3,\n  xlab = paste(\"Axis\", combs[1, i]),\n  ylab = paste(\"Axis\", combs[2, i]),\n  zlab = paste(\"Axis\", combs[3, i]))__\n평활화된 선과 데이터 점들을 모두 플롯하는 한 가지 방법은 plot3d 함수를 사용하여 선을 추가하는 것입니다.\noutlpce134 = lpc(dmap1$X[,c(1,3,4)], scale=FALSE, h=0.5)\nplot3d(dmap1$X[,c(1,3,4)], col=colc, pch=20, \n       xlab=\"Axis1\", ylab=\"Axis3\", zlab=\"Axis4\")\nplot3d(outlpce134$LPC, type=\"l\", lwd=7, add=TRUE)\n\noutlpce134 = lpc(dmap1$X[,c(1,3,4)], scale=FALSE, h=0.7)\nplot3d(outlpce134$LPC, type=\"l\", lwd=7,\n       xlab=\"Axis1\", ylab=\"Axis3\", zlab=\"Axis4\")\nplot3d(dmap1$X[,c(1,3,4)], col=colc, \n       xlab=\"\", ylab=\"\", zlab=\"\", add=TRUE)__\n\n그림 9.39: 축 1, 3, 4에 대한 확산 맵 투영. 아래 그림은 세포들이 발달 과정에서 따르는 평활화된 경로를 보여줍니다.\n\n그림 9.40: 축 1, 3, 4에 대한 확산 맵 투영. 아래 그림은 세포들이 발달 과정에서 따르는 평활화된 경로를 보여줍니다.\n__\n연습 문제 9.8\n여기서는 그림 9.41에서처럼 세포 발달 궤적을 보여줄 수 있는 더 정교한 거리와 확산 맵을 탐구합니다.\n확산 맵 방법은 거리의 추정을 국소적인 점들로 제한하며, 이는 종종 국소적인 거리만이 정확하게 표현되어야 하고 점들이 멀어질수록 동일한 ’기준’으로 측정되지 않는다는 아이디어를 더 발전시킨 것입니다. 이 방법 또한 거리를 입력으로 사용하지만, 유사성의 지표로서 국소적인 확률적 전이(transitions)를 생성하고, 이들은 표준 MDS에서와 마찬가지로 고윳값과 고유벡터가 계산되는 인접 행렬(affinity matrix)로 결합됩니다.\n9.5.1절에서 사용 가능한 세포들 사이에서 계산된 dist2n.euclid 및 dist1n.l1 객체의 l1 및 l2 거리에 대해 diffusionMap 패키지의 diffuse 함수 출력을 비교해 보세요.\n\n그림 9.41: 3차원 확산 맵 투영 출력.\n__\n해결책\n__\nlibrary(\"diffusionMap\")\ndmap2 = diffuse(dist2n.euclid, neigen = 11)__\n\n\nPerforming eigendecomposition\nComputing Diffusion Coordinates\nElapsed time: 7.243 seconds\n\n\ndmap1 = diffuse(dist1n.l1, neigen = 11)__\n\n\nPerforming eigendecomposition\nComputing Diffusion Coordinates\nElapsed time: 4.849 seconds\n\n\nplot(dmap2)__\ndmap 객체에 대한 기본 플롯은 색상의 사용을 허용하지 않는다는 점에 유의하세요. 이는 세포 발달을 이해하는 데 필수적이므로, 수동으로 색상을 추가합니다. 물론 여기서는 정적인 3D 플롯을 사용하지만, 코드에서 제공하는 plot3d 예제들로 보완되어야 합니다.\n관련 매개변수들을 쉽게 삽입할 수 있도록 맞춤형 래퍼 함수 scp3d를 사용합니다:\nlibrary(\"scatterplot3d\")\nscp3d = function(axestop = 1:3, dmapRes = dmap1, color = colc,\n           anglea = 20, pch = 20)\nscatterplot3d(dmapRes$X[, axestop], color = colc,\n    xlab = paste(\"Axis\",axestop[1]), ylab = paste(\"Axis\", axestop[2]),\n    zlab = paste(\"Axis\",axestop[3]), pch = pch, angle = anglea)__\n\n\nscp3d()\nscp3d(anglea=310)\nscp3d(anglea=210)\nscp3d(anglea=150)__\n데이터를 시각화하는 가장 좋은 방법은 rgl 패키지를 사용하여 회전 가능한 대화형 플롯을 만드는 것입니다.\n# 대화형 플롯\nlibrary(\"rgl\")\nplot3d(dmap1$X[,1:3], col=colc, size=3)\nplot3d(dmap1$X[,2:4], col=colc, size=3)__\nBelkin, Mikhail, and Partha Niyogi. 2003. “Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.” Neural Computation 15 (6): 1373–96.\nBengio, Yoshua, Jean-François Paiement, Pascal Vincent, Olivier Delalleau, Nicolas Le Roux, and Marie Ouimet. 2004. “Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering.” Advances in Neural Information Processing Systems 16: 177–84.\nBraak, Cajo ter. 1985. “Correspondence Analysis of Incidence and Abundance Data: Properties in Terms of a Unimodal Respose.” Biometrics 41 (January).\nBrodie, Eoin L, Todd Z DeSantis, Dominique C Joyner, Seung M Baek, Joern T Larsen, Gary L Andersen, Terry C Hazen, et al. 2006. “Application of a High- Density Oligonucleotide Microarray Approach to Study Bacterial Population Dynamics During Uranium Reduction and Reoxidation.” Applied and Environmental Microbiology 72 (9): 6288–98.\nCallahan, Ben J, Kris Sankaran, Julia A Fukuyama, Paul J McMurdie, and Susan P Holmes. 2016. “Bioconductor Workflow for Microbiome Data Analysis: From Raw Reads to Community Analyses.” F1000Research 5.\nChessel, Daniel, Anne Dufour, and Jean Thioulouse. 2004. “The ade4 Package - i: One-Table Methods.” R News 4 (1): 5–10. http://CRAN.R-project.org/doc/Rnews/.\nDiaconis, Persi, Sharad Goel, and Susan Holmes. 2008. “Horseshoes in Multidimensional Scaling and Kernel Methods.” Annals of Applied Statistics 2: 777. https://doi.org/DOI:10.1214/08-AOAS165.\nEkman, Gosta. 1954. “Dimensions of Color Vision.” The Journal of Psychology 38 (2): 467–74.\nGoslee, Sarah C, Dean L Urban, et al. 2007. “The Ecodist Package for Dissimilarity-Based Analysis of Ecological Data.” Journal of Statistical Software 22 (7): 1–19.\nGreenacre, Michael J. 2007. Correspondence Analysis in Practice. Chapman & Hall.\nGuillot, Gilles, and François Rousset. 2013. “Dismantling the Mantel Tests.” Methods in Ecology and Evolution 4 (4): 336–44.\nHastie, Trevor, and Werner Stuetzle. 1989. “Principal Curves.” Journal of the American Statistical Association 84 (406): 502–16.\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2008. The Elements of Statistical Learning. 2^{} ed. Springer.\nHolmes, Susan. 2006. “Multivariate Analysis: The French way.” In Probability and Statistics: Essays in Honor of David a. Freedman , edited by D. Nolan and T. P. Speed. Vol. 56. IMS Lecture Notes–Monograph Series. Beachwood, OH: IMS. http://www.imstat.org/publications/lecnotes.htm.\nHolmes, Susan, Alexander V Alekseyenko, Alden Timme, Tyrrell Nelson, Pankaj Jay Pasricha, and Alfred Spormann. 2011. “Visualization and Statistical Comparisons of Microbial Communities Using r Packages on Phylochip Data.” In Pacific Symposium on Biocomputing , 142–53. World Scientific.\nIzenman, Alan Julian. 2008. “Nonlinear Dimensionality Reduction and Manifold Learning.” In Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning , 597–632. New York, NY: Springer New York.\nJosse, Julie, and Susan Holmes. 2016. “Measuring Multivariate Association and Beyond.” Statistics Surveys 10: 132–67.\nKashyap, Purna C, Angela Marcobal, Luke K Ursell, Samuel A Smits, Erica D Sonnenburg, Elizabeth K Costello, Steven K Higginbottom, et al. 2013. “Genetically Dictated Change in Host Mucus Carbohydrate Landscape Exerts a Diet-Dependent Effect on the Gut Microbiota.” PNAS 110 (42): 17059–64.\nKendall, David. 1969. “Incidence Matrices, Interval Graphs and Seriation in Archeology.” Pacific Journal of Mathematics 28 (3): 565–70.\nLeek, Jeffrey T, Robert B Scharpf, Héctor Corrada Bravo, David Simcha, Benjamin Langmead, W Evan Johnson, Donald Geman, Keith Baggerly, and Rafael A Irizarry. 2010. “Tackling the Widespread and Critical Impact of Batch Effects in High-Throughput Data.” Nature Reviews Genetics 11 (10): 733–39.\nMoignard, Victoria, Steven Woodhouse, Laleh Haghverdi, Andrew J Lilly, Yosuke Tanaka, Adam C Wilkinson, Florian Buettner, et al. 2015. “Decoding the Regulatory Network of Early Blood Development from Single-Cell Gene Expression Measurements.” Nature Biotechnology.\nNelson, Tyrell A, Susan Holmes, Alexander Alekseyenko, Masha Shenoy, Todd DeSantis, Cindy Wu, Gary Andersen, et al. 2010. “PhyloChip Microarray Analysis Reveals Altered Gastrointestinal Microbial Communities in a Rat Model of Colonic Hypersensitivity.” Neurogastroenterology & Motility.\nPagès, Jérôme. 2016. Multiple Factor Analysis by Example Using R. CRC Press.\nPerraudeau, Fanny, Davide Risso, Kelly Street, Elizabeth Purdom, and Sandrine Dudoit. 2017. “Bioconductor Workflow for Single-Cell RNA Sequencing: Normalization, Dimensionality Reduction, Clustering, and Lineage Inference.” F1000Research 6.\nPrentice, IC. 1977. “Non-Metric Ordination Methods in Ecology.” The Journal of Ecology , 85–94.\nRhee, Soo-Yon, Matthew J Gonzales, Rami Kantor, Bradley J Betts, Jaideep Ravela, and Robert W Shafer. 2003. “Human Immunodeficiency Virus Reverse Transcriptase and Protease Sequence Database.” Nucleic Acids Research 31 (1): 298–303.\nRoweis, Sam T, and Lawrence K Saul. 2000. “Nonlinear Dimensionality Reduction by Locally Linear Embedding.” Science 290 (5500): 2323–26.\nTenenbaum, Joshua B, Vin De Silva, and John C Langford. 2000. “A Global Geometric Framework for Nonlinear Dimensionality Reduction.” Science 290 (5500): 2319–23.\nTrosset, Michael W, and Carey E Priebe. 2008. “The Out-of-Sample Problem for Classical Multidimensional Scaling.” Computational Statistics & Data Analysis 52 (10): 4635–42.\nR 버전 4.5.1(2025-06-13)을 사용하여 2025-09-01 01:33에 페이지가 구축되었습니다.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>9.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "10-chap.html",
    "href": "10-chap.html",
    "title": "12  10.1 이 장의 목표",
    "section": "",
    "text": "12.1 10.2 그래프(Graphs)\n네트워크와 트리는 생물학적 시스템에 대한 지식을 표현하는 데 종종 사용됩니다. 또한 실험이나 연구로부터 얻은 관측치를 직접 인코딩하는 데에도 사용될 수 있습니다. 계통수(Phylogenetic trees)는 종들 사이의 가족 및 유사성 관계를 표현하기 위해 그려졌으며, 심지어 이러한 트리에 기계론적이고 인과적인 해석을 부여한 다윈의 유명한 노트북 스케치 이전부터 존재했습니다. 네트워크에서 노드(nodes)와 에지(edges)의 의미는 다를 수 있으며 명시되어야 합니다. 예를 들어, 네트워크는 그림 10.1에서처럼 단백질들 사이의 관계를 도식화할 수 있는데, 여기서 노드는 유전자나 그들이 인코딩하는 단백질을 나타낼 수 있고, 에지는 직접적인 물리적 상호작용이거나 실험 결과를 나타내는 좀 더 추상적인 “기능적” 또는 “유전적” 상호작용일 수 있습니다. 이 책에서 우리는 _그래프(graph)_와 _네트워크(network)_라는 용어를 거의 혼용하여 사용합니다. 전자는 수학적 구조를 좀 더 연상시키고, 후자는 생물학적 해석을 좀 더 연상시킵니다.\n그림 10.1: 단백질 사이의 쌍별 관계를 나타내는 작은 단백질-단백질 네트워크.\n리는 2장에서 상태 전이의 시퀀스를 마르코프 체인으로 모델링할 수 있음을 보았으며, 이는 에지에 가중치가 있는 유향 그래프(directed graphs)로 표현될 수 있습니다. 노드가 화학적 대사산물이고 에지가 화학 반응을 나타내는 대사 경로(Metabolic pathways)가 그 예입니다. 암 유전체학에서는 돌연변이의 계통을 나타내기 위해 돌연변이 이력 트리(Mutation history trees)가 사용됩니다.\n전염 네트워크(Transmission networks)는 감염병의 역학을 연구하는 데 중요합니다. 실제 네트워크는 매우 클 수 있으므로, 이를 표현하고 시각화하기 위한 특수한 방법들이 필요할 것입니다. 이 장은 데이터 분석 워크플로에 그래프를 통합하는 방법들에 초점을 맞출 것입니다.\n이 장에서 우리는 다음을 수행할 것입니다:",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "10-chap.html#그래프graphs",
    "href": "10-chap.html#그래프graphs",
    "title": "12  10.1 이 장의 목표",
    "section": "",
    "text": "12.1.1 10.2.1 그래프란 무엇이며 어떻게 인코딩할 수 있는가?\n그래프(graph)는 흔히 \\((V,E)\\)로 표시되는 두 세트의 조합으로 정의됩니다. 여기서 \\(V\\)는 노드(nodes) 또는 정점(vertices)의 세트이고, \\(E\\)는 정점들 사이의 에지(edges) 세트입니다. \\(E\\)의 각 원소는 노드 쌍으로, 즉 \\(V\\)의 두 원소로 구성됩니다. 그래프를 표현하는 직관적인 방법은 출발-도착(from-to) 표현입니다. 정점 세트를 \\(V=(, , , )\\)라고 하면, 출발-도착(또는 에지 리스트) 표현은 다음과 같은 형태의 표입니다.\n  from to\n1    A  B\n2    B  C\n3    A  E\n4    C  D\n5    E  F\n출발-도착 표에서 행의 순서는 아무런 역할을 하지 않습니다. 유향 그래프(directed graph) 또는 지향 그래프(oriented graph)에서 에지는 순서가 있는 쌍입니다. 즉, 위 표의 첫 번째 줄은 A에서 B로 가는 에지가 있음을 나타내지만, B에서 A로 가는 에지도 있는지는 말해주지 않습니다 — 이는 표의 별도 행에 표시되어야 합니다.\n무향 그래프(undirected graph)에서 에지는 순서가 없는 쌍입니다. 즉, A에서 B로 가는 에지는 B에서 A로 가는 에지와 구분되지 않습니다. 무향 그래프는 노드 사이의 대칭적인 관계를 인코딩하고, 유향 그래프는 비대칭적인 관계를 나타냅니다.\n그래프를 그 시각화와 혼동하지 않는 것이 중요합니다. 그림 10.1에서처럼 그래프를 2차원 영역에 그리는 것이 가능하지만, 이는 선택 사항이며 유일하지 않습니다 — 동일한 그래프를 그리는 방법은 항상 여러 가지가 있습니다. 또한 그러한 시각화에서 에지가 겹치지 않는다는 보장도 없습니다. 그래프에 따라 그런 일이 발생할 수 있습니다. 그래프는 물리적 공간(2D든 3D든)에 살고 있는 것이 아니라 말 그대로 노드와 에지의 세트일 뿐입니다.\n출발-도착 표와 동등한 대안적인 표현은 인접 행렬(adjacency matrix)입니다. 이는 그래프의 노드 수만큼의 행(및 열)을 가진 정사각 행렬입니다. 행렬의 \\(i\\)번째 행과 \\(j\\)번째 열에 0이 아닌 항목이 있으면 이는 \\(i\\)번째 정점과 \\(j\\)번째 정점 사이에 에지가 있음을 인코딩합니다.\n__\n질문 10.1\n무향 그래프의 경우, 인접 행렬 \\(A\\)의 어떤 점이 특별한가요?\n__\n해결책\n__\n인접 행렬은 대칭입니다. 즉, \\(M = M^T\\)입니다. 예시가 그림 10.2와 10.3에 나와 있습니다. g1은 아래 코드를 통해 출발-도착 표(2열 행렬 edges에 인코딩됨)로부터 생성되었습니다.\nlibrary(\"igraph\")\nedges = matrix(c(1,3, 2,3, 3,4, 4,5, 4,6), byrow = TRUE, ncol = 2)\ng1 = graph_from_edgelist(edges, directed = FALSE)\nvertex_attr(g1, name = \"name\") = 1:6\nplot(g1, vertex.size = 25, edge.width = 5, vertex.color = \"coral\")__\n\n그림 10.2: 번호가 매겨진 노드가 있는 작은 무향 그래프.\n\n그림 10.3: 그림 10.2에 표시된 그래프의 인접 행렬은 0과 1로 구성된 대칭 \\(n n\\) 행렬이며, 여기서 \\(n\\)은 노드의 수입니다.\n__\n질문 10.2\n위의 질문 10.1의 답변에 나온 방법 외에, 에지 리스트 데이터 프레임으로부터 그래프를 생성하는 대안적인 방법을 제시할 수 있나요?\n__\n해결책\n__\nedges = \"1,3\\n2,3\\n3,4\\n4,6\\n4,5\"\ndf = read.csv(textConnection(edges), header = FALSE)\nsg = graph_from_data_frame(df, directed = FALSE)\nsg __\n\n\nIGRAPH 1467322 UN-- 6 5 -- \n+ attr: name (v/c)\n+ edges from 1467322 (vertex names):\n[1] 1--3 2--3 3--4 4--6 4--5\n일반적으로 여러분은 별도의 파일에서 출발-도착 표를 읽기 위해 read.csv 함수를 사용할 것입니다. 여기서는 예제를 더 간결하게 유지하기 위해 책의 저자들이 즉석에서 문자열 edges를 만들고 textConnection 함수를 사용하여 이를 파일과 동등하게 만들기로 결정했습니다. 표기법 \"\\n\"은 줄바꿈을 나타냅니다.\n\n12.1.1.1 단순 그래프의 요소들\n\n노드 또는 정점. 이들은 그림 10.2에서 번호가 들어 있는 색칠된 원들입니다.\n에지 또는 연결, 노드들을 잇는 세그먼트이며 방향이 있을 수도 있고 없을 수도 있습니다.\n에지 속성(edge length 등). 별도로 지정되지 않으면 에지 길이는 모두 동일하게 대개 1이라고 가정합니다. 예를 들어, 그래프 상의 두 노드 사이의 거리를 계산하려면 최단 경로를 따라 에지들의 길이를 합산합니다.\n에지 및 노드 속성: 선택적으로, 각 에지나 각 노드는 유형, 색상, 가중치, 에지 너비, 노드 크기 등과 같은 추가적인 연속형 또는 범주형 변수들에 매핑될 수 있습니다. 응용 분야와 의도된 계산에 따라 거의 모든 것이 가능합니다.\n\n우리는 또한 에지 길이가 있는 유향 그래프를 네트워크(network)라고 부릅니다. 네트워크의 인접 행렬은 에지 길이에 대응하는 양수들로 이루어진 \\(nn\\) 행렬입니다.\n\n\n12.1.1.2 기본 개념들\n노드의 _차수(degree)_는 그 노드에 연결된 에지의 수입니다. 유향 그래프에서는 들어오는 에지와 나가는 에지에 대해 _진입 차수(in-degree)_와 _진출 차수(out-degree)_를 구분합니다. 또한 사이클(cycles)을 포함하는 유향 그래프와 그렇지 않은 그래프(순환 및 비순환 그래프라고 함)를 추가로 구분할 수 있습니다.\n큰 그래프의 경우, 정점 차수의 분포를 살펴봄으로써 전체적인 그래프 구조를 요약할 수 있으며, 중심성(centrality)이나 매개 중심성(betweenness)과 같은 척도를 사용하여 그래프에서 특히 흥미로운 영역이나 특정 노드 및 에지를 식별할 수 있습니다. 이러한 척도들은 network , igraph와 같은 다양한 패키지에서 사용할 수 있습니다.\n만약 에지의 수가 노드의 수와 같은 규모(order of magnitude)라면(\\(#EO(#V)\\)라고 씀), 우리는 그 그래프가 희소(sparse)하다고 말합니다. 일부 그래프는 노드가 매우 많습니다. 예를 들어, ppiData 패키지에는 약 20,000개의 에지를 가진 약 2,500개의 단백질에 대한 예측된 단백질 상호작용(ppipred) 그래프가 들어 있습니다1. 그러한 그래프에 대한 완전한 인접 행렬은 600만 개 이상의 메모리 단위를 필요로 하며, 그 중 대부분은 0을 포함합니다. 이는 불필요한 낭비입니다. 동일한 그래프의 에지 리스트 표현은 더 콤팩트합니다: 에지가 있는 곳에만 저장 공간을 사용하며, 우리 예제에서는 20,000개의 메모리 단위에 해당합니다. 에지 리스트 표현의 한 가지 구체적인 선택은 Matrix 패키지에 구현된 것과 같은 희소 행렬 인코딩입니다.\n1 유전자 및 종 계통수(phylogenies)는 이보다 훨씬 더 클 수도 있습니다.\n반면에 조밀한(dense) 그래프에서는 에지의 수가 잠재적인 에지의 수, 즉 노드 수의 제곱(\\(#EO(#V^2)\\)라고 씀)과 같은 규모입니다. 크고 조밀한 그래프의 경우 저장 메모리 공간이 문제가 될 수 있습니다.\n\n\n12.1.1.3 그래프 레이아웃(Graph layout)\n우리는 미적 또는 실무적인 이유로 동일한 그래프가 서로 다른 방식으로 그려지는 여러 예시를 보게 될 것입니다. 이는 그래프 레이아웃의 선택을 통해 이루어집니다.\n에지가 거리를 나타내는 길이를 가질 때, 그래프의 2D 표현 문제는 9장에서 보았던 다차원 척도법과 동일합니다. 이는 종종 정점 지점들을 가능한 한 멀리 퍼뜨림으로써 유사한 방식으로 해결됩니다. 길이가 없는 에지의 단순한 경우, 알고리즘은 서로 다른 기준을 선택할 수 있습니다. 프루히터만과 라잉골드(Fruchterman and Reingold) 방법이 기본적인 선택입니다. 이는 마치 (뉴턴의) 물리적 힘의 영향을 받는 것처럼 유사한 점들이 서로 끌어당기고 밀어내는 물리 기반 모델에 기초합니다.\n__\n태스크\nigraph 패키지를 사용하여 다음을 수행하세요.\n\n12개의 노드와 50개 이상의 에지를 가진 조밀한 무작위 그래프를 만듭니다.\n서로 다른 레이아웃으로 그래프를 그리는 실험을 해보세요: 노드들을 원형으로 배치하거나, 그래프를 가능한 한 대칭적으로 표현하고 노드나 에지가 겹치지 않게 해보세요.\n\n\n\n12.1.1.4 데이터로부터의 그래프\n대개 데이터는 그래프 형태로 도착하지 않습니다. 그래픽이나 네트워크 표현은 종종 다른 데이터 유형으로부터 변환된 결과입니다.\n거리나 유사성으로부터: 그래프는 거리나 유사성 관계를 이진화(binarising)함으로써 이를 단순화할 수 있습니다. 노드들이 비슷하거나 가까우면 연결하고, 그렇지 않으면 연결하지 않습니다. 따라서 입력값은 관심 있는 모든 객체 쌍(유전자, 단백질, 종, 표현형, \\(…\\)) 사이의 유사성 또는 거리 척도이며, 여기에 임계값(threshold)이 적용됩니다. 척도 세트는 조밀한 행렬로 구현되거나 즉석에서 계산될 수 있습니다.\n이분 그래프(Bipartite graphs): 일부 데이터는 갈라파고스 제도의 핀치새 종과 섬들(그림 10.4), 또는 전사 인자와 그들이 결합하는 것으로 간주되는 유전자 조절 영역 사이의 관계처럼 두 가지 유형의 객체 사이의 존재 또는 부재 관계로 자연스럽게 나타납니다. 그러한 관계는 직사각형 행렬에서 0/1 값으로 인코딩될 수 있으며, 여기서 행은 한 객체 유형을 나타내고 열은 다른 유형을 나타냅니다. 결과로 나오는 그래프는 두 가지 유형의 노드(예: 핀치새 노드와 섬 노드)를 가지며, 에지는 서로 다른 유형의 노드 사이에만 존재할 수 있습니다(예: 분류군과 섬 사이에는 존재하지만 분류군 사이나 섬 사이에는 존재하지 않음). 그림 10.4의 에지는 _~에 산다_는 관계를 나타냅니다.\n\n그림 10.4: 이 이분(bipartite) 그래프는 각 분류군을 그것이 관찰된 사이트들과 연결합니다.\n__\n질문 10.3\nfinch.csv 데이터를 불러오고, 이것이 이분 네트워크를 나타냄을 강조하도록 그리는 실험을 해보세요.\n__\n해결책\n__\n다음 코드의 출력은 그림 10.5에 나와 있습니다.\nfinch = readr::read_csv(\"../data/finch.csv\", comment = \"#\", col_types = \"cc\")\nfinch __\n\n\n# A tibble: 122 × 2\n   .tail .head             \n   &lt;chr&gt; &lt;chr&gt;             \n 1 C     Large ground finch\n 2 D     Large ground finch\n 3 E     Large ground finch\n 4 F     Large ground finch\n 5 G     Large ground finch\n 6 H     Large ground finch\n 7 I     Large ground finch\n 8 J     Large ground finch\n 9 L     Large ground finch\n10 M     Large ground finch\n# ℹ 112 more rows\n\n\nlibrary(\"network\")\nfinch.nw  = as.network(finch, bipartite = TRUE, directed = FALSE)\nis.island = nchar(network.vertex.names(finch.nw)) == 1\nplot(finch.nw, vertex.cex = 2.5, displaylabels = TRUE, \n     vertex.col = ifelse(is.island, \"forestgreen\", \"gold3\"),\n     label= sub(\" finch\", \"\", network.vertex.names(finch.nw)))\nfinch.nw |&gt; as.matrix() |&gt; t() |&gt; (\\(x) x[, order(colnames(x))])()__\n\n\n                          A B C D E F G H I J K L M N O P Q\nLarge ground finch        0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1\nMedium ground finch       1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0\nSmall ground finch        1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0\nSharp-beaked ground finch 0 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1\nCactus ground finch       1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 0\nLarge cactus ground finch 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\nLarge tree finch          0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0\nMedium tree finch         0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\nSmall tree finch          0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0\nVegetarian finch          0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0\nWoodpecker finch          0 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0\nMangrove finch            0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\nWarbler finch             1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\nFigure 10.5: The finches graph. There are many ways to improve the layout, including better taking into account the bipartite nature of the graph.\n__\nQuestion 10.4\nMake a plot of the graph g1 using the ggraph package, with a choice of layout and provided geoms such as geom_edge_link, geom_node_point and geom_node_text.\n__\nSolution\n__\nThe output of the following code is shown in Figure 10.6.\nlibrary(\"ggraph\")\nggraph(g1, layout = \"nicely\") + \n  geom_edge_link() + \n  geom_node_point(size=6,color=\"#8856a7\") + \n  geom_node_text(label=vertex_attr(g1)$name,  color=\"white\")__\n\nFigure 10.6: A ggraph example.\n\n\n12.1.1.5 An example: a four state Markov chain\nIn Chapter 2, we saw how a Markov chain can summarize transitions between nucleotides (considered the states of the system). This is often schematized by a graph. The igraph package provides many choices for graph “decoration”:\nlibrary(\"markovchain\")\nstatesNames = c(\"A\", \"C\", \"G\",\"T\")\nT1MC = new(\"markovchain\", states = statesNames, transitionMatrix =\n  matrix(c(0.2,0.1,0.4,0.3,0,1,0,0,0.1,0.2,0.2,0.5,0.1,0.1,0.8,0.0),\n         nrow = 4,byrow = TRUE, dimnames = list(statesNames, statesNames)))\nplot(T1MC, edge.arrow.size = 0.4, vertex.color = \"purple\",\n     edge.arrow.width = 2.2, edge.width = 5, edge.color = \"blue\",\n     edge.curved = TRUE, edge.label.cex = 2.5, vertex.size= 32,\n     vertex.label.cex = 3.5, edge.loop.angle = 3,\n     vertex.label.family = \"sans\", vertex.label.color = \"white\")__\n\nFigure 10.7: A four state Markov chain with arrows representing possible transitions between states.\nMarkov chains are simple models of dynamical systems, and the states are represented by the nodes in the graph. The transition matrix gives us the weights on the directed edges (arrows) between the states.\n__\nQuestion 10.5\nWhich state do you think this Markov chain will end up in?\n__\n해결책\n__\nC를 제외한 모든 노드는 나가는 에지(outgoing edges)를 가지고 있으며, C는 들어오는 에지와 자기 자신으로 향하는 에지만을 가집니다. 따라서 C는 흡수 상태(absorbing state)입니다. 이 체인의 모든 프로세스는 조만간 C에 도달하여 머물게 됩니다.\n__\n태스크\n\nset.seed 함수의 입력값을 바꾸어 보고 플롯이 변하는지 확인해 보세요.\n이 특정 plot 함수에 대한 도움말에 접속해 보세요.\n그래프를 다시 그리고, 전이 확률(transition probabilities) 레이블은 녹색으로, 정점(vertices)은 갈색으로 표시해 보세요.\n\n연습 문제 10.3에서 어노테이션이 달린 상태 공간 마르코프 체인 그래프의 완전한 예시를 구축하는 방법을 살펴볼 것입니다.\n\n\n\n12.1.2 10.2.2 여러 레이어가 있는 그래프: 에지와 노드의 레이블\n여기에 정점에 어노테이션이 달린, STRING 데이터베이스에서 다운로드한 그래프를 그리는 예시가 있습니다.\ndatf = read.table(\"../data/string_graph.txt\", header = TRUE)\ngrs = graph_from_data_frame(datf[, c(\"node1\", \"node2\")], directed = FALSE)\nE(grs)$weight = 1\nV(grs)$size = centr_degree(grs)$res\nggraph(grs) +\n  geom_edge_arc(color = \"black\",  strength = 0.05, alpha = 0.8)+\n  geom_node_point(size = 2.5, alpha = 0.5, color = \"orange\") +\n  geom_node_label(aes(label=vertex_attr(grs)$name), size = 3, alpha = 0.9, color = \"#8856a7\", repel = TRUE) __\n\n그림 10.8: Yu 등 (2012)에서 정렬된 T 세포의 차등 유전자 발현 패턴을 사용하여 발견된 섭동된 케모카인 하위 네트워크. 오른쪽 모서리에 있는 CXCR3, CXCL13, CCL19, CSCR5 및 CCR7 유전자의 클릭(clique)과 유사한 구조에 주목하세요.\n그림 10.8은 (Nacu et al. 2007)에서 GXNA를 사용하여 유방암 전이 연구에서 발견되고 Yu 등 (2012)에 의해 보고된 전체 섭동 케모카인 하위 네트워크를 보여줍니다.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "10-chap.html#유전자-세트-농축에서-네트워크로",
    "href": "10-chap.html#유전자-세트-농축에서-네트워크로",
    "title": "12  10.1 이 장의 목표",
    "section": "12.2 10.3 유전자 세트 농축에서 네트워크로",
    "text": "12.2 10.3 유전자 세트 농축에서 네트워크로\n\n\n\n차등 발현될 가능성이 있는 유전자들의 길고 구조화되지 않은 세탁물 리스트(laundry list)는 위협적일 수 있습니다.\n\n\n차등 발현될 가능성이 있는 유전자들의 길고 구조화되지 않은 세탁물 리스트(laundry list)는 위협적일 수 있습니다.\n8장에서 우리는 차등 발현된 유전자 리스트를 찾는 방법들을 공부했습니다. 적은 표본 크기와 낮은 FDR을 유지하려는 노력은 종종 차등 발현을 감지하는 낮은 검정력으로 이어집니다. 따라서 차등 발현된 것으로 자신 있게 선언할 수 있는 긴 유전자 리스트를 얻는 것은 초기에는 승리입니다. 그러나 기저의 생물학을 이해하려면 생물학적 시스템에서 유의미한 행위자들의 단순한 리스트 이상의 것이 필요합니다.\n\n12.2.1 10.3.1 사전 정의된 유전자 세트를 사용하는 방법(GSEA)\n가장 초기 접근 방식 중 하나는 유의미한 유전자 리스트에서 과다하게 나타나거나(overrepresented) 농축된(enriched) 유전자 속성을 찾는 것이었습니다. 이러한 유전자 클래스들은 종종 유전자 온톨로지(Gene Ontology, GO) 범주에 기반합니다(예: 기관 성장에 관여하는 유전자, 또는 섭식 행동에 관여하는 유전자). 유전자 온톨로지(GO)는 유전자와 유전자 산물을 설명하는 세 가지 온톨로지의 모음입니다. 이러한 온톨로지들은 유향 비순환 그래프(DAGS) 구조를 가진 제한된 어휘들입니다. 가장 구체적인 용어들이 그래프의 잎(leaves)이 됩니다. GO 그래프는 노드(여기서는 유전자 온톨로지 용어)와 더 구체적인 용어(자식)에서 덜 구체적인 용어(부모)로 가는 에지들로 구성되며, 흔히 이러한 에지들은 방향성이 있습니다. 노드와 에지는 시각화될 수 있는 여러 속성을 가질 수 있습니다. 실험에서 유의미한 것으로 지정된 특정 유전자 세트에 대해 GO 어노테이션을 사용하는 주된 목적은 이 리스트에서 특정 GO 용어의 농축(enrichment)을 찾는 것이며, 우리는 아래에서 이 용어에 통계적 의미를 부여할 것입니다. 다른 많은 유용한 중요 유전자 세트 리스트들이 존재합니다.\n__\n태스크\n유용한 유전자 세트 데이터베이스를 찾아보세요.\n예를 들어, MsigDB 분자 시그니처 데이터베이스(Liberzon et al. 2011)에는 많은 유전자 세트가 포함되어 있으며, 이는 Bioconductor 패키지 GSEABase의 getBroadSets 함수를 사용하여 R 내부에서 대략 다음과 같이 액세스할 수 있습니다:\nlibrary(\"GSEABase\")\n## 웹사이트 로그인이 필요합니다.\nfl   =  \"/path/to/msigdb_v5.1.xml\"\ngss  =  getBroadSets(fl) \norganism(gss[[1]])\ntable(sapply(gss, organism))__\n\n\n12.2.2 10.3.2 2원 분할표 검정을 이용한 유전자 세트 분석\n표 10.1: 유의미한 세트에서 각 범주의 유전자 수가 동일하더라도, 아래의 시뮬레이션과 2원 분할표(two-way tables)의 검정 이론은 모두 파란색 범주가 농축되었음을 보여줍니다.\n\n\n\n노란색\n파란색\n빨간색\n\n\n\n\n\n유의미함\n25\n25\n25\n\n\n전체(Universe)\n500\n100\n400\n\n\n\n여기서는 흔히 피셔의 “정확” 검정(Fisher’s “exact” test) 또는 초기하 검정(hypergeometric test)이라 불리는 기본적인 접근 방식을 설명하는 것부터 시작하겠습니다.\n\n\n\n소위 ‘정확’ 검정이라고 불리는 이유는 비모수적이며 전수 조사(exhaustive enumerations)에 기반하기 때문이지, 답이 확실하기 때문은 아닙니다. 결국 이것은 통계학이니까요.\n\n\n소위 ‘정확’ 검정이라고 불리는 이유는 비모수적이며 전수 조사(exhaustive enumerations)에 기반하기 때문이지, 답이 확실하기 때문은 아닙니다. 결국 이것은 통계학이니까요.\n잠재적으로 유의미할 수 있는 후보 유전자들의 전체 집합(universe)을 정의하십시오; 이 유니버스의 크기를 \\(N\\)이라고 합시다. 우리는 또한 실제로 유의미하게 나타난 유전자들의 기록을 가지고 있는데, 그 수가 \\(m\\)개라고 가정해 봅시다.\n우리는 유전자 유니버스에서 식별된 유전자들에 대응하는 총 \\(N\\)개의 공이 상자 안에 들어 있는 장난감 모델을 만듭니다. 이 유전자들은 서로 다른 기능적 범주로 나뉘어 있습니다. \\(N=1,000\\)개의 유전자가 있고 그중 500개는 노란색, 100개는 파란색, 400개는 빨간색이라고 가정해 봅시다. 그런 다음 \\(m=75\\)개의 유전자 하위 집합에 유의미함(significant)이라는 레이블이 붙습니다. 이 유의미하게 흥미로운 유전자들 중에 노란색 25개, 빨간색 25개, 파란색 25개가 있다고 가정합시다. 파란색 범주가 농축되었거나 과다하게 나타났다고 할 수 있을까요?\n우리는 일부 범주는 매우 수가 많고 다른 범주는 드물다는 사실을 고려하기 위해 이 초기하 2원 분할표 검정을 사용합니다.\n__\n질문 10.6\n20,000번의 시뮬레이션을 통해 몬테카를로 실험을 실행하고, 유의미한 세트에서 어떤 범주도 과다하게 나타나지 않는다는 귀무 가설하에서 파란색이 25개일 때의 유의성 p-값을 계산하세요.\n__\n해결책\n__\n귀무 가설하에서 75개는 다음과 같이 우리의 균등하지 않은 상자들로부터 무작위로 샘플링됩니다:\nuniverse = c(rep(\"Yellow\", 500), rep(\"Blue\", 100), rep(\"Red\", 400))\ncountblue = replicate(20000, {\n  pick75 = sample(universe, 75, replace = FALSE)\n  sum(pick75 == \"Blue\")\n})\nsummary(countblue)__\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   6.000   7.000   7.496   9.000  20.000 \n그림 10.9의 히스토그램은 귀무 모델하에서 25만큼 큰 값을 갖는 것이 매우 드물 것임을 보여줍니다.\n\n그림 10.9: 20,000번의 시뮬레이션에서도 파란색 카운트가 25에 근접하는 경우가 없음을 볼 수 있습니다. 우리는 그러한 사건이 우연히 일어났을 가능성을 기각하고 파란색이 농축되었다고 결론 내릴 수 있습니다.\n일반적인 경우, 유전자 유니버스는 \\(N\\)개의 공이 들어 있는 항아리이며, 만약 우리가 \\(m\\)개의 공을 무작위로 뽑고 파란색 공의 비율이 \\(k/N\\)이라면, \\(k\\)개 크기의 추출에서 \\(km/N\\)개의 파란색 공을 볼 것으로 기대합니다.\n\n12.2.2.1 GOplot을 이용한 유전자 농축 네트워크 플로팅\n여기서는 유전자 기능 범주와 유의미한 유전자 세트 사이의 연결을 요약하는 매력적인 방법을 보여줍니다.\nlibrary(\"GOplot\")\ndata(\"EC\")\ncirc  =  circle_dat(EC$david, EC$genelist)\nchord =  chord_dat(circ, EC$genes, EC$process)\nGOChord(chord, limit = c(0, 5))__\n\n그림 10.10: 이 그래프는 두 가지 정상 상태 조직(뇌와 심장, Nolan 등 (2013) 참조)의 내피 세포에서의 차등 발현 연구에서 GO 용어와 유의미하게 변한 유전자 사이의 대응 관계를 보여줍니다. 정규화 후 차등 발현 분석이 수행되어 유전자 리스트를 얻었습니다. 그 다음 GOplot 패키지를 사용하여 차등 발현된 유전자 세트(조정된 p-값 &lt; 0.05)에 대한 유전자 어노테이션 농축 분석을 수행했습니다.\n사실, 유전자 온톨로지 그래프는 서로 다른 프로세스의 유전자들이 종종 생산적으로 상호작용하기 때문에 의미 있는 _유전자 상호작용_을 반드시 캡처하는 것은 아닙니다. 많은 양의 정보가 사용되지 않은 채로 남아 있는데, 예를 들어 모든 유의미한 유전자는 p-값의 잠재적으로 큰 변동에도 불구하고 보통 동일한 가중치가 부여됩니다.\n\n\n\n12.2.3 10.3.3 유의미한 하위 그래프 및 고득점 모듈\n우리는 유전자 온톨로지 이상의 것을 마음대로 사용할 수 있습니다. 유전자 네트워크의 다양한 데이터베이스가 있으며, 그중에서 우리의 차등 발현 실험에서 얻은 p-값과 같은 유의성 점수를 투영할 알려진 골격(skeleton) 그래프를 선택할 수 있습니다. 우리는 Ideker 등 (2002)이 처음 제안한 아이디어를 따를 것입니다. 이는 Nacu 등 (2007)에서 더 발전되었습니다. 많은 개선 사항이 포함된 세심한 구현체는 Bioconductor 패키지 BioNet (Beisser et al. 2010)으로 제공됩니다. 이러한 방법들은 모두 점수가 매겨진 골격 네트워크에서 특히 섭동된 것으로 보이는 하위 그래프나 모듈을 검색합니다.\n네트워크의 각 유전자 노드에는 t-통계량이나 p-값으로부터 계산된 점수가 할당됩니다. 종종 경로(pathways)는 상향 조절된 유전자와 하향 조절된 유전자를 모두 포함하는데, Ideker 등 (2002)에서 지적했듯이, 이는 검정 통계량의 절대값을 취하거나 p-값으로부터 계산된 점수를 통합함으로써 캡처될 수 있습니다2. Beisser 등 (2010)은 6장에서 했던 것처럼 유전자의 p-값을 모델링합니다: p-값이 균등하게 분포할 섭동되지 않은 유전자와, 섭동된 유전자로부터 나온 균등하지 않게 분포된 p-값의 혼합물입니다. 우리는 Pounds와 Morris (2003)를 따라 p-값에 대해 베타 분포를 사용하여 데이터의 신호를 모델링합니다.\n2 작은 p-값이 큰 점수를 주도록 \\(-p\\)와 같은 것이 필요할 것입니다.\n노드 점수 함수가 주어지면, 그래프에서 연결된 핫스팟, 즉 높은 결합 점수를 가진 유전자들의 하위 그래프를 검색합니다.\n\n12.2.3.1 하위 그래프 검색 알고리즘 사용하기\n일반적인 그래프의 최대 점수 하위 그래프를 찾는 것은 일반적으로 다루기 힘든(intractable) 것으로 알려져 있으므로(NP-어렵 문제라고 함), 다양한 근사 알고리즘이 제안되었습니다. Ideker 등 (2002)은 시뮬레이티드 어닐링(simulated annealing) 사용을 제안했지만, 이는 속도가 느리고 해석하기 어려운 큰 하위 그래프를 생성하는 경향이 있습니다. Nacu 등 (2007)은 시드 정점에서 시작하여 그 주변을 점진적으로 확장했습니다. Beisser 등 (2010)은 소위 최소 신장 트리(MST)를 사용하여 검색을 시작했는데, 이 그래프는 이 장의 뒷부분에서 공부할 것입니다.\n\n\n\n12.2.4 10.3.4 BioNet 구현 예시\n방법을 설명하기 위해 BioNet 패키지의 데이터를 보여줍니다.\ninteractome 데이터는 2,034개의 서로 다른 유전자 산물과 8,399개의 상호작용으로 구성된 네트워크의 연결 성분(connected component)을 포함합니다. 이것이 우리가 작업할 골격 그래프를 구성합니다 (Beisser et al. (2010) 참조).\ndataLym은 3,583개 유전자에 대한 관련 p-값과 \\(t\\) 통계량을 포함하며, 다음과 같이 액세스하여 분석을 수행할 수 있습니다:\nlibrary(\"BioNet\")\nlibrary(\"DLBCL\")\ndata(\"dataLym\")\ndata(\"interactome\")\ninteractome __\n\n\nA graphNEL graph with undirected edges\nNumber of Nodes = 9386 \nNumber of Edges = 36504 \n\n\npval = dataLym$t.pval\nnames(pval)  =  dataLym$label\nsubnet = subNetwork(dataLym$label, interactome)\nsubnet = rmSelfLoops(subnet)\nsubnet __\n\n\nA graphNEL graph with undirected edges\nNumber of Nodes = 2559 \nNumber of Edges = 7788 \n\n12.2.4.1 베타-균등 모델 적합시키기 (Fit a Beta-Uniform model)\np-값은 4장에서 공부한 혼합 모델 유형으로 적합됩니다. 여기에는 확률 (_0)을 갖는 귀무 가설의 균등 성분과 대립 가설에 해당하는 p-값에 대한 베타 분포((a x^{a - 1})에 비례)가 포함됩니다 (Pounds and Morris 2003). [f(x|a,_0)= _0 + (1-_0) a x^{a - 1} 0 &lt;x ; ; 0&lt;a&lt;1] [fdr]을 0.001로 설정하여 모델을 실행합니다:\n패키지는 실제로 (_0)에 다른 이름을 부여합니다: ()를 사용하고 이를 혼합 매개변수(mixing parameter)라고 부릅니다.\nfb = fitBumModel(pval, plot = FALSE)\nfb __\n\n\nBeta-Uniform-Mixture (BUM) model\n\n3583 pvalues fitted\n\nMixture parameter (lambda): 0.482\nshape parameter (a):        0.180\nlog-likelihood:         4471.8\n\n\nscores=scoreNodes(subnet, fb, fdr = 0.001)__\n\n그림 10.11: qqplot은 데이터에 대한 베타-균등 혼합 모델 적합의 품질을 보여줍니다. 빨간색 점은 베타 분포의 이론적 분위수를 x 좌표로, 관측된 분위수를 y 좌표로 가집니다. 파란색 선은 이 모델이 잘 들어맞음을 보여줍니다.\n\n그림 10.12: p-값에 대한 혼합 성분의 히스토그램입니다. 베타 분포는 빨간색, 균등 분포는 파란색으로 표시됩니다. \\(_0\\)는 분포가 균등해야 하는 귀무 성분에 할당된 혼합 비율입니다.\n그 다음 다음을 사용하여 고득점 하위 그래프에 대한 휴리스틱 검색을 실행합니다:\nhotSub  =  runFastHeinz(subnet, scores)\nhotSub __\n\n\nA graphNEL graph with undirected edges\nNumber of Nodes = 144 \nNumber of Edges = 221 \n\n\nlogFC=dataLym$diff\nnames(logFC)=dataLym$label __\n__\n질문 10.7\n다음 코드를 사용하여 그림 10.13을 만들었습니다:\nplotModule(hotSub, layout = layout.davidson.harel, scores = scores,\n                  diff.expr = logFC)__\n\n그림 10.13: ABC와 GCB B세포 림프종 사이의 차등 발현에 대해 최대로 농축된 것으로 찾아진 하위 그래프입니다. 노드는 빨간색과 초록색으로 칠해져 있습니다: 초록색은 ACB에서의 상향 조절을, 빨간색은 GBC에서의 상향 조절을 나타냅니다. 노드의 모양은 점수를 나타냅니다: 직사각형은 음수 점수를, 원은 양수 점수를 나타냅니다.\nigraph.from.graphNEL 함수를 사용하여 모듈 객체를 변환하고 섹션 10.2.2에 표시된 ggraph 방법을 사용하여 플로팅하세요.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "10-chap.html#계통수-phylogenetic-trees",
    "href": "10-chap.html#계통수-phylogenetic-trees",
    "title": "12  10.1 이 장의 목표",
    "section": "12.3 10.4 계통수 (Phylogenetic Trees)",
    "text": "12.3 10.4 계통수 (Phylogenetic Trees)\n\n그림 10.14: 수학적 객체로서, 계층적 클러스터링 트리(5장에서 연구됨)는 계통수와 동일합니다. 이들은 팁에 레이블이 있는 뿌리가 있는 이진(rooted binary) 트리입니다.\n생물학에서 그래프의 정말 중요한 용도 중 하나는 계통수를 구축하는 것입니다. 트리는 사이클(cycles)(셀프 루프나 여러 정점을 거치는 루프를 일컫는 공식 용어)이 없는 그래프입니다. 계통수는 보통 팁에 있는 현대의3 분류군(taxa)에 해당하는 잎(leaves)에만 레이블이 있는 뿌리가 있는 이진 트리입니다. 내부 노드는 팁에 있는 현대(contemporaneous) 데이터로부터 추론되어야 하는 조상(ancestral) 서열에 해당합니다. 많은 방법들이 서로 다른 종이나 집단에서 얻은 정렬된 DNA 서열을 사용하여 트리를 추론하거나 추정합니다. 트리의 팁은 보통 OTU(Operational Taxonomic Units)라고 불립니다. 이러한 분석에서 관심 있는 통계적 매개변수(parameter)는 잎에 OTU 레이블이 있는 뿌리가 있는 이진 트리입니다 (자세한 내용은 Holmes (1999, 2003b)를 참조하세요).\n3 현대적이기 때문에, 트리는 종종 잎들이 모두 뿌리에서 같은 거리에 있도록 표현됩니다.\n\n12.3.0.1 HIV의 예\n\n그림 10.15: 이 계통수는 아프리카에서의 서로 다른 HIV/SIV 균주의 역사를 설명합니다 (Wertheim and Worobey 2009), [그림 출처].\nHIV는 매우 빠르게 진화함으로써(수개월 내에 여러 돌연변이가 나타날 수 있음) 스스로를 보호하는 바이러스입니다. 따라서 그 진화는 실시간으로 추적될 수 있는 반면, 대형 유기체의 진화는 수백만 년에 걸쳐 일어났습니다. HIV 트리는 약물 내성 탐지 및 이해와 같은 의료 목적으로 구축됩니다. 이들은 개별 유전자에 대해 추정됩니다. 서로 다른 유전자는 진화 역사에서 차이를 보일 수 있으며 따라서 서로 다른 유전자 트리(gene trees)를 생성할 수 있습니다. 그림 10.15의 계통수는 바이러스가 원숭이에서 인간으로 옮겨간 시기를 보여줍니다 (Wertheim and Worobey 2009).\n\n\n12.3.0.2 계통수의 특별한 요소들\n\n대부분의 계통수는 뿌리가 있는 것으로 표시되며, 뿌리(root)는 나중에 보게 되겠지만 보통 트리의 팁에 외집단(outgroup)을 포함시킴으로써 찾아집니다.\n이 공통 조상으로부터 유래된 형질을 상동(homologous)이라고 합니다(집단 연구를 수행하는 유전학자들은 상동이라는 용어 대신 가계에 의한 동일성(identity by descent, IBD)으로 대체합니다).\n공통 조상에 의해 정의된 트리의 자매들을 분기군(clades) 또는 단계통군(monophyletic groups)이라고 하며, 이들은 단순한 유사성 이상의 공통점을 가집니다.\n\n\n\n12.3.1 10.4.1 진화를 위한 마르코프 모델 (Markovian models for evolution)\n현대 데이터로부터 조상 종에서 일어난 일을 추론하기 위해, 우리는 시간이 지남에 따라 치환(substitutions)과 삭제(deletions)가 어떻게 발생하는지에 대한 가정을 세워야 합니다. 우리가 사용하는 모델은 모두 마르코프 모델이며 시간 동질적(time homogeneous)이라고 합니다. 즉, 돌연변이율이 역사 전반에 걸쳐 일정합니다.\n\n이것을 분자 시계(molecular clock) 가설이라고 합니다. 만약 이 가정을 하지 않는다면 비식별성(non-identifiability) 문제에 직면하게 됩니다 (즉, 관측된 데이터가 주어졌을 때 가능한 많은 돌연변이 역사들 사이의 차이를 말할 수 없습니다).\n\n12.3.1.1 연속 마르코프 체인과 생성 행렬 (Generator matrix)\n우리는 그림 10.7에서 본 [A, C, G, T] 상태에 대한 마르코프 체인을 사용할 것입니다. 그러나 이제는 상태의 변화, 즉 돌연변이가 무작위 시간에 발생한다고 가정합니다. 이러한 돌연변이 사건 사이의 간격은 지수 분포를 따를 것입니다. 이러한 연속 시간 마르코프 체인은 다음과 같은 속성을 가집니다:\n\n기억 없음(No Memory). (P(Y(u+t)=j;|;Y(t)=i))는 (t) 이전의 시간에 의존하지 않습니다.\n시간 동질성(Time homogeneity). 확률 (P(Y(h+t)=j,|,Y(t)=i))는 (t)에 의존하지 않고, 사건 사이의 시간인 (h)와 (i) 및 (j)에 의존합니다.\n선형성(Linearity). 순간 전이율은 대략 선형 형태입니다.\n\n\n여기서 (o(h))라고 쓴 오차항을 사용합니다. 이것은 리틀 (o) 오브 (h)라고 읽으며, 이 오차항이 (h)보다 훨씬 느리게(즉, 하위 선형으로) 커진다는 것을 의미합니다.\n[ \\[\\begin{align} P_{ij}(h)&=q_{ij}h+o(h), \\quad\\text{for }j\\neq i\\\\\nP_{ii}(h)&=1-q_i(h)+ o(h), \\qquad\\text{where }q_i=\\sum_{j\\neq i}q_{ij}.\n\\end{align}\\] ]\n(q_{ij})는 순간 전이율(instantaneous transition rate)로 알려져 있습니다. 이러한 비율은 표 10.2와 같은 행렬을 정의합니다.\n  * **지수 분포(Exponential distribution)**. 변화 사이의 시간은 지수 분포를 따른다고 가정합니다.\n\n\n\n\n\n\n\n(Q = \n(Q = \n\n\n\n\n\n표 10.2: 두 가지 전이율 행렬의 예입니다. 왼쪽은 Jukes-Cantor(JC69) 모델이고, 오른쪽은 Kimura(K80) 2-매개변수 모델을 보여줍니다.\n생성기(generator)라고 불리는 순간 변화 확률 행렬입니다. 가장 단순한 모델인 Jukes-Cantor 모델에서는 모든 돌연변이가 동일하게 발생할 가능성이 있습니다 (표 10.2의 왼쪽 참조). 좀 더 유연한 모델인 Kimura 모델은 표 10.2의 오른쪽에 나와 있습니다.\n__\n질문 10.8\n왜 Kimura 모델이 더 유연하다고 말할까요?\n__\n해결책\n__\nJukes-Cantor 모델은 하나의 매개변수만 가지며, 모든 전이(transitions)와 트랜스버전(transversions)이 동일하게 발생할 가능성이 있다고 가정합니다. Kimura 모델에는 전이와 트랜스버전(퓨린에서 피리미딘으로 또는 그 반대로 발생하는 돌연변이)에 대해 각각 하나씩 매개변수가 있습니다.\n\n가장 유연한 모델은 일반화된 시간 가역적(Generalized Time Reversible, GTR) 모델이라고 하며, 6개의 자유 매개변수를 가집니다. 우리는 알려진 트리로부터 이러한 생성 모델에 따라 시뮬레이션된 데이터의 예를 보여줄 것입니다. ### 10.4.2 Simulating data and plotting a tree\nSuppose we already know our phylogenetic tree and want to simulate the evolution of the nucleotides down this tree. First we visualize the tree tree1 using ggtree; loading the tree and the relevant packages with:\nlibrary(\"phangorn\")\nlibrary(\"ggtree\")\nload(file.path(DATA,\"tree1.RData\"))__\n__\nTask\nUse the ggtree function to plot tree1; make the tips of the tree green triangles, the ancestral nodes, red circles.\nggtree(tree1, lwd = 2, color = \"darkgreen\", alpha = 0.8, right = TRUE) +\n  geom_tiplab(size = 7, angle = 90, offset = 0.05) +\n  geom_point(aes(shape = isTip, color = isTip), size = 5, alpha = 0.6)__\n\nFigure 10.16: This is the tree we use as our true parameter. We generate nucleotides one at a time from the root and `dropping’ them down the tree. With some probability proportional to the edge lengths, mutations occur down the branches.\nNow we generate some sequences from our tree. Each sequence starts with a new nucleotide letter generated randomly at the root; mutations may occur as we go down the tree. You can see in Figure 10.17 that the colors are not equally represented, because the frequency at the root was chosen to be different from the uniform, see the following code.\nseqs6 = simSeq(tree1, l = 60, type = \"DNA\", bf = c(1, 1, 3, 3)/8, rate = 0.1)\nseqs6 __\n\n\n6 sequences with 60 character and 30 different site patterns.\nThe states are a c g t \n\n\nmat6df = data.frame(as.character(seqs6))\np = ggtree(tree1, lwd = 1.2) + geom_tiplab(aes(x = branch), size = 5, vjust = 2)\ngheatmap(p, mat6df[, 1:60], offset = 0.01, colnames = FALSE)__\n\nFigure 10.17: The tree on the left was used to generate the sequences on the right according to a Jukes Cantor model. The nucleotide frequencies generated at the root were quite unequal, with A and C being generated more rarely. As the sequences percolate down the tree, mutations occur, they are more likely to occur on the longer branches.\n__\nQuestion 10.9\nExperiment with the code above. Change the bf and rate arguments in the simSeq function to make mutations more likely. Do you think sequences generated with a very high mutation rate would make it easier to infer the tree that generated them?\n__\nSolution\n__\nVery high mutation rates result in mutations overwriting themselves and make inference more difficult. Of course, there is a sweet spot because some mutations have to occur in order for us to resolve the tree branches. After a certain time and a certain number of mutations it may be very difficult to see what was happening at the root. See Mossel (2003) for details.\n__\nQuestion 10.10\nEstimation bias: distance underestimation.\n1) If we only count the number of changes between two sequences using a simple Hamming distance, but there has been much evolutionary change between the two, why do we underestimate the distance between the sequences?\n2) Will be the bias be larger for smaller evolutionary distances?\nThe standard Markovian models of evolution we saw above enable us to improve these estimates.\n\n\n\n12.3.2 10.4.3 Estimating a phylogenetic tree\n\n“In solving a problem of this sort, the grand thing is to be able to reason backward. That is a very useful accomplishment, and a very easy one, but people do not practise it much. In the everyday affairs of life it is more useful to reason forward, and so the other comes to be neglected. There are fifty who can reason synthetically for one who can reason analytically”. Sherlock Holmes\nWhen the true tree-parameter is known, the above-mentioned probabilistic generative models of evolution tells us what patterns to expect in the sequences. As we have seen in earlier chapters, statistics means going back from the data to reasonable estimates of the parameters; here the tree itself and the branch lengths, even the evolutionary rates can be considered to be the parameters.\n\nFigure 10.18: A Steiner tree, the inner points are represented as squares. The method for creating the shortest tree that passes through all outer 1,2,5,6 is to create two inside (“ancester”) points 3 and 4.\nThere are several approaches to estimation: tree `building’ is no exception, here are the main ones:\nA nonparametric estimate: the parsimony tree Parsimony is a nonparametric method that minimizes the number of changes necessary to explain the data, it’s solution is the same as that of the Steiner tree problem (see Figure 10.18).\nA parametric estimate: the maximum likelihood tree In order to estimate the tree using a maximum likelihood or Bayesian approach one needs a model for molecular evolution that integrates mutation rates and branch edge lengths. ML estimation (e.g., Phyml, FastML, RaxML) use efficient optimization algorithms to maximize the likelihood of a tree under the model assumptions.\nBayesian posterior distributions for trees Bayesian estimation, MrBayes (Ronquist et al. 2012) or BEAST (Bouckaert et al. 2014) both use MCMC to find posterior distributions of the phylogenies. Bayesian methods are not directly integrated into R and require the user to import the collections of trees generated by Monte Carlo methods in order to summarize them and make confidence statements see Chakerian and Holmes (2012) for simple examples.\nThe semi-parametric approach: distance based methods These methods, called Neighbor Joining and UPGMA, are quite similar to the hierachical clusering algorithms we already encountered in Chapter 5. However, the distance estimation steps uses the parametric evolutionary models of Table 10.2; the `parametric’ part of why we call the method semi-parametric.\nThe neighbor-joining algorithm itself uses Steiner points as the summary of two combined points, and proceeds iteratively as in hierarchical clustering. It can be quite fast and is often used as a good starting point for the more time-consuming methods.\nLet’s start by estimating the tree from the data seqs6 using the nj (neighbor joining) on DNA distances based on the one-parameter Jukes-Cantor model, we make Figure 10.19 using the ggtree function:\ntree.nj = nj(dist.ml(seqs6, \"JC69\"))\nggtree(tree.nj) + geom_tiplab(size = 7) __\n\nFigure 10.19: Trees built with a neighbor joining algorithm are very fast to compute and are often used as initial values for more expensive estimation procedures such as the maximum likelihood or parsimony.\n__\nQuestion 10.11\nGenerate the maximum likelihood scores of the tree1 given the seqs6 data and compare them to those of the neighbor joining tree.\n__\nSolution\n__\nfit = pml(tree1, seqs6, k = 4)__\n__\nQuestion 10.12\nWhen we have aligned amino acids from which we want to infer a tree, we use (\\(20 \\)) transition matrices. The methods for estimating the phylogenetic are very similar. Try this in phangorn with an HIV amino acid sequence downloaded from https://www.hiv.lanl.gov/content/sequence/NEWALIGN/align.html.\nThe quality of the tree estimates depend on the number of sequences per taxa and the distance to the root. We can evaluate the quality of the estimates either by using parametric and nonparametric bootstraps or performing Bayesian tree estimation using MCMC. For examples of how to visualize and compare the sampling distribution of trees, see Chakerian and Holmes (2012).\n\n\n12.3.3 10.4.4 Application to 16S rRNA data\nIn Chapter 5 we saw how to use a probabilistic clustering method to denoise 16S rRNA sequences. We can now reload these denoised sequences and preprocess them before building their phylogeny4.\n4 In order to keep all the information and be able to compare sequences from different experiments, we use the sequences themselves as their label(Callahan, McMurdie, and Holmes 2017).\nlibrary(\"dada2\")\nseqtab = readRDS(file.path(DATA,\"seqtab.rds\"))\nseqs = getSequences(seqtab)\nnames(seqs) = seqs __\nOne of the benefits of using well-studied marker loci such as the 16S rRNA gene is the ability to taxonomically classify the sequenced variants. dada2 includes a naive Bayesian classifier method for this purpose (Wang et al. 2007). This classifier compares sequence variants to training sets of classified sequences. Here we use the RDP v16 training set (Cole et al. 2009)5. For example, code for such a classification might look like this.\n5 See the download link on the dada2 website: https://benjjneb.github.io/dada2/training.html\nfastaRef = \"../tmp/rdp_train_set_16.fa.gz\"\ntaxtab = assignTaxonomy(seqtab, refFasta = fastaRef)__\nSince the assignTaxonomy function runs for a while, the above code is not live and we here load a previously computed result, a table of taxonomic information:\ntaxtab = readRDS(file.path(DATA,\"taxtab16.rds\"))\ndim(taxtab)__\n\n\n[1] 268   6\n__\nQuestion 10.13\nWrite one line of code using R’s pipe operator |&gt; that shows just the first 6 rows of the taxonomic information without the row names.\n__\nSolution\n__\nhead(taxtab) |&gt; `rownames&lt;-`(NULL)__\n\n\n     Kingdom    Phylum          Class         Order          \n[1,] \"Bacteria\" \"Bacteroidetes\" \"Bacteroidia\" \"Bacteroidales\"\n[2,] \"Bacteria\" \"Bacteroidetes\" \"Bacteroidia\" \"Bacteroidales\"\n[3,] \"Bacteria\" \"Bacteroidetes\" \"Bacteroidia\" \"Bacteroidales\"\n[4,] \"Bacteria\" \"Bacteroidetes\" \"Bacteroidia\" \"Bacteroidales\"\n[5,] \"Bacteria\" \"Bacteroidetes\" \"Bacteroidia\" \"Bacteroidales\"\n[6,] \"Bacteria\" \"Bacteroidetes\" \"Bacteroidia\" \"Bacteroidales\"\n     Family               Genus        \n[1,] \"Porphyromonadaceae\" NA           \n[2,] \"Porphyromonadaceae\" NA           \n[3,] \"Porphyromonadaceae\" NA           \n[4,] \"Porphyromonadaceae\" \"Barnesiella\"\n[5,] \"Bacteroidaceae\"     \"Bacteroides\"\n[6,] \"Porphyromonadaceae\" NA           \n__\nQuestion 10.14\nWhat is the difference between taxonomic and phylogenetic information?\nNote that as the seqs data are randomly generated, they are “cleaner” than the real data we will have to handle.\nIn particular naturally occurring raw sequences have to be aligned. This is necessary as there are often extra nucleotides in some sequences, a consequence of what we call indel events6. Also mutations occur and appear as substitutions of one nucleotide by another.\n6 A nucleotide is deleted or inserted and it is often hard to distinguish which took place.\nHere is an example of what the first few characters of aligned sequences looks like:\nreadLines(file.path(DATA,\"mal2.dna.txt\")) |&gt; head(12) |&gt; cat(sep=\"\\n\")__\n\n\n    11   1620\nPre1        GTACTTGTTA GGCCTTATAA GAAAAAAGT- TATTAACTTA AGGAATTATA\nPme2        GTATCTGTTA AGCCTTATAA AAAGATAGT- T-TAAATTAA AGGAATTATA\nPma3        GTATTTGTTA AGCCTTATAA GAGAAAAGTA TATTAACTTA AGGA-TTATA\nPfa4        GTATTTGTTA GGCCTTATAA GAAAAAAGT- TATTAACTTA AGGAATTATA\nPbe5        GTATTTGTTA AGCCTTATAA GAAAAA--T- TTTTAATTAA AGGAATTATA\nPlo6        GTATTTGTTA AGCCTTATAA GAAAAAAGT- TACTAACTAA AGGAATTATA\nPfr7        GTACTTGTTA AGCCTTATAA GAAAGAAGT- TATTAACTTA AGGAATTATA\nPkn8        GTACTTGTTA AGCCTTATAA GAAAAGAGT- TATTAACTTA AGGAATTATA\nPcy9        GTACTCGTTA AGCCTTTTAA GAAAAAAGT- TATTAACTTA AGGAATTATA\nPvi10       GTACTTGTTA AGCCTTTTAA GAAAAAAGT- TATTAACTTA AGGAATTATA\nPga11       GTATTTGTTA AGCCTTATAA GAAAAAAGT- TATTAATTTA AGGAATTATA\nWe will perform this multiple-alignment on our seqs data using the DECIPHER package (Wright 2015):\nlibrary(\"DECIPHER\")\nalignment = AlignSeqs(DNAStringSet(seqs), anchor = NA, verbose = FALSE)__\nWe use the phangorn package to build the MLE tree (under the GTR model), but will use the neighbor-joining tree as our starting point.\nphangAlign = phangorn::phyDat(as(alignment, \"matrix\"), type = \"DNA\")\ndm = phangorn::dist.ml(phangAlign)\ntreeNJ = phangorn::NJ(dm)   # Note: tip order != sequence order\nfit = phangorn::pml(treeNJ, data = phangAlign)\nfitGTR = update(fit, k = 4, inv = 0.2)\nfitGTR = phangorn::optim.pml(fitGTR, model = \"GTR\", optInv = TRUE,\n         optGamma = TRUE,  rearrangement = \"stochastic\",\n         control = phangorn::pml.control(trace = 0))__",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "10-chap.html#combining-phylogenetic-trees-into-a-data-analysis",
    "href": "10-chap.html#combining-phylogenetic-trees-into-a-data-analysis",
    "title": "12  10.1 이 장의 목표",
    "section": "12.4 10.5 Combining phylogenetic trees into a data analysis",
    "text": "12.4 10.5 Combining phylogenetic trees into a data analysis\nWe now need to combine the phylogenetic tree and the denoised read abundances with the complementary information provided about the samples from which the reads were gathered. This information about the sample is often provided as a spreadhseet (or .csv), and sometimes called the meta -data7. This data combination step is facilitated by the specialized containers and accessors that phyloseq provides.\n7 We consider the prefix meta unhelpful and potentially confusing here: the data about the samples is just that: data.\nThe following set of steps contains a few data cleanup and reorganization tasks—a dull but necessary part of applied statistics—that end in the creation of the object ps1.\nsamples = read.csv(\"../data/MIMARKS_Data_combined.csv\", header = TRUE)\nsamples$SampleID = paste0(gsub(\"00\", \"\", samples$host_subject_id), \n                          \"D\", samples$age-21) \nsamples = samples[!duplicated(samples$SampleID), ] \nstopifnot(all(rownames(seqtab) %in% samples$SampleID))\nrownames(samples) = samples$SampleID \nkeepCols = c(\"collection_date\", \"biome\", \"target_gene\", \"target_subfragment\", \n  \"host_common_name\", \"host_subject_id\", \"age\", \"sex\", \"body_product\", \"tot_mass\",\n  \"diet\", \"family_relationship\", \"genotype\", \"SampleID\") \nsamples = samples[rownames(seqtab), keepCols] __\nThe sample-by-sequence feature table, the sample (meta)data, the sequence taxonomies, and the phylogenetic tree—are combined into a single object as follows:\nlibrary(\"phyloseq\")\npso = phyloseq(tax_table(taxtab), \n               sample_data(samples),\n               otu_table(seqtab, taxa_are_rows = FALSE), \n               phy_tree(fitGTR$tree))__\nWe have already encountered several cases of combining heterogeneous datasets into special data classes that automate the linking and keeping consistent the different parts of the dataset (e.g., in Chapter 8, when we studied the pasilla data).\n__\nTask\nLook at the detailed phyloseq documentation here. Try a few filtering operations. For instance, create a subset of the data that contains the tree, taxa abundance table, the sample and taxa information for only the samples that have more than 5000 reads.\nThis can be done in one line:\nprune_samples(rowSums(otu_table(pso)) &gt; 5000, pso)__\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 268 taxa and 10 samples ]\nsample_data() Sample Data:       [ 10 samples by 14 sample variables ]\ntax_table()   Taxonomy Table:    [ 268 taxa by 6 taxonomic ranks ]\nphy_tree()    Phylogenetic Tree: [ 268 tips and 266 internal nodes ]\nWe can also make other data transformations while maintaining the integrity of the links between all the data components.\n__\nQuestion 10.15\nWhat do the following lines of code do?\nprevalence = apply(X = otu_table(pso),\n                   MARGIN = ifelse(taxa_are_rows(pso), yes = 1, no = 2),\n                   FUN = function(x) {sum(x &gt; 0)})\nprevdf = data.frame(Prevalence = prevalence,\n                    TotalAbundance = taxa_sums(pso),\n                    tax_table(pso))\ntab = table(prevdf$Phylum)\nkeepPhyla = names(tab)[tab&gt;5]\nprevdf1   = subset(prevdf,   Phylum %in% keepPhyla)\nps2v      = subset_taxa(pso, Phylum %in% keepPhyla)__\nPlotting the abundances for certain bacteria can be done using barcharts. ggplot2 expressions have been hardwired into suitable one-line calls in the phyloseq package. There is also an interactive Shiny-phyloseq browser based tool (McMurdie and Holmes 2015). For more details, please see the online vignettes.\n\n12.4.1 10.5.1 Hierarchical multiple testing\nHypothesis testing can identify individual bacteria whose abundance relates to sample variables of interest. A standard approach is very similar to the approach we already visited in Chapter 6. Compute a test statistic for each taxa individually; then jointly adjust p-values to ensure a false discovery rate upper bound. However, this procedure does not exploit the structure among the tested hypotheses. For example, if we observe that one Ruminococcus species is strongly associated with age, but the biological reason for this sits at the genus level, then we would expect other species to have such an association as well. To integrate such information, Benjamini and Yekutieli (2003) and Benjamini and Bogomolov (2014) proposed a hierarchical testing procedure, where lower level taxonomic groups are only tested if higher levels are found to be be associated. In the case where many related species have a slight signal, this pooling of information can increase power.\nWe apply this method to test the association between microbial abundance and age. We use the data object ps1, which is similar to pso from above, but has undergone some additional transformation and filtering steps. We also need to apply the normalization protocols available in the DESeq2 package, which we discussed in Chapter 8, following Love, Huber, and Anders (2014) for RNA-Seq data and McMurdie and Holmes (2014) for 16S rRNA generated count data.\n# warning: !expr c(\"DESeqDataSet.se, design = design, ignoreRank.: some variables in design formula are characters, converting to factors\")\nlibrary(\"DESeq2\")\nps1 = readRDS(file.path(DATA,\"ps1.rds\"))\nps_dds = phyloseq_to_deseq2(ps1, design = ~ ageBin + family_relationship)\ngeometricmean = function(x)\n   if (all(x == 0)) { 0 } else { exp(mean(log(x[x != 0]))) }\ngeoMeans = apply(counts(ps_dds), 1, geometricmean)\nps_dds = estimateSizeFactors(ps_dds, geoMeans = geoMeans)\nps_dds = estimateDispersions(ps_dds)\nabund = getVarianceStabilizedData(ps_dds)__\nWe use the structSSI package to perform the hierarchical testing (Sankaran and Holmes 2014). For more convenient printing, we first shorten the names of the taxa:\nrownames(abund) = substr(rownames(abund), 1, 5) |&gt; make.names(unique = TRUE)__\nThe hierarchical testing procedure we are now going to do differs from standard multiple hypothesis testing in that univariate tests are done not only for every taxon, but for each higher-level taxonomic group. A helper function, treePValues, is available for this: it expects an edge list encoding parent-child relationships, with the first row specifying the root node.\nlibrary(\"structSSI\")\nel = phy_tree(ps1)$edge\nel0 = el\nel0 = el0[rev(seq_len(nrow(el))), ]\nel_names = c(rownames(abund), seq_len(phy_tree(ps1)$Nnode))\nel[, 1] = el_names[el0[, 1]]\nel[, 2] = el_names[el0[, 2]]\nunadj_p = treePValues(el, abund, sample_data(ps1)$ageBin)__\nWe can now do our FDR calculations using the hierarchical testing procedure. The test results are guaranteed to control several variants of FDR, but at different levels; we defer details to (Benjamini and Yekutieli 2003; Benjamini and Bogomolov 2014; Sankaran and Holmes 2014).\n__\nTask\nTry the following code, including the interactive plotting command that will open a browser window:\nhfdr_res = hFDR.adjust(unadj_p, el, 0.75)\nsummary(hfdr_res)\n#plot(hfdr_res, height = 5000) # not run: opens in a browser __\n\nFigure 10.20: A screenshot of a subtree with many differentially abundant microbes, as determined by the hierarchical testing procedure. Currently the user is hovering over the node associated with microbe GCGAG.33; this causes the adjusted p-value (0.029) to appear.\nThe plot opens in a new browser – a static screenshot of a subtree is displayed in Figure 10.20. Nodes are shaded according to p-values, from blue to orange, representing the strongest to weakest associations. Grey nodes were never tested, to focus power on more promising subtrees. Scanning the full tree; it becomes clear that the association between age group and taxonomic abundances is present in only a few isolated taxonomic groups. It is quite strong in those groups. To give context to these results, we can retrieve the taxonomic identity of the rejected hypotheses.\nlibrary(\"dplyr\")\noptions(digits = 3)\ntax = tax_table(ps1)[, c(\"Family\", \"Genus\")] |&gt; data.frame()\ntax$seq = rownames(abund)\nhfdr_res@p.vals$seq = rownames(hfdr_res@p.vals)\nleft_join(tax, hfdr_res@p.vals[,-3]) |&gt;\n  arrange(adjp) |&gt; head(9) |&gt; dplyr::select(1,2,4,5)__\n\n\n              Family       Genus hypothesisName hypothesisIndex\n1 Porphyromonadaceae        &lt;NA&gt;           &lt;NA&gt;              NA\n2 Porphyromonadaceae        &lt;NA&gt;           &lt;NA&gt;              NA\n3 Porphyromonadaceae        &lt;NA&gt;           &lt;NA&gt;              NA\n4 Porphyromonadaceae Barnesiella           &lt;NA&gt;              NA\n5     Bacteroidaceae Bacteroides           &lt;NA&gt;              NA\n6 Porphyromonadaceae Barnesiella           &lt;NA&gt;              NA\n7      Rikenellaceae   Alistipes           &lt;NA&gt;              NA\n8 Porphyromonadaceae        &lt;NA&gt;           &lt;NA&gt;              NA\n9 Porphyromonadaceae        &lt;NA&gt;           &lt;NA&gt;              NA\nIt seems that the most strongly associated bacteria all belong to family Lachnospiraceae.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "10-chap.html#minimum-spanning-trees",
    "href": "10-chap.html#minimum-spanning-trees",
    "title": "12  10.1 이 장의 목표",
    "section": "12.5 10.6 Minimum spanning trees",
    "text": "12.5 10.6 Minimum spanning trees\nA very simple and useful graph is the so-called minimum spanning tree (MST). Given a set of vertices, a spanning tree is a tree that goes through all points at least once. Examples are shown in Figure 10.21. Given distances between vertices, the MST is the spanning tree with the minimum total length (see Figure 10.21).\nGreedy algorithms work well for computing the MST and there are many implementations in R: mstree in ade4 , mst in ape , spantree in vegan , mst in igraph.\n\n\n\n\n\n\n\n\nFigure 10.21: Two spanning trees for the same set of six vertices. The blue graph is the minimum spanning tree, if the Euclidean distances between the points in the 2D plane are used.\nHere we are going to take the DNA sequence distances between strains of HIV from patients all over the world and construct their minimum spanning tree. The result is shown in Figure 10.22.\nload(file.path(DATA, \"dist2009c.RData\"))\ncountry09 = attr(dist2009c, \"Label\")\nmstree2009 = ape::mst(dist2009c)\ngr09 = graph_from_adjacency_matrix(mstree2009, mode = \"undirected\")\nggraph(gr09, layout=\"fr\") +\n  geom_edge_link(color = \"black\",alpha=0.5) +\n  geom_node_point(aes(color = vertex_attr(gr09)$name), size = 2) +\n  geom_node_text(aes(label = vertex_attr(gr09)$name), color=\"black\",size=2) +\n  theme_void() +\n  guides(color=guide_legend(keyheight=0.1,keywidth=0.1,\n      title=\"Countries\"))__\n\nFigure 10.22: The minimum spanning tree computed from DNA distances between HIV sequences from samples taken in 2009 and whose country of origin was known, data as published in the HIVdb database (Rhee et al. 2003).\n__\nQuestion 10.16\nMake the network plot again, but replace geom_node_text with labels that repel each other to minimize the overlapping node labels.\n__\nSolution\n__\nSee Figure 10.23. Maybe a better, or additional approach would be to first cluster those vertices that are very close together and from the same country.\nlibrary(\"ggraph\")\nggraph(gr09, layout=\"fr\") +\n  geom_edge_link(color = \"black\",alpha=0.5) +\n  geom_node_point(aes(color = vertex_attr(gr09)$name), size = 2) +\n  geom_node_label(aes(label = vertex_attr(gr09)$name), color=\"black\",size=2,repel=TRUE) +\n  theme_void() +\n  guides(color=guide_legend(keyheight=0.1,keywidth=0.1,\n      title=\"Countries\"))__\n\nFigure 10.23: Solution to Question 10.16.\nIt could be preferable to use a graph layout that incorporates the known geographic coordinates. Thus, we might be able to see how the virus jumped large distances across the world through traveller mobility. We introduce approximate country coordinates, which we then jitter slightly to reduce overlapping.\nlibrary(\"rworldmap\")\nmat = match(country09, countriesLow$NAME)\ncoords2009 = data.frame(\n  lat = countriesLow$LAT[mat],\n  lon = countriesLow$LON[mat],\n  country = country09)\nlayoutCoordinates = cbind(\n  x = jitter(coords2009$lon, amount = 15),\n  y = jitter(coords2009$lat, amount = 8))\nlabc = names(table(country09)[which(table(country09) &gt; 1)])\nmatc = match(labc, countriesLow$NAME)\ndfc = data.frame(\n  latc = countriesLow$LAT[matc],\n  lonc = countriesLow$LON[matc],\n  labc)\ndfctrans = dfc\ndfctrans[, 1] = (dfc[,1] + 31) / 93\ndfctrans[, 2] = (dfc[,2] + 105) / 238\nCountries = vertex_attr(gr09)$name \n\nggraph(gr09, layout=layoutCoordinates) +\n  geom_node_point(aes(color=Countries),size = 3, alpha=0.75) +\n  geom_edge_arc(color = \"black\", alpha = 0.5, strength=0.15) +\n  geom_label(data=dfc,aes(x=lonc,y=latc,label=labc,fill=labc),colour=\"white\",alpha=0.8,size=3,show.legend=F) +\n  theme_void()  __\n\nFigure 10.24: A minimum spanning tree between HIV cases. The geographic locations of the cases were jittered to reduce overlapping. The DNA sequence distances between the HIV strains were used as input to an undirected minimum spanning tree algorithm.\nThe input to the minimum spanning tree algorithm is a distance matrix or a graph with a length edge attribute. Figure 10.24 is the minimum spanning tree between cases of HIV, for which strain information was made available through the HIVdb database@HIVdb. The DNA distances were computed using the Jukes- Cantor mutation model.\n__\nQuestion 10.17\nThe above analysis provided an undirected network of connections, in fact several implementations of the minimum spanning tree (ie for instance mstree in ade4) provide a directed path through the points, which can provide meaningful information on the (apparent) spread of disases. Make a directed network version of the above maps.\nMST is a very useful component of a simple nonparametric test for detecting differences between factors that are mapped onto its vertices.\n\n12.5.1 10.6.1 MST based testing: the Friedman–Rafsky test\nGraph-based two-sample tests8 were introduced by Friedman and Rafsky (Friedman and Rafsky 1979) as a generalization of the Wald-Wolfowitz runs test (see Figure 10.25). Our previous examples show graph vertices associated with covariates such as country of origin. Here we test whether the covariate is significantly associated to the graph structure.\n8 Tests that explore whether two samples are drawn from the same distribution.\nThe Friedman-Rafsky tests for two/multiple sample segregation on a minimum spanning tree. It was conceived as a generalization of the univariate Wald- Wolfowitz runs test. If we are comparing two samples, say men and women, whose coordinates represent a measurement of interest. We color the two groups blue and red as in Figure 10.25, the Wald-Wolfowitz test looks for long runs of the same color that would indicate that the two groups have different means.\n\nFigure 10.25: Seeing the number of runs in a one-dimensional, two-sample, nonparametric Wald-Wolfowitz test can indicate whether the two groups have the same distributions.\nInstead of looking for consecutive values of one type (‘runs’), we count the number of connected nodes of the same type.\nOnce the minimum spanning tree has been constructed, the vertices are assigned `colors’ according to the different levels of a categorical variable. We call pure edges those whose two nodes have the same level of the factor variable. We use \\(S_O\\), the number of pure edges as our test statistic. To evaluate whether our observed value could have occurred by chance when the groups have the same distributions, we permute the vertix labels (colors) randomly and recount how many pure edges there are. This label swapping is repeated many times, creating our null distribution for \\(S\\).\n\n\n12.5.2 10.6.2 Example: Bacteria sharing between mice\nHere we illustrate the idea on a collection of samples from mice whose stool were analyzed for their microbial content. We read in a data set with many mice and many taxa, we compute the Jaccard distance and then use the mst function from the igraph package. We annotate the graph with the relevant covariates as shown in the code below:\nps1  = readRDS(file.path(DATA,\"ps1.rds\"))\nsampledata = data.frame( sample_data(ps1))\nd1 = as.matrix(phyloseq::distance(ps1, method=\"jaccard\"))\ngr = graph_from_adjacency_matrix(d1,  mode = \"undirected\", weighted = TRUE)\nnet = igraph::mst(gr)\nV(net)$id = sampledata[names(V(net)), \"host_subject_id\"]\nV(net)$litter = sampledata[names(V(net)), \"family_relationship\"]__\nWe make a ggraph object from the resulting igraph generated minimum spanning tree and then plot it, as shown in Figure 10.26.\nggraph(net, layout=\"fr\")+\n  geom_edge_arc(color = \"darkgray\") +\n  geom_node_point(aes(color = id, shape = litter)) + \n  theme(legend.position=\"bottom\")__\n\nFigure 10.26: The minimum spanning tree based on Jaccard dissimilarity and annotated with the mice ID and litter factors\nNow we compute the null distribution and p-value for the test, this is implemented in the phyloseqGraphTest package:\nlibrary(\"phyloseqGraphTest\")\ngt = graph_perm_test(ps1, \"host_subject_id\", distance=\"jaccard\",\n                     type=\"mst\",  nperm=1000)\ngt$pval __\n\n\n[1] 0.000999\nWe can take a look at the complete histogram of the null distribution generatedby permutation using:\nplot_permutations(gt)__\n\nFigure 10.27: The permutation histogram of the number of pure edges in the network obtained from the minimal spanning tree with Jaccard similarity.\n\n12.5.2.1 Different choices for the skeleton graph\nIt is not necessary to use an MST for the skeleton graph that defines the edges. Graphs made by linking nearest neighbors (Schilling 1986) or distance thresholding work as well.\nThe Bioconductor package phyloseq has functionality for creating graphs based on thresholding a distance matrix through the function make_network. We create a network by creating an edge between samples whose Jaccard dissimilarity is less than a threshold, which we set below via the parameter max.dist. We can also use the ggraph package to add attributes to the vertices indicating which mouse the sample came from and which litter the mouse was in. We see that in the resulting network, shown in Figure 10.28, there is grouping of the samples by both mouse and litter.\nnet = make_network(ps1, max.dist = 0.35)\nsampledata = data.frame(sample_data(ps1))\nV(net)$id = sampledata[names(V(net)), \"host_subject_id\"]\nV(net)$litter = sampledata[names(V(net)), \"family_relationship\"]__\n\n\nggraph(net, layout=\"fr\") +\n  geom_edge_link(color = \"darkgray\") +\n  geom_node_point(aes(color = id, shape = litter)) + \n    theme(plot.margin = unit(c(0, 5, 2, 0), \"cm\"))+\n    theme(legend.position = c(1.4, 0.3),legend.background = element_blank(),\n          legend.margin=margin(0, 3, 0, 0, \"cm\"))+\n         guides(color=guide_legend(ncol=2))+\n  theme_graph(background = \"white\")__\n\nFigure 10.28: A co-occurrence network created by using a threshold on the Jaccard dissimilarity matrix. The colors represent which mouse the sample came from; the shape represents which litter the mouse was in.\nNote that no matter which graph we build between the samples, we can approximate a null distribution by permuting the labels of the nodes of the graph. However, sometimes it will preferable to adjust the permutation distribution to account for known structure between the covariates.\n\n\n\n12.5.3 10.6.3 Friedman–Rafsky test with nested covariates\nIn the test above, we took a rather naïve approach and showed there was a significant difference between individual mice (the host_subject_id variable). Here we perform a slightly different permutation test to find out if we control for the difference between mice; is there a litter (the family_relationship variable) effect? The setup of the test is similar, it is simply how the permutations are generated which differs. We maintain the nested structure of the two factors using the grouping argument. We permute the family_relationship labels but keep the host_subject_id structure intact.\ngt = graph_perm_test(ps1, \"family_relationship\",\n        grouping = \"host_subject_id\",\n        distance = \"jaccard\", type = \"mst\", nperm= 1000)\ngt$pval __\n\n\n[1] 0.002\nThis test has a small p-value, and we reject the null hypothesis that the two samples come from the same distribution. From the plot of the minimum spanning tree in Figure 10.27, we see by eye that the samples group by litter more than we would expect by chance.\nplot_permutations(gt)__\n\nFigure 10.29: The permutation histogram obtained from the minimal spanning tree with Jaccard similarity.\n__\nQuestion 10.18\nThe \\(k\\)-nearest neighbors graph is obtained by putting an edge between two samples whenever one of them is in the set of \\(k\\)-nearest neighbors of the other. Redo the test, defining the graph using nearest neighbors defined with the Jaccard distance. What would you conclude?\n__\nSolution\n__\ngtnn1 = graph_perm_test(ps1, \"family_relationship\",\n                      grouping = \"host_subject_id\",\n                      distance = \"jaccard\", type = \"knn\", knn = 1)\ngtnn1$pval __\n\n\n[1] 0.004\nFigure 10.30 shows that pairs of samples having edges between them in this nearest neighbor graph are much more likely to be from the same litter.\nplot_test_network(gtnn1)__\n\nFigure 10.30: The graph obtained from a nearest-neighbor graph with Jaccard similarity.\nNote: The dual graph\nIn the examples above we sought to show relationships between samples through their shared taxa. It can also be of interest to ask the question about taxa: do some of the taxa co-occur more often than one would expect? This approach can help study microbial `communities’ as they assemble in the microbiome. The methods we developed above all apply to this use-case, all one really does is transpose the data. It is always preferable with sparse data such as the microbiome to use Jaccard and not build correlation networks that can be appropriate in other settings.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "10-chap.html#summary-of-this-chapter",
    "href": "10-chap.html#summary-of-this-chapter",
    "title": "12  10.1 이 장의 목표",
    "section": "12.6 10.7 Summary of this chapter",
    "text": "12.6 10.7 Summary of this chapter\n\n12.6.0.1 Annotated graphs\nIn this chapter we have learnt how to store and plot data that have more structure than simple arrays: graphs have edges and nodes that can also be associated to extra annotations that can be displayed usefully.\n\n\n12.6.0.2 Important examples of graphs and useful R packages\nWe started by specific examples such as Markov chain graphs, phylogenetic trees and minimum spanning trees. We saw how to use the ggraph and igraph packages to visualize graphs and show as much information as possible by using specific graph layout algorithms.\n\n\n12.6.0.3 Combining graphs with statistical data\nWe then approached the problem of incorporating a known `skeleton’ graph into differential expression analyses. This enables use to pinpoint perturbation hotspots in a network. We saw how evolutionary models defined along rooted binary trees serve as the basis for phylogenetic tree estimation and how we can incorporate these trees as supplementary information in a differential abundance analysis using the R packages structSSI and phyloseq.\n\n\n12.6.0.4 Linking co-occurrence to other variables\nGraph and network tools also enable the creation of networks from co- occurrence data and can be used to visualize and test the effect of factor covariates. We saw the Friedman-Rafsky test which provides an easy way of testing dependencies of a variable with the edge structure of a skeleton graph.\n\n\n12.6.0.5 Context and intepretation aids\nThis chapter illustrated ways of incorporating interactions of players in a network and we saw how useful it was to combine this with statistical scores. This often provides biological insight into analyses of complex biological systems.\n\n\n12.6.0.6 Previous knowledge or outcome\nWe saw that graphs can be both useful to encode our previous knowledge, metabolic network information, gene ontologies and phylogenetic trees of known bacteria are all available in standard databases. It is beneficial in a study to incorprate all known information and doing so by combining these skeleton networks with observed data enhances our understanding of experimental results in the context of what is already known.\nOn the other hand, the graph can be the outcome that we want to predict and we saw how to build graphs from data (phylogenetic trees, co-occurrence networks and minimum spanning trees).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "10-chap.html#further-reading",
    "href": "10-chap.html#further-reading",
    "title": "12  10.1 이 장의 목표",
    "section": "12.7 10.8 Further reading",
    "text": "12.7 10.8 Further reading\nFor complete developments and many important consequences of the evolutionary models used in phylogenetic trees, see the books by Li (1997; Li and Graur 1991). The book by Felsenstein (2004) is the classic text on estimating phylogenetic trees.\nThe book written by the author of the ape packages, Paradis (2011) contains many use-cases and details about manipulation of trees in R. A review of bootstrapping for phylogenetic trees can be found in Holmes (2003a).\nWe can use a tree as well as abundances in a contingency table data through an extension of PCoA-MDS called DPCoA (Double principal coordinate analysis). For microbiome data, the phylogenetic tree provides distances between taxa; these distances serve as the basis for the first PCoA. A second PCoA enables the projection of the weighted sample points. This has proved very effective in microbial ecology applications, see Purdom (2010) or Fukuyama et al. (2012) for details.\nGraphs can be used to predict vertex covariates. There is a large field of applied statistics and machine learning that considers the edges in the graph as a response variable for which one can make predictions based on covariates or partial knowledge of the graph; these include ERGM ’s (Exponential Random Graph Models, Robins et al. (2007)) and kernel methods for graphs (Schölkopf, Tsuda, and Vert 2004).\nFor theoretical properties of the Friedman-Rafsky test and more examples see Bhattacharya (2015).\nA full list packages that deal with graphs and networks is available at: http://www.bioconductor.org/packages/release/BiocViews.html#___GraphAndNetwork.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "10-chap.html#exercises",
    "href": "10-chap.html#exercises",
    "title": "12  10.1 이 장의 목표",
    "section": "12.8 10.9 Exercises",
    "text": "12.8 10.9 Exercises\n__\nExercise 10.1\nCreate a function that plots a graph starting from an adjacency matrix. Show how it works on an example.\n__\nSolution\n__\nggplotadjacency = function(a) {\n  n = nrow(a)\n  p = ncol(a)\n  fromto  = reshape2::melt(a)\n  stopifnot(identical(nrow(fromto), n*p))\n  fromto$value = as.factor(fromto$value)\n  cols = c(\"white\", \"darkblue\")\n  ggplot(data = fromto, aes(x = Var1, y = Var2, fill = value)) +\n    geom_tile(colour = \"black\") +\n    coord_fixed(ratio = 1, ylim = c(0.5, n + 0.5), xlim = c(0.5, p + 0.5)) +\n    scale_fill_manual(values = cols) +\n    scale_x_continuous(name = \"\" , breaks = 1:p, labels = paste(1:p)) +\n    scale_y_reverse(  name = \"\" , breaks = n:1, labels = paste(n:1)) + \n    theme_bw() +\n    theme(axis.text = element_text(size = 14),\n      legend.key = element_rect(fill = \"white\"),\n      legend.background = element_rect(fill = \"white\"),\n      panel.border = element_blank(),\n      panel.grid.major = element_blank(),\n      panel.grid.minor = element_blank(),\n      axis.line = element_line(color = \"white\"),\n      axis.ticks.x = element_blank(),\n      axis.ticks.y = element_blank() \n    )\n}__\n__\nExercise 10.2\nThe relationships between gene functions is organized hierarchically into a graph called the Gene Ontology (GO) graph. The biological processes are organized at finer and finer scale. Take one of the databases providing the GO information for the organisms you are interested in. Choose a gene list and build the GO graph for that list.\nHint: Some examples can be found in the packages , , .\n__\nExercise 10.3\nMarkov chain graph of transitions between states of the vaginal microbiota : In DiGiulio et al. (2015) the authors use an igraph plot to represent the transitions rates between community state types CSTs using the markovchain package. Load the data and the transition rates and state names into an object of the special class markovchain and tailor the layout carefully to include the percentage of preterm birth as a covariate for the vertices (make the vertex size proportional to this variable). Include the size of transitions between states as the width of the arrows.\n__\nSolution\n__\nNote: the below code is not live, a version of it used to run for the authors at one point, here it is given as a starting point for the reader to finish, it has opportunities for modernization and improvement.\n\nFigure 10.31: This figure was originally created for the study done in DiGiulio et al. (2015), where the nodes designate states of the vaginal microbiome and arrows represent transitions between states with different probabilities.\nlibrary(\"markovchain\")\n# Make Markov chain object\nmcPreg  =  new(\"markovchain\", states = CSTs,\n              transitionMatrix = trans, name=\"PregCST\")\nmcPreg\n# Set up igraph of the markov chain\nnetMC  =  markovchain:::.getNet(mcPreg, round = TRUE)__\nNow define a number of plotting parameters, and assign node colors based on the association of that CST and preterm outcome.\nwts  =  E(netMC)$weight/100\nedgel  =  get.edgelist(netMC)\nelcat  =  paste(edgel[,1], edgel[,2])\nelrev  =  paste(edgel[,2], edgel[,1])\nedge.curved  =  sapply(elcat, function(x) x %in% elrev)\nsamples_def  =  data.frame(sample_data(ps))\nsamples_def  =  samples_def[samples$Preterm | samples$Term,] # Only those definitely assigned, i.e. not marginal\npremat  =  table(samples_def$CST, samples_def$Preterm)\nrownames(premat)  =  markovchain::states(mcPreg)\ncolnames(premat)  =  c(\"Term\", \"Preterm\")\npremat\npremat  =  premat/rowSums(premat)\nvert.CSTclrs  =  CSTColors __\n\n\ndefault.par  =  par(no.readonly = TRUE)\n# Define color scale\n# Plotting function for markov chain\nplotMC  =  function(object, ...) {\n    netMC  =  markovchain:::.getNet(object, round = TRUE)\n    plot.igraph(x = netMC, ...)\n}\n# Color bar for the markov chain visualization, gradient in strength of preterm association\ncolor.bar  =  function(lut, min, max=-min, nticks=11, ticks=seq(min, max, len=nticks), title=NULL) {\n    scale = (length(lut)-1)/(max-min)\n    cur.par = par(no.readonly = TRUE)\n    par(mar = c(0, 4, 1, 4) + 0.1, oma = c(0, 0, 0, 0) + 0.1)\n    par(ps = 10, cex = 0.8)\n    par(tcl=-0.2, cex.axis=0.8, cex.lab = 0.8)\n    plot(c(min,max), c(0,10), type='n', bty='n', xaxt='n', xlab=\", yaxt='n', ylab=\", main=title)\n    axis(1, c(0, 0.5, 1))\n    for (i in 1:(length(lut)-1)) {\n      x = (i-1)/scale + min\n      rect(x,0,x+1/scale,10, col=lut[i], border=NA)\n    }\n}\n\npal  =  colorRampPalette(c(\"grey50\", \"maroon\", \"magenta2\"))(101)\nvert.clrs  =  sapply(states(mcPreg), function(x) pal[1+round(100*premat[x,\"Preterm\"])])\nvert.sz  =  4 + 2*sapply(states(mcPreg),\n              function(x) nrow(unique(sample_data(ps)[sample_data(ps)$CST==x,\"SubjectID\"])))\nvert.sz  =  vert.sz * 0.85\nvert.font.clrs  =  c(\"white\", \"white\", \"white\", \"white\", \"white\")\n\n# E(netMC) to see edge list, have to define loop angles individually by the # in edge list, not vertex\nedge.loop.angle = c(0, 0, 0, 0, 3.14, 3.14, 0, 0, 0, 0, 3.14, 0, 0, 0, 0, 0)-0.45\nlayout  =  matrix(c(0.6,0.95, 0.43,1, 0.3,0.66, 0.55,0.3, 0.75,0.65), nrow = 5, ncol = 2, byrow = TRUE)\n\n# Color by association with preterm birth\nlayout(matrix(c(1,1,2,2), 2, 2, byrow = TRUE), heights=c(1,10))\ncolor.bar(pal, min=0, max=1, nticks=6, title=\"Fraction preterm\")\npar(mar=c(0,1,1,1)+0.1)\nedge.arrow.size=0.8\nedge.arrow.width=1.4\nedge.width = (15*wts + 0.1)*0.6\nedge.labels  =  as.character(E(netMC)$weight/100)\nedge.labels[edge.labels&lt;0.4]  =  NA  # labels only for self-loops\nplotMC(mcPreg, edge.arrow.size=edge.arrow.size, edge.arrow.width = edge.arrow.width,\n       edge.width=edge.width, edge.curved=edge.curved,\n       vertex.color=vert.clrs, vertex.size=(vert.sz),\n       vertex.label.font = 2, vertex.label.cex = 1,\n       vertex.label.color = vert.font.clrs, vertex.frame.color = NA,\n       layout=layout, edge.loop.angle = edge.loop.angle)\npar(default.par)__\n__\nExercise 10.4\nProtein interaction networks : Read the Wikipedia article about the STRING database (http://www.string- db.org).\nThe protein Cyclin B1 is encoded by the CCNB1 gene. You can read about it on wikipedia here: https://en.wikipedia.org/wiki/Cyclin_B1.\nUse STRING to generate a text file (call it ccnb1datsmall.txt) of edges around the CCNB1 gene. Choose nodes that are connected by evidence of co- expression with a confidence higher than 0.9. Collect no more than 50 interactions and additional nodes that are two steps away from CCNB1 in the graph.\n__\nSolution\n__\n\nGo to http://www.string-db.org.\nEnter CCNB1 as the protein name and Homo sapiens as the organism. Click “Continue!”\nSelect the option with protein CCNB1 (the top one).\nScroll down to “info and Parameters …” 4a. For Active Prediction Methods – unselect everything except “Co-Expression” 4b. For required confidence – select “highest confidence (0.900) 4c. For interactors shown – select”no more than 50 interactors” 4d. For additional (white) nodes – select “100” (these are nodes two steps away from CCNB1) 4e. Click “Update Parameters”. You should get something that looks like the image below.\nClick “save” under the picture (showing a diskette). This will open up a new window so you can choose which format to save the data.\nScroll down to the “Text Summary (TXT - simple tab delimited flatfile)” file and save that document as ccnb1datsmall.txt.\n\n__\nExercise 10.5\nRead the txt file ccnb1datsmall.txt into R and make a plot of the graph using one of the graph visualization methods covered in this chapter.\n__\nSolution\n__\n\nFigure 10.32: This network was created with the STRING website by setting a 2 step neighborhood around the CCNB1 gene for co-expression levels \\(\\) 0.900.\ndat = read.table(file.path(DATA,\"ccnb1datsmall.txt\"), header = TRUE, comment.char = \"\", stringsAsFactors = TRUE)\nv = levels(unlist(dat[,1:2]))        # vertex names\nn = length(v)                        # number of vertices\ne = matrix(match(as.character(unlist(dat[,1:2])), v),ncol=2) # edge list\nw = dat$coexpression                 # edge weights __\nM is our co-expression network adjacency matrix. Since the STRING data only says if proteins i and j are co-expressed and doesn’t distinguish between (i,j) and (j,i) we want to make M symmetric (undirected) by considering the weight on (i,j) is the same as from (j,i). A is our co-expression graph adjacency matrix and we make \\(A_{ij} = 1\\) if they are coexpressed.\nM = matrix(0, n, n)\nM[e] = w\nM = M + t(M)\ndimnames(M) = list(v, v)\nA = 1*(M &gt; 0)__\nWe use default plotting parameters and generate the graph using the package igraph starting with e, the vector of edges (an alternative is to use the adjacency matrix A).\nNote: We use a seed to make the graph always look the same. Graph layout often contains an optimization with a random component that makes the picture look different, although the graph itself is the same.\nlibrary(igraph)\nnet = network(e, directed=FALSE)\npar(mar=rep(0,4))\nplot(net, label=v)__\nYou could make a graph with ggraph.\n__\nExercise 10.6\nMake a heatmap showing the adjacency matrix of the graph created in Exercise 10.5.\n__\nSolution\n__\nWe use defaults in making a heatmap except for changing the colors, you can experiment and add additional parameters.\nbreaks  =  c(0, seq(0.9, 1, length=11))\ncols  =  grey(1-c(0,seq(0.5,1,length=10)))\nccnb1ind  =  which(v == \"CCNB1\")\nvcols  =  rep(\"white\",n)\nvcols[ccnb1ind]  =  \"blue\"\nvcols[which(M[,ccnb1ind]&gt;0 | M[ccnb1ind,])]  =  \"red\"\npar(mar = rep(0, 4))\nheatmap(M, symm = TRUE, ColSideColors = vcols, RowSideColors = vcols,\n        col = cols, breaks = breaks,  frame = TRUE)\nlegend(\"topleft\", c(\"Neighbors(CCNB1)\", \"CCNB1\"),\n       fill = c(\"red\",\"blue\"),\n       bty = \"n\", inset = 0, xpd = TRUE,  border = FALSE)__\n\nFigure 10.33: This represents the adjacency of the CCNB1 network – 2 step neighborhood with co-expression levels \\(\\) 0.900, generated from R (darker is closer to 1, we ignore values &lt; 0.9).\n__\nExercise 10.7\nThe visualization shows the strongest interactions in the two step neighborhood of CCNB1. Both the plotted graph and the heatmap image show the same data: there seems to be a cluster of proteins which are all similar to CCNB1 and there is also another cluster in the other proteins. Many of the proteins in the CCNB1 cluster are coexpressed at the same time as each other.\nWhy might this be the case?\nConversely, proteins which are coexpressed with a protein that is coexpressed with CCNB1 (two steps away) do not tend to be coexpressed with each other.\nIs it easier for you to see this in one of the figures (the plot or the heatmap) than the other?\n__\nExercise 10.8\nCompare the use of ape and phangorn in the analysis of HIV GAG data. Compute the Jukes Cantor distances between the sequences using both packages and compare them to the Hamming distances.\nlibrary(\"ape\")\nlibrary(\"phangorn\")\nGAG = read.dna(file.path(DATA, \"DNA_GAG_20.txt\"))__\n__\nExercise 10.9\nPerform the Friedman–Rafksy type test with a “two-nearest” neighbor-graph using the Bray-Curtis dissimilarity.\n__\nSolution\n__\ngt = graph_perm_test(ps1, \"family_relationship\", distance = \"bray\", \n                     grouping = \"host_subject_id\", type = \"knn\", knn = 2)\ngt$pval __\n\n\n[1] 0.004\n\n\nplot_test_network(gt)\npermdf = data.frame(perm=gt$perm)\nobs = gt$observed\nymax = max(gt$perm)\nggplot(permdf, aes(x = perm)) + geom_histogram(bins = 20) +\n  geom_segment(aes(x = obs, y = 0, xend = obs, yend = ymax/10), color = \"red\") +\n  geom_point(aes(x = obs, y = ymax/10), color = \"red\") + xlab(\"Number of pure edges\")__\n\n\n\\(\\)\n\n\n\n\\(\\)\n\nFigure 10.34: The graph (a) and permutation histogram (b) obtained from a two nearest-neighbor graph with Jaccard similarity.\nBeisser, Daniela, Gunnar W Klau, Thomas Dandekar, Tobias Müller, and Marcus T Dittrich. 2010. “BioNet: An R-Package for the Functional Analysis of Biological Networks.” Bioinformatics 26 (8): 1129–30.\nBenjamini, Yoav, and Marina Bogomolov. 2014. “Selective Inference on Multiple Families of Hypotheses.” Journal of the Royal Statistical Society: Series B 76 (1): 297–318.\nBenjamini, Yoav, and Daniel Yekutieli. 2003. “Hierarchical FDR Testing of Trees of Hypotheses.” Technical report, Department of Statistics; Operations Research, Tel Aviv University.\nBhattacharya, Bhaswar B. 2015. “Power of Graph-Based Two-Sample Tests.” arXiv Preprint arXiv:1508.07530.\nBouckaert, Remco, Joseph Heled, Denise Kühnert, Tim Vaughan, Chieh-Hsi Wu, Dong Xie, Marc A Suchard, Andrew Rambaut, and Alexei J Drummond. 2014. “BEAST 2: A Software Platform for Bayesian Evolutionary Analysis.” PLoS Computational Biology 10 (4): e1003537.\nCallahan, Benjamin J, Paul J McMurdie, and Susan P Holmes. 2017. “Exact Sequence Variants Should Replace Operational Taxonomic Units in Marker Gene Data Analysis.” ISME Journal , 1–5.\nChakerian, John, and Susan Holmes. 2012. “Computational Tools for Evaluating Phylogenetic and Hierarchical Clustering Trees.” Journal of Computational and Graphical Statistics 21 (3): 581–99.\nCole, J. R., Q. Wang, E. Cardenas, J. Fish, B. Chai, R. J. Farris, A. S. Kulam-Syed-Mohideen, et al. 2009. “The Ribosomal Database Project: Improved Alignments and New Tools for rRNA Analysis.” Nucleic Acids Research 37 (Supplement 1): D141–45.\nDiGiulio, Daniel B., Benjamin J. Callahan, Paul J. McMurdie, Elizabeth K. Costello, Deirdre J. Lyelle, Anna Robaczewska, Christine L. Sun, et al. 2015. “Temporal and Spatial Variation of the Human Microbiota During Pregnancy.” PNAS.\nFelsenstein, Joseph. 2004. Inferring Phylogenies. Boston: Sinauer.\nFriedman, Jerome H, and Lawrence C Rafsky. 1979. “Multivariate Generalizations of the Wald-Wolfowitz and Smirnov Two-Sample Tests.” The Annals of Statistics , 697–717.\nFukuyama, Julia, Paul J McMurdie, Les Dethlefsen, David A Relman, and Susan Holmes. 2012. “Comparisons of Distance Methods for Combining Covariates and Abundances in Microbiome Studies.” In Pac Symp Biocomput. World Scientific.\nHolmes, Susan. 1999. “Phylogenetic Trees: An Overview.” In Statistics and Genetics , 81–118. IMA 112. New York: Springer.\n———. 2003a. “Bootstrapping Phylogenetic Trees: Theory and Methods.” Statistical Science 18 (2): 241–55.\n———. 2003b. “Statistics for phylogenetic trees.” Theoretical Population Biology 63 (1): 17–32.\nIdeker, Trey, Owen Ozier, Benno Schwikowski, and Andrew F Siegel. 2002. “Discovering Regulatory and Signalling Circuits in Molecular Interaction Networks.” Bioinformatics 18 Suppl 1 (January): S233–40. http://bioinformatics.oxfordjournals.org/cgi/reprint/18/suppl\\_1/S233.\nLi, Wen-Hsiung. 1997. Molecular Evolution. Sinauer Associates Incorporated.\nLi, Wen-Hsiung, and Dan Graur. 1991. Fundamentals of Molecular Evolution. Vol. 48. Sinauer Associates Sunderland, MA.\nLiberzon, Arthur, Aravind Subramanian, Reid Pinchback, Helga Thorvaldsdóttir, Pablo Tamayo, and Jill P Mesirov. 2011. “Molecular Signatures Database (MSigDB) 3.0.” Bioinformatics 27 (12): 1739–40.\nLove, Michael I, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation of Fold Change and Dispersion for RNA-seq Data with DESeq2.” Gnome Biology 15 (12): 1–21.\nMcMurdie, Paul J, and Susan Holmes. 2014. “Waste Not, Want Not: Why Rarefying Microbiome Data Is Inadmissible.” PLoS Computational Biology 10 (4): e1003531.\n———. 2015. “Shiny-Phyloseq: Web Application for Interactive Microbiome Analysis with Provenance Tracking.” Bioinformatics 31 (2): 282–83.\nMossel, Elchanan. 2003. “On the Impossibility of Reconstructing Ancestral Data and Phylogenies.” Journal of Computational Biology 10 (5): 669–76.\nNacu, Serban, Rebecca Critchley-Thorne, Peter Lee, and Susan Holmes. 2007. “Gene Expression Network Analysis and Applications to Immunology.” Bioinformatics 23 (7, 7): 850–58. https://doi.org/10.1093/bioinformatics/btm019.\nNolan, Daniel J, Michael Ginsberg, Edo Israely, Brisa Palikuqi, Michael G Poulos, Daylon James, Bi-Sen Ding, et al. 2013. “Molecular Signatures of Tissue-Specific Microvascular Endothelial Cell Heterogeneity in Organ Maintenance and Regeneration.” Developmental Cell 26 (2): 204–19.\nParadis, Emmanuel. 2011. Analysis of Phylogenetics and Evolution with r. Springer Science & Business Media.\nPounds, Stan, and Stephan W Morris. 2003. “Estimating the Occurrence of False Positives and False Negatives in Microarray Studies by Approximating and Partitioning the Empirical Distribution of p-Values.” Bioinformatics 19 (10): 1236–42.\nPurdom, Elizabeth. 2010. “Analysis of a Data Matrix and a Graph: Metagenomic Data and the Phylogenetic Tree.” Annals of Applied Statistics , July.\nRhee, Soo-Yon, Matthew J Gonzales, Rami Kantor, Bradley J Betts, Jaideep Ravela, and Robert W Shafer. 2003. “Human Immunodeficiency Virus Reverse Transcriptase and Protease Sequence Database.” Nucleic Acids Research 31 (1): 298–303.\nRobins, Garry, Tom Snijders, Peng Wang, Mark Handcock, and Philippa Pattison. 2007. “Recent Developments in Exponential Random Graph (p*) Models for Social Networks.” Social Networks 29 (2): 192–215.\nRonquist, Fredrik, Maxim Teslenko, Paul van der Mark, Daniel L Ayres, Aaron Darling, Sebastian Höhna, Bret Larget, Liang Liu, Marc A Suchard, and John P Huelsenbeck. 2012. “MrBayes 3.2: Efficient Bayesian Phylogenetic Inference and Model Choice Across a Large Model Space.” Systematic Biology 61 (3): 539–42.\nSankaran, Kris, and Susan Holmes. 2014. “structSSI: Simultaneous and Selective Inference for Grouped or Hierarchically Structured Data.” Journal of Statistical Software 59 (1): 1–21.\nSchilling, Mark F. 1986. “Multivariate Two-Sample Tests Based on Nearest Neighbors.” Journal of the American Statistical Association 81 (395): 799–806.\nSchölkopf, Bernhard, Koji Tsuda, and Jean-Philippe Vert. 2004. Kernel Methods in Computational Biology. MIT press.\nWang, Q., G. M. Garrity, J. M. Tiedje, and J. R. Cole. 2007. “Naive Bayesian Classifier for Rapid Assignment of rRNA Sequences into the New Bacterial Taxonomy.” Applied and Environmental Microbiology 73 (16): 5261.\nWertheim, Joel O, and Michael Worobey. 2009. “Dating the Age of the SIV Lineages That Gave Rise to HIV-1 and HIV-2.” PLoS Computational Biology 5 (5): e1000377.\nWright, Erik S. 2015. “DECIPHER: Harnessing Local Sequence Context to Improve Protein Multiple Sequence Alignment.” BMC Bioinformatics 16 (1): 1.\nYu, Hongxiang, Diana L Simons, Ilana Segall, Valeria Carcamo-Cavazos, Erich J Schwartz, Ning Yan, Neta S Zuckerman, et al. 2012. “PRC2/EED-EZH2 Complex Is up-Regulated in Breast Cancer Lymph Node Metastasis Compared to Primary Tumor and Correlates with Tumor Proliferation in Situ.” PloS One 7 (12): e51239.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>10.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html",
    "href": "11-chap.html",
    "title": "13  11.1 이 장의 목표",
    "section": "",
    "text": "13.1 11.2 이미지 로드\n이미지는 풍부한 데이터 소스입니다. 이 장에서 우리는 이미지에서 어떻게 정량적 정보를 추출할 수 있는지, 그리고 데이터를 요약하고 이해하기 위해 통계적 방법을 어떻게 사용할 수 있는지 살펴볼 것입니다. 이 장의 목표는 이미지 데이터로 작업을 시작하는 것이 쉽다는 것을 보여주는 것입니다. 기본적인 R 환경을 다룰 수 있다면 이미지 작업을 시작할 준비가 된 것입니다. 그렇긴 하지만, 이 장이 이미지 분석에 대한 일반적인 입문서는 아닙니다. 이 분야는 광범위하며 신호 처리, 정보 이론, 수학, 공학 및 컴퓨터 과학의 많은 영역을 다루고 있으며, 체계적인 개요를 제시하는 훌륭한 책들이 많이 있습니다.\n우리는 주로 일련의 2차원 이미지, 특히 세포 이미지를 공부할 것입니다. 세포의 위치와 모양을 식별하는 방법과 식별된 모양 및 패턴의 특성(예: 크기, 강도, 색상 분포 및 상대적 위치)을 정량적으로 측정하는 방법을 배울 것입니다. 이러한 정보는 후속 분석에 사용될 수 있습니다. 예를 들어 서로 다른 조건(예: 서로 다른 약물의 효과 하에 있거나 분화 및 성장 단계가 다른 경우) 사이의 세포를 비교할 수 있습니다. 또는 이미지의 객체들이 서로 어떻게 연관되어 있는지 측정할 수 있습니다. 예를 들어 객체들이 함께 모이거나 서로 밀어내는 것을 선호하는지, 아니면 이웃한 객체들 사이에 특정 특성이 공유되는 경향이 있는지(세포 간 통신을 나타냄) 등을 측정할 수 있습니다. 유전학의 언어로 말하자면, 이미지를 복잡한 표현형이나 다변량 양적 형질로 사용할 수 있다는 뜻입니다.\n여기서 우리는 2차원 이상의 이미지 분석은 다루지 않을 것입니다. 3D 분할(segmentation) 및 등록(registration)이나 시간적 추적(temporal tracking)은 고려하지 않을 것입니다. 이것들은 정교한 작업이며, 이 장의 범위 내에서 우리가 조립할 수 있는 것보다 전문화된 소프트웨어가 아마도 더 나은 성능을 보일 것입니다.\n고처리량 이미징 데이터와 유전체학의 다른 고처리량 데이터 사이에는 유사점이 있습니다. 예를 들어 염색 효율의 변화, 조명 또는 기타 여러 요인으로 인해 배치 효과(batch effects)가 작용하는 경향이 있습니다. 실험 설계와 분석 선택에서 적절한 예방 조치를 취해야 합니다. 원칙적으로 이미지의 강도 값은 물리적 단위(예: 복사 에너지 또는 형광체 농도)로 보정될 수 있습니다. 하지만 생물학적 이미징 실무에서 이것이 항상 수행되는 것은 아니며, 아마도 필요하지 않을 수도 있습니다. 다소 달성하기 쉽고 분명히 가치 있는 것은 이미지의 공간적 차원에 대한 보정, 즉 픽셀 단위와 미터법 거리 사이의 변환 계수입니다.\n이 장에서 우리는 다음을 수행할 것입니다:\nR에서 이미지를 처리하는 데 유용한 툴킷은 바이오컨덕터 패키지인 EBImage (Pau et al. 2010)입니다. 기본 기능을 시연하기 위해 간단한 사진을 읽어 들이는 것으로 시작합니다.\nEBImage는 현재 세 가지 이미지 파일 형식인 jpeg, png, tiff를 지원합니다. 위에서 우리는 MSMB 패키지의 샘플 이미지를 로드했습니다. 자신의 데이터로 작업할 때는 그 패키지가 필요하지 않으며, readImage 함수에 파일 이름을 제공하기만 하면 됩니다. 이 장의 뒷부분에서 보게 되겠지만, readImage는 한 번에 여러 이미지를 읽을 수 있으며, 이들은 모두 단일 이미지 데이터 객체로 조립됩니다. 이것이 작동하려면 이미지들이 동일한 차원과 색상 모드를 가져야 합니다.\n__\n질문 11.1\nRBioFormats 패키지(GitHub에서 사용 가능: https://github.com/aoles/RBioFormats)는 더 많은 이미지 파일 형식을 읽고 쓰는 기능을 제공합니다. 얼마나 많은 서로 다른 파일 형식이 지원되나요?\n__\n해결책\n__\nRBioFormats 패키지의 read.image 함수 매뉴얼 페이지(이는 EBImage::readImage와는 다름에 유의)와 The Open Microscopy Environment 웹사이트의 Bio-Formats 프로젝트 온라인 문서(http://www.openmicroscopy.org/site/support/bio-formats5.5/supported-formats.html)를 참조하세요.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#이미지-로드",
    "href": "11-chap.html#이미지-로드",
    "title": "13  11.1 이 장의 목표",
    "section": "",
    "text": "library(\"EBImage\")\nimagefile = system.file(\"images\", \"mosquito.png\", package = \"MSMB\")\nmosq = readImage(imagefile)__",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#이미지-표시",
    "href": "11-chap.html#이미지-표시",
    "title": "13  11.1 이 장의 목표",
    "section": "13.2 11.3 이미지 표시",
    "text": "13.2 11.3 이미지 표시\n방금 읽어 들인 이미지를 시각화해 봅시다. 기본 함수는 EBImage::display입니다.\nEBImage::display(mosq)__\n위의 명령은 웹 브라우저 창(getOption(\"browser\")에 의해 설정됨)에서 이미지를 엽니다. 마우스나 키보드 단축키를 사용하여 이미지를 확대하거나 축소하고, 팬(pan) 이동을 하거나 여러 이미지 프레임을 순환하며 볼 수 있습니다.\n또는 method = \"raster\" 인수를 사용하여 display를 호출함으로써 R의 내장 플로팅 기능을 사용하여 이미지를 표시할 수도 있습니다. 그러면 이미지는 현재 장치로 출력됩니다. 이런 방식으로 이미지 데이터를 다른 플로팅 기능과 결합하여 예를 들어 텍스트 레이블을 추가할 수 있습니다.\nEBImage::display(mosq)\ntext(x = 85, y = 800, label = \"A mosquito\", adj = 0, col = \"orange\", cex = 1.5)__\n\n그림 11.1: 조지아주 디케이터 교외에서 사체로 발견된 모기 (출처: CDC / Janice Haney Carr).\n결과 플롯은 그림 11.1에 나와 있습니다. 평소와 같이 R 장치에 표시된 그래픽은 base R 함수인 dev.print나 dev.copy를 사용하여 저장할 수 있습니다.\n컬러 이미지를 읽고 볼 수도 있습니다(그림 11.2 참조).\nimagefile = system.file(\"images\", \"hiv.png\", package = \"MSMB\")\nhivc = readImage(imagefile)__\n\n\nEBImage::display(hivc, method = \"raster\")__\n\n그림 11.2: 배양된 림프구에서 싹이 트고 있는 HIV-1 비리온의 주사 전자 현미경 사진 (출처: CDC / C. Goldsmith, P. Feorino, E.L. Palmer, W.R. McManus).\n또한 이미지에 여러 프레임이 있는 경우, all = TRUE 인수를 지정하여 그리드 배열로 한 번에 모두 표시할 수 있습니다(그림 11.3).\nnuc = readImage(system.file(\"images\", \"nuclei.tif\", package = \"EBImage\"))\nEBImage::display(1 - nuc, all = TRUE)__\n\n그림 11.3: EBImage 패키지에 포함된 세포 핵 이미지 4개를 타일 형식으로 표시한 모습.\n또는 예를 들어 두 번째 프레임과 같이 단일 프레임만 볼 수도 있습니다.\nEBImage::display(1 - nuc, frame = 2)__\n__\n질문 11.2\n왜 그림 11.3의 코드에서 display 함수에 1 - nuc 인수를 전달했나요? nuc를 직접 표시하면 어떻게 보이나요?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#r에서-이미지는-어떻게-저장되나요",
    "href": "11-chap.html#r에서-이미지는-어떻게-저장되나요",
    "title": "13  11.1 이 장의 목표",
    "section": "13.3 11.4 R에서 이미지는 어떻게 저장되나요?",
    "text": "13.3 11.4 R에서 이미지는 어떻게 저장되나요?\n먼저 이미지 객체의 클래스를 식별하여 무슨 일이 일어나고 있는지 살펴봅시다.\nclass(mosq)__\n\n\n[1] \"Image\"\nattr(,\"package\")\n[1] \"EBImage\"\n이 객체가 Image 클래스를 가지고 있음을 알 수 있습니다. 이것은 기본 R 클래스 중 하나가 아니라 EBImage 패키지에 의해 정의된 것입니다. 도움말 브라우저를 통하거나 class?Image를 입력하여 이 클래스에 대해 더 자세히 알아볼 수 있습니다. 이 클래스는 기본 R 클래스인 array 에서 파생되었으므로, R 배열로 할 수 있는 모든 작업을 Image 객체로도 할 수 있습니다. 게다가 몇 가지 추가적인 기능과 동작들을 가지고 있습니다1.\n1 R의 용어로 추가적인 기능은 슬롯(slots)이라고 불리고 동작들은 메서드(methods)라고 불립니다. 메서드는 함수의 일종입니다.\n__\n질문 11.3\nImage 객체의 슬롯이 무엇인지, 그리고 어떤 메서드를 적용할 수 있는지 어떻게 알 수 있나요?\n__\n해결책\n__\n클래스 정의는 간단합니다. showClass(\"Image\")로 접근할 수 있습니다. R 함수 호출을 통해 Image 클래스에 적용 가능한 모든 메서드를 찾는 것은 번거로운 일입니다. 가장 좋은 방법은 클래스의 매뉴얼 페이지를 참조하여 저자가 언급하기로 선택한 메서드를 확인하는 것입니다.\n이미지의 차원은 dim 메서드를 사용하여 추출할 수 있습니다. for regular arrays.\ndim(mosq)__\n\n\n[1] 1400  952\nThe hist method has been redefined2 compared to the ordinary hist function for arrays: it uses different and possibly more useful defaults (Figure 11.4).\n2 In object oriented parlance, overloaded.\nhist(mosq)__\n\nFigure 11.4: Histogram of the pixel intensities in mosq. Note that the range is between 0 and 1.\nIf we want to directly access the data matrix as an R array , we can use the accessor function imageData.\nimageData(mosq)[1:3, 1:6]__\n\n\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n[1,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784\n[2,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784\n[3,] 0.1960784 0.1960784 0.2000000 0.2039216 0.2000000 0.1960784\nA useful summary of an Image object is printed if we simply type the object’s name.\nmosq __\n\n\nImage \n  colorMode    : Grayscale \n  storage.mode : double \n  dim          : 1400 952 \n  frames.total : 1 \n  frames.render: 1 \n\nimageData(object)[1:5,1:6]\n          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n[1,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784\n[2,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784\n[3,] 0.1960784 0.1960784 0.2000000 0.2039216 0.2000000 0.1960784\n[4,] 0.1960784 0.1960784 0.2039216 0.2078431 0.2000000 0.1960784\n[5,] 0.1960784 0.2000000 0.2117647 0.2156863 0.2000000 0.1921569\nNow let us look at the color image.\nhivc __\n\n\nImage \n  colorMode    : Color \n  storage.mode : double \n  dim          : 1400 930 3 \n  frames.total : 3 \n  frames.render: 1 \n\nimageData(object)[1:5,1:6,1]\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    0    0    0    0    0    0\n[2,]    0    0    0    0    0    0\n[3,]    0    0    0    0    0    0\n[4,]    0    0    0    0    0    0\n[5,]    0    0    0    0    0    0\nThe two images differ by their property colorMode, which is Grayscale for mosq and Color for hivc. What is the point of this property? It turns out to be convenient when we are dealing with stacks of images. If colorMode is Grayscale, then the third and all higher dimensions of the array are considered as separate image frames corresponding, for instance, to different \\(z\\)-positions, time points, replicates, etc. On the other hand, if colorMode is Color, then the third dimension is assumed to hold different color channels, and only the fourth and higher dimensions – if present – are used for multiple image frames. In hivc, there are three color channels, which correspond to the red, green and blue intensities of our photograph. However, this does not necessarily need to be the case, there can be any number of color channels.\n__\nQuestion 11.4\nDescribe how R stores the data nuc.\n__\nSolution\n__\nnuc __\n\n\nImage \n  colorMode    : Grayscale \n  storage.mode : double \n  dim          : 510 510 4 \n  frames.total : 4 \n  frames.render: 4 \n\nimageData(object)[1:5,1:6,1]\n           [,1]       [,2]       [,3]       [,4]       [,5]       [,6]\n[1,] 0.06274510 0.07450980 0.07058824 0.08235294 0.10588235 0.09803922\n[2,] 0.06274510 0.05882353 0.07843137 0.09019608 0.09019608 0.10588235\n[3,] 0.06666667 0.06666667 0.08235294 0.07843137 0.09411765 0.09411765\n[4,] 0.06666667 0.06666667 0.07058824 0.08627451 0.08627451 0.09803922\n[5,] 0.05882353 0.06666667 0.07058824 0.08235294 0.09411765 0.10588235\n\n\ndim(imageData(nuc))__\n\n\n[1] 510 510   4\nWe see that we have 4 frames in total, which correspond to the 4 separate images (frames.render).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#writing-images-to-file",
    "href": "11-chap.html#writing-images-to-file",
    "title": "13  11.1 이 장의 목표",
    "section": "13.4 11.5 Writing images to file",
    "text": "13.4 11.5 Writing images to file\nDirectly saving images to disk in the array representation that we saw in the previous section would lead to large file sizes – in most cases, needlessly large. It is common to use compression algorithms to reduce the storage consumption. There are two main types of image3 compression:\n3 In an analogous way, this is also true for movies and music.\n\nLossless compression: it is possible to exactly reconstruct the original image data from the compressed file. Simple priciples of lossless compression are: (i) do not spend more bits on representing a pixel than needed (e.g., the pixels in the mosq image have a range of 256 gray scale values, and this could be represented by 8 bits, although mosq stores them in a 64-bit numeric format4); and (2) identify patterns (such as those that you saw above in the printed pixel values for mosq and hivc) and represent them by much shorter to write down rules instead.\nLossy compression: additional savings are made compared to lossless compression by dropping details that a human viewer would be unlikely to notice anyway.\n\n4 While this is somewhat wasteful of memory, it is more compatible with the way the rest of R works, and is rarely a limiting factor on modern computer hardware.\n5 https://en.wikipedia.org/wiki/Portable_Network_Graphics\n6 https://en.wikipedia.org/wiki/JPEG\nAn example for a storage format with lossless compression is PNG5, an example for lossy compression is the JPEG6 format. While JPEG is good for your holiday pictures, it is good practice to store scientific images in a lossless format.\nWe read the image hivc from a file in PNG format, so let’s now write it out as a JPEG file. The lossiness is specified by the quality parameter, which can lie between 1 (worst) and 100 (best).\noutput_file = file.path(tempdir(), \"hivc.jpeg\")\nwriteImage(hivc, output_file, quality = 85)__\nSimilarly, we could have written the image as a TIFF file and chosen among several compression algorithms (see the manual page of the writeImage and writeTiff functions). The package RBioFormats lets you write to many further image file formats.\n__\nQuestion 11.5\nHow big is the hivc object in R’s memory? How big is the JPEG file? How much RAM would you expect a three color, 16 Megapixel image to occupy?\n__\nSolution\n__\nobject.size(hivc) |&gt; format(units = \"Mb\")__\n\n\n[1] \"29.8 Mb\"\n\n\n(object.size(hivc) / prod(dim(hivc))) |&gt; format() |&gt; paste(\"per pixel\")__\n\n\n[1] \"8 bytes per pixel\"\n\n\nfile.info( output_file )$size __\n\n\n[1] 294904\n\n\n16 * 3 * 8 __\n\n\n[1] 384",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#manipulating-images",
    "href": "11-chap.html#manipulating-images",
    "title": "13  11.1 이 장의 목표",
    "section": "13.5 11.6 Manipulating images",
    "text": "13.5 11.6 Manipulating images\nNow that we know that images are stored as arrays of numbers in R, our method of manipulating images becomes clear – simple algebra! For example, we can take our original image, shown again in Figure 11.5a, and flip the bright areas to dark and vice versa by multiplying the image with -1 Figure 11.5b).\nmosqinv = normalize(-mosq)__\n__\nQuestion 11.6\nWhat does the function normalize do?\nWe could also adjust the contrast through multiplication (Figure 11.5c) and the gamma-factor through exponentiation Figure 11.5d).\nmosqcont = mosq * 3\nmosqexp = mosq ^ (1/3)__\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.5: The original mosquito image (a) and three different image transformations: (b) subtraction, (c) multiplication, (d) power transformation.\nFurthermore, we can crop, threshold and transpose images with matrix operations (Figure 11.6).\nmosqcrop   = mosq[100:438, 112:550]\nmosqthresh = mosq &gt; 0.5\nmosqtransp = transpose(mosq)__\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.6: Three further image transformations: (a) cropping, (b) thresholding, (c) transposition.\n__\nQuestion 11.7\nWhat data type is mosqthresh, the result of the thresholding?\n__\nSolution\n__\nIt is an Image object whose pixels are binary values represented by an R array of type logical. You can inspect the object by typing its name into the console.\nmosqthresh __\n\n\nImage \n  colorMode    : Grayscale \n  storage.mode : logical \n  dim          : 1400 952 \n  frames.total : 1 \n  frames.render: 1 \n\nimageData(object)[1:5,1:6]\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n[1,] FALSE FALSE FALSE FALSE FALSE FALSE\n[2,] FALSE FALSE FALSE FALSE FALSE FALSE\n[3,] FALSE FALSE FALSE FALSE FALSE FALSE\n[4,] FALSE FALSE FALSE FALSE FALSE FALSE\n[5,] FALSE FALSE FALSE FALSE FALSE FALSE\n__\nQuestion 11.8\nInstead of the transpose function as above, could we also use R’s base function t?\n__\nSolution\n__\nIn this instance, the values of t(mosq) and transpose(mosq) happen to be the same, but transpose is preferable since it also works with color and multiframe images.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#spatial-transformations",
    "href": "11-chap.html#spatial-transformations",
    "title": "13  11.1 이 장의 목표",
    "section": "13.6 11.7 Spatial transformations",
    "text": "13.6 11.7 Spatial transformations\nWe just saw one type of spatial transformation, transposition, but there are many more—here are some examples:\nmosqrot   = EBImage::rotate(mosq, angle = 30)\nmosqshift = EBImage::translate(mosq, v = c(100, 170))\nmosqflip  = flip(mosq)\nmosqflop  = flop(mosq)__\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.7: Spatial transformations: (a) rotation, (b) translation, (c) reflection about the central horizontal axis (flip), (d) reflection about the central vertical axis (flop).\nIn the code above, the function rotate7 rotates the image clockwise with the given angle, translate moves the image by the specified two-dimensional vector (pixels that end up outside the image region are cropped, and pixels that enter into the image region are set to zero). The functions flip and flop reflect the image around the central horizontal and vertical axis, respectively. The results of these operations are shown in Figure 11.7.\n7 Here we call the function with its namespace qualifier EBImage:: to avoid confusion with a function of the same name in the namespace of the spatstat package, which we will attach later.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#linear-filters",
    "href": "11-chap.html#linear-filters",
    "title": "13  11.1 이 장의 목표",
    "section": "13.7 11.8 Linear filters",
    "text": "13.7 11.8 Linear filters\nLet’s now switch to an application in cell biology. We load images of human cancer cells that were studied by Laufer, Fischer and co-workers (Laufer et al. 2013). They are shown in Figure 11.8.\nimagefiles = system.file(\"images\", c(\"image-DAPI.tif\", \"image-FITC.tif\", \"image-Cy3.tif\"), package = \"MSMB\")\ncells = readImage(imagefiles)__\n\nFigure 11.8: Human colon cancer cells (HCT116). The four images show the same cells: the leftmost image corresponds to DAPI staining of the cells’ DNA, the second to immunostaining against alpha-tubulin, the third to actin. They are displayed as gray-scale images. The rightmost image is obtained by overlaying the three images as color channels of an RGB image (red: actin, green: alpha- tubulin, blue: DNA).\nThe Image object cells is a three-dimensional array of size 340 \\(\\) 490 \\(\\) 3, where the last dimension indicates that there are three individual grayscale frames. Our goal now is to computationally identify and quantitatively characterize the cells in these images. That by itself would be a modest goal, but note that the dataset of Laufer et al.contains over 690,000 images, each of which has 2,048 \\(\\) 2,048 pixels. Here, we are looking at three of these, out of which a small region was cropped. Once we know how to achieve our stated goal, we can apply our abilities to such large image collections, and that is no longer a modest aim!\n\n13.7.1 11.8.1 Interlude: the intensity scale of images\nHowever, before we can start with real work, we need to deal with a slightly mundane data conversion issue. This is, of course, not unusual. Let us inspect the dynamic range (the minimum and the maximum value) of the images.\napply(cells, 3, range)__\n\n\n      image-DAPI  image-FITC   image-Cy3\n[1,] 0.001586938 0.002899214 0.001663233\n[2,] 0.031204700 0.062485695 0.055710689\nWe see that the maximum values are small numbers well below 1. The reason for this is that the readImage function recognizes that the TIFF images uses 16 bit integers to represent each pixel, and it returns the data – as is common for numeric variables in R – in an array of double precision floating point numbers, with the integer values (whose theoretical range is from 0 to \\(2^{16}-1=65535\\)) stored in the mantissa of the floating point representation and the exponents chosen so that the theoretical range is mapped to the interval \\([0,1]\\). However, the scanner that was used to create these images only used the lower 11 or 12 bits, and this explains the small maximum values in the images. We can rescale these data to approximately cover the range \\([0,1]\\) as follows8.\n8 The function normalize provides a more flexible interface to the scaling of images.\ncells[,,1]   = 32 * cells[,,1]\ncells[,,2:3] = 16 * cells[,,2:3]\napply(cells, 3, range)__\n\n\n     image-DAPI image-FITC  image-Cy3\n[1,] 0.05078202 0.04638743 0.02661173\n[2,] 0.99855039 0.99977111 0.89137102\nWe can keep in mind that these multiplications with a multiple of 2 have no impact on the underlying precision of the stored data.\n\n\n13.7.2 11.8.2 Noise reduction by smoothing\nNow we are ready to get going with analyzing the images. As our first goal is segmentation of the images to identify the individual cells, we can start by removing local artifacts or noise from the images through smoothing. An intuitive approach is to define a window of a selected size around each pixel and average the values within that window. After applying this procedure to all pixels, the new, smoothed image is obtained. Mathematically, we can express this as\n\\[ f^*(x,y) = {s=-a}^{a}{t=-a}^{a} f(x+s, y+t), \\]\nwhere \\(f(x,y)\\) is the value of the pixel at position \\(x\\), \\(y\\), and \\(a\\) determines the window size, which is \\(2a+1\\) in each direction. \\(N=(2a+1)^2\\) is the number of pixels averaged over, and \\(f^*\\) is the new, smoothed image.\nMore generally, we can replace the moving average by a weighted average, using a weight function \\(w\\), which typically has highest weight at the window midpoint (\\(s=t=0\\)) and then decreases towards the edges.\n\\[ (w * f)(x,y) = {s=-}^{+} {t=-}^{+} w(s,t), f(x+s, y+s) \\]\nFor notational convenience, we let the summations range from \\(-\\) to \\(\\), even if in practice the sums are finite as \\(w\\) has only a finite number of non-zero values. In fact, we can think of the weight function \\(w\\) as another image, and this operation is also called the convolution of the images \\(f\\) and \\(w\\), indicated by the the symbol \\(*\\). In EBImage , the 2-dimensional convolution is implemented by the function filter2, and the auxiliary function makeBrush can be used to generate weight functions \\(w\\).\nw = makeBrush(size = 51, shape = \"gaussian\", sigma = 7)\nnucSmooth = filter2(getFrame(cells, 1), w)__\n\nFigure 11.9: nucSmooth, a smoothed version of the DNA channel in the image object cells (the original version is shown in the leftmost panel of Figure 11.8).\n__\nQuestion 11.9\nHow does the weight matrix w look like?\n__\nSolution\n__\nSee Figure 11.10\nlibrary(\"tibble\")\nlibrary(\"ggplot2\")\ntibble(w = w[(nrow(w)+1)/2, ]) |&gt;\n  ggplot(aes(y = w, x = seq(along = w))) + geom_point()__\n\nFigure 11.10: The middle row of the weight matrix, w[26, ].\nIn fact, the filter2 function does not directly perform the summation indicated in Equation 11.2. Instead, it uses the Fast Fourier Transformation in a way that is mathematically equivalent and computationally more efficient.\nThe convolution in Equation 11.2 is a linear operation, in the sense that \\(w(c_1f_1+c_2f_2)= c_1wf_1 + c_2w*f_2\\) for any two images \\(f_1\\), \\(f_2\\) and numbers \\(c_1\\), \\(c_2\\). There is beautiful and powerful theory underlying linear filters (Vetterli, Kovačević, and Goyal 2014).\nTo proceed we now use smaller smoothing bandwidths than what we displayed in Figure 11.9 for demonstration. Let’s use a sigma of 1 pixel for the DNA channel and 3 pixels for actin and tubulin.\ncellsSmooth = Image(dim = dim(cells))\nsigma = c(1, 3, 3)\nfor(i in seq_along(sigma))\n  cellsSmooth[,,i] = filter2( cells[,,i],\n         filter = makeBrush(size = 51, shape = \"gaussian\",\n                            sigma = sigma[i]) )__\nThe smoothed images have reduced pixel noise, yet still the needed resolution.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#adaptive-thresholding",
    "href": "11-chap.html#adaptive-thresholding",
    "title": "13  11.1 이 장의 목표",
    "section": "13.8 11.9 Adaptive thresholding",
    "text": "13.8 11.9 Adaptive thresholding\nThe idea of adaptive thresholding is that, compared to straightforward thresholding as we did for Figure 11.6b, the threshold is allowed to be different in different regions of the image. In this way, one can anticipate spatial dependencies of the underlying background signal caused, for instance, by uneven illumination or by stray signal from nearby bright objects. In fact, we have already seen an example for uneven background in the bottom right image of Figure 11.3.\nOur colon cancer images (Figure 11.8) do not have such artefacts, but for demonstration, let’s simulate uneven illumination by multiplying the image with a two-dimensional bell function illuminationGradient, which has highest value in the middle and falls off to the sides (Figure 11.11).\npy = seq(-1, +1, length.out = dim(cellsSmooth)[1])\npx = seq(-1, +1, length.out = dim(cellsSmooth)[2])\nilluminationGradient = Image(outer(py, px, function(x, y) exp(-(x^2 + y^2))))\nnucBadlyIlluminated = cellsSmooth[,,1] * illuminationGradient __\nWe now define a smoothing window, disc, whose size is 21 pixels, and therefore bigger than the nuclei we want to detect, but small compared to the length scales of the illumination artifact. We use it to compute the image localBackground (shown in Figure 11.11 (c)) and the thresholded image nucBadThresh.\ndisc = makeBrush(21, \"disc\")\ndisc = disc / sum(disc)\nlocalBackground = filter2(nucBadlyIlluminated, disc)\noffset = 0.02\nnucBadThresh = (nucBadlyIlluminated - localBackground &gt; offset)__\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.11: a: illuminationGradient, a function that has its maximum at the center and falls off towards the sides, and which simulates uneven illumination sometimes seen in images. (b) nucBadlyIlluminated, the image that results from multiplying the DNA channel in cellsSmooth with illuminationGradient. (c) localBackground, the result of applying a linear filter with a bandwidth that is larger than the objects to be detected. (d) nucBadThresh, the result of adaptive thresholding. The nuclei at the periphery of the image are reasonably well identified, despite the drop off in signal strength.\nAfter having seen that this may work, let’s do the same again for the actual (not artificially degraded) image, as we need this for the next steps.\nnucThresh = (cellsSmooth[,,1] - filter2(cellsSmooth[,,1], disc) &gt; offset)__\nBy comparing each pixel’s intensity to a background determined from the values in a local neighborhood, we assume that the objects are relatively sparse distributed in the image, so that the signal distribution in the neighborhood is dominated by background. For the nuclei in our images, this assumption makes sense, for other situations, you may need to make different assumptions. The adaptive thresholding that we have done here uses a linear filter, filter2, and therefore amounts to (weighted) local averaging. Other distribution summaries, e.g. the median or a low quantile, tend to be preferable, even if they are computationally more expensive. For local median filtering, EBimage provides the function medianFilter.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#morphological-operations-on-binary-images",
    "href": "11-chap.html#morphological-operations-on-binary-images",
    "title": "13  11.1 이 장의 목표",
    "section": "13.9 11.10 Morphological operations on binary images",
    "text": "13.9 11.10 Morphological operations on binary images\nThe thresholded image nucThresh (shown in the left panel of Figure [morphop] is not yet satisfactory. The boundaries of the nuclei are slightly rugged, and there is noise at the single-pixel level. An effective and simple way to remove these nuisances is given by a set of morphological operations (Serra 1983).\nProvided a binary image (with values, say, 0 and 1, representing back- and foreground pixels), and a binary mask9 (which is sometimes also called the structuring element), these operations work as follows.\n9 An example for a mask is a circle with a given radius, or more precisely, the set of pixels within a certain distance from a center pixel.\n\nerode: For every foreground pixel, put the mask around it, and if any pixel under the mask is from the background, then set all these pixels to background.\ndilate: For every background pixel, put the mask around it, and if any pixel under the mask is from the foreground, then set all these pixels to foreground.\nopen: perform erode followed by dilate.\n\nWe can also think of these operations as filters, however, in contrast to the linear filters of Section 11.8 they operate on binary images only, and there is no linearity.\nLet us apply morphological opening to our image.\nnucOpened = EBImage::opening(nucThresh, kern = makeBrush(5, shape = \"disc\"))__\nThe result of this is subtle, and you will have to zoom into the images in Figure 11.12 to spot the differences, but this operation manages to smoothen out some pixel-level features in the binary images that for our application are undesirable.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#segmentation-of-a-binary-image-into-objects",
    "href": "11-chap.html#segmentation-of-a-binary-image-into-objects",
    "title": "13  11.1 이 장의 목표",
    "section": "13.10 11.11 Segmentation of a binary image into objects",
    "text": "13.10 11.11 Segmentation of a binary image into objects\nThe binary image nucOpened represents a segmentation of the image into foreground and background pixels, but not into individual nuclei. We can take one step further and extract individual objects defined as connected sets of pixels. In EBImage , there is a handy function for this purpose, bwlabel.\nnucSeed = bwlabel(nucOpened)\ntable(nucSeed)__\n\n\nnucSeed\n     0      1      2      3      4      5      6      7      8      9     10 \n155408    511    330    120    468    222    121    125    159    116    520 \n    11     12     13     14     15     16     17     18     19     20     21 \n   115    184    179    116    183    187    303    226    164    309    194 \n    22     23     24     25     26     27     28     29     30     31     32 \n   148    345    287    203    379    371    208    222    320    443    409 \n    33     34     35     36     37     38     39     40     41     42     43 \n   493    256    169    225    376    214    228    341    269    119    315 \nThe function returns an image, nucSeed, of integer values, where 0 represents the background, and the numbers from 1 to 43 index the different identified objects.\n__\nQuestion 11.10\nWhat are the numbers in the above table?\n__\nSolution\n__\nThey correspond to the area (in pixels) of each of the objects. We could use this information to remove objects that are too large or too small compared to what we expect.\nTo visualize such images, the function colorLabels is convenient, which converts the (grayscale) integer image into a color image, using distinct, arbitrarily chosen colors for each object.\nEBImage::display(colorLabels(nucSeed))__\nThis is shown in the middle panel of Figure 11.12. The result is already encouraging, although we can spot two types of errors:\n\nSome neighboring objects were not properly separated.\nSome objects contain holes.\n\nIndeed, we could change the occurrences of these by playing with the disc size and the parameter offset in Section 11.9: making the offset higher reduces the probability that two neighboring object touch and are seen as one object by bwlabel; on the other hand, that leads to even more and even bigger holes. Vice versa for making it lower.\nSegmentation is a rich and diverse field of research and engineering, with a large body of literature, software tools (Schindelin et al. 2012; Chaumont et al. 2012; Carpenter et al. 2006; Held et al. 2010) and practical experience in the image analysis and machine learning communities. What is the adequate approach to a given task depends hugely on the data and the underlying question, and there is no universally best method. It is typically even difficult to obtain a “ground truth” or “gold standards” by which to evaluate an analysis – relying on manual annotation of a modest number of selected images is not uncommon. Despite the bewildering array of choices, it is easy to get going, and we need not be afraid of starting out with a simple solution, which we can successively refine. Improvements can usually be gained from methods that allow inclusion of more prior knowledge of the expected shapes, sizes and relations between the objects to be identified.\nFor statistical analyses of high-throughput images, we may choose to be satisfied with a simple method that does not rely on too many parameters or assumptions and results in a perhaps sub-optimal but rapid and good enough result (Rajaram et al. 2012). In this spirit, let us proceed with what we have. We generate a lenient foreground mask, which surely covers all nuclear stained regions, even though it also covers some regions between nuclei. To do so, we simply apply a second, less stringent adaptive thresholding.\nnucMask = cellsSmooth[,,1] - filter2(cellsSmooth[,,1], disc) &gt; 0 __\nand apply another morphological operation, fillHull, which fills holes that are surrounded by foreground pixels.\nnucMask = fillHull(nucMask)__\nTo improve nucSeed, we can now propagate its segmented objects until they fill the mask defined by nucMask. Boundaries between nuclei, in those places where the mask is connected, can be drawn by Voronoi tessellation, which is implemented in the function propagate, and will be explained in the next section.\nnuclei = propagate(cellsSmooth[,,1], nucSeed, mask = nucMask)__\nThe result is displayed in the rightmost panel of Figure 11.12.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.12: Different steps in the segmentation of the nuclei. From (a-e): nucThresh, nucOpened, nucSeed, nucMask, nuclei.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#voronoi-tessellation",
    "href": "11-chap.html#voronoi-tessellation",
    "title": "13  11.1 이 장의 목표",
    "section": "13.11 11.12 Voronoi tessellation",
    "text": "13.11 11.12 Voronoi tessellation\nVoronoi tessellation is useful if we have a set of seed points (or regions) and want to partition the space that lies between these seeds in such a way that each point in the space is assigned to its closest seed. As this is an intuitive and powerful idea, we’ll use this section for a short digression on it. Let us consider a basic example. We use the image nuclei as seeds. To call the function propagate, we also need to specify another image: for now we just provide a trivial image of all zeros, and we set the parameter lambda to a large positive value (we will come back to these choices).\nzeros        = Image(dim = dim(nuclei))\nvoronoiExamp = propagate(seeds = nuclei, x = zeros, lambda = 100)\nvoronoiPaint = paintObjects(voronoiExamp, 1 - nucOpened)__\n\nFigure 11.13: Example of a Voronoi segmentation, indicated by the gray lines, using the nuclei (indicated by black regions) as seeds.\n__\nQuestion 11.11\nHow do you select partition elements from the tessellation?\n__\nSolution\n__\nThe result, voronoiExamp, of the above call to propagate is simply an image of integers whose values indicate the different partitions.\nhead(table(voronoiExamp))__\n\n\nvoronoiExamp\n   1    2    3    4    5    6 \n5645 4735  370 5964 3333 1377 \n\n\nind = which(voronoiExamp == 13, arr.ind = TRUE)\nhead(ind, 3)__\n\n\n     row col\n[1,] 112 100\n[2,] 113 100\n[3,] 114 100\nThe result is shown in Figure 11.13. This looks interesting, but perhaps not yet as useful as the image nuclei in Figure [morphop]. We note that the basic definition of Voronoi tessellation, which we have given above, allows for two generalizations:\n\nBy default, the space that we partition is the full, rectangular image area – but indeed we could restrict ourselves to any arbitrary subspace. This is akin to finding the shortest distance from each point to the next seed not in a simple flat landscape, but in a landscape that is interspersed by lakes and rivers (which you cannot cross), so that all paths need to remain on the land. propagate allows for this generalization through its mask parameter.\nBy default, we think of the space as flat – but in fact it could have hills and canyons, so that the distance between two points in the landscape not only depends on their \\(x\\)- and \\(y\\)-positions but also on the ascents and descents, up and down in \\(z\\)-direction, that lie in between. We can think of \\(z\\) as an “elevation”. You can specify such a landscape to propagate through its x argument.\n\nMathematically, we say that instead of the simple default case (a flat rectangle, or image, with a Euclidean metric on it), we perform the Voronoi segmentation on a Riemann manifold that has a special shape and a special metric. Let us use the notation \\(x\\) and \\(y\\) for the column and row coordinates of the image, and \\(z\\) for the elevation. For two neighboring points, defined by coordinates \\((x, y, z)\\) and \\((x+x, y+y, z+z)\\), the distance \\(s\\) between them is thus not obtained by the usual Euclidean metric on the 2D image,\n\\[ s^2 = x^2 + y^2 \\]\nbut instead\n\\[ s^2 = , \\]\nwhere the parameter \\(\\) is a real number \\(\\). To understand this, lets look at some important cases:\n\\[ ]\nFor \\(\\), the metric becomes the isotropic Euclidean metric, i.e., a movement in \\(z\\)-direction is equally “expensive” or “far” as in \\(x\\)- or \\(y\\)-direction. In the extreme case of \\(\\), only the \\(z\\)-movements matter, whereas lateral movements (in \\(x\\)- or \\(y\\)-direction) do not contribute to the distance. In the other extreme case, \\(\\), only lateral movements matter, and movement in \\(z\\)-direction is “free”. Distances between points further apart are obtained by summing \\(s\\) along the shortest path between them. The parameter \\(\\) serves as a convenient control of the relative weighting between sideways movement (along the \\(x\\) and \\(y\\) axes) and vertical movement. Intuitively, if you imagine yourself as a hiker in such a landscape, by choosing \\(\\) you can specify how much you are prepared to climb up and down to overcome a mountain, versus walking around it. When we used lambda = 100 in our call to propagate at the begin of this section, this value was effectively infinite, so we were in the third boundary case of Equation 11.5.\nFor the purpose of cell segmentation, these ideas were put forward by Thouis Jones et al. (Jones, Carpenter, and Golland 2005; Carpenter et al. 2006), who also wrote the efficient algorithm that is used by propagate.\n__\nTask\nTry out the effect of using different \\(\\)s.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#segmenting-the-cell-bodies",
    "href": "11-chap.html#segmenting-the-cell-bodies",
    "title": "13  11.1 이 장의 목표",
    "section": "13.12 11.13 Segmenting the cell bodies",
    "text": "13.12 11.13 Segmenting the cell bodies\n\nFigure 11.14: Histogram of the actin channel in cellsSmooth, after taking the logarithm.\n\nFigure 11.15: Zoom into Figure 11.14.\nTo determine a mask of cytoplasmic area in the images, let us explore a different way of thresholding, this time using a global threshold which we find by fitting a mixture model to the data. The histograms show the distributions of the pixel intensities in the actin image. We look at the data on the logarithmic scale, and in Figure 11.15 zoom into the region where most of the data lie.\nhist(log(cellsSmooth[,,3]) )\nhist(log(cellsSmooth[,,3]), xlim = -c(3.6, 3.1), breaks = 300)__\nLooking at the these histograms for many images, we can set up the following model for the purpose of segmentation: the signal in the cytoplasmic channels of the Image cells is a mixture of two distributions, a log-Normal background and a foreground with another, unspecified, rather flat, but mostly non-overlapping distribution10. Moreover the majority of pixels are from the background. We can then find robust estimates for the location and width parameters of the log-Normal component from the half range mode (implemented in the package genefilter) and from the root mean square of the values that lie left of the mode.\n10 This is an application of the ideas we saw in Chapter 4 on mixture models.\nlibrary(\"genefilter\")\nbgPars = function(x) {\n  x    = log(x)\n  loc  = half.range.mode( x )\n  left = (x - loc)[ x &lt; loc ]\n  wid  = sqrt( mean(left^2) )\n  c(loc = loc, wid = wid, thr = loc + 6*wid)\n}\ncellBg = apply(cellsSmooth, MARGIN = 3, FUN = bgPars)\ncellBg __\n\n\n           [,1]        [,2]        [,3]\nloc -2.90176965 -2.94427499 -3.52191681\nwid  0.00635322  0.01121337  0.01528207\nthr -2.86365033 -2.87699477 -3.43022437\nThe function defines as a threshold the location loc plus 6 widths wid11.\n11 The choice of the number 6 here is ad hoc; we could make the choice of threshold more objective by estimating the weights of the two mixture components and assigning each pixel to either fore- or background based on its posterior probability according to the mixture model. More advanced segmentation methods use the fact that this is really a classification problem and include additional features and more complex classifiers to separate foreground and background regions (e.g., (Berg et al. 2019)).\nhist(log(cellsSmooth[,,3]), xlim = -c(3.6, 3.1), breaks = 300)\nabline(v = cellBg[c(\"loc\", \"thr\"), 3], col = c(\"brown\", \"red\"))__\n\nFigure 11.16: As in Figure 11.15, but with loc and thr shown by vertical lines.\nWe can now define cytoplasmMask by the union of all those pixels that are above the threshold in the actin or tubulin image, or that we have already classified as nuclear in the image nuclei.\ncytoplasmMask = (cellsSmooth[,,2] &gt; exp(cellBg[\"thr\", 2])) |\n       nuclei | (cellsSmooth[,,3] &gt; exp(cellBg[\"thr\", 3]))__\nThe result is shown in the left panel of Figure 11.17. To define the cellular bodies, we can now simply extend the nucleus segmentation within this mask by the Voronoi tessellation based propagation algorithm of Section 11.12. This method makes sure that there is exactly one cell body for each nucleus, and the cell bodies are delineated in such a way that a compromise is reached between compactness of cell shape and following the actin and \\(\\)-tubulin intensity signal in the images. In the terminology of the propagate algorithm, cell shape is kept compact by the \\(x\\) and \\(y\\) components of the distance metric 11.4, and the actin signal is used for the \\(z\\) component. \\(\\) controls the trade-off.\ncellbodies = propagate(x = cellsSmooth[,,3], seeds = nuclei,\n                       lambda = 1.0e-2, mask = cytoplasmMask)__\nAs an alternative representation to the colorLabel plots, we can also display the segmentations of nuclei and cell bodies on top of the original images using the paintObjects function; the Images nucSegOnNuc, nucSegOnAll and cellSegOnAll that are computed below are show in the middle to right panels of Figure 11.17\ncellsColor = EBImage::rgbImage(red   = cells[,,3],\n                               green = cells[,,2],\n                               blue  = cells[,,1])\nnucSegOnNuc  = paintObjects(nuclei, tgt = EBImage::toRGB(cells[,,1]), col = \"#ffff00\")\nnucSegOnAll  = paintObjects(nuclei,     tgt = cellsColor,    col = \"#ffff00\")\ncellSegOnAll = paintObjects(cellbodies, tgt = nucSegOnAll,   col = \"#ff0080\")__\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.17: Steps in the segmentation of the cell bodies. From (a-d): cytoplasmMask, cellbodies (blue: DAPI, red: actin, green: alpha-tubulin), nucSegOnNuc, nucSegOnAll, cellSegOnAll.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#feature-extraction",
    "href": "11-chap.html#feature-extraction",
    "title": "13  11.1 이 장의 목표",
    "section": "13.13 11.14 Feature extraction",
    "text": "13.13 11.14 Feature extraction\nNow that we have the segmentations nuclei and cellbodies together with the original image data cells, we can compute various descriptors, or features, for each cell. We already saw in the beginning of Section 11.11 how to use the base R function table to determine the total number and sizes of the objects. Let us now take this further and compute the mean intensity of the DAPI signal (cells[,,1]) in the segmented nuclei, the mean actin intensity (cells[,,3]) in the segmented nuclei and the mean actin intensity in the cell bodies.\nmeanNucInt       = tapply(cells[,,1], nuclei, mean)\nmeanActIntInNuc  = tapply(cells[,,3], nuclei, mean)\nmeanActIntInCell = tapply(cells[,,3], cellbodies, mean)__\nWe can visualize the features in pairwise scatterplots (Figure 11.18). We see that they are correlated with each other, although each feature also carries independent information.\nlibrary(\"GGally\")\nggpairs(tibble(meanNucInt, meanActIntInNuc, meanActIntInCell))__\n\nFigure 11.18: Pairwise scatterplots of per-cell intensity descriptors.\nWith a little more work, we could also compute more sophisticated summary statistics – e.g., the ratio of nuclei area to cell body area; or entropies, mutual information and correlation of the different fluorescent signals in each cell body, as more or less abstract measures of cellular morphology. Such measures can be used, for instance, to detect subtle drug induced changes of cellular architecture.\nWhile it is easy and intuitive to perform these computations using basic R idioms like in the tapply expressions above, the package EBImage also provides the function computeFeatures which efficiently computes a large collection of features that have been commonly used in the literature (a pioneering reference is Boland and Murphy. (2001)). Details about this function are described in its manual page, and an example application is worked through in the HD2013SGI vignette. Below, we compute features for intensity, shape and texture for each cell from the DAPI channel using the nucleus segmentation (nuclei) and from the actin and tubulin channels using the cell body segmentation (cytoplasmRegions).\nF1 = computeFeatures(nuclei,     cells[,,1], xname = \"nuc\",  refnames = \"nuc\")\nF2 = computeFeatures(cellbodies, cells[,,2], xname = \"cell\", refnames = \"tub\")\nF3 = computeFeatures(cellbodies, cells[,,3], xname = \"cell\", refnames = \"act\")\ndim(F1)__\n\n\n[1] 43 89\nF1 is a matrix with 43 rows (one for each cell) and 89 columns, one for each of the computed features.\nF1[1:3, 1:5]__\n\n\n  nuc.0.m.cx nuc.0.m.cy nuc.0.m.majoraxis nuc.0.m.eccentricity nuc.0.m.theta\n1   119.5523   17.46895          44.86819            0.8372059     -1.314789\n2   143.4511   15.83709          26.15009            0.6627672     -1.213444\n3   336.5401   11.48175          18.97424            0.8564444      1.470913\nThe column names encode the type of feature, as well the color channel(s) and segmentation mask on which it was computed. We can now use multivariate analysis methods – like those we saw in Chapters 5, 7 and 9 – for many dfferent tasks, such as\n\ndetecting cell subpopulations (clustering)\nclassifying cells into pre-defined cell types or phenotypes (classification)\nseeing whether the absolute or relative frequencies of the subpopulations or cell types differ between images that correspond to different biological conditions\n\nIn addition to these “generic” machine learning tasks, we also know the cell’s spatial positions, and in the following we will explore some ways to make use of these in our analyses.\n__\nTask\nUse explorative multivariate methods to visualize the matrices F1, F2, F3: PCA, heatmap. What’s special about the “outlier” cells?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#spatial-statistics-point-processes",
    "href": "11-chap.html#spatial-statistics-point-processes",
    "title": "13  11.1 이 장의 목표",
    "section": "13.14 11.15 Spatial statistics: point processes",
    "text": "13.14 11.15 Spatial statistics: point processes\nIn the previous sections, we have seen ways how to use images of cells to extract their positions and various shape and morphological features. We’ll now explore spatial distributions of the position. In order to have interesting data to work on, we’ll change datasets and look at breast cancer lymph node biopsies.\n\n13.14.1 11.15.1 A case study: Interaction between immune cells and cancer cells\nThe lymph nodes function as an immunologic filter for the bodily fluid known as lymph. Antigens are filtered out of the lymph in the lymph node before returning it to the circulation. Lymph nodes are found throughout the body, and are composed mostly of T cells, B cells, dendritic cells and macrophages. The nodes drain fluid from most of our tissues. The lymph ducts of the breast usually drain to one lymph node first, before draining through the rest of the lymph nodes underneath the arm. That first lymph node is called the sentinel lymph node. In a similar fashion as the spleen, the macrophages and dendritic cells that capture antigens present these foreign materials to T and B cells, consequently initiating an immune response.\nT lymphocytes are usually divided into two major subsets that are functionally and phenotypically different.\n\nCD4+ T-cells, or T helper cells: they are pertinent coordinators of immune regulation. The main function of T helper cells is to augment or potentiate immune responses by the secretion of specialized factors that activate other white blood cells to fight off infection.\nCD8+ T cells, or T killer/suppressor cells: these cells are important in directly killing certain tumor cells, viral-infected cells and sometimes parasites. The CD8+ T cells are also important for the down-regulation of immune responses.\n\nBoth types of T cells can be found throughout the body. They often depend on the secondary lymphoid organs (the lymph nodes and spleen) as sites where activation occurs.\nDendritic Cells or CD1a cells are antigen-presenting cells that process antigen and present peptides to T cells.\nTyping the cells can be done by staining the cells with protein antibodies that provide specific signatures. For instance, different types of immune cells have different proteins expressed, mostly in their cell membranes.\n\nFigure 11.19: Biopsy of an enlarged lymph node revealed an intact capsule and obliterated sinuses (upper left panel, stained with hematoxylin and eosin, original magnification \\(\\) 100). The infiltrate was composed of an admixture of small lymphocytes, macrophages, and plasma cells (upper right panel, hematoxylin and eosin, original magnification \\(\\) 400). The infiltrate was composed of a mixture of CD3 positive T-cells (including both CD4 and CD8 positive cells) and CD20 positive B-cells. Numerous macrophages were also CD4 positive. (From: Hurley et al., Diagnostic Pathology (2008) 3:13)\n\nFigure 11.20: A stained lymph node; this image is the basis for the spatial data in brcalymphnode.\nWe’ll look at data by Setiadi et al. (2010). After segmentating the image shown in Figure 11.20 using the segmentation method GemIdent (Holmes, Kapelner, and Lee 2009), the authors obtained the coordinates and the type of all the cells in the image. We call this type of data a marked point process , and it can be seen as a simple table with 3 columns.\nlibrary(\"readr\")\nlibrary(\"dplyr\")\ncellclasses = c(\"T_cells\", \"Tumor\", \"DCs\", \"other_cells\")\nbrcalymphnode = lapply(cellclasses, function(k) {\n    read_csv(file.path(\"..\", \"data\", sprintf(\"99_4525D-%s.txt\", k))) |&gt;\n    transmute(x = globalX, y = globalY, class = k)\n}) |&gt; bind_rows() |&gt; mutate(class = factor(class))\n\nbrcalymphnode __\n\n\n# A tibble: 209,462 × 3\n       x     y class  \n   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  \n 1  6355 10382 T_cells\n 2  6356 10850 T_cells\n 3  6357 11070 T_cells\n 4  6357 11082 T_cells\n 5  6358 10600 T_cells\n 6  6361 10301 T_cells\n 7  6369 10309 T_cells\n 8  6374 10395 T_cells\n 9  6377 10448 T_cells\n10  6379 10279 T_cells\n# ℹ 209,452 more rows\n\n\ntable(brcalymphnode$class)__\n\n\n        DCs other_cells     T_cells       Tumor \n        878       77081      103681       27822 \nWe see that there are over a 100,000 T cells, around 28,000 tumor cells, and only several hundred dendritic cells. Let’s plot the \\(x\\)- and \\(y\\)-positions of the cells (Figure 11.21).\nggplot(filter(brcalymphnode, class %in% c(\"T_cells\", \"Tumor\")),\n   aes(x = x, y = y, col = class)) + geom_point(shape = \".\") +\n   facet_grid( . ~ class) + guides(col = \"none\")__\n\nFigure 11.21: Scatterplot of the \\(x\\) and \\(y\\) positions of the T- and tumor cells in brcalymphnode. The locations were obtained by a segmentation algorithm from a high resolution version of Figure 11.20. Some rectangular areas in the T-cells plot are suspiciously empty, this could be because the corresponding image tiles within the overall composite image went missing, or were not analyzed.\n__\nQuestion 11.12\nCompare Figures 11.20 and 11.21. Why are the \\(y\\)-axis inverted relative to each other?\n__\nSolution\n__\nFigure 11.20 follows the convention for image data, where the origin is in the top left corner of the image, while Figure 11.21 follows the convention for Cartesian plots, with the origin at the bottom left.\nTo use the functionality of the spatstat package, it is convenient to convert our data in brcalymphnode into an object of class ppp ; we do this by calling the eponymous function.\nlibrary(\"spatstat\")__\n\n\nln = with(brcalymphnode, ppp(x = x, y = y, marks = class, \n                             xrange = range(x), yrange = range(y)))\nln __\n\n\nMarked planar point pattern: 209462 points\nMultitype, with levels = DCs, other_cells, T_cells, Tumor \nwindow: rectangle = [3839, 17276] x [6713, 23006] units\nppp objects are designed to capture realizations of a spatial point process , that is, a set of isolated points located in a mathematical space; in our case, as you can see above, the space is a two-dimensional rectangle that contains the range of the \\(x\\)- and \\(y\\)-coordinates. In addition, the points can be marked with certain properties. In ln, the mark is simply the factor variable class. More generally, it could be several attributes, times, or quantitative data as well. There are similarities between a marked point process and an image, although for the former, the points can lie anywhere within the space, whereas in an image, the pixels are covering the space in regular, rectangular way.\n\n\n13.14.2 11.15.2 Convex hull\nAbove, we (implicitly) confined the point process to lie in a rectangle. In fact, the data generating process is more confined, by the shape of the tissue section. We can approximate this and compute a tighter region from the convex hull of the points12.\n12 You can use str(cvxhull) to look at the internal structure of this S3 object.\ncvxhull = convexhull.xy(cbind(ln$x, ln$y))\nggplot(as_tibble(cvxhull$bdry[[1]]), aes(x = x, y = y)) +\n  geom_polygon(fill = NA, col = \"black\") + geom_point() + coord_fixed()__\n\nFigure 11.22: Polygon describing the convex hull of the points in ln.\nWe can see the polygon in Figure 11.22 and now call ppp again, this time with the polygon.\nln = with(brcalymphnode, ppp(x = x, y = y, marks = class, \n                             poly = cvxhull$bdry[[1]]))\nln __\n\n\nMarked planar point pattern: 209462 points\nMultitype, with levels = DCs, other_cells, T_cells, Tumor \nwindow: polygonal boundary\nenclosing rectangle: [3839, 17276] x [6713, 23006] units\n\n\n13.14.3 11.15.3 Other ways of defining the space for the point process\nWe do not have to use the convex hull to define the space on which the point process is considered. Alternatively, we could have provided an image mask to ppp that defines the space based on prior knowledge; or we could use density estimation on the sampled points to only identify a region in which there is a high enough point density, ignoring sporadic outliers. These choices are part of the analyst’s job when considering spatial point processes.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#first-order-effects-the-intensity",
    "href": "11-chap.html#first-order-effects-the-intensity",
    "title": "13  11.1 이 장의 목표",
    "section": "13.15 11.16 First order effects: the intensity",
    "text": "13.15 11.16 First order effects: the intensity\nOne of the most basic questions of spatial statistics is whether neighboring points are “clustering”, i.e., whether and to what extent they are closer to each other than expected “by chance”; or perhaps the opposite, whether they seem to repel each other. There are many examples where this kind of question can be asked, for instance\n\ncrime patterns within a city,\ndisease patterns within a country,\nsoil measurements in a region.\n\nIt is usually not hard to find reasons why such patterns exist: good and bad neighborhoods, local variations in lifestyle or environmental exposure, the common geological history of the soil. Sometimes there may also be mechanisms by which the observed events attract or repel each other – the proverbial “broken windows” in a neighborhood, or the tendency of many cell types to stick close to other cells.\nThe cell example highlights that spatial clustering (or anticlustering) can depend on the objects’ attributes (or marks, in the parlance of spatial point processes). It also highlights that the answer can depend on the length scale considered. Even if cells attract each other, they have a finite size, and cannot occupy the same space. So there will be some minmal distance between them, on the scale of which they essentially repel each other, while at further distances, they attract.\nTo attack these questions more quantitatively, we need to define a probabilistic model of what we expect by chance. Let’s count the number of points lying in a subregion, say, a circle of area \\(a\\) around a point \\(p=(x,y)\\); call this \\(N(p, a)\\)13 The mean and covariance of \\(N\\) provide first and second order properties. The first order is the intensity of the process:\n13 As usual, we use the uppercase notation \\(N(p, a)\\) for the random variable, and the lowercase \\(n(p, a)\\) for its realizations, or samples.\n\\[ (p) = _{a} . \\]\nHere we used infinitesimal calculus to define the local intensity \\((p)\\). As for time series, a stationary process is one where we have homogeneity all over the region, i.e., \\((p) = \\); then the intensity in an area \\(A\\) is proportional to the area: \\(E[N(, A)] = A\\). Later we’ll also look at higher order statistics, such as the spatial covariance\n\\[ (p_1, p_2) = _{a } . \\]\nIf the process is stationary, this will only depend on the relative position of the two points (the vector between them). If it only depends on the distance, i.e., only on the length but not on the direction of the vector, it is called second order isotropic.\n\nFigure 11.23: Rain drops falling on the floor are modelled by a Poisson process. The number of drops falling on a particular spot only depends on the rate \\(\\) (and on the size of the spot), but not on what happens at other spots.\n\n13.15.1 11.16.1 Poisson Process\nThe simplest spatial process is the Poisson process. We will use it as a null model against which to compare our data. It is stationary with intensity \\(\\), and there are no further dependencies between occurrences of points in non-overlapping regions of the space. Moreover, the number of points in a region of area \\(A\\) follows a Poisson distribution with rate \\(A\\).\n\n\n13.15.2 11.16.2 Estimating the intensity\nTo estimate the intensity, divide up the area into subregions, small enough to see potential local variations of \\((p)\\), but big enough to contain a sufficient sample of points. This is analogous to 2D density estimation, and instead of hard region boundaries, we can use a smooth kernel function \\(K\\).\n\\[ (p) = _i e(p_i) K(p-p_i). \\]\nThe kernel function depends on a smoothing parameter, \\(\\), the larger it is, the larger the regions over which we compute the local estimate for each \\(p\\). \\(e(p)\\) is an edge correction factor, and takes into account the estimation bias caused when the support of the kernel (the “smoothing window”) would fall outside the space on which the point process is defined. The function density, which is defined for ppp objects in the spatstat package, implements Equation 11.8.\nd = density(subset(ln, marks == \"Tumor\"), edge=TRUE, diggle=TRUE)\nplot(d)__\n\nFigure 11.24: Intensity estimate for the cells marked Tumor in ppp. The support of the estimate is the polygon that we specified earlier on (Figure 11.22).\nThe plot is shown in Figure 11.24.\n__\nQuestion 11.13\nHow does the estimate look without edge correction?\n__\nSolution\n__\nd0 = density(subset(ln, marks == \"Tumor\"), edge = FALSE)\nplot(d0)__\n\nFigure 11.25: As Figure 11.24, but without edge correction.\nNow estimated intensity is smaller towards the edge of the space, reflecting edge bias (Figure 11.25).\ndensity gives us as estimate of the intensity of the point process. A related, but different task is the estimation of the (conditional) probability of being a particular cell class. The function relrisk computes a nonparametric estimate of the spatially varying risk of a particular event type. We’re interested in the probability that a cell that is present at particular spatial location will be a tumor cell (Figure 11.26).\nrr = relrisk(ln, sigma = 250)__\n\n\nplot(rr)__\n\nFigure 11.26: Estimates of the spatially varying probability of each of the cell classes, conditional on there being cells.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#second-order-effects-spatial-dependence",
    "href": "11-chap.html#second-order-effects-spatial-dependence",
    "title": "13  11.1 이 장의 목표",
    "section": "13.16 11.17 Second order effects: spatial dependence",
    "text": "13.16 11.17 Second order effects: spatial dependence\nIf we pick a point at random in our spatial process, what is the distance \\(W\\) to its nearest neighbor? For a homogenous Poisson process, the cumulative distribution function of this distance is\n\\[ G(w) = P(Ww) = 1-e{-w2}. \\]\nPlotting \\(G\\) gives a way of noticing departure from the homogenous Poisson process. An estimator of \\(G\\), which also takes into account edge effects (A. J. Baddeley 1998; Ripley 1988), is provided by the function Gest of the spatstat package.\ngln = Gest(ln)\ngln __\n\n\nFunction value object (class 'fv')\nfor the function r -&gt; G(r)\n.....................................................................\n        Math.label      Description                                  \nr       r               distance argument r                          \ntheo    G[pois](r)      theoretical Poisson G(r)                     \nhan     hat(G)[han](r)  Hanisch estimate of G(r)                     \nrs      hat(G)[bord](r) border corrected estimate of G(r)            \nkm      hat(G)[km](r)   Kaplan-Meier estimate of G(r)                \nhazard  hat(h)[km](r)   Kaplan-Meier estimate of hazard function h(r)\ntheohaz h[pois](r)      theoretical Poisson hazard function h(r)     \n.....................................................................\nDefault plot formula:  .~r\nwhere \".\" stands for 'km', 'rs', 'han', 'theo'\nRecommended range of argument r: [0, 20.998]\nAvailable range of argument r: [0, 52.443]\n\n\nlibrary(\"RColorBrewer\")\nplot(gln, xlim = c(0, 10), lty = 1, col = brewer.pal(4, \"Set1\"))__\n\nFigure 11.27: Estimates of \\(G\\), using three different edge effect corrections –which here happen to essentially lie on top of each other– and the theoretical distribution for a homogenous Poisson process.\nThe printed summary of the object gln gives an overview of the computed estimates; further explanations are in the manual page of Gest. In Figure 11.27 we see that the empirical distribution function and that of our null model, a homogenous Poisson process with a suitably chosen intensity, cross at around 4.5 units. Cell to cell distances that are shorter than this value are less likely than for the null model, in particular, there are essentially no distances below around 2; this, of course, reflects the fact that our cells have finite size and cannot overlap the same space. There seems to be trend to avoid very large distances –compared to the Poisson process–, perhaps indicative of a tendency of the cells to cluster.\n\n13.16.1 11.17.1 Ripley’s \\(K\\) function\nIn homogeneous spatial Poisson process, if we randomly pick any point and count the number of points within a distance of at most \\(r\\), we expect this number to grow as the area of the circle, \\(r^2\\). For a given dataset, we can compare this expectation to the observed number of neighbors within distance \\(r\\), averaged across all points.\nThe \\(K\\) function (variously called Ripley’s \\(K\\)-function or the reduced second moment function) of a stationary point process is defined so that \\(K(r)\\) is the expected number of (additional) points within a distance \\(r\\) of a given, randomly picked point. Remember that \\(\\) is the intensity of the process, i.e., the expected number of points per unit area. The \\(K\\) function is a second order moment property of the process.\nThe definition of \\(K\\) can be generalized to inhomogeneous point processes and written as in (A. Baddeley, Moller, and Waagepetersen 2000),\n\\[ K_{}(r)= {i,j} 𝟙{d(p_i, p_j) r} { (x_i) (x_j) }, \\]\nwhere \\(d(p_i, p_j)\\) is the distance between points \\(p_i\\) and \\(p_j\\), and \\(e(p_i, p_j, r)\\) is an edge correction factor14. For estimation and visualisation, it is useful to consider a transformation of \\(K\\) (and analogously, of \\(K_{}\\)), the so- called \\(L\\) function.\n14 See the manual page of Kinhom for more.\n\\[ L(r)=. \\]\nFor a homogeneous spatial Poisson process, the theoretical value is \\(L(r) = r\\). By comparing that to the estimate of \\(L\\) for a dataset, we can learn about inter-point dependence and spatial clustering. The square root in Equation 11.11 has the effect of stabilising the variance of the estimator, so that compared to \\(K\\), \\(L\\) is more appropriate for data analysis and simulations. The computations in the function Linhom of the spatstat package take a few minutes for our data (Figure 11.28).\nLln = Linhom(subset(ln, marks == \"T_cells\"))__\n\n\nLln __\n\n\nFunction value object (class 'fv')\n\n\nfor the function r -&gt; L[inhom](r)\n\n\n................................................................................\n           Math.label                \nr          r                         \ntheo       L[pois](r)                \nborder     {hat(L)[inhom]^{bord}}(r) \nbord.modif {hat(L)[inhom]^{bordm}}(r)\n           Description                                      \nr          distance argument r                              \ntheo       theoretical Poisson L[inhom](r)                  \nborder     border-corrected estimate of L[inhom](r)         \nbord.modif modified border-corrected estimate of L[inhom](r)\n................................................................................\nDefault plot formula:  .~.x\nwhere \".\" stands for 'bord.modif', 'border', 'theo'\nRecommended range of argument r: [0, 694.7]\nAvailable range of argument r: [0, 694.7]\n\n\nplot(Lln, lty = 1, col = brewer.pal(3, \"Set1\"))__\n\nFigure 11.28: Estimate of \\(L_{}\\), Equations 11.10 and 11.11, of the T cell pattern.\nWe could now proceed with looking at the \\(L\\) function also for other cell types, and for different tumors as well as for healthy lymph nodes. This is what Setiadi and colleagues did in their report (Setiadi et al. 2010), where by comparing the spatial grouping patterns of T and B cells between healthy and breast cancer lymph nodes they saw that B cells appeared to lose their normal localization in the extrafollicular region of the lymph nodes in some tumors.\n\n13.16.1.1 The pair correlation function\ndescribes how point density varies as a function of distance from a reference point. It provides a perspective inspired by physics for looking at spatial clustering. For a stationary point process, it is defined as\n\\[ g(r)=(r). \\]\nFor a stationary Poisson process, the pair correlation function is identically equal to 1. Values \\(g(r) &lt; 1\\) suggest inhibition between points; values greater than 1 suggest clustering.\nThe spatstat package allows computing estimates of \\(g\\) even for inhomogeneous processes, if we call pcf as below, the definition 11.12 is applied to the estimate of \\(K_{}\\).\npcfln = pcf(Kinhom(subset(ln, marks == \"T_cells\")))__\n\n\nplot(pcfln, lty = 1)\nplot(pcfln, lty = 1, xlim = c(0, 10))__\n\n\n\n\n\n\n\n\nFigure 11.29: Estimate of the pair correlation function, Equation 11.12, of the T cell pattern.\nAs we see in Figure 11.29, the T cells cluster, although at very short distances, there is also evidence for avoidance.\n__\nQuestion 11.14\nThe sampling resolution in the plot of the pair correlation function in the bottom panel of Figure 11.29 is low; how can it be increased?\n__\nSolution\n__\nThe answer lies in the r argument of the Kinhom function; see Figure 11.30.\npcfln2 = pcf(Kinhom(subset(ln, marks == \"T_cells\"),\n                    r = seq(0, 10, by = 0.2)))\nplot(pcfln2, lty = 1)__\n\nFigure 11.30: Answer to Question 11.14: as in the bottom panel of Figure 11.29, but with denser sampling.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#summary-of-this-chapter",
    "href": "11-chap.html#summary-of-this-chapter",
    "title": "13  11.1 이 장의 목표",
    "section": "13.17 11.18 Summary of this chapter",
    "text": "13.17 11.18 Summary of this chapter\nWe learned to work with image data in R. Images are basically just arrays, and we can use familiar idioms to manipulate them. We can extract quantitative features from images, and then many of the analytical questions are not unlike those with other high-throughput data: we summarize the features into statistics such as means and variances, do hypothesis testing for differences between conditions, perform analysis of variance, apply dimension reduction, clustering and classification.\nOften we want to compute such quantitative features not on the whole image, but for individual objects shown in the image, and then we need to first segment the image to demarcate the boundaries of the objects of interest. We saw how to do this for images of nuclei and cells.\nWhen the interest is on the positions of the objects and how these positions relate to each other, we enter the realm of spatial statistics. We have explored some of the functionality of the spatstat package, have encountered the point process class, and we learned some of the specific diagnostic statistics used for point patterns, like Ripley’s \\(K\\) function.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#further-reading",
    "href": "11-chap.html#further-reading",
    "title": "13  11.1 이 장의 목표",
    "section": "13.18 11.19 Further reading",
    "text": "13.18 11.19 Further reading\n\nThere is a vast amount of literature on image analysis. When navigating it, it is helpful to realize that the field is driven by two forces: specific application domains (we saw the analysis of high-throughput cell-based assays) and available computer hardware. Some algorithms and concepts that were developed in the 1970s are still relevant, others have been superseeded by more systematic and perhaps computationally more intensive methods. Many algorithms imply certain assumptions about the nature of the data and and scientific questions asked, which may be fine for one application, but need a fresh look in another. A classic introduction is The Image Processing Handbook (Russ and Neal 2015), which now is its seventh edition.\nFor spatial point pattern analysis, Diggle (2013; Ripley 1988; Cressie 1991; Chiu et al. 2013).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "11-chap.html#exercises",
    "href": "11-chap.html#exercises",
    "title": "13  11.1 이 장의 목표",
    "section": "13.19 11.20 Exercises",
    "text": "13.19 11.20 Exercises\n__\nExercise 11.1\nLoad some images from your personal photo library into R and try out the manipulations from Section 11.6 on them.\n__\nExercise 11.2\nExplore the effect of the parameter lambda in the propagate function (Sections 11.12, 11.13) using a shiny app that displays the cellbodies image as in Figure 11.17.\n__\nExercise 11.3\nConsider the two-dimensional empirical autocorrelation function,\n\\[ a(v_x, v_y) = _{(x,y)I} B(x, y);B(x+v_x, , y+v_y), \\]\nwhere \\(B\\) is an image, i.e., a function over the set of pixels \\(I\\), the tuple \\((x,y)\\) runs over all the pixel coordinates, and \\(v=(v_x, v_y)\\) is the offset vector. Using the Wiener–Khinchin theorem, we can compute this function efficiently using the Fast Fourier Transformation.\nautocorr2d = function(x) {\n  y = fft(x/sum(x))\n  abs(gsignal::fftshift(fft(y * Conj(y), inverse = TRUE), MARGIN = 1:2)) \n}__\nBelow, we’ll use this little helper function, which shows a matrix as a heatmap with ggplot2 (similar to base R’s image).\nmatrix_as_heatmap = function(m)\n  ggplot(reshape2::melt(m), aes(x = Var1, y = Var2, fill = value)) + \n    geom_tile() + coord_fixed() +\n    scale_fill_continuous(type = \"viridis\") +\n    scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))__\nNow let’s apply autocorr2d to each of the three color channels separately. The result is shown in Figure 11.31.\nnm = dimnames(cells)[[3]]\nac = lapply(nm, function(i) autocorr2d(cells[,, i])) |&gt; setNames(sub(\"^image-\", \"\", nm))\n\nfor (w in names(ac)) \n  print(matrix_as_heatmap(ac[[w]]) + ggtitle(w))\n\ncy = dim(cells)[1] / 2\ncx = dim(cells)[2] / 2\nr  = round(sqrt((col(cells[,,1]) - cx)^2 + (row(cells[,,1]) - cy)^2))\n\nmatrix_as_heatmap(r) + ggtitle(\"radius r\")__\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.31: Autocorrelation functions of the three color channels of the cells image, shown as heatmaps. The peaks in the centres correspond to signal correlations over short distances. Also shown is the radial coordinate r.\nSince the images are (or should be) isotropic, i.e., there is no preferred direction, we can average over the angular coordinate. The result is shown in Figure 11.32. We can see that the signals in the different color channels have different length scales.\naggregate_by_radius = function(x, r)\n  tibble(x = as.vector(x),\n         r = as.vector(r)) |&gt;\n  group_by(r) |&gt;\n  summarize(value = mean(x))\n\nlapply(names(ac), function(w) \n  cbind(channel = w, \n        aggregate_by_radius(ac[[w]], r))\n  ) |&gt; \n  bind_rows() |&gt; \n  dplyr::filter(r &lt;= 50) |&gt;\n  ggplot(aes(x = r, y = value, col = channel)) + geom_line() + \n    scale_color_manual(values = c(`Cy3` = \"red\", `FITC` = \"green\", `DAPI` = \"blue\"))__\nExtend the autocorr2d function to also compute the cross-correlation between different channels.\n\nWhat is the motivation behind the sum normalization in the above implementation autocorr2d?\nWould it make sense to subtract the mean of x before the other computations?\nWhat is the relation between this function and the usual empirical variance or correlation, i.e. the functions var and sd in base R?\nHow might plots such as Figure 11.32 be used for the construction of quality metrics in a high-throughput screening setting, i.e., when thousands or millions of images need to be analyzed?\nHow would a 3- or \\(n\\)-dimensional extension of autocorr2d look like? What would it be good for?\n\n\nFigure 11.32: Autocorrelation functions of the three color channels of the cells image, aggregated by radius.\n__\nExercise 11.4\nHave a look at the workshop “Working with Image Data” https://github.com/wolfganghuber/WorkingWithImageData, which goes through some of the same content as this chapter, but on different images, and also has additional examples on segmentation and optical flow.\n__\nExercise 11.5\nCompute and display the Voronoi tessellation for the Ukrainian cities from Chapter 9. Either use their MDS-coordinates in the 2D plane with Euclidean distances, or the latitudes and longitudes using the great circle distance (Haversine formula).\n__\nExercise 11.6\nDownload 3D image data from light sheet microscopy15, load it into an EBImage Image object and explore the data.\n15 For instance, http://www.digital-embryo.org\nBaddeley, Adrain, Jesper Moller, and Rasmus Waagepetersen. 2000. “Non- and Semiparametric Estimation of Interaction in Inhomogeneous Point Patterns.” Statistica Neerlandica 54: 329–50.\nBaddeley, Adrian J. 1998. “Spatial Sampling and Censoring.” In Stochastic Geometry: Likelihood and Computation , edited by O. E. Barndorff-Nielsen, W. S. Kendall, and M. N. M. van Lieshout, 37–78. Chapman; Hall.\nBerg, Stuart, Dominik Kutra, Thorben Kroeger, Christoph N Straehle, Bernhard X Kausler, Carsten Haubold, Martin Schiegg, et al. 2019. “Ilastik: Interactive Machine Learning for (Bio)image Analysis.” Nature Methods 16 (12): 1226–32.\nBoland, Michael V., and Robert F. Murphy. 2001. “A neural network classifier capable of recognizing the patterns of all major subcellular structures in fluorescence microscope images of HeLa cells.” Bioinformatics 17 (12): 1213–23.\nCarpenter, Anne E, Thouis R Jones, Michael R Lamprecht, Colin Clarke, In Han Kang, Ola Friman, David A Guertin, Joo Han Chang, Robert A Lindquist, and Jason Moffat. 2006. “CellProfiler: Image Analysis Software for Identifying and Quantifying Cell Phenotypes.” Genome Biology 7: R100.\nChaumont, Fabrice de, Stéphane Dallongeville, Nicolas Chenouard, Nicolas Hervé, Sorin Pop, Thomas Provoost, Vannary Meas-Yedid, et al. 2012. “Icy: an open bioimage informatics platform for extended reproducible research.” Nature Methods 9: 690–96.\nChiu, Sung Nok, Dietrich Stoyan, Wilfrid S. Kendall, and Joseph Mecke. 2013. Stochastic Geometry and Its Applications. Springer.\nCressie, Noel A. 1991. Statistics for Spatial Data. John Wiley; Sons.\nDiggle, Peter J. 2013. Statistical Analysis of Spatial and Spatio-Temporal Point Patterns. Chapman; Hall/CRCs.\nHeld, M., M. H. A. Schmitz, B. Fischer, T. Walter, B. Neumann, M. H. Olma, M. Peter, J. Ellenberg, and D. W. Gerlich. 2010. “CellCognition: Time-Resolved Phenotype Annotation in High-Throughput Live Cell Imaging.” Nature Methods 7: 747.\nHolmes, Susan, Adam Kapelner, and Peter P Lee. 2009. “An Interactive Java Statistical Image Segmentation System: GemIdent.” Journal of Statistical Software 30 (10).\nJones, T., A. Carpenter, and P. Golland. 2005. “Voronoi-Based Segmentation of Cells on Image Manifolds.” Computer Vision for Biomedical Image Applications , 535.\nLaufer, Christina, Bernd Fischer, Maximilian Billmann, Wolfgang Huber, and Michael Boutros. 2013. “Mapping genetic interactions in human cancer cells with RNAi and multiparametric phenotyping.” Nature Methods 10: 427–31.\nPau, Grégoire, Florian Fuchs, Oleg Sklyar, Michael Boutros, and Wolfgang Huber. 2010. “EBImage R Package for Image Processing with Applications to Cellular Phenotypes.” Bioinformatics 26 (7): 979–81.\nRajaram, S., B. Pavie, L. F. Wu, and S. J. Altschuler. 2012. “PhenoRipper: software for rapidly profiling microscopy images.” Nature Methods 9: 635–37.\nRipley, B. D. 1988. Statistical Inference for Spatial Processes. Cambridge University Press.\nRuss, John C., and F. Brent Neal. 2015. The Image Processing Handbook. 7th ed. CRC Press;\nSchindelin, Johannes, Ignacio Arganda-Carreras, Erwin Frise, Verena Kaynig, Mark Longair, Tobias Pietzsch, Stephan Preibisch, et al. 2012. “Fiji: an open- source platform for biological-image analysis.” Nature Methods 9: 676–82.\nSerra, Jean. 1983. Image Analysis and Mathematical Morphology. Academic Press.\nSetiadi, A Francesca, Nelson C Ray, Holbrook E Kohrt, Adam Kapelner, Valeria Carcamo-Cavazos, Edina B Levic, Sina Yadegarynia, et al. 2010. “Quantitative, Architectural Analysis of Immune Cell Subsets in Tumor-Draining Lymph Nodes from Breast Cancer Patients and Healthy Lymph Nodes.” PLoS One 5 (8): e12420.\nVetterli, Martin, Jelena Kovačević, and Vivek Goyal. 2014. Foundations of Signal Processing. Cambridge University Press.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>11.1 이 장의 목표</span>"
    ]
  },
  {
    "objectID": "12-chap.html",
    "href": "12-chap.html",
    "title": "14  12.1 Goals for this chapter",
    "section": "",
    "text": "14.1 12.2 What are the data?\nIn a supervised learning setting, we have a yardstick or plumbline to judge how well we are doing: the response itself.\nA frequent question in biological and biomedical applications is whether a property of interest (say, disease type, cell type, the prognosis of a patient) can be “predicted”, given one or more other properties, called the predictors. Often we are motivated by a situation in which the property to be predicted is unknown (it lies in the future, or is hard to measure), while the predictors are known. The crucial point is that we learn the prediction rule from a set of training data in which the property of interest is also known. Once we have the rule, we can either apply it to new data, and make actual predictions of unknown outcomes; or we can dissect the rule with the aim of better understanding the underlying biology.\nCompared to unsupervised learning and what we have seen in Chapters 5, 7 and 9, where we do not know what we are looking for or how to decide whether our result is “right”, we are on much more solid ground with supervised learning: the objective is clearly stated, and there are straightforward criteria to measure how well we are doing.\nThe central issues in supervised learning 1 are overfitting and generalizability : did we just learn the training data “by heart” by constructing a rule that has 100% accuracy on the training data, but would perform poorly on any new data? Or did our rule indeed pick up some of the pertinent patterns in the system being studied, which will also apply to yet unseen new data? (Figure 12.1)\n1 Sometimes the term statistical learning is used, more or less exchangeably.\nFigure 12.1: An example for overfitting : two regression lines are fit to data in the \\((x, y)\\)-plane (black points). We can think of such a line as a rule that predicts the \\(y\\)-value, given an \\(x\\)-value. Both lines are smooth, but the fits differ in what is called their bandwidth , which intuitively can be interpreted their stiffness. The blue line seems overly keen to follow minor wiggles in the data, while the orange line captures the general trend but is less detailed. The effective number of parameters needed to describe the blue line is much higher than for the orange line. Also, if we were to obtain additional data, it is likely that the blue line would do a worse job than the orange line in modeling the new data. We’ll formalize these concepts –training error and test set error– later in this chapter. Although exemplified here with line fitting, the concept applies more generally to prediction models.\nIn this chapter we will:\nThe basic data structure for both supervised and unsupervised learning is (at least conceptually) a dataframe, where each row corresponds to an object and the columns are different features (usually numerical values) of the objects2. While in unsupervised learning we aim to find (dis)similarity relationships between the objects based on their feature values (e.g., by clustering or ordination), in supervised learning we aim to find a mathematical function (or a computational algorithm) that predicts the value of one of the features from the other features. Many implementations require that there are no missing values, whereas other methods can be made to work with some amount of missing data.\n2 This is a simplified description. Machine learning is a huge field, and lots of generalizations of this simple conceptual picture have been made. Already the construction of relevant features is an art by itself — we have seen examples with images of cells in Chapter 11, and more generally there are lots of possibilities to extract features from images, sounds, movies, free text, \\(…\\) Moreover, there is a variant of machine learning methods called kernel methods that do not need features at all; instead, kernel methods use distances or measures of similarity between objects. It may be easier, for instance, to define a measure of similarity between two natural language text objects than to find relevant numerical features to represent them. Kernel methods are beyond the scope of this book.\nThe feature that we select over all the others with the aim of predicting is called the objective or the response. Sometimes the choice is natural, but sometimes it is also instructive to reverse the roles, especially if we are interested in dissecting the prediction function for the purpose of biological understanding, or in disentangling correlations from causation.\nThe framework for supervised learning covers both continuous and categorical response variables. In the continuous case we also call it regression , in the categorical case, classification. It turns out that this distinction is not a detail, as it has quite far-reaching consequences for the choice of loss function (Section 12.5) and thus the choice of algorithm (Friedman 1997).\nThe first question to consider in any supervised learning task is how the number of objects compares to the number of predictors. The more objects, the better, and much of the hard work in supervised learning has to do with overcoming the limitations of having a finite (and typically, too small) training set.\nFigure 12.2: In supervised learning, we assign two different roles to our variables. We have labeled the explanatory variables \\(X\\) and the response variable(s) \\(Y\\). There are also two different sets of observations: the training set \\(X_\\) and \\(Y_\\) and the test set \\(X_v\\) and \\(Y_v\\). (The subscripts refer to alternative names for the two sets: “learning” and “validation”.)\n__\nTask\nGive examples where we have encountered instances of supervised learning with a categorical response in this book.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html#what-are-the-data",
    "href": "12-chap.html#what-are-the-data",
    "title": "14  12.1 Goals for this chapter",
    "section": "",
    "text": "14.1.1 12.2.1 Motivating examples\n\n14.1.1.1 Predicting diabetes type\nThe diabetes dataset (Reaven and Miller 1979) presents three different groups of diabetes patients and five clinical variables measured on them.\ndata(\"diabetes\", package = \"rrcov\")\nhead(diabetes)__\n\n\n    rw fpg glucose insulin sspg  group\n1 0.81  80     356     124   55 normal\n2 0.95  97     289     117   76 normal\n3 0.94 105     319     143  105 normal\n4 1.04  90     356     199  108 normal\n5 1.00  90     323     240  143 normal\n6 0.76  86     381     157  165 normal\nThe univariate distributions (more precisely, some density estimates of them) are shown in Figure 12.3.\nlibrary(\"reshape2\")\nggplot(melt(diabetes, id.vars = \"group\"), aes(x = value, col = group)) +\n geom_density() + facet_wrap( ~variable, ncol = 1, scales = \"free\") +\n theme(legend.position = \"bottom\")__\n\nFigure 12.3: We see already from the one-dimensional distributions that some of the individual variables could potentially predict which group a patient is more likely to belong to. Our goal is to combine variables to improve over such one-dimensional prediction models.\nThe variables are explained in the manual page of the dataset, and in the paper (Reaven and Miller 1979):\n\nrw: relative weight\nfpg: fasting plasma glucose\nglucose: area under the plasma glucose curve for the three hour oral glucose tolerance test (OGTT)\ninsulin: area under the plasma insulin curve for the OGTT\nsspg: steady state plasma glucose response\ngroup: normal, chemical diabetes and overt diabetes\n\n\n\n14.1.1.2 Predicting cellular phenotypes\nNeumann et al. (2010) observed human cancer cells using live-cell imaging. The cells were genetically engineered so that their histones were tagged with a green fluorescent protein (GFP). A genome- wide RNAi library was applied to the cells, and for each siRNA perturbation, movies of a few hundred cells were recorded for about two days, to see what effect the depletion of each gene had on cell cycle, nuclear morphology and cell proliferation. Their paper reports the use of an automated image classification algorithm that quantified the visual appearance of each cell’s nucleus and enabled the prediction of normal mitosis states or aberrant nuclei. The algorithm was trained on the data from around 3000 cells that were annotated by a human expert. It was then applied to almost 2 billions images of nuclei (Figure 12.4). Using automated image classification provided scalability (annotating 2 billion images manually would take a long time) and objectivity.\n\nFigure 12.4: The data were images of \\(2^9\\) nuclei from movies. The images were segmented to identify the nuclei, and numeric features were computed for each nucleus, corresponding to size, shape, brightness and lots of other more or less abstract quantitative summaries of the joint distribution of pixel intensities. From the features, the cells were classified into 16 different nuclei morphology classes, represented by the rows of the barplot. Representative images for each class are shown in black and white in the center column. The class frequencies, which are very unbalanced, are shown by the lengths of the bars.\n\n\n14.1.1.3 Predicting embryonic cell states\nWe will revisit the mouse embryo data (Ohnishi et al. 2014), which we have already seen in Chapters 3, 5 and 7. We’ll try to predict cell state and genotype from the gene expression measurements in Sections 12.3.2 and 12.6.3.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html#linear-discrimination",
    "href": "12-chap.html#linear-discrimination",
    "title": "14  12.1 Goals for this chapter",
    "section": "14.2 12.3 Linear discrimination",
    "text": "14.2 12.3 Linear discrimination\nWe start with one of the simplest possible discrimination problems3: we have objects described by two continuous features (so the objects can be thought of as points in the 2D plane) and falling into three groups. Our aim is to define class boundaries, which are lines in the 2D space.\n3 Arguably the simplest possible problem is a single continuous feature, two classes, and the task of finding a single threshold to discriminate between the two groups – as in Figure 6.2.\n\n14.2.1 12.3.1 Diabetes data\nLet’s see whether we can predict the group from the sspg and glucose variables in the diabetes data. It’s always a good idea to first visualise the data (Figure 12.5).\nggdb = ggplot(mapping = aes(x = sspg, y = glucose)) +\n  geom_point(aes(colour = group), data = diabetes)\nggdb __\n\nFigure 12.5: Scatterplot of two of the variables in the diabetes data. Each point is a sample, and the color indicates the diabetes type as encoded in the group variable.\nWe’ll start with a method called linear discriminant analysis (LDA). This method is a foundation stone of classification, many of the more complicated (and sometimes more powerful) algorithms are really just generalizations of LDA.\nlibrary(\"MASS\")\ndiabetes_lda = lda(group ~ sspg + glucose, data = diabetes)\ndiabetes_lda __\n\n\nCall:\nlda(group ~ sspg + glucose, data = diabetes)\n\nPrior probabilities of groups:\n   normal  chemical     overt \n0.5241379 0.2482759 0.2275862 \n\nGroup means:\n             sspg   glucose\nnormal   114.0000  349.9737\nchemical 208.9722  493.9444\novert    318.8788 1043.7576\n\nCoefficients of linear discriminants:\n                LD1         LD2\nsspg    0.005036943 -0.01539281\nglucose 0.005461400  0.00449050\n\nProportion of trace:\n   LD1    LD2 \n0.9683 0.0317 \n\n\nghat = predict(diabetes_lda)$class\ntable(ghat, diabetes$group)__\n\n\n          \nghat       normal chemical overt\n  normal       69       12     1\n  chemical      7       24     6\n  overt         0        0    26\n\n\nmean(ghat != diabetes$group)__\n\n\n[1] 0.1793103\n__\nQuestion 12.1\nWhat do the different parts of the above output mean?\nNow, let’s visualise the LDA result. We are going to plot the prediction regions for each of the three groups. We do this by creating a grid of points and using our prediction rule on each of them. We’ll then also dig a bit deeper into the mechanics of LDA and plot the class centers (diabetes_lda$means) and ellipses that correspond to the fitted covariance matrix (diabetes_lda$scaling). Assembling this visualization requires us to write a bit of code.\nmake1Dgrid = function(x) {\n  rg = grDevices::extendrange(x)\n  seq(from = rg[1], to = rg[2], length.out = 100)\n}__\nSet up the points for prediction, a \\(100 \\) grid that covers the data range.\ndiabetes_grid = with(diabetes,\n  expand.grid(sspg = make1Dgrid(sspg),\n              glucose = make1Dgrid(glucose)))__\nDo the predictions.\ndiabetes_grid$ghat =\n  predict(diabetes_lda, newdata = diabetes_grid)$class __\nThe group centers.\ncenters = diabetes_lda$means __\nCompute the ellipse. We start from a unit circle (approximated by a polygon with 360 sides) and apply the corresponding affine transformation from the LDA output.\nunitcircle = exp(1i * seq(0, 2*pi, length.out = 360)) |&gt;\n          (\\(z) cbind(Re(z), Im(z)))() \nellipse = unitcircle %*% solve(diabetes_lda$scaling) |&gt; as_tibble()__\nAll three ellipses, one for each group center.\nlibrary(\"dplyr\")\nellipses = lapply(rownames(centers), function(gr) {\n  mutate(ellipse,\n     sspg    = sspg    + centers[gr, \"sspg\"],\n     glucose = glucose + centers[gr, \"glucose\"],\n     group   = gr)\n}) |&gt; bind_rows()__\nNow we are ready to plot (Figure 12.6).\nggdb + geom_raster(aes(fill = ghat),\n            data = diabetes_grid, alpha = 0.25, interpolate = TRUE) +\n    geom_point(data = as_tibble(centers), pch = \"+\", size = 8) +\n    geom_path(aes(colour = group), data = ellipses) +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0))__\n\nFigure 12.6: As Figure 12.5, with the classification regions from the LDA model shown. The three ellipses represent the class centers and the covariance matrix of the LDA model; note that there is only one covariance matrix, which is the same for all three classes. Therefore also the sizes and orientations of the ellipses are the same for the three classes, only their centers differ. They represent contours of equal class membership probability.\n__\nQuestion 12.2\nWhy is the boundary between the prediction regions for chemical and overt not perpendicular to the line between the group centers?\n__\nSolution\n__\nThe boundaries would be perpendicular if the ellipses were circles. In general, a boundary is tangential to the contours of equal class probabilities, and due the elliptic shape of the contours, a boundary is in general not perpendicular to the line between centers.\n__\nQuestion 12.3\nHow confident would you be about the predictions in those areas of the 2D plane that are far from all of the cluster centers?\n__\nSolution\n__\nPredictions that are far from any cluster center should be assessed critically, as this amounts to an extrapolation into regions where the LDA model may not be very good and/or there may be no training data nearby to support the prediction. We could use the distance to the nearest center as a measure of confidence in the prediction for any particular point; although we will see that resampling and cross-validation based methods offer more generic and usually more reliable measures.\n__\nQuestion 12.4\nWhy is the boundary between the prediction regions for normal and chemical not half-way between the centers, but shifted in favor of normal? Hint: have a look at the prior argument of lda. Try again with uniform prior.\n__\nSolution\n__\nThe result of the following code chunk is shown in Figure 12.7. The suffix _up is short for “uniform prior”.\ndiabetes_up = lda(group ~ sspg + glucose, data = diabetes,\n  prior = (\\(n) rep(1/n, n)) (nlevels(diabetes$group)))\n\ndiabetes_grid$ghat_up =\n  predict(diabetes_up, newdata = diabetes_grid)$class\n\nstopifnot(all.equal(diabetes_up$means, diabetes_lda$means))\n\nellipse_up  = unitcircle %*% solve(diabetes_up$scaling) |&gt; as_tibble()\nellipses_up = lapply(rownames(centers), function(gr) {\n  mutate(ellipse_up,\n     sspg    = sspg    + centers[gr, \"sspg\"],\n     glucose = glucose + centers[gr, \"glucose\"],\n     group   = gr)\n}) |&gt; bind_rows()\n\nggdb + geom_raster(aes(fill = ghat_up),\n            data = diabetes_grid, alpha = 0.4, interpolate = TRUE) +\n    geom_point(data = data.frame(centers), pch = \"+\", size = 8) +\n    geom_path(aes(colour = group), data = ellipses_up) +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0))__\n\nFigure 12.7: As Figure 12.6, but with uniform class priors.\nThe stopifnot line confirms that the class centers are the same, as they are independent of the prior. The joint covariance is not.\n__\nQuestion 12.5\nFigures 12.6 and 12.7 show both the fitted LDA model, through the ellipses, and the prediction regions, through the area coloring. What part of this visualization is generic for all sorts of classification methods, what part is method-specific?\n__\nSolution\n__\nThe prediction regions can be shown for any classification method, including a “black box” method. The cluster centers and ellipses in Figures 12.6 and 12.7 are method-specific.\n__\nQuestion 12.6\nWhat is the difference in the prediction accuracy if we use all 5 variables instead of just glucose and sspg?\n__\nSolution\n__\ndiabetes_lda5 = lda(group ~ rw + fpg + glucose + sspg + insulin, data = diabetes)\ndiabetes_lda5 __\n\n\nCall:\nlda(group ~ rw + fpg + glucose + sspg + insulin, data = diabetes)\n\nPrior probabilities of groups:\n   normal  chemical     overt \n0.5241379 0.2482759 0.2275862 \n\nGroup means:\n                rw       fpg   glucose     sspg  insulin\nnormal   0.9372368  91.18421  349.9737 114.0000 172.6447\nchemical 1.0558333  99.30556  493.9444 208.9722 288.0000\novert    0.9839394 217.66667 1043.7576 318.8788 106.0000\n\nCoefficients of linear discriminants:\n                  LD1          LD2\nrw       1.3624356881 -3.784142444\nfpg     -0.0336487883  0.036633317\nglucose  0.0125763942 -0.007092017\nsspg     0.0042431866  0.001134070\ninsulin -0.0001022245 -0.006173424\n\nProportion of trace:\n   LD1    LD2 \n0.8812 0.1188 \n\n\nghat5 = predict(diabetes_lda5)$class\ntable(ghat5, diabetes$group)__\n\n\n          \nghat5      normal chemical overt\n  normal       73        5     1\n  chemical      3       31     5\n  overt         0        0    27\n\n\nmean(ghat5 != diabetes$group)__\n\n\n[1] 0.09655172\n__\nQuestion 12.7\nInstead of approximating the prediction regions by classification from a grid of points, compute the separating lines explicitly from the linear determinant coefficients.\n__\nSolution\n__\nSee Section 4.3, Equation (4.10) in (Hastie, Tibshirani, and Friedman 2008).\n\n\n14.2.2 12.3.2 Predicting embryonic cell state from gene expression\nAssume that we already know that the four genes FN1 , TIMD2 , GATA4 and SOX7 are relevant to the classification task4. We want to build a classifier that predicts the developmental time (embryonic days: E3.25, E3.5, E4.5). We load the data and select four corresponding probes.\n4 Later in this chapter we will see methods that can drop this assumption and screen all available features.\nlibrary(\"Hiiragi2013\")__\n\n\nIn chunk 'loadHiiragi2': Warning: replacing previous import 'boot::logit' by 'gtools::logit' whenloading 'Hiiragi2013'\n\n\nIn chunk 'loadHiiragi2': Warning: replacing previous import 'boot::inv.logit' by 'gtools::inv.logit'when loading 'Hiiragi2013'\n\n\ndata(\"x\")\nprobes = c(\"1426642_at\", \"1418765_at\", \"1418864_at\", \"1416564_at\")\nembryoCells = t(Biobase::exprs(x)[probes, ]) |&gt; as_tibble() |&gt;\n  mutate(Embryonic.day = x$Embryonic.day) |&gt;\n  dplyr::filter(x$genotype == \"WT\")__\nWe can use the Bioconductor annotation package associated with the microarray to verify that the probes correspond to the intended genes.\nannotation(x)__\n\n\n[1] \"mouse4302\"\n\n\nlibrary(\"mouse4302.db\")\nanno = AnnotationDbi::select(mouse4302.db, keys = probes,\n                             columns = c(\"SYMBOL\", \"GENENAME\"))\nanno __\n\n\n     PROBEID SYMBOL                                            GENENAME\n1 1426642_at    Fn1                                       fibronectin 1\n2 1418765_at  Timd2 T cell immunoglobulin and mucin domain containing 2\n3 1418864_at  Gata4                              GATA binding protein 4\n4 1416564_at   Sox7                SRY (sex determining region Y)-box 7\n\n\nmt = match(anno$PROBEID, colnames(embryoCells))\ncolnames(embryoCells)[mt] = anno$SYMBOL __\nNow we are ready to visualize the data in a pairs plot (Figure 12.8).\nlibrary(\"GGally\")\nggpairs(embryoCells, mapping = aes(col = Embryonic.day),\n  columns = anno$SYMBOL, upper = list(continuous = \"points\"))__\n\nFigure 12.8: Expression values of the discriminating genes, with the prediction target Embryonic.day shown by color.\nWe can now call lda on these data. The linear combinations LD1 and LD2 that serve as discriminating variables are given in the slot ed_lda$scaling of the output from lda.\nec_lda = lda(Embryonic.day ~ Fn1 + Timd2 + Gata4 + Sox7,\n             data = embryoCells)\nround(ec_lda$scaling, 1)__\n\n\n       LD1  LD2\nFn1   -0.2  0.4\nTimd2  0.5  0.0\nGata4 -0.1  0.6\nSox7  -0.7 -0.5\nFor the visualization of the learned model in Figure 12.9, we need to build the prediction regions and their boundaries by expanding the grid in the space of the two new coordinates LD1 and LD2.\nec_rot = predict(ec_lda)$x |&gt; as_tibble() |&gt;\n           mutate(ed = embryoCells$Embryonic.day)\nec_lda2 = lda(ec_rot[, 1:2], predict(ec_lda)$class)\nec_grid = with(ec_rot, expand.grid(\n  LD1 = make1Dgrid(LD1),\n  LD2 = make1Dgrid(LD2)))\nec_grid$edhat = predict(ec_lda2, newdata = ec_grid)$class\nggplot() +\n  geom_point(aes(x = LD1, y = LD2, colour = ed), data = ec_rot) +\n  geom_raster(aes(x = LD1, y = LD2, fill = edhat),\n            data = ec_grid, alpha = 0.4, interpolate = TRUE) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  coord_fixed()__\n\nFigure 12.9: LDA classification regions for Embryonic.day.\n__\nQuestion 12.8\nRepeat these analyses using quadratic discriminant analysis (qda). What difference do you see in the shape of the boundaries?\n__\nSolution\n__\nSee code below and Figure 12.10.\nlibrary(\"gridExtra\")\n\nec_qda = qda(Embryonic.day ~ Fn1 + Timd2 + Gata4 + Sox7,\n             data = embryoCells)\n\nvariables = colnames(ec_qda$means)\npairs = combn(variables, 2)\nlapply(seq_len(ncol(pairs)), function(i) {\n  grid = with(embryoCells,\n    expand.grid(x = make1Dgrid(get(pairs[1, i])),\n                y = make1Dgrid(get(pairs[2, i])))) |&gt;\n    `colnames&lt;-`(pairs[, i])\n\n  for (v in setdiff(variables, pairs[, i]))\n    grid[[v]] = median(embryoCells[[v]])\n\n  grid$edhat = predict(ec_qda, newdata = grid)$class\n\n  x &lt;- pairs[1,i]\n  y &lt;- pairs[2,i]\n  ggplot() + \n    geom_point(\n      data = embryoCells,\n      aes(x = .data[[x]], y = .data[[y]], colour = Embryonic.day)\n    ) +\n    geom_raster(\n      aes(x = .data[[x]], y = .data[[y]], fill = edhat),\n      data = grid, alpha = 0.4, interpolate = TRUE\n    ) +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0)) +\n    coord_fixed() +\n    if (i != ncol(pairs)) theme(legend.position = \"none\")\n}) |&gt; (\\(g) grid.arrange(grobs = g, ncol = 2))()__\n\nFigure 12.10: QDA for the mouse cell data. Shown are all pairwise plots of the four features. In each plot, the other two features are set to the median.\n__\nQuestion 12.9\nWhat happens if you call lda or qda with a lot more genes, say the first 1000, in the Hiiragi dataset?\n__\nSolution\n__\nlda(t(Biobase::exprs(x))[, 1:1000], x$Embryonic.day)__\n\n\n warnings()\nqda(t(Biobase::exprs(x))[, 1:1000], x$Embryonic.day)__\n\n\nError in qda.default(x, grouping, ...): some group is too small for 'qda'\nThe lda function manages to fit a model, but complains (with the warning) about the fact that there are more variables than replicates, which means that the variables are not linearly independent, and thus are redundant of each other. The qda function aborts with an error, since the QDA model with so many parameters cannot be fitted from the available data (at least, without making further assumptions, such as some sort of regularization, which it is not equipped for).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html#machine-learning-vs-rote-learning",
    "href": "12-chap.html#machine-learning-vs-rote-learning",
    "title": "14  12.1 Goals for this chapter",
    "section": "14.3 12.4 Machine learning vs rote learning",
    "text": "14.3 12.4 Machine learning vs rote learning\nComputers are really good at memorizing facts. In the worst case, a machine learning algorithm is a roundabout way of doing this5. The central goal in statistical learning, however, is generalizability. We want an algorithm that is able to generalize, i.e., interpolate and extrapolate from given data to make good predictions about future data.\n5 The not-so roundabout way is database technologies.\nLet’s look at the following example. We generate random data (rnorm) for n objects, with different numbers of features (given by p). We train a LDA on these data and compute the misclassification rate , i.e., the fraction of times the prediction is wrong (pred != resp).\np = 2:21\nn = 20\n\nmcl = lapply(p, function(pp) {\n  replicate(100, {\n    xmat = matrix(rnorm(n * pp), nrow = n)\n    resp = sample(c(\"apple\", \"orange\"), n, replace = TRUE)\n    fit  = lda(xmat, resp)\n    pred = predict(fit)$class\n    mean(pred != resp)\n  }) |&gt; mean() |&gt; (\\(x) tibble(mcl = x, p = pp))()\n}) |&gt; bind_rows()__\n\n\n ggplot(mcl, aes(x = p, y = mcl)) + \n  geom_line() + geom_point() +\n  ylab(\"Misclassification rate\")__\n\nFigure 12.11: Misclassification rate of LDA applied to random data. While the number of observations n is held constant (at 20), we are increasing the number of features p starting from 2 up to 21. The misclassification rate becomes almost zero as p approaches 20. The LDA model becomes so elaborate and over-parameterized that it manages to learn the random labels “by heart”. (As p becomes even larger, the “performance” degrades again somewhat, apparently due to numerical properties of the lda implementation used here.)\n__\nQuestion 12.10\nWhat is the purpose of the replicate loop in the above code? What happens if you omit it (or replace the 100 by 1)?\n__\nSolution\n__\nFor each single replicate, the curve is a noisier version of Figure 12.11. Averaging the measured misclassifications rate over 100 replicates makes the estimate more stable. We can do this since we are working with simulated data.\nFigure 12.11 seems to imply that we can perfectly predict random labels from random data, if we only fit a complex enough model, i.e., one with many parameters. How can we overcome such an absurd conclusion? The problem with the above code is that the model performance is evaluated on the same data on which it was trained. This generally leads to positive bias, as you see in this crass example. How can we overcome this problem? The key idea is to assess model performance on different data than those on which the model was trained.\n\n14.3.1 12.4.1 Cross-validation\nA naive approach might be to split the data in two halves, and use the first half for learning (“training”) and the second half for assessment (“testing”). It turns out that this is needlessly variable and needlessly inefficient. It is needlessly variable, since by splitting the data only once, our results can be quite affected by how the split happens to fall. It seems better to do the splitting many times, and average. This will give us more stable results. It is needlessly inefficient, since the performance of machine learning algorithms depends on the number of observations, and the performance measured on half the data is likely6 to be worse than what it is with all the data. For this reason, it is better to use unequal sizes of training and test data. In the extreme case, we’ll use as much as \\(n-1\\) observations for training, and the remaining one for testing. After we’ve done this likewise for all observations, we can average our performance metric. This is called leave- one-out cross-validation.\n6 Unless we have such an excess of data that it doesn’t matter.\n\nSee Chapter Model Assessment and Selection in the book by Hastie, Tibshirani, and Friedman (2008) for further discussion on these trade-offs.\nAn alternative is \\(k\\) -fold cross-validation, where the observations are repeatedly split into a training set of size of around \\(n(k-1)/k\\) and a test set of size of around \\(n/k\\). Both alternatives have pros and contras, and there is not a universally best choice. An advantage of leave- one-out is that the amount of data used for training is close to the maximally available data; this is especially important if the sample size is limiting and “every little matters” for the algorithm. A drawback of leave-one-out is that the training sets are all very similar, so they may not model sufficiently well the kind of sampling changes to be expected if a new dataset came along. For large \\(n\\), leave-one-out cross-validation can be needlessly time-consuming.\nestimate_mcl_loocv = function(x, resp) {\n  vapply(seq_len(nrow(x)), function(i) {\n    fit  = lda(x[-i, ], resp[-i])\n    ptrn = predict(fit, newdata = x[-i,, drop = FALSE])$class\n    ptst = predict(fit, newdata = x[ i,, drop = FALSE])$class\n    c(train = mean(ptrn != resp[-i]), test = (ptst != resp[i]))\n  }, FUN.VALUE = numeric(2)) |&gt; rowMeans() |&gt; t() |&gt; as_tibble()\n}\n\nxmat = matrix(rnorm(n * last(p)), nrow = n)\nresp = sample(c(\"apple\", \"orange\"), n, replace = TRUE)\n\nmcl = lapply(p, function(k) {\n  estimate_mcl_loocv(xmat[, 1:k], resp)\n}) |&gt; bind_rows() |&gt; data.frame(p) |&gt; melt(id.var = \"p\")__\n\n\n ggplot(mcl, aes(x = p, y = value, col = variable)) + geom_line() +\n  geom_point() + ylab(\"Misclassification rate\")__\n\nFigure 12.12: Cross-validation: the misclassification rate of LDA applied to random data, when evaluated on test data that were not used for learning, hovers around 0.5 independent of p. The misclassification rate on the training data is also shown. It behaves similar to what we already saw in Figure 12.11.\nThe result is show in Figure 12.12.\n__\nQuestion 12.11\nWhy are the curves in Figure 12.12 more variable (“wiggly”) than in Figure 12.11? How can you overcome this?\n__\nSolution\n__\nOnly one dataset (xmat, resp) was used to calculate Figure 12.12, whereas for Figure 12.11, we had the data generated within a replicate loop. You could similarly extend the above code to average the misclassification rate curves over many replicate simulated datasets.\n\n\n14.3.2 12.4.2 The curse of dimensionality\nIn Section 12.4.1 we have seen overfitting and cross-validation on random data, but how does it look if there is in fact a relevant class separation?\np   = 2:20\nmcl = replicate(100, {\n  xmat = matrix(rnorm(n * last(p)), nrow = n)\n  resp = sample(c(\"apple\", \"orange\"), n, replace = TRUE)\n  xmat[, 1:6] = xmat[, 1:6] + as.integer(factor(resp))\n\n  lapply(p, function(k) {\n    estimate_mcl_loocv(xmat[, 1:k], resp)\n  }) |&gt; bind_rows() |&gt; cbind(p = p) |&gt; melt(id.var = \"p\")\n}, simplify = FALSE) |&gt; bind_rows()__\n\n\nmcl = group_by(mcl, p, variable) |&gt; summarise(value = mean(value))\n\nggplot(mcl, aes(x = p, y = value, col = variable)) + geom_line() +\n   geom_point() + ylab(\"Misclassification rate\")__\n\nFigure 12.13: As we increase the number of features included in the model, the misclassification rate initially improves; as we start including more and more irrelevant features, it increases again, as we are fitting noise.\n\nFigure 12.14: Idealized version of Figure 12.13, from Hastie, Tibshirani, and Friedman (2008). A recurrent goal in machine learning is finding the sweet spot in the variance &lt;- bias trade- off.\nThe result is shown in Figure 12.13. The group centers are the vectors (in \\(^{20}\\)) given by the coordinates \\((1, 1, 1, 1, 1, 1, 0, 0, 0, …)\\) (apples) and \\((2, 2, 2, 2, 2, 2, 0, 0, 0, …)\\) (oranges), and the optimal decision boundary is the hyperplane orthogonal to the line between them. For \\(p\\) smaller than \\(6\\), the decision rule cannot reach this hyperplane – it is biased. As a result, the misclassification rate is suboptimal, and it decreases with \\(p\\). But what happens for \\(p\\) larger than \\(6\\)? The algorithm is, in principle, able to model the optimal hyperplane, and it should not be distracted by the additional features. The problem is that it is. The more additional features enter the dataset, the higher the probability that one or more of them happen to fall in a way that they look like good, discriminating features in the training data – only to mislead the classifier and degrade its performance in the test data. Shortly we’ll see how to use penalization to (try to) control this problem.\nThe term curse of dimensionality was coined by Bellman (1961). It refers to the fact that high- dimensional spaces are very hard, if not impossible, to sample thoroughly: for instance, to cover a 2-dimensional square of side length 1 with grid points that are 0.1 apart, we need \\(10^2=100\\) points. In 100 dimensions, we need \\(10^{100}\\) – which is more than the number of protons in the universe. In genomics, we often aim to fit models to data with thousands of features. Also our intuitions about distances between points or about the relationship between a volume and its surface break down in a high-dimensional settings. We’ll explore some of the weirdnesses of high-dimensional spaces in the next few questions.\n__\nQuestion 12.12\nAssume you have a dataset with 1 000 000 data points in \\(p\\) dimensions. The data are uniformly distributed in the unit hybercube (i.e., all features lie in the interval \\([0,1]\\)). What’s the side length of a hybercube that can be expected to contain just 10 of the points, as a function of \\(p\\)?\n__\nSolution\n__\nSee Figure 12.15.\nsideLength = function(p, pointDensity = 1e6, pointsNeeded = 10)\n  (pointsNeeded / pointDensity) ^ (1 / p)\nggplot(tibble(p = 1:400, sideLength = sideLength(p)),\n       aes(x = p, y = sideLength)) + geom_line(col = \"red\") +\n  geom_hline(aes(yintercept = 1), linetype = 2)__\n\nFigure 12.15: Side length of a \\(p\\)-dimensional hybercube expected to contain 10 points out of 1 million uniformly distributed ones, as a function of the \\(p\\). While for \\(p=1\\), this length is conveniently small, namely \\(10/106=10{-5}\\), for larger \\(p\\) it approaches 1, i.,e., becomes the same as the range of each the features. This means that a “local neighborhood” of 10 points encompasses almost the same data range as the whole dataset.\nNext, let’s look at the relation between inner regions of the feature space versus its boundary regions. Generally speaking, prediction at the boundaries of feature space is more difficult than in its interior, as it tends to involve extrapolation, rather than interpolation. In the next question you’ll see how this difficulty explodes with feature space dimension.\n__\nQuestion 12.13\nWhat fraction of a unit cube’s total volume is closer than 0.01 to any of its surfaces, as a function of the dimension?\n__\nSolution\n__\nSee code below and Figure 12.16.\ntibble(\n  p = 1:400,\n  volOuterCube = 1 ^ p,\n  volInnerCube = 0.98 ^ p,  # 0.98 = 1 - 2 * 0.01\n  `V(shell)` = volOuterCube - volInnerCube) |&gt;\nggplot(aes(x = p, y =`V(shell)`)) + geom_line(col = \"blue\")__\n\nFigure 12.16: Fraction of a unit cube’s total volume that is in its “shell” (here operationalised as those points that are closer than 0.01 to its surface) as a function of the dimension \\(p\\).\n__\nQuestion 12.14\nWhat is the coefficient of variation (ratio of standard deviation over average) of the distance between two randomly picked points in the unit hypercube, as a function of the dimension?\n__\nSolution\n__\nWe solve this one by simulation. We generate n pairs of random points in the hypercube (x1, x2) and compute their Euclidean distances. See Figure 12.17. This result can also be predicted from the central limit theorem.\nn = 1000\ndf = tibble(\n  p = round(10 ^ seq(0, 4, by = 0.25)),\n  cv = vapply(p, function(k) {\n    x1 = matrix(runif(k * n), nrow = n)\n    x2 = matrix(runif(k * n), nrow = n)\n    d = sqrt(rowSums((x1 - x2)^2))\n    sd(d) / mean(d)\n  }, FUN.VALUE = numeric(1)))\nggplot(df, aes(x = log10(p), y = cv)) + geom_line(col = \"orange\") +\n  geom_point()__\n\nFigure 12.17: Coefficient of variation (CV) of the distance between randomly picked points in the unit hypercube, as a function of the dimension. As the dimension increases, everybody is equally far away from everyone else: there is almost no variation in the distances any more.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html#objective-functions",
    "href": "12-chap.html#objective-functions",
    "title": "14  12.1 Goals for this chapter",
    "section": "14.4 12.5 Objective functions",
    "text": "14.4 12.5 Objective functions\nWe’ve already seen the misclassification rate (MCR) used to assess our classification performance in Figures 12.11–12.13. Its population version is defined as\n\\[ = , = , \\]\nand for a finite sample\n\\[ = {i=1}^n 𝟙{ y_i}. \\]\nThis is not the only choice we could make. Perhaps we care more about the misclassification of apples as oranges than vice versa, and we can reflect this by introducing weights that depend on the type of error made into the sum of Equation 12.2 (or the integral of Equation 12.1). This can get even more elaborate if we have more than two classes. Often we want to see the whole confusion table , which we can get via\ntable(truth, response)__\nAn important special case is binary classification with asymmetric costs – think about, say, a medical test. Here, the sensitivity (a.k.a. true positive rate or recall) is related to the misclassification of healthy as ill, and the specificity (or true negative rate) depends on the probability of misclassification of ill as healthy. Often, there is a single parameter (e.g., a threshold) that can be moved up and down, allowing a trade- off between sensitivity and specificity (and thus, equivalently, between the two types of misclassification). In those cases, we usually are not content to know the classifier performance at one single choice of threshold, but at many (or all) of them. This leads to receiver operating characteristic (ROC) or precision-recall curves.\n__\nQuestion 12.15\nWhat are the exact relationships between the per-class misclassification rates and sensitivity and specificity?\n__\nSolution\n__\nThe sensitivity or true positive rate is\n\\[ = , \\]\nwhere \\(\\) is the number of true positives and \\(\\) the number of all positives. The specificity or true negative rate is\n\\[ = , \\]\nwhere \\(\\) is the number of true negatives and \\(\\) the number of all negatives. See also https://en.wikipedia.org/wiki/Sensitivity_and_specificity\nAnother cost function can be computed from the Jaccard index , which we already saw in Chapter 5.\n\\[ J(A,B) = , \\]\nwhere \\(A\\) is the set of observations for which the true class is 1 (\\(A=\\{i,|,y_i=1\\}\\)) and \\(B\\) is the set of observations for which the predicted class is 1. The number \\(J\\) is between 0 and 1, and when \\(J\\) is large, it indicates high overlap of the two sets. Note that \\(J\\) does not depend on the number of observations for which both true and predicted class is 0 – so it is particularly suitable for measuring the performance of methods that try to find rare events.\nWe can also consider probabilistic class predictions, which come in the form \\((Y,|,X)\\). In this case, a possible risk function would be obtained by looking at distances between the true probability distribution and the estimated probability distributions. For two classes, the finite sample version of the \\(\\) is\n\\[ = -_{i=1}^n y_i(_i) + (1 - y_i)(1 - _i), \\]\nwhere \\(_i \\) is the prediction, and \\(y_i\\{0,1\\}\\) is the truth.\n\n\n\nNote that the \\log\\text{loss} will be infinite if a prediction is totally confident (\\hat{p}_i is exactly 0 or 1) but wrong.\n\n\nNote that the \\(\\) will be infinite if a prediction is totally confident (\\(_i\\) is exactly \\(0\\) or \\(1\\)) but wrong.\nFor continuous continuous response variables (regression), a natural choice is the mean squared error (MSE). It is the average squared error,\n\\[ = _{i=1}^n ( _i - Y_i )^2. \\]\nThe population version is defined analogously, by turning the summation into an integral as in Equations 12.1 and 12.2.\nStatisticians call functions like Equations 12.1—12.5 variously (and depending on context and predisposition) risk function , cost function , objective function 7.\n7 There is even an R package dedicated to evaluation of statistical learners called metrics.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html#variancebias-trade-off",
    "href": "12-chap.html#variancebias-trade-off",
    "title": "14  12.1 Goals for this chapter",
    "section": "14.5 12.6 Variance–bias trade-off",
    "text": "14.5 12.6 Variance–bias trade-off\n\n\n\nFigure 12.18 (a):\n\n\n\n\n\n\n\n\nFigure 12.18 (b):\n\n\n\n\n\nFigure 12.18: In bull’s eye (a), the estimates are systematically off target, but in a quite reproducible manner. The green segment represents the bias. In bull’s eye (b), the estimates are not biased, as they are centered in the right place, however they have high variance. We can distinguish the two scenarios since we see the result from many shots. If we only had one shot and missed the bull’s eye, we could not easily know whether that’s because of bias or variance.\nAn important fact that helps us understand the tradeoffs when picking a statistical learning model is that the MSE is the sum of two terms, and often the choices we can make are such that one of those terms goes down while the other one goes up. The bias measures how different the average of all the different estimates is from the truth, and variance, how much an individual one might scatter from the average value (Figure 12.18). In applications, we often only get one shot, therefore being reliably almost on target can beat being right on the long term average but really off today. The decomposition\n\\[ = {} + {} \\]\nfollows by straightforward algebra.\nWhen trying to minimize the MSE, it is important to realize that sometimes we can pay the price of a small bias to greatly reduce variance, and thus overall improve MSE. We already encountered shrinkage estimation in Chapter 8. In classification (i.e., when we have categorical response variables), different objective functions than the MSE are used, and there is usually no such straightforward decomposition as in Equation 12.6. The good news is that we can usually go even much further than in the case of continuous responses with our trading biases for variance. This is because the discreteness of the response absorbs certain biases (Friedman 1997), so that the cost of higher bias is almost zero, while we still get the benefit of better (smaller) variance.\n\n14.5.1 12.6.1 Penalization\nIn high-dimensional statistics, we are constantly plagued by variance: there is just not enough data to fit all the possible parameters. One of the most fruitful ideas in high-dimensional statistics is penalization : a tool to actively control and exploit the variance-bias tradeoff. Penalization is part of a larger class of regularization methods that are used to ensure stable estimates.\nAlthough generalization of LDA to high-dimensional settings is possible (Clemmensen et al. 2011; Witten and Tibshirani 2011), it turns out that logistic regression is a more general approach8, and therefore we’ll now switch to that, using the glmnet package.\n8 It fits into the framework of generalized linear models, which we encountered in Chapter 8.\nFor multinomial—or, for the special case of two classes, binomial—logistic regression models, the posterior log-odds between \\(k\\) classes and can be written in the form (see the section on Logistic Regression in the book by Hastie, Tibshirani, and Friedman (2008) for a more complete presentation):\n\\[ = ^0_i + _i x, \\]\nwhere \\(i=1,…,k-1\\) enumerates the different classes and the \\(k\\)-th class is chosen as a reference. The data matrix \\(x\\) has dimensions \\(np\\), where \\(n\\) is number of observations and \\(p\\) the number of features. The \\(p\\)-dimensional vector \\(_i\\) determines how the classification odds for class \\(i\\) versus class \\(k\\) depend on \\(x\\). The numbers \\(^0_i\\) are intercepts and depend, among other things, on the classes’ prior probabilities. Instead of the log odds 12.7 (i.e., ratios of class probabilities), we can also write down an equivalent model for the class probabilities themselves, and the fact that we here used the \\(k\\)-th class as a reference is an arbitrary choice, as the model estimates are equivariant under this choice (Hastie, Tibshirani, and Friedman 2008). The model is fit by maximising the log-likelihood \\((, ^0; x)\\), where \\(=(1,…,{k-1})\\) and analogously for \\(^0\\).\nSo far, so good. But as \\(p\\) gets larger, there is an increasing chance that some of the estimates go wildly off the mark, due to random sampling happenstances in the data (remember Figure 12.1). This is true even if for each individual coordinate of the vector \\(_i\\), the error distribution is bounded: the probabilty of there being one coordinate that is in the far tails increases the more coordiates there are, i.e., the larger \\(p\\) is.\nA related problem can also occur, not in 12.7, but in other, non-linear models, as the model dimension \\(p\\) increases while the sample size \\(n\\) remains the same: the likelihood landscape around its maximum becomes increasingly flat, and the maximum-likelihood estimate of the model parameters becomes more and more variable. Eventually, the maximum is no longer a point, but a submanifold, and the maximum likelihood estimate is unidentifiable. Both of these limitations can be overcome with a modification of the objective: instead of maximising the bare log-likelihood, we maximise a penalized version of it,\n\\[ = _(, ^0; x) + (), \\]\nwhere \\(\\) is a real number, and \\(\\) is a convex function, called the penalty function. Popular choices are \\(()=||^2\\) (ridge regression) and \\(()=||^1\\) (lasso).\n\n\n\nHere, |\\beta|\\nu=\\sum_i\\beta_i\\nu is the L_\\nu-norm of the vector \\beta. Variations are possible, for instead we could include in this summation only some but not all of the elements of \\beta; or we could scale different elements differently, for instance based on some prior belief of their scale and importance.\n\n\nHere, \\(||^=_ii^\\) is the \\(L\\)-norm of the vector \\(\\). Variations are possible, for instead we could include in this summation only some but not all of the elements of \\(\\); or we could scale different elements differently, for instance based on some prior belief of their scale and importance.\nIn the elastic net , ridge and lasso are hybridized by using the penalty function \\(()=(1-)||1+||2\\) with some further parameter \\(\\). The crux is, of course, how to choose the right \\(\\), and we will discuss that in the following.\n\n\n14.5.2 12.6.2 Example: predicting colon cancer from stool microbiome composition\nZeller et al. (2014) studied metagenome sequencing data from fecal samples of 156 humans that included colorectal cancer patients and tumor-free controls. Their aim was to see whether they could identify biomarkers (presence or abundance of certain taxa) that could help with early tumor detection. The data are available from Bioconductor through its ExperimentHub service under the identifier EH361.\nlibrary(\"ExperimentHub\")\neh = ExperimentHub()\nzeller = eh[[\"EH361\"]]__\n\n\ntable(zeller$disease)__\n\n\n       cancer large_adenoma             n small_adenoma \n           53            15            61            27 \n__\nQuestion 12.16\nExplore the eh object to see what other datasets there are.\n__\nSolution\n__\nType eh into the R prompt and study the output.\nFor the following, let’s focus on the normal and cancer samples and set the adenomas aside.\nzellerNC = zeller[, zeller$disease %in% c(\"n\", \"cancer\")]__\nBefore jumping into model fitting, as always it’s a good idea to do some exploration of the data. First, let’s look at the sample annotations. The following code prints the data from three randomly picked samples. (Only looking at the first ones, say with the R function head, is also an option, but may not be representative of the whole dataset).\npData(zellerNC)[ sample(ncol(zellerNC), 3), ]__\n\n\n                   subjectID age gender bmi country disease tnm_stage\nCCIS50148151ST-4-0    FR-503  87 female  15  france  cancer    t2n1m0\nCCIS16383318ST-4-0    FR-139  61 female  24  france       n      &lt;NA&gt;\nCCIS95097901ST-4-0    FR-696  52   male  24  france       n      &lt;NA&gt;\n                   ajcc_stage localization     fobt wif-1_gene_methylation_test\nCCIS50148151ST-4-0        iii           rc negative                    negative\nCCIS16383318ST-4-0       &lt;NA&gt;         &lt;NA&gt; negative                    negative\nCCIS95097901ST-4-0       &lt;NA&gt;         &lt;NA&gt; negative                    negative\n                     group bodysite ethnicity number_reads\nCCIS50148151ST-4-0     crc    stool     white     54709150\nCCIS16383318ST-4-0 control    stool     white     78085760\nCCIS95097901ST-4-0 control    stool     white     51567166\nNext, let’s explore the feature names:\n\n\n\nWe define the helper function formatfn to line wrap these long character strings for the available space here.\n\n\nWe define the helper function formatfn to line wrap these long character strings for the available space here.\nformatfn = function(x)\n   gsub(\"|\", \"| \", x, fixed = TRUE) |&gt; lapply(strwrap)\n\nrownames(zellerNC)[1:4]__\n\n\n[1] \"k__Bacteria\"                  \"k__Viruses\"                  \n[3] \"k__Bacteria|p__Firmicutes\"    \"k__Bacteria|p__Bacteroidetes\"\n\n\nrownames(zellerNC)[nrow(zellerNC) + (-2:0)] |&gt; formatfn()__\n\n\n[[1]]\n[1] \"k__Bacteria| p__Proteobacteria| c__Deltaproteobacteria|\"         \n[2] \"o__Desulfovibrionales| f__Desulfovibrionaceae| g__Desulfovibrio|\"\n[3] \"s__Desulfovibrio_termitidis\"                                     \n\n[[2]]\n[1] \"k__Viruses| p__Viruses_noname| c__Viruses_noname| o__Viruses_noname|\"\n[2] \"f__Baculoviridae| g__Alphabaculovirus|\"                              \n[3] \"s__Bombyx_mori_nucleopolyhedrovirus|\"                                \n[4] \"t__Bombyx_mori_nucleopolyhedrovirus_unclassified\"                    \n\n[[3]]\n[1] \"k__Bacteria| p__Proteobacteria| c__Deltaproteobacteria|\"         \n[2] \"o__Desulfovibrionales| f__Desulfovibrionaceae| g__Desulfovibrio|\"\n[3] \"s__Desulfovibrio_termitidis| t__GCF_000504305\"                   \nAs you can see, the features are a mixture of abundance quantifications at different taxonomic levels, from k ingdom over p hylum to s pecies. We could select only some of these, but here we continue with all of them. Next, let’s look at the distribution of some of the features. Here, we show an arbitrary choice of two, number 510 and 527; in practice, it is helpful to scroll through many such plots quickly to get an impression (Figure 12.19).\nggplot(melt(Biobase::exprs(zellerNC)[c(510, 527), ]), aes(x = value)) +\n    geom_histogram(bins = 25) +\n    facet_wrap( ~ Var1, ncol = 1, scales = \"free\")__\n\nFigure 12.19: Histograms of the distributions for two randomly selected features. The distributions are highly skewed, with many zero values and a thin, long tail of non-zero values.\nIn the simplest case, we fit model 12.7 as follows.\nlibrary(\"glmnet\")\nglmfit = glmnet(x = t(Biobase::exprs(zellerNC)),\n                y = factor(zellerNC$disease),\n                family = \"binomial\")__\nA remarkable feature of the glmnet function is that it fits 12.7 not only for one choice of \\(\\), but for all possible \\(\\)s at once. For now, let’s look at the prediction performance for, say, \\(\\). The name of the function parameter is s:\npredTrsf = predict(glmfit, newx = t(Biobase::exprs(zellerNC)),\n                   type = \"class\", s = 0.04)\ntable(predTrsf, zellerNC$disease)__\n\n\n        \npredTrsf cancer  n\n  cancer     51  0\n  n           2 61\nNot bad – but remember that this is on the training data, without cross- validation. Let’s have a closer look at glmfit. The glmnet package offers a a diagnostic plot that is worth looking at (Figure 12.20).\nplot(glmfit, col = RColorBrewer::brewer.pal(8, \"Dark2\"), lwd = sqrt(3), ylab = \"\")__\n\nFigure 12.20: Regularization paths for glmfit.\n__\nQuestion 12.17\nWhat are the \\(x\\)- and \\(y\\)-axes in Figure 12.20? What are the different lines?\n__\nSolution\n__\nConsult the manual page of the function plot.glmnet in the glmnet package.\nLet’s get back to the question of how to choose the parameter \\(\\). We could try many different choices –and indeed, all possible choices– of \\(\\), assess classification performance in each case using cross- validation, and then choose the best \\(\\).\n\n\n\nYou’ll already realize from the description of this strategy that if we optimize \\lambda in this way, the resulting apparent classification performance will likely be exaggerated. We need a truly independent dataset, or at least another, outer cross-validation loop to get a more realistic impression of the generalizability. We will get back to this question at the end of the chapter.\n\n\nYou’ll already realize from the description of this strategy that if we optimize \\(\\) in this way, the resulting apparent classification performance will likely be exaggerated. We need a truly independent dataset, or at least another, outer cross-validation loop to get a more realistic impression of the generalizability. We will get back to this question at the end of the chapter.\nWe could do so by writing a loop as we did in the estimate_mcl_loocv function in Section 12.4.1. It turns out that the glmnet package already has built-in functionality for that, with the function cv.glmnet, which we can use instead.\ncvglmfit = cv.glmnet(x = t(Biobase::exprs(zellerNC)),\n                     y = factor(zellerNC$disease),\n                     family = \"binomial\")\nplot(cvglmfit)__\n\nFigure 12.21: Diagnostic plot for cv.glmnet: shown is a measure of cross- validated prediction performance, the deviance, as a function of \\(\\). The dashed vertical lines show lambda.min and lambda.1se.\nThe diagnostic plot is shown in Figure 12.21. We can access the optimal value with\ncvglmfit$lambda.min __\n\n\n[1] 0.0529391\nAs this value results from finding a minimum in an estimated curve, it turns out that it is often too small, i.e., that the implied penalization is too weak. A heuristic recommended by the authors of the glmnet package is to use a somewhat larger value instead, namely the largest value of \\(\\) such that the performance measure is within 1 standard error of the minimum.\ncvglmfit$lambda.1se __\n\n\n[1] 0.08830775\n__\nQuestion 12.18\nHow does the confusion table look like for \\(=;\\)lambda.1se?\n__\nSolution\n__\ns0 = cvglmfit$lambda.1se\npredict(glmfit, newx = t(Biobase::exprs(zellerNC)),type = \"class\", s = s0) |&gt;\n    table(zellerNC$disease)__\n\n\n        \n         cancer  n\n  cancer     38  5\n  n          15 56\n__\nQuestion 12.19\nWhat features drive the classification?\n__\nSolution\n__\ncoefs = coef(glmfit)[, which.min(abs(glmfit$lambda - s0))]\ntopthree = order(abs(coefs), decreasing = TRUE)[1:3]\nas.vector(coefs[topthree])__\n\n\n[1] -71.471393  -8.770704  -1.465249\n\n\nformatfn(names(coefs)[topthree])__\n\n\n[[1]]\n[1] \"k__Bacteria| p__Candidatus_Saccharibacteria|\"      \n[2] \"c__Candidatus_Saccharibacteria_noname|\"            \n[3] \"o__Candidatus_Saccharibacteria_noname|\"            \n[4] \"f__Candidatus_Saccharibacteria_noname|\"            \n[5] \"g__Candidatus_Saccharibacteria_noname|\"            \n[6] \"s__candidate_division_TM7_single_cell_isolate_TM7b\"\n\n[[2]]\n[1] \"k__Bacteria| p__Firmicutes| c__Clostridia| o__Clostridiales|\"        \n[2] \"f__Ruminococcaceae| g__Subdoligranulum| s__Subdoligranulum_variabile\"\n\n[[3]]\n[1] \"k__Bacteria| p__Firmicutes| c__Clostridia| o__Clostridiales|\"\n[2] \"f__Lachnospiraceae| g__Lachnospiraceae_noname|\"              \n[3] \"s__Lachnospiraceae_bacterium_7_1_58FAA\"                      \n__\nQuestion 12.20\nHow do the results change if we transform the data, say, with the asinh transformation as we saw in Chapter 5?\n__\nSolution\n__\nSee Figure 12.22.\ncv.glmnet(x = t(asinh(Biobase::exprs(zellerNC))),\n          y = factor(zellerNC$disease),\n          family = \"binomial\") |&gt; plot()__\n\nFigure 12.22: like Figure 12.21, but using an \\(\\) transformation of the data.\n__\nQuestion 12.21\nWould a good classification performance on these data mean that this assay is ready for screening and early cancer detection?\n__\nSolution\n__\nNo. The performance here is measured on a set of samples in which the cases have similar prevalence as the controls. This serves well enough to explore the biology. However, in a real-life application, the cases will be much less frequent. To be practically useful, the assay must have a much higher specificity, i.e., rarely diagnose disease where there is none. To establish specificity, a much larger set of normal samples need to be tested.\n\n\n14.5.3 12.6.3 Example: classifying mouse cells from their expression profiles\nFigures 12.21 and 12.22 are textbook examples of how we expect the dependence of (cross-validated) classification performance versus model complexity (\\(\\)) to look. Now let’s get back to the mouse embryo cells data. We’ll try to classify the cells from embryonic day E3.25 with respect to their genotype.\nsx = x[, x$Embryonic.day == \"E3.25\"]\nembryoCellsClassifier = cv.glmnet(t(Biobase::exprs(sx)), sx$genotype,\n                family = \"binomial\", type.measure = \"class\")\nplot(embryoCellsClassifier)__\n\nFigure 12.23: Cross-validated misclassification error versus penalty parameter for the mouse cells data.\nIn Figure 12.23 we see that the misclassification error is (essentially) monotonously increasing with \\(\\), and is smallest for \\(\\), i.e., if we apply no penalization at all.\n__\nQuestion 12.22\nWhat is going on with these data?\n__\nSolution\n__\nIt looks that inclusion of more, and even of all features, does not harm the classification performance. In a way, these data are “too easy”. Let’s do a \\(t\\)-test for all features:\nmouse_de = rowttests(sx, \"genotype\")\nggplot(mouse_de, aes(x = p.value)) +\n  geom_histogram(boundary = 0, breaks = seq(0, 1, by = 0.01))__\n\nFigure 12.24: Histogram of p-values for the per-feature \\(t\\)-tests between genotypes in the E3.25 cells.\nThe result, shown in Figure 12.24, shows that large number of genes are differentially expressed, and thus informative for the class distinction. We can also compute the pairwise distances between all cells, using all features.\ndists = as.matrix(dist(scale(t(Biobase::exprs(x)))))\ndiag(dists) = +Inf __\nand then for each cell determine the class of its nearest neighbor\nnn = sapply(seq_len(ncol(dists)), function(i) which.min(dists[, i]))\ntable(x$sampleGroup, x$sampleGroup[nn]) |&gt; `colnames&lt;-`(NULL)__\n\n\n                 \n                  [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n  E3.25             33    0    0    0    3    0    0    0\n  E3.25 (FGF4-KO)    1   15    0    1    0    0    0    0\n  E3.5 (EPI)         2    0    3    0    6    0    0    0\n  E3.5 (FGF4-KO)     0    0    0    8    0    0    0    0\n  E3.5 (PE)          0    0    0    0   11    0    0    0\n  E4.5 (EPI)         0    0    0    0    2    2    0    0\n  E4.5 (FGF4-KO)     1    0    0    0    0    0    9    0\n  E4.5 (PE)          0    0    0    0    2    0    0    2\nUsing all features, the 1 nearest-neighbor classifier is correct in almost all cases, including for the E3.25 wildtype vs FGF4-KO distinction. This means that for these data, there is no apparent benefit in regularization or feature selection. Limitations of using all features might become apparent with truly new data, but that is out of reach for cross-validation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html#a-large-choice-of-methods",
    "href": "12-chap.html#a-large-choice-of-methods",
    "title": "14  12.1 Goals for this chapter",
    "section": "14.6 12.7 A large choice of methods",
    "text": "14.6 12.7 A large choice of methods\nWe have now seen three classification methods: linear discriminant analysis (lda), quadratic discriminant analysis (qda) and logistic regression using elastic net penalization (glmnet). In fact, there are hundreds of different learning algorithms9 available in R and its add-on packages. You can get an overview in the CRAN task view Machine Learning & Statistical Learning. Some examples are:\n9 For an introduction to the subject that uses R and provides many examples and exercises, we recommend (James et al. 2013).\n\nSupport vector machines: the function svm in the package e1071 ; ksvm in kernlab\nTree based methods in the packages rpart , tree , randomForest\nBoosting methods: the functions glmboost and gamboost in package mboost\nPenalizedLDA in the package PenalizedLDA , dudi.discr and dist.pcaiv in ade4).\n\nThe complexity and heterogeneity of choices of learning strategies, tuning parameters and evaluation criteria in each of these packages can be confusing. You will already have noted differences in the interfaces of the lda, qda and glmnet functions, i.e., in how they expect their input data to presented and what they return. There is even greater diversity across all the other packages and functions. At the same time, there are common tasks such as cross-validation, parameter tuning and performance assessment that are more or less the same no matter what specific method is used. As you have seen, e.g., in our estimate_mcl_loocv function, the looping and data shuffling involved led to rather verbose code.\nSo what to do if you want to try out and explore different learning algorithms? Fortunately, there are several projects that provide unified interfaces to the large number of different machine learning interfaces in R, and also try to provide “best practice” implementations of the common tasks such as parameter tuning and performance assessment. The two most well-known ones are the packages caret and mlr. Here were have a look at caret. You can get a list of supported methods through its getModelInfo function. There are quite a few, here we just show the first 8.\nlibrary(\"caret\")\ncaretMethods = names(getModelInfo())\nhead(caretMethods, 8)__\n\n\n[1] \"ada\"         \"AdaBag\"      \"AdaBoost.M1\" \"adaboost\"    \"amdai\"      \n[6] \"ANFIS\"       \"avNNet\"      \"awnb\"       \n\n\nlength(caretMethods)__\n\n\n[1] 239\nWe will check out a neural network method, the nnet function from the eponymous package. The parameter slot informs us on the the available tuning parameters10.\n10 They are described in the manual of the nnet function.\ngetModelInfo(\"nnet\", regex = FALSE)[[1]]$parameter __\n\n\n  parameter   class         label\n1      size numeric #Hidden Units\n2     decay numeric  Weight Decay\nLet’s try it out.\ntrnCtrl = trainControl(\n  method = \"repeatedcv\",\n  repeats = 3,\n  classProbs = TRUE)\ntuneGrid = expand.grid(\n  size = c(2, 4, 8),\n  decay = c(0, 1e-2, 1e-1))\nnnfit = train(\n  Embryonic.day ~ Fn1 + Timd2 + Gata4 + Sox7,\n  data = embryoCells,\n  method = \"nnet\",\n  tuneGrid  = tuneGrid,\n  trControl = trnCtrl,\n  metric = \"Accuracy\")__\nThat’s quite a mouthful, but the nice thing is that this syntax is standardized and applies across many different methods. All you need to do specify the name of the method and the grid of tuning parameters that should be explored via the tuneGrid argument.\nNow we can have a look at the output (Figure 12.25).\nnnfit __\n\n\nNeural Network \n\n66 samples\n 4 predictor\n 3 classes: 'E3.25', 'E3.5', 'E4.5' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 60, 59, 60, 60, 59, 59, ... \nResampling results across tuning parameters:\n\n  size  decay  Accuracy   Kappa    \n  2     0.00   0.7083333  0.4279755\n  2     0.01   0.7676587  0.5858922\n  2     0.10   0.7681349  0.5767463\n  4     0.00   0.7515476  0.5638426\n  4     0.01   0.8004762  0.6486256\n  4     0.10   0.7638889  0.5676798\n  8     0.00   0.7385714  0.5393148\n  8     0.01   0.7348016  0.5281220\n  8     0.10   0.7532540  0.5525435\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were size = 4 and decay = 0.01.\n\n\nplot(nnfit)\npredict(nnfit) |&gt; head(10)__\n\n\n [1] E3.25 E3.25 E3.25 E3.25 E3.25 E3.25 E3.25 E3.25 E3.25 E3.25\nLevels: E3.25 E3.5 E4.5\n\nFigure 12.25: Parameter tuning of the neural net by cross-validation.\n__\nQuestion 12.23\nWill the accuracy that we obtained above for the optimal tuning parameters generalize to a new dataset? What could you do to address that?\n__\nSolution\n__\nNo, it is likely to be too optimistic, as we have picked the optimum. To get a somewhat more realistic estimate of prediction performance when generalized, we could formalize (into computer code) all our data preprocessing choices and the above parameter tuning procedure, and embed this in another, outer cross- validation loop (Ambroise and McLachlan 2002). However, this is likely still not enough, as we discuss in the next section.\n\n14.6.1 12.7.1 Method hacking\nIn Chapter 6 we encountered p-value hacking. A similar phenomenon exists in statistical learning: given a dataset, we explore various different methods of preprocessing (such as normalization, outlier detection, transformation, feature selection), try out different machine learning algorithms and tune their parameters until we are content with the result. The measured accuracy is likely to be too optimistic, i.e., will not generalize to a new dataset. Embedding as many of our methodical choices into a computational formalism and having an outer cross-validation loop (not to be confused with the inner loop that does the parameter tuning) will ameliorate the problem. But is unlikely to address it completely, since not all our choices can be formalized.\nThe gold standard remains validation on truly unseen data. In addition, it is never a bad thing if the classifier is not a black box but can be interpreted in terms of domain knowledge. Finally, report not just summary statistics, such as misclassification rates, but lay open the complete computational workflow, so that anyone (including your future self) can convince themselves of the robustness of the result or of the influence of the preprocessing, model selection and tuning choices (Holmes 2018).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html#summary-of-this-chapter",
    "href": "12-chap.html#summary-of-this-chapter",
    "title": "14  12.1 Goals for this chapter",
    "section": "14.7 12.8 Summary of this chapter",
    "text": "14.7 12.8 Summary of this chapter\nWe have seen examples of machine learning applications; we have focused on predicting categorical variables (like diabetes type or cell class). Predicting continuous outcomes is also part of machine learning, although we have not considered it here. There are many parallels and overlaps between machine learning and statistical regression (which we studied in Chapter 8). One can consider them two different names for pretty much the same activity, although each has its own flavors: in machine learning, the emphasis is on the prediction of the outcome variables, whereas in regression we often care at least as much about the role of the covariates – which of them have an effect on the outcome, and what is the nature of these effects? In other words, we do not only want predictions, we also want to understand them.\nWe saw linear and quadratic discriminant analysis, two intuitive methods for partitioning a two-dimensional data plane (or a \\(p\\)-dimensional space) into regions using either linear or quadratic separation lines (or hypersurfaces). We also saw logistic regression, which takes a slightly different approach but is more amenable to operating in higher dimensions and to regularization.\nWe encountered the main challenge of machine learning: how to avoid overfitting? We explored why overfitting happens in the context of the so- called curse of dimensionality, and we learned how it may be overcome using regularization.\nIn other words, machine learning would be easy if we had infinite amounts of data representatively covering the whole space of possible inputs and outputs11. The challenge is to make the best out of a finite amount of training data, and to generalize these to new, unseen inputs. There is a vigorous trade-off between the amount, resolution and coverage of training data and the complexity of the model. Many models have continuous parameters that enable us to “tune” their complexity or the strength of their regularization. Cross-validation can help us with such tuning, although it is not a panacea, and caveats apply, as we saw in Section 12.6.3.\n11 It would “just” be a formidable database / data management problem.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html#further-reading",
    "href": "12-chap.html#further-reading",
    "title": "14  12.1 Goals for this chapter",
    "section": "14.8 12.9 Further reading",
    "text": "14.8 12.9 Further reading\n\nAn introduction to statistical learning that employs many concrete data examples and uses little mathematical formalism is given by James et al. (2013). An extension, with more mathematical background, is the textbook by Hastie, Tibshirani, and Friedman (2008).\nThe CRAN task view on machine learning gives an overview over machine learning software in R.\nRStudio’s API for the “deep learning” platforms Keras and TensorFlow and the associated teaching materials and demos are a good place to try out some of the recent developments in this field.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "12-chap.html#exercises",
    "href": "12-chap.html#exercises",
    "title": "14  12.1 Goals for this chapter",
    "section": "14.9 12.10 Exercises",
    "text": "14.9 12.10 Exercises\n__\nExercise 12.1\nApply a kernel support vector machine , available in the kernlab package, to the zeller microbiome data. What kernel function works well?\n__\nExercise 12.2\nUse glmnet for a prediction of a continuous variable , i.e., for regression. Use the prostate cancer data from Chapter 3 of (Hastie, Tibshirani, and Friedman 2008). The data are available in the CRAN package ElemStatLearn. Explore the effects of using ridge versus lasso penalty.\n__\nExercise 12.3\nConsider smoothing as a regression and model selection problem (remember Figure 12.1). What is the equivalent quantity to the penalization parameter \\(\\) in Equation 12.8? How do you choose it?\n__\nSolution\n__\nWe refer to Chapter 5 of (Hastie, Tibshirani, and Friedman 2008)\n__\nExercise 12.4\nScale invariance. Consider a rescaling of one of the features in the (generalized) linear model 12.7. For instance, denote the \\(\\)-th column of \\(x\\) by \\(x_{}\\), and suppose that \\(p\\) and that we rescale \\(x_{} s, x_{}\\) with some number \\(s\\). What will happen to the estimate \\(\\) from Equation 12.8 in (a) the unpenalized case (\\(\\)) and (b) the penalized case (\\(&gt;0\\))?\n__\nSolution\n__\nIn the unpenalized case, the estimates will be scaled by \\(1/s\\), so that the resulting model is, in effect, the same. In the penalized case, the penalty from the \\(\\)-th component of \\(\\) will be different. If \\(|s|&gt;1\\), the amplitude of the feature is increased, smaller \\(\\)-components are required for it to have the same effect in the prediction, and therefore the feature is more likely to receive a non-zero and/or larger estimate, possibly on the cost of the other features; conversely for \\(|s|&lt;1\\). Regular linear regression is scale-invariant, whereas penalized regression is scale-dependent. It’s important to remember this when interpreting penalized model fits.\n__\nExercise 12.5\nIt has been quipped that all classification methods are just refinements of two archetypal ideas : discriminant analysis and \\(k\\) nearest neighbors. In what sense might that be a useful classification?\n__\nSolution\n__\nIn linear discriminant analysis, we consider our objects as elements of \\(^p\\), and the learning task is to define regions in this space, or boundary hyperplanes between them, which we use to predict the class membership of new objects. This is archetypal for classification by partition. Generalizations of linear discriminant analysis permit more general spaces and more general boundary shapes.\nIn \\(k\\) nearest neighbors, no embedding into a coordinate space is needed, but instead we require a distance (or dissimilarity) measure that can be computed between each pair of objects, and the classification decision for a new object depends on its distances to the training objects and their classes. This is archetypal for kernel-based methods.\nAmbroise, Christophe, and Geoffrey J. McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” PNAS 99 (10): 6562–66.\nBellman, Richard Ernest. 1961. Adaptive Control Processes: A Guided Tour. Princeton University Press.\nClemmensen, Line, Trevor Hastie, Daniela Witten, and Bjarne Ersbøll. 2011. “Sparse Discriminant Analysis.” Technometrics 53: 406–13.\nFriedman, Jerome H. 1997. “On Bias, Variance, 0/1—Loss, and the Curse-of- Dimensionality.” Data Mining and Knowledge Discovery 1: 55–77.\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2008. The Elements of Statistical Learning. 2^{} ed. Springer.\nHolmes, Susan. 2018. “Statistical Proof? The Problem of Irreproducibility.” Bulletin of the AMS 55 (1): 31–55.\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Springer.\nNeumann, B., T. Walter, J. K. Heriche, J. Bulkescher, H. Erfle, C. Conrad, P. Rogers, et al. 2010. “Phenotypic profiling of the human genome by time-lapse microscopy reveals cell division genes.” Nature 464 (7289): 721–27.\nOhnishi, Y., W. Huber, A. Tsumura, M. Kang, P. Xenopoulos, K. Kurimoto, A. K. Oles, et al. 2014. “Cell-to-Cell Expression Variability Followed by Signal Reinforcement Progressively Segregates Early Mouse Lineages.” Nature Cell Biology 16 (1): 27–37.\nReaven, GM, and RG Miller. 1979. “An Attempt to Define the Nature of Chemical Diabetes Using a Multidimensional Analysis.” Diabetologia 16 (1): 17–24.\nWitten, Daniela M, and Robert Tibshirani. 2011. “Penalized Classification Using Fisher’s Linear Discriminant.” JRSSB 73 (5): 753–72.\nZeller, Georg, Julien Tap, Anita Y Voigt, Shinichi Sunagawa, Jens Roat Kultima, Paul I Costea, Aurélien Amiot, et al. 2014. “Potential of Fecal Microbiota for Early-Stage Detection of Colorectal Cancer.” Molecular Systems Biology 10 (11): 766. https://doi.org/10.15252/msb.20145645.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>12.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html",
    "href": "13-chap.html",
    "title": "15  13.1 Goals for this chapter",
    "section": "",
    "text": "15.1 13.2 Types of experiments\nIn the same way a film director will view daily takes to correct potential lighting or shooting issues before they affect too much footage, it is a good idea not to wait until all the runs of an experiment have been finished before looking at the data. Intermediate data analyses and visualizations will track unexpected sources of variation and enable you to adjust the protocol. Much is known about sequential design of experiments (Mead 1990), but even in a more pragmatic setting it is important to be aware of sources of variation as they occur and adjust for them.\nWe have now seen many different biological datasets and data types, and methods for analyzing them. To conclude this book, we recapitulate some of the general lessons we learned. Three great pieces of good advice are:\n1 Presidential Address to the First Indian Statistical Congress, 1938. Sankhya 4, 14-17.\nIn this chapter we will:",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#types-of-experiments",
    "href": "13-chap.html#types-of-experiments",
    "title": "15  13.1 Goals for this chapter",
    "section": "",
    "text": "15.1.0.1 The art of “good enough”.\nWe need experimental design in order to deal with the fact that our resources are finite, our instruments not perfect, and that the real world is complicated. We want to get the the best possible outcome nonetheless. This invariably results in hard decisions and tradeoffs. Experimental design aims to rationalize such decisions. Our experimental interventions and our measurement instruments have limited precision and accuracy; often we don’t know these limitations at the outset and have to collect preliminary data to estimate them. We may only be able to observe the phenomenon of interest indirectly rather than directly. Our treatment conditions may have undesired but hard to avoid side effects, our measurements may be overlaid with interfering signals or “background noise”. Sample sizes are limited for practical and economic reasons. There is little point in prescribing unrealistic ideals – we need to make choices that are pragmatic and feasible. A quote from (Bacher and Kendziorski 2016) explains this clearly: “Generally speaking, a well-designed experiment is one that is sufficiently powered and one in which technical artifacts and biological features that may systematically affect measurements are balanced, randomized or controlled in some other way in order to minimize opportunities for multiple explanations for the effect(s) under study.”\nTo start with, let us discuss the major different types of experiments, since each of them requires different approaches.\nIn a controlled experiment , we have control over all relevant variables: the (model) system under study, the environmental conditions, the experimental readout. For instance, we could have a well-characterized cell line growing in laboratory conditions on defined media, temperature and atmosphere, we’ll administer a precise amount of a drug, and after 72h we measure the activity of a specific pathway reporter.\nIn a study , we have less control: important conditions that may affect the measured outcome are not under control of the researcher, usually because of ethical concerns or logistical constraints. For instance, in an ecological field study, this could be the weather, the availability of nutrition resources or the activity of predators. In an observational study , even the variable of interest is not controlled by the researcher. For instance, in a clinical trial, this might be the assignment of the individual subjects to groups. Since there are many possibilities for confounding (Section 13.4.1), interpretation of an observational study can be difficult. Here’s where the old adage “correlation is not causation” appertains.\nIn a randomized controlled trial , we still have to deal with lack of control over many of the factors that impact the outcome, but we control assignment of the variable of interest (say, the type of treatment in a clinical trial), therefore we can expect that –with high enough sample size– all the nuisance effects average out and the observed effect can really be causally assigned to the intervention. Such trials are usually prospective 2, i.e., the outcome is not known at the time of the assignment of the patients to the groups.\n2 The antonym is retrospective; observational studies can be prospective or retrospective.\nA meta-analysis is an observational study on several previous experiments or studies. One motivation of a meta-analysis is to increase power by increasing effective sample size. Another is to overcome the limitations of individual experiments or studies, which might suffer from researcher bias or other biases, be underpowered, or can otherwise be flawed or random. The hope is that by pooling results from many studies, such “study-level” problems average out.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#partitioning-error-bias-and-noise",
    "href": "13-chap.html#partitioning-error-bias-and-noise",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.2 13.3 Partitioning error: bias and noise",
    "text": "15.2 13.3 Partitioning error: bias and noise\n\n\n\nStatisticians use the term error for any deviations of a measured value from the true value. This is different from the everyday use of the word. In statistics, error is an unavoidable aspect of life. It is not “bad”, it is something to be cherished, reckoned with, tamed and controlled.\n\n\nStatisticians use the term error for any deviations of a measured value from the true value. This is different from the everyday use of the word. In statistics, error is an unavoidable aspect of life. It is not “bad”, it is something to be cherished, reckoned with, tamed and controlled.\nWe broadly distinguish between two types of error. The first, which we call noise , “averages out” if we just perform enough replicates. The second, which we call bias , remains; it even becomes more apparent with more replication. Recall the bull’s eye in Figure 12.18: in the lower panel, there is a lot of noise, but no bias, and the center of the cloud of points is in the right place. In the upper panel, there is much less noise, but bias. No amount of replication will remedy the fact that the center of the points is in the wrong place.\nBias is more difficult to deal with than noise: noise is easily recognized just from looking at replicates, and it averages out as we analyze more and more replicates. With bias, it can be hard to even recognize that it is there, and then we need to find ways to measure it and adjust for it, usually with some quantitative model.\n__\nQuestion 13.1\nGive two examples in previous chapters where we have modeled bias in high throughput data.\n__\nSolution\n__\nFor instance, in Chapter 8, we modeled the sampling noise with the gamma-Poisson distribution, we estimated sequencing depth bias with the library size factors and took it into account when testing for differential expression. We also modeled sampling biases caused by the two different protocols used (single-end, paired-end) by introducing a blocking factor into our generalized linear model.\n\n15.2.1 13.3.1 Error models: noise is in the eye of the beholder\nThe efficiency of most biochemical or physical processes involving DNA- polymers depends on their sequence content. For instance, occurrences of long homopolymer stretches, palindromes, overall or local GC content can modify the efficiency of PCR, or the dynamics of how the polymer is being pulled through a nanopore. The size and nature of such effects is challenging to model. They depend in subtle ways on factors like concentration, temperature, enzyme used, etc. So: when looking at RNA-Seq data, should we treat GC content as noise or as bias?\n__\nQuestion 13.2\nHow does the DESeq2 method address this issue?\n__\nSolution\n__\nDESeq2 offers both options. If size factors are used to model per-sample sampling bias, then such effects are not explicitly modeled.\nNote: The assumption is then that, for each gene, any such bias would effect the counts in the same way across all samples, so that for the purpose of differential expression analysis, it cancels out. To the extent that such effects are sample-specific, they are treated as noise. However, as described in its vignette, DESeq2 also allows specifying sample- and gene-dependent normalization factors for a matrix, and these are intended to contain explicit estimates of such biases.\nRemember that the noun sample here, by convention, refers to one column of the count matrix, e.g., one sequencing library corresponding to one replicate of one biological condition. The same term (here as the verb form sampling) is also used in its more general, statistical sense, as in “a sample of data from a distribution”. There is no easy way around this ambiguity, so we just need to be aware of it.\nFormal error models can help us decompose the variability into noise and bias. A standard decomposition you may have encountered is called ANOVA (ANalysis Of VAriance). In these types of models, variability is measured by sums of squares and aportioned according to its origin. For instance, when doing supervised classification in a linear discriminant analysis (LDA) in Chapter 12, we computed the total sum of squares \\(C\\) as\n\\[ C_{} = C_{} + C_{}. \\]\nHowever, there are usually multiple ways of doing such a decomposition: an effect that at one stage is considered within-group variation (noise) might be considered a between-groups effect once the right (sub)groups are assigned.\n\n\n\nMaybe this is akin to the vision of “personalized medicine”: better patient stratification that converts within group variation (incl. unsuccessful or unnecessary treatments) into between groups variation (where every group gets exactly what they need).\n\n\nMaybe this is akin to the vision of “personalized medicine”: better patient stratification that converts within group variation (incl. unsuccessful or unnecessary treatments) into between groups variation (where every group gets exactly what they need).\n\n15.2.1.1 Determinism versus chance.\n\nFigure 13.1: A carefully constructed coin tossing machine can be made to provide deterministic coin flips.\nEveryone thinks of the outcome of a coin toss as random, thus a perfect example of noise. But if we meticulously registered the initial conditions of the coin flip and solved the mechanical equations, we could predict which side has a higher probability of coming up (Diaconis, Holmes, and Montgomery 2007).\nSo, rather than asking whether a certain effect or process is random or deterministic, it is more fruitful to say whether we care to model it deterministically (as bias), or whether we ignore the details, treat it as stochastic, and use probabilistic modeling (noise). In this sense, probabilistic models are a way of quantifying our ignorance, taming our uncertainty.\n\n\n15.2.1.2 Latent factors.\nSometimes we explicitly know about factors that cause bias, for instance, when a different reagent batch was used in different phases of the experiments. We call this batch effects (Jeffrey T. Leek et al. 2010). At other times, we may expect that such factors are at work but have no explicit record of them. We call these latent factors. We can treat them as adding to the noise, and in Chapter 4 we saw how to use mixture models to do so. But this may not be enough: with high- dimensional data, noise caused by latent factors tends to be correlated, and this can lead to faulty inference (Jeffrey T. Leek et al. 2010). The good news is that these same correlations can be exploited to estimate latent factors from the data, model them as bias and thus reduce the noise (Jeffrey T. Leek and Storey 2007; Stegle et al. 2010).\n\n\n\n15.2.2 13.3.2 Biological versus technical replicates\n__\nQuestion 13.3\nImagine you want to test whether a weight loss drug works. Which of the following study designs would you use:\n\nA person is weighed on milligram precision scales, with 20 replicates. He follows the diet, and four weeks later, he is weighed again, with 20 replicates.\nTen people weigh themselves once on their bathroom scales and report the number. Four weeks later, they weigh themselves and report again.\n\nSurely the first option must be better since it has 20 replicates on a very precise instrument rather than only ten on an older piece of equipment?\n__\nSolution\n__\nWhat we have here is a (placative) instance of the difference between technical versus biological replicates. The number of replicates is less important than what types of variation are allowed to affect them. The 20 replicates in the first design are wasted on re-measuring something that we already know with more than enough precision. Whereas the far more important question –how does the effect generalize to different people– starts to be addressed with the second design, although in practice more people would be needed.\nNote: Inference or generalizations can only be made to a wider population if we have a representative, randomized sample of that population in our study. In the first case if a weight loss occurs, one could only infer about that person at that time.\nAnalogous questions arise in biological experimentation, e.g., do you rather do five replicates on the same cell line, or one replicate each on three different cell lines?\n__\nQuestion 13.4\nFor reliable variant calling with the sequencing technology used by the 1000 Genomes project, one needs about \\(30\\) coverage per genome. However, the average depth of the data produced was 5.1 for 1,092 individuals (1000 Genomes Project Consortium 2012). Why was that study design chosen?\n__\nSolution\n__\nThe project’s aim was finding common genetic variants, i.e., finding variants that have a prevalence of more than, say, 1% in the population. It was not to call high-confidence genotypes of individual people. Therefore, it was more cost-efficient to sample more individuals each with low coverage (say, 1092 individuals at 5x) than fewer individuals with high coverage (say, 182 at 30x). In this way, common variants would still be found with \\(&gt;=30\\) coverage (\\(1092 % = 55\\)), since they would be present in several of the 1000 people, but more of them would be found, and there would be more precise estimates of their population frequency.\nThe technical versus biological replicates terminology has some value, but is often too coarse. The observed effect may or may not be generalizable at many different levels: different labs, different operators within one lab, different technologies, different machines from the same technology, different variants of the protocol, different strains, litters, sexes, individual animals, and so forth. It’s better to name the levels of replication more explicitly.\n\n\n15.2.3 13.3.3 Units vs. fold-changes\nMeasurements in physics are usually reported as multiples of SI3 units, such as meters, kilograms, seconds. A length measured in meters by a lab in Australia using one instrument is directly comparable to one measured a year later by a lab in Canada using a different instrument, or by alien scientists in a far-away galaxy. In biology, it is rarely possible or practical to make measurements that are as standardized. The situation here is more like that where human body parts (feets, inches, etc.) are used for length measurements, and where the size of these body parts is even different in different towns and countries, let alone galaxies.\n3 International System of Units (French: Système International d’Unités)\nBiologists often report measurements as multipes of (i.e., fold changes with regard to) some local, more or less ad hoc reference. The challenge with this is that fold changes and proportions are ratios. The denominator is a random variable (as it changes from lab to lab and probably from experiment to experiment), which can create high instability and very unequal variances between experiments; see the sections on transformations and sufficiency a little later in this chapter. Even when seemingly absolute values exist (e.g., TPKM values in an RNA-Seq experiment), due to experiment-specific sampling biases they do not translate into universal units, and they often lack an indication of their precision.\n\n\n15.2.4 13.3.4 Regular and catastrophic noise\nRegular noise can be modelled by simple probability models such as independent normal distributions, Poissons, or mixtures such as gamma–Poisson or Laplace. We can use relatively straightforward methods to take such noise into account in our data analyses and to compute the probability of extraordinarily large or small values. In the real world, this is only part of the story: measurements can be completely off scale (a sample swap, a contamination or a software bug), and they can go awry all at the same time (a whole microtiter plate went bad, affecting all data measured from it). Such events are hard to model or even correct for – our best chance to deal with them is data quality assessment, outlier detection and documented removal.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#basic-principles-in-the-design-of-experiments",
    "href": "13-chap.html#basic-principles-in-the-design-of-experiments",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.3 13.4 Basic principles in the design of experiments",
    "text": "15.3 13.4 Basic principles in the design of experiments\n\n15.3.1 13.4.1 Confounding\n\nFigure 13.2: Comparison of a (hypothetical) biomarker between samples from disease and healthy states. If we are only given the information shown in the left panel, we might conclude that this biomarker performs well in detecting the disease. If, in addition, we are told that the data were acquired in two separate batches (e.g., different labs, different machines, different time points) as indicated in the panel on the right hand side, the conclusion will be different.\n__\nQuestion 13.5\nConsider the data shown in Figure 13.2. How can we decide whether the observed differences in the biomarker level are due to disease vs. healthy, or due to the batch?\n__\nSolution\n__\nIt is impossible to know from these data: the two variables are confounded.\nConfounding need not only be between a biological and a technical variable, it can also be more subtle. For instance, the biomarker might have nothing to do with the disease directly – it might just be a marker of a life style that causes the disease (as well as other things), or of an inflammation that is caused by the disease (as well as by many other things), etc.\n\n\n15.3.2 13.4.2 Effect size and replicates\n\nFigure 13.3: Confounding is the reason that one of the seven rules of experimental design listed by the Persian physician-scientist Abu ’Ali al- Husayn ibn Sina (Avicenna) around AD 1020 was “to study one possible cause of a disease at a time” (Stigler 2016).\nThe effect size is the difference between the group centers, as shown by the red arrow in Figure 13.4. A larger sample size in each group increases the precision with which the locations of each group and the effect size are known, thus increasing our power to detect a difference (Figure 13.5). On the other hand, the performance of the biomarker as a diagnostic for distinguishing individual samples between healthy and disease states depends on the within-group distributions (and the relative prevalences of both states), and is not improved by replication.\n\nFigure 13.4: The red arrow shows the effect size, as measured by the difference between the centers of the two groups. Here we locate the centers by the medians; sometimes the mean is used.\n\nFigure 13.5: On the left, the boxplot was created with samples of size 6. On the right the sample sizes are 60. The measurements have the same underlying error distribution in both cases.\n\n\n15.3.3 13.4.3 Clever combinations: Hotelling’s weighting example\nTo get the best data out of available resources, capitalizing on cancellations and symmetries is an important aspect. Here is a famous illustration of how Hotelling devised an improved weighing scheme. Suppose we are given a set of eight unknown weights \\(= (_1, …,_8)\\). In the following code, we simulate such a set of true weights using R’s random number generator.\n\nFigure 13.6: The example in this section uses the pharmacist’s balance weighing analogy introduced by Yates and developed by Hotelling (1944) and Mood (1946).\ntheta = round((2 * sample(8, 8) + rnorm(8)), 1)\ntheta __\n\n\n[1] 10.7 13.4 16.4  3.9  8.5 16.0  1.2  4.4\nMethod 1 : Naïve method, using eight weighings. Suppose we use a pharmacist’s balance (Figure 13.6) that weighs each weight \\(_i\\) individually, with errors distributed normally with a standard deviation of 0.1. We compute the vector of errors errors1 and their sum of squares as follows:\nX = theta + rnorm(length(theta), 0, 0.1)\nX __\n\n\n[1] 10.513279 13.268145 16.507673  3.881881  8.395974 16.073952  1.131341\n[8]  4.289040\n\n\nerrors1 = X - theta\nerrors1 __\n\n\n[1] -0.18672051 -0.13185519  0.10767279 -0.01811869 -0.10402607  0.07395242\n[7] -0.06865871 -0.11095993\n\n\nsum(errors1^2)__\n\n\n[1] 0.09748857\nMethod 2 : Hotelling’s method, also using eight weighings. The method is based on a Hadamard matrix, which we compute here.\nlibrary(\"survey\")\nh8 = hadamard(6)\ncoef8 = 2*h8 - 1\ncoef8 __\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    1    1    1    1    1    1    1    1\n[2,]    1   -1    1   -1    1   -1    1   -1\n[3,]    1    1   -1   -1    1    1   -1   -1\n[4,]    1   -1   -1    1    1   -1   -1    1\n[5,]    1    1    1    1   -1   -1   -1   -1\n[6,]    1   -1    1   -1   -1    1   -1    1\n[7,]    1    1   -1   -1   -1   -1    1    1\n[8,]    1   -1   -1    1   -1    1    1   -1\nWe use coef8 as the coefficients in a new weighing scheme, as follows: the first column of the matrix tells us to put all the weights on one side of the balance and to weigh that. Call the result Y[1]. The second column tell us to place weights 1, 3, 5, 7 on one side of the balance and weights 2, 4, 6, 8 on the other. We then measure the difference and call the result Y[2]. And so forth, for all eight columns of coef8. We can express the necessary computations in matrix multiplication form as below.\nY = theta  %*% coef8 + rnorm(length(theta), 0, 0.1)__\nAs in the first method, each of the eight weight measurements has a normal error with standard deviation of 0.1.\n__\nQuestion 13.6\n\nCheck that coef8 is -up to an overall factor- an orthogonal matrix (\\(C^t C = \\) for some \\(\\)).\nCheck that if we multiply theta with coef8 times coef8 transposed and divide by 8, we obtain theta again.\n\n__\nSolution\n__\ncoef8 %*% t(coef8)__\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    8    0    0    0    0    0    0    0\n[2,]    0    8    0    0    0    0    0    0\n[3,]    0    0    8    0    0    0    0    0\n[4,]    0    0    0    8    0    0    0    0\n[5,]    0    0    0    0    8    0    0    0\n[6,]    0    0    0    0    0    8    0    0\n[7,]    0    0    0    0    0    0    8    0\n[8,]    0    0    0    0    0    0    0    8\n\n\ntheta %*% coef8 %*% t(coef8) / ncol(coef8)__\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,] 10.7 13.4 16.4  3.9  8.5   16  1.2  4.4\nWe combine these results to estimate theta using the orthogonality of coef8.\nthetahat = Y %*% t(coef8) / ncol(coef8)__\nSince we know the true \\(\\), we can compute the errors and their sum of squares.\nerrors2 = as.vector(thetahat) - theta\nerrors2 __\n\n\n[1] -0.005213746  0.025216488  0.003201562  0.033880188 -0.029459127\n[6] -0.043173774  0.083202870 -0.025818188\n\n\nsum(errors2^2)__\n\n\n[1] 0.01214228\nWe see that the sum of squares here is substantially smaller than that of the first procedure. Were we just lucky?\n__\nQuestion 13.7\n\nRepeat the above experiment B = 10000 times, each time using a different theta, and look at the sampling distributions of sum of squared errors in both schemes.\nWhat do you think the relationship between the two variances is?\n\n__\nSolution\n__\nB  = 10000\ntc = t(coef8) / ncol(coef8)\nsse = replicate(B, {\n  theta = round((2 * sample(8, 8)) + rnorm(8), 1)\n  X = theta + rnorm(length(theta), 0, 0.1)\n  err1 = sum((X - theta)^2)\n  Y = coef8 %*% theta + rnorm(length(theta), 0, 0.1)\n  thetahat = tc %*% Y\n  err2 = sum((thetahat - theta)^2)\n  c(err1, err2)\n})\nrowMeans(sse)__\n\n\n[1] 0.079591221 0.009954419\n\n\nggplot(tibble(lr = log2(sse[1, ] / sse[2, ])), aes(x = lr)) +\n  geom_histogram(bins = 50) +\n  geom_vline(xintercept = log2(8), col = \"orange\") +\n  xlab(\"log2 ratio of SSE, Method 1 vs 2\")__\n\nFigure 13.7: Logarithm (base 2) of the ratios of sum of squared error for the two methods. The vertical orange line corresponds to 8.\nThe second scheme is more efficient than the first by a factor of 8 because the errors generated by the measurement have a sum of squares that is 8 times lower (Figure 13.7).\nThis example shows us that when several quantities are to be ascertained there is an opportunity to increase the accuracy and reduce the cost by combining measurements in one experiment and making comparisons between similar groups.\nIbn Sina’s rule that an optimal design can only vary one factor at a time was superseded in the 20th century by RA Fisher. He realized that one could modify the factors in combinations and still come to a conclusion—sometimes, an even better conclusion, as in the weighing example—as long as the contrasts were carefully designed.\n\nFigure 13.8: On the left, two samples each of size 6 are being compared. On the right, the same data are shown, but colored by the time of data collection. We note a tendency of the data to fall into blocks according to these times. Because of this, comparison between the groups is diluted. This effect can be mitigated by comparing within times, i.,e., by blocking into three groups. Paired analysis, such as demonstrated in Questions 13.8—13.10, is a special case of blocking.\n\n\n15.3.4 13.4.4 Blocking and pairing\nDarwin suspected that corn growth is affected by the composition of the soil and the humidity in the pots. For this reason, when he wanted to compare plants grown from cross-pollinated seeds to plants grown from self-pollinated seeds, he planted one seedling of each type in each of 15 pots. Each pot in Darwin’s Zea Mays experiment is a block, only the factor of interest (pollination method), called the treatment , is different within each block (Figure 13.9).\n\nFigure 13.9: A paired experiment is the simplest case of blocking.\nIn fact, RA Fisher criticized Darwin’s experiment because he systematically put the cross-pollinated plants on the same side of the pot. This could have induced confounding of a “side” effect with the cross effect, if one side of the pot received more sunlight for instance. It would have been preferable to randomize the side of the pot, e.,g., by flipping a coin.\nBlock what you can, randomize what you cannot.\n(George Box, 1978)\n\n15.3.4.1 Comparing a paired versus an unpaired design\nWhen comparing various possible designs, we do power simulations similar to what we saw in Chapter 1. Let’s suppose the sample size is 15 in each group and the effect size is 0.2. We also need to make assumptions about the standard deviations of the measurements, here we suppose both groups have the same sd=0.25 and simulate data:\nn = 15\neffect = 0.2\npots   = rnorm(n, 0, 1)\nnoiseh = rnorm(n, 0, 0.25)\nnoisea = rnorm(n, 0, 0.25)\nhybrid = pots + effect + noiseh\nautoz  = pots + noisea __\n__\nQuestion 13.8\nPerform both a simple \\(t\\)-test and a paired \\(t\\)-test. Which is more powerful in this case?\n__\nSolution\n__\nt.test(hybrid, autoz, paired = FALSE)__\n\n\n    Welch Two Sample t-test\n\ndata:  hybrid and autoz\nt = 0.77183, df = 26.012, p-value = 0.4472\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.3145706  0.6928591\nsample estimates:\nmean of x mean of y \n0.5073519 0.3182076 \n\n\nt.test(hybrid, autoz, paired = TRUE)__\n\n\n    Paired t-test\n\ndata:  hybrid and autoz\nt = 1.8783, df = 14, p-value = 0.08133\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.02683705  0.40512561\nsample estimates:\nmean difference \n      0.1891443 \nMaybe we were just lucky with our simulated data here?\n__\nQuestion 13.9\nCheck which method is generally more powerful. Repeat the above computations \\(1000\\) times and compute the average probability of rejection for these 1000 trials, using a false positive rate \\(\\).\n__\nSolution\n__\nB     = 1000\nalpha = 0.05\nwhat  = c(FALSE, TRUE)\npvs = replicate(B, {\n  pots   = rnorm(n, 0, 1)\n  noiseh = rnorm(n, 0, 0.25)\n  noisea = rnorm(n, 0, 0.25)\n  hybrid = pots + effect + noiseh\n  autoz  = pots + noisea\n  vapply(what,\n    function(paired)\n      t.test(hybrid, autoz, paired = paired)$p.value,\n    double(1)) |&gt; setNames(paste(what))\n})\nrowMeans(pvs &lt;= alpha)__\n\n\nFALSE  TRUE \n0.000 0.532 \nWe can compare the p-values obtained using both methods (Figure 13.10).\ntidyr::pivot_longer(as.data.frame(t(pvs)), cols = everything(), names_to = \"paired\") |&gt;\n  ggplot(aes(x = value, fill = paired)) +\n  geom_histogram(binwidth = 0.01, boundary = 0, alpha = 1/3)__\n\nFigure 13.10: Results from the power calculation, comparing the p-value distributions from the ordinary unpaired and the paired \\(t\\)-test.\n__\nQuestion 13.10\n\nWrite a function that compares the power of the two types of tests for different values of the effect size, sample size, size of the pot effects (as measured by their standard deviation), noise standard deviation and sample size.\nUse your function to find out which of the standard deviations (pots or noise) has the largest effect on the improvement produced by pairing for \\(n=15\\).\nHow big should \\(n\\) be to attain a power of 80% if the two standard deviations are both 0.5?\n\n__\nSolution\n__\npowercomparison = function(effect = 0.2, n = 15, alpha = 0.05,\n                sdnoise, sdpots, B = 1000) {\n  what = c(FALSE, TRUE)\n  pvs = replicate(B, {\n    pots   = rnorm(n, 0, sdpots)\n    noiseh = rnorm(n, 0, sdnoise)\n    noisea = rnorm(n, 0, sdnoise)\n    hybrid = pots + effect + noiseh\n    autoz  = pots + noisea\n    vapply(what,\n      function(paired)\n        t.test(hybrid, autoz, paired = paired)$p.value,\n      double(1)) |&gt; setNames(paste(what))\n  })\n  rowMeans(pvs &lt;= alpha)\n}__\nHere are a few simulations showing that when the pot effects are small compared to the noise standard deviation, pairing hardly makes a difference. If the pot effects are large, then pairing does make a big difference.\npowercomparison(sdpots = 0.5,  sdnoise = 0.25)__\n\n\nFALSE  TRUE \n0.034 0.533 \n\n\npowercomparison(sdpots = 0.25, sdnoise = 0.25)__\n\n\nFALSE  TRUE \n0.242 0.524 \n\n\npowercomparison(sdpots = 0.1,  sdnoise = 0.25)__\n\n\nFALSE  TRUE \n0.510 0.534 \nFor 100 plants of each type and both standard deviation at 0.5, the power of the paired test is about 80%.\npowercomparison(sdpots = 0.5, sdnoise = 0.5, n = 100)__\n\n\nFALSE  TRUE \n0.513 0.796 \n__\nQuestion 13.11\nPaired designs take into account a natural pairing of the observations — for instance, twin studies, or studies of patients before and after a treatment. What can be done when pairing is not available?\n__\nSolution\n__\nMatched designs try to create pairs of subjects that have as much similarity as possible through matching age, sex, background health etc. One is treated, the other serves as a control.\nA balanced design is an experimental design where all the different factor combinations have the same number of observation replicates. The effect of each factor is identifiable. If there are nuisance factors, it is good to make sure they are balanced with the factors of interest. Sometimes this is inconvenient or impractical for logistic or economic reasons – but in such cases analysts are on thin ice and need to proceed with caution.\n\n\n15.3.4.2 Randomization\nOften we don’t know which nuisance factors will be important, or we cannot plan for them ahead of time. In such cases, randomization is a practical strategy: at least in the limit of large enough sample size, the effect of any nuisance factor should average out.\nRandomization can also help reduce unconscious bias. For instance, if the samples from one of the groups are extremely hard to come by, we might be tempted to be extra careful when handling them, compared to samples from the other groups. Unfortunately this might bias the measurement outcomes and thus invalidate the comparison. See Senn (2004) for an extensive discussion of some of the pitfalls that occur when trying to improve on simple randomization.\n\n\n\n15.3.5 13.4.5 How many replicates do I need?\n\n\n\nBeware of underpowered me-too studies.\n\n\nBeware of underpowered me-too studies.\nIn Section 1.4.1 we showed a simulation experiment calculating how many nucleotides were necessary to achieve a 80% true positive rate, given that we knew the alternative. Now, recall the discussion of experiments versus studies from Section 13.2. For the cell line experiment, we might get the correct result already from one replicate; usually we’ll do two or three to be sure. On the other hand, for a study comparing the effect of two alternative drugs on patients, our intuition tells us that there is so much uncontrolled variability that we’ll likely need dozens (if not more) patients until we can be sure about the result. The number of replicates needed is highly context specific. It depends on the amount of uncontrolled variability and the effect size. A pragmatic approach is to check out previous successful (or unsuccessful) experiments or studies that did something comparable and use simulations, subsampling or bootstrapping to get an estimate of the planned study’s power.\n\n15.3.5.1 Power depends on sample sizes, effect sizes and variability.\n\nFigure 13.11: The elephant in the room with power calculations is the effect size. Especially in ’omics studies, when we are screening thousands of genes (or other features) for differences, we rarely have a precise idea of what effect size to expect. However, even so, power calculations are useful for order-of-magnitude calculations, or for qualitative comparisons such as shown in this section for paired versus unpaired tests. Source: Wikimedia CH.\nThe package pwr provides functions for doing the standard power calculations. There are always four quantities involved in these computations: sample size, effect size, significance level (false positive rate) and the power itself which is the probability of rejecting a hypothesis when you should (true positive rate). The functions pwr.2p.test, pwr.chisq.test, pwr.f2.test provide the calculations for tests of two proportions, the chisquared test and general linear tests respectively.\nHere is an example of the power calculcation for a two sample \\(t\\)-test with \\(n=15\\). The function requires several arguments:\nlibrary(\"pwr\")\nstr(pwr.t.test)__\n\n\nfunction (n = NULL, d = NULL, sig.level = 0.05, power = NULL, type = c(\"two.sample\", \n    \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))  \nIf you call the function with a value for power and effect size, it will return the sample size needed, or if you specify the sample size and effect size, it returns the power.\npwr.t.test(n = 15, d = 0.4, sig.level = 0.05, type = \"two.sample\")__\n\n\n     Two-sample t test power calculation \n\n              n = 15\n              d = 0.4\n      sig.level = 0.05\n          power = 0.1848496\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\npwr.t.test(n = 15, d = 0.4, sig.level = 0.05, type = \"paired\")__\n\n\n     Paired t test power calculation \n\n              n = 15\n              d = 0.4\n      sig.level = 0.05\n          power = 0.3031649\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\nIf we want to know what sample size would be required to detect a given effect size:\npwr.t.test(d = 0.4, sig.level = 0.05, type = \"two.sample\", power=0.8)__\n\n\n     Two-sample t test power calculation \n\n              n = 99.08032\n              d = 0.4\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\npwr.t.test(d = 0.4, sig.level = 0.05, type = \"paired\", power=0.8)__\n\n\n     Paired t test power calculation \n\n              n = 51.00945\n              d = 0.4\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\nWe see that we would need about twice as many observations for the same power when not using a paired test.\n\n\n15.3.5.2 Effective sample size\nA sample of independent observations is more informative than the same number of dependent observations. Suppose you want to do an opinion poll by knocking at people’s doors and asking them a question. In the first scenario, you pick \\(n\\) people at \\(n\\) random places throughout the country. In the second scenario, to save travel time, you pick \\(n/3\\) random places and then at each of these interview three people who live next door to each other. In both cases, the number of people polled is \\(n\\), but if we assume that people living in the same neighborhood are more likely to have the same opinion, the data from the second scenario are (positively) correlated. To explore this, let’s do a simulation.\ndoPoll = function(n = 100, numPeoplePolled = 12) {\n  opinion = sort(rnorm(n))\n  i1 = sample(n, numPeoplePolled)\n  i2 = sample(seq(3, n, by = 3), numPeoplePolled / 3)\n  i2 = c(i2, i2 - 1, i2 - 2)\n  c(independent = mean(opinion[i1]), correlated = mean(opinion[i2]))\n}\nresponses = replicate(5000, doPoll())\n\ntidyr::pivot_longer(as.data.frame(t(responses)), \n        cols = everything(), names_to = \"design\") |&gt;\nggplot(aes(x = value, col = design)) + geom_density() +\n  geom_vline(xintercept = 0) + xlab(\"Opinion poll result\")__\n\nFigure 13.12: Density estimates for the polling result using the two sampling methods. The correlated method has higher spread. The truth is indicated by the vertical line.\nThere are 100 people in the country, of which in the first approach (i1) we randomly sample 12. In the second approach, we sample 4 people as well as two neighbors for each (i2). The “opinion” in our case is a real number, normally distributed in the population with mean 0 and standard deviation 1. We model the spatio-sociological structure of our country by sorting the houses from most negative to most positive opinion in the first line of the doPoll function. The output is shown in Figure 13.12.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#mean-variance-relationships-and-variance-stabilizing-transformations",
    "href": "13-chap.html#mean-variance-relationships-and-variance-stabilizing-transformations",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.4 13.5 Mean-variance relationships and variance-stabilizing transformations",
    "text": "15.4 13.5 Mean-variance relationships and variance-stabilizing transformations\nIn Chapters 4 and 8 we saw examples for data transformations that compress or stretch the space of quantitative measurements in such a way that the measurements’ variance is more similar throughout. Thus the variance between replicate measurements is no longer highly dependent on the mean value.\nThe mean-variance relationship of our data before transformation can in principle be any function, but in many cases, the following prototypic relationships are found, at least approximately:\n\nconstant: the variance is independent of the mean, \\(v(m)=c\\).\nPoisson: the variance is proportional to to the mean, \\(v(m)=am\\).\nquadratic: the standard deviation is proportional to the mean, therefore the variance grows quadratically, \\(v(m)=bm^2\\).\n\nHere \\(v(m)\\) is the function that describes the trend of the variance \\(v\\) as a function of the mean \\(m\\). The real numbers \\(a, b, c\\) parameterize factors affecting the variance besides the mean.\n__\nQuestion 13.12\nGive examples for biological assays or measurement technologies whose data show these types of mean-variance relationships.\nReal data can also be affected by a combination of these. For instance, with DNA microarrays, the fluorescence intensities are subject to a combination of background noise that is largely independent of the signal, and multiplicative noise whose standard deviation is proportional to the signal (Rocke and Durbin 2001). Therefore, the mean-variance relationship is \\(v(m)=bm^2+c\\). For bright spots (large \\(m\\)), the multiplicative noise dominates (\\(bm^2\\)), whereas for faint ones, the background \\(c\\).\n__\nQuestion 13.13\nWhat is the point of applying a variance-stabilizing transformation?\n__\nSolution\n__\nAnalyzing the data on the transformed scale tends to:\n\nImprove visualization, since the physical space on the plot is used more “fairly” throughout the range of the data. A similar argument applies to the color space in the case of a heatmap.\nImprove the outcome of ordination methods such as PCA or clustering based on correlation, as the results are not so much dominated by the signal from a few very highly expressed genes, but more uniformly from many genes throughout the dynamic range.\nImprove the estimates and inference from statistical models that are based on assuming identically distributed (and hence, homoskedastic) noise.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#data-quality-assessment-and-quality-control",
    "href": "13-chap.html#data-quality-assessment-and-quality-control",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.5 13.6 Data quality assessment and quality control",
    "text": "15.5 13.6 Data quality assessment and quality control\nWe distinguish between data quality assessment (QA) –steps taken to measure and monitor data quality– and quality control (QC) –removing bad data. These activities pervade all phases of an analysis, from assembling the raw data over transformation, summarization, model fitting, hypothesis testing or screening for “hits” to interpretation. QA-related questions include:\n\nHow do the marginal distributions of the variables look (histograms, ECDF plots)?\nHow do their joint distributions look (scatter plots, pairs plot)?\nHow well do replicates agree (as compared to different biological conditions)? Are the magnitudes of the differences between several conditions plausible?\nIs there evidence of batch effects? These could be of a categorical (stepwise) or continuous (gradual) nature, e.g.. due to changes in experimental reagents, protocols or environmental factors. Factors associated with such effects may be explicitly known, or unkown and latent , and often they are somewhere in between (e.g., when a measurement apparatus slowly degrades over time, and we have recorded the times, but don’t really know exactly at what time the degradation is how bad).\n\nFor the last two sets of questions, heatmaps, principal component plots and other ordination plots (as we have seen in Chapters 7 and 9) are useful.\n\nFigure 13.13: Henry Ford’s (possibly apocryphal) quote: “If I had asked people what they wanted, they would have said faster horses.” expresses the view of quality as fitness for purpose , versus adherence to specifications. (Source: Ford)\nIt’s not easy to define quality , and the word is used with many meanings. The most pertinent for us is fitness for purpose 4, and this contrasts to other definitions of quality that are based on normative specifications. For instance, in differential expression analysis with RNA-Seq data, our purpose may be the detection of differentially expressed genes between two biological conditions. We can check specifications such as the number of reads, read length, base calling quality, fraction of aligned reads, but ultimately these measures in isolation have little bearing on our purpose. More to the point will be the identification of samples that are not behaving as expected, e.g., because of a sample swap or degradation; or genes that were not measured properly. We saw an example for this in Section 8.10.3. Useful plots include ordination plots, such as Figure 8.6, and heatmaps, such as Figure 8.7. A quality metric is any value that we use to measure quality, and having explicit quality metrics helps automating QA/QC.\n4 http://en.wikipedia.org/wiki/Quality_%28business%29",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#longitudinal-data",
    "href": "13-chap.html#longitudinal-data",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.6 13.7 Longitudinal data",
    "text": "15.6 13.7 Longitudinal data\nLongitudinal data 5 have time as a covariate. The first question is whether we are looking at a handful of time points –say, the response of a cell line measured 48h, 72h and 96h after exposure to a drug– or a long and densely sampled time series –say, patch clamp data in electrophysiology or a movie from life cell microscopy.\n5 A related but different concept is survival data , where time is the outcome variable.\nIn the first case, time is usually best thought of as just another discrete experimental factor. Perhaps the multiple time points were chosen because the experimenter was not sure which one would give the most useful results. One can then try to identify the best time point and focus on that. Depending on the data, the other time points could serve for validation, as “more-or-less” replicates. When designing the experiment, we’ll try to cover those time periods more densely when we expect most to happen, e.g., directly after a perturbation.\nIn a screening context, we can ask whether there is any effect at all, regardless of which time point and which shape, using something like an \\(F\\)-test. We then just need to make sure that we account for the dependencies between the measurements at the different time points and determine the null distribution accordingly.\nIn the second case, with time series, we may want to fit dynamical models to the data. We can write \\(X(t)\\) for the state of our system at time \\(t\\), and we have many choices, depending on whether\n\n\\(X\\) is continuous or discrete,\nthe dynamics of \\(X\\)6 are deterministic or stochastic,\nthe dynamics are smooth and/or jumpy,\nwe observe \\(X\\) directly or only some noisy and/or reduced version \\(Y = g(X)+\\)7 of it.\n\n6 The value of \\(X(t+t)\\), given \\(X(t)\\), in other words, the temporal evolution\n7 Here \\(g\\) denotes a function that looses information, e.g., by dropping some of the variables of a vector-valued \\(X\\), and \\(\\) is a noise term.\nWe have many modeling tools at hand, including\n\nMarkov Models: discrete state space; the dynamics are stochastic and occur by jumping between states.\nOrdinary or partial differential equations: continuous state space; the dynamics are deterministic and smooth and are described by a differential equation, possibly derived from first principles rooted in physics or chemistry.\nMaster equation, Fokker-Planck equation: the dynamics are stochastic and are described by (partial) differential equations for the probability distribution of \\(X\\) in space and time.\nPiece-wise deterministic stochastic processes: a combination of the above, samples from the process involve deterministic, smooth movements as well as occasional jumps.\n\nIf we don’t observe \\(X\\) directly, but only a noisy and/or summarized version \\(Y\\), then in the case of Markov models, the formalism of Hidden Markov Models (Durbin et al. 1998) makes it relatively straightforward to fit such models. For the other types of processes, analogous approaches are possible, but these are technically more demanding, and we refer to specialized literature.\nTaking a more data-driven (rather than model-driven) view, methods for analyzing time series data include:\n\nNon-parametric smoothing followed by clustering or classification into prototypic shapes\nChange point detection\nAutoregressive models\nFourier and wavelet decomposition\n\nIt’s outside the scope of this book to go into details, and there is a huge number of choices8. Many methods originated in physics, econometrics or signal processing, so it’s worthwhile to scan the literature in these fields.\n8 One start point is the CRAN taskview https://cran.r-project.org/web/views/TimeSeries.html.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#data-integration-use-everything-you-could-know",
    "href": "13-chap.html#data-integration-use-everything-you-could-know",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.7 13.8 Data integration: use everything you (could) know",
    "text": "15.7 13.8 Data integration: use everything you (could) know\n\n\n\nDon’t pretend you are dumb.\n\n\nDon’t pretend you are dumb.\nThere is an attraction to seemingly “unbiased” approaches that analyse the data at hand without reference to what is already known. Such tendencies are reinforced by the fact that statistical methods have often been developed to be generic and self-contained, for instance, to work of a general matrix without specific reference to what the rows and column mean in an application, or what other, more or less relevant data might be around.\nGeneric approaches are a good way to get started, and for analyses that are straightforward and highly powered, such an approach might work out. But often, it is wasteful. Recall the example of an RNA-Seq experiment for differential expression. As we saw in Chapters 6 and 8, we could perform a hypothesis test for each recorded gene, regardless of its signal strength9 or anything else, and then run a multiple testing method that treats all tests the same (i.e., as exchangeable). But this is inefficient: we can improve our detection power by filtering out or downweighting hypotheses with lower power or with higher prior probability \\(_0\\) of being true.\n9 i.e., average read counts\nSimilarly, in the interpretation of single p-values, we don’t need to ignore everything else we know, and for instance, blindly stick to an arbitrary 5% cutoff no matter what, but rather, we can let prior knowledge on the test’s power and on \\(_0\\) guide our interpretation (Altman and Krzywinski 2017).\nOther potential examples of misplaced objectivity include:\n\nPenalization or feature selection in high-dimensional regression or classification. It is easy to use schemes that treat all features the same, for instance, standardize all of them to zero mean and unit variance. But sometime we know that some classes of features are likely to be more or less informative than others (Wiel et al. 2016). We can also use graphs or networks to represent “other” data and use approaches like the group or graph lasso (Jacob, Obozinski, and Vert 2009) to structure your penalties in high-dimensional modeling.\nUnsupervised clustering of our objects of interest (samples, genes or sequences) and subsequent search for over-represented annotations. We can be better off by incorporating the different uncertainties with which these were measured as well as their different frequencies into the clustering algorithm. We can use probabilities and similarities to check whether the members of clusters are more similar than two randomly picked objects (Callahan et al. 2016).\n\nWhen embarking on an analysis, it’s important to anticipate that rarely we’ll be done by applying a single method and getting a straightforward result. We need to dig out other, related datasets, look for confirmations (or else) of our results, get further interpretation. An example is gene set enrichment analysis: after we’ve analyzed our data and found a list of genes that appear to be related to our comparison of interest, we’ll overlap them with other gene lists, such as those from the Molecular Signatures Database (Liberzon et al. 2011) in order to explore the broader biological processes involved; or we might load up datasets looking at levels of regulation10 up-stream or down-stream of ours in search for context.\n10 Genome, chromatin state, transcription, mRNA life cycle, translation, protein life cycle, localization and interactions; metabolites, \\(…\\)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#sharpen-your-tools-reproducible-research",
    "href": "13-chap.html#sharpen-your-tools-reproducible-research",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.8 13.9 Sharpen your tools: reproducible research",
    "text": "15.8 13.9 Sharpen your tools: reproducible research\nAnalysis projects often begin with a simple script, perhaps to try out a few initial ideas and explore the quality of the pilot data. Then more ideas are added, more data come in, other datasets are integrated, more people become involved. Eventually the paper needs to be written, figures be done ‘properly’, and the analysis be saved for the scientific record and to document its integrity. Here are a few principles that can help with such a process11.\n11 An excellent and very readable outline of good computing practices for researchers, including data management, programming, collaborating with colleagues, organizing projects, tracking work and writing manuscripts, is given by Wilson et al. (2017).\nUse an integrated development environment. RStudio is a great choice; there are also other platforms such as Emacs or Eclipse.\nUse literate programming tools such as Rmarkdown or Jupyter. This is more readable (for yourself and for others) than burying explanations and usage instructions in comments in the source code or in separate README files, in addition you can directly embed figures and tables in these documents. Such documents are good starting points for the supplementary material of your paper. Moreover, they’re great for reporting analyses to your collaborators. Anticipate re-engineering of the data formats and the software. The first version of how you represent the data and structure the analysis workflow will rarely be capable of supporting the project as it evolves. Don’t be afraid12 to make a clean cut and redesign as soon as you notice that you are doing a lot of awkward data manipulations or repetitive steps. This is time well- invested. Almost always it also helps to unearth bugs.\n12 The professionals do it, too: “Most software at Google gets rewritten every few years.” (Henderson 2017)\nReuse existing tools. Don’t reinvent the wheel; your time is better spent on things that are actually new. Before using a self-made “heuristic” or a temporary “short-cut”, spend a couple of minutes researching to see if something like this hasn’t been done before. More often than not, it has, and sometimes there is a clean, scalable and already tested solution.\nUse version control , such as . This takes time to learn, but this time is well-invested. In the long run it will be infinitely better than all your self-grown attempts at managing evolving code with version numbers, switches and the like. Moreover, this is the sanest option for collaborative work on code, and it provides an extra backup of your codebase, especially if the server is distinct from your personal computer.\nUse functions rather than copy-pasting (or repeatedly source-ing) stretches of code.\nUse the R package system. Soon you’ll note recurring function or variable definitions that you want to share between your different scripts. It is fine to use the R function source to manage them initially, but it is never too early to move them into your own package – at the latest when you find yourself starting to write emails or code comments explaining others (or yourself) how to use some functionality. Assembling existing code into an R package is not hard, and it offers you many goodies including standardized ways of documentation, showing code usage examples, code testing, versioning and provision to others. And quite likely you’ll soon appreciate the benefits of using namespaces.\nCentralize the location of the raw data files and automate the derivation of intermediate data. Store the input data at a centralized file server that is professionally backed up. Mark the files as read-only. Have a clear and linear workflow for computing the derived data (e.g., normalized, summarized, transformed etc.) from the raw files, and store these in a separate directory. Anticipate that this workflow will need to be run several times13, and version it. Use the BiocFileCache package to mirror these files on your personal computer14.\n13 Always once more than the final, final time before the final data freeze…\n14 A more basic alternative is the utility. A popular solution offered by some organizations is based on ownCloud. Commercial options include Dropbox, Google Drive, and the like.\n15 In computer science, the term data warehouse is sometimes used for such a concept.\nThink in terms of cooking recipes and try to automate them. When developing downstream analysis ideas that bring together several different data types, you don’t want to do the conversion from data type specific formats into a representation suitable for machine learning or generic statistical method each time anew, on an ad hoc basis. Have a recipe script that assembles the different ingredients and cooks them up as an easily consumable15 matrix, data frame or Bioconductor SummarizedExperiment.\nKeep a hyperlinked webpage with an index of all analyses. This is helpful for collaborators (especially if the page and the analysis can be accessed via a web browser) and also a good starting point for the methods part of your paper. Structure it in chronological or logical order, or a combination of both.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#data-representation",
    "href": "13-chap.html#data-representation",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.9 13.10 Data representation",
    "text": "15.9 13.10 Data representation\nGetting data ready for analysis or visualization often involves a lot of shuffling until they are in the right shape and format for an analytical algorithm or a graphics routine. As we saw in Chapter 3, ggplot2 likes its data in dataframe objects, with one row per measurement record. The reasons behind this choice are well explained in Hadley Wickham’s paper on tidy data (Wickham 2014).\n\n15.9.1 13.10.1 Wide vs long table format\nRecall the Hiiragi data (for space reasons we select only four genes, and print only the first five columns of xwdf):\nlibrary(\"magrittr\")\ndata(\"x\", package = \"Hiiragi2013\")\nxwdf = tibble(\n  probe  = c(\"1420085_at\", \"1418863_at\", \"1425463_at\", \"1416967_at\"),\n  symbol = c(      \"Fgf4\",      \"Gata4\",      \"Gata6\",       \"Sox2\"))\nxwdf %&lt;&gt;% bind_cols(as_tibble(Biobase::exprs(x)[xwdf$probe, ]))\ndim(xwdf)__\n\n\n[1]   4 103\n\n\nxwdf[, 1:5]__\n\n\n# A tibble: 4 × 5\n  probe      symbol `1 E3.25` `2 E3.25` `3 E3.25`\n  &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 1420085_at Fgf4        3.03      9.29      2.94\n2 1418863_at Gata4       4.84      5.53      4.42\n3 1425463_at Gata6       5.50      6.16      4.58\n4 1416967_at Sox2        1.73      9.70      4.16\nEach row of this dataframe corresponds to one of the selected genes. The first two column contain the Affymetrix probe identifier and the gene symbol. The remaining 101 columns report the measured expression values, one for each sample. The sample identifiers, together with information on the time point when the sample was taken, are recorded in the column names as a concatenated string. This is an example for a data table in wide format. Now let us call the pivot_longer function from the tidyr package and have a look at its output.\nlibrary(\"tidyr\")\nxldf = pivot_longer(xwdf, cols = !all_of(c(\"probe\", \"symbol\")),\n                          names_to = \"sample\")\ndim(xldf)__\n\n\n[1] 404   4\n\n\nhead(xldf)__\n\n\n# A tibble: 6 × 4\n  probe      symbol sample  value\n  &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;   &lt;dbl&gt;\n1 1420085_at Fgf4   1 E3.25  3.03\n2 1420085_at Fgf4   2 E3.25  9.29\n3 1420085_at Fgf4   3 E3.25  2.94\n4 1420085_at Fgf4   4 E3.25  9.72\n5 1420085_at Fgf4   5 E3.25  8.92\n6 1420085_at Fgf4   6 E3.25 11.3 \nIn xldf, each row corresponds to exactly one of the 404 measured values, stored in the column value. Then there are additional columns probe, symbol and sample, which store the associated covariates. This is an instance of long format.\nIn xwdf, some columns refer to data from all the samples (namely, probe and symbol), whereas other columns (those with the expression measurements) contain information that is sample-specific. We somehow have to “know” this when interpreting the dataframe. This is what Hadley Wickham calls untidy data 16. In contrast, in the tidy dataframe xldf each row forms exactly one observation, its value is in the column named value, and all other information associated with that observation is in the other colums of the same row. If we want to add additional columns, say, Ensembl gene identifiers or chromosome locations, we can simply add them. Similarly, if we want to add data from further genes or additional samples, we can simply add the corresponding rows to xldf. In either, we can assume that we will not break existing code. This is in contrast to xwdf, adding columns might invalidate existing code, as we cannot be sure how it differentiates between data columns (with measured values) and covariate columns.\n16 Recall the Anna Karenina principle: there are many different ways for data to be untidy.\nAlso, subsetting by probe identifier, by gene symbol, or by samples, or indeed by any other covariate, is straightforward and can always use the same dplyr::filter syntax. In contrast, for xwdf, we need to remember that subsetting samples amounts to column subsetting, whereas subsetting genes to row subsetting.\nThe Hiiragi data have another natural wide format representation besides xwdf: instead of one row per gene and columns for the different samples, we could also have the data in a dataframe with one row per sample and columns for the different genes. Both of these wide representations can be useful. For instance, if we want to produce scatterplots using ggplot2 of the expression values of all genes between two samples, or all samples between two genes, we need to use one or the other of the two wide formats.\nTo transform from the long format into the wide format (either of them), you can use the pivot_wider function from the tidyr package—the complement of the pivot_longer function that we already used above.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#tidy-data-using-it-wisely",
    "href": "13-chap.html#tidy-data-using-it-wisely",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.10 13.11 Tidy data – using it wisely",
    "text": "15.10 13.11 Tidy data – using it wisely\nIn tidy data (Wickham 2014),\n\neach variable forms a column,\neach observation forms a row,\neach type of observational unit forms a table.\n\nThe success of the tidyverse attests to the power of its underlying ideas and the quality of its implementation. Much of the code for this book has adopted these ideas and uses the tidyverse.\nNevertheless, dataframes in the long format are not a panacea. Here are some things to keep in mind:\nEfficiency and integrity. Even though there are only 4 probe-gene symbol relationships, we are repeatedly storing them 404 times in the rows of xldf. In this instance, the extra storage cost is negligible. In other cases it could be more considerable. More important is the diffusion of information: when we are given an object like xldf and want to know all the probe-gene symbol relationships it uses, we have to gather this information back from the many copies of it in the dataframe; we cannot be sure, without further checking, that the redundant copies of the information are consistent with each other; if we want to update the information, we have to change it in many places. This speaks for workflow designs in which an object like xldf is not used for long term data storage, but is assembled at a relatively late stage of analysis from more normalized17 data containers that contain the primary data objects.\n17 Data normalization is the process of organizing a database to reduce redundancy and improve integrity; see e.g. https://en.wikipedia.org/wiki/Database_normalization.\nLack of contracts and standardization. When we write a function that expects to work on an object like xldf, we have no guarantee that the column probe does indeed contain valid probe identifiers; nor that such a column even exists. There is not even a direct way to express programmatically what “an object like xldf” means in the tidyverse. Object oriented (OO) programming, and its incarnation S4 in R, solves such questions. For instance, the above-mentioned checks could be performed by a validObject method for a suitably defined class, and the class definition would formalize the notion of “an object like xldf”. Addressing such issues is behind the object-oriented design of the data structures in Bioconductor, such as the SummarizedExperiment class. Other potentially useful features of OO data representations include\n\nAbstraction of interface from implementation and encapsulation: the user accesses the data only through defined channels and does not need to see how the data are stored “inside” – which means the inside can be changed and optimized without breaking user-level code.\nPolymorphism: you can have different functions with the same name, such as plot or filter, for different classes of objects, and R figures out for you which one to call.\nInheritance: you can build up more complex data representations from simpler ones.\nReflection and self-documentation: you can send programmatic queries to an object to ask for information about itself.\n\nAll of these make it easier to write high-level code that focuses on the big picture functionality rather than on implementation details of the building blocks – albeit at the cost of more initial investment in infrastructure and “bureaucracy”.\nData provenance and metadata. There is no obvious place in an object like xldf to add information about data provenance, e.g., who performed the experiment, where it was published, where the data were downloaded from or which version of the data we’re looking at (data bugs exist \\(…\\)). Neither are there any explanations of the columns, such as units and assay type. Again, the data classes in Bioconductor try to address this need.\n\nFigure 13.14: Sequential data analyses workflows can be leaky. If insufficient information is passed from one stage to the next, the procedure can end up being suboptimal and losing power.\nMatrix-like data. Many datasets in biology have a natural matrix-like structure, since a number of features (e.g., genes; conventionally the rows of the matrix) were assayed on several samples (conventionally, columns of the matrix). Unrolling the matrix into a long form like xldf makes some operations (say, PCA, SVD, clustering of features or samples) more awkward.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#leaky-pipelines-and-statistical-sufficiency",
    "href": "13-chap.html#leaky-pipelines-and-statistical-sufficiency",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.11 13.12 Leaky pipelines and statistical sufficiency",
    "text": "15.11 13.12 Leaky pipelines and statistical sufficiency\nData analysis pipelines in high-throughput biology often work as ‘funnels’ that successively summarise and compress the data. In high-throughput sequencing, we may start with microscopy images of a flow cell, perform base calling to derive sequencing reads, then align them to a reference, then only count the aligned reads for each position, summarise positions to genes (or other kinds of regions), then “normalize” these numbers by library size to make them comparable across libraries, etc. At each step, we loose information, yet it is important to make sure we still have enough information for the task at hand18. The problem is particularly acute if we build our data pipeline from a series of components from separate developers.\n18 For instance, for the RNA-Seq differential expression analysis that we saw in Chapter 8, we needed the actual read counts, not “normalized” versions; for some analyses, gene-level summaries might suffice, for others, we’ll want to look at the exon or isoform level.\nStatisticians have a concept for whether certain summaries enable the reconstruction of all the relevant information in the data: sufficiency. In a Bernoulli random experiment with a known number of trials, \\(n\\), the number of successes is a sufficient statistic for estimating the probability of success \\(p\\).\n__\nQuestion 13.14\nIn a 4 state Markov chain (A, C, G, T) such as the one we saw in Chapter 13, what are the sufficient statistics for the estimation of the transition probabilities?\nIterative approaches akin to what we saw when we used the EM algorithm can sometimes help avoid information loss. For instance, when analyzing mass spectroscopy data, a first run guesses at peaks individually for every sample. After this preliminary spectra-spotting, another iteration allows us to borrow strength from the other samples to spot spectra that may have been overlooked (looked like noise) before.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#efficient-computing",
    "href": "13-chap.html#efficient-computing",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.12 13.13 Efficient computing",
    "text": "15.12 13.13 Efficient computing\nThe rapid progress in data acquisition technologies leads to ever large datasets, and dealing with these is a challenge. It is tempting to jump right into software technologies that are designed for big data and scalability. But usually it is more helpful to first take a step back. Software engineers know the risks of premature optimization , or to paraphrase John Tukey19: “A slow and clumsy solution to the right problem is worth a good deal more than a fast and scalable solution to the wrong problem.” Sometimes, a good strategy is to figure out what is the right solution on a subset of the data before embarking on the quest for scalability and performance.\n19 http://stats.stackexchange.com/a/744\nIt’s also good to keep in mind the value of your own time, versus CPU time. If you can save some of your time developing code, even at the cost of longer computations, that can be a worthwhile trade-off.\nHaving considered all that, let’s talk about performance. R has a reputation for being slow and wasteful of memory, and that perception is sometimes invoked to motivate choosing other platforms. In some cases, this is justified: nobody would advocate writing a short read aligner, or the steering logic of a self-driving car in R. For statistical analyses, however, it is possible to write very efficient code using one or more of these concepts:\nVectorization. Consider the following alternative choices of computing the same result.\na = runif(1e6)\nb = runif(length(a))\nsystem.time({\n  z1 = numeric(length(a))\n  for (i in seq(along = a))\n    z1[i] = a[i]^2 * b[i]\n})__\n\n\n   user  system elapsed \n  0.076   0.001   0.076 \n\n\nsystem.time({\n  z2 = a^2 * b\n})__\n\n\n   user  system elapsed \n  0.003   0.000   0.003 \n\n\nidentical(z1, z2)__\n\n\n[1] TRUE\nThe vectorized version (z2) is many times faster than the explicitly indexed one (z1) and even easier to read. Sometimes, translating an algorithm that is formulated with indices is a little harder — say, if there are if-conditions, or if the computation for index i involves results from index i-1. Language constructs such as vectorized conditionals with ifelse, shifting of vectors with functions such as lead and lag in the dplyr package, and generally the infrastructure of dplyr , which is designed to express computations on whole dataframes (rather than row by row), can help.\nParallelization. Parallelizing computations with R is easy, not least because it is a functional language in which it is natural to express computations as functions with explicit input, output, and no side effects. The landscape of R packages and functionality to support parallized computing is fast-moving; the CRAN task view “High-Performance and Parallel Computing” and the package BiocParallel are good starting points.\nOut-of-memory-data and chunking. Some datasets are too big to load into random access memory (RAM) and manipulate all at once. Chunking means splitting the data into manageable portions (“chunks”) and then sequentially loading each portion from mass storage, computing on it, storing the result and removing the portion from RAM before loading the next one. R also offers infrastructure for working with large datasets that are stored on disk in a relational database management systems (the DBI package) or in HDF5 (the rhdf5 package). The Bioconductor project provides the class SummarizedExperiment , which can store big data matrices either in RAM or in an HDF5 backend in a manner that is transparent to the user of objects of this class.\nJudicious use of lower level languages. The Rcpp package makes it easy to write portions of your code in C++ and include them seamlessly within your R code. Many convenient wrappers are provided, such as below the C++ class NumericVector that wraps the R class numeric vector.\nlibrary(\"Rcpp\")\ncppFunction(\"\n  NumericVector myfun(NumericVector x, NumericVector y) {\n    int n = x.size();\n    NumericVector out(n);\n    for(int i = 0; i &lt; n; ++i) {\n      out[i] = pow(x[i], 2) * y[i];\n    }\n    return out;\n  }\")\nz3 = myfun(a, b)\nidentical(z1, z3)__\n\n\n[1] TRUE\nIn practice, the above code should also contain a check on the length of y. Here, we provided the C++ code to Rcpp as an R character vector, and this is convenient for short injections. For larger functions, you can store the C++ code in an extra file. The idea is, of course, not to write a lot of code in C++, but only the most time critical parts.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#summary-of-this-chapter",
    "href": "13-chap.html#summary-of-this-chapter",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.13 13.14 Summary of this chapter",
    "text": "15.13 13.14 Summary of this chapter\nIn this last chapter, we have tried to collect, generalize and sort some of the concepts and ideas that popped up throughout the book, and that can help you design informative experiments or studies and analyze them effectively. Some of these ideas are intuitive and natural. Others are perhaps less intuitive, such as Hotelling’s weighting example in Section 13.4.3. It requires formal mathematical reasoning. Even when you cannot do an analytical computation, you might be able to do simulations or compute on existing, similar data to benchmark different, non-obvious design choices.\nYet again other ideas require discipline and foresight: for instance, the “dailies” might be easily forgotten or rationalized away in the heat of an experimental campaign, with so many other concerns competing for our time and attention. You might get away with skipping on keeping your kitchen tidy or eating healthily on individual occasions – as a general approach, it is not recommended.\nWe emphasized the importance of computing practices. Throughout the book, with its quantity of interweaved code and almost all “live” data visualizations, we have seen many examples of how to set up computational analyses. Nevertheless, running your own analysis on your own data is something very different from following the computations in a book – just like reading a cookbook is very different from preparing a banquet, or even just one dish. To equip you further, we highly recommend the resources mentioned in Section 13.15. And we wish you good cooking!",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#further-reading",
    "href": "13-chap.html#further-reading",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.14 13.15 Further reading",
    "text": "15.14 13.15 Further reading\n\nThis chapter presented merely a pragmatic and brief introduction to experimental design. There are many book-long treatments that offer detailed advice on setting up experiments to avoid confounding and optimize power (Wu and Hamada 2011; Box, Hunter, and Hunter 1978; Glass 2007).\nWe have not scratched the surface of more sophisticated procedures. For instance if you have the possibility of setting up a sequence of experiments that you might stop once you can make a decision, you will need to study sequential design (Lai 2001). Exploring complex response surfaces by choosing “good” starting points and then using successive results to choose further points can be very effective; Box, Draper, et al. (1987) is an invaluable resource.\nGentleman et al. (2004) explain the ideas behind Bioconductor data structures and software design, and Huber et al. (2015) give an update on how Bioconductor supports collaborative software development for users and developers.\nGit and GitHub. Jenny Bryan’s website Happy Git and GitHub for the useR is a great introduction to using version control with R.\nWickham (2014) explains the principles of tidy data.\nGood enough practices. Wilson et al. (2017) give a pragmatic and wise set of recommendations for how to be successful in scientific computing.\nThe manual Writing R Extensions is the ultimate reference for R package authoring. It can be consumed in conjunction with the Bioconductor package guidelines.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "13-chap.html#exercises",
    "href": "13-chap.html#exercises",
    "title": "15  13.1 Goals for this chapter",
    "section": "15.15 13.16 Exercises",
    "text": "15.15 13.16 Exercises\n__\nExercise 13.1\nSet up a simulation experiment to decide how many subjects you need, given that you know your measurements will be affected by noise that follows a symmetric Laplace distribution (infinite mixture of normal distributions as defined in Chapter 4). You will need to set up a table with different possible noise levels and effect sizes.\n__\nExercise 13.2\nUse the Bioconductor package PROPER to decide the number of samples for an RNA-Seq experiment, and compare the results to those from the RNASeqPower Bioconductor package.\n__\nExercise 13.3\nCheck out R’s model.matrix function. Read its manual page and explore the examples given there.\n__\nExercise 13.4\nGo back to one of your recent data analyses and assemble it into an R package.\n__\nSolution\n__\n\nCollect one or more recurrent operations (e.g., plots) into functions and document them with manual pages (you may use roxygen2).\nAdd the dataset under the data or inst/extdata directories.\nIf it is not already in that format, convert your analysis script to Rmarkdown.\nRun R CMD build and R CMD check until all errors and warnings disappear.\n\nA simple intro is given here: &lt;https://hilaryparker.com/2014/04/29/writing-an- r-package-from-scratch&gt;, futher details are in the manual Writing R Extensions that comes with every installation of R.\n__\nExercise 13.5\nOpen an account at GitHub and upload your package. Hint: follow the instructions at Jenny Bryan’s Happy Git and GitHub for the useR site.\n__\nExercise 13.6\nCheck out the renjin project and the renjin package. Compare code compiled with renjin with native R code, and with code translated into C/C++ with Rcpp as above.\n__\nSolution\n__\nSee the Gist at https://gist.github.com/wolfganghuber/909e14e45af6888eec384b82682b3766.\n1000 Genomes Project Consortium. 2012. “An Integrated Map of Genetic Variation from 1,092 Human Genomes.” Nature 491 (7422): 56–65.\nAltman, Naomi, and Martin Krzywinski. 2017. “Points of Significance: Interpreting p Values.” Nature Methods 14 (3): 213–14. https://doi.org/10.1038/nmeth.4210.\nBacher, Rhonda, and Christina Kendziorski. 2016. “Design and Computational Analysis of Single-Cell RNA-Sequencing Experiments.” Genome Biology 17 (1): 1.\nBox, George EP, Norman Richard Draper, et al. 1987. Empirical Model-Building and Response Surfaces. Vol. 424. Wiley New York.\nBox, George EP, William G Hunter, and J Stuart Hunter. 1978. Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building. John Wiley & Sons.\nCallahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy J Johnson, and Susan P Holmes. 2016. “DADA2: High Resolution Sample Inference from Amplicon Data.” Nature Methods , 1–4.\nDiaconis, Persi, Susan Holmes, and Richard Montgomery. 2007. “Dynamical Bias in the Coin Toss.” SIAM Review 49 (2): 211–35.\nDurbin, Richard, Sean Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis. Cambridge University Press.\nFisher, Ronald Aylmer. 1935. The Design of Experiments. Oliver & Boyd.\nGentleman, Robert C, Vincent J Carey, Douglas M Bates, Ben Bolstad, Marcel Dettling, Sandrine Dudoit, Byron Ellis, et al. 2004. “Bioconductor: Open Software Development for Computational Biology and Bioinformatics.” Genome Biology 5 (10): R80. https://doi.org/10.1186/gb-2004-5-10-r80.\nGlass, David J. 2007. Experimental Design for Biologists. Cold Spring Harbor Laboratory Press.\nHenderson, Fergus. 2017. “Software Engineering at Google.” ArXiv e-Prints. https://arxiv.org/abs/1702.01715.\nHotelling, Harold. 1944. “Some Improvements in Weighing and Other Experimental Techniques.” The Annals of Mathematical Statistics 15 (3): 297–306.\nHuber, Wolfgang, Vincent J Carey, Robert Gentleman, Simon Anders, Marc Carlson, Benilton S Carvalho, Hector Corrada Bravo, et al. 2015. “Orchestrating High-Throughput Genomic Analysis with Bioconductor.” Nature Methods 12 (2): 115–21.\nJacob, Laurent, Guillaume Obozinski, and Jean-Philippe Vert. 2009. “Group Lasso with Overlap and Graph Lasso.” In Proceedings of the 26th Annual International Conference on Machine Learning , 433–40. ACM.\nLai, Tze Leung. 2001. Sequential Analysis. Wiley Online Library.\nLeek, Jeffrey T, Robert B Scharpf, Héctor Corrada Bravo, David Simcha, Benjamin Langmead, W Evan Johnson, Donald Geman, Keith Baggerly, and Rafael A Irizarry. 2010. “Tackling the Widespread and Critical Impact of Batch Effects in High-Throughput Data.” Nature Reviews Genetics 11 (10): 733–39.\nLeek, Jeffrey T., and John D. Storey. 2007. “Capturing heterogeneity in gene expression studies by surrogate variable analysis.” PLoS Genetics 3 (9): 1724–35.\nLiberzon, Arthur, Aravind Subramanian, Reid Pinchback, Helga Thorvaldsdóttir, Pablo Tamayo, and Jill P Mesirov. 2011. “Molecular Signatures Database (MSigDB) 3.0.” Bioinformatics 27 (12): 1739–40.\nMead, Roger. 1990. The Design of Experiments: Statistical Principles for Practical Applications. Cambridge University Press.\nMood, Alexander M. 1946. “On Hotelling’s Weighing Problem.” The Annals of Mathematical Statistics , 432–46.\nRocke, David M, and Blythe Durbin. 2001. “A Model for Measurement Error for Gene Expression Arrays.” Journal of Computational Biology 8 (6): 557–69.\nSenn, Stephen. 2004. “Controversies Concerning Randomization and Additivity in Clinical Trials.” Statistics in Medicine 23: 3729–53.\nStegle, O., L. Parts, R. Durbin, and J. Winn. 2010. “A Bayesian framework to account for complex non-genetic factors in gene expression levels greatly increases power in eQTL studies.” PLoS Computational Biology 6 (5): e1000770.\nStigler, Stephen M. 2016. The Seven Pillars of Statistical Wisdom. Harvard University Press.\nWickham, Hadley. 2014. “Tidy Data.” Journal of Statistical Software 59 (10).\nWiel, Mark A, Tonje G Lien, Wina Verlaat, Wessel N Wieringen, and Saskia M Wilting. 2016. “Better Prediction by Use of Co-Data: Adaptive Group- Regularized Ridge Regression.” Statistics in Medicine 35 (3): 368–81.\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” Edited by Francis Ouellette. PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510.\nWu, CF Jeff, and Michael S Hamada. 2011. Experiments: Planning, Analysis, and Optimization. Vol. 552. John Wiley & Sons.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>13.1 Goals for this chapter</span>"
    ]
  },
  {
    "objectID": "14-chap.html",
    "href": "14-chap.html",
    "title": "Modern Statistics for Modern Biology",
    "section": "",
    "text": "As a supplement to the index, we provide a list of statistical concepts and procedures and the chapters in which they are covered.\n\n\n\nMethod\nChapter\n\n\n\n\nAnalysis of variance\n8 High-Throughput Count Data & Generalized Linear Models\n\n\nBayesian statistics\n2 Statistical Modeling\n\n\nBootstrap\n4 Mixture Models, 5 Clustering\n\n\nChi-square test\n2 Statistical Modeling\n\n\nClustering\n4 Mixture Models, 5 Clustering\n\n\nCorrespondence Analysis\n9 Multivariate methods for heterogeneous data\n\n\nData Transformations\n4 Mixture Models, 8 High-Throughput Count Data & Generalized Linear Models\n\n\nDiffusion Map models\n9 Multivariate methods for heterogeneous data\n\n\nDistances\n4 Mixture Models, 9 Multivariate methods for heterogeneous data\n\n\nExploratory Data Analysis (EDA)\n3 Data visualization\n\n\nFisher’s Exact test\n10 Networks and Trees\n\n\nFalse Discovery Rate (FDR)\n6 Testing\n\n\nGoodness of fit\n2 Statistical Modeling\n\n\nGeneralized Linear Models\n8 High-Throughput Count Data & Generalized Linear Models\n\n\nHypergeometric Test\n10 Networks and Trees\n\n\nHypothesis Testing\n1 Generative Models for Discrete Data, 6 Testing\n\n\nMarkov Chains\n2 Statistical Modeling, 10 Networks and Trees\n\n\nMaximum Likelihood\n2 Statistical Modeling\n\n\nMultidimensional Scaling (MDS)\n9 Multivariate methods for heterogeneous data\n\n\nMultiple hypothesis testing\n6 Testing\n\n\nMultivariate Regression\n8 High-Throughput Count Data & Generalized Linear Models, 9 Multivariate methods for heterogeneous data, 12 Supervised Learning\n\n\nOrdination and gradient detection\n9 Multivariate methods for heterogeneous data\n\n\nP-value\n1 Generative Models for Discrete Data\n\n\nPermutation tests\n10 Networks and Trees\n\n\nPhylogenetics\n10 Networks and Trees\n\n\nPower calculations\n13 Design of High Throughput Experiments and their Analyses\n\n\nPrincipal Components (PCA)\n7 Multivariate Analysis\n\n\nPrincipal Coordinates Analysis (PCoA)\n9 Multivariate methods for heterogeneous data\n\n\nRegression\n7 Multivariate Analysis,\n\n\nRobust methods\n12 Supervised Learning\n\n\nSpatial statistics\n11 Image data\n\n\n\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>14-chap.html</span>"
    ]
  },
  {
    "objectID": "15-chap.html",
    "href": "15-chap.html",
    "title": "Modern Statistics for Modern Biology",
    "section": "",
    "text": "For Sonia, Sara, Agnès, Johnny, Camille\n\\(…\\) and the “girls” who make me love the life sciences.\nFor Alexander and Daniel.\nThis work would not be imaginable without the R language and environment for statistical computing, the Comprehensive R Archive Network (CRAN) and the Bioconductor project. We thank everyone who has contributed to these projects. Today virtually every statistical algorithm, every imaginable interface for data handling and visualization, and many methods from all over computer science and mathematics are readily accessible through these projects.\nWe thank JJ Allaire and the RStudio (since 2022: Posit) team for making available such a powerful development environment and many useful R packages, which we have greatly enjoyed when writing this book. This includes the quarto publishing system, which builds upon knitr and pandoc.\nWe particularly thank the Bioconductor project, powered by its amazing community of developers, for fostering interoperability, scalability and usability of R-based methods for genome-scale data, for making a vast range of biological data and annotation resources easy to work with in R, and for orchestrating collaborative, distributed development – all these aspects are essential for the complex biological data analysis workflows that you will see in this book.\nWe are grateful to the package developers we have worked with and whose packages play at center stage in the different chapters of this book, including Simon Anders, Ben Callahan, Michael Love, Joey McMurdie, Andrzej Oleś. Trevor Martin was a student in Stats 366 at Stanford in 2012 and co-taught the class with Susan in 2013, 2014 and 2015. As a graduate student in genetics, he brought many of the examples to life and participated in earlier versions of the material we present here. We are thankful for his help and perspective. Teaching assistants for Stats 366 who have helped develop exercises and questions include Austen Head, Haben Michael, Julia Fukuyama, Lan Huong Nguyen, Christof Seiler and Nikolaos Ignatiadis. Their enthusiasm for making interesting quizzes and lab material helped nurture students from a wide range of backgrounds on the arduous journey of approaching challenging new concepts within a computational environment that has tremendous power, yet can also be overwhelming.\nMany students have provided valuable feedback over the years, and we are grateful for their many questions and quizzical looks that fed our motivation to keep evolving this course. In particular, we have received extensive feedback from Jessica Grembi, Kris Sankaran, Varun Gupta and Chao Jiang.\nMike Smith has created the infrastructure for the online HTML version that you are currently perusing, including a continuous integration framework that enables us to continually update the book and see the changes online within minutes, and the R package msmbstyle that was used for rendering the print version of the book and the first years of the HTML version. We thank Helena Lucia Crowell for her excellent work porting the book’s sources from Sweave to quarto. Sviatoslav Kharuk made further improvements. We are very grateful for their help and responsiveness.\nWe thank Lorraine Garchery for design of the cover art both for the printed and online version.\nWe thank David Tranah and Diana Gillooly from Cambridge University press for their constant effort helping us to make the book grammatically correct, aesthetically attractive and pedagogically coherent. Much potential for improvement remains, the responsibility for which stays with us.\nWe thank our family and supporters who have encouraged us and provided feedback on preliminary chapters: David Relman, Alfred Spormann, Catherine Blish, Don Knuth, Persi Diaconis, Gretchen and Barry Mazur, …..\nSusan Holmes, Stanford\nWolfgang Huber, Heidelberg\nThe following readers contributed to improvements of the online version by pointing out typos and opportunities for clarification: Eva-Maria Geissen, Nick Cox, Constantin Ahlmann-Eltze, Tümay Capraz, Irilenia Nobeli, Asger Hobolth, Iulian Ichim, …\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>15-chap.html</span>"
    ]
  },
  {
    "objectID": "16-chap.html",
    "href": "16-chap.html",
    "title": "Modern Statistics for Modern Biology",
    "section": "",
    "text": "1000 Genomes Project Consortium. 2012. “An Integrated Map of Genetic Variation from 1,092 Human Genomes.” Nature 491 (7422): 56–65.\nAbbott, Edwin A. 1884. Flatland: A Romance of Many Dimensions. OUP Oxford.\nAgresti, Alan. 2007. An Introduction to Categorical Data Analysis. John Wiley.\nAltman, Naomi, and Martin Krzywinski. 2017. “Points of Significance: Interpreting p Values.” Nature Methods 14 (3): 213–14. https://doi.org/10.1038/nmeth.4210.\nAmbroise, Christophe, and Geoffrey J. McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” PNAS 99 (10): 6562–66.\nAnders, Simon, and Wolfgang Huber. 2010. “Differential Expression Analysis for Sequence Count Data.” Genome Biology 11: R106. http://genomebiology.com/2010/11/10/R106.\nAnders, Simon, Alejandro Reyes, and Wolfgang Huber. 2012. “Detecting differential usage of exons from RNA-Seq data.” Genome Research 22 (10): 2008–17.\nAnscombe, Francis J. 1948. “The Transformation of Poisson, Binomial and Negative-Binomial Data.” Biometrika , 246–54.\nAure, Miriam Ragle, Valeria Vitelli, Sandra Jernström, Surendra Kumar, Marit Krohn, Eldri U Due, Tonje Husby Haukaas, et al. 2017. “Integrative Clustering Reveals a Novel Split in the Luminal A Subtype of Breast Cancer with Impact on Outcome.” Breast Cancer Research 19 (1): 44.\nBacher, Rhonda, and Christina Kendziorski. 2016. “Design and Computational Analysis of Single-Cell RNA-Sequencing Experiments.” Genome Biology 17 (1): 1.\nBaddeley, Adrain, Jesper Moller, and Rasmus Waagepetersen. 2000. “Non- and Semiparametric Estimation of Interaction in Inhomogeneous Point Patterns.” Statistica Neerlandica 54: 329–50.\nBaddeley, Adrian J. 1998. “Spatial Sampling and Censoring.” In Stochastic Geometry: Likelihood and Computation , edited by O. E. Barndorff-Nielsen, W. S. Kendall, and M. N. M. van Lieshout, 37–78. Chapman; Hall.\nBeisser, Daniela, Gunnar W Klau, Thomas Dandekar, Tobias Müller, and Marcus T Dittrich. 2010. “BioNet: An R-Package for the Functional Analysis of Biological Networks.” Bioinformatics 26 (8): 1129–30.\nBelkin, Mikhail, and Partha Niyogi. 2003. “Laplacian Eigenmaps for Dimensionality Reduction and Data Representation.” Neural Computation 15 (6): 1373–96.\nBellman, Richard Ernest. 1961. Adaptive Control Processes: A Guided Tour. Princeton University Press.\nBendall, Sean C, Garry P Nolan, Mario Roederer, and Pratip K Chattopadhyay. 2012. “A Deep Profiler’s Guide to Cytometry.” Trends in Immunology 33 (7): 323–32.\nBengio, Yoshua, Jean-François Paiement, Pascal Vincent, Olivier Delalleau, Nicolas Le Roux, and Marie Ouimet. 2004. “Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering.” Advances in Neural Information Processing Systems 16: 177–84.\nBenjamini, Yoav, and Marina Bogomolov. 2014. “Selective Inference on Multiple Families of Hypotheses.” Journal of the Royal Statistical Society: Series B 76 (1): 297–318.\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society B 57: 289–300.\nBenjamini, Yoav, and Daniel Yekutieli. 2003. “Hierarchical FDR Testing of Trees of Hypotheses.” Technical report, Department of Statistics; Operations Research, Tel Aviv University.\nBerg, Stuart, Dominik Kutra, Thorben Kroeger, Christoph N Straehle, Bernhard X Kausler, Carsten Haubold, Martin Schiegg, et al. 2019. “Ilastik: Interactive Machine Learning for (Bio)image Analysis.” Nature Methods 16 (12): 1226–32.\nBhattacharya, Bhaswar B. 2015. “Power of Graph-Based Two-Sample Tests.” arXiv Preprint arXiv:1508.07530.\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\nBoland, Michael V., and Robert F. Murphy. 2001. “A neural network classifier capable of recognizing the patterns of all major subcellular structures in fluorescence microscope images of HeLa cells.” Bioinformatics 17 (12): 1213–23.\nBouckaert, Remco, Joseph Heled, Denise Kühnert, Tim Vaughan, Chieh-Hsi Wu, Dong Xie, Marc A Suchard, Andrew Rambaut, and Alexei J Drummond. 2014. “BEAST 2: A Software Platform for Bayesian Evolutionary Analysis.” PLoS Computational Biology 10 (4): e1003537.\nBox, George EP, Norman Richard Draper, et al. 1987. Empirical Model-Building and Response Surfaces. Vol. 424. Wiley New York.\nBox, George EP, William G Hunter, and J Stuart Hunter. 1978. Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building. John Wiley & Sons.\nBraak, Cajo ter. 1985. “Correspondence Analysis of Incidence and Abundance Data: Properties in Terms of a Unimodal Respose.” Biometrics 41 (January).\nBrodie, Eoin L, Todd Z DeSantis, Dominique C Joyner, Seung M Baek, Joern T Larsen, Gary L Andersen, Terry C Hazen, et al. 2006. “Application of a High- Density Oligonucleotide Microarray Approach to Study Bacterial Population Dynamics During Uranium Reduction and Reoxidation.” Applied and Environmental Microbiology 72 (9): 6288–98.\nBronštein, Il’ja N., and Konstantin A Semendjajew. 1979. Taschenbuch Der Mathematik. B.G. Teubner Verlagsgesellschaft, Leipzig; Verlag Nauka, Moscow.\nBrooks, Angela N, Li Yang, Michael O Duff, Kasper D Hansen, Jung W Park, Sandrine Dudoit, Steven E Brenner, and Brenton R Graveley. 2011. “Conservation of an RNA Regulatory Map Between Drosophila and Mammals.” Genome Research , 193–202. https://doi.org/10.1101/gr.108662.110.\nBulmer, Michael George. 2003. Francis Galton: Pioneer of Heredity and Biometry. JHU Press.\nCallahan, Ben J, Kris Sankaran, Julia A Fukuyama, Paul J McMurdie, and Susan P Holmes. 2016. “Bioconductor Workflow for Microbiome Data Analysis: From Raw Reads to Community Analyses.” F1000Research 5.\nCallahan, Benjamin J, Paul J McMurdie, and Susan P Holmes. 2017. “Exact Sequence Variants Should Replace Operational Taxonomic Units in Marker Gene Data Analysis.” ISME Journal , 1–5.\nCallahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy J Johnson, and Susan P Holmes. 2016. “DADA2: High Resolution Sample Inference from Amplicon Data.” Nature Methods , 1–4.\nCannings, Chris, and Anthony WF Edwards. 1968. “Natural Selection and the de Finetti Diagram.” Annals of Human Genetics 31 (4): 421–28.\nCaporaso, J. G., J. Kuczynski, J. Stombaugh, K. Bittinger, F. D. Bushman, E. K. Costello, N. Fierer, et al. 2010. “QIIME Allows Analysis of High-Throughput Community Sequencing Data.” Nature Methods 7 (5): 335–36.\nCarpenter, Anne E, Thouis R Jones, Michael R Lamprecht, Colin Clarke, In Han Kang, Ola Friman, David A Guertin, Joo Han Chang, Robert A Lindquist, and Jason Moffat. 2006. “CellProfiler: Image Analysis Software for Identifying and Quantifying Cell Phenotypes.” Genome Biology 7: R100.\nCarr, Daniel B, Richard J Littlefield, WL Nicholson, and JS Littlefield. 1987. “Scatterplot Matrix Techniques for Large N.” Journal of the American Statistical Association 82 (398): 424–36.\nChakerian, John, and Susan Holmes. 2012. “Computational Tools for Evaluating Phylogenetic and Hierarchical Clustering Trees.” Journal of Computational and Graphical Statistics 21 (3): 581–99.\nChaumont, Fabrice de, Stéphane Dallongeville, Nicolas Chenouard, Nicolas Hervé, Sorin Pop, Thomas Provoost, Vannary Meas-Yedid, et al. 2012. “Icy: an open bioimage informatics platform for extended reproducible research.” Nature Methods 9: 690–96.\nChen, Min, Yang Xie, and Michael Story. 2011. “An Exponential-Gamma Convolution Model for Background Correction of Illumina BeadArray Data.” Communications in Statistics-Theory and Methods 40 (17): 3055–69.\nChessel, Daniel, Anne Dufour, and Jean Thioulouse. 2004. “The ade4 Package - i: One-Table Methods.” R News 4 (1): 5–10. http://CRAN.R-project.org/doc/Rnews/.\nChiu, Sung Nok, Dietrich Stoyan, Wilfrid S. Kendall, and Joseph Mecke. 2013. Stochastic Geometry and Its Applications. Springer.\nClemmensen, Line, Trevor Hastie, Daniela Witten, and Bjarne Ersbøll. 2011. “Sparse Discriminant Analysis.” Technometrics 53: 406–13.\nCleveland, William S. 1988. The Collected Works of John w. Tukey: Graphics 1965-1985. Vol. 5. CRC Press.\nCleveland, William S., Marylyn E. McGill, and Robert McGill. 1988. “The Shape Parameter of a Two-Variable Graph.” Journal of the American Statistical Association 83: 289–300.\nCole, J. R., Q. Wang, E. Cardenas, J. Fish, B. Chai, R. J. Farris, A. S. Kulam-Syed-Mohideen, et al. 2009. “The Ribosomal Database Project: Improved Alignments and New Tools for rRNA Analysis.” Nucleic Acids Research 37 (Supplement 1): D141–45.\nCook, R. Dennis. 1977. “Detection of Influential Observation in Linear Regression.” Technometrics.\nCressie, Noel A. 1991. Statistics for Spatial Data. John Wiley; Sons.\nDiaconis, Persi, and David Freedman. 1980. “Finite Exchangeable Sequences.” The Annals of Probability , 745–64.\nDiaconis, Persi, Sharad Goel, and Susan Holmes. 2008. “Horseshoes in Multidimensional Scaling and Kernel Methods.” Annals of Applied Statistics 2: 777. https://doi.org/DOI:10.1214/08-AOAS165.\nDiaconis, Persi, and Susan Holmes. 1994. “Gray Codes for Randomization Procedures.” Statistics and Computing 4 (4): 287–302.\nDiaconis, Persi, Susan Holmes, and Richard Montgomery. 2007. “Dynamical Bias in the Coin Toss.” SIAM Review 49 (2): 211–35.\nDiday, Edwin, and M Paula Brito. 1989. “Symbolic Cluster Analysis.” In Conceptual and Numerical Analysis of Data , 45–84. Springer.\nDiggle, Peter J. 2013. Statistical Analysis of Spatial and Spatio-Temporal Point Patterns. Chapman; Hall/CRCs.\nDiGiulio, Daniel B., Benjamin J. Callahan, Paul J. McMurdie, Elizabeth K. Costello, Deirdre J. Lyelle, Anna Robaczewska, Christine L. Sun, et al. 2015. “Temporal and Spatial Variation of the Human Microbiota During Pregnancy.” PNAS.\nDundar, Murat, Ferit Akova, Halid Z. Yerebakan, and Bartek Rajwa. 2014. “A Non-Parametric Bayesian Model for Joint Cell Clustering and Cluster Matching: Identification of Anomalous Sample Phenotypes with Random Effects.” BMC Bioinformatics 15 (1): 1–15. https://doi.org/10.1186/1471-2105-15-314.\nDurbin, Richard, Sean Eddy, Anders Krogh, and Graeme Mitchison. 1998. Biological Sequence Analysis. Cambridge University Press.\nEfron, Bradley. 2010. Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction. Cambridge University Press.\nEfron, Bradley, and Robert J Tibshirani. 1994. An Introduction to the Bootstrap. CRC press.\nEfron, B., and R. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman & Hall/CRC.\nEkman, Gosta. 1954. “Dimensions of Color Vision.” The Journal of Psychology 38 (2): 467–74.\nElson, D, and E Chargaff. 1952. “On the Desoxyribonucleic Acid Content of Sea Urchin Gametes.” Experientia 8 (4): 143–45.\nFelsenstein, Joseph. 2004. Inferring Phylogenies. Boston: Sinauer.\nFinetti, Bruno de. 1926. “Considerazioni Matematiche Sull’ereditarieta Mendeliana.” Metron 6: 3–41.\nFisher, Ronald Aylmer. 1935. The Design of Experiments. Oliver & Boyd.\nFlury, Bernard. 1997. A First Course in Multivariate Statistics. Springer.\nFreedman, David A. 1991. “Statistical Models and Shoe Leather.” Sociological Methodology 21 (2): 291–313.\nFreedman, David, Robert Pisani, and Roger Purves. 1997. Statistics. New York, NY: WW Norton.\nFriedman, Jerome H. 1997. “On Bias, Variance, 0/1—Loss, and the Curse-of- Dimensionality.” Data Mining and Knowledge Discovery 1: 55–77.\nFriedman, Jerome H, and Lawrence C Rafsky. 1979. “Multivariate Generalizations of the Wald-Wolfowitz and Smirnov Two-Sample Tests.” The Annals of Statistics , 697–717.\nFukuyama, Julia, Paul J McMurdie, Les Dethlefsen, David A Relman, and Susan Holmes. 2012. “Comparisons of Distance Methods for Combining Covariates and Abundances in Microbiome Studies.” In Pac Symp Biocomput. World Scientific.\nGentleman, Robert C, Vincent J Carey, Douglas M Bates, Ben Bolstad, Marcel Dettling, Sandrine Dudoit, Byron Ellis, et al. 2004. “Bioconductor: Open Software Development for Computational Biology and Bioinformatics.” Genome Biology 5 (10): R80. https://doi.org/10.1186/gb-2004-5-10-r80.\nGlass, David J. 2007. Experimental Design for Biologists. Cold Spring Harbor Laboratory Press.\nGoslee, Sarah C, Dean L Urban, et al. 2007. “The Ecodist Package for Dissimilarity-Based Analysis of Ecological Data.” Journal of Statistical Software 22 (7): 1–19.\nGrantham, Richard, Christian Gautier, Manolo Gouy, M Jacobzone, and R Mercier. 1981. “Codon Catalog Usage Is a Genome Strategy Modulated for Gene Expressivity.” Nucleic Acids Research 9 (1): 213–13.\nGreenacre, Michael J. 2007. Correspondence Analysis in Practice. Chapman & Hall.\nGrolemund, Garrett, and Hadley Wickham. 2017. R for Data Science. O’Reilly.\nGrün, Bettina, Theresa Scharl, and Friedrich Leisch. 2012. “Modelling Time Course Gene Expression Data with Finite Mixtures of Linear Additive Models.” Bioinformatics 28 (2): 222–28. https://doi.org/10.1093/bioinformatics/btr653.\nGuillot, Gilles, and François Rousset. 2013. “Dismantling the Mantel Tests.” Methods in Ecology and Evolution 4 (4): 336–44.\nHallett, Robin M, Anna Dvorkin-Gheva, Anita Bane, and John A Hassell. 2012. “A Gene Signature for Predicting Outcome in Patients with Basal-Like Breast Cancer.” Scientific Reports 2.\nHastie, Trevor, and Werner Stuetzle. 1989. “Principal Curves.” Journal of the American Statistical Association 84 (406): 502–16.\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2008. The Elements of Statistical Learning. 2^{} ed. Springer.\nHead, Megan L, Luke Holman, Rob Lanfear, Andrew T Kahn, and Michael D Jennions. 2015. “The Extent and Consequences of p-Hacking in Science.” PLoS Biology 13 (3): e1002106.\nHeld, M., M. H. A. Schmitz, B. Fischer, T. Walter, B. Neumann, M. H. Olma, M. Peter, J. Ellenberg, and D. W. Gerlich. 2010. “CellCognition: Time-Resolved Phenotype Annotation in High-Throughput Live Cell Imaging.” Nature Methods 7: 747.\nHelmholtz, H. von. 1867. Handbuch Der Physiologischen Optik. Leipzig: Leopold Voss.\nHenderson, Fergus. 2017. “Software Engineering at Google.” ArXiv e-Prints. https://arxiv.org/abs/1702.01715.\nHoeting, Jennifer A, David Madigan, Adrian E Raftery, and Chris T Volinsky. 1999. “Bayesian Model Averaging: A Tutorial.” Statistical Science , 382–401.\nHolmes - Junca, Susan. 1985. “Outils Informatiques Pour l’évaluation de La Pertinence d’un résultat En Analyse Des Données.” PhD thesis, Université Montpellier II, France.\nHolmes, Susan. 1999. “Phylogenetic Trees: An Overview.” In Statistics and Genetics , 81–118. IMA 112. New York: Springer.\n———. 2003a. “Bootstrapping Phylogenetic Trees: Theory and Methods.” Statistical Science 18 (2): 241–55.\n———. 2003b. “Statistics for phylogenetic trees.” Theoretical Population Biology 63 (1): 17–32.\n———. 2006. “Multivariate Analysis: The French way.” In Probability and Statistics: Essays in Honor of David a. Freedman , edited by D. Nolan and T. P. Speed. Vol. 56. IMS Lecture Notes–Monograph Series. Beachwood, OH: IMS. http://www.imstat.org/publications/lecnotes.htm.\n———. 2018. “Statistical Proof? The Problem of Irreproducibility.” Bulletin of the AMS 55 (1): 31–55.\nHolmes, Susan, Alexander V Alekseyenko, Alden Timme, Tyrrell Nelson, Pankaj Jay Pasricha, and Alfred Spormann. 2011. “Visualization and Statistical Comparisons of Microbial Communities Using r Packages on Phylochip Data.” In Pacific Symposium on Biocomputing , 142–53. World Scientific.\nHolmes, Susan, Michael He, Tong Xu, and Peter P Lee. 2005. “Memory t Cells Have Gene Expression Patterns Intermediate Between Naive and Effector.” PNAS 102 (15): 5519–23.\nHolmes, Susan, Adam Kapelner, and Peter P Lee. 2009. “An Interactive Java Statistical Image Segmentation System: GemIdent.” Journal of Statistical Software 30 (10).\nHornik, Kurt. 2005. “A CLUE for CLUster Ensembles.” Journal of Statistical Software 14 (12).\nHotelling, Harold. 1933. “Analysis of a Complex of Statistical Variables into Principal Components.” Journal of Educational Psychology 24 (6): 417–41.\n———. 1944. “Some Improvements in Weighing and Other Experimental Techniques.” The Annals of Mathematical Statistics 15 (3): 297–306.\nHuber, Peter J. 1964. “Robust Estimation of a Location Parameter.” The Annals of Mathematical Statistics 35: 73–101.\nHuber, Wolfgang, Vincent J Carey, Robert Gentleman, Simon Anders, Marc Carlson, Benilton S Carvalho, Hector Corrada Bravo, et al. 2015. “Orchestrating High-Throughput Genomic Analysis with Bioconductor.” Nature Methods 12 (2): 115–21.\nHulett, Henry R, William A Bonner, Janet Barrett, and Leonard A Herzenberg. 1969. “Cell Sorting: Automated Separation of Mammalian Cells as a Function of Intracellular Fluorescence.” Science 166 (3906): 747–49.\nIdeker, Trey, Owen Ozier, Benno Schwikowski, and Andrew F Siegel. 2002. “Discovering Regulatory and Signalling Circuits in Molecular Interaction Networks.” Bioinformatics 18 Suppl 1 (January): S233–40. http://bioinformatics.oxfordjournals.org/cgi/reprint/18/suppl\\_1/S233.\nIgnatiadis, Nikolaos, and Wolfgang Huber. 2021. “Covariate Powered Cross- Weighted Multiple Testing.” Journal of the Royal Statistical Society: Series B 83: 720–51. https://doi.org/10.1111/rssb.12411.\nIgnatiadis, Nikolaos, Bernd Klaus, Judith Zaugg, and Wolfgang Huber. 2016. “Data-Driven Hypothesis Weighting Increases Detection Power in Genome-Scale Multiple Testing.” Nature Methods 13: 577–80.\nIhaka, Ross. 2003. “Color for Presentation Graphics.” In Proceedings of the 3rd International Workshop on Distributed Statistical Computing , edited by Kurt Hornik and Friedrich Leisch. Vienna, Austria: http://www.r-project.org/conferences/DSC-2003/Proceedings/; ISSN 1609-395X.\nIhaka, Ross, and Robert Gentleman. 1996. “R: A Language for Data Analysis and Graphics.” Journal of Computational and Graphical Statistics 5 (3): 299–314.\nIrizarry, R. A., B. Hobbs, F. Collin, Y. D. Beazer-Barclay, K. J. Antonellis, U. Scherf, and T. P. Speed. 2003. “Exploration, Normalization, and Summaries of High Density Oligonucleotide Array Probe Level Data.” Biostatistics 4 (2): 249–64.\nIrizarry, Rafael A, Hao Wu, and Andrew P Feinberg. 2009. “A Species- Generalized Probabilistic Model-Based Definition of CpG Islands.” Mammalian Genome 20 (9-10): 674–80.\nIzenman, Alan Julian. 2008. “Nonlinear Dimensionality Reduction and Manifold Learning.” In Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning , 597–632. New York, NY: Springer New York.\nJacob, Laurent, Guillaume Obozinski, and Jean-Philippe Vert. 2009. “Group Lasso with Overlap and Graph Lasso.” In Proceedings of the 26th Annual International Conference on Machine Learning , 433–40. ACM.\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Springer.\nJolicoeur, Pierre, and James E Mosimann. 1960. “Size and Shape Variation in the Painted Turtle. A Principal Component Analysis.” Growth 24: 339–54.\nJolliffe, Ian. 2002. Principal Component Analysis. Wiley Online Library.\nJones, T., A. Carpenter, and P. Golland. 2005. “Voronoi-Based Segmentation of Cells on Image Manifolds.” Computer Vision for Biomedical Image Applications , 535.\nJosse, Julie, and Susan Holmes. 2016. “Measuring Multivariate Association and Beyond.” Statistics Surveys 10: 132–67.\nKahneman, Daniel. 2011. Thinking, Fast and Slow. Macmillan.\nKashyap, Purna C, Angela Marcobal, Luke K Ursell, Samuel A Smits, Erica D Sonnenburg, Elizabeth K Costello, Steven K Higginbottom, et al. 2013. “Genetically Dictated Change in Host Mucus Carbohydrate Landscape Exerts a Diet-Dependent Effect on the Gut Microbiota.” PNAS 110 (42): 17059–64.\nKaufman, Leonard, and Peter J Rousseeuw. 2009. Finding Groups in Data: An Introduction to Cluster Analysis. Vol. 344. John Wiley & Sons.\nKendall, David. 1969. “Incidence Matrices, Interval Graphs and Seriation in Archeology.” Pacific Journal of Mathematics 28 (3): 565–70.\nKéry, Marc, and J Andrew Royle. 2015. Applied Hierarchical Modeling in Ecology: Analysis of Distribution, Abundance and Species Richness in r and BUGS: Volume 1: Prelude and Static Models. Academic Press.\nKorthauer, K., P. K. Kimes, C. Duvallet, A. Reyes, A. Subramanian, M. Teng, C. Shukla, E. J. Alm, and S. C. Hicks. 2019. “A practical guide to methods controlling false discoveries in computational biology.” Genome Biology 20 (1): 118.\nKozich, James J, Sarah L Westcott, Nielson T Baxter, Sarah K Highlander, and Patrick D Schloss. 2013. “Development of a Dual-Index Sequencing Strategy and Curation Pipeline for Analyzing Amplicon Sequence Data on the MiSeq Illumina Sequencing Platform.” Applied and Environmental Microbiology 79 (17): 5112–20.\nKristiansson, Erik, Michael Thorsen, Markus J Tamás, and Olle Nerman. 2009. “Evolutionary Forces Act on Promoter Length: Identification of Enriched Cis- Regulatory Elements.” Molecular Biology and Evolution 26 (6): 1299–1307.\nKuan, Pei Fen, Dongjun Chung, Guangjin Pan, James A Thomson, Ron Stewart, and Sündüz Keleş. 2011. “A Statistical Framework for the Analysis of ChIP-Seq Data.” Journal of the American Statistical Association 106 (495): 891–903.\nLai, Tze Leung. 2001. Sequential Analysis. Wiley Online Library.\nLange, Kenneth. 2016. MM Optimization Algorithms. SIAM.\nLaufer, Christina, Bernd Fischer, Maximilian Billmann, Wolfgang Huber, and Michael Boutros. 2013. “Mapping genetic interactions in human cancer cells with RNAi and multiparametric phenotyping.” Nature Methods 10: 427–31.\nLawrence, Michael S., Petar Stojanov, Paz Polak, Gregory V. Kryukov, Kristian Cibulskis, Andrey Sivachenko, Scott L. Carter, et al. 2013. “Mutational Heterogeneity in Cancer and the Search for New Cancer-Associated Genes.” Nature 499 (7457): 214–18. https://doi.org/10.1038/nature12213.\nLeek, Jeffrey T, Robert B Scharpf, Héctor Corrada Bravo, David Simcha, Benjamin Langmead, W Evan Johnson, Donald Geman, Keith Baggerly, and Rafael A Irizarry. 2010. “Tackling the Widespread and Critical Impact of Batch Effects in High-Throughput Data.” Nature Reviews Genetics 11 (10): 733–39.\nLeek, Jeffrey T., and John D. Storey. 2007. “Capturing heterogeneity in gene expression studies by surrogate variable analysis.” PLoS Genetics 3 (9): 1724–35.\nLi, Wen-Hsiung. 1997. Molecular Evolution. Sinauer Associates Incorporated.\nLi, Wen-Hsiung, and Dan Graur. 1991. Fundamentals of Molecular Evolution. Vol. 48. Sinauer Associates Sunderland, MA.\nLiberzon, Arthur, Aravind Subramanian, Reid Pinchback, Helga Thorvaldsdóttir, Pablo Tamayo, and Jill P Mesirov. 2011. “Molecular Signatures Database (MSigDB) 3.0.” Bioinformatics 27 (12): 1739–40.\nLove, Michael I., Simon Anders, Vladislav Kim, and Wolfgang Huber. 2015. “RNA- Seq Workflow: Gene-Level Exploratory Analysis and Differential Expression.” F1000Research 4 (1070). https://doi.org/10.12688/f1000research.7035.1.\nLove, Michael I, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation of Fold Change and Dispersion for RNA-seq Data with DESeq2.” Gnome Biology 15 (12): 1–21.\nMandal, Rakesh, Sophie St-Hilaire, John G Kie, and DeWayne Derryberry. 2009. “Spatial Trends of Breast and Prostate Cancers in the United States Between 2000 and 2005.” International Journal of Health Geographics 8 (1): 53.\nMardia, Kanti, John T Kent, and John M Bibby. 1979. Multiariate Analysis. New York: Academic Press.\nMarin, Jean-Michel, and Christian Robert. 2007. Bayesian Core: A Practical Approach to Computational Bayesian Statistics. Springer Science & Business Media.\nMcCormick Jr, William T, Paul J Schweitzer, and Thomas W White. 1972. “Problem Decomposition and Data Reorganization by a Clustering Technique.” Operations Research 20 (5): 993–1009.\nMcElreath, Richard. 2015. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. Chapman; Hall/CRC.\nMcLachlan, Geoffrey, and Thriyambakam Krishnan. 2007. The EM Algorithm and Extensions. Vol. 382. John Wiley & Sons.\nMcLachlan, Geoffrey, and David Peel. 2004. Finite Mixture Models. John Wiley & Sons.\nMcMurdie, Paul J, and Susan Holmes. 2014. “Waste Not, Want Not: Why Rarefying Microbiome Data Is Inadmissible.” PLoS Computational Biology 10 (4): e1003531.\n———. 2015. “Shiny-Phyloseq: Web Application for Interactive Microbiome Analysis with Provenance Tracking.” Bioinformatics 31 (2): 282–83.\nMead, Roger. 1990. The Design of Experiments: Statistical Principles for Practical Applications. Cambridge University Press.\nMoignard, Victoria, Steven Woodhouse, Laleh Haghverdi, Andrew J Lilly, Yosuke Tanaka, Adam C Wilkinson, Florian Buettner, et al. 2015. “Decoding the Regulatory Network of Early Blood Development from Single-Cell Gene Expression Measurements.” Nature Biotechnology.\nMollon, John. 1995. “Seeing Colour.” In Colour: Art and Science , edited by T. Lamb and J. Bourriau. Cambridge Unversity Press.\nMood, Alexander M. 1946. “On Hotelling’s Weighing Problem.” The Annals of Mathematical Statistics , 432–46.\nMossel, Elchanan. 2003. “On the Impossibility of Reconstructing Ancestral Data and Phylogenies.” Journal of Computational Biology 10 (5): 669–76.\nMourant, AE, Ada Kopec, and K Domaniewska-Sobczak. 1976. “The Distribution of the Human Blood Groups 2nd Edition.” Oxford University Press London.\nMüllner, Daniel. 2013. “Fastcluster: Fast Hierarchical, Agglomerative Clustering Routines for r and Python.” Journal of Statistical Software 53 (9): 1–18.\nNacu, Serban, Rebecca Critchley-Thorne, Peter Lee, and Susan Holmes. 2007. “Gene Expression Network Analysis and Applications to Immunology.” Bioinformatics 23 (7, 7): 850–58. https://doi.org/10.1093/bioinformatics/btm019.\nNelson, Tyrell A, Susan Holmes, Alexander Alekseyenko, Masha Shenoy, Todd DeSantis, Cindy Wu, Gary Andersen, et al. 2010. “PhyloChip Microarray Analysis Reveals Altered Gastrointestinal Microbial Communities in a Rat Model of Colonic Hypersensitivity.” Neurogastroenterology & Motility.\nNeumann, B., T. Walter, J. K. Heriche, J. Bulkescher, H. Erfle, C. Conrad, P. Rogers, et al. 2010. “Phenotypic profiling of the human genome by time-lapse microscopy reveals cell division genes.” Nature 464 (7289): 721–27.\nNeyman, Jerzy, and Egon S Pearson. 1936. Sufficient Statistics and Uniformly Most Powerful Tests of Statistical Hypotheses. University California Press.\nNolan, Daniel J, Michael Ginsberg, Edo Israely, Brisa Palikuqi, Michael G Poulos, Daylon James, Bi-Sen Ding, et al. 2013. “Molecular Signatures of Tissue-Specific Microvascular Endothelial Cell Heterogeneity in Organ Maintenance and Regeneration.” Developmental Cell 26 (2): 204–19.\nO’Neill, Kieran, Nima Aghaeepour, Josef Špidlen, and Ryan Brinkman. 2013. “Flow Cytometry Bioinformatics.” PLoS Computational Biology 9 (12): e1003365.\nOhnishi, Y., W. Huber, A. Tsumura, M. Kang, P. Xenopoulos, K. Kurimoto, A. K. Oles, et al. 2014. “Cell-to-Cell Expression Variability Followed by Signal Reinforcement Progressively Segregates Early Mouse Lineages.” Nature Cell Biology 16 (1): 27–37.\nOzsolak, Fatih, and Patrice M Milos. 2011. “RNA sequencing: advances, challenges and opportunities.” Nature Reviews Genetics 12: 87–98.\nPagès, Jérôme. 2016. Multiple Factor Analysis by Example Using R. CRC Press.\nParadis, Emmanuel. 2011. Analysis of Phylogenetics and Evolution with r. Springer Science & Business Media.\nPau, Grégoire, Florian Fuchs, Oleg Sklyar, Michael Boutros, and Wolfgang Huber. 2010. “EBImage R Package for Image Processing with Applications to Cellular Phenotypes.” Bioinformatics 26 (7): 979–81.\nPearson, Karl. 1901. “LIII. On Lines and Planes of Closest Fit to Systems of Points in Space.” The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 2 (11): 559–72.\nPerraudeau, Fanny, Davide Risso, Kelly Street, Elizabeth Purdom, and Sandrine Dudoit. 2017. “Bioconductor Workflow for Single-Cell RNA Sequencing: Normalization, Dimensionality Reduction, Clustering, and Lineage Inference.” F1000Research 6.\nPerrière, Guy, and Jean Thioulouse. 2002. “Use and Misuse of Correspondence Analysis in Codon Usage Studies.” Nucleic Acids Research 30 (20): 4548–55.\nPounds, Stan, and Stephan W Morris. 2003. “Estimating the Occurrence of False Positives and False Negatives in Microarray Studies by Approximating and Partitioning the Empirical Distribution of p-Values.” Bioinformatics 19 (10): 1236–42.\nPrentice, IC. 1977. “Non-Metric Ordination Methods in Ecology.” The Journal of Ecology , 85–94.\nPurdom, Elizabeth. 2010. “Analysis of a Data Matrix and a Graph: Metagenomic Data and the Phylogenetic Tree.” Annals of Applied Statistics , July.\nPurdom, Elizabeth, and Susan P Holmes. 2005. “Error Distribution for Gene Expression Data.” Statistical Applications in Genetics and Molecular Biology 4 (1).\nRajaram, S., B. Pavie, L. F. Wu, and S. J. Altschuler. 2012. “PhenoRipper: software for rapidly profiling microscopy images.” Nature Methods 9: 635–37.\nReaven, GM, and RG Miller. 1979. “An Attempt to Define the Nature of Chemical Diabetes Using a Multidimensional Analysis.” Diabetologia 16 (1): 17–24.\nReyes, Alejandro, Simon Anders, Robert J. Weatheritt, Toby J. Gibson, Lars M. Steinmetz, and Wolfgang Huber. 2013. “Drift and Conservation of Differential Exon Usage Across Tissues in Primate Species.” Proceedings of the National Academy of Sciences 110 (38): 15377–82. https://doi.org/10.1073/pnas.1307202110.\nReyes, Alejandro, and Wolfgang Huber. 2017. “Alternative Start and Termination Sites of Transcription Drive Most Transcript Isoform Differences Across Human Tissues.” Nucleic Acids Research 46 (2): 582–92. https://doi.org/10.1093/nar/gkx1165.\nRhee, Soo-Yon, Matthew J Gonzales, Rami Kantor, Bradley J Betts, Jaideep Ravela, and Robert W Shafer. 2003. “Human Immunodeficiency Virus Reverse Transcriptase and Protease Sequence Database.” Nucleic Acids Research 31 (1): 298–303.\nRice, John. 2006. Mathematical Statistics and Data Analysis. Cengage Learning.\nRipley, B. D. 1988. Statistical Inference for Spatial Processes. Cambridge University Press.\nRobert, Christian, and George Casella. 2009. Introducing Monte Carlo Methods with R. Springer Science & Business Media.\nRobins, Garry, Tom Snijders, Peng Wang, Mark Handcock, and Philippa Pattison. 2007. “Recent Developments in Exponential Random Graph (p*) Models for Social Networks.” Social Networks 29 (2): 192–215.\nRobinson, M. D., D. J. McCarthy, and G. K. Smyth. 2009. “edgeR: A Bioconductor Package for Differential Expression Analysis of Digital Gene Expression Data.” Bioinformatics 26 (1): 139–40. https://doi.org/10.1093/bioinformatics/btp616.\nRocke, David M, and Blythe Durbin. 2001. “A Model for Measurement Error for Gene Expression Arrays.” Journal of Computational Biology 8 (6): 557–69.\nRonquist, Fredrik, Maxim Teslenko, Paul van der Mark, Daniel L Ayres, Aaron Darling, Sebastian Höhna, Bret Larget, Liang Liu, Marc A Suchard, and John P Huelsenbeck. 2012. “MrBayes 3.2: Efficient Bayesian Phylogenetic Inference and Model Choice Across a Large Model Space.” Systematic Biology 61 (3): 539–42.\nRosen, Michael J, Benjamin J Callahan, Daniel S Fisher, and Susan P Holmes. 2012. “Denoising PCR-Amplified Metagenome Data.” BMC Bioinformatics 13 (1): 283.\nRousseeuw, Peter J. 1987. “Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis.” Journal of Computational and Applied Mathematics 20: 53–65.\nRousseeuw, Peter J., and Annick M. Leroy. 1987. Robust Regression and Outlier Detection. Wiley. https://doi.org/10.1002/0471725382.\nRoweis, Sam T, and Lawrence K Saul. 2000. “Nonlinear Dimensionality Reduction by Locally Linear Embedding.” Science 290 (5500): 2323–26.\nRuss, John C., and F. Brent Neal. 2015. The Image Processing Handbook. 7th ed. CRC Press;\nSankaran, Kris, and Susan Holmes. 2014. “structSSI: Simultaneous and Selective Inference for Grouped or Hierarchically Structured Data.” Journal of Statistical Software 59 (1): 1–21.\nSchilling, Mark F. 1986. “Multivariate Two-Sample Tests Based on Nearest Neighbors.” Journal of the American Statistical Association 81 (395): 799–806.\nSchindelin, Johannes, Ignacio Arganda-Carreras, Erwin Frise, Verena Kaynig, Mark Longair, Tobias Pietzsch, Stephan Preibisch, et al. 2012. “Fiji: an open- source platform for biological-image analysis.” Nature Methods 9: 676–82.\nSchloss, P D, S L Westcott, T Ryabin, J R Hall, M Hartmann, E B Hollister, R A Lesniewski, et al. 2009. “Introducing mothur: Open-Source, Platform- Independent, Community-Supported Software for Describing and Comparing Microbial Communities.” Applied and Environmental Microbiology 75 (23): 7537–41.\nSchloss, P. D., A. M. Schuber, J. P. Zackular, K. D. Iverson, Young V. B., and Petrosino J. F. 2012. “Stabilization of the Murine Gut Microbiome Following Weaning.” Gut Microbes 3 (4): 383–93.\nSchölkopf, Bernhard, Koji Tsuda, and Jean-Philippe Vert. 2004. Kernel Methods in Computational Biology. MIT press.\nSchweder, T., and E. Spjøtvoll. 1982. “Plots of P-values to Evaluate Many Tests Simultaneously.” Biometrika 69: 493–502. https://doi.org/10.1093/biomet/69.3.493.\nSenn, Stephen. 2004. “Controversies Concerning Randomization and Additivity in Clinical Trials.” Statistics in Medicine 23: 3729–53.\nSerra, Jean. 1983. Image Analysis and Mathematical Morphology. Academic Press.\nSetiadi, A Francesca, Nelson C Ray, Holbrook E Kohrt, Adam Kapelner, Valeria Carcamo-Cavazos, Edina B Levic, Sina Yadegarynia, et al. 2010. “Quantitative, Architectural Analysis of Immune Cell Subsets in Tumor-Draining Lymph Nodes from Breast Cancer Patients and Healthy Lymph Nodes.” PLoS One 5 (8): e12420.\nShalizi, Cosma. 2017. Advanced Data Analysis from an Elementary Point of View. Cambridge University Press. https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf.\nSlonim, Noam, Gurinder Singh Atwal, Gašper Tkačik, and William Bialek. 2005. “Information-Based Clustering.” PNAS 102 (51): 18297–302.\nStegle, O., L. Parts, R. Durbin, and J. Winn. 2010. “A Bayesian framework to account for complex non-genetic factors in gene expression levels greatly increases power in eQTL studies.” PLoS Computational Biology 6 (5): e1000770.\nSteijger, T., J. F. Abril, P. G. Engstrom, F. Kokocinski, T. J. Hubbard, R. Guigo, J. Harrow, et al. 2013. “Assessment of transcript reconstruction methods for RNA-seq.” Nature Methods 10 (12): 1177–84.\nStigler, Stephen M. 2016. The Seven Pillars of Statistical Wisdom. Harvard University Press.\nStorey, John D. 2003. “The Positive False Discovery Rate: A Bayesian Interpretation and the q-Value.” The Annals of Statistics 31 (6). https://doi.org/10.1214/aos/1074290335.\nStrang, Gilbert. 2009. Introduction to Linear Algebra. Fourth. Wellesley- Cambridge Press.\nTenenbaum, Joshua B, Vin De Silva, and John C Langford. 2000. “A Global Geometric Framework for Nonlinear Dimensionality Reduction.” Science 290 (5500): 2319–23.\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society. Series B (Methodological) , 267–88.\nTibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. “Estimating the Number of Clusters in a Data Set via the Gap Statistic.” JRSSB 63 (2): 411–23.\nTrosset, Michael W, and Carey E Priebe. 2008. “The Out-of-Sample Problem for Classical Multidimensional Scaling.” Computational Statistics & Data Analysis 52 (10): 4635–42.\nTseng, George C, and Wing H Wong. 2005. “Tight Clustering: A Resampling-Based Approach for Identifying Stable and Tight Patterns in Data.” Biometrics 61 (1): 10–16.\nTukey, John W. 1977. “Exploratory Data Analysis.” Massachusetts: Addison- Wesley.\nTversky, Amos, and Daniel Kahneman. 1974. “Heuristics and Biases: Judgement Under Uncertainty.” Science 185: 1124–30.\n———. 1975. “Judgment Under Uncertainty: Heuristics and Biases.” In Utility, Probability, and Human Decision Making , 141–62. Springer.\nVerhulst, Pierre-François. 1845. “Recherches mathématiques Sur La Loi d’accroissement de La Population.” Nouveaux Mémoires de l’Académie Royale Des Sciences Et Belles-Lettres de Bruxelles 18: 1–42.\nVetterli, Martin, Jelena Kovačević, and Vivek Goyal. 2014. Foundations of Signal Processing. Cambridge University Press.\nWang, Q., G. M. Garrity, J. M. Tiedje, and J. R. Cole. 2007. “Naive Bayesian Classifier for Rapid Assignment of rRNA Sequences into the New Bacterial Taxonomy.” Applied and Environmental Microbiology 73 (16): 5261.\nWasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA’s Statement on p-Values: Context, Process, and Purpose.” The American Statistician.\nWertheim, Joel O, and Michael Worobey. 2009. “Dating the Age of the SIV Lineages That Gave Rise to HIV-1 and HIV-2.” PLoS Computational Biology 5 (5): e1000377.\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28.\n———. 2014. “Tidy Data.” Journal of Statistical Software 59 (10).\n———. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer New York. http://had.co.nz/ggplot2/book.\nWiel, Mark A, Tonje G Lien, Wina Verlaat, Wessel N Wieringen, and Saskia M Wilting. 2016. “Better Prediction by Use of Co-Data: Adaptive Group- Regularized Ridge Regression.” Statistics in Medicine 35 (3): 368–81.\nWilkinson, Leland. 1999. “Dot Plots.” The American Statistician 53 (3): 276.\n———. 2005. The Grammar of Graphics. Springer.\nWills, Quin F, Kenneth J Livak, Alex J Tipping, Tariq Enver, Andrew J Goldson, Darren W Sexton, and Chris Holmes. 2013. “Single-Cell Gene Expression Analysis Reveals Genetic Associations Masked in Whole-Tissue Experiments.” Nature Biotechnology 31 (8): 748–52.\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt, and Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.” Edited by Francis Ouellette. PLOS Computational Biology 13 (6): e1005510. https://doi.org/10.1371/journal.pcbi.1005510.\nWitten, Daniela M, and Robert Tibshirani. 2011. “Penalized Classification Using Fisher’s Linear Discriminant.” JRSSB 73 (5): 753–72.\nWitten, Daniela M, Robert Tibshirani, and Trevor Hastie. 2009. “A Penalized Matrix Decomposition, with Applications to Sparse Principal Components and Canonical Correlation Analysis.” Biostatistics , kxp008.\nWright, Erik S. 2015. “DECIPHER: Harnessing Local Sequence Context to Improve Protein Multiple Sequence Alignment.” BMC Bioinformatics 16 (1): 1.\nWu, CF Jeff, and Michael S Hamada. 2011. Experiments: Planning, Analysis, and Optimization. Vol. 552. John Wiley & Sons.\nYu, Hongxiang, Diana L Simons, Ilana Segall, Valeria Carcamo-Cavazos, Erich J Schwartz, Ning Yan, Neta S Zuckerman, et al. 2012. “PRC2/EED-EZH2 Complex Is up-Regulated in Breast Cancer Lymph Node Metastasis Compared to Primary Tumor and Correlates with Tumor Proliferation in Situ.” PloS One 7 (12): e51239.\nZeileis, Achim, Christian Kleiber, and Simon Jackman. 2008. “Regression Models for Count Data in R.” Journal of Statistical Software 27 (8). http://www.jstatsoft.org/v27/i08/.\nZeller, Georg, Julien Tap, Anita Y Voigt, Shinichi Sunagawa, Jens Roat Kultima, Paul I Costea, Aurélien Amiot, et al. 2014. “Potential of Fecal Microbiota for Early-Stage Detection of Colorectal Cancer.” Molecular Systems Biology 10 (11): 766. https://doi.org/10.15252/msb.20145645.\nZou, Hui, Trevor Hastie, and Robert Tibshirani. 2006. “Sparse Principal Component Analysis.” Journal of Computational and Graphical Statistics 15 (2): 265–86.\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>16-chap.html</span>"
    ]
  }
]