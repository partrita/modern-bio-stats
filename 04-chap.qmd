![](imgs/t_distribution.png)

생물학적 데이터 분석의 주요 과제 중 하나는 이질성(heterogeneity)을 다루는 것입니다. 우리가 관심을 갖는 수치들은 종종 단순하고 단봉형(unimodal)인 "교과서적인 분포"를 따르지 않습니다. 예를 들어, [2장](02-chap.html)의 마지막 부분에서 [그림 2.27](02-chap.html#fig-ScoreMixture-1)의 서열 점수 히스토그램이 CpG 섬과 비섬(non-islands)이라는 두 개의 별개 최빈값(mode)을 갖는 것을 보았습니다. 우리는 이 데이터를 몇 가지(이 경우 두 개) 성분의 단순한 혼합으로 볼 수 있습니다. 이를 **유한 혼합(finite mixtures)**이라고 부릅니다. 다른 혼합물은 관측치 수만큼이나 많은 성분을 포함할 수 있는데, 이를 **무한 혼합(infinite mixtures)**이라고 부릅니다1.

1 모델링 선택의 많은 부분이 그러하듯이, 혼합물의 적절한 복잡성은 보는 사람의 관점에 달려 있으며, 종종 데이터의 양과 우리가 달성하고자 하는 해상도 및 매끄러움에 따라 달라집니다.

[1장](01-chap.html)에서 포아송 분포를 이용한 단순한 생성 모델이 에피토프(epitope) 검출에서 어떻게 유용한 추론으로 이어지는지 보았습니다. 불행히도, 이러한 단순한 모델로 실제 데이터를 만족스럽게 피팅하는 것은 종종 어렵습니다. 그러나 정규 분포나 포아송 분포와 같은 단순한 모델은 이 장에서 다룰 혼합 프레임워크를 사용하여 더 현실적인 모델을 구축하기 위한 구성 요소 역할을 할 수 있습니다. 혼합물은 유세포 분석 데이터, 생체 측정값, RNA-Seq, ChIP-Seq, 마이크로바이옴 및 현대 생명공학 기술을 사용하여 수집된 다른 많은 유형의 데이터에서 자연스럽게 발생합니다. 이 장에서는 단순한 예제들을 통해 혼합물을 사용하여 더 현실적인 분포 모델을 구축하는 방법을 배울 것입니다.

## 4.1 이 장의 목표

이 장에서 우리는 다음을 수행할 것입니다:

  * 두 개의 정규 모집단으로 구성된 분포로부터 우리만의 혼합 모델 데이터를 생성합니다.

  * 기댓값-최대화(Expectation-Maximization, EM) 알고리즘을 통해 데이터 세트의 기저에 깔린 혼합물을 어떻게 "역설계(reverse engineer)"할 수 있는지 살펴봅니다.

  * ChIP-Seq 데이터와 같이 0이 아주 많은 데이터를 위해 영-과잉(zero-inflation)이라고 불리는 특수한 유형의 혼합 모델을 사용합니다.

  * 경험적 누적 분포(empirical cumulative distribution)를 발견합니다: 관측된 데이터로부터 구축할 수 있는 특수한 혼합물입니다. 이를 통해 붓스트랩(bootstrap)을 사용하여 추정치의 가변성을 어떻게 시뮬레이션할 수 있는지 살펴볼 것입니다.

  * 많은 성분을 가진 무한 혼합 모델의 사례로서 라플라스 분포(Laplace distribution)를 구축합니다. 이를 사용하여 프로모터 길이와 마이크로어레이 강도를 모델링할 것입니다.

  * RNA-Seq 데이터에 유용한 계층적 모델인 감마-포아송(gamma-Poisson) 분포를 처음으로 접하게 됩니다. 이것이 서로 다른 포아송 분포 소스들을 혼합함으로써 자연스럽게 발생한다는 것을 보게 될 것입니다.

  * 혼합 모델을 통해 데이터 변환을 선택하는 방법을 살펴봅니다.

## 4.2 유한 혼합(Finite mixtures)

### 4.2.1 단순한 예제와 컴퓨터 실험

여기에 두 개의 동일한 크기의 성분으로 구성된 혼합 모델의 첫 번째 예제가 있습니다. 생성 과정은 두 단계로 이루어집니다:

**공정한 동전을 던집니다.**

앞면이 나오면: 평균 1, 분산 0.25인 정규 분포에서 난수를 생성합니다.

뒷면이 나오면: 평균 3, 분산 0.25인 정규 분포에서 난수를 생성합니다. 그림 4.1에 표시된 히스토그램은 다음 코드를 사용하여 이 두 단계를 10,000번 반복하여 생성되었습니다.

    
    
    coinflips = (runif(10000) > 0.5)
    table(coinflips)__
    
    
    coinflips
    FALSE  TRUE 
     5003  4997 
    
    
    oneFlip = function(fl, mean1 = 1, mean2 = 3, sd1 = 0.5, sd2 = 0.5) {
      if (fl) {
       rnorm(1, mean1, sd1)
      } else {
       rnorm(1, mean2, sd2)
      }
    }
    fairmix = vapply(coinflips, oneFlip, numeric(1))
    library("ggplot2")
    library("dplyr")
    ggplot(tibble(value = fairmix), aes(x = value)) +
         geom_histogram(fill = "purple", binwidth = 0.1)__

[![](04-chap_files/figure-html/fig-twocoins-1.png)](04-chap_files/figure-
html/fig-twocoins-1.png "그림 4.1: 두 정규 분포의 공정한 혼합물로부터 얻은 10,000개의 난수 히스토그램. 히스토그램의 왼쪽 부분은 (A)에서 생성된 숫자가 지배적이고, 오른쪽은 (B)에서 생성된 숫자가 지배적입니다.")

그림 4.1: 두 정규 분포의 공정한 혼합물로부터 얻은 10,000개의 난수 히스토그램. 히스토그램의 왼쪽 부분은 (A)에서 생성된 숫자가 지배적이고, 오른쪽은 (B)에서 생성된 숫자가 지배적입니다.

__

질문 4.1

R의 벡터화된 구문을 사용하여 `vapply` 루프를 제거하고 `fairmix` 벡터를 더 효율적으로 생성하려면 어떻게 해야 할까요?

__

해결책

__

    
    
    means = c(1, 3)
    sds   = c(0.5, 0.5)
    values = rnorm(length(coinflips),
                   mean = ifelse(coinflips, means[1], means[2]),
                   sd   = ifelse(coinflips, sds[1],   sds[2]))__

__

질문 4.2

개선된 코드를 사용하여 백만 번의 동전 던지기를 수행하고 200개의 빈(bin)을 가진 히스토그램을 만들어 보세요. 무엇을 알 수 있나요?

__

해결책

__

    
    
    fair = tibble(
      coinflips = (runif(1e6) > 0.5),
      values = rnorm(length(coinflips),
                     mean = ifelse(coinflips, means[1], means[2]),
                     sd   = ifelse(coinflips, sds[1],   sds[2])))
    ggplot(fair, aes(x = values)) +
         geom_histogram(fill = "purple", bins = 200)__

[![](04-chap_files/figure-html/fig-
limitinghistogram-1.png)](04-chap_files/figure-html/fig-
limitinghistogram-1.png "그림 4.2: 그림 4.1과 유사하지만 백만 개의 관측치를 사용한 경우.")

그림 4.2: 그림 4.1과 유사하지만 백만 개의 관측치를 사용한 경우.

그림 4.2는 빈의 수와 빈당 관측치 수를 늘림에 따라 히스토그램이 매끄러운 곡선에 가까워지는 것을 보여줍니다. 이 매끄러운 한계 곡선을 확률 변수 `fair$values`의 **밀도(density)** 함수라고 부릅니다.

정규 분포 \\(N(\mu,\sigma)\\) 확률 변수의 밀도 함수는 다음과 같이 명시적으로 쓸 수 있습니다. 우리는 보통 이를 다음과 같이 부릅니다.

\\[ \phi(x)=\frac{1}{\sigma
\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}. \\]

__

질문 4.3

  1. `coinflips`가 `TRUE`인 `fair$values` 값들에 대한 히스토그램을 그리세요. 힌트: `aes` 호출 시 `y = after_stat(density)`를 사용하고(이는 수직축이 비율을 나타냄을 의미함), binwidth를 0.01로 설정하세요.
  2. \\(\phi(z)\\)에 해당하는 선을 겹쳐서 그리세요.

__

해결책

__

    
    
    ggplot(dplyr::filter(fair, coinflips), aes(x = values)) +
      geom_histogram(aes(y = after_stat(density)), fill = "purple", binwidth = 0.01) +
      stat_function(fun = dnorm, color = "red",
                    args = list(mean = means[1], sd = sds[1]))__

[![](04-chap_files/figure-html/fig-
overlaydensity-1.png)](04-chap_files/figure-html/fig-overlaydensity-1.png
"그림 4.3: 정규 분포 N\(\\mu=1,\\sigma^2=0.5^2\)로부터 얻은 50만 개의 난수 히스토그램. 곡선은 dnorm 함수를 사용하여 계산된 이론적 밀도 \\phi\(x\)입니다.")

그림 4.3: 정규 분포 \\(N(\mu=1,\sigma^2=0.5^2)\\)로부터 얻은 50만 개의 난수 히스토그램. 곡선은 `dnorm` 함수를 사용하여 계산된 이론적 밀도 \\(\phi(x)\\)입니다.

사실 우리는 `fair$values` 전체의 밀도(히스토그램이 따라가는 한계 곡선)에 대한 수학적 공식을 두 밀도의 합으로 쓸 수 있습니다.

\\[ f(x)=\frac{1}{2}\phi_1(x)+\frac{1}{2}\phi_2(x), \tag{4.1}\\]

여기서 \\(\phi_1\\)은 정규 분포 \\(N(\mu_1=1,\sigma^2=0.25)\\)의 밀도이고, \\(\phi_2\\)는 정규 분포 \\(N(\mu_2=3,\sigma^2=0.25)\\)의 밀도입니다. 그림 4.4는 다음 코드를 통해 생성되었습니다.

    
    
    fairtheory = tibble(
      x = seq(-1, 5, length.out = 1000),
      f = 0.5 * dnorm(x, mean = means[1], sd = sds[1]) +
          0.5 * dnorm(x, mean = means[2], sd = sds[2]))
    ggplot(fairtheory, aes(x = x, y = f)) +
      geom_line(color = "red", linewidth = 1.5) + ylab("mixture density")__

[![](04-chap_files/figure-html/fig-twodensity-1.png)](04-chap_files/figure-
html/fig-twodensity-1.png "그림 4.4: 혼합물의 이론적 밀도.")

그림 4.4: 혼합물의 이론적 밀도.

이 경우, 두 성분 분포의 겹침이 거의 없기 때문에 혼합 모델이 매우 뚜렷하게 보입니다. 그림 4.4는 두 개의 뚜렷한 정점을 보여줍니다. 우리는 이를 **이봉(bimodal)** 분포라고 부릅니다. 실제로는 많은 경우 혼합 성분 사이의 분리가 그렇게 명확하지 않지만, 그럼에도 불구하고 이는 중요합니다.

[![](04-chap_files/figure-html/fig-histmystery-1.png)](04-chap_files/figure-
html/fig-histmystery-1.png "그림 4.5: 인식하기 더 어려운 두 정규 분포의 혼합물.")

그림 4.5: 인식하기 더 어려운 두 정규 분포의 혼합물.

__

질문 4.4

그림 4.5는 분산이 동일한 두 정규 분포의 공정한 혼합물 히스토그램입니다. 성분 분포의 두 _평균_ 매개변수를 추측할 수 있나요? 힌트: 시행착오법을 사용하여 다양한 혼합물을 시뮬레이션하여 일치하는 히스토그램을 만들 수 있는지 확인할 수 있습니다. 이 장의 R 코드를 살펴보면 데이터가 정확히 어떻게 생성되었는지 알 수 있습니다.

__

해결책

__

다음 코드는 동전의 _앞면_에서 생성된 점은 빨간색으로, _뒷면_에서 생성된 점은 파란색으로 표시합니다. 그림 4.6에 표시된 출력 결과는 두 기저 분포를 보여줍니다.

    
    
    head(mystery, 3)__
    
    
    # A tibble: 3 × 2
      coinflips values
      <lgl>      <dbl>
    1 FALSE       2.40
    2 FALSE       1.66
    3 TRUE        1.22
    
    
    br = with(mystery, seq(min(values), max(values), length.out = 30))
    ggplot(mystery, aes(x = values)) +
      geom_histogram(data = dplyr::filter(mystery, coinflips),
         fill = "red", alpha = 0.2, breaks = br) +
      geom_histogram(data = dplyr::filter(mystery, !coinflips),
         fill = "darkblue", alpha = 0.2, breaks = br) __

[![](04-chap_files/figure-html/fig-
betterhistogram-1-1.png)](04-chap_files/figure-html/fig-
betterhistogram-1-1.png "그림 4.6: 그림 4.5의 혼합물이지만, 두 성분을 빨간색과 파란색으로 표시한 경우.")

그림 4.6: 그림 4.5의 혼합물이지만, 두 성분을 빨간색과 파란색으로 표시한 경우.

그림 4.6에서 두 성분 분포의 막대는 서로 겹쳐서 표시됩니다. 성분을 표시하는 다른 방법은 아래 코드로 생성된 그림 4.7입니다.

    
    
    ggplot(mystery, aes(x = values, fill = coinflips)) +
      geom_histogram(data = dplyr::filter(mystery, coinflips),
         fill = "red", alpha = 0.2, breaks = br) +
      geom_histogram(data = dplyr::filter(mystery, !coinflips),
         fill = "darkblue", alpha = 0.2, breaks = br) +
      geom_histogram(fill = "purple", breaks = br, alpha = 0.2)__

[![](04-chap_files/figure-html/fig-
comparecomponents-1-1.png)](04-chap_files/figure-html/fig-
comparecomponents-1-1.png "그림 4.7: 그림 4.6과 유사하지만, 두 혼합 성분에 대해 누적 막대(stacked bars)를 사용한 경우.")

그림 4.7: 그림 4.6과 유사하지만, 두 혼합 성분에 대해 누적 막대(stacked bars)를 사용한 경우.

__

질문 4.5

왜 그림 4.7의 막대 높이는 그림 4.5와 같지만, 그림 4.6의 막대 높이는 그렇지 않을까요?

__

Solution

__

In Figures 4.7 and 4.5, each count occupies a different piece of vertical
space in a bin. In Figure 4.6, in the overlapping (darker) region, some of the
counts falling within the same bin are overplotted.

In Figures 4.6 and 4.7, we were able to use the `coinflips` column in the data
to disentangle the components. In real data, this information is missing.

[![A book-long treatment on the subject of finite mixtures \(McLachlan and
Peel 2004\).](imgs/book_icon.png)](imgs/book_icon.png "A book-long treatment
on the subject of finite mixtures \[@mclachlan2004\].")

A book-long treatment on the subject of finite mixtures ([McLachlan and Peel
2004](16-chap.html#ref-mclachlan2004)).

### 4.2.2 Discovering the hidden class labels

We use a method called the _expectation-maximization (EM) algorithm_ to infer
the value of the hidden groupings. The EM algorithm is a popular iterative
procedure that alternates between pretending we know one part of the solution
to compute the other part, and pretending the other part is known and
computing the first part, and so on, until convergence. More concretely, it
alternates between

  * pretending we know the probabilities with which each observation belongs to the different mixture components and, from this, estimating the parameters of the components, and

  * pretending we know the parameters of the mixture components and estimating the probabilities with which each observation belongs to the components.

Let’s take a simple example. We measure a variable \\(x\\) on a series of
objects that we think come from two groups, although we do not know the group
labels. We start by _augmenting_ 2 the data with the unobserved (latent) group
label, which we will call \\(U\\).

2 Adding another variable which was not measured, called a hidden or **latent
variable**.

We are interested in finding the values of \\(U\\) and the unknown parameters
\\(\theta\\) of the underlying distribution of the groups. We will use the
maximum likelihood approach introduced in [Chapter 2](02-chap.html) to
estimate the parameters that make the data \\(x\\) the most likely. We can
write the probability densities

\\[ p(x,u\,|\,\theta) = p(x\,|\,u,\theta)\,p(u\,|\,\theta). \tag{4.2}\\]

#### Mixture of normals

For instance, we could generalize our previous mixture model with two normal
distributions Equation 4.1 by allowing non-equal mixture fractions,

\\[ f(x)=\lambda\phi_1(x)+(1-\lambda)\phi_2(x), \tag{4.3}\\]

for \\(\lambda\in[0,1]\\). Similarly as above, \\(\phi_k\\) is the density of
the normal \\(N(\mu_k,\sigma_k^2)\\) for \\(k=1\\) and \\(k=2\\),
respectively. Then, the parameter vector \\(\theta\\) is a five-tuple of the
two means, the two standard deviations, and the mixture parameter
\\(\lambda\\). In other words,
\\(\theta=(\mu_1,\mu_2,\sigma_1,\sigma_2,\lambda)\\). Here is an example of
이러한 모델에 따라 생성된 데이터입니다. 레이블은 \\(u\\)로 표시됩니다.

    
    
    mus = c(-0.5, 1.5)
    lambda = 0.5
    u = sample(2, size = 100, replace = TRUE, prob = c(lambda, 1-lambda))
    x = rnorm(length(u), mean = mus[u])
    dux = tibble(u, x)
    head(dux)__
    
    
    # A tibble: 6 × 2
          u     x
      <int> <dbl>
    1     2 0.303
    2     2 2.65 
    3     1 0.484
    4     2 3.04 
    5     2 1.10 
    6     2 1.96 

만약 레이블 \\(u\\)를 알고 있다면, 각 그룹에 대해 별도의 MLE를 사용하여 매개변수를 추정할 수 있습니다. 전체 가능도(likelihood)는 다음과 같습니다.

\\[ p(x, u \,|\, \theta) = \left( \prod_{\\{i:\,u_i=1\\}} \phi_1(x_i) \right)
\times \left( \prod_{\\{i:\,u_i=2\\}} \phi_2(x_i) \right). \tag{4.4}\\]

이 식을 최대화하는 작업은 세 가지 독립적인 부분으로 나눌 수 있습니다: 식 4.4의 우변에 있는 첫 번째 괄호 안의 식을 최대화하여 \\(\mu_1\\)과 \\(\sigma_1\\)을 찾고, 두 번째 괄호를 최대화하여 \\(\mu_2\\)와 \\(\sigma_2\\)를 찾으며, 레이블의 경험적 빈도로부터 \\(\lambda\\)를 찾습니다.

    
    
    group_by(dux, u) |> summarize(mu = mean(x), sigma = sd(x))__
    
    
    # A tibble: 2 × 3
          u     mu sigma
      <int>  <dbl> <dbl>
    1     1 -0.558  1.05
    2     2  1.41   1.04
    
    
    table(dux$u) / nrow(dux)__
    
    
       1    2 
    0.55 0.45 

__

질문 4.6

혼합 비율 \\(\lambda=\frac{1}{2}\\)은 알고 있지만 \\(u_i\\)는 모른다고 가정해 봅시다. 이 경우 밀도는 \\(\frac{1}{2}\phi_1(x)+\frac{1}{2}\phi_2(x)\\)입니다. (로그) 가능도를 직접 써보세요. 여기에서 MLE를 명시적으로 구하는 것을 방해하는 요인은 무엇인가요?

__

해결책

__

정규 혼합물의 가능도 계산에 대해서는 Shalizi ([2017](16-chap.html#ref-CosmaShalizi2017))의 _혼합 모델(Mixture Models)_ 장을 참조하십시오. "혼합 모델을 추정하려고 하면, 사후 레이블 확률(posterior label probabilities)에 의해 주어진 가중치를 사용하여 가중 최대 가능도(weighted maximum likelihood)를 수행하게 됩니다. 거듭 강조하지만, 이 확률들은 우리가 추정하고자 하는 매개변수에 의존하므로, 이는 일종의 악순환처럼 보입니다."

대부분의 경우 우리는 \\(u\\) 레이블이나 혼합 비율을 알지 못합니다. 우리가 할 수 있는 일은 레이블에 대한 초기 추측에서 시작하여 이를 알고 있다고 가정하고 위와 같이 매개변수를 추정한 다음, 추정치가 실질적으로 변하지 않을 때까지(즉, 수렴할 때까지) 그리고 가능도가 최적값에 도달할 때까지 매 단계마다 레이블과 매개변수에 대한 현재의 최선의 추측을 업데이트하는 반복적인 사이클을 거치는 것입니다.

사실 우리는 각 관측치에 대해 "딱딱한(hard)" 레이블 \\(u\\)(그룹 1 아니면 2에 속함)를 부여하는 대신, 합이 1이 되는 멤버십 가중치(membership weights)로 대체하는 더 정교한 작업을 수행할 수 있습니다. 혼합 모델 4.3은 다음 식을 제안합니다.

\\[ w(x, 1) = \frac{\lambda\phi_1(x)}{\lambda\phi_1(x)+(1-\lambda)\phi_2(x)}
\tag{4.5}\\]

이 식은 값 \\(x\\)를 갖는 관측치가 첫 번째 혼합 성분에 의해 생성되었을 확률로 해석될 수 있으며, 두 번째 성분에 대해서는 유사하게 \\(w(x, 2) = 1 - w(x, 1)\\)이 됩니다. 즉, \\(\lambda\\)가 아직 보지 못한 관측치가 혼합 성분 1에서 나올 사전 확률(prior probability)이라면, \\(w(x,1)\\)은 그 값 \\(x\\)를 관찰한 후의 상응하는 사후 확률(posterior probability)입니다. 이는 다음과 같은 반복 알고리즘을 제안합니다:

**E 단계** : \\(\theta\\)(즉, 평균, 표준 편차 및 \\(\lambda\\))가 알려져 있다고 가정하고, 멤버십 가중치 4.5를 평가합니다.

**M 단계** : 각 관측치 \\(x_i\\)의 멤버십 가중치가 주어졌을 때, \\(\theta\\)의 새롭고 개선된 최대 가능도 추정치를 결정합니다.

\\(\theta\\)와 가능도가 수렴할 때까지 반복합니다. 이 시점에서 코드가 포함된 데모인 연습 문제 4.1을 확인해 보시기 바랍니다. 사실, 이 알고리즘은 여기서 다루는 특정 응용 예제보다 훨씬 더 일반적입니다. ([Bishop 2006](16-chap.html#ref-Bishop:PRML))은 매우 읽기 쉬운 설명을 제시하고 있으며, 주요 내용은 다음과 같습니다:

우리의 목표는 관측 변수 \\(x\\), 관측되지 않은 변수 \\(u\\) 및 일부 매개변수 \\(\theta\\)를 포함하는 확률 모델의 주변 가능도(marginal likelihood)를 최대화하는 것입니다. 우리의 단순한 예제에서 \\(u\\)는 두 가지 가능한 값을 갖는 범주형 변수이고, \\(x\\)는 실수입니다. 일반적으로 \\(x\\)와 \\(u\\) 모두 모든 유형의 개별 변수들의 튜플(즉, 다변량)일 수 있습니다. 주변 가능도는 \\(u\\)의 모든 가능한 값에 대해 기댓값(즉, 가중 평균)을 취하여 계산됩니다:

\\[ L_\text{marg}(\theta; x) = \int_U p(x, u\,|\,\theta) \, \text{d}U.
\tag{4.6}\\]

우리의 구체적인 예제에서, 적분은 가능한 모든 멤버십 구성에 대해 (확률적으로) 평균을 내는 것에 해당하며, 따라서 멤버십 가중치를 고려한 가중 합이 됩니다.

\\[ L_\text{marg}(\theta; x) = \prod_{i=1}^n \sum_{u=1}^2 p(x_i, u\,|\,\theta)
\, w(x_i, u\,|\,\theta). \tag{4.7}\\]

이 수치를 직접 최대화하는 것은 다루기 힘든(intractable) 일입니다.

데이터와 현재의 매개변수 추정치 \\(\theta_t\\)가 주어졌을 때 우리가 파악할 수 있는 것은 잠재 변수의 조건부 분포 \\(p(u\,|\,x, \theta_t)\\)입니다. 우리는 이를 사용하여 일반적인 매개변수 값 \\(\theta\\)에 대해 평가된 완전 데이터 로그 가능도(complete data log likelihood) \\(\log p(x, u\,|\,\theta)\\)의 기댓값을 찾을 수 있습니다. 이 기댓값은 종종 다음과 같이 표시됩니다.

\\[ Q(\theta, \theta_t) = \int_U \log p(x, u\,|\,\theta) \, p(u\,|\, x,
\theta_t) \, \text{d}U. \tag{4.8}\\]

M 단계에서 우리는 이 함수를 최대화하여 수정된 매개변수 추정치 \\(\theta_{t+1}\\)을 결정합니다.

\\[ \theta_{t+1} = \arg \max_\theta \, Q(\theta, \theta_t). \tag{4.9}\\]

E 단계는 \\(Q\\)의 핵심 요소인 \\(p(u\,|\, x, \theta_t)\\)를 계산하는 것으로 구성됩니다.

이 두 단계(E와 M)는 개선 사항이 작아질 때까지 반복됩니다. 이는 우리가 가능도의 평평한 부분에 가까워졌고 최대값에 도달했음을 나타내는 수치적 지표입니다. 반복 궤적은 시작 지점에 따라 달라지겠지만, 도달하는 지점은 달라지지 않기를 바랍니다. 이는 산 정상에 오르는 것과 비슷합니다. 산 정상에 오르는 길은 시작 지점에 따라 다를 수 있고 경로도 다를 수 있지만, 산봉우리가 하나만 있고 여러 개가 아닌 이상 항상 정상으로 이어집니다. 따라서 예방 조치로서, 이러한 절차를 서로 다른 시작 지점에서 여러 번 반복하여 항상 동일한 답을 얻는지 확인하는 것이 좋습니다.

__

질문 4.7

여러 R 패키지에서 **[mclust](https://cran.r-project.org/web/packages/mclust/)**, **[EMcluster](https://cran.r-project.org/web/packages/EMcluster/)**, **[EMMIXskew](https://cran.r-project.org/web/packages/EMMIXskew/)**를 포함한 EM 구현을 제공합니다. 하나를 선택하여 서로 다른 시작 값으로 EM 함수를 여러 번 실행해 보세요. 그런 다음 **[mixtools](https://cran.r-project.org/web/packages/mixtools/)** 패키지의 `normalmixEM` 함수를 사용하여 출력을 비교해 보세요.

__

해결책

__

여기서는 **[mixtools](https://cran.r-project.org/web/packages/mixtools/)**의 출력을 보여줍니다.

    
    
    library("mixtools")
    y = c(rnorm(100, mean = -0.2, sd = 0.5),
          rnorm( 50, mean =  0.5, sd =   1))
    gm = normalmixEM(y, k = 2, 
                        lambda = c(0.5, 0.5),
                        mu = c(-0.01, 0.01), 
                        sigma = c(3, 3))__
    
    
    number of iterations= 134 
    
    
    with(gm, c(lambda, mu, sigma, loglik))__
    
    
    [1]    0.9263    0.0737   -0.1859    1.9216    0.6134    0.4746 -170.5662

EM 알고리즘은 매우 유익합니다:

  1. 우리는 관측치가 한 그룹에 속하는지 다른 그룹에 속하는지 결정하지 않고, 멤버십 확률을 가중치로 사용하여 여러 그룹에 참여할 수 있게 함으로써 더 미묘한 추정치를 얻는 "부드러운(soft)" 평균화의 첫 번째 사례를 보았습니다 ([Slonim et al. 2005](16-chap.html#ref-Slonim:2005)).

  2. 미지수가 너무 많은 어려운 문제를 더 간단한 문제들을 번갈아 가며 해결함으로써 어떻게 다룰 수 있는지 보여줍니다.

  3. 숨겨진 변수(hidden variables)가 있는 데이터 생성 모델을 고려하면서도 그 매개변수를 추정할 수 있었습니다. 숨겨진 변수의 값을 명시적으로 확정하지 않고도 그렇게 할 수 있었습니다: 식 4.8의 기댓값 단계에서 멤버십 확률로 구체화된 이들에 대해 (가중) 평균을 취했습니다. 이 기본 아이디어는 매우 강력하여 머신러닝의 많은 고급 알고리즘의 출발점이 됩니다 ([Bishop 2006](16-chap.html#ref-Bishop:PRML)).

  4. 이는 모델 평균화(model averaging, [Hoeting et al. 1999](16-chap.html#ref-hoeting1999))의 더 일반적인 경우로 확장될 수 있습니다. 우리 데이터에 어떤 모델이 적합한지 확신할 수 없는 경우 여러 모델을 동시에 고려하는 것이 때때로 유익할 수 있습니다. 우리는 이들을 가중 모델로 결합할 수 있습니다. 가중치는 모델의 가능도에 의해 제공됩니다.

### 4.2.3 영-과잉 데이터(zero inflated data)를 위한 모델

생태학적 및 분자적 데이터는 종종 카운트(counts)의 형태로 나타납니다. 예를 들어, 여러 장소 각각에 있는 여러 종 각각의 개체 수일 수 있습니다. 이러한 데이터는 종종 두 가지 시나리오의 혼합으로 볼 수 있습니다: 종이 존재하지 않으면 카운트는 반드시 0이지만, 종이 존재한다면 우리가 관찰하는 개체 수는 무작위 샘플링 분포에 따라 달라지며, 이 분포에도 0이 포함될 수 있습니다. 우리는 이를 다음과 같은 혼합 모델로 모델링합니다:

\\[ f_{\text{zi}}(x) = \lambda \, \delta(x) + (1-\lambda) \,
f_{\text{count}}(x), \tag{4.10}\\]

여기서 \\(\delta\\)는 모든 질량이 0에 있는 확률 분포를 나타내는 디랙 델타 함수입니다. 첫 번째 혼합 성분에서 발생하는 0은 "구조적(structural)"이라고 불립니다. 우리의 예제에서 이는 특정 종이 특정 서식지에 살지 않기 때문에 발생합니다. 두 번째 성분인 \\(f_{\text{count}}\\) 역시 단순히 무작위 샘플링으로 인해 0 및 다른 작은 숫자를 포함할 수 있습니다. R 패키지 **[pscl](https://cran.r-project.org/web/packages/pscl/)** ([Zeileis, Kleiber, and Jackman 2008](16-chap.html#ref-Zeileis:2008))과 **[zicounts](https://cran.r-project.org/web/packages/zicounts/)**는 **영-과잉(zero inflated)** 카운트를 다루기 위한 많은 예제와 함수를 제공합니다.

#### 예시: ChIP-Seq 데이터

ChIP-Seq 데이터의 예를 들어보겠습니다. 이 데이터는 염색질 면역 침전(chromatin immunoprecipitation, ChIP)을 통해 얻은 DNA 조각의 서열입니다. 이 기술을 사용하면 전사 인자, 뉴클레오솜, 히스톤 수정, 염색질 리모델링 효소, 샤페론, 중합효소 및 기타 단백질의 게놈 DNA 상의 위치를 매핑할 수 있습니다. 이는 DNA 요소 백과사전(ENCODE) 프로젝트에서 사용된 주요 기술이었습니다. 여기서는 **[mosaicsExample](https://bioconductor.org/packages/mosaicsExample/)** 패키지의 예제([Kuan et al. 2011](16-chap.html#ref-Kuan2011statistical))를 사용합니다. 이 예제는 GM12878 세포주에 적용된 STAT1 단백질 및 H3K4me3 히스톤 수정에 대한 항체의 ChIP-Seq으로부터 22번 염색체에서 측정된 데이터를 보여줍니다. 여기서는 `binTFBS` 객체를 생성하는 코드는 보여주지 않지만, 이 장의 소스 코드 파일에 나와 있으며 **[mosaics](https://bioconductor.org/packages/mosaics/)** 패키지의 비네트를 따릅니다.

    
    
    binTFBS __
    
    
    Summary: bin-level data (class: BinData)
    ----------------------------------------
    - # of chromosomes in the data: 1
    - total effective tag counts: 462479
      (sum of ChIP tag counts of all bins)
    - control sample is incorporated
    - mappability score is NOT incorporated
    - GC content score is NOT incorporated
    - uni-reads are assumed
    ----------------------------------------

이 객체로부터 빈(bin)당 카운트의 히스토그램을 생성할 수 있습니다.

    
    
    bincts = print(binTFBS)
    ggplot(bincts, aes(x = tagCount)) +
      geom_histogram(binwidth = 1, fill = "forestgreen")__

[![](04-chap_files/figure-html/fig-chipseqzeros-1.png)](04-chap_files/figure-
html/fig-chipseqzeros-1.png "그림 4.8: ChIP-Seq 데이터 세트에서 22번 염색체를 따라 200nt 창(window)에서 발견된 결합 부위의 수입니다.")

그림 4.8: ChIP-Seq 데이터 세트에서 22번 염색체를 따라 200nt 창(window)에서 발견된 결합 부위의 수.

그림 4.8을 보면 0이 아주 많다는 것을 알 수 있는데, 이 플롯만으로는 다른 작은 숫자들(\\(1, 2, ...\\))의 빈도를 고려할 때 0의 개수가 정말로 특별한지는 즉각적으로 명확하지 않습니다.

__

질문 4.8

  1. \(y\)축에 로그(밑 10) 스케일을 사용하여 카운트 히스토그램을 다시 그리세요.  

  2. 카운트가 0인 빈의 비율인 \(\pi_0\)을 추정하세요.

__

해결책

__

    
    
    ggplot(bincts, aes(x = tagCount)) + scale_y_log10() +
       geom_histogram(binwidth = 1, fill = "forestgreen")__

[![](04-chap_files/figure-html/fig-
ChipseqHistlogY-1.png)](04-chap_files/figure-html/fig-ChipseqHistlogY-1.png
"Figure 4.9: As Figure fig-chipseqzeros, but using a logarithm base 10 scale
on the y-axis. The fraction of zeros seems elevated compared to that of ones,
twos, …")

Figure 4.9: As Figure 4.8, but using a logarithm base 10 scale on the
\\(y\\)-axis. The fraction of zeros seems elevated compared to that of ones,
twos, …

### 4.2.4 More than two components

So far we have looked at mixtures of two components. We can extend our
description to cases where there may be more. For instance, when weighing
N=7,000 nucleotides obtained from mixtures of deoxyribonucleotide
monophosphates (each type has a different weight, measured with the same
standard deviation sd=3), we might observe the histogram (shown in Figure
4.10) generated by the following code.

    
    
    masses = c(A =  331, C =  307, G =  347, T =  322)
    probs  = c(A = 0.12, C = 0.38, G = 0.36, T = 0.14)
    N  = 7000
    sd = 3
    nuclt   = sample(length(probs), N, replace = TRUE, prob = probs)
    quadwts = rnorm(length(nuclt),
                    mean = masses[nuclt],
                    sd   = sd)
    ggplot(tibble(quadwts = quadwts), aes(x = quadwts)) +
      geom_histogram(bins = 100, fill = "purple")__

[![](04-chap_files/figure-html/fig-
nucleotideweights-1-1.png)](04-chap_files/figure-html/fig-
nucleotideweights-1-1.png "Figure 4.10: Simulation of 7,000 nucleotide mass
measurements.")

Figure 4.10: Simulation of 7,000 nucleotide mass measurements.

__

Question 4.9

Repeat this simulation experiment with \\(N=1000\\) nucleotide measurements.
What do you notice in the histogram?

__

Question 4.10

What happens when \\(N=7000\\) but the standard deviation is 10?

__

Question 4.11

Plot the theoretical density curve for the distribution simulated in Figure
4.10.

In this case, as we have enough measurements with good enough precision, we
are able to distinguish the four nucleotides and decompose the distribution
shown in Figure 4.10. With fewer data and/or more noisy measurements, the four
modes and the distribution component might be less clear.

## 4.3 Empirical distributions and the nonparametric bootstrap

In this section, we will consider an extreme case of mixture models, where we
model our sample of \\(n\\) data points as a mixture of \\(n\\) point masses.
We could use almost any set of data here; to be concrete, we use Darwin’s _Zea
Mays_ data3 in which he compared the heights of 15 pairs of _Zea Mays_ plants
(15 self-hybridized versus 15 crossed). The data are available in the
**[HistData](https://cran.r-project.org/web/packages/HistData/)** package, and
we plot the distribution of the 15 differences in height:

3 They were collected by Darwin who asked his cousin, Francis Galton to
analyse them. R.A. Fisher re-analysed the same data using a paired t-test
([Bulmer 2003](16-chap.html#ref-bulmer2003)). We will get back to this example
in [Chapter 13](13-chap.html).

    
    
    library("HistData")
    ZeaMays$diff __
    
    
     [1]  6.125 -8.375  1.000  2.000  0.750  2.875  3.500  5.125  1.750  3.625
    [11]  7.000  3.000  9.375  7.500 -6.000
    
    
    ggplot(ZeaMays, aes(x = diff, ymax = 1/15, ymin = 0)) +
      geom_linerange(linewidth = 1, col = "forestgreen") + ylim(0, 0.1)__

[![](04-chap_files/figure-html/fig-ecdfZea-1.png)](04-chap_files/figure-
html/fig-ecdfZea-1.png "Figure 4.11: The observed sample can be seen as a
mixture of point masses at each of the values \(real point masses would be
bars without any width whatsoever\).")

Figure 4.11: The observed sample can be seen as a mixture of point masses at
each of the values (real point masses would be bars without any width
whatsoever).

[Section 3.6.7](03-chap.html#sec-graphics-ecdf)에서 우리는 크기 \\(n\\)인 표본에 대한 경험적 누적 분포 함수(empirical cumulative distribution function, ECDF)가 다음과 같음을 보았습니다.

\\[ \hat{F}_n(x)= \frac{1}{n}\sum_{i=1}^n {\mathbb 1}_{x \leq x_i},
\tag{4.11}\\]

그리고 [그림 3.24](03-chap.html#fig-graphics-onedecdf)에서 ECDF 플롯을 보았습니다. 우리는 또한 우리 표본의 _밀도_를 다음과 같이 쓸 수 있습니다.

\\[ \hat{f}_n(x) =\frac{1}{n}\sum_{i=1}^n \delta_{x_i}(x) \tag{4.12}\\]

일반적으로 확률 분포의 밀도는 (존재한다면) 분포 함수의 도함수입니다. 우리는 여기서 이 원리를 적용했습니다: 식 4.11로 정의된 분포의 밀도는 식 4.12입니다. 함수 \\(\delta_a\\)를 계단 함수(step function) \\({\mathbb 1}_{x \leq a}\\)의 "도함수"로 간주할 수 있기 때문에 이렇게 할 수 있었습니다: 이 함수는 계단이 있는 한 점 \\(a\\)를 제외하고는 거의 모든 곳에서 완전히 평평하며, 그 점에서의 값은 "무한"입니다. 식 4.12는 우리의 데이터 표본이 그림 4.11에서와 같이 관측된 값 \\(x_1, x_2, ..., x_n\\)에서의 \\(n\\)개 **점 질량(point masses)**의 혼합물로 간주될 수 있음을 강조합니다.

이것이 타당하기 위해서는 (표준 미적분학을 넘어서는) 약간의 고급 수학이 필요하지만, 여기서는 다루지 않겠습니다.

[![](imgs/BootstrapPrincipleNew.png)](imgs/BootstrapPrincipleNew.png
"그림 4.12: 통계량 \\tau의 값은 기저 분포 F로부터 생성된 데이터(회색 행렬들)로부터 추정됩니다. F로부터 얻은 서로 다른 표본들은 서로 다른 데이터를 생성하고, 따라서 추정치 \\hat{\\tau}의 값도 달라집니다: 이를 표집 가변성(sampling variability)이라고 합니다. 모든 \\hat{\\tau}들의 분포가 표집 분포(sampling distribution)입니다.")

그림 4.12: 통계량 \\(\tau\\)의 값은 기저 분포 \\(F\\)로부터 생성된 데이터(회색 행렬들)로부터 추정됩니다. \\(F\\)로부터 얻은 서로 다른 표본들은 서로 다른 데이터를 생성하고, 따라서 추정치 \\(\hat{\\tau}\\)의 값도 달라집니다: 이를 **표집 가변성(sampling variability)**이라고 합니다. 모든 \\(\hat{\\tau}\\)들의 분포가 **표집 분포(sampling distribution)**입니다.

평균, 최솟값 또는 중앙값과 같은 우리 표본의 통계량은 이제 ECDF의 함수로 쓰여질 수 있습니다. 예를 들어, \\(\bar{x} = \int \delta_{x_i}(x)\,\text{d}x\\)입니다. 또 다른 예로, 만약 \\(n\\)이 홀수라면 중앙값은 정렬된 리스트의 정중앙에 있는 값인 \\(x_{(\frac{n+1}{2})}\\)입니다.

통계량 \\(\hat{\\tau}\\)의 실제 **표집 분포**는 통계량을 계산하기 위한 많은 서로 다른 데이터 표본을 필요로 하기 때문에 알기 어려운 경우가 많습니다. 이는 그림 4.12에 나와 있습니다.

**붓스트랩(bootstrap)** 원리는 원래 표본으로부터 구축된 경험적 분포에서 뽑은 새로운 표본들을 생성함으로써 \\(\hat{\\tau}\\)의 실제 표집 분포를 근사합니다(그림 4.13). 우리는 데이터를 (\\(\delta\\)들의 혼합 분포로 간주하여) _재사용_하여 표본을 추출하고 그로부터 계산된 통계량 \\(\hat{\\tau}^*\\)의 표집 분포를 살펴봄으로써 새로운 "데이터 세트"를 만듭니다. 이를 **비모수적 붓스트랩(nonparametric bootstrap)** 재표본 추출 접근법이라고 하며, 완전한 참고 문헌으로는 Bradley Efron과 Tibshirani ([1994](16-chap.html#ref-Efron:1994))를 참조하십시오. 이는 아무리 복잡하더라도 기본적으로 모든 통계량에 적용할 수 있는 매우 다재다능하고 강력한 방법입니다. [5장](05-chap.html)에서 이 방법의 응용 예시, 특히 클러스터링에 대한 적용 사례를 살펴볼 것입니다.

[![](imgs/BootstrapPrinciple2New.png)](imgs/BootstrapPrinciple2New.png
"그림 4.13: 붓스트랩은 (그림 4.12에서처럼) 기저의 실제 분포 F가 아니라 경험적 분포 함수 \\hat{F}_n으로부터 표본을 추출함으로써 표집 가변성을 시뮬레이션합니다.")

그림 4.13: 붓스트랩은 (그림 4.12에서처럼) 기저의 실제 분포 \\(F\\)가 아니라 경험적 분포 함수 \\(\hat{F}_n\\)으로부터 표본을 추출함으로써 표집 가변성을 시뮬레이션합니다.

이러한 아이디어를 사용하여 그림 4.11에서 보았던 Zea Mays 차이값의 중앙값에 대한 표집 분포를 추정해 봅시다. 이전 섹션들과 유사한 시뮬레이션을 사용합니다: 15개의 값(각각이 15개 성분 혼합물의 한 성분임)으로부터 크기가 15인 표본을 \\(B=1000\\)번 추출합니다. 그런 다음 이 15개 값으로 구성된 1000개 표본 각각의 중앙값을 계산하고 그 분포를 살펴봅니다. 이것이 중앙값의 붓스트랩 표집 분포입니다.

    
    
    B = 1000
    meds = replicate(B, {
      i = sample(15, 15, replace = TRUE)
      median(ZeaMays$diff[i])
    })
    ggplot(tibble(medians = meds), aes(x = medians)) +
      geom_histogram(bins = 30, fill = "purple")__

[![](04-chap_files/figure-html/fig-bootmedian-1.png)](04-chap_files/figure-
html/fig-bootmedian-1.png "그림 4.14: Zea Mays 차이값의 중앙값에 대한 붓스트랩 표집 분포.")

그림 4.14: Zea Mays 차이값의 중앙값에 대한 붓스트랩 표집 분포.

__

질문 4.12

이 시뮬레이션을 바탕으로 중앙값에 대한 99% 신뢰 구간을 추정해 보세요. 이 구간과 0 사이의 겹침을 보고 무엇을 결론지을 수 있나요?

__

질문 4.13

**[bootstrap](https://cran.r-project.org/web/packages/bootstrap/)** 패키지의 `bootstrap` 함수를 사용하여 `median`과 `mean` 모두에 대해 동일한 분석을 다시 수행해 보세요. 평균과 중앙값의 표집 분포 사이에서 어떤 차이점을 발견했나요?

__

해결책

__

    
    
    library("bootstrap")
    bootstrap(ZeaMays$diff, B, mean)
    bootstrap(ZeaMays$diff, B, median)__

#### 왜 비모수적(nonparametric)인가요?

이론 통계학에서 비모수적 방법이란 무한히 많은 자유도나 알 수 없는 매개변수의 수를 가진 방법을 말합니다.

![](imgs/devil.png)

실제로 우리는 무한대까지 기다리지 않습니다. 매개변수의 수가 가용 데이터의 양만큼 많거나 그보다 많아지면 그 방법을 비모수적이라고 부릅니다. 붓스트랩은 \\(n\\)개의 성분을 가진 혼합물을 사용하므로, 크기가 \\(n\\)인 표본의 경우 비모수적 방법의 자격을 갖춥니다.

그 이름에도 불구하고, 비모수적 방법이 매개변수를 사용하지 않는 방법은 아닙니다. 모든 통계적 방법은 알 수 없는 수치를 추정합니다.

__

질문 4.14

표본이 \\(n=3\\)개의 서로 다른 값으로 구성되어 있다면, 몇 가지의 서로 다른 붓스트랩 재표본이 가능할까요? \\(n=15\\)인 경우에 대해서도 답해 보세요.

__

해결책

__

모든 붓스트랩 재표본의 집합은 합이 \\(n\\)인 \\(n\\)개 정수 벡터의 집합과 동일합니다. 관측치 \\(x_1, x_2, ..., x_n\\)이 붓스트랩 표본에 나타나는 횟수를 \\(\mathbf{k} = (k_1, k_2, ..., k_n)\\)이라고 합시다. 각 \\(k_i\\)를 (다항 분포에서처럼) 상자로 생각할 수 있고, \\(n\\)개의 공을 떨어뜨릴 \\(n\\)개의 상자가 있습니다. 구성을 세는 방법은 \\(n\\)개의 공을 상자에 나누는 방법의 수를 세는 것입니다. 즉, 공을 나타내는 `o`를 \\(n\\)번 쓰고 그 사이에 구분선 `|`를 \\(n-1\\)번 쓰는 것입니다. 따라서 우리는 `o`(공) 또는 `|`(구분선) 중 하나를 선택해야 하는 \\(2n-1\\)개의 자리를 채워야 합니다. \\(n=3\\)인 경우, 가능한 배치는 `oo||o`일 수 있으며, 이는 \\(\mathbf{k} = (2,0,1)\\)에 해당합니다. 일반적으로 이 숫자는 \\({2n-1} \choose {n-1}\\)이며, 따라서 \\(n=3\\)과 \\(15\\)에 대한 답은 다음과 같습니다.

    
    
    c(N3 = choose(5, 3), N15 = choose(29, 15))__
    
    
          N3      N15 
          10 77558760 

__

질문 4.15

**[bootstrap](https://cran.r-project.org/web/packages/bootstrap/)** 패키지에 구현된 붓스트랩을 사용할 때 발생할 수 있는 두 가지 유형의 오류는 무엇인가요? 그중 하나를 개선하기 위해 어떤 매개변수를 수정할 수 있나요?

__

해결책

__

무작위 재표본 추출을 통한 데이터 하위 집합의 몬테카를로 시뮬레이션은 전수 붓스트랩(exhaustive bootstrap)을 근사합니다([Diaconis and Holmes 1994](16-chap.html#ref-Diaconis1994)). `bootstrap` 함수의 `nboot` 인수의 크기를 늘리면 몬테카를로 오류를 줄일 수 있지만, 전수 붓스트랩도 여전히 정확하지는 않습니다. 우리는 여전히 실제 분포 대신 데이터의 근사 분포 함수를 사용하고 있기 때문입니다. 표본 크기가 작거나 원래 표본에 편향이 있는 경우, `nboot`를 아무리 크게 선택하더라도 근사는 여전히 상당히 좋지 않을 수 있습니다.

## 4.4 Infinite mixtures

Sometimes mixtures can be useful even if we don’t aim to assign a label to
each observation or, to put it differently, if we allow as many `labels’ as
there are observations. If the number of mixture components is as big as (or
bigger than) the number of observations, we say we have an **infinite
mixture**. Let’s look at some examples.

### 4.4.1 Infinite mixture of normals

[![](imgs/LaplacePortrait_web.png)](imgs/LaplacePortrait_web.png "Figure 4.15:
Laplace knew already that the probability density
f_X\(y\)=\\frac{1}{2\\phi}\\exp\\left\(-\\frac{|y-\\theta|}{\\phi}\\right\),\\qquad\\phi>0
has the median as its location parameter \\theta and the median absolute
deviation \(MAD\) as its scale parameter \\phi.")

Figure 4.15: Laplace knew already that the probability density
\\[f_X(y)=\frac{1}{2\phi}\exp\left(-\frac{|y-\theta|}{\phi}\right),\qquad\phi>0\\]
has the median as its location parameter \\(\theta\\) and the median absolute
deviation (MAD) as its scale parameter \\(\phi\\).

Consider the following two-level data generating scheme:

**Level 1** Create a sample of `W`s from an exponential distribution.

    
    
    w = rexp(10000, rate = 1)__

**Level 2** The \\(w\\)s serve as the variances of normal variables with mean
\\(\mu\\) generated using `rnorm`.

    
    
    mu  = 0.3
    lps = rnorm(length(w), mean = mu, sd = sqrt(w))
    ggplot(data.frame(lps), aes(x = lps)) +
      geom_histogram(fill = "purple", binwidth = 0.1)__

[![](04-chap_files/figure-html/fig-
Laplacedistribution-1.png)](04-chap_files/figure-html/fig-
Laplacedistribution-1.png "Figure 4.16: Data sampled from a Laplace
distribution.")

Figure 4.16: Data sampled from a Laplace distribution.

This turns out to be a rather useful distribution. It has well-understood
properties and is named after Laplace, who proved that the median is a good
estimator of its location parameter \\(\theta\\) and that the median absolute
deviation can be used to estimate its scale parameter \\(\phi\\). From the
formula in the caption of Figure 4.15 we see that the \\(L_1\\) distance
(absolute value of the difference) holds a similar position in the Laplace
density as the \\(L_2\\) (square of the difference) does for the normal
density.

Conversely, in Bayesian regression4, having a Laplace distribution as a prior
on the coefficients amounts to an \\(L_1\\) penalty, called the _lasso_
([Tibshirani 1996](16-chap.html#ref-Tibshirani1996)), while a normal
distribution as a prior leads to an \\(L_2\\) penalty, called ridge
regression.

4 Don’t worry if you are not familiar with this, in that case just skip this
sentence.

__

Question 4.16

Write a random variable whose distribution is the symmetric Laplace as a
function of normal and exponential random variables.

__

Solution

__

We can write the hierarchical model with variances generated as exponential
variables, \\(W\\), as:

\\[ X = \sqrt{W} \cdot Z, \qquad W \sim Exp(1), \qquad Z \sim N(0,1).
\tag{4.13}\\]

#### Asymmetric Laplace

In the Laplace distribution, the variances of the normal components depend on
\\(W\\), while the means are unaffected. A useful extension adds another
parameter \\(\theta\\) that controls the locations or centers of the
components. We generate data `alps` from a hierarchical model with \\(W\\) an
exponential variable; the output shown in Figure 4.17 is a histogram of normal
\\(N(\theta+w\mu,\sigma w)\\) random numbers, where the \\(w\\)’s themselves
were randomly generated from an exponential distribution with mean \\(1\\) as
shown in the code:

    
    
    mu = 0.3; sigma = 0.4; theta = -1
    w  = rexp(10000, 1)
    alps = rnorm(length(w), theta + mu * w, sigma * sqrt(w))
    ggplot(tibble(alps), aes(x = alps)) +
      geom_histogram(fill = "purple", binwidth = 0.1)__

[![](04-chap_files/figure-html/fig-
ALaplacedistribution-1.png)](04-chap_files/figure-html/fig-
ALaplacedistribution-1.png "Figure 4.17: Histogram of data generated from an
asymmetric Laplace distribution – a scale mixture of many normals whose means
and variances are dependent. We write X \\sim AL\(\\theta, \\mu, \\sigma\).")

Figure 4.17: Histogram of data generated from an asymmetric Laplace
distribution – a scale mixture of many normals whose means and variances are
dependent. We write \\(X \sim AL(\theta, \mu, \sigma)\\).

Such hierarchical mixture distributions, where every instance of the data has
its own mean and variance, are useful models in many biological settings.
Examples are shown in Figure 4.18.

[![](imgs/LaplaceMixturePromoterLengths.png)](imgs/LaplaceMixturePromoterLengths.png
"Figure 4.18 \(a\): The lengths of the promoters shorter than 2000bp from
Saccharomyces cerevisiae as studied by @Kristiansson2009.")

(a) The lengths of the promoters shorter than 2000bp from Saccharomyces
cerevisiae as studied by Kristiansson et al. ([2009](16-chap.html#ref-
Kristiansson2009)).

![](imgs/tcellhist.png): The log-
ratios of microarray gene expression measurements for 20,000 genes
\[@Purdom2005\].")

(b) The log-ratios of microarray gene expression measurements for 20,000 genes
([Purdom and Holmes 2005](16-chap.html#ref-Purdom2005)).

Figure 4.18: Histogram of real data. Both distributions can be modeled by
asymmetric Laplace distributions.

__

Question 4.17

Looking at the log-ratio of gene expression values from a microarray, one gets
a distribution as shown on the right of Figure 4.18. How would one explain
that the data have a histogram of this form?

The Laplace distribution is an example of where the consideration of the
generative process indicates how the variance and mean are linked. The
expectation value and variance of an asymmetric Laplace distribution
\\(AL(\theta, \mu, \sigma)\\) are

\\[ E(X)=\theta+\mu\quad\quad\text{and}\quad\quad\text{var}(X)=\sigma^2+\mu^2.
\tag{4.14}\\]

Note the variance is dependent on the mean, unless \\(\mu = 0\\) (the case of
the symmetric Laplace Distribution). This is the feature of the distribution
that makes it useful. Having a mean-variance dependence is very common for
physical measurements, be they microarray fluorescence intensities, peak
heights from a mass spectrometer, or reads counts from high-throughput
sequencing, as we’ll see in the next section.

### 4.4.2 Infinite mixtures of Poisson variables.

[![](imgs/three-worlds_web.jpg)](imgs/three-worlds_web.jpg "Figure 4.19: How
to count the fish in a lake? MC Escher.")

Figure 4.19: How to count the fish in a lake? MC Escher.

A similar two-level hierarchical model is often also needed to model real-
world count data. At the lower level, simple Poisson and binomial
distributions serve as the building blocks, but their parameters may depend on
some underlying (latent) process. In ecology, for instance, we might be
interested in variations of fish species in all the lakes in a region. We
sample the fish species in each lake to estimate their true abundances, and
that could be modeled by a Poisson. But the true abundances will vary from
lake to lake. And if we want to see whether, for instance, changes in climate
or altitude play a role, we need to disentangle such systematic effects from
random lake-to-lake variation. The different Poisson rate parameters
\\(\lambda\\) can be modeled as coming from a distribution of rates. Such a
hierarchical model also enables us to add supplementary steps in the
hierarchy, for instance we could be interested in many different types of
fish, model altitude and other environmental factors separately, etc.

Further examples of sampling schemes that are well modeled by mixtures of
Poisson variables include applications of high-throughput sequencing, such as
RNA-Seq, which we will cover in detail in [Chapter 8](08-chap.html), or 16S
rRNA-Seq data used in microbial ecology.

### 4.4.3 Gamma distribution: two parameters (shape and scale)

Now we are getting to know a new distribution that we haven’t seen before. The
gamma distribution is an extension of the (one-parameter) exponential
distribution, but it has two parameters, which makes it more flexible. It is
often useful as a building block for the upper level of a hierarchical model.
The gamma distribution is positive-valued and continuous. While the density of
the exponential has its maximum at zero and then simply decreases towards 0 as
the value goes to infinity, the density of the gamma distribution has its
maximum at some finite value. Let’s explore it by simulation examples. The
histograms in Figure 4.20 were generated by the following lines of code:

    
    
    ggplot(tibble(x = rgamma(10000, shape = 2, rate = 1/3)),
       aes(x = x)) + geom_histogram(bins = 100, fill= "purple")
    ggplot(tibble(x = rgamma(10000, shape = 10, rate = 3/2)),
       aes(x = x)) + geom_histogram(bins = 100, fill= "purple")__

[![](04-chap_files/figure-html/fig-gammahist1-1.png)](04-chap_files/figure-
html/fig-gammahist1-1.png "Figure 4.20 \(a\): gamma\(2,\\frac{1}{3}\)")

(a) gamma\\((2,\frac{1}{3})\\)

[![](04-chap_files/figure-html/fig-gammahist1-2.png)](04-chap_files/figure-
html/fig-gammahist1-2.png "Figure 4.20 \(b\): gamma\(10,\\frac{3}{2}\)")

(b) gamma\\((10,\frac{3}{2})\\)

Figure 4.20: Histograms of random samples of gamma distributions. The gamma is
a flexible two parameter distribution: [see
Wikipedia](http://en.wikipedia.org/wiki/Gamma_distribution).

#### Gamma–Poisson mixture: a hierarchical model

  1. Generate a set of parameters: \\(\lambda_1,\lambda_2,...\\) from a gamma distribution.

  2. Use these to generate a set of Poisson(\\(\lambda_i\\)) random variables, one for each \\(\lambda_1\\).

    
    
    lambda = rgamma(10000, shape = 10, rate = 3/2)
    gp = rpois(length(lambda), lambda = lambda)
    ggplot(tibble(x = gp), aes(x = x)) +
      geom_histogram(bins = 100, fill= "purple")__

[![](04-chap_files/figure-html/fig-
generatepoissongamma-1.png)](04-chap_files/figure-html/fig-
generatepoissongamma-1.png "Figure 4.21: Histogram of gp, generated via a
gamma-Poisson hierachical model.")

Figure 4.21: Histogram of `gp`, generated via a gamma-Poisson hierachical
model.

The resulting values are said to come from a gamma–Poisson mixture. Figure
4.21 shows the histogram of `gp`.

__

Question 4.18

  1. Are the values generated from such a gamma–Poisson mixture continuous or discrete ?  

  2. What is another name for this distribution? Hint: Try the different distributions provided by the `goodfit` function from the **[vcd](https://cran.r-project.org/web/packages/vcd/)** package.

__

Solution

__

    
    
    library("vcd")
    ofit = goodfit(gp, "nbinomial")
    plot(ofit, xlab = "")
    ofit$par __
    
    
    $size
    [1] 9.911829
    
    $prob
    [1] 0.5963857

[![](04-chap_files/figure-html/fig-goofy-1.png)](04-chap_files/figure-
html/fig-goofy-1.png "Figure 4.22: Goodness of fit plot. The rootogram shows
the theoretical probabilities of the gamma-Poisson distribution \(a.k.a.
negative binomial\) as red dots and the square roots of the observed
frequencies as the height of the rectangular bars. The bars all end close to
the horizontal axis, which indicates a good fit to the negative binomial
distribution.")

Figure 4.22: Goodness of fit plot. The **rootogram** shows the theoretical
probabilities of the gamma-Poisson distribution (a.k.a. negative binomial) as
red dots and the square roots of the observed frequencies as the height of the
rectangular bars. The bars all end close to the horizontal axis, which
indicates a good fit to the negative binomial distribution.

In R, and in some other places, the gamma-Poisson distribution travels under
the alias name of **negative binomial distribution**. The two names are
synonyms; the second one alludes to the fact that Equation 4.15 bears some
formal similarities to the probabilities of a binomial distribution. The first
name, gamma–Poisson distribution, is more indicative of its generating
mechanism, and that’s what we will use in the rest of the book. It is a
discrete distribution, that means that it takes values only on the natural
numbers (in contrast to the gamma distribution, which covers the whole
positive real axis). Its probability distribution is

\\[ \text{P}(K=k)=\left(\begin{array}{c}k+a-1\\\k\end{array}\right) \, p^a \,
(1-p)^k, \tag{4.15}\\]

which depends on the two parameters \\(a\in\mathbb{R}^+\\) and
\\(p\in[0,1]\\). Equivalently, the two parameters can be expressed by the mean
\\(\mu=pa/(1-p)\\) and a parameter called the **dispersion** \\(\alpha=1/a\\).
The variance of the distribution depends on these parameters, and is
\\(\mu+\alpha\mu^2\\).

[![](04-chap_files/figure-html/fig-mixtures-
dgammapois-1.png)](04-chap_files/figure-html/fig-mixtures-dgammapois-1.png
"Figure 4.23: Visualization of the hierarchical model that generates the
gamma-Poisson distribution. The top panel shows the density of a gamma
distribution with mean 50 \(vertical black line\) and variance 30. Assume that
in one particular experimental replicate, the value 60 is realized. This is
our latent variable. The observable outcome is distributed according to the
Poisson distribution with that rate parameter, shown in the middle panel. In
one particular experiment the outcome may be, say, 55, indicated by the dashed
green line. Overall, if we repeat these two subsequent random process many
times, the outcomes will be distributed as shown in the bottom panel – the
gamma-Poisson distribution.")

Figure 4.23: Visualization of the hierarchical model that generates the gamma-
Poisson distribution. The top panel shows the density of a gamma distribution
with mean 50 (vertical black line) and variance 30. Assume that in one
particular experimental replicate, the value 60 is realized. This is our
latent variable. The observable outcome is distributed according to the
Poisson distribution with that rate parameter, shown in the middle panel. In
one particular experiment the outcome may be, say, 55, indicated by the dashed
green line. Overall, if we repeat these two subsequent random process many
times, the outcomes will be distributed as shown in the bottom panel – the
gamma-Poisson distribution.

__

Question 4.19

If you are more interested in analytical derivations than illustrative
simulations, try writing out the mathematical derivation of the gamma-Poisson
probability distribution.

__

Solution

__

Recall that the final distribution is the result of a two step process:

  1. Generate a \\(\text{gamma}(a,b)\\) distributed number, call it \\(x\\), from the density

\\[ f_\Gamma(x, a, b)=\frac{b^a}{\Gamma(a)}\,x^{a-1}\,e^{-b x}, \tag{4.16}\\]

where \\(\Gamma\\) is the so-called \\(\Gamma\\)-function,
\\(\Gamma(a)=\int_0^\infty x^{a-1}\,e^{-x}\,\text{d}x\\) (not to be confused
with the gamma distribution, even though there is this incidental relation).

  2. Generate a number \\(k\\) from the Poisson distribution with rate \\(x\\). The probability distribution is

\\[ f_{\text{Pois}}(k, \lambda=x)=\frac{x^{k}e^{-x}}{k!} \\]

If \\(x\\) only took on a finite set of values, we could solve the problem
simply by summing over all the possible cases, each weighted by their
probability according to \\(f_\Gamma\\). But \\(x\\) is continuous, so we have
to write the sum out as an integral instead of a discrete sum. We call the
distribution of \\(K\\) the marginal. Its probability mass function is

\\[ \begin{aligned} P(K=k)&=&\int_{x=0}^{\infty} \, f_{\text{Pois}}(k,
\lambda=x)\; f_\Gamma(x, a, b) \;dx\\\ &=& \int_{x=0}^{\infty}
\frac{x^{k}e^{-x}}{k!}\;\frac{b^a}{\Gamma(a)} x^{a-1}e^{-bx}\; dx
\end{aligned} \\]

Collect terms and move terms independent of \\(x\\) outside the integral

\\[ P(K=k)=\frac{b^a}{\Gamma(a)\,k!} \int_{x=0}^{\infty} x^{k+a-1}e^{-(b+1)x}
dx \\]

Because we know the gamma density sums to one: \\(\int_0^\infty
x^{k+a-1}e^{-(b+1)x} dx = \frac{\Gamma(k+a)}{(b+1)^{k+a}}\\)

\\[ \begin{aligned} P(K=k)
&=&\frac{\Gamma(k+a)}{\Gamma(a)\Gamma(k+1)}\frac{b^a}{(b+1)^{a}(b+1)^k}
={k+a-1\choose k}\left(\frac{b}{b+1}\right)^a\left(1-\frac{b}{b+1}\right)^k
\end{aligned} \\]

where in the last line we used that \\(\Gamma(v+1)=v!\\). This is the same as
Equation (4.15), the gamma-Poisson with size parameter \\(a\\) and probability
\\(p=\frac{b}{b+1}\\).

### 4.4.4 Variance stabilizing transformations

A key issue we need to control when we analyse experimental data is how much
variability there is between repeated measurements of the same underlying true
value, i.e., between replicates. This will determine whether and how well we
can see any true differences, i.e., between different conditions. Data that
arise through the type of hierarchical models we have studied in this chapter
often turn out to have very heterogeneous variances, and this can be a
challenge. We will see how in such cases **variance-stabilizing
transformations** ([Anscombe 1948](16-chap.html#ref-Anscombe1948)) can help.
Let’s start with a series of Poisson variables with rates from 10 to 100:

Note how we construct the dataframe (or, more precisely, the _tibble_)
`simdat`: the output of the `lapply` loop is a list of _tibble_ s, one for
each value of `lam`. With the pipe operator `|>` we send it to the function
`bind_rows` (from the
**[dplyr](https://cran.r-project.org/web/packages/dplyr/)** package). The
result is a dataframe of all the list elements neatly stacked on top of each
other.

    
    
    simdat = lapply(seq(10, 100, by = 10), function(lam)
        tibble(n = rpois(200, lambda = lam),
               `sqrt(n)` = sqrt(n),
           lambda = lam)) |>
      bind_rows() |>
      tidyr::pivot_longer(cols = !lambda)
    ggplot(simdat, aes(x = factor(lambda), y = value)) + xlab(expression(lambda)) +
      geom_violin() + facet_grid(rows = vars(name), scales = "free")__

[![](04-chap_files/figure-html/fig-
seriesofpoisson-1.png)](04-chap_files/figure-html/fig-seriesofpoisson-1.png
"Figure 4.24: Poisson distributed measurement data, for eight different
choices of the mean lambda. In the upper panel, the y-axis is proportional to
the data; in the lower panel, it is on a square-root scale. Note how the
distribution widths change in the first case, but less so in the second.")

Figure 4.24: Poisson distributed measurement data, for eight different choices
of the mean `lambda`. In the upper panel, the \\(y\\)-axis is proportional to
the data; in the lower panel, it is on a square-root scale. Note how the
distribution widths change in the first case, but less so in the second.

The data that we see in the upper panel of Figure 4.24 are an example of what
is called **heteroscedasticity** : the standard deviations (or, equivalently,
the variance) of our data is different in different regions of our data space.
In particular, it increases along the \\(x\\)-axis, with the mean. For the
Poisson distribution, we indeed know that the standard deviation is the square
root of the mean; for other types of data, there may be other dependencies.
This can be a problem if we want to apply subsequent analysis techniques (for
instance, regression, or a statistical test) that are based on assuming that
the variances are the same. In Figure 4.24, the numbers of replicates for each
value of lambda are quite large. In practice, this is not always the case.
Moreover, the data are usually not explicitly stratified by a known mean as in
our simulation, so the heteroskedasticity may be harder to see, even though it
is there. However, as we see in the lower panel of Figure 4.24, if we simply
apply the square root transformation, then the transformed variables will have
approximately the same variance. This works even if we do not know the
underlying mean for each observation, the square root transformation does not
need this information. We can verify this more quantitatively, as in the
following code, which shows the standard deviations of the sampled values `n`
and `sqrt(n)` for the difference choices of `lambda`.

The standard deviation of the square root transformed values is consistently
around 0.5, so we would use the transformation `2*sqrt(n)` to achieve unit
variance.

    
    
    summarise(group_by(simdat, name, lambda), sd(value)) |> tidyr::pivot_wider(values_from = `sd(value)`)__
    
    
    # A tibble: 10 × 3
       lambda     n `sqrt(n)`
        <dbl> <dbl>     <dbl>
     1     10  2.95     0.478
     2     20  4.19     0.470
     3     30  5.62     0.521
     4     40  5.99     0.473
     5     50  7.69     0.546
     6     60  7.59     0.492
     7     70  8.69     0.520
     8     80  8.99     0.505
     9     90  9.44     0.498
    10    100  9.84     0.495

Another example, now using the gamma-Poisson distribution, is shown in Figure
4.25. We generate gamma-Poisson variables `u`5 and plot the 95% confidence
intervals around the mean.

5 To catch a greater range of values for the mean value `mu`, without creating
too dense a sequence, we use a geometric series: \\(\mu_{i+1} = 2\mu_i\\).

    
    
    muvalues = 2^seq(0, 10, by = 1)
    simgp = lapply(muvalues, function(mu) {
      u = rnbinom(n = 1e4, mu = mu, size = 4)
      tibble(mean = mean(u), sd = sd(u),
             lower = quantile(u, 0.025),
             upper = quantile(u, 0.975),
             mu = mu)
      } ) |> bind_rows()
    head(as.data.frame(simgp), 2)__
    
    
        mean       sd lower upper mu
    1 0.9965 1.106440     0     4  1
    2 2.0233 1.748503     0     6  2
    
    
    ggplot(simgp, aes(x = mu, y = mean, ymin = lower, ymax = upper)) +
      geom_point() + geom_errorbar()__

[![](04-chap_files/figure-html/fig-seriesofnb-1.png)](04-chap_files/figure-
html/fig-seriesofnb-1.png "Figure 4.25: gamma-Poisson distributed measurement
data, for a range of \\mu from 1 to 1024.")

Figure 4.25: gamma-Poisson distributed measurement data, for a range of
\\(\mu\\) from 1 to 1024.

__

Question 4.20

How can we find a transformation for these data that stabilizes the variance,
similar to the square root function for the Poisson distributed data?

__

Solution

__

If we divide the values that correspond to `mu[1]` (and which are centered
around `simgp$mean[1]`) by their standard deviation `simgp$sd[1]`, the values
that correspond to `mu[2]` (and which are centered around `simgp$mean[2]`) by
their standard deviation `simgp$sd[2]`, and so on, then the resulting values
will have, by construction, a standard deviation (and thus variance) of 1. And
rather than defining 11 separate transformations, we can achieve our goal by
defining one single piecewise linear _and_ continuous function that has the
appropriate slopes at the appropriate values.

    
    
    simgp = mutate(simgp,
      slopes = 1 / sd,
      trsf   = cumsum(slopes * mean))
    ggplot(simgp, aes(x = mean, y = trsf)) +
      geom_point() + geom_line() + xlab("")__

[![](04-chap_files/figure-html/fig-pcwlin-1-1.png)](04-chap_files/figure-
html/fig-pcwlin-1-1.png "Figure 4.26: Piecewise linear function that
stabilizes the variance of the data in Figure fig-seriesofnb.")

Figure 4.26: Piecewise linear function that stabilizes the variance of the
data in Figure 4.25.

We see in Figure 4.26 that this function has some resemblance to a square root
function in particular at its lower end. At the upper end, it seems to look
more like a logarithm. The more mathematically inclined will see that an
elegant extension of these numerical calculations can be done through a little
calculus known as the **delta method** , as follows.

Call our transformation function \\(g\\), and assume it’s differentiable
(that’s not a very strong assumption: pretty much any function that we might
consider reasonable here is differentiable). Also call our random variables
\\(X_i\\), with means \\(\mu_i\\) and variances \\(v_i\\), and we assume that
\\(v_i\\) and \\(\mu_i\\) are related by a functional relationship \\(v_i =
v(\mu_i)\\). Then, for values of \\(X_i\\) in the neighborhood of its mean
\\(\mu_i\\),

\\[ g(X_i) = g(\mu_i) + g'(\mu_i) (X_i-\mu_i) + ... \tag{4.17}\\]

where the dots stand for higher order terms that we can neglect. The variances
of the transformed values are then

\\[ \begin{align} \text{Var}(g(X_i)) &\simeq g'(\mu_i)^2\\\ \text{Var}(X_i) &=
g'(\mu_i)^2 \, v(\mu_i), \end{align} \tag{4.18}\\]

where we have used the rules \\(\text{Var}(X-c)=\text{Var}(X)\\) and
\\(\text{Var}(cX)=c^2\,\text{Var}(X)\\) that hold whenever \\(c\\) is a
constant number. Requiring that this be constant leads to the differential
equation

\\[ g'(x) = \frac{1}{\sqrt{v(x)}}. \tag{4.19}\\]

For a given mean-variance relationship \\(v(\mu)\\), we can solve this for the
function \\(g\\). Let’s check this for some simple cases:

  * if \\(v(\mu)=\mu\\) (Poisson), we recover \\(g(x)=\sqrt{x}\\), the square root transformation.

  * If \\(v(\mu)=\alpha\,\mu^2\\), solving the differential equation 4.19 gives \\(g(x)=\log(x)\\). This explains why the logarithm transformation is so popular in many data analysis applications: it acts as a variance stabilizing transformation whenever the data have a constant coefficient of variation, that is, when the standard deviation is proportional to the mean.

__

Question 4.21

What is the variance-stabilizing transformation associated with \\(v(\mu) =
\mu + \alpha\,\mu^2\\)?

__

Solution

__

To solve the differential equation 4.19 with this function \\(v(\cdot)\\), we
need to compute the integral

\\[\label{eq:} \int \frac{dx}{\sqrt{x + \alpha x^2}}. \tag{4.20}\\]

A closed form expression can be looked up in a reference table such as
([Bronštein and Semendjajew 1979](16-chap.html#ref-BronsteinSemendjajew)).
These authors provide the general solution

\\[ \int \frac{dx}{\sqrt{ax^2+bx+c}} = \frac{1}{\sqrt{a}}
\ln\left(2\sqrt{a(ax^2+bx+c)}+2ax+b\right) + \text{const.}, \tag{4.21}\\]

into which we can plug in our special case \\(a=\alpha\\), \\(b=1\\),
\\(c=0\\), to obtain the variance-stabilizing transformation

\\[ \begin{align} g_\alpha(x) &= \frac{1}{2\sqrt{\alpha}}
\ln\left(2\sqrt{\alpha x (\alpha x+1)} + 2\alpha x + 1\right) \\\ &=
\frac{1}{2\sqrt{\alpha}} {\displaystyle \operatorname {arcosh}} (2\alpha
x+1).\\\ \end{align} \tag{4.22}\\]

For the second line in Equation 4.22, we used the identity \\({\displaystyle
\operatorname {arcosh}}(z) = \ln\left(z+\sqrt{z^2-1}\right)\\). In the limit
of \\(\alpha\to0\\), we can use the linear approximation
\\(\ln(1+\varepsilon)=\varepsilon+O(\varepsilon^2)\\) to see that
\\(g_0(x)=\sqrt{x}\\). Note that if \\(g_\alpha\\) is a variance-stabilizing
transformation, then so is \\(ug_\alpha+v\\) for any pair of numbers \\(u\\)
and \\(v\\), and we have used this freedom to insert an extra factor
\\(\frac{1}{2}\\) for reasons that become apparent in the following. You can
verify that the function \\(g_\alpha\\) from Equation 4.22 fulfills condition
4.19 by computing its derivative, which is an elementary calculation. We can
plot it:

    
    
    f = function(x, a) 
      ifelse (a==0, 
        sqrt(x), 
        log(2*sqrt(a) * sqrt(x*(a*x+1)) + 2*a*x+1) / (2*sqrt(a)))
    x  = seq(0, 24, by = 0.1)
    df = lapply(c(0, 0.05*2^(0:5)), function(a) 
      tibble(x = x, a = a, y = f(x, a))) %>% bind_rows()
    ggplot(df, aes(x = x, y = y, col = factor(a))) + 
      geom_line() + labs(col = expression(alpha))__

[![](04-chap_files/figure-html/fig-
plotvstgammapoisson-1-1.png)](04-chap_files/figure-html/fig-
plotvstgammapoisson-1-1.png "Figure 4.27: Graph of the function Equation eq-
mixtures-vstgammapoisson for different choices of \\alpha.")

Figure 4.27: Graph of the function Equation 4.22 for different choices of
\\(\alpha\\).

and empirically verify the equivalence of two terms in Equation 4.22:

    
    
    f2 = function(x, a) ifelse (a==0, sqrt(x), acosh(2*a*x + 1) / (2*sqrt(a)))  
    with(df, max(abs(f2(x,a) - y)))__
    
    
    [1] 8.881784e-16

As we see in Figure 4.27, for small values of \\(x\\), \\(g_\alpha(x)\approx
\sqrt{x}\\) (independently of \\(\alpha\\)), whereas for large values
(\\(x\to\infty\\)) and \\(\alpha>0\\), it behaves like a logarithm:

\\[ \begin{align} &\frac{1}{2\sqrt{\alpha}}\ln\left(2\sqrt{\alpha\left(\alpha
x^2+x\right)}+2\alpha x+1\right)\\\ \approx&\frac{1}{2\sqrt{\alpha}}
\ln\left(2\sqrt{\alpha^2x^2}+2\alpha x\right)\\\
=&\frac{1}{2\sqrt{\alpha}}\ln\left(4\alpha x\right)\\\
=&\frac{1}{2\sqrt{\alpha}}\ln x+\text{const.} \end{align} \\]

We can verify this empirically by, say,

    
    
      a = c(0.2, 0.5, 1)
      f(1e6, a) __
    
    
    [1] 15.196731 10.259171  7.600903
    
    
      1/(2*sqrt(a)) * (log(1e6) + log(4*a))__
    
    
    [1] 15.196728 10.259170  7.600902

## 4.5 Summary of this chapter

We have given motivating examples and ways of using mixtures to model
biological data. We saw how the EM algorithm is an interesting example of
fitting a difficult-to-estimate probabilistic model to data by iterating
between partial, simpler problems.

#### Finite mixture models

We have seen how to model mixtures of two or more normal distributions with
different means and variances. We have seen how to decompose a given sample of
data from such a mixture, even without knowing the latent variable, using the
EM algorithm. The EM approach requires that we know the parametric form of the
distributions and the number of components. In [Chapter 5](05-chap.html), we
will see how we can find groupings in data even without relying on such
information – this is then called clustering. We can keep in mind that there
is a strong conceptual relationship between clustering and mixture modeling.

#### Common infinite mixture models

Infinite mixture models are good for constructing new distributions (such as
the gamma-Poisson or the Laplace) out of more basic ones (such as binomial,
normal, Poisson). Common examples are

  * mixtures of normals (often with a hierarchical model on the means and the variances);

  * beta-binomial mixtures – where the probability \\(p\\) in the binomial is generated according to a \\(\text{beta}(a, b)\\) distribution;

  * gamma-Poisson for read counts (see [Chapter 8](08-chap.html));

  * gamma-exponential for PCR.

#### Applications

Mixture models are useful whenever there are several layers of experimental
variability. For instance, at the lowest layer, our measurement precision may
be limited by basic physical detection limits, and these may be modeled by a
Poisson distribution in the case of a counting-based assay, or a normal
distribution in the case of the continuous measurement. On top of there may be
one (or more) layers of instrument-to-instrument variation, variation in the
reagents, operator variaton etc.

Mixture models reflect that there is often heterogeneous amounts of
variability (variances) in the data. In such cases, suitable data
transformations, i.e., variance stabilizing transformations, are necessary
before subsequent visualization or analysis. We’ll study in depth an example
for RNA-Seq in [Chapter 8](08-chap.html), and this also proves useful in the
normalization of next generation reads in microbial ecology ([McMurdie and
Holmes 2014](16-chap.html#ref-mcmurdie2014)).

Another important application of mixture modeling is the two-component model
in multiple testing – we will come back to this in [Chapter 6](06-chap.html).

#### The ECDF and bootstrapping

We saw that by using the observed sample as a mixture we could generate many
simulated samples that inform us about the sampling distribution of an
estimate. This method is called the bootstrap and we will return to it several
times, as it provides a way of evaluating estimates even when a closed form
expression is not available (we say it is non-parametric).

## 4.6 Further reading

A useful book-long treatment of finite mixture models is by McLachlan and Peel
([2004](16-chap.html#ref-mclachlan2004)); for the EM algorithm, see also the
book by McLachlan and Krishnan ([2007](16-chap.html#ref-
mclachlan2007algorithm)). A recent book that presents all EM type algorithms
within the Majorize-Minimization (MM) framework is by Lange
([2016](16-chap.html#ref-lange2016mm)).

There are in fact mathematical reasons why many natural phenomena can be seen
as mixtures: this occurs when the observed events are exchangeable (the order
in which they occur doesn’t matter). The theory underlying this is quite
mathematical, a good way to start is to look at the Wikipedia entry and the
paper by Diaconis and Freedman ([1980](16-chap.html#ref-diaconis1980finite)).

In particular, we use mixtures for high-throughput data. You will see examples
in Chapters [8](08-chap.html) and [11](11-chap.html).

The bootstrap can be used in many situations and is a very useful tool to know
about, a friendly treatment is given in ([B. Efron and Tibshirani
1993](16-chap.html#ref-efront)).

A historically interesting paper is the original article on variance
stabilization by Anscombe ([1948](16-chap.html#ref-Anscombe1948)), who
proposed ways of making variance stabilizing transformations for Poisson and
gamma-Poisson random variables. Variance stabilization is explained using the
delta method in many standard texts in theoretical statistics, e.g., those by
Rice ([2006, chap. 6](16-chap.html#ref-Rice:2007)) and Kéry and Royle ([2015,
35](16-chap.html#ref-Kery2015)).

Kéry and Royle ([2015](16-chap.html#ref-Kery2015)) provide a nice exploration
of using R to build hierarchical models for abundance estimation in niche and
spatial ecology.

## 4.7 Exercises

__

Exercise 4.1

**The EM algorithm step by step.** As an example dataset, we use the values in
the file `Myst.rds`. As always, it is a good idea to first visualize the data.
The histogram is shown in Figure 4.28. We are going to model these data as a
mixture of two normal distributions with unknown means and standard
deviations, and unknown mixture fraction. We’ll call the two components A and
B.

    
    
    mx = readRDS("../data/Myst.rds")$yvar
    str(mx)__
    
    
     num [1:1800] 0.3038 0.0596 -0.0204 0.1849 0.2842 ...
    
    
    ggplot(tibble(mx), aes(x = mx)) + geom_histogram(binwidth = 0.025)__

We start by randomly assigning the membership weights for each of the values
in `mx` for each of the components

    
    
    wA = runif(length(mx))
    wB = 1 - wA __

We also need to set up some housekeeping variables: `iter` counts over the
iterations of the EM algorithm; `loglik` stores the current log-likelihood;
`delta` stores the change in the log-likelihood from the previous iteration to
the current one. We also define the parameters `tolerance`, `miniter` and
`maxiter` of the algorithm.

    
    
    iter      = 0
    loglik    = -Inf
    delta     = +Inf
    tolerance = 1e-12
    miniter   = 50
    maxiter   = 1000 __

Study the code below and answer the following questions:

  1. Which lines correspond to the E-step, which to the M-step?

  2. What is the role of `tolerance`, `miniter` and `maxiter`?

  3. Compare the result of what we are doing here to the output of the `normalmixEM` function from the **[mixtools](https://cran.r-project.org/web/packages/mixtools/)** package.

    
    
    while((delta > tolerance) && (iter <= maxiter) || (iter < miniter)) {
      lambda = mean(wA)
      muA = weighted.mean(mx, wA)
      muB = weighted.mean(mx, wB)
      sdA = sqrt(weighted.mean((mx - muA)^2, wA))
      sdB = sqrt(weighted.mean((mx - muB)^2, wB))
    
      pA   =    lambda    * dnorm(mx, mean = muA, sd = sdA)
      pB   = (1 - lambda) * dnorm(mx, mean = muB, sd = sdB)
      ptot = pA + pB
      wA   = pA / ptot
      wB   = pB / ptot
    
      loglikOld = loglik
      loglik = sum(log(pA + pB))
      delta = abs(loglikOld - loglik)
      iter = iter + 1
    }
    iter __
    
    
    [1] 447
    
    
    c(lambda, muA, muB, sdA, sdB)__
    
    
    [1]  0.4756 -0.1694  0.1473  0.0983  0.1498

[![](04-chap_files/figure-html/fig-
EMillustrate-1-1.png)](04-chap_files/figure-html/fig-EMillustrate-1-1.png
"Figure 4.28: Histogram of mx, our example data for the EM algorithm.")

Figure 4.28: Histogram of `mx`, our example data for the EM algorithm.

__

Solution

__

The first five lines in the `while` loop implement the _Maximization step_.
Given the current values of `wA` and `wB`, we estimate the parameters of the
mixture model using the maximum-likelihood estimators: the mixture fraction
`lambda` by the mean of `wA`, and the parameters of the two normal
distribution components (`muA`, `sdA`) and (`muB`, `sdB`) by the sample means
and the sample standard deviations. To take into account the membership
weights, we use the weighted mean (function `weighted.mean`) and standard
deviation.

Next comes the _Expectation step_. For each of the elements in the data vector
`mx`, we compute the probability densities `pA` and `pB` for the generative
distribution models A and B, using the normal density function `dnorm`,
weighted by the mixture fractions `lambda` and `(1-lambda)`, respectively.
From this, we compute the updated membership weights `wA` and `wB`, according
to Equation 4.5.

Given the membership weights and the parameters, the logarithmic likelihood
`loglik` is easily computed, and the `while` loop iterates these steps.

The termination criterion for the loop is based on `delta`, the change in the
likelihood. The loop can end if this becomes smaller than `tolerance`. This is
a simple way of checking whether the algorithm has converged. The additional
conditions on `iter` make sure that at least `miniter` iterations are run, and
that the loop always stops after `maxiter` iterations. The latter is to make
sure that the loop terminates in finite time no matter what. (“Professional”
implementations of such iterative algorithms typically work a bit harder to
decide what is the best time to stop.)

Finally, let’s compare our estimates to those from the function `normalmixEM`
from the **[mixtools](https://cran.r-project.org/web/packages/mixtools/)**
package.

    
    
    gm = mixtools::normalmixEM(mx, k = 2)__
    
    
    number of iterations= 215 
    
    
    with(gm, c(lambda[1], mu, sigma))__
    
    
    [1]  0.4757 -0.1694  0.1473  0.0983  0.1498

__

Exercise 4.2

Why do we often consider the logarithm of the likelihood rather than the
likelihood? E.g., in the EM code above, why did we work with the probabilities
on the logarithmic scale?

__

Solution

__

Likelihoods often (whenever the data points are sampled independently) take
the form of a product. This is, for instance, the case in Equation 4.4.
Calculating the derivative, for likelihood optimisation, would then require
application of the product rule. On the logarithmic scale, the product turns
into a sum, and the derivative of a sum is simply the sum of the derivatives
of the individual summands.

An additional reason comes from the way computers implement arithmetic. They
commonly use a floating point representation of numbers with a finite number
of bits. E.g., the IEEE 754-2008 standard uses 64 bits for a _double-
precision_ number: 1 bit for the sign, 52 for the mantissa (also called
significand), 11 for the exponent. Multiplication between such numbers implies
addition of the exponents, but the range of the exponent is only \\(0\\) to
\\(2^{11}-1=2047\\). Even likelihoods that involve only a few hundred data
points can lead to arithmetic overflow or other problems with precision. On
the logarithmic scale, where the product is a sum, the workload tends to be
better distributed between mantissa and exponent, and log-likelihoods even
with millions of data points to be handled with reasonable precision.

See also Gregory Gundersen’s post on the [Log-Sum-Exp Trick for normalizing
vectors of log
probabilities](https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/).

__

Exercise 4.3

Compare the theoretical values of the gamma-Poisson distribution with
parameters given by the estimates in `ofit$par` in Section 4.4.3 to the data
used for the estimation using a QQ-plot.

__

Exercise 4.4

**Mixture modeling examples for regression**. The
**[flexmix](https://cran.r-project.org/web/packages/flexmix/)** package
([Grün, Scharl, and Leisch 2012](16-chap.html#ref-Grun2012)) enables us to
cluster and fit regressions to the data at the same time. The standard M-step
`FLXMRglm` of **[flexmix](https://cran.r-project.org/web/packages/flexmix/)**
is an interface to R’s generalized linear modeling facilities (the `glm`
function). Load the package and an example dataset.

    
    
    library("flexmix")
    data("NPreg")__

  1. First, plot the data and try to guess how the points were generated.

  2. Fit a two component mixture model using the commands

    
    
    m1 = flexmix(yn ~ x + I(x^2), data = NPreg, k = 2)__

  3. Look at the estimated parameters of the mixture components and make a truth table that cross-classifies true classes versus cluster memberships. What does the summary of the object `m1` show us?

  4. Plot the data again, this time coloring each point according to its estimated class.

__

Solution

__

    
    
    ggplot(NPreg, aes(x = x, y = yn)) + geom_point()__

[![](04-chap_files/figure-html/fig-npreg-1.png)](04-chap_files/figure-
html/fig-npreg-1.png "Figure 4.29: The points seem to come from two different
generative processes, one is linear; the other quadratic.")

Figure 4.29: The points seem to come from two different generative processes,
one is linear; the other quadratic.

The components are:

    
    
    modeltools::parameters(m1, component = 1)__
    
    
                          Comp.1
    coef.(Intercept) -0.20998685
    coef.x            4.81807854
    coef.I(x^2)       0.03613061
    sigma             3.47665584
    
    
    modeltools::parameters(m1, component = 2)__
    
    
                         Comp.2
    coef.(Intercept) 14.7167886
    coef.x            9.8468507
    coef.I(x^2)      -0.9683734
    sigma             3.4795657

The parameter estimates of both components are close to the true values. A
cross-tabulation of true classes and cluster memberships can be obtained by

    
    
    table(NPreg$class, modeltools::clusters(m1))__
    
    
       
         1  2
      1 95  5
      2  5 95

For our example data, the ratios of both components are approximately 0.7,
indicating the overlap of the classes at the cross-section of line and
parabola.

    
    
    summary(m1)__

The summary shows the estimated prior probabilities \\(\hat\pi_k\\), the
number of observations assigned to the two clusters, the number of
observations where \\(p_{nk}>\delta\\) (with a default of
\\(\delta=10^{-4}\\)), and the ratio of the latter two numbers. For well-
separated components, a large proportion of observations with non-vanishing
posteriors \\(p_{nk}\\) should be assigned to their cluster, giving a ratio
close to 1.

    
    
    NPreg = mutate(NPreg, gr = factor(class))
    ggplot(NPreg, aes(x = x, y = yn, group = gr)) +
       geom_point(aes(colour = gr, shape = gr)) +
       scale_colour_hue(l = 40, c = 180)__

[![](04-chap_files/figure-html/fig-npregC-1.png)](04-chap_files/figure-
html/fig-npregC-1.png "Figure 4.30: Regression example using flexmix with the
points colored according to their estimated class. You can see that at the
intersection we have an `identifiability’ problem: we cannot distinguish
points that belong to the straight line from ones that belong to the
parabole.")

Figure 4.30: Regression example using `flexmix` with the points colored
according to their estimated class. You can see that at the intersection we
have an `identifiability’ problem: we cannot distinguish points that belong to
the straight line from ones that belong to the parabole.

__

Exercise 4.5

**Other hierarchical noise models:**  
Find two papers that explore the use of other infinite mixtures for modeling
molecular biology technological variation.

__

Solution

__

The paper by Chen, Xie, and Story ([2011](16-chap.html#ref-Chen2011)) explores
an exponential-Poisson model for modeling background noise in bead arrays.
Wills et al. ([2013](16-chap.html#ref-Wills2013)) compares several Poisson
mixture models.

Anscombe, Francis J. 1948. “The Transformation of Poisson, Binomial and
Negative-Binomial Data.” _Biometrika_ , 246–54.

Bishop, Christopher M. 2006. _Pattern Recognition and Machine Learning_.
Springer.

Bronštein, Il’ja N., and Konstantin A Semendjajew. 1979. _Taschenbuch Der
Mathematik_. B.G. Teubner Verlagsgesellschaft, Leipzig; Verlag Nauka, Moscow.

Bulmer, Michael George. 2003. _Francis Galton: Pioneer of Heredity and
Biometry_. JHU Press.

Chen, Min, Yang Xie, and Michael Story. 2011. “An Exponential-Gamma
Convolution Model for Background Correction of Illumina BeadArray Data.”
_Communications in Statistics-Theory and Methods_ 40 (17): 3055–69.

Diaconis, Persi, and David Freedman. 1980. “Finite Exchangeable Sequences.”
_The Annals of Probability_ , 745–64.

Diaconis, Persi, and Susan Holmes. 1994. “Gray Codes for Randomization
Procedures.” _Statistics and Computing_ 4 (4): 287–302.

Efron, Bradley, and Robert J Tibshirani. 1994. _An Introduction to the
Bootstrap_. CRC press.

Efron, B., and R. Tibshirani. 1993. _An Introduction to the Bootstrap_.
Chapman & Hall/CRC.

Grün, Bettina, Theresa Scharl, and Friedrich Leisch. 2012. “Modelling Time
Course Gene Expression Data with Finite Mixtures of Linear Additive Models.”
_Bioinformatics_ 28 (2): 222–28.
<https://doi.org/10.1093/bioinformatics/btr653>.

Hoeting, Jennifer A, David Madigan, Adrian E Raftery, and Chris T Volinsky.
1999. “Bayesian Model Averaging: A Tutorial.” _Statistical Science_ , 382–401.

Kéry, Marc, and J Andrew Royle. 2015. _Applied Hierarchical Modeling in
Ecology: Analysis of Distribution, Abundance and Species Richness in r and
BUGS: Volume 1: Prelude and Static Models_. Academic Press.

Kristiansson, Erik, Michael Thorsen, Markus J Tamás, and Olle Nerman. 2009.
“Evolutionary Forces Act on Promoter Length: Identification of Enriched Cis-
Regulatory Elements.” _Molecular Biology and Evolution_ 26 (6): 1299–1307.

Kuan, Pei Fen, Dongjun Chung, Guangjin Pan, James A Thomson, Ron Stewart, and
Sündüz Keleş. 2011. “A Statistical Framework for the Analysis of ChIP-Seq
Data.” _Journal of the American Statistical Association_ 106 (495): 891–903.

Lange, Kenneth. 2016. _MM Optimization Algorithms_. SIAM.

McLachlan, Geoffrey, and Thriyambakam Krishnan. 2007. _The EM Algorithm and
Extensions_. Vol. 382. John Wiley & Sons.

McLachlan, Geoffrey, and David Peel. 2004. _Finite Mixture Models_. John Wiley
& Sons.

McMurdie, Paul J, and Susan Holmes. 2014. “Waste Not, Want Not: Why Rarefying
Microbiome Data Is Inadmissible.” _PLoS Computational Biology_ 10 (4):
e1003531.

Purdom, Elizabeth, and Susan P Holmes. 2005. “Error Distribution for Gene
Expression Data.” _Statistical Applications in Genetics and Molecular Biology_
4 (1).

Rice, John. 2006. _Mathematical Statistics and Data Analysis_. Cengage
Learning.

Shalizi, Cosma. 2017. _Advanced Data Analysis from an Elementary Point of
View_. Cambridge University Press.
<https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf>.

Slonim, Noam, Gurinder Singh Atwal, Gašper Tkačik, and William Bialek. 2005.
“Information-Based Clustering.” _PNAS_ 102 (51): 18297–302.

Tibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.”
_Journal of the Royal Statistical Society. Series B (Methodological)_ ,
267–88.

Wills, Quin F, Kenneth J Livak, Alex J Tipping, Tariq Enver, Andrew J Goldson,
Darren W Sexton, and Chris Holmes. 2013. “Single-Cell Gene Expression Analysis
Reveals Genetic Associations Masked in Whole-Tissue Experiments.” _Nature
Biotechnology_ 31 (8): 748–52.

Zeileis, Achim, Christian Kleiber, and Simon Jackman. 2008. “Regression Models
for Count Data in R.” _Journal of Statistical Software_ 27 (8).
<http://www.jstatsoft.org/v27/i08/>.

Page built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)

