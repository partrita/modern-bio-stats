![](imgs/starlings_copyrightfree.jpg)

세포, 질병, 유기체의 범주를 찾고 그 이름을 짓는 것은 자연과학의 핵심 활동입니다. [4장](04-chap.html)에서 우리는 일부 데이터를 명확한 모수적 생성 모델을 가진 서로 다른 그룹이나 모집단의 혼합물로 모델링할 수 있음을 보았습니다. 우리는 그러한 예제들에서 EM 알고리즘을 사용하여 성분들을 어떻게 분리할 수 있는지 보았습니다. 이제 우리는 **클러스터(clusters)**가 반드시 예쁜 타원형1 모양을 가질 필요는 없는 경우로 그룹들을 풀어내는 아이디어를 확장해 보려 합니다.

1 다변량 정규 분포를 이용한 혼합 모델링은 타원형 클러스터 경계를 함축합니다.

군집화(Clustering)는 데이터(연속형 또는 준연속형)를 가져와서 새로운 범주형 _그룹_ 변수를 추가하며, 이는 때때로 중간 상태를 무시하는 대가를 치르더라도 의사 결정을 단순화할 수 있습니다. 예를 들어, 공복 혈당, 당화혈색소, 섭취 2시간 후 혈장 포도당 수치와 관련된 복잡한 고차원 진단 수치들을 단순히 환자를 당뇨병 "그룹"에 할당함으로써 의료 결정을 단순화합니다.

이 장에서 우리는 저차원 및 고차원 **비모수적(nonparametric)** 환경 모두에서 의미 있는 클러스터나 그룹을 찾는 방법을 공부할 것입니다. 그러나 주의할 점이 있습니다: 군집화 알고리즘은 클러스터를 찾도록 설계되었으므로, 클러스터가 없는 곳에서도 클러스터를 찾아낼 것입니다2. 따라서 클러스터의 존재를 뒷받침하는 사전 지식이 없는 경우, 클러스터 _검증(validation)_은 우리 프로세스의 필수적인 구성 요소입니다.

2 이는 인간을 연상시킵니다: 우리는 무작위성 속에서도 패턴을 보기를 좋아합니다.

## 5.1 이 장의 목표

이 장에서 우리는 다음을 수행할 것입니다:

  * 유익하게 군집화될 수 있는 다양한 유형의 데이터를 공부합니다.

  * 클러스터를 정의하는 데 도움이 되는 (비)유사성 척도와 **거리(distances)**를 살펴봅니다.

  * 데이터를 더 _조밀한_ 세트로 분할하여 숨겨진 또는 잠재된 군집을 찾아냅니다.

  * 수십만 개의 세포 각각에 대한 바이오마커가 주어졌을 때 군집화를 사용해 봅니다. 예를 들어 면역 세포가 어떻게 자연스럽게 조밀한 하위 집단으로 그룹화될 수 있는지 보게 될 것입니다.

  * 실제 단일 세포 데이터에 **\\(k\\) -평균(\\(k\\)-means)**, **\\(k\\) -메도이드(\\(k\\)-medoids)**와 같은 비모수적 알고리즘을 실행합니다.

  * 관측치와 그룹을 세트의 계층 구조로 결합하는 군집화에 대한 재귀적 접근 방식을 실험합니다. 이러한 방법은 **계층적 군집화(hierarchical clustering)**로 알려져 있습니다.

  * 재표본 추출 기반의 붓스트랩 접근 방식을 통해 클러스터를 검증하는 방법을 공부하며, 이를 단일 세포 데이터 세트에서 시연할 것입니다.

[![](imgs/SnowMapSmallest_web.png)](imgs/SnowMapSmallest_web.png "그림 5.1: 존 스노우의 콜레라 사례 지도: 각 집의 작은 막대 그래프는 진단된 사례들의 군집을 나타냅니다.")

그림 5.1: 존 스노우의 콜레라 사례 지도: 각 집의 작은 막대 그래프는 진단된 사례들의 군집을 나타냅니다.

[![David Freedman은 이 발견으로 이어진 모든 단계에 대해 훌륭하고 상세한 설명을 남겼습니다 (Freedman 1991).](imgs/book_icon.png)](imgs/book_icon.png "David Freedman은 이 발견으로 이어진 모든 단계에 대해 훌륭하고 상세한 설명을 남겼습니다 \[@freedman1991\].")

David Freedman은 이 발견으로 이어진 모든 단계에 대해 훌륭하고 상세한 설명을 남겼습니다 ([Freedman 1991](16-chap.html#ref-freedman1991)).

## 5.2 데이터란 무엇이며 왜 군집화하는가?

### 5.2.1 군집화는 때때로 발견으로 이어질 수 있습니다.

존 스노우(John Snow)는 콜레라 사례 지도를 만들고 사례들의 _클러스터_를 식별했습니다. 그런 다음 그는 펌프의 위치에 대한 추가 정보를 수집했습니다. 밀집된 사례 클러스터들이 브로드 스트리트(Broadstreet) 펌프와 가깝다는 사실은 물이 범인일 가능성을 지목했습니다. 그는 콜레라 발생의 원인을 추론할 수 있게 해주는 별도의 정보원들을 수집했습니다.

이제 그림 5.2에 표시된 또 다른 런던 지도를 살펴봅시다. 빨간색 점들은 제2차 세계대전 중 폭격을 받은 위치를 나타냅니다. 전쟁 중에 분석 팀들은 많은 이론을 내놓았습니다. 그들은 폭격 패턴(유틸리티 공장, 병기창과의 근접성, \\(...\\))에 대한 합리적인 설명을 찾으려 노력했습니다. 사실, 전쟁 후에 폭격은 특정 목표물을 타격하려는 시도 없이 무작위로 분포되었다는 것이 밝혀졌습니다.

[![](imgs/RedBombsLondon_web.png)](imgs/RedBombsLondon_web.png "그림 5.2: 1940년 9월 7일 런던에 떨어진 폭탄의 위치 지도입니다. 영국 국립 보존 기록관 웹사이트 http://bombsight.org에서 묘사한 것입니다.")

그림 5.2: 1940년 9월 7일 런던에 떨어진 폭탄의 위치 지도. 영국 국립 보존 기록관 웹사이트 <http://bombsight.org>에서 가져온 것입니다.

군집화는 복잡한 다변량 데이터를 이해하는 데 유용한 기법이며, 이는 **비지도(unsupervised)**3 학습입니다. 탐색적 기법은 데이터를 해석하는 데 중요할 수 있는 그룹화된 모습을 보여줍니다.

3 모든 변수가 동일한 상태를 가지며, 설명 변수의 정보를 바탕으로 한 변수(감독 반응)의 값을 예측하거나 학습하려고 하지 않기 때문에 이렇게 불립니다.

예를 들어, 군집화를 통해 연구자들은 암 생물학에 대한 이해를 높일 수 있었습니다. 해부학적 위치와 조직 병리학적 소견으로는 동일해 보였던 종양들이 유전자 발현 데이터와 같은 분자적 특성에 따라 여러 클러스터로 나뉘었습니다 ([Hallett et al. 2012](16-chap.html#ref-Hallett2012)). 결국 이러한 군집화는 새롭고 더 적절한 질병 유형의 정의로 이어질 수 있습니다. 적절성은 예를 들어 서로 다른 환자 예후와 연관되어 있다는 사실로 입증됩니다. 이 장에서 우리가 하고자 하는 것은 그림 5.3과 같은 그림들이 어떻게 구성되는지, 그리고 어떻게 해석해야 하는지를 이해하는 것입니다.

[![](imgs/BreastCancerSubType_Biomed.png)](imgs/BreastCancerSubType_Biomed.png
"그림 5.3: 유방암 샘플들(The Cancer Genome Atlas (TCGA) 및 Molecular Taxonomy of Breast Cancer International Consortium (METABRIC)에서 가져옴)은 그들의 miRNA 발현을 사용하여 그룹으로 나뉠 수 있습니다 \[@Aure2017\]. 저자들은 하단 플롯에서 서로 다른 그룹의 생존 기간이 다르다는 것을 보여줍니다. 따라서 이러한 클러스터들은 생물학적 및 임상적으로 의미가 있었습니다. 이러한 분석의 약속은 그룹들을 사용하여 더 구체적이고 최적화된 치료를 제공할 수 있다는 것입니다.")

그림 5.3: 유방암 샘플들(The Cancer Genome Atlas (TCGA) 및 Molecular Taxonomy of Breast Cancer International Consortium (METABRIC)에서 가져옴)은 그들의 miRNA 발현을 사용하여 그룹으로 나뉠 수 있습니다 ([Aure et al. 2017](16-chap.html#ref-Aure2017)). 저자들은 하단 플롯에서 서로 다른 그룹의 생존 기간이 다르다는 것을 보여줍니다. 따라서 이러한 클러스터들은 생물학적 및 임상적으로 의미가 있었습니다. 이러한 분석의 약속은 그룹들을 사용하여 더 구체적이고 최적화된 치료를 제공할 수 있다는 것입니다.

[4장](04-chap.html)에서 우리는 이미 그룹을 찾아내기 위한 한 가지 기법인 EM 알고리즘을 공부했습니다. 이 장에서 우리가 탐구하는 기법들은 더 일반적이며 더 복잡한 데이터에 적용될 수 있습니다. 이들 중 상당수는 관측치 쌍 사이의 거리에 기초하며(이는 전체 대 전체일 수도 있고, 때로는 전체 대 일부일 수도 있음), 정규 분포, 감마-포아송 등과 같은 특정 분포군을 포함하는 데이터의 생성 메커니즘에 대해 명시적인 가정을 하지 않습니다. 문헌과 과학 소프트웨어 분야에는 군집화 알고리즘이 넘쳐나며, 이는 위협적으로 느껴질 수 있습니다. 사실 이는 데이터 유형의 다양성과 각 분야에서 추구하는 목표의 다양성과 연결되어 있습니다.

__

태스크

[BiocViews Clustering](http://www.bioconductor.org/packages/release/BiocViews.html) 또는 [CRAN의 Cluster view](https://cran.r-project.org/web/views/Cluster.html)를 찾아보고 군집화 도구를 제공하는 패키지 수를 세어 보세요.

[![](imgs/ClusteringA.png)](imgs/ClusteringA.png "그림 5.4: 우리는 군집화 알고리즘에서 이루어지는 선택들을 취해진 단계에 따라 분해합니다: 관측치-특성 직사각형 테이블 X에서 시작하여, 관측치 간 거리 척도를 선택하고 거리 행렬을 계산합니다(여기서는 삼각형으로 도식화됨). 거리는 클러스터를 구성하는 데 사용됩니다. 왼쪽에는 계층적 군집 트리를 구축하는 응집형(agglomerative) 방법을 도식화했습니다. 오른쪽에는 데이터를 하위 집합으로 분리하는 분할형(partitioning) 방법을 도식화했습니다. 두 유형의 방법 모두 클러스터 수 k의 선택을 필요로 합니다. k-평균과 같은 분할 접근법의 경우 이 선택은 처음에 이루어져야 합니다. 계층적 군집화의 경우 분석이 끝날 때까지 이를 미룰 수 있습니다.")

그림 5.4: 우리는 군집화 알고리즘에서 이루어지는 선택들을 취해진 단계에 따라 분해합니다: 관측치-특성 직사각형 테이블 \\(X\\)에서 시작하여, 관측치 간 거리 척도를 선택하고 거리 행렬을 계산합니다(여기서는 삼각형으로 도식화됨). 거리는 클러스터를 구성하는 데 사용됩니다. 왼쪽에는 계층적 군집 트리를 구축하는 응집형(agglomerative) 방법을 도식화했습니다. 오른쪽에는 데이터를 하위 집합으로 분리하는 분할형(partitioning) 방법을 도식화했습니다. 두 유형의 방법 모두 클러스터 수 \\(k\\)의 선택을 필요로 합니다. \\(k\\)-평균과 같은 분할 접근법의 경우 이 선택은 처음에 이루어져야 합니다. 계층적 군집화의 경우 분석이 끝날 때까지 이를 미룰 수 있습니다.

## 5.3 How do we measure similarity?

[![Of a feather: how the distances are measured and similarities between
observations defined has a strong impact on the clustering
result.](imgs/devil.png)](imgs/devil.png "Of a feather: how the distances are
measured and similarities between observations defined has a strong impact on
the clustering result.")

**Of a feather** : how the distances are measured and similarities between
observations defined has a strong impact on the clustering result.

Our first step is to decide what we mean by _similar_. There are multiple ways
of comparing birds: for instance, a distance using size and weight will give a
different clustering than one using diet or habitat. Once we have chosen the
relevant features, we have to decide how we combine differences between the
multiple features into a single number. Here is a selection of choices, some
of them are illustrated in Figure 5.5.

![Figure 5.5 \(a\):](imgs/FourDistances_a.png)

(a)

![Figure 5.5 \(b\):](imgs/FourDistances_b.png)

(b)

![Figure 5.5 \(c\):](imgs/FourDistances_c.png)

(c)

![Figure 5.5 \(d\):](imgs/FourDistances_d.png)

(d)

Figure 5.5: Equal-distance contour plots according to four different
distances: points on any one curve are all the same distance from the center
point.

**Euclidean** The Euclidean distance between two points \\(A=(a_1,...,a_p)\\)
and \\(B= (b_1,...,b_p)\\) in a \\(p\\)-dimensional space (for the \\(p\\)
features) is the square root of the sum of squares of the differences in all
\\(p\\) coordinate directions:

\\[ d(A,B)=\sqrt{(a_1-b_1)^2+(a_2-b_2)^2+... +(a_p-b_p)^2}. \\]

**Manhattan** The Manhattan, City Block, Taxicab or \\(L_1\\) distance takes
the sum of the absolute differences in all coordinates.

\\[ d(A,B)=|a_1-b_1|+|a_2-b_2|+... +|a_p-b_p|. \\]

**Maximum** The maximum of the absolute differences between coordinates is
also called the \\(L_\infty\\) distance:

\\[ d_\infty(A,B)= \max_{i}|a_i-b_i|. \\]

**Weighted Euclidean distance** is a generalization of the ordinary Euclidean
distance, by giving different directions in feature space different weights.
We have already encountered one example of a weighted Euclidean distance in
[Chapter 2](02-chap.html), the \\(\chi^2\\) distance. It is used to compare
rows in contingency tables, and the weight of each feature is the inverse of
the expected value. The _Mahalanobis_ distance is another weighted Euclidean
distance that takes into account the fact that different features may have a
different dynamic range, and that some features may be positively or
negatively correlated with each other. The weights in this case are derived
from the covariance matrix of the features. See also Question 5.1.

**Minkowski** Allowing the exponent to be \\(m\\) instead of \\(2\\), as in
the Euclidean distance, gives the Minkowski distance

\\[ d(A,B) = \left( (a_1-b_1)^m+(a_2-b_2)^m+... +(a_p-b_p)^m
\right)^\frac{1}{m}. \tag{5.1}\\]

**Edit, Hamming** This distance is the simplest way to compare character
sequences. It simply counts the number of differences between two character
strings. This could be applied to nucleotide or amino acid sequences –
although in that case, the different character substitutions are usually
associated with different contributions to the distance (to account for
physical or evolutionary similarity), and deletions and insertions may also be
allowed.

**Binary** When the two vectors have binary bits as coordinates, we can think
of the non-zero elements as ‘on’ and the zero elements as ‘off’. The binary
distance is the proportion of features having only one bit on amongst those
features that have at least one bit on.

**Jaccard Distance** Occurrence of traits or features in ecological or
mutation data can be translated into presence and absence and encoded as 1’s
and 0’s. In such situations, co-occurence is often more informative than co-
absence. For instance, when comparing mutation patterns in HIV, the co-
existence in two different strains of a mutation tends to be a more important
observation than its co-absence. For this reason, biologists use the **Jaccard
index**. Let’s call our two observation vectors \\(S\\) and \\(T\\),
\\(f_{11}\\) the number of times a feature co-occurs in \\(S\\) and \\(T\\),
\\(f_{10}\\) (and \\(f_{01}\\)) the number of times a feature occurs in
\\(S\\) but not in \\(T\\) (and vice versa), and \\(f_{00}\\) the number of
times a feature is co-absent. The Jaccard index is

\\[ J(S,T) = \frac{f_{11}}{f_{01}+f_{10}+f_{11}}, \tag{5.2}\\]

(i.e., it ignores \\(f_{00}\\)), and the **Jaccard dissimilarity** is

\\[ d_J(S,T) = 1-J(S,T) = \frac{f_{01}+f_{10}}{f_{01}+f_{10}+f_{11}}.
\tag{5.3}\\]

**Correlation based distance**

\\[ d(A,B)=\sqrt{2(1-\text{cor}(A,B))}. \\]

[![](05-chap_files/figure-html/fig-Mahalanobis-1.png)](05-chap_files/figure-
html/fig-Mahalanobis-1.png "Figure 5.6: An example for the use of Mahalanobis
distances to measure the distance of a new data point \(red\) from two cluster
centers.")

Figure 5.6: An example for the use of Mahalanobis distances to measure the
distance of a new data point (red) from two cluster centers.

__

Question 5.1

Which of the two cluster centers in Figure 5.6 is the red point closest to?

__

Solution

__

A naïve answer would use the Euclidean metric and decide that the point is
closer to the left cluster. However, as we see that the features have
different ranges and correlations, and that these even differ between the two
clusters, it makes sense to use cluster-specific Mahalanobis distances. The
figure shows contour lines for both clusters. These were obtained from a
density estimate; the Mahalanobis distance approximates these contours with
ellipses. The distance between the red point and each of the cluster centers
corresponds to the number of contour lines crossed. We see that as the group
on the right is more spread out, the red point is in fact closer to it.

[![](imgs/DistanceTriangle.png)](imgs/DistanceTriangle.png "Figure 5.7: The
lower triangle of distances can be computed by any of a hundred different
functions in various R packages \(vegdist in vegan, daisy in cluster,
genetic_distance in gstudio, dist.dna in ape, Dist in amap, distance in
ecodist, dist.multiPhylo in distory, shortestPath in gdistance, % dudi.dist
and dist.genet in ade4\).")

Figure 5.7: The lower triangle of distances can be computed by any of a
hundred different functions in various R packages (`vegdist` in
**[vegan](https://cran.r-project.org/web/packages/vegan/)** , `daisy` in
**[cluster](https://cran.r-project.org/web/packages/cluster/)** ,
`genetic_distance` in
**[gstudio](https://cran.r-project.org/web/packages/gstudio/)** , `dist.dna`
in **[ape](https://cran.r-project.org/web/packages/ape/)** , `Dist` in
**[amap](https://cran.r-project.org/web/packages/amap/)** , `distance` in
**[ecodist](https://cran.r-project.org/web/packages/ecodist/)** ,
`dist.multiPhylo` in
**[distory](https://cran.r-project.org/web/packages/distory/)** ,
`shortestPath` in
**[gdistance](https://cran.r-project.org/web/packages/gdistance/)** , %
`dudi.dist` and `dist.genet` in
**[ade4](https://cran.r-project.org/web/packages/ade4/)**).

### 5.3.1 Computations related to distances in R

The `dist` function in R is designed to use less space than the full \\(n^2\\)
positions a complete \\(n \times n\\) distance matrix between \\(n\\) objects
would require. The function computes one of six choices of distance
(`euclidean`, `maximum`, `manhattan`, `canberra`, `binary`, `minkowski`) and
outputs a vector of values sufficient to reconstruct the complete distance
matrix. The function returns a special object of class `dist` that encodes the
relevant vector of size \\(n\times(n-1)/2\\). Here is the output for a \\(3\\)
by \\(3\\) matrix:

    
    
    mx  = c(0, 0, 0, 1, 1, 1)
    my  = c(1, 0, 1, 1, 0, 1)
    mz  = c(1, 1, 1, 0, 1, 1)
    mat = rbind(mx, my, mz)
    dist(mat)__
    
    
             mx       my
    my 1.732051         
    mz 2.000000 1.732051
    
    
    dist(mat, method = "binary")__
    
    
              mx        my
    my 0.6000000          
    mz 0.6666667 0.5000000

In order to access a particular distance (for example the distance between
observations 1 and 2), one has to turn the `dist` class object back into a
matrix.

    
    
    load("../data/Morder.RData")
    sqrt(sum((Morder[1, ] - Morder[2, ])^2))__
    
    
    [1] 5.593667
    
    
    as.matrix(dist(Morder))[2, 1]__
    
    
    [1] 5.593667

Let’s look at how we would compute the Jaccard distance we defined above
between HIV strains.

    
    
    mut = read.csv("../data/HIVmutations.csv")
    mut[1:3, 10:16]__
    
    
      p32I p33F p34Q p35G p43T p46I p46L
    1    0    1    0    0    0    0    0
    2    0    1    0    0    0    1    0
    3    0    1    0    0    0    0    0

__

Question 5.2

Compare the Jaccard distance (available as the function `vegdist` in the R
package **[vegan](https://cran.r-project.org/web/packages/vegan/)**) between
mutations in the HIV data `mut` to the correlation based distance.

__

Solution

__

    
    
    library("vegan")
    mutJ = vegdist(mut, "jaccard")
    mutC = sqrt(2 * (1 - cor(t(mut))))
    mutJ __
    
    
          1     2     3     4
    2 0.800                  
    3 0.750 0.889            
    4 0.900 0.778 0.846      
    5 1.000 0.800 0.889 0.900
    
    
    as.dist(mutC)__
    
    
         1    2    3    4
    2 1.19               
    3 1.10 1.30          
    4 1.32 1.13 1.30     
    5 1.45 1.19 1.30 1.32

[![](imgs/birds_and_dinosaurs.png)](imgs/birds_and_dinosaurs.png "Figure 5.8:
An example of computing the cophenetic distance \(xkcd\).")

Figure 5.8: An example of computing the cophenetic distance (xkcd).

It can also be interesting to compare complex objects that are not traditional
vectors or real numbers using dissimilarities or distances. Gower’s distance
for data of mixed modalities (both categorical factors and continuous
variables) can be computed with the `daisy` function. In fact, distances can
be defined between any pairs of objects, not just points in \\({\mathbb
R}^p\\) or character sequences. For instance, the `shortest.paths` function
from the **[igraph](https://cran.r-project.org/web/packages/igraph/)** package
that we will see in [Chapter 10](10-chap.html) computes the distance between
vertices on a graph and the function `cophenetic` computes the distance
between leaves of a tree as illustrated in Figure 5.8. We can compute the
distance between trees using `dist.multiPhylo` in the
**[distory](https://cran.r-project.org/web/packages/distory/)** package.

The Jaccard index between graphs can be computed by looking at two graphs
built on the same nodes and counting the number of co-occurring edges. This is
implemented in the function `similarity` in the
**[igraph](https://cran.r-project.org/web/packages/igraph/)** package.
Distances and dissimilarities are also used to compare images, sounds, maps
and documents. A distance can usefully encompass domain knowledge and, if
carefully chosen, can lead to the solution of many hard problems involving
heterogeneous data. Asking yourself what is the _relevant_ notion of
“closeness” or similarity for your data can provide useful ways of
representing them, as we will explore in [Chapter 9](09-chap.html).

## 5.4 Nonparametric mixture detection

### 5.4.1 \\(k\\)-methods: \\(k\\)-means, \\(k\\)-medoids and PAM

[![The centers of the groups are sometimes called medoids, thus the name PAM
\(partitioning around medoids\).](imgs/devil.png)](imgs/devil.png "The centers
of the groups are sometimes called medoids, thus the name PAM \(partitioning
around medoids\).")

The centers of the groups are sometimes called medoids, thus the name PAM
(partitioning around medoids).

Partitioning or iterative relocation methods work well in high-dimensional
settings, where we cannot4 easily use probability densities, the EM algorithm
and parametric mixture modeling in the way we did in [Chapter
4](04-chap.html). Besides the distance measure, the main choice to be made is
the number of clusters \\(k\\). The PAM (partitioning around medoids, Kaufman
and Rousseeuw ([2009](16-chap.html#ref-Kaufman2009))) method is as follows:

4 This is due to the so-called curse of dimensionality. We will discuss this
in more detail in [Chapter 12](12-chap.html).

  1. Starts from a matrix of \\(p\\) features measured on a set of \\(n\\) observations.

  2. Randomly pick \\(k\\) distinct _cluster centers_ out of the \\(n\\) observations (“seeds”).

  3. Assign each of the remaining observation to the group to whose center it is the closest.

  4. For each group, choose a new center from the observations in the group, such that the sum of the distances of group members to the center is minimal; this is called the _medoid_.

  5. Repeat Steps 3 and 4 until the groups stabilize.

Each time the algorithm is run, different initial seeds will be picked in Step
2, and in general, this can lead to different final results. A popular
implementation is the `pam` function in the package
**[cluster](https://cran.r-project.org/web/packages/cluster/)**.

A slight variation of the method replaces the medoids by the arithmetic means
(centers of gravity) of the clusters and is called \\(k\\)-means. While in
PAM, the centers are observations, this is not, in general, the case with
\\(k\\)-means. The function `kmeans` comes with every installation of R in the
**[stats](https://cran.r-project.org/web/packages/stats/)** package; an
example run is shown in Figure 5.9.

[![](05-chap_files/figure-html/fig-clust-
kmeansastep1-1.png)](05-chap_files/figure-html/fig-clust-kmeansastep1-1.png
"Figure 5.9 \(a\): ")

(a)

[![](05-chap_files/figure-html/fig-clust-
kmeansastep1-2.png)](05-chap_files/figure-html/fig-clust-kmeansastep1-2.png
"Figure 5.9 \(b\): ")

(b)

[![](05-chap_files/figure-html/fig-clust-
kmeansastep1-3.png)](05-chap_files/figure-html/fig-clust-kmeansastep1-3.png
"Figure 5.9 \(c\): ")

(c)

Figure 5.9: An example run of the \\(k\\)-means algorithm. The initial,
randomly chosen centers (black circles) and groups (colors) are shown in (a).
The group memberships are assigned based on their distance to centers. At each
iteration (b) and (c), the group centers are redefined, and the points
reassigned to the cluster centers.

These so-called \\(k\\)-methods are the most common off-the-shelf methods for
clustering; they work particularly well when the clusters are of comparable
size and convex (blob-shaped). On the other hand, if the true clusters are
very different in size, the larger ones will tend to be broken up; the same is
true for groups that have pronounced non-spherical or non-elliptic shapes.

__

Question 5.3

The \\(k\\)-means algorithm alternates between computing the average point and
assigning the points to clusters. How does this alternating, iterative method
differ from an EM-algorithm?

__

Solution

__

In the EM algorithm, each point participates in the computation of the mean of
all the groups through a probabilistic weight assigned to it. In the
\\(k\\)-means method, the points are either attributed to a cluster or not, so
each point participates only, and entirely, in the computation of the center
of one cluster.

### 5.4.2 Tight clusters with resampling

There are clever schemes that repeat the process many times using different
initial centers or resampled datasets. Repeating a clustering procedure
multiple times on the same data, but with different starting points creates
_strong forms_ according to Diday and Brito ([1989](16-chap.html#ref-
Diday1989)). Repeated subsampling of the dataset and applying a clustering
method will result in groups of observations that are “almost always” grouped
together; these are called _tight clusters_ ([Tseng and Wong
2005](16-chap.html#ref-Tseng:2005)). The study of strong forms or tight
clusters facilitates the choice of the number of clusters. A recent package
developed to combine and compare the output from many different clusterings is
**[clusterExperiment](https://bioconductor.org/packages/clusterExperiment/)**.
Here we give an example from its vignette. Single-cell RNA-Seq experiments
provide counts of reads, representing gene transcripts, from individual cells.
The single cell resolution enables scientists, among other things, to follow
cell lineage dynamics. Clustering has proved very useful for analysing such
data.

__

Question 5.4

Follow the vignette of the package
**[clusterExperiment](https://bioconductor.org/packages/clusterExperiment/)**.
Call the ensemble clustering function `clusterMany`, using `pam` for the
individual clustering efforts. Set the choice of genes to include at either
the 60, 100 or 150 most variable genes. Plot the clustering results for
\\(k\\) varying between 4 and 9. What do you notice?

__

Solution

__

The following code produces Figure 5.10.

    
    
    library("clusterExperiment")
    fluidigm = scRNAseq::ReprocessedFluidigmData()
    se = fluidigm[, fluidigm$Coverage_Type == "High"]
    assays(se) = list(normalized_counts = as.matrix(limma::normalizeQuantiles(assay(se))))
    ce = clusterMany(se, clusterFunction = "pam", ks = c(5, 7, 9), run = TRUE,
                     isCount = TRUE, reduceMethod = "var", nFilterDims = c(60, 100, 150))__
    
    
    9 parameter combinations, 0 use sequential method, 0 use subsampling method
    Running Clustering on Parameter Combinations...
    done.
    
    
    clusterLabels(ce) = sub("FilterDims", "", clusterLabels(ce))
    plotClusters(ce, whichClusters = "workflow", axisLine = -1)__

[![](05-chap_files/figure-html/fig-quiltclust-1-1.png)](05-chap_files/figure-
html/fig-quiltclust-1-1.png "Figure 5.10: Comparison of clustering results
\(rows\), for different numbers of included genes and for varying numbers of
clusters, k. Each column of the heatmap corresponds to a cell, and the colors
represent the cluster assignments.")

Figure 5.10: Comparison of clustering results (rows), for different numbers of
included genes and for varying numbers of clusters, \\(k\\). Each column of
the heatmap corresponds to a cell, and the colors represent the cluster
assignments.

## 5.5 군집화 예시: 유세포 분석 및 질량 분석

[![유세포 분석을 위한 생물정보학적 방법들에 대한 리뷰는 (O’Neill et al. 2013)와 잘 관리된 위키백과 문서에서 찾을 수 있습니다.](imgs/book_icon.png)](imgs/book_icon.png "유세포 분석을 위한 생물정보학적 방법들에 대한 리뷰는 \[@oneill2013flow\]와 잘 관리된 위키백과 문서에서 찾을 수 있습니다.")

유세포 분석을 위한 생물정보학적 방법들에 대한 리뷰는 ([O’Neill et al. 2013](16-chap.html#ref-oneill2013flow))와 [잘 관리된 위키백과 문서](https://en.wikipedia.org/wiki/Flow_cytometry_bioinformatics)에서 찾을 수 있습니다.

단일 세포에 대한 측정값을 연구하면 세포 유형과 역학을 분석할 수 있는 초점과 해상도가 모두 향상됩니다. 유세포 분석(flow cytometry)은 약 10가지의 서로 다른 세포 마커를 동시에 측정할 수 있게 해줍니다. 질량 분석(mass cytometry)은 측정 컬렉션을 세포당 최대 80개의 단백질로 확장합니다. 이 기술의 특히 유망한 응용 분야는 면역 세포 역학 연구입니다.

### 5.5.1 유세포 분석 및 질량 분석

발달의 서로 다른 단계에서 면역 세포는 표면에 고유한 단백질 조합을 발현합니다. 이러한 단백질 마커는 **CD** (**clusters of differentiation**, 분화 클러스터)라고 불리며 유세포 분석(형광 사용, Hulett et al. ([1969](16-chap.html#ref-flowsort)) 참조) 또는 질량 분석(중원소 리포터의 단일 세포 원자 질량 분석법 사용, Bendall et al. ([2012](16-chap.html#ref-BendallCell)) 참조)에 의해 수집됩니다. 흔히 사용되는 CD의 예로 CD4가 있는데, 이 단백질은 "CD4+"라고 불리는 보조 T 세포(helper T cells)에 의해 발현됩니다. 그러나 일부 세포는 CD4를 발현하지만(따라서 CD4+임), 실제로는 보조 T 세포가 아니라는 점에 유의하세요. 먼저 세포 분석(cytometry) 데이터를 위한 유용한 Bioconductor 패키지인 **[flowCore](https://bioconductor.org/packages/flowCore/)**와 **[flowViz](https://bioconductor.org/packages/flowViz/)**를 불러오고, 다음과 같이 예시 데이터 객체 `fcsB`를 읽어들입니다:

    
    
    library("flowCore")
    library("flowViz")
    fcsB = read.FCS("../data/Bendall_2011.fcs", truncate_max_range = FALSE)
    slotNames(fcsB)__
    
    
    [1] "exprs"       "parameters"  "description"

그림 5.11은 `fcsB` 데이터에서 사용 가능한 두 변수의 산점도를 보여줍니다. (이러한 플롯을 만드는 방법은 아래에서 살펴보겠습니다.) 이 두 차원에서 명확한 이봉성과 군집화를 볼 수 있습니다.

__

질문 5.5

  1. `fcsB` 객체의 구조를 살펴보세요(힌트: `colnames` 함수). 얼마나 많은 변수가 측정되었나요?  

  2. 처음 몇 행을 보기 위해 데이터를 하위 집합화해 보세요(힌트: `Biobase::exprs(fcsB)` 사용). 얼마나 많은 세포가 측정되었나요?

### 5.5.2 데이터 전처리

먼저 동위원소(isotopes)와 마커(항체) 사이의 매핑을 보고하는 테이블 데이터를 불러온 다음, `fcsB`의 열 이름에 있는 동위원소 이름을 마커 이름으로 바꿉니다. 이렇게 하면 후속 분석 및 플로팅 코드가 더 읽기 쉬워집니다:

    
    
    markersB = readr::read_csv("../data/Bendall_2011_markers.csv")
    mt = match(markersB$isotope, colnames(fcsB))
    stopifnot(!any(is.na(mt)))
    colnames(fcsB)[mt] = markersB$marker __

이제 그림 5.11을 생성할 준비가 되었습니다.

    
    
    flowPlot(fcsB, plotParameters = colnames(fcsB)[2:3], logy = TRUE)__

[![](05-chap_files/figure-html/fig-
ObviousClusters-1.png)](05-chap_files/figure-html/fig-ObviousClusters-1.png
"Figure 5.11: Cell measurements that show clear clustering in two
dimensions.")

Figure 5.11: Cell measurements that show clear clustering in two dimensions.

Plotting the data in two dimensions as in Figure 5.11 already shows that the
cells can be grouped into subpopulations. Sometimes just one of the markers
can be used to define populations on their own; in that case simple
**rectangular gating** is used to separate the populations; for instance, CD4+
cells can be gated by taking the subpopulation with high values for the CD4
marker. Cell clustering can be improved by carefully choosing transformations
of the data. The left part of Figure 5.12 shows a simple one dimensional
histogram before transformation; on the right of Figure 5.12 we see the
distribution after transformation. It reveals a bimodality and the existence
of two cell populations.

**Data Transformation: hyperbolic arcsin (asinh)**. It is standard to
transform both flow and mass cytometry data using one of several special
functions. We take the example of the inverse hyperbolic sine (asinh):

\\[ \operatorname{asinh}(x) = \log{(x + \sqrt{x^2 + 1})}. \\]

From this we can see that for large values of \\(x\\),
\\(\operatorname{asinh}(x)\\)는 로그 함수처럼 행동하며 실제로는 \\(\log(x)+\log(2)\\)와 거의 같습니다. \(x\)가 작을 때 이 함수는 \(x\)에 대해 거의 선형적입니다.

__

태스크

변환의 두 가지 주요 영역인 작은 값과 큰 값을 확인하기 위해 다음 코드를 실행해 보세요.

    
    
    v1 = seq(0, 1, length.out = 100)
    plot(log(v1), asinh(v1), type = 'l')__
    
    
     plot(v1, asinh(v1), type = 'l')__
    
    
    v3 = seq(30, 3000, length = 100)
    plot(log(v3), asinh(v3), type= 'l')__

이것은 [4장](04-chap.html)과 [8장](08-chap.html)에서도 언급된 분산 안정화 변환의 또 다른 예입니다. 그림 5.12는 **[flowCore](https://bioconductor.org/packages/flowCore/)** 패키지의 `arcsinhTransform` 함수를 사용하는 다음 코드로 생성되었습니다.

    
    
    asinhtrsf = arcsinhTransform(a = 0.1, b = 1)
    fcsBT = transform(fcsB, transformList(colnames(fcsB)[-c(1, 2, 41)], asinhtrsf))
    densityplot(~`CD3all`, fcsB)
    densityplot(~`CD3all`, fcsBT)__

[![](05-chap_files/figure-html/fig-
plotTransformations-1.png)](05-chap_files/figure-html/fig-
plotTransformations-1.png "그림 5.12 (a): ")

(a)

[![](05-chap_files/figure-html/fig-
plotTransformations-2.png)](05-chap_files/figure-html/fig-
plotTransformations-2.png "그림 5.12 (b): ")

(b)

그림 5.12: 패널 (a)는 CD3all 변수의 히스토그램을 보여줍니다: 세포들이 0 근처에 군집해 있고 몇 개의 큰 값들이 있습니다. (b)에서는 asinh 변환 후 세포들이 군집을 이루어 두 그룹 또는 유형으로 나뉘는 것을 볼 수 있습니다.

__

질문 5.6

다음 코드는 \\(k\\)-평균을 사용하여 데이터를 2개의 그룹으로 나누기 위해 몇 개의 차원을 사용하나요?

    
    
    kf = kmeansFilter("CD3all" = c("Pop1","Pop2"), filterId="myKmFilter")
    fres = flowCore::filter(fcsBT, kf)
    summary(fres)__
    
    
    Pop1: 33434 of 91392 events (36.58%)
    Pop2: 57958 of 91392 events (63.42%)
    
    
    fcsBT1 = flowCore::split(fcsBT, fres, population = "Pop1")
    fcsBT2 = flowCore::split(fcsBT, fres, population = "Pop2")__

다음 코드로 생성된 그림 5.13은 CD3와 CD56 마커에 의해 확장된 두 차원으로 데이터를 단순 투영한 것을 보여줍니다:

    
    
    library("flowPeaks")
    fp = flowPeaks(Biobase::exprs(fcsBT)[, c("CD3all", "CD56")])
    plot(fp)__

[![](05-chap_files/figure-html/fig-flowCD3CD56-1-1.png)](05-chap_files/figure-
html/fig-flowCD3CD56-1-1.png "그림 5.13: 변환 후 이 세포들은 kmeans를 사용하여 군집화되었습니다.")

그림 5.13: 변환 후 이 세포들은 `kmeans`를 사용하여 군집화되었습니다.

어느 영역에 밀집된 점들을 플롯할 때는 겹쳐그리기(overplotting)를 피해야 합니다. [3장](03-chap.html)에서 선호되는 기법들 중 일부를 보았습니다. 여기서는 등고선(contours)과 음영(shading)을 사용합니다. 다음과 같이 수행합니다:

    
    
    flowPlot(fcsBT, plotParameters = c("CD3all", "CD56"), logy = FALSE)
    contour(fcsBT[, c(40, 19)], add = TRUE)__

[![](05-chap_files/figure-html/fig-
groupcontourCD3CD56-1.png)](05-chap_files/figure-html/fig-
groupcontourCD3CD56-1.png "그림 5.14: 그림 5.13과 유사하지만 등고선을 사용했습니다.")

그림 5.14: 그림 5.13과 유사하지만 등고선을 사용했습니다.

이 코드는 그림 5.13보다 더 많은 정보를 담고 있는 그림 5.14를 생성합니다.

__

태스크

Bioconductor 패키지 **[ggcyto](https://bioconductor.org/packages/ggcyto/)**는 `ggplot`을 사용하여 각 환자의 데이터를 서로 다른 패싯(facet)에 그릴 수 있게 해줍니다. 다음과 같은 방식으로 이 접근법을 사용한 출력과 위에서 수행한 작업을 비교해 보세요:

    
    
    library("ggcyto")
    library("labeling")
    
    p1 = ggcyto(fcsB, aes(x = CD4)) + geom_histogram(bins = 60)
    p2 = ggcyto(fcsB, aes(x = CD8)) + geom_histogram(bins = 60)
    p3 = ggcyto(fcsB, aes(x = CD4, y = CD8)) + geom_density2d(colour = "black")
    
    fcsBT = transform(fcsB, transformList(colnames(fcsB)[-c(1, 2, 41)], 
                                          arcsinhTransform(a = 0, b = 1)))
                                          
    p1t = ggcyto(fcsBT, aes(x = CD4))            + geom_histogram(bins = 90)
    p2t = ggcyto(fcsBT, aes(x = CD4,y = CD8))    + geom_density2d(colour = "black")
    p3t = ggcyto(fcsBT, aes(x = CD45RA,y = CD20))+ geom_density2d(colour = "black")__

### 5.5.3 밀도 기반 군집화(Density-based clustering)

마커 수가 적고 세포 수가 많은 유세포 분석과 같은 데이터 세트는 밀도 기반 군집화에 적합합니다. 이 방법은 희소한 영역에 의해 분리된 고밀도 영역을 찾습니다. 이 방법은 클러스터가 반드시 볼록할 필요가 없는 경우에도 대처할 수 있다는 장점이 있습니다. 이러한 방법의 한 구현체로 dbscan이 있습니다. 다음 코드를 실행하여 예시를 살펴보겠습니다.

    
    
    library("dbscan")
    mc5 = Biobase::exprs(fcsBT)[, c(15,16,19,40,33)]
    res5 = dbscan::dbscan(mc5, eps = 0.65, minPts = 30)
    mc5df = data.frame(mc5, cluster = as.factor(res5$cluster))
    table(mc5df$cluster)__
    
    
        0     1     2     3     4     5     6     7     8 
    76053  4031  5450  5310   257   160    63    25    43 
    
    
    ggplot(mc5df, aes(x=CD4,    y=CD8,  col=cluster))+geom_density2d()
    ggplot(mc5df, aes(x=CD3all, y=CD20, col=cluster))+geom_density2d()__

[![](05-chap_files/figure-html/fig-dbscanfcs5-1.png)](05-chap_files/figure-
html/fig-dbscanfcs5-1.png "그림 5.15 (a): ")

(a)

[![](05-chap_files/figure-html/fig-dbscanfcs5-2.png)](05-chap_files/figure-
html/fig-dbscanfcs5-2.png "그림 5.15 (b): ")

(b)

그림 5.15: 이 두 플롯은 5개의 마커를 사용하여 `dbscan`으로 군집화한 결과를 보여줍니다. 여기서는 데이터를 CD4-CD8 및 C3all-CD20 평면에 투영한 것만 보여줍니다.

출력 결과는 그림 5.15에 나와 있습니다. 2D 투영에서의 클러스터 중첩을 통해 군집화의 다차원적 특성을 이해할 수 있습니다.

__

질문 5.7

입력 데이터에서 CD 마커 변수 하나를 추가하여 차원을 6으로 늘려보세요.  
그런 다음 `eps`를 변화시키면서, 적어도 두 개가 100개 이상의 점을 가진 4개의 클러스터를 찾아보세요.  
7개의 CD 마커 변수로 이 작업을 반복해 보세요. 무엇을 알 수 있나요?

__

해결책

__

다음 6개 마커를 사용한 예시입니다.

    
    
    mc6 = Biobase::exprs(fcsBT)[, c(15, 16, 19, 33, 25, 40)]
    res = dbscan::dbscan(mc6, eps = 0.65, minPts = 20)
    mc6df = data.frame(mc6, cluster = as.factor(res$cluster))
    table(mc6df$cluster)__
    
    
        0     1     2     3     4     5     6 
    91068    34    61    20    67   121    21 

우리는 eps=0.75일 때 eps=0.65일 때보다 충분히 큰 클러스터를 찾기가 더 쉽다는 것을 알 수 있으며, eps=0.55일 때는 불가능합니다. 차원수를 7로 늘리면 eps를 훨씬 더 크게 만들어야 합니다.

    
    
    mc7 = Biobase::exprs(fcsBT)[, c(11, 15, 16, 19, 25, 33, 40)]
    res = dbscan::dbscan(mc7, eps = 0.95, minPts = 20)
    mc7df = data.frame(mc7, cluster = as.factor(res$cluster))
    table(mc7df$cluster)__
    
    
        0     1     2     3     4     5     6     7     8     9    10 
    90249    21   102   445   158   119    19   224    17    20    18 

이는 소위 **차원의 저주(curse of dimensionality)**가 실제로 작동하는 것을 보여주며, 이에 대해서는 [12장](12-chap.html)에서 더 자세히 다룹니다.

#### 밀도 기반 군집화(dbscan)는 어떻게 작동하나요?

dbscan 방법은 _밀도 연결성(density-connectedness)_ 기준에 따라 고밀도 영역의 점들을 군집화합니다. 이 방법은 점들이 연결되어 있는지 확인하기 위해 반지름 \\(\epsilon\\)인 작은 이웃 구(neighborhood spheres)를 살펴봅니다.

dbscan의 기본 구성 요소는 밀도 도달 가능성(density-reachability) 개념입니다: 점 \\(q\\)가 점 \\(p\\)로부터 주어진 임계값 \\(\epsilon\\)보다 멀리 있지 않고, \\(p\\)가 충분히 많은 점들에 둘러싸여 있어 \\(p\\)(및 \\(q\\))를 밀집 영역의 일부로 간주할 수 있다면, 점 \\(q\\)는 점 \\(p\\)로부터 직접 **밀도 도달 가능(density-reachable)**합니다. \\(p_1 = p\\)이고 \\(p_n = q\\)인 일련의 점 \\(p_1, ..., p_n\\)이 있어서 각 \\(p_{i+1}\\)이 \\(p_i\\)로부터 직접 밀도 도달 가능하다면, \\(q\\)는 \\(p\\)로부터 _밀도 도달 가능_하다고 합니다.

[![방법이 이웃에서 높은 점 밀도를 찾는 것이 중요합니다. 클러스터 사이의 공백(void) 또는 "누락된 점"에 의해 클러스터를 정의하려는 다른 방법들이 존재합니다. 그러나 이러한 방법들은 차원의 저주에 취약하며, 가짜 "공백"을 만들어낼 수 있습니다.](imgs/devil.png)](imgs/devil.png "방법이 이웃에서 높은 점 밀도를 찾는 것이 중요합니다. 클러스터 사이의 공백(void) 또는 \"누락된 점\"에 의해 클러스터를 정의하려는 다른 방법들이 존재합니다. 그러나 이러한 방법들은 차원의 저주에 취약하며, 가짜 \"공백\"을 만들어낼 수 있습니다.")

방법이 이웃에서 높은 점 밀도를 찾는 것이 중요합니다. 클러스터 사이의 공백(void) 또는 "누락된 점"에 의해 클러스터를 정의하려는 다른 방법들이 존재합니다. 그러나 이러한 방법들은 차원의 저주에 취약하며, 가짜 "공백"을 만들어낼 수 있습니다.

그러면 _클러스터_는 다음 속성들을 만족하는 점들의 하위 집합입니다:

  1. 클러스터 내의 모든 점은 서로 밀도 연결되어 있습니다.

  2. 어떤 점이 클러스터의 임의의 점에 밀도 연결되어 있다면, 그 점 역시 클러스터의 일부입니다.

  3. 점들의 그룹이 클러스터로 간주되려면 적어도 `MinPts`개의 점을 가져야 합니다.

## 5.6 계층적 군집화(Hierarchical clustering)

[![](imgs/LinnaeusClass-01.png)](imgs/LinnaeusClass-01.png "그림 5.16: 특성 유사성에 따라 유기체들을 군집화한 린네의 분류 체계의 일부입니다.")

그림 5.16: 특성 유사성에 따라 유기체들을 군집화한 린네의 분류 체계의 일부.

계층적 군집화는 유사한 관측치와 하위 클래스를 반복적으로 조립하는 상향식(bottom-up) 접근 방식입니다. 그림 5.16은 린네가 특정 특성에 따라 유기체들의 중첩된 클러스터를 어떻게 만들었는지 보여줍니다. 이러한 계층적 조직은 많은 분야에서 유용하게 사용되어 왔으며, _자연의 사다리(ladder of nature)_를 상정한 아리스토텔레스까지 거슬러 올라갑니다.

**덴드로그램 순서(Dendrogram ordering)**. 그림 5.17의 예에서 볼 수 있듯이, 레이블의 순서는 형제 쌍(sibling pairs) 내에서는 중요하지 않습니다. 수평 거리는 대개 무의미한 반면, 수직 거리는 어떤 정보를 인코딩합니다. 이러한 속성들은 계통발생학적(monophyletic)이지 않지만(즉, 동일한 서브트리나 클레이드(clade)에 속하지 않음) 플롯에서는 이웃으로 나타나는 대상(예를 들어 오른쪽 나무의 B와 D)에 대해 해석을 내릴 때 기억해야 할 중요한 사항입니다.

[![](imgs/SameTree-01.png)](imgs/SameTree-01.png "그림 5.17: 동일한 계층적 군집 트리의 세 가지 표현 방식.")

그림 5.17: **동일한** 계층적 군집 트리의 세 가지 표현 방식.

**하향식 계층 구조(Top-down hierarchies)**. 대안적인 하향식 접근 방식은 모든 객체를 가져와서 선택된 기준에 따라 순차적으로 분할합니다. 이러한 소위 **재귀적 분할(recursive partitioning)** 방법은 종종 의사 결정 나무(decision trees)를 만드는 데 사용됩니다. 이들은 예측(예를 들어 의료 진단이 주어졌을 때의 생존 기간)에 유용할 수 있습니다: 우리는 그러한 사례들에서 분할을 통해 불균질한 모집단을 더 균질한 하위 그룹으로 나누기를 희망합니다. 이 장에서 우리는 상향식 접근 방식에 집중합니다. [12장](12-chap.html)에서 비지도 학습과 분류에 대해 이야기할 때 분할 방식으로 다시 돌아올 것입니다.

### 5.6.1 집계된 클러스터 간의 (비)유사성을 어떻게 계산하는가?

[![](imgs/ClusterStepChoiceSingle1b.png)](imgs/ClusterStepChoiceSingle1b.png
"Figure 5.18: In the single linkage method, the distance between groups C_1
and C_2 is defined as the distance between the closest two points from the
groups.")

Figure 5.18: In the single linkage method, the distance between groups
\\(C_1\\) and \\(C_2\\) is defined as the distance between the closest two
points from the groups.

A hierarchical clustering algorithm, which works by aggregation, is easy
enough to get started, by grouping the most similar observations together. But
we will need more than just the distances between all pairs of individual
objects. Once an aggregation is made, one is required to say how the distance
between the newly formed cluster and all other points (or existing clusters)
is computed. There are different choices, all based on the object-object
distances, and each choice results in a different type of hierarchical
clustering.

The **minimal jump** method, also called **single linkage** or nearest
neighbor method computes the distance between clusters as the smallest
distance between any two points in the two clusters (as shown in Figure 5.18):

\\[ d_{12} = \min_{i \in C_1, j \in C_2 } d_{ij}. \\]

This method tends to create clusters that look like contiguous strings of
points. The cluster tree often looks like a comb.

[![](imgs/ClusterStepChoiceComplete1b.png)](imgs/ClusterStepChoiceComplete1b.png
"Figure 5.19: In the complete linkage method, the distance between groups C_1
and C_2 is defined as the maximum distance between pairs of points from the
two groups.")

Figure 5.19: In the complete linkage method, the distance between groups
\\(C_1\\) and \\(C_2\\) is defined as the maximum distance between pairs of
points from the two groups.

The **maximum jump** (or **complete linkage**) method defines the distance
between clusters as the largest distance between any two objects in the two
clusters, as represented in Figure 5.19:

\\[ d_{12} = \max_{i \in C_1, j \in C_2 } d_{ij}. \\]

The **average linkage** method is half way between the two above (here,
\\(|C_k|\\) denotes the number of elements of cluster \\(k\\)):

\\[ d_{12} = \frac{1}{|C_1| |C_2|}\sum_{i \in C_1, j \in C_2 } d_{ij} \\]

[![](imgs/BetweenWithinb.png)](imgs/BetweenWithinb.png "Figure 5.20: The Ward
method maximizes the between group sum of squares \(red edges\), while
minimizing the sums of squares within groups \(black edges\).")

Figure 5.20: The Ward method maximizes the between group sum of squares (red
edges), while minimizing the sums of squares within groups (black edges).

**Ward’s method** takes an analysis of variance approach, where the goal is to
minimize the variance within clusters. This method is very efficient, however,
it tends to create clusters of smaller sizes.

Advantages and disadvantages of various choices of defining distances between aggregates ([Chakerian and Holmes 2012](16-chap.html#ref-distory-paper)). Method | Pros | Cons  
---|---|---  
Single linkage | number of clusters | comblike trees  
Complete linkage | compact classes | one observation can alter groups  
Average linkage | similar size and variance | not robust  
Centroid | robust to outliers | smaller number of clusters  
Ward | minimising an inertia | classes small if high variability  
  
[![](imgs/CalderHand.png)](imgs/CalderHand.png "Figure 5.21: Hierarchical
clustering output has similar properties to a mobile: the branches can rotate
freely from their suspension points.")

Figure 5.21: Hierarchical clustering output has similar properties to a
mobile: the branches can rotate freely from their suspension points.

These are the choices we have to make building hierarchical clustering trees.
An advantage of hierarchical clustering compared to the partitioning methods
좋은 점은 그룹화의 강도를 그래픽으로 진단할 수 있다는 것입니다: 트리의 내부 에지(inner edges)의 길이입니다.

클러스터들의 크기가 거의 같다는 사전 지식이 있다면, 그룹 내 분산을 최소화하는 평균 연결법(average linkage)이나 와드 방법(Ward’s method)을 사용하는 것이 가장 좋은 전술입니다.

__

질문 5.8

**세포군에 대한 계층적 군집화** `Morder` 데이터는 10명의 환자로부터 얻은 3가지 유형(naïve, effector, memory)의 T 세포에 대한 156개 유전자의 발현 측정값입니다([Holmes et al. 2005](16-chap.html#ref-holmes2005memory)). **[pheatmap](https://cran.r-project.org/web/packages/pheatmap/)** 패키지를 사용하여, 이 데이터의 유클리드 거리와 맨해튼 거리에 대해 덴드로그램이나 재정렬 없이 두 개의 단순한 히트맵을 만드세요.

__

질문 5.9

이제 이 두 거리를 사용한 계층적 군집 트리에서의 순서 차이를 살펴보세요. 어떤 차이점이 눈에 띄나요?

![](imgs/single14heatmap.png):
")

(a)

![](imgs/average14heatmap.png):
")

(b)

[![](imgs/complete14heatmap.png)](imgs/complete14heatmap.png
"그림 5.22 (c): ")

(c)

그림 5.22: 서로 다른 응집(agglomeration) 선택으로 만들어진 세 개의 계층적 군집 플롯. (a)의 단일 연결법(single linkage)에 대한 빗 모양 구조에 주목하세요. 평균 연결법 (b)와 완전 연결법 (c) 트리는 내부 분기의 길이에 의해서만 다릅니다.

[![](imgs/apeclust14.png)](imgs/apeclust14.png "그림 5.23: 이 트리는 여러 가지 다른 방식으로 그려질 수 있습니다. 여기에 나타난 잎들의 순서는 (8,11,9,10,7,5,6,1,4,2,3)입니다.")

그림 5.23: 이 트리는 여러 가지 다른 방식으로 그려질 수 있습니다. 여기에 나타난 잎들의 순서는 \\((8,11,9,10,7,5,6,1,4,2,3)\\)입니다.

__

질문 5.10

계층적 군집 트리는 그림 5.21의 콜더 모빌(Calder mobile)과 같아서 많은 내부 피벗 점들을 중심으로 회전할 수 있으며, 주어진 트리와 일치하는 많은 팁(tips) 순서를 제공합니다. 그림 5.23의 트리를 보세요. 이 트리와 일관성을 유지하면서 팁 레이블을 정렬할 수 있는 방법은 몇 가지가 있을까요?

행 및/또는 열이 계층적 군집 트리에 기반하여 정렬된 히트맵을 흔히 볼 수 있습니다. 때때로 이것은 일부 클러스터를 매우 강력해 보이게 만듭니다 – 트리가 실제로 암시하는 것보다 더 강력하게 말이죠. 히트맵에서 행과 열을 정렬하는 대안적인 방법들이 있는데, 예를 들어 서열화 방법(ordination methods)5을 사용하여 순서를 찾는 **[NeatMap](https://cran.r-project.org/web/packages/NeatMap/)** 패키지가 있습니다.

5 이들은 [9장](09-chap.html)에서 설명될 것입니다.

## 5.7 클러스터 수 검증 및 선택

우리가 설명한 군집화 방법들은 다양한 제약 조건 하에서 데이터의 좋은 그룹화를 제공하도록 맞춤화되어 있습니다. 그러나 군집화 방법은 클러스터가 없더라도 항상 그룹을 제공한다는 점을 명심하세요. 만약 데이터에 실제 클러스터가 없다면, 계층적 군집 트리는 상대적으로 짧은 내부 분기를 보여줄 수 있지만, 이를 정량화하기는 어렵습니다. 일반적으로 보다 객관적인 기준으로 클러스터 선택을 검증하는 것이 중요합니다.

군집화 결과의 품질을 평가하는 한 가지 기준은 그룹 내 거리를 작게 유지하면서 그룹 간 차이를 어느 정도까지 최대화하느냐 하는 것입니다(그림 5.20에서 빨간색 선의 길이를 최대화하고 검은색 선의 길이를 최소화하는 것). 우리는 이를 그룹 내 제곱 거리 합(within-groups sum of squared distances, WSS)으로 공식화합니다:

\\[ \text{WSS}_k=\sum_{\ell=1}^k \sum_{x_i \in C_\ell} d^2(x_i,
\bar{x}_{\ell}) \tag{5.4}\\]

여기서 \\(k\\)는 클러스터의 수, \\(C_\ell\\)은 \\(\ell\\)번째 클러스터에 있는 객체들의 집합, 그리고 \\(\bar{x}_\ell\\)은 \\(\ell\\)번째 클러스터의 질량 중심(평균점)입니다. 우리는 동일한 클러스터 알고리즘에 대해 서로 다른 \\(k\\) 값에 걸쳐 이 수치를 비교하는 데 관심이 있으므로 식 5.4에서 WSS의 \\(k\\)에 대한 의존성을 명시합니다. 하지만 WSS 그 자체로는 충분한 기준이 되지 못합니다: WSS의 최솟값은 단순히 각 점을 개별 클러스터로 만듦으로써 얻어질 수 있기 때문입니다. WSS는 유용한 구성 요소이지만, 이 숫자만 보는 것보다 더 정교한 아이디어가 필요합니다.

한 가지 아이디어는 \\(k\\)의 함수로서 \\(\text{WSS}_k\\)를 살펴보는 것입니다. 이는 항상 감소 함수이겠지만, 급격히 감소하다가 완만해지는 뚜렷한 영역이 있다면, 우리는 이를 _엘보우(elbow)_라고 부르며 이를 클러스터 수의 잠재적인 최적 지점으로 간주할 수 있습니다.

__

질문 5.11

**\\(\text{WSS}_k\\)에 대한 대안적 표현**. R을 사용하여 클러스터 내의 모든 점 쌍 사이의 거리 합을 계산하고 이를 \\(\text{WSS}_k\\)와 비교해 보세요. \\(\text{WSS}_k\\)가 다음과 같이 쓰여질 수 있음을 알 수 있나요?

\\[ \text{WSS}_k=\sum_{\ell=1}^k \frac{1}{2 n_\ell} \sum_{x_i \in C_\ell}
\sum_{x_j \in C_\ell} d^2(x_i,x_j), \tag{5.5}\\]

여기서 \\(n_\ell\\)은 \\(\ell\\)번째 클러스터의 크기입니다.

질문 5.11은 클러스터 내 제곱합 \\(\text{WSS}_k\\)가 클러스터 내의 모든 점과 중심 사이의 거리뿐만 아니라, 클러스터 내의 모든 점 쌍 사이의 평균 거리도 측정한다는 것을 보여줍니다.

데이터에 적합한 클러스터 수를 결정하는 데 도움이 되는 다양한 지수와 통계량의 거동을 살펴볼 때, 정답을 실제로 알고 있는 경우를 살펴보는 것이 유용할 수 있습니다.

시작하기 위해, 네 개의 그룹에서 나오는 데이터를 시뮬레이션합니다. 우리는 파이프(`%>%`) 연산자와 **[dplyr](https://cran.r-project.org/web/packages/dplyr/)**의 `bind_rows` 함수를 사용하여 각 클러스터에 해당하는 네 개의 _tibble_을 하나의 큰 _tibble_로 연결합니다.6

6 파이프 연산자는 왼쪽에 있는 값을 오른쪽 함수로 전달합니다. 이는 코드에서 데이터의 흐름을 더 쉽게 따라갈 수 있게 해줍니다: `f(x) %>% g(y)`는 `g(f(x), y)`와 동일합니다.

    
    
    library("dplyr")
    simdat = lapply(c(0, 8), function(mx) {
      lapply(c(0,8), function(my) {
        tibble(x = rnorm(100, mean = mx, sd = 2),
               y = rnorm(100, mean = my, sd = 2),
               class = paste(mx, my, sep = ":"))
       }) %>% bind_rows
    }) %>% bind_rows
    simdat __
    
    
    # A tibble: 400 × 3
            x      y class
        <dbl>  <dbl> <chr>
     1 -2.42  -4.59  0:0  
     2  1.89  -1.56  0:0  
     3  0.558  2.17  0:0  
     4  2.51  -0.873 0:0  
     5 -2.52  -0.766 0:0  
     6  3.62   0.953 0:0  
     7  0.774  2.43  0:0  
     8 -1.71  -2.63  0:0  
     9  2.01   1.28  0:0  
    10  2.03  -1.25  0:0  
    # ℹ 390 more rows
    
    
    simdatxy = simdat[, c("x", "y")] # class 레이블 제외 __
    
    
    ggplot(simdat, aes(x = x, y = y, col = class)) + geom_point() +
      coord_fixed()__

[![](05-chap_files/figure-html/fig-simdat-1-1.png)](05-chap_files/figure-
html/fig-simdat-1-1.png "그림 5.24: 클래스 레이블로 색상이 입혀진 simdat 데이터. 여기서는 우리가 데이터를 생성했으므로 레이블을 알고 있습니다 – 보통은 이를 알지 못합니다.")

그림 5.24: 클래스 레이블로 색상이 입혀진 `simdat` 데이터. 여기서는 우리가 데이터를 생성했으므로 레이블을 알고 있습니다 – 보통은 이를 알지 못합니다.

우리는 \\(k\\)-평균 방법으로 얻은 클러스터들에 대해 그룹 내 제곱합을 계산합니다:

    
    
    wss = tibble(k = 1:8, value = NA_real_)
    wss$value[1] = sum(scale(simdatxy, scale = FALSE)^2)
    for (i in 2:nrow(wss)) {
      km  = kmeans(simdatxy, centers = wss$k[i])
      wss$value[i] = sum(km$withinss)
    }
    ggplot(wss, aes(x = k, y = value)) + geom_col()__

[![](05-chap_files/figure-html/fig-WSS-1.png)](05-chap_files/figure-html/fig-
WSS-1.png "그림 5.25: k의 함수로서 WSS 통계량의 막대 그래프는 마지막 실질적인 도약이 k=4 직전에 있음을 보여줍니다. 이는 이 데이터에 대한 최선의 선택이 k=4임을 나타냅니다.")

그림 5.25: \\(k\\)의 함수로서 WSS 통계량의 막대 그래프는 마지막 실질적인 도약이 \\(k=4\\) 직전에 있음을 보여줍니다. 이는 이 데이터에 대한 최선의 선택이 \\(k=4\\)임을 나타냅니다.

__

질문 5.12

  1. 위의 코드를 여러 번 실행하고 서로 다른 실행에서의 `wss` 값을 비교해 보세요. 왜 이들은 서로 다를까요?  

  2. `simdat`와 동일한 범위와 차원을 가진, 정규 분포 대신 균등 분포(uniform distributions)로부터 오는 데이터 세트를 만드세요. 이 데이터에 대해 WSS 값을 계산해 보세요. 무엇을 결론지을 수 있나요?

__

질문 5.13

이른바 **칼린스키-하라바츠(Calinski-Harabasz)** 지수는 WSS와 BSS(between group sums of squares, 그룹 간 제곱합)를 사용합니다. 이는 분산 분석에서 사용되는 \\(F\\) 통계량 — 인자에 의해 설명되는 평균 제곱합 대 평균 잔차 제곱합의 비율 — 에서 영감을 받았습니다:

\\[ \text{CH}(k)=\frac{\text{BSS}_k}{\text{WSS}_k}\times\frac{N-k}{N-1} \qquad
\text{where} \quad \text{BSS}_k = \sum_{\ell=1}^k
n_\ell(\bar{x}_{\ell}-\bar{x})^2, \\]

여기서 \\(\bar{x}\\)는 전체 질량 중심(평균점)입니다. `simdat` 데이터에 대한 칼린스키-하라바츠 지수를 플롯해 보세요.

__

해결책

__

그림 5.26을 생성하는 코드는 다음과 같습니다.

    
    
    library("fpc")
    library("cluster")
    CH = tibble(
      k = 2:8,
      value = sapply(k, function(i) {
        p = pam(simdatxy, i)
        calinhara(simdatxy, p$cluster)
      })
    )
    ggplot(CH, aes(x = k, y = value)) + geom_line() + geom_point() +
      ylab("CH index")__

[![](05-chap_files/figure-html/fig-CHIndex-1-1.png)](05-chap_files/figure-
html/fig-CHIndex-1-1.png "그림 5.26: simdat 데이터에 대해 계산된, 서로 다른 k 선택에 대한 그룹 간 분산과 그룹 내 분산의 비율인 칼린스키-하라바츠 지수.")

그림 5.26: `simdat` 데이터에 대해 계산된, 서로 다른 \\(k\\) 선택에 대한 그룹 간 분산과 그룹 내 분산의 비율인 칼린스키-하라바츠 지수.

### 5.7.1 갭 통계량(gap statistic) 사용하기

그룹 내 제곱합의 로그(\\(\log(\text{WSS}_k)\\))를 취하고 이를 구조가 덜한 시뮬레이션 데이터의 평균과 비교하는 것은 \\(k\\)를 선택하는 좋은 방법이 될 수 있습니다. 이것이 Tibshirani, Walther, Hastie ([2001](16-chap.html#ref-gap2001))에 의해 도입된 **갭 통계량(gap statistic)**의 기본 아이디어입니다. 우리는 클러스터 수인 여러 \\(k\\) 값에 대해 \\(\log(\text{WSS}_k)\\)를 계산하고, 이를 다양한 가능한 '군집되지 않은' 분포를 가진 유사한 차원의 참조 데이터에서 얻은 것과 비교합니다. 위에서 했던 것처럼 균등하게 분포된 데이터를 사용하거나, 원래 데이터와 동일한 공분산 구조를 가진 시뮬레이션 데이터를 사용할 수 있습니다.

![](imgs/roulette.png)

이 알고리즘은 관측된 데이터에 대한 갭 통계량을 유사한 구조를 가진 시뮬레이션 데이터의 평균과 비교하는 몬테카를로 방법입니다.

**갭 통계량 계산 알고리즘 ([Tibshirani, Walther, and Hastie 2001](16-chap.html#ref-gap2001)):**

  1. 데이터를 \\(k\\)개의 클러스터로 군집화하고 다양한 \\(k\\) 선택에 대해 \\(\text{WSS}_k\\)를 계산합니다.

  2. 균질한 분포로부터 몬테카를로 샘플링을 사용하여 \\(B\\)개의 그럴듯한 참조 데이터 세트를 생성하고, 이 새로운 시뮬레이션 데이터에 대해 위의 1단계를 다시 수행합니다. 그 결과 시뮬레이션 데이터에 대한 \\(B\\)개의 새로운 그룹 내 제곱합 \\(W_{kb}^*\\)(\\(b=1,...,B\\))을 얻습니다.

  3. \\(\text{gap}(k)\\)-통계량을 계산합니다:

\\[ \text{gap}(k) = \overline{l}_k - \log \text{WSS}_k \quad\text{with}\quad
\overline{l}_k =\frac{1}{B}\sum_{b=1}^B \log W^*_{kb} \\]

군집화가 잘 되었다면(즉, WSS가 더 작다면) 첫 번째 항이 두 번째 항보다 클 것으로 예상됩니다. 따라서 갭 통계량은 대부분 양수일 것이며 우리는 그 최댓값을 찾습니다.

  4. 표준 편차

\\[ \text{sd}_k^2 =
\frac{1}{B-1}\sum_{b=1}^B\left(\log(W^*_{kb})-\overline{l}_k\right)^2 \\]

를 사용하여 최적의 \\(k\\)를 선택하는 데 도움을 줄 수 있습니다. 여러 선택지가 있는데, 예를 들어 다음과 같은 최소의 \\(k\\)를 선택하는 것입니다.

\\[ \text{gap}(k) \geq \text{gap}(k+1) - s'_{k+1}\qquad \text{where }
s'_{k+1}=\text{sd}_{k+1}\sqrt{1+1/B}. \\]

The packages **[cluster](https://cran.r-project.org/web/packages/cluster/)**
and **[clusterCrit](https://cran.r-project.org/web/packages/clusterCrit/)**
provide implementations.

__

Question 5.14

Make a function that plots the gap statistic as in Figure 5.27. Show the
output for the `simdat` example dataset clustered with the `pam` function.

__

Solution

__

    
    
    library("cluster")
    library("ggplot2")
    pamfun = function(x, k)
      list(cluster = pam(x, k, cluster.only = TRUE))
    
    gss = clusGap(simdatxy, FUN = pamfun, K.max = 8, B = 50,
                  verbose = FALSE)
    plot_gap = function(x) {
      gstab = data.frame(x$Tab, k = seq_len(nrow(x$Tab)))
      ggplot(gstab, aes(k, gap)) + geom_line() +
        geom_errorbar(aes(ymax = gap + SE.sim,
                          ymin = gap - SE.sim), width=0.1) +
        geom_point(size = 3, col=  "red")
    }
    plot_gap(gss)__

[![](05-chap_files/figure-html/fig-GapStat-1-1.png)](05-chap_files/figure-
html/fig-GapStat-1-1.png "Figure 5.27: The gap statistic, see Question wrn-
clustering-gapstat.")

Figure 5.27: The gap statistic, see Question 5.14.

Let’s now use the method on a real example. We load the
**[Hiiragi](https://bioconductor.org/packages/Hiiragi/)** data that we already
explored in [Chapter 3](03-chap.html) and will see how the cells cluster.

    
    
    library("Hiiragi2013")__
    
    
    In chunk 'Hiiragi': Warning: replacing previous import 'boot::logit' by 'gtools::logit' whenloading 'Hiiragi2013'
    
    
    In chunk 'Hiiragi': Warning: replacing previous import 'boot::inv.logit' by 'gtools::inv.logit'when loading 'Hiiragi2013'
    
    
    data("x")__

We start by choosing the 50 most variable genes (features)7.

7 The intention behind this step is to reduce the influence of technical (or
batch) effects. Although individually small, when accumulated over all the
45101 features in `x`, many of which match genes that are weakly or not
expressed, without this feature selection step, such effects are prone to
suppress the biological signal.

    
    
    selFeats = order(rowVars(Biobase::exprs(x)), decreasing = TRUE)[1:50]
    embmat = t(Biobase::exprs(x)[selFeats, ])
    embgap = clusGap(embmat, FUN = pamfun, K.max = 24, verbose = FALSE)
    k1 = maxSE(embgap$Tab[, "gap"], embgap$Tab[, "SE.sim"])
    k2 = maxSE(embgap$Tab[, "gap"], embgap$Tab[, "SE.sim"],
               method = "Tibs2001SEmax")
    c(k1, k2)__
    
    
    [1] 9 7

The default choice for the number of clusters, `k1`, is the first value of
\\(k\\) for which the gap is not larger than the first local maximum minus a
standard error \\(s\\) (see the manual page of the `clusGap` function). This
gives a number of clusters \\(k = 9\\), whereas the choice recommended by
Tibshirani, Walther, and Hastie ([2001](16-chap.html#ref-gap2001)) is the
smallest \\(k\\) such that \\(\text{gap}(k) \geq \text{gap}(k+1) -
s_{k+1}'\\), this gives \\(k = 7\\). Let’s plot the gap statistic (Figure
5.28).

    
    
    plot(embgap, main = "")
    cl = pamfun(embmat, k = k1)$cluster
    table(pData(x)[names(cl), "sampleGroup"], cl)__
    
    
                     cl
                       1  2  3  4  5  6  7  8  9
      E3.25           23 11  1  1  0  0  0  0  0
      E3.25 (FGF4-KO)  0  0  1 16  0  0  0  0  0
      E3.5 (EPI)       2  1  0  0  0  8  0  0  0
      E3.5 (FGF4-KO)   0  0  8  0  0  0  0  0  0
      E3.5 (PE)        0  0  0  0  9  2  0  0  0
      E4.5 (EPI)       0  0  0  0  0  0  0  4  0
      E4.5 (FGF4-KO)   0  0  0  0  0  0  0  0 10
      E4.5 (PE)        0  0  0  0  0  0  4  0  0

[![](05-chap_files/figure-html/fig-gapHiiragi-1-1.png)](05-chap_files/figure-
html/fig-gapHiiragi-1-1.png "그림 5.28: Hiiragi2013 데이터에 대한 갭 통계량.")

그림 5.28: **[Hiiragi2013](https://bioconductor.org/packages/Hiiragi2013/)** 데이터에 대한 갭 통계량.

위에서 우리는 `pamfun`으로부터 얻은 군집화 결과와 데이터의 어노테이션에 포함된 샘플 레이블을 비교한 것을 볼 수 있습니다.

__

질문 5.15

가장 가변적인 상위 50개 유전자만 사용하는 대신 `x`의 모든 특성을 사용하면 결과가 어떻게 달라질까요?

### 5.7.2 붓스트랩을 이용한 클러스터 검증

[![](imgs/BootstrapClusterNew.png)](imgs/BootstrapClusterNew.png
"그림 5.29 (a): ")

(a)

[![](imgs/BootstrapCluster2New.png)](imgs/BootstrapCluster2New.png
"그림 5.29 (b): ")

(b)

그림 5.29: 동일한 분포 \\(F\\)로부터 얻은 서로 다른 표본들은 서로 다른 군집화 결과로 이어집니다. (a)에서 우리는 실제 표집 가변성을 봅니다. 붓스트랩은 (b)에서 보듯이 경험적 분포 함수 \\(\hat{F}_n\\)을 사용하여 하위 표본(subsamples)을 추출함으로써 이러한 표집 가변성을 시뮬레이션합니다.

우리는 [4장](04-chap.html)에서 붓스트랩 원리를 보았습니다: 이상적으로는 기저의 데이터 생성 프로세스로부터 많은 새로운 표본(데이터 세트)을 얻어 각각에 군집화 방법을 적용한 다음, 군집화를 비교하기 위해 위에서 사용했던 것과 같은 지수를 사용하여 클러스터가 얼마나 안정적인지 또는 얼마나 변하는지 보고 싶을 것입니다. 물론 우리에게는 이러한 추가 표본이 없습니다. 따라서 우리는 단순히 데이터의 서로 다른 무작위 하위 표본을 취하여 매번 얻는 서로 다른 군집화 결과를 비교함으로써 새로운 데이터 세트를 만들 것입니다. Tibshirani, Walther, Hastie ([2001](16-chap.html#ref-gap2001))는 갭 통계량을 사용하여 클러스터 수를 추론할 때 붓스트랩 재표본 추출을 사용할 것을 권장합니다.

우리는 계속해서 **[Hiiragi2013](https://bioconductor.org/packages/Hiiragi2013/)** 데이터를 사용하겠습니다. 여기서는 생쥐 배아의 배아기 3.5일(E3.5) 배반포의 내세포집단(ICM)이 다분화능 에피블라스트(EPI)와 원시 내배엽(PE)에 해당하는 두 개의 클러스터로 "자연스럽게" 나뉘는 반면, 배아기 3.25일(E3.25)의 데이터는 아직 이러한 대칭성 붕괴(symmetry breaking)를 보여주지 않는다는 가설에 대한 조사를 따라갑니다.

우리는 군집화 과정에서 실제 그룹 레이블을 사용하지 않고, 결과의 최종 해석에서만 이를 사용할 것입니다. 우리는 (E3.5)와 (E3.25)라는 두 가지 서로 다른 데이터 세트에 각각 붓스트랩을 적용할 것입니다. 붓스트랩의 각 단계는 데이터의 무작위 하위 집합에 대한 군집화 결과를 생성할 것이며, 우리는 이를 클러스터 앙상블의 합의(consensus)를 통해 비교해야 할 것입니다. **[clue](https://cran.r-project.org/web/packages/clue/)** 패키지([Hornik 2005](16-chap.html#ref-Hornik2005))에 이를 위한 유용한 프레임워크가 있습니다. Ohnishi 등 ([2014](16-chap.html#ref-Ohnishi2014))의 부록에서 가져온 `clusterResampling` 함수가 이 접근 방식을 구현합니다:

    
    
    clusterResampling = function(x, ngenes = 50, k = 2, B = 250,
                                 prob = 0.67) {
      mat = Biobase::exprs(x)
      ce = cl_ensemble(list = lapply(seq_len(B), function(b) {
        selSamps = sample(ncol(mat), size = round(prob * ncol(mat)),
                          replace = FALSE)
        submat = mat[, selSamps, drop = FALSE]
        sel = order(rowVars(submat), decreasing = TRUE)[seq_len(ngenes)]
        submat = submat[sel,, drop = FALSE]
        pamres = pam(t(submat), k = k)
        pred = cl_predict(pamres, t(mat[sel, ]), "memberships")
        as.cl_partition(pred)
      }))
      cons = cl_consensus(ce)
      ag = sapply(ce, cl_agreement, y = cons)
      list(agreements = ag, consensus = cons)
    }__

`clusterResampling` 함수는 다음 단계들을 수행합니다:

  1. 복원 추출 없이 샘플의 67%를 선택하여 데이터(모든 E3.25 샘플 또는 모든 E3.5 샘플)의 무작위 하위 집합을 추출합니다.

  2. (하위 집합에서) 전체 분산 기준 상위 `ngenes`개의 특성을 선택합니다.

  3. \\(k\\)-평균 군집화를 적용하고, **[clue](https://cran.r-project.org/web/packages/clue/)** 패키지의 `cl_predict` 메서드를 사용하여 하위 집합에 포함되지 않았던 샘플들이 어느 클러스터 중심과 가장 가까운지에 따라 그들의 클러스터 멤버십을 예측합니다.

  4. 1-3단계를 `B`번 반복합니다.

  5. 합의 군집화(consensus clustering, `cl_consensus`)를 적용합니다.

  6. `B`개의 군집화 결과 각각에 대해 `cl_agreement` 함수를 통해 합의 결과와의 일치도를 측정합니다. 여기서 일치도가 높으면 1에 가까운 값을, 낮으면 더 작은 값을 갖습니다. 일치도가 전반적으로 높다면 \\(k\\)개 클래스로의 군집화는 안정적이고 재현 가능한 것으로 간주될 수 있습니다. 반대로 낮다면 샘플들을 \\(k\\)개 클러스터로 나누는 안정적인 분할이 뚜렷하지 않음을 의미합니다.

합의 군집화를 위한 클러스터 간 거리 척도로 멤버십의 _유클리드_ 비유사성이 사용됩니다. 즉, \\(\mathbf{u}\\)와 \\(\mathbf{v}\\)의 모든 열 순열 사이의 최소 제곱 차이합의 제곱근입니다(여기서 \\(\mathbf{u}\\)와 \\(\mathbf{v}\\)는 클러스터 멤버십 행렬임). 일치도 측정값으로는 \\(1 - d/m\\) 수치가 사용되는데, 여기서 \\(d\\)는 유클리드 비유사성이고 \\(m\\)은 최대 유클리드 비유사성의 상한선입니다.

    
    
    iswt = (x$genotype == "WT")
    cr1 = clusterResampling(x[, x$Embryonic.day == "E3.25" & iswt])
    cr2 = clusterResampling(x[, x$Embryonic.day == "E3.5"  & iswt])__

결과는 그림 5.30에 나와 있습니다. 이들은 E3.5 데이터가 두 개의 클러스터로 나뉜다는 가설을 확인해 줍니다.

    
    
    ag1 = tibble(agreements = cr1$agreements, day = "E3.25")
    ag2 = tibble(agreements = cr2$agreements, day = "E3.5")
    p1 <- ggplot(bind_rows(ag1, ag2), aes(x = day, y = agreements)) +
      geom_boxplot() +
      ggbeeswarm::geom_beeswarm(cex = 1.5, col = "#0000ff40")
    mem1 = tibble(y = sort(cl_membership(cr1$consensus)[, 1]),
                  x = seq(along = y), day = "E3.25")
    mem2 = tibble(y = sort(cl_membership(cr2$consensus)[, 1]),
                  x = seq(along = y), day = "E3.5")
    p2 <- ggplot(bind_rows(mem1, mem2), aes(x = x, y = y, col = day)) +
      geom_point() + facet_grid(~ day, scales = "free_x")
    gridExtra::grid.arrange(p1, p2, widths = c(2.4,4.0))__

[![](05-chap_files/figure-html/fig-figClue1-1.png)](05-chap_files/figure-
html/fig-figClue1-1.png "그림 5.30: E3.25 및 E3.5 샘플을 이용한 클러스터 안정성 분석. 왼쪽: B개의 군집화 결과와 합의 결과 사이의 일치도에 대한 beeswarm 플롯; 1은 완벽한 일치를 나타내며, 낮은 값은 낮은 일치도를 나타냅니다. 오른쪽: 합의 군집화의 멤버십 확률. E3.25의 경우 확률이 분산되어 있어 개별 군집화 결과들이 자주 일치하지 않음을 나타내는 반면, E3.5의 경우 분포가 이봉형(bimodal)이며 모호한 샘플은 하나뿐입니다.")

그림 5.30: E3.25 및 E3.5 샘플을 이용한 클러스터 안정성 분석. 왼쪽: `B`개의 군집화 결과와 합의 결과 사이의 일치도에 대한 beeswarm 플롯; \\(1\\)은 완벽한 일치를 나타내며, 낮은 값은 낮은 일치도를 나타냅니다. 오른쪽: 합의 군집화의 멤버십 확률. E3.25의 경우 확률이 분산되어 있어 개별 군집화 결과들이 자주 일치하지 않음을 나타내는 반면, E3.5의 경우 분포가 이봉형(bimodal)이며 모호한 샘플은 하나뿐입니다.

#### 계산 및 메모리 문제

[![계산 복잡도. 어떤 알고리즘이 O(n^k)라고 불리는 것은, n이 커짐에 따라 자원 소비(CPU 시간 또는 메모리)가 n^k에 비례하여 증가함을 의미합니다. 다른(때로는 상당한) 기본 비용이나 n의 더 낮은 거듭제곱에 비례하여 증가하는 비용이 있을 수 있지만, n이 무한대로 감에 따라 이들은 항상 지배적인 항에 비해 무시할 수 있는 수준이 됩니다.](imgs/devil.png)](imgs/devil.png "계산 복잡도. 어떤 알고리즘이 O(n^k)라고 불리는 것은, n이 커짐에 따라 자원 소비(CPU 시간 또는 메모리)가 n^k에 비례하여 증가함을 의미합니다. 다른(때로는 상당한) 기본 비용이나 n의 더 낮은 거듭제곱에 비례하여 증가하는 비용이 있을 수 있지만, n이 무한대로 감에 따라 이들은 항상 지배적인 항에 비해 무시할 수 있는 수준이 됩니다.")

**계산 복잡도(Computational complexity)**. 어떤 알고리즘이 \\(O(n^k)\\)라고 불리는 것은, \\(n\\)이 커짐에 따라 자원 소비(CPU 시간 또는 메모리)가 \\(n^k\\)에 비례하여 증가함을 의미합니다. 다른(때로는 상당한) 기본 비용이나 \\(n\\)의 더 낮은 거듭제곱에 비례하여 증가하는 비용이 있을 수 있지만, \\(n\to\infty\\)임에 따라 이들은 항상 지배적인 항에 비해 무시할 수 있는 수준이 됩니다.

\\(n\\)개 객체의 전체 대 전체 거리 계산이 (시간과 메모리 측면에서) \\(O(n^2)\\) 작업임을 기억하는 것이 중요합니다. 전통적인 계층적 군집화 접근 방식(**[stats](https://cran.r-project.org/web/packages/stats/)** 패키지의 `hclust` 등)은 시간 측면에서 심지어 \\(O(n^3)\\)입니다. \\(n\\)이 크다면 이는 비실용적일 수 있습니다8. 우리는 전체 대 전체 거리 행렬의 완전한 계산을 피할 수 있습니다. 예를 들어, \\(k\\)-평균은 각 객체와 클러스터 중심 사이의 거리만 추적하면 되므로 \\(O(n)\\)의 계산만 필요하다는 장점이 있으며, 클러스터 중심의 수는 \\(n\\)이 증가하더라도 일정하게 유지됩니다.

8 예를 들어 백만 개 객체에 대한 거리 행렬을 8바이트 부동 소수점으로 저장하면 약 4테라바이트를 차지하며, `hclust`와 같은 알고리즘은 각 반복이 1나노초만 걸린다는 낙관적인 가정 하에서도 30년 동안 실행될 것입니다.

**[fastclust](https://cran.r-project.org/web/packages/fastclust/)** ([Müllner 2013](16-chap.html#ref-Mullner:2013)) 및 **[dbscan](https://cran.r-project.org/web/packages/dbscan/)**과 같은 빠른 구현체들은 많은 수의 관측치를 처리하기 위해 신중하게 최적화되었습니다.

## 5.8 노이즈 제거 수단으로서의 군집화

어떤 기저의 실제 값(예를 들어 게놈의 DNA 서열로 표현되는 종)을 반영하지만 기술적 노이즈에 의해 변질된 측정값 세트를 생각해 봅시다. 군집화는 이러한 노이즈를 제거하는 데 사용될 수 있습니다.

### 5.8.1 서로 다른 베이스라인 빈도를 가진 노이즈 섞인 관측치

동일한 오차 분산으로 만들어진 관측치들의 이변량 분포(bivariate distribution)가 있다고 가정해 봅시다. 그러나 샘플링은 베이스라인 빈도가 매우 다른 두 그룹으로부터 이루어집니다. 더 나아가, 오차는 연속적이고 독립적인 이변량 정규 분포를 따른다고 가정합시다. 다음 코드에서 생성된 것처럼 우리는 `seq1`을 \\(10^{3}\\)개, `seq2`를 \\(10^{5}\\)개 가지고 있습니다:

    
    
    library("mixtools")
    seq1 = rmvnorm(n = 1e3, mu = -c(1, 1), sigma = 0.5 * diag(c(1, 1)))
    seq2 = rmvnorm(n = 1e5, mu =  c(1, 1), sigma = 0.5 * diag(c(1, 1)))
    twogr = data.frame(
      rbind(seq1, seq2),
      seq = factor(c(rep(1, nrow(seq1)),
                     rep(2, nrow(seq2))))
    )
    colnames(twogr)[1:2] = c("x", "y")
    library("ggplot2")
    ggplot(twogr, aes(x = x, y = y, colour = seq,fill = seq)) +
      geom_hex(alpha = 0.5, bins = 50) + coord_fixed()__

[![](05-chap_files/figure-html/fig-seqradius-1.png)](05-chap_files/figure-
html/fig-seqradius-1.png "그림 5.31: 두 그룹 모두 동일한 분산의 노이즈 분포를 가지고 있음에도 불구하고, 그룹의 겉보기 반지름은 매우 다릅니다. seq2의 10^{5}개 인스턴스는 10^{3}개뿐인 seq1보다 오차가 발생할 기회가 훨씬 더 많습니다. 따라서 우리는 데이터를 군집화할 때 빈도가 중요하다는 것을 알 수 있습니다.")

그림 5.31: 두 그룹 모두 동일한 분산의 노이즈 분포를 가지고 있음에도 불구하고, 그룹의 겉보기 반지름은 매우 다릅니다. `seq2`의 \\(10^{5}\\)개 인스턴스는 \\(10^{3}\\)개뿐인 `seq1`보다 오차가 발생할 기회가 훨씬 더 많습니다. 따라서 우리는 데이터를 군집화할 때 빈도가 중요하다는 것을 알 수 있습니다.

관측된 값들은 그림 5.31과 같이 보일 것입니다.

__

질문 5.16

데이터 `seq1`과 `seq2`를 가져와서 그룹 중심으로부터의 거리에 따라 두 그룹으로 군집화해 보세요. 결과가 두 서열 유형 각각의 빈도에 의존해야 한다고 생각하시나요?

__

해결책

__

분류학적 군집화(taxonomic clustering), 즉 OTU(operational taxonomic unit) 군집화([Caporaso et al. 2010](16-chap.html#ref-caporaso2010qiime); [P. D. Schloss et al. 2009](16-chap.html#ref-mothur)) 방법들에서 종종 사용되는 이러한 접근 방식은 최적이 아닙니다.

오직 유사성에만 기반한 방법들은 _대표성(representativeness)_ 휴리스틱에 내재된 편향으로 인해 고통받습니다. 군집화 및 분류학적 할당에서 대표성과 거리 기반 휴리스틱만을 사용하려는 우리의 자연스러운 경향이 어떻게 편향된 결과로 이어질 수 있는지 설명하는 데 도움이 되는 인지 심리학의 세계로 잠시 외도해 봅시다.

1970년대에 Tversky와 Kahneman ([1975](16-chap.html#ref-tversky1975judgment))은 우리가 일반적으로 가장 유사한 _대표자(representatives)_를 살펴봄으로써 그룹을 할당한다고 지적했습니다. 군집화와 그룹 할당에서 이는 새로운 서열을 그 중심까지의 거리에 따라 그룹에 할당하는 것을 의미할 것입니다. 사실 이는 서로 다른 그룹의 보급률(prevalence) 차이를 무시하고 동일한 반지름을 가진 공을 취하는 것과 같습니다. 이러한 심리학적 오류는 많은 다양한 휴리스틱과 편향을 다루는 중요한 Science 논문([Tversky and Kahneman 1974](16-chap.html#ref-tversky1974heuristics))에서 처음 논의되었습니다.

[![우리의 자연스러운 휴리스틱과 우리가 확률 계산을 할 때 그것들이 우리를 오도할 수 있는 방식에 대한 단행본 규모의 논의는 Kahneman (2011)을 참조하십시오(특히 14장과 15장을 추천합니다).](imgs/book_icon.png)](imgs/book_icon.png "우리의 자연스러운 휴리스틱과 우리가 확률 계산을 할 때 그것들이 우리를 오도할 수 있는 방식에 대한 단행본 규모의 논의는 @kahneman2011을 참조하십시오(특히 14장과 15장을 추천합니다).")

우리의 자연스러운 휴리스틱과 우리가 확률 계산을 할 때 그것들이 우리를 오도할 수 있는 방식에 대한 단행본 규모의 논의는 Kahneman ([2011](16-chap.html#ref-kahneman2011))을 참조하십시오. (특히 14장과 15장을 추천합니다.)
when we make probability calculations (we recommend especially Chapters 14 and
15).

__

Task

Simulate `n=2000` binary variables of length `len=200` that indicate the
quality of `n` sequencing reads of length `len`. For simplicity, let us assume
that sequencing errors occur independently and uniformly with probability
`perr=0.001`. That is, we only care whether a base was called correctly
(`TRUE`) or not (`FALSE`).

    
    
    n    = 2000
    len  = 200
    perr = 0.001
    seqs = matrix(runif(n * len) >= perr, nrow = n, ncol = len)__

Now, compute all pairwise distances between reads.

    
    
    dists = as.matrix(dist(seqs, method = "manhattan"))__

For various values of number of reads `k` (from 2 to `n`), the maximum
distance within this set of reads is computed by the code below and shown in
Figure 5.32.

    
    
    library("tibble")
    dfseqs = tibble(
      k = 10 ^ seq(log10(2), log10(n), length.out = 20),
      diameter = vapply(k, function(i) {
        s = sample(n, i)
        max(dists[s, s])
        }, numeric(1)))
    ggplot(dfseqs, aes(x = k, y = diameter)) + geom_point()+geom_smooth()__

[![](05-chap_files/figure-html/fig-diameter-1.png)](05-chap_files/figure-
html/fig-diameter-1.png "Figure 5.32: The diameter of a set of sequences as a
function of the number of sequences.")

Figure 5.32: The diameter of a set of sequences as a function of the number of
sequences.

We will now improve the 16SrRNA-read clustering using a denoising mechanism
that incorporates error probabilities.

### 5.8.2 Denoising 16S rRNA sequences

**What are the data?** In the bacterial 16SrRNA gene there are so-called
**variable regions** that are taxa-specific. These provide fingerprints that
enables _taxon_ 9 identification. The raw data are FASTQ-files with quality
scored sequences of PCR-amplified DNA regions10. We use an iterative
alternating approach11 to build a probabilistic noise model from the data. We
call this a _de novo_ method, because we use clustering, and we use the
cluster centers as our denoised sequence variants (a.k.a. Amplicon Sequence
Variants, ASVs, see ([Benjamin J. Callahan, McMurdie, and Holmes
2017](16-chap.html#ref-Callahan:2017))). After finding all the denoised
variants, we create contingency tables of their counts across the different
samples. We will show in [Chapter 10](10-chap.html) how these tables can be
used to infer properties of the underlying bacterial communities using
networks and graphs.

9 Calling different groups of bacteria _taxa_ rather than _species_ highlights
the approximate nature of the concept, as the notion of species is more fluid
in bacteria than, say, in animals.

10 The [FASTQ format is described
here](https://en.wikipedia.org/wiki/FASTQ_format).

11 Similar to the EM algorithm we saw in [Chapter 4](04-chap.html).

In order to improve data quality, one often has to start with the raw data and
model all the sources of variation carefully. One can think of this as an
example of _cooking from scratch_ (see the gruesome details in Ben J. Callahan
et al. ([2016](16-chap.html#ref-Callahan2016Bioc)) and Exercise 5.5).

__

Question 5.17

Suppose that we have two sequences of length 200 (`seq1` and `seq2`) present
in our sample at very different abundances. We are told that the technological
sequencing errors occur as independent Bernoulli(0.0005) random events for
each nucleotide.  
What is the distribution of the number of errors per sequence?

__

Solution

__

Probability theory tells us that the sum of 200 independent Poisson(0.0005)
will be Poisson(0.1).

We can also verify this by Monte Carlo simulation:

    
    
    simseq10K = replicate(1e5, sum(rpois(200, 0.0005)))
    mean(simseq10K)__
    
    
    [1] 0.10143
    
    
    vcd::distplot(simseq10K, "poisson")__

[![](05-chap_files/figure-html/fig-seqradiusex-1.png)](05-chap_files/figure-
html/fig-seqradiusex-1.png "Figure 5.33: distplot for the simseq10K data.")

Figure 5.33: `distplot` for the `simseq10K` data.

Figure 5.33 shows us how close the distribution is to being Poisson
distributed.

### 5.8.3 Infer sequence variants

The DADA method (Divisive Amplicon Denoising Algorithm, Rosen et al.
([2012](16-chap.html#ref-Rosen:2012))) uses a parameterized model of
substitution errors that distinguishes sequencing errors from real biological
variation. The model computes the probabilities of base substitutions, such as
seeing an \\({\tt A}\\) instead of a \\({\tt C}\\). It assumes that these
probabilities are independent of the position along the sequence. Because
error rates vary substantially between sequencing runs and PCR protocols, the
model parameters are estimated from the data themselves using an EM-type
approach. A read is classified as noisy or exact given the current parameters,
and the noise model parameters are updated accordingly12.

12 In the case of a large data set, the noise model estimation step does not
have to be done on the complete set. See
<https://benjjneb.github.io/dada2/bigdata.html> for tricks and tools when
dealing with large data sets.

13 F stands for forward strand and R for reverse strand.

The dereplicated sequences13 are read in and then divisive denoising and
estimation is run with the `dada` function as in the following code:

    
    
    derepFs = readRDS(file="../data/derepFs.rds")
    derepRs = readRDS(file="../data/derepRs.rds")
    library("dada2")
    ddF = dada(derepFs, err = NULL, selfConsist = TRUE)
    ddR = dada(derepRs, err = NULL, selfConsist = TRUE)__

In order to verify that the error transition rates have been reasonably well
estimated, we inspect the fit between the observed error rates (black points)
and the fitted error rates (black lines) (Figure 5.34).

    
    
    plotErrors(ddF)__
    
    
    In chunk 'fig-rerrorprofile1': Warning: Removed 82 rows containing missing values or values outside the scale range(`geom_line()`).

[![](05-chap_files/figure-html/fig-
rerrorprofile1-1.png)](05-chap_files/figure-html/fig-rerrorprofile1-1.png
"Figure 5.34: Forward transition error rates as provided by plotErrors\(ddF\).
This shows the frequencies of each type of nucleotide transition as a function
of quality.")

Figure 5.34: Forward transition error rates as provided by `plotErrors(ddF)`.
This shows the frequencies of each type of nucleotide transition as a function
of quality.

Once the errors have been estimated, the algorithm is rerun on the data to
find the sequence variants:

    
    
    dadaFs = dada(derepFs, err=ddF[[1]]$err_out, pool = TRUE)
    dadaRs = dada(derepRs, err=ddR[[1]]$err_out, pool = TRUE)__

**Note:** The sequence inference function can run in two different modes:
Independent inference by sample (`pool = FALSE`), and pooled inference from
the sequencing reads combined from all samples. Independent inference has two
advantages: as a functions of the number of samples, computation time is
linear and memory requirements are constant. Pooled inference is more
computationally taxing, however it can improve the detection of rare variants
that occur just once or twice in an individual sample but more often across
all samples. As this dataset is not particularly large, we performed pooled
inference.

Sequence inference removes nearly all substitution and **indel** 14 errors
from the data. We merge the inferred forward and reverse sequences, while
removing paired sequences that do not perfectly overlap as a final control
against residual errors.

14 The term _indel_ stands for insertion-deletion; when comparing two
sequences that differ by a small stretch of characters, it is a matter of
viewpoint whether this is an insertion or a deletion, thus the name.

    
    
    mergers = mergePairs(dadaFs, derepFs, dadaRs, derepRs)__

We produce a contingency table of counts of ASVs. This is a higher-resolution
analogue of the “OTU15 table”, i.e., a samples by features table whose cells
contain the number of times each sequence variant was observed in each sample.

15 operational taxonomic units

    
    
    seqtab.all = makeSequenceTable(mergers[!grepl("Mock",names(mergers))])__

__

Question 5.18

Explore the components of the objects `dadaRs` and `mergers`.

__

Solution

__

`dadaRs` is a list of length 20. Its elements are objects class _dada_ that
contain the denoised reads. We will see in [Chapter 10](10-chap.html) how to
align the sequences, assign their taxonomies and combine them with the sample
information for downstream analyses.

    
    
    [1] 20
    
    
    [1] 20
    
    
    [1] "list"
    
    
     [1] "F3D0"   "F3D1"   "F3D141" "F3D142" "F3D143" "F3D144" "F3D145" "F3D146"
     [9] "F3D147" "F3D148" "F3D149" "F3D150" "F3D2"   "F3D3"   "F3D5"   "F3D6"  
    [17] "F3D7"   "F3D8"   "F3D9"   "Mock"  
    
    
    [1] "list"
    
    
    [1] 20

Chimera are sequences that are artificially created during PCR amplification
by the melding of two (in rare cases, more) of the original sequences. To
complete our denoising workflow, we remove them with a call to the function
`removeBimeraDenovo`, leaving us with a clean contingency table we will use
later on.

    
    
    seqtab = removeBimeraDenovo(seqtab.all)__

__

Question 5.19

Why do you think the chimera are quite easy to recognize?  
What proportion of the reads were chimeric in the `seqtab.all` data?  
What proportion of unique sequence variants are chimeric?

__

Solution

__

Here we observed some sequence variants as chimeric, but these only represent
7% of all reads.

## 5.9 Summary of this chapter

**Of a feather: how to compare observations** We saw at the start of the
chapter how finding the **right distance** is an essential first step in a
clustering analysis; this is a case where the _garbage in, garbage out_ motto
is in full force. Always choose a distance that is scientifically meaningful
and compare output from as many distances as possible; sometimes the same data
require different distances when different scientific objectives are pursued.

**Two ways of clustering** We saw there are two approaches to clustering:

  * iterative partitioning approaches such as \\(k\\)-means and \\(k\\)-medoids (PAM) that alternated between estimating the cluster centers and assigning points to them;  

  * hierarchical clustering approaches that first agglomerate points, and subsequently the growing clusters, into nested sequences of sets that can be represented by hierarchical clustering _trees_.

**Biological examples** Clustering is important tool for finding latent
classes in single cell measurements, especially in immunology and single cell
data analyses. We saw how density-based clustering is useful for lower
dimensional data where sparsity is not an issue.

**Validating** Clustering algorithms _always_ deliver clusters, so we need to
assess their quality and the number of clusters to choose carefully. Such
validation steps are performed using visualization tools and repeating the
clustering on many resamples of the data. We saw how statistics such as
WSS/BSS or \\(\log(\text{WSS})\\) can be calibrated using simulations on data
where we understand the group structure and can provide useful benchmarks for
choosing the number of clusters on new data. Of course, the use of
biologically relevant information to inform and confirm the meaning of
clusters is always the best validation approach.

There is arguably no ground truth to compare a clustering result against, in
general. The old adage of “all models are wrong, some are useful” also applies
here. A good clustering is one that turns out to be useful.

**Distances and probabilities** Finally: distances are not everything. We
showed how important it was to take into account baseline frequencies and
local densities when clustering. This is essential in a cases such as
clustering to denoise 16S rRNA sequence reads where the true class or taxa
group occur at very different frequencies.

## 5.10 Further reading

For a complete book on _Finding groups in data_ , see Kaufman and Rousseeuw
([2009](16-chap.html#ref-Kaufman2009)). The vignette of the
**[clusterExperiment](https://bioconductor.org/packages/clusterExperiment/)**
package contains a complete workflow for generating clusters using many
different techniques, including preliminary dimension reduction (PCA) that we
will cover in [Chapter 7](07-chap.html). There is no consensus on methods for
deciding how many clusters are needed to describe data in the absence of
contiguous biological information. However, making hierarchical clusters of
the _strong forms_ is a method that has the advantage of allowing the user to
decide how far down to cut the hierarchical tree and be careful not to cut in
places where these inner branches are short. See the vignette of
**[clusterExperiment](https://bioconductor.org/packages/clusterExperiment/)**
for an application to single cell RNA experimental data.

In analyzing the Hiiragi data, we used cluster probabilities, a concept
already mentioned in [Chapter 4](04-chap.html), where the EM algorithm used
them as weights to compute expected value statistics. The notion of
probabilistic clustering is well-developed in the Bayesian nonparametric
mixture framework, which enriches the mixture models we covered in [Chapter
4](04-chap.html) to more general settings. See Dundar et al.
([2014](16-chap.html#ref-Dundar2014)) for a real example using this framework
for flow cytometry. In the denoising and assignment of high-throughput
sequencing reads to specific strains of bacteria or viruses, clustering is
essential. In the presence of noise, clustering into groups of _true_ strains
of very unequal sizes can be challenging. Using the data to create a noise
model enables both denoising and cluster assignment concurrently. Denoising
algorithms such as those by Rosen et al. ([2012](16-chap.html#ref-Rosen:2012))
or Benjamin J. Callahan et al. ([2016](16-chap.html#ref-dada2)) use an
iterative workflow inspired by the EM method ([McLachlan and Krishnan
2007](16-chap.html#ref-mclachlan2007algorithm)).

## 5.11 Exercises

__

Exercise 5.1

We can define the average dissimilarity of a point \\(x_i\\) to a cluster
\\(C_k\\) as the average of the distances from \\(x_i\\) to all points in
\\(C_k\\). Let \\(A(i)\\) be the average dissimilarity of all points in the
cluster that \\(x_i\\) belongs to. Let \\(B(i)\\) be the lowest average
dissimilarity of \\(x_i\\) to any other cluster of which \\(x_i\\) is not a
member. The cluster with this lowest average dissimilarity is said to be the
**neighboring cluster** of \\(x_i\\), because it is the next best fit cluster
for point \\(x_i\\). The **silhouette index** is

\\[ S(i)=\frac{B(i)-A(i)}{\max_i(A(i),B(i))}. \\]

Compute the silhouette index for the `simdat` data we simulated in Section
5.7.

    
    
    library("cluster")
    pam4 = pam(simdatxy, 4)
    sil = silhouette(pam4, 4)
    plot(sil, col=c("red","green","blue","purple"), main="Silhouette")__

Change the number of clusters \\(k\\) and assess which \\(k\\) gives the best
silhouette index.

Now, repeat this for groups that have uniform (unclustered) data distributions
over a whole range of values.

__

Exercise 5.2

Make a “character” representation of the distance between the 20 locations in
the `dune` data from the
**[vegan](https://cran.r-project.org/web/packages/vegan/)** package using the
function `symnum`.

Make a heatmap plot of these distances.

__

Exercise 5.3

Load the `spirals` data from the
**[kernlab](https://cran.r-project.org/web/packages/kernlab/)** package. Plot
the results of using \\(k\\)-means on the data. This should give you something
similar to Figure 5.35.

[![](05-chap_files/figure-html/fig-kmeanspital1-1.png)](05-chap_files/figure-
html/fig-kmeanspital1-1.png "Figure 5.35 \(a\): ")

(a)

[![](05-chap_files/figure-html/fig-kmeanspital1-2.png)](05-chap_files/figure-
html/fig-kmeanspital1-2.png "Figure 5.35 \(b\): ")

(b)

Figure 5.35: An example of non-convex clusters. In (a), we show the result of
\\(k\\)-means clustering with \\(k=2\\). In (b), we have the output from
`dbscan`. The colors represent the three clusters found by the algorithm for
the settings .

You’ll notice that the clustering in Figure 5.35 seems unsatisfactory. Show
how a different method, such as `specc` or `dbscan`, could cluster `spirals`
data in a more useful manner.

Repeat the `dbscan` clustering with different parameters. How robust is the
number of groups?

__

Exercise 5.4

Looking at graphical representations in simple two-dimensional maps can often
reveal important clumping patterns. We saw an example for this with the map
that enabled Snow to discover the source of the London cholera outbreak. Such
clusterings can often indicate important information about hidden variables
acting on the observations. Look at a map for breast cancer incidence in the
US at:  
<http://www.huffingtonpost.com/bill-davenhall/post_1663_b_817254.html>
([Mandal et al. 2009](16-chap.html#ref-mandal2009)); the areas of high
incidence seem spatially clustered. Can you guess the reason(s) for this
clustering and high incidence rates on the West and East coasts and around
Chicago?

__

Exercise 5.5

**Amplicon bioinformatics: from raw reads to dereplicated sequences**. As a
supplementary exercise, we provide the intermediate steps necessary to a full
data preprocessing workflow for denoising 16S rRNA sequences. We start by
setting the directories and loading the downloaded data:

    
    
    base_dir = "../data"
    miseq_path = file.path(base_dir, "MiSeq_SOP")
    filt_path = file.path(miseq_path, "filtered")
    fnFs = sort(list.files(miseq_path, pattern="_R1_001.fastq"))
    fnRs = sort(list.files(miseq_path, pattern="_R2_001.fastq"))
    sampleNames = sapply(strsplit(fnFs, "_"), `[`, 1)
    if (!file_test("-d", filt_path)) dir.create(filt_path)
    filtFs = file.path(filt_path, paste0(sampleNames, "_F_filt.fastq.gz"))
    filtRs = file.path(filt_path, paste0(sampleNames, "_R_filt.fastq.gz"))
    fnFs = file.path(miseq_path, fnFs)
    fnRs = file.path(miseq_path, fnRs)
    print(length(fnFs))__
    
    
    [1] 20

The data are highly-overlapping Illumina Miseq \\(2\times 250\\) amplicon
sequences from the V4 region of the 16S rRNA gene ([Kozich et al.
2013](16-chap.html#ref-Kozich2013)). There were originally 360 fecal samples
collected longitudinally from 12 mice over the first year of life. These were
collected by P. D. Schloss et al. ([2012](16-chap.html#ref-
schloss2012stabilization)) to investigate the development and stabilization of
the murine microbiome. We have selected 20 samples to illustrate how to
preprocess the data.

We will need to filter out low-quality reads and trim them to a consistent
length. While generally recommended filtering and trimming parameters serve as
a starting point, no two datasets are identical and therefore it is always
worth inspecting the quality of the data before proceeding. We show the
sequence quality plots for the two first samples in Figure 5.36. They are
generated by:

    
    
    plotQualityProfile(fnFs[1:2]) + ggtitle("Forward")
    plotQualityProfile(fnRs[1:2]) + ggtitle("Reverse")__

[![](05-chap_files/figure-html/fig-profile-1-1.png)](05-chap_files/figure-
html/fig-profile-1-1.png "Figure 5.36 \(a\): ")

(a)

[![](05-chap_files/figure-html/fig-profile-1-2.png)](05-chap_files/figure-
html/fig-profile-1-2.png "Figure 5.36 \(b\): ")

(b)

Figure 5.36: Quality scores. The lines show positional summary statistics:
green is the mean, orange is the median, and the dashed orange lines are the
25th and 75th quantiles.

Note that we also see the background distribution of quality scores at each
position in Figure 5.36 as a grey-scale heat map. The dark colors correspond
to higher frequency.

__

Exercise 5.6

Generate similar plots for four randomly selected sets of forward and reverse
reads. Compare forward and reverse read qualities; what do you notice?

__

Solution

__

    
    
    ii = sample(length(fnFs), 4)
    plotQualityProfile(fnFs[ii]) + ggtitle("Forward")__
    
    
     plotQualityProfile(fnRs[ii]) + ggtitle("Reverse")__

__

Exercise 5.7

Here, the forward reads maintain high quality throughout, while the quality of
the reverse reads drops significantly at about position 160. Therefore, we
truncate the forward reads at position 240, and trimm the first 10 nucleotides
as these positions are of lower quality. The reverse reads are trimmed at
position 160. Combine these trimming parameters with standard filtering
parameters remember to enforce a maximum of 2 expected errors per-read. (Hint:
Trim and filter on paired reads jointly, i.e., both reads must pass the filter
for the pair to pass. The input arguments should be chosen following the
**[dada2](https://bioconductor.org/packages/dada2/)** vignette carefully. We
recommend filtering out all reads with any ambiguous nucleotides.)

__

Solution

__

Most Illumina sequencing data show a trend of decreasing quality towards the
end of the reads.

    
    
    out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),
            maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,  trimLeft=10,
            compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
    head(out)__
    
    
                                  reads.in reads.out
    F3D0_S188_L001_R1_001.fastq       7793      7139
    F3D1_S189_L001_R1_001.fastq       5869      5314
    F3D141_S207_L001_R1_001.fastq     5958      5478
    F3D142_S208_L001_R1_001.fastq     3183      2926
    F3D143_S209_L001_R1_001.fastq     3178      2955
    F3D144_S210_L001_R1_001.fastq     4827      4323

The `maxN` parameter omits all reads with more than `maxN = 0` ambiguous
nucleotides and `maxEE` at 2 excludes reads with more than 2 expected errors.

The sequence data was imported into R from demultiplexed _fastq_ files (i.e.
one _fastq_ for each sample) and simultaneously dereplicated to remove
redundancy. Name the resulting objects by their sample provenance; they will
have _derep_ as their class.

    
    
    derepFs = derepFastq(filtFs, verbose = FALSE)
    derepRs = derepFastq(filtRs, verbose = FALSE)
    names(derepFs) = sampleNames
    names(derepRs) = sampleNames __

__

Exercise 5.8

Use R to create a map like the one shown in Figure 5.2. Hint: go to the
[website of the British National Archives](http://bombsight.org) and download
street addresses of hits, use an address resolution service to convert these
into geographic coordinates, and display these as points on a map of London.

__

Solution

__

See the Gist
<https://gist.github.com/wolfganghuber/523622aff6a156d26e77a87ccbe7bd0d> by
Andrzej Oles.

Aure, Miriam Ragle, Valeria Vitelli, Sandra Jernström, Surendra Kumar, Marit
Krohn, Eldri U Due, Tonje Husby Haukaas, et al. 2017. “Integrative Clustering
Reveals a Novel Split in the Luminal A Subtype of Breast Cancer with Impact on
Outcome.” _Breast Cancer Research_ 19 (1): 44.

Bendall, Sean C, Garry P Nolan, Mario Roederer, and Pratip K Chattopadhyay.
2012. “A Deep Profiler’s Guide to Cytometry.” _Trends in Immunology_ 33 (7):
323–32.

Callahan, Ben J, Kris Sankaran, Julia A Fukuyama, Paul J McMurdie, and Susan P
Holmes. 2016. “Bioconductor Workflow for Microbiome Data Analysis: From Raw
Reads to Community Analyses.” _F1000Research_ 5\.

Callahan, Benjamin J, Paul J McMurdie, and Susan P Holmes. 2017. “Exact
Sequence Variants Should Replace Operational Taxonomic Units in Marker Gene
Data Analysis.” _ISME Journal_ , 1–5.

Callahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy J
Johnson, and Susan P Holmes. 2016. “DADA2: High Resolution Sample Inference
from Amplicon Data.” _Nature Methods_ , 1–4.

Caporaso, J. G., J. Kuczynski, J. Stombaugh, K. Bittinger, F. D. Bushman, E.
K. Costello, N. Fierer, et al. 2010. “QIIME Allows Analysis of High-Throughput
Community Sequencing Data.” _Nature Methods_ 7 (5): 335–36.

Chakerian, John, and Susan Holmes. 2012. “Computational Tools for Evaluating
Phylogenetic and Hierarchical Clustering Trees.” _Journal of Computational and
Graphical Statistics_ 21 (3): 581–99.

Diday, Edwin, and M Paula Brito. 1989. “Symbolic Cluster Analysis.” In
_Conceptual and Numerical Analysis of Data_ , 45–84. Springer.

Dundar, Murat, Ferit Akova, Halid Z. Yerebakan, and Bartek Rajwa. 2014. “A
Non-Parametric Bayesian Model for Joint Cell Clustering and Cluster Matching:
Identification of Anomalous Sample Phenotypes with Random Effects.” _BMC
Bioinformatics_ 15 (1): 1–15. <https://doi.org/10.1186/1471-2105-15-314>.

Freedman, David A. 1991. “Statistical Models and Shoe Leather.” _Sociological
Methodology_ 21 (2): 291–313.

Hallett, Robin M, Anna Dvorkin-Gheva, Anita Bane, and John A Hassell. 2012. “A
Gene Signature for Predicting Outcome in Patients with Basal-Like Breast
Cancer.” _Scientific Reports_ 2\.

Holmes, Susan, Michael He, Tong Xu, and Peter P Lee. 2005. “Memory t Cells
Have Gene Expression Patterns Intermediate Between Naive and Effector.” _PNAS_
102 (15): 5519–23.

Hornik, Kurt. 2005. “A CLUE for CLUster Ensembles.” _Journal of Statistical
Software_ 14 (12).

Hulett, Henry R, William A Bonner, Janet Barrett, and Leonard A Herzenberg.
1969. “Cell Sorting: Automated Separation of Mammalian Cells as a Function of
Intracellular Fluorescence.” _Science_ 166 (3906): 747–49.

Kahneman, Daniel. 2011. _Thinking, Fast and Slow_. Macmillan.

Kaufman, Leonard, and Peter J Rousseeuw. 2009. _Finding Groups in Data: An
Introduction to Cluster Analysis_. Vol. 344. John Wiley & Sons.

Kozich, James J, Sarah L Westcott, Nielson T Baxter, Sarah K Highlander, and
Patrick D Schloss. 2013. “Development of a Dual-Index Sequencing Strategy and
Curation Pipeline for Analyzing Amplicon Sequence Data on the MiSeq Illumina
Sequencing Platform.” _Applied and Environmental Microbiology_ 79 (17):
5112–20.

Mandal, Rakesh, Sophie St-Hilaire, John G Kie, and DeWayne Derryberry. 2009.
“Spatial Trends of Breast and Prostate Cancers in the United States Between
2000 and 2005.” _International Journal of Health Geographics_ 8 (1): 53.

McLachlan, Geoffrey, and Thriyambakam Krishnan. 2007. _The EM Algorithm and
Extensions_. Vol. 382. John Wiley & Sons.

Müllner, Daniel. 2013. “Fastcluster: Fast Hierarchical, Agglomerative
Clustering Routines for r and Python.” _Journal of Statistical Software_ 53
(9): 1–18.

O’Neill, Kieran, Nima Aghaeepour, Josef Špidlen, and Ryan Brinkman. 2013.
“Flow Cytometry Bioinformatics.” _PLoS Computational Biology_ 9 (12):
e1003365.

Ohnishi, Y., W. Huber, A. Tsumura, M. Kang, P. Xenopoulos, K. Kurimoto, A. K.
Oles, et al. 2014. “Cell-to-Cell Expression Variability Followed by Signal
Reinforcement Progressively Segregates Early Mouse Lineages.” _Nature Cell
Biology_ 16 (1): 27–37.

Rosen, Michael J, Benjamin J Callahan, Daniel S Fisher, and Susan P Holmes.
2012. “Denoising PCR-Amplified Metagenome Data.” _BMC Bioinformatics_ 13 (1):
283.

Schloss, P D, S L Westcott, T Ryabin, J R Hall, M Hartmann, E B Hollister, R A
Lesniewski, et al. 2009. “Introducing mothur: Open-Source, Platform-
Independent, Community-Supported Software for Describing and Comparing
Microbial Communities.” _Applied and Environmental Microbiology_ 75 (23):
7537–41.

Schloss, P. D., A. M. Schuber, J. P. Zackular, K. D. Iverson, Young V. B., and
Petrosino J. F. 2012. “Stabilization of the Murine Gut Microbiome Following
Weaning.” _Gut Microbes_ 3 (4): 383–93.

Tibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. “Estimating the
Number of Clusters in a Data Set via the Gap Statistic.” _JRSSB_ 63 (2):
411–23.

Tseng, George C, and Wing H Wong. 2005. “Tight Clustering: A Resampling-Based
Approach for Identifying Stable and Tight Patterns in Data.” _Biometrics_ 61
(1): 10–16.

Tversky, Amos, and Daniel Kahneman. 1974. “Heuristics and Biases: Judgement
Under Uncertainty.” _Science_ 185: 1124–30.

———. 1975. “Judgment Under Uncertainty: Heuristics and Biases.” In _Utility,
Probability, and Human Decision Making_ , 141–62. Springer.

Page built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)

