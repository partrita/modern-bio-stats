<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>13&nbsp; 11.1 Goals for this chapter – Modern Statistics for Modern Biology</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./12-chap.html" rel="next">
<link href="./10-chap.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-22854ec117201859c8a7ba6f538122c9.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="msmb.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./11-chap.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">11.1 Goals for this chapter</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Modern Statistics for Modern Biology</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Home</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The challenge: heterogeneity</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">1.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">2.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">3.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">4.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">5.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">6.1 Goals for this Chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">7.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">8.1 Goals of this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">9.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">10.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-chap.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">11.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">12.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">13.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">14-chap.html</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">15-chap.html</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">16-chap.html</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#loading-images" id="toc-loading-images" class="nav-link active" data-scroll-target="#loading-images"><span class="header-section-number">13.1</span> 11.2 Loading images</a></li>
  <li><a href="#displaying-images" id="toc-displaying-images" class="nav-link" data-scroll-target="#displaying-images"><span class="header-section-number">13.2</span> 11.3 Displaying images</a></li>
  <li><a href="#how-are-images-stored-in-r" id="toc-how-are-images-stored-in-r" class="nav-link" data-scroll-target="#how-are-images-stored-in-r"><span class="header-section-number">13.3</span> 11.4 How are images stored in R?</a></li>
  <li><a href="#writing-images-to-file" id="toc-writing-images-to-file" class="nav-link" data-scroll-target="#writing-images-to-file"><span class="header-section-number">13.4</span> 11.5 Writing images to file</a></li>
  <li><a href="#manipulating-images" id="toc-manipulating-images" class="nav-link" data-scroll-target="#manipulating-images"><span class="header-section-number">13.5</span> 11.6 Manipulating images</a></li>
  <li><a href="#spatial-transformations" id="toc-spatial-transformations" class="nav-link" data-scroll-target="#spatial-transformations"><span class="header-section-number">13.6</span> 11.7 Spatial transformations</a></li>
  <li><a href="#linear-filters" id="toc-linear-filters" class="nav-link" data-scroll-target="#linear-filters"><span class="header-section-number">13.7</span> 11.8 Linear filters</a>
  <ul class="collapse">
  <li><a href="#interlude-the-intensity-scale-of-images" id="toc-interlude-the-intensity-scale-of-images" class="nav-link" data-scroll-target="#interlude-the-intensity-scale-of-images"><span class="header-section-number">13.7.1</span> 11.8.1 Interlude: the intensity scale of images</a></li>
  <li><a href="#noise-reduction-by-smoothing" id="toc-noise-reduction-by-smoothing" class="nav-link" data-scroll-target="#noise-reduction-by-smoothing"><span class="header-section-number">13.7.2</span> 11.8.2 Noise reduction by smoothing</a></li>
  </ul></li>
  <li><a href="#adaptive-thresholding" id="toc-adaptive-thresholding" class="nav-link" data-scroll-target="#adaptive-thresholding"><span class="header-section-number">13.8</span> 11.9 Adaptive thresholding</a></li>
  <li><a href="#morphological-operations-on-binary-images" id="toc-morphological-operations-on-binary-images" class="nav-link" data-scroll-target="#morphological-operations-on-binary-images"><span class="header-section-number">13.9</span> 11.10 Morphological operations on binary images</a></li>
  <li><a href="#segmentation-of-a-binary-image-into-objects" id="toc-segmentation-of-a-binary-image-into-objects" class="nav-link" data-scroll-target="#segmentation-of-a-binary-image-into-objects"><span class="header-section-number">13.10</span> 11.11 Segmentation of a binary image into objects</a></li>
  <li><a href="#voronoi-tessellation" id="toc-voronoi-tessellation" class="nav-link" data-scroll-target="#voronoi-tessellation"><span class="header-section-number">13.11</span> 11.12 Voronoi tessellation</a></li>
  <li><a href="#segmenting-the-cell-bodies" id="toc-segmenting-the-cell-bodies" class="nav-link" data-scroll-target="#segmenting-the-cell-bodies"><span class="header-section-number">13.12</span> 11.13 Segmenting the cell bodies</a></li>
  <li><a href="#feature-extraction" id="toc-feature-extraction" class="nav-link" data-scroll-target="#feature-extraction"><span class="header-section-number">13.13</span> 11.14 Feature extraction</a></li>
  <li><a href="#spatial-statistics-point-processes" id="toc-spatial-statistics-point-processes" class="nav-link" data-scroll-target="#spatial-statistics-point-processes"><span class="header-section-number">13.14</span> 11.15 Spatial statistics: point processes</a>
  <ul class="collapse">
  <li><a href="#a-case-study-interaction-between-immune-cells-and-cancer-cells" id="toc-a-case-study-interaction-between-immune-cells-and-cancer-cells" class="nav-link" data-scroll-target="#a-case-study-interaction-between-immune-cells-and-cancer-cells"><span class="header-section-number">13.14.1</span> 11.15.1 A case study: Interaction between immune cells and cancer cells</a></li>
  <li><a href="#convex-hull" id="toc-convex-hull" class="nav-link" data-scroll-target="#convex-hull"><span class="header-section-number">13.14.2</span> 11.15.2 Convex hull</a></li>
  <li><a href="#other-ways-of-defining-the-space-for-the-point-process" id="toc-other-ways-of-defining-the-space-for-the-point-process" class="nav-link" data-scroll-target="#other-ways-of-defining-the-space-for-the-point-process"><span class="header-section-number">13.14.3</span> 11.15.3 Other ways of defining the space for the point process</a></li>
  </ul></li>
  <li><a href="#first-order-effects-the-intensity" id="toc-first-order-effects-the-intensity" class="nav-link" data-scroll-target="#first-order-effects-the-intensity"><span class="header-section-number">13.15</span> 11.16 First order effects: the intensity</a>
  <ul class="collapse">
  <li><a href="#poisson-process" id="toc-poisson-process" class="nav-link" data-scroll-target="#poisson-process"><span class="header-section-number">13.15.1</span> 11.16.1 Poisson Process</a></li>
  <li><a href="#estimating-the-intensity" id="toc-estimating-the-intensity" class="nav-link" data-scroll-target="#estimating-the-intensity"><span class="header-section-number">13.15.2</span> 11.16.2 Estimating the intensity</a></li>
  </ul></li>
  <li><a href="#second-order-effects-spatial-dependence" id="toc-second-order-effects-spatial-dependence" class="nav-link" data-scroll-target="#second-order-effects-spatial-dependence"><span class="header-section-number">13.16</span> 11.17 Second order effects: spatial dependence</a>
  <ul class="collapse">
  <li><a href="#ripleys-k-function" id="toc-ripleys-k-function" class="nav-link" data-scroll-target="#ripleys-k-function"><span class="header-section-number">13.16.1</span> 11.17.1 Ripley’s \(K\) function</a></li>
  </ul></li>
  <li><a href="#summary-of-this-chapter" id="toc-summary-of-this-chapter" class="nav-link" data-scroll-target="#summary-of-this-chapter"><span class="header-section-number">13.17</span> 11.18 Summary of this chapter</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">13.18</span> 11.19 Further reading</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">13.19</span> 11.20 Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">11.1 Goals for this chapter</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><a href="imgs/chap13-colorLabelscellbodies.png"><img src="imgs/chap13-colorLabelscellbodies.png" class="img-fluid"></a></p>
<p>Images are a rich source of data. In this chapter, we will see how quantitative information can be extracted from images, and how we can use statistical methods to summarize and understand the data. The goal of the chapter is to show that getting started working with image data is easy – if you are able to handle the basic R environment, you are ready to start working with images. That said, this chapter is not a general introduction to image analysis. The field is extensive; it touches many areas of signal processing, information theory, mathematics, engineering and computer science, and there are excellent books that present a systematic overview.</p>
<p>We will mainly study series of two-dimensional images, in particular, images of cells. We will learn how to identify the cells’ positions and shapes and how to quantitatively measure characteristics of the identified shapes and patterns, such as sizes, intensities, color distributions and relative positions. Such information can then be used in down-stream analyses: for instance, we can compare cells between different conditions, say under the effect of different drugs, or in different stages of differentiation and growth; or we can measure how the objects in the image relate to each other, e.g., whether they like to cluster together or repel each other, or whether certain characteristics tend to be shared between neighboring objects, indicative of cell-to-cell communication. In the language of genetics, what this means is that we can use images as complex phenotypes or as multivariate quantitative traits.</p>
<p>We will here not touch upon image analysis in more than two dimensions: we won’t consider 3D segmentation and registration, nor temporal tracking. These are sophisticated tasks for which specialized software would likely perform better than what we could assemble in the scope of this chapter.</p>
<p>There are similarities between data from high-throughput imaging and other high-throughput data in genomics. Batch effects tend to play a role, for instance because of changes in staining efficiency, illumination or many other factors. We’ll need to take appropriate precautions in our experimental design and analysis choices. In principle, the intensity values in an image can be calibrated in physical units, corresponding, say to radiant energy or fluorophore concentration; however this is not always done in practice in biological imaging, and perhaps also not needed. Somewhat easier to achieve and clearly valuable is a calibration of the spatial dimensions of the image, i.e., the conversion factor between pixel units and metric distances.</p>
<p>In this chapter we will:</p>
<ul>
<li><p>Learn how to read, write and manipulate images in R.</p></li>
<li><p>Understand how to apply filters and transformations to images.</p></li>
<li><p>Combine these skills to do segmentation and feature extraction; we will use cell segmentation as an example.</p></li>
<li><p>Learn how to use statistical methods to analyse spatial distributions and dependencies.</p></li>
<li><p>Get to know the most basic distribution for a spatial point process: the homogeneous Poisson process.</p></li>
<li><p>Recognize whether your data fit that basic assumption or whether they show evidence of clumping or exclusion.</p></li>
</ul>
<section id="loading-images" class="level2" data-number="13.1">
<h2 data-number="13.1" class="anchored" data-anchor-id="loading-images"><span class="header-section-number">13.1</span> 11.2 Loading images</h2>
<p>A useful toolkit for handling images in R is the Bioconductor package <strong><a href="https://bioconductor.org/packages/EBImage/">EBImage</a></strong> (<a href="16-chap.html#ref-EBImage">Pau et al. 2010</a>). We start out by reading in a simple picture to demonstrate the basic functions.</p>
<pre><code>library("EBImage")
imagefile = system.file("images", "mosquito.png", package = "MSMB")
mosq = readImage(imagefile)__</code></pre>
<p><strong><a href="https://bioconductor.org/packages/EBImage/">EBImage</a></strong> currently supports three image file formats: <code>jpeg</code>, <code>png</code> and <code>tiff</code>. Above, we loaded a sample image from the <strong><a href="https://bioconductor.org/packages/MSMB/">MSMB</a></strong> package. When you are working with your own data, you do not need that package, just provide the name(s) of your file(s) to the <code>readImage</code> function. As you will see later in this chapter, <code>readImage</code> can read multiple images in one go, which are then all assembled into a single image data object. For this to work, the images need to have the same dimensions and color mode.</p>
<p>__</p>
<p>Question 11.1</p>
<p>The <strong>RBioFormats</strong> package (available on GitHub: <a href="https://github.com/aoles/RBioFormats" class="uri">https://github.com/aoles/RBioFormats</a>) provides functionality for reading and writing many more image file formats. How many different file formats are supported?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>See the manual page of the <code>read.image</code> function in the <strong>RBioFormats</strong> package (note that this is distinct from <code>EBImage::readImage</code>) and the online documentation of the Bio-Formats project on the website of The Open Microscopy Environment, &lt;http://www.openmicroscopy.org/site/support/bio- formats5.5/supported-formats.html&gt;.</p>
</section>
<section id="displaying-images" class="level2" data-number="13.2">
<h2 data-number="13.2" class="anchored" data-anchor-id="displaying-images"><span class="header-section-number">13.2</span> 11.3 Displaying images</h2>
<p>Let’s visualize the image that we just read in. The basic function is <code>EBImage::display</code>.</p>
<pre><code>EBImage::display(mosq)__</code></pre>
<p>The above command opens the image in a window of your web browser (as set by <code>getOption("browser")</code>). Using the mouse or keyboard shortcuts, you can zoom in and out of the image, pan and cycle through multiple image frames.</p>
<p>Alternatively, we can also display the image using R’s built-in plotting by calling <code>display</code> with the argument <code>method = "raster"</code>. The image then goes to the current device. In this way, we can combine image data with other plotting functionality, for instance, to add text labels.</p>
<pre><code>EBImage::display(mosq)
text(x = 85, y = 800, label = "A mosquito", adj = 0, col = "orange", cex = 1.5)__</code></pre>
<p><a href="11-chap_files/figure- html/fig-mosquito-1.jpeg" title="Figure 11.1: Mosquito discovered deceased in the suburbs of Decatur, Georgia (credit: CDC / Janice Haney Carr)."><img src="11-chap_files/figure-html/fig-mosquito-1.jpeg" class="img-fluid"></a></p>
<p>Figure 11.1: Mosquito discovered deceased in the suburbs of Decatur, Georgia (credit: CDC / Janice Haney Carr).</p>
<p>The resulting plot is shown in Figure 11.1. As usual, the graphics displayed in an R device can be saved using the <strong><a href="https://cran.r-project.org/web/packages/base/">base</a></strong> R functions <code>dev.print</code> or <code>dev.copy</code>.</p>
<p>Note that we can also read and view color images, see Figure 11.2.</p>
<pre><code>imagefile = system.file("images", "hiv.png", package = "MSMB")
hivc = readImage(imagefile)__


EBImage::display(hivc, method = "raster")__</code></pre>
<p><a href="11-chap_files/figure-html/fig- hiv-1.jpeg" title="Figure 11.2: Scanning electron micrograph of HIV-1 virions budding from a cultured lymphocyte (credit: CDC / C. Goldsmith, P. Feorino, E.L. Palmer, W.R. McManus)."><img src="11-chap_files/figure-html/fig-hiv-1.jpeg" class="img-fluid"></a></p>
<p>Figure 11.2: Scanning electron micrograph of HIV-1 virions budding from a cultured lymphocyte (credit: CDC / C. Goldsmith, P. Feorino, E.L. Palmer, W.R. McManus).</p>
<p>Furthermore, if an image has multiple frames, they can be displayed all at once in a grid arrangement by specifying the function argument <code>all = TRUE</code> (Figure 11.3),</p>
<pre><code>nuc = readImage(system.file("images", "nuclei.tif", package = "EBImage"))
EBImage::display(1 - nuc, all = TRUE)__</code></pre>
<p><a href="11-chap_files/figure-html/fig-image-oneminus-1.png &quot;Figure 11.3: Tiled display of four images of cell nuclei from the EBImage package.&quot;"><img src="11-chap_files/figure-html/fig-image- oneminus-1.png" class="img-fluid"></a></p>
<p>Figure 11.3: Tiled display of four images of cell nuclei from the <strong><a href="https://bioconductor.org/packages/EBImage/">EBImage</a></strong> package.</p>
<p>or we can just view a single frame, for instance, the second one.</p>
<pre><code>EBImage::display(1 - nuc, frame = 2)__</code></pre>
<p>__</p>
<p>Question 11.2</p>
<p>Why did we pass the argument <code>1 - nuc</code> to the <code>display</code> function in the code for Figure 11.3? How does it look if we display <code>nuc</code> directly?</p>
</section>
<section id="how-are-images-stored-in-r" class="level2" data-number="13.3">
<h2 data-number="13.3" class="anchored" data-anchor-id="how-are-images-stored-in-r"><span class="header-section-number">13.3</span> 11.4 How are images stored in R?</h2>
<p>Let’s dig into what’s going on by first identifying the class of the image object.</p>
<pre><code>class(mosq)__


[1] "Image"
attr(,"package")
[1] "EBImage"</code></pre>
<p>So we see that this object has the class <em>Image</em>. This is not one of the base R classes, rather, it is defined by the package <strong><a href="https://bioconductor.org/packages/EBImage/">EBImage</a></strong>. We can find out more about this class through the help browser or by typing <code>class ? Image</code>. The class is derived from the base R class <em>array</em> , so you can do with <em>Image</em> objects everything that you can do with R arrays; in addition, they have some extra features and behaviors1.</p>
<p>1 In R’s parlance, the extra features are called <strong>slots</strong> and the behaviors are called methods; methods are a special kind of function.</p>
<p>__</p>
<p>Question 11.3</p>
<p>How can you find out what the slots of an <em>Image</em> object are and which methods can be applied to it?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>The class definition is easy, it is accessed with <code>showClass("Image")</code>. Finding all the methods applicable to the <em>Image</em> class by an analogous call to an R function is painful; your best bet is to consult the manual page of the class and see which methods the author chose to mention.</p>
<p>The dimensions of the image can be extracted using the <code>dim</code> method, just like for regular arrays.</p>
<pre><code>dim(mosq)__


[1] 1400  952</code></pre>
<p>The <code>hist</code> method has been redefined2 compared to the ordinary <code>hist</code> function for arrays: it uses different and possibly more useful defaults (Figure 11.4).</p>
<p>2 In object oriented parlance, <em>overloaded</em>.</p>
<pre><code>hist(mosq)__</code></pre>
<p><a href="11-chap_files/figure- html/fig-mosqhist-1.png" title="Figure 11.4: Histogram of the pixel intensities in mosq. Note that the range is between 0 and 1."><img src="11-chap_files/figure-html/fig-mosqhist-1.png" class="img-fluid"></a></p>
<p>Figure 11.4: Histogram of the pixel intensities in <code>mosq</code>. Note that the range is between 0 and 1.</p>
<p>If we want to directly access the data matrix as an R <em>array</em> , we can use the accessor function <code>imageData</code>.</p>
<pre><code>imageData(mosq)[1:3, 1:6]__


          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]
[1,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784
[2,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784
[3,] 0.1960784 0.1960784 0.2000000 0.2039216 0.2000000 0.1960784</code></pre>
<p>A useful summary of an <em>Image</em> object is printed if we simply type the object’s name.</p>
<pre><code>mosq __


Image 
  colorMode    : Grayscale 
  storage.mode : double 
  dim          : 1400 952 
  frames.total : 1 
  frames.render: 1 

imageData(object)[1:5,1:6]
          [,1]      [,2]      [,3]      [,4]      [,5]      [,6]
[1,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784
[2,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784
[3,] 0.1960784 0.1960784 0.2000000 0.2039216 0.2000000 0.1960784
[4,] 0.1960784 0.1960784 0.2039216 0.2078431 0.2000000 0.1960784
[5,] 0.1960784 0.2000000 0.2117647 0.2156863 0.2000000 0.1921569</code></pre>
<p>Now let us look at the color image.</p>
<pre><code>hivc __


Image 
  colorMode    : Color 
  storage.mode : double 
  dim          : 1400 930 3 
  frames.total : 3 
  frames.render: 1 

imageData(object)[1:5,1:6,1]
     [,1] [,2] [,3] [,4] [,5] [,6]
[1,]    0    0    0    0    0    0
[2,]    0    0    0    0    0    0
[3,]    0    0    0    0    0    0
[4,]    0    0    0    0    0    0
[5,]    0    0    0    0    0    0</code></pre>
<p>The two images differ by their property <code>colorMode</code>, which is <code>Grayscale</code> for <code>mosq</code> and <code>Color</code> for <code>hivc</code>. What is the point of this property? It turns out to be convenient when we are dealing with stacks of images. If <code>colorMode</code> is <code>Grayscale</code>, then the third and all higher dimensions of the array are considered as separate image frames corresponding, for instance, to different \(z\)-positions, time points, replicates, etc. On the other hand, if <code>colorMode</code> is <code>Color</code>, then the third dimension is assumed to hold different color channels, and only the fourth and higher dimensions – if present – are used for multiple image frames. In <code>hivc</code>, there are three color channels, which correspond to the red, green and blue intensities of our photograph. However, this does not necessarily need to be the case, there can be any number of color channels.</p>
<p>__</p>
<p>Question 11.4</p>
<p>Describe how R stores the data <code>nuc</code>.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>nuc __


Image 
  colorMode    : Grayscale 
  storage.mode : double 
  dim          : 510 510 4 
  frames.total : 4 
  frames.render: 4 

imageData(object)[1:5,1:6,1]
           [,1]       [,2]       [,3]       [,4]       [,5]       [,6]
[1,] 0.06274510 0.07450980 0.07058824 0.08235294 0.10588235 0.09803922
[2,] 0.06274510 0.05882353 0.07843137 0.09019608 0.09019608 0.10588235
[3,] 0.06666667 0.06666667 0.08235294 0.07843137 0.09411765 0.09411765
[4,] 0.06666667 0.06666667 0.07058824 0.08627451 0.08627451 0.09803922
[5,] 0.05882353 0.06666667 0.07058824 0.08235294 0.09411765 0.10588235


dim(imageData(nuc))__


[1] 510 510   4</code></pre>
<p>We see that we have 4 frames in total, which correspond to the 4 separate images (<code>frames.render</code>).</p>
</section>
<section id="writing-images-to-file" class="level2" data-number="13.4">
<h2 data-number="13.4" class="anchored" data-anchor-id="writing-images-to-file"><span class="header-section-number">13.4</span> 11.5 Writing images to file</h2>
<p>Directly saving images to disk in the array representation that we saw in the previous section would lead to large file sizes – in most cases, needlessly large. It is common to use compression algorithms to reduce the storage consumption. There are two main types of image3 compression:</p>
<p>3 In an analogous way, this is also true for movies and music.</p>
<ul>
<li><p>Lossless compression: it is possible to exactly reconstruct the original image data from the compressed file. Simple priciples of lossless compression are: (i) do not spend more bits on representing a pixel than needed (e.g., the pixels in the <code>mosq</code> image have a range of 256 gray scale values, and this could be represented by 8 bits, although <code>mosq</code> stores them in a 64-bit numeric format4); and (2) identify patterns (such as those that you saw above in the printed pixel values for <code>mosq</code> and <code>hivc</code>) and represent them by much shorter to write down rules instead.</p></li>
<li><p>Lossy compression: additional savings are made compared to lossless compression by dropping details that a human viewer would be unlikely to notice anyway.</p></li>
</ul>
<p>4 While this is somewhat wasteful of memory, it is more compatible with the way the rest of R works, and is rarely a limiting factor on modern computer hardware.</p>
<p>5 <a href="https://en.wikipedia.org/wiki/Portable_Network_Graphics" class="uri">https://en.wikipedia.org/wiki/Portable_Network_Graphics</a></p>
<p>6 <a href="https://en.wikipedia.org/wiki/JPEG" class="uri">https://en.wikipedia.org/wiki/JPEG</a></p>
<p>An example for a storage format with lossless compression is PNG5, an example for lossy compression is the JPEG6 format. While JPEG is good for your holiday pictures, it is good practice to store scientific images in a lossless format.</p>
<p>We read the image <code>hivc</code> from a file in PNG format, so let’s now write it out as a JPEG file. The lossiness is specified by the <em>quality</em> parameter, which can lie between 1 (worst) and 100 (best).</p>
<pre><code>output_file = file.path(tempdir(), "hivc.jpeg")
writeImage(hivc, output_file, quality = 85)__</code></pre>
<p>Similarly, we could have written the image as a <code>TIFF</code> file and chosen among several compression algorithms (see the manual page of the <code>writeImage</code> and <code>writeTiff</code> functions). The package <strong>RBioFormats</strong> lets you write to many further image file formats.</p>
<p>__</p>
<p>Question 11.5</p>
<p>How big is the <code>hivc</code> object in R’s memory? How big is the JPEG file? How much RAM would you expect a three color, 16 Megapixel image to occupy?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>object.size(hivc) |&gt; format(units = "Mb")__


[1] "29.8 Mb"


(object.size(hivc) / prod(dim(hivc))) |&gt; format() |&gt; paste("per pixel")__


[1] "8 bytes per pixel"


file.info( output_file )$size __


[1] 294904


16 * 3 * 8 __


[1] 384</code></pre>
</section>
<section id="manipulating-images" class="level2" data-number="13.5">
<h2 data-number="13.5" class="anchored" data-anchor-id="manipulating-images"><span class="header-section-number">13.5</span> 11.6 Manipulating images</h2>
<p>Now that we know that images are stored as arrays of numbers in R, our method of manipulating images becomes clear – simple algebra! For example, we can take our original image, shown again in Figure 11.5a, and flip the bright areas to dark and vice versa by multiplying the image with -1 Figure 11.5b).</p>
<pre><code>mosqinv = normalize(-mosq)__</code></pre>
<p>__</p>
<p>Question 11.6</p>
<p>What does the function <code>normalize</code> do?</p>
<p>We could also adjust the contrast through multiplication (Figure 11.5c) and the gamma-factor through exponentiation Figure 11.5d).</p>
<pre><code>mosqcont = mosq * 3
mosqexp = mosq ^ (1/3)__</code></pre>
<p><a href="11-chap_files/figure- html/fig-manip1-1.jpeg" title="Figure 11.5 (a):"><img src="11-chap_files/figure-html/fig-manip1-1.jpeg" class="img-fluid"></a></p>
<ol type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-manip1-2.jpeg" title="Figure 11.5 (b):"><img src="11-chap_files/figure-html/fig-manip1-2.jpeg" class="img-fluid"></a></p>
<ol start="2" type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-manip1-3.jpeg" title="Figure 11.5 (c):"><img src="11-chap_files/figure-html/fig-manip1-3.jpeg" class="img-fluid"></a></p>
<ol start="3" type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-manip1-4.jpeg" title="Figure 11.5 (d):"><img src="11-chap_files/figure-html/fig-manip1-4.jpeg" class="img-fluid"></a></p>
<ol start="4" type="a">
<li></li>
</ol>
<p>Figure 11.5: The original mosquito image (a) and three different image transformations: (b) subtraction, (c) multiplication, (d) power transformation.</p>
<p>Furthermore, we can crop, threshold and transpose images with matrix operations (Figure 11.6).</p>
<pre><code>mosqcrop   = mosq[100:438, 112:550]
mosqthresh = mosq &gt; 0.5
mosqtransp = transpose(mosq)__</code></pre>
<p><a href="11-chap_files/figure- html/fig-manip5-1.png" title="Figure 11.6 (a):"><img src="11-chap_files/figure-html/fig-manip5-1.png" class="img-fluid"></a></p>
<ol type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-manip5-2.png" title="Figure 11.6 (b):"><img src="11-chap_files/figure-html/fig-manip5-2.png" class="img-fluid"></a></p>
<ol start="2" type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-manip5-3.png" title="Figure 11.6 (c):"><img src="11-chap_files/figure-html/fig-manip5-3.png" class="img-fluid"></a></p>
<ol start="3" type="a">
<li></li>
</ol>
<p>Figure 11.6: Three further image transformations: (a) cropping, (b) thresholding, (c) transposition.</p>
<p>__</p>
<p>Question 11.7</p>
<p>What data type is <code>mosqthresh</code>, the result of the thresholding?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>It is an <em>Image</em> object whose pixels are binary values represented by an R array of type <em>logical</em>. You can inspect the object by typing its name into the console.</p>
<pre><code>mosqthresh __


Image 
  colorMode    : Grayscale 
  storage.mode : logical 
  dim          : 1400 952 
  frames.total : 1 
  frames.render: 1 

imageData(object)[1:5,1:6]
      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]
[1,] FALSE FALSE FALSE FALSE FALSE FALSE
[2,] FALSE FALSE FALSE FALSE FALSE FALSE
[3,] FALSE FALSE FALSE FALSE FALSE FALSE
[4,] FALSE FALSE FALSE FALSE FALSE FALSE
[5,] FALSE FALSE FALSE FALSE FALSE FALSE</code></pre>
<p>__</p>
<p>Question 11.8</p>
<p>Instead of the <code>transpose</code> function as above, could we also use R’s <strong><a href="https://cran.r-project.org/web/packages/base/">base</a></strong> function <code>t</code>?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>In this instance, the values of <code>t(mosq)</code> and <code>transpose(mosq)</code> happen to be the same, but <code>transpose</code> is preferable since it also works with color and multiframe images.</p>
</section>
<section id="spatial-transformations" class="level2" data-number="13.6">
<h2 data-number="13.6" class="anchored" data-anchor-id="spatial-transformations"><span class="header-section-number">13.6</span> 11.7 Spatial transformations</h2>
<p>We just saw one type of spatial transformation, transposition, but there are many more—here are some examples:</p>
<pre><code>mosqrot   = EBImage::rotate(mosq, angle = 30)
mosqshift = EBImage::translate(mosq, v = c(100, 170))
mosqflip  = flip(mosq)
mosqflop  = flop(mosq)__</code></pre>
<p><a href="11-chap_files/figure- html/fig-flipflop-1.jpeg" title="Figure 11.7 (a):"><img src="11-chap_files/figure-html/fig-flipflop-1.jpeg" class="img-fluid"></a></p>
<ol type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-flipflop-2.jpeg" title="Figure 11.7 (b):"><img src="11-chap_files/figure-html/fig-flipflop-2.jpeg" class="img-fluid"></a></p>
<ol start="2" type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-flipflop-3.jpeg" title="Figure 11.7 (c):"><img src="11-chap_files/figure-html/fig-flipflop-3.jpeg" class="img-fluid"></a></p>
<ol start="3" type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-flipflop-4.jpeg" title="Figure 11.7 (d):"><img src="11-chap_files/figure-html/fig-flipflop-4.jpeg" class="img-fluid"></a></p>
<ol start="4" type="a">
<li></li>
</ol>
<p>Figure 11.7: Spatial transformations: (a) rotation, (b) translation, (c) reflection about the central horizontal axis (<code>flip</code>), (d) reflection about the central vertical axis (<code>flop</code>).</p>
<p>In the code above, the function <code>rotate</code>7 rotates the image clockwise with the given angle, <code>translate</code> moves the image by the specified two-dimensional vector (pixels that end up outside the image region are cropped, and pixels that enter into the image region are set to zero). The functions <code>flip</code> and <code>flop</code> reflect the image around the central horizontal and vertical axis, respectively. The results of these operations are shown in Figure 11.7.</p>
<p>7 Here we call the function with its namespace qualifier <code>EBImage::</code> to avoid confusion with a function of the same name in the namespace of the <strong><a href="https://cran.r-project.org/web/packages/spatstat/">spatstat</a></strong> package, which we will attach later.</p>
</section>
<section id="linear-filters" class="level2" data-number="13.7">
<h2 data-number="13.7" class="anchored" data-anchor-id="linear-filters"><span class="header-section-number">13.7</span> 11.8 Linear filters</h2>
<p>Let’s now switch to an application in cell biology. We load images of human cancer cells that were studied by Laufer, Fischer and co-workers (<a href="16-chap.html#ref-Laufer:NatMeth:2013">Laufer et al.&nbsp;2013</a>). They are shown in Figure 11.8.</p>
<pre><code>imagefiles = system.file("images", c("image-DAPI.tif", "image-FITC.tif", "image-Cy3.tif"), package = "MSMB")
cells = readImage(imagefiles)__</code></pre>
<p><a href="11-chap_files/figure- html/fig-LauferCells-1.png" title="Figure 11.8: Human colon cancer cells (HCT116). The four images show the same cells: the leftmost image corresponds to DAPI staining of the cells’ DNA, the second to immunostaining against alpha- tubulin, the third to actin. They are displayed as gray-scale images. The rightmost image is obtained by overlaying the three images as color channels of an RGB image (red: actin, green: alpha-tubulin, blue: DNA)."><img src="11-chap_files/figure-html/fig-LauferCells-1.png" class="img-fluid"></a></p>
<p>Figure 11.8: Human colon cancer cells (HCT116). The four images show the same cells: the leftmost image corresponds to DAPI staining of the cells’ DNA, the second to immunostaining against alpha-tubulin, the third to actin. They are displayed as gray-scale images. The rightmost image is obtained by overlaying the three images as color channels of an RGB image (red: actin, green: alpha- tubulin, blue: DNA).</p>
<p>The <em>Image</em> object <code>cells</code> is a three-dimensional array of size 340 \(\) 490 \(\) 3, where the last dimension indicates that there are three individual grayscale frames. Our goal now is to computationally identify and quantitatively characterize the cells in these images. That by itself would be a modest goal, but note that the dataset of Laufer et al.contains over 690,000 images, each of which has 2,048 \(\) 2,048 pixels. Here, we are looking at three of these, out of which a small region was cropped. Once we know how to achieve our stated goal, we can apply our abilities to such large image collections, and that is no longer a modest aim!</p>
<section id="interlude-the-intensity-scale-of-images" class="level3" data-number="13.7.1">
<h3 data-number="13.7.1" class="anchored" data-anchor-id="interlude-the-intensity-scale-of-images"><span class="header-section-number">13.7.1</span> 11.8.1 Interlude: the intensity scale of images</h3>
<p>However, before we can start with real work, we need to deal with a slightly mundane data conversion issue. This is, of course, not unusual. Let us inspect the dynamic range (the minimum and the maximum value) of the images.</p>
<pre><code>apply(cells, 3, range)__


      image-DAPI  image-FITC   image-Cy3
[1,] 0.001586938 0.002899214 0.001663233
[2,] 0.031204700 0.062485695 0.055710689</code></pre>
<p>We see that the maximum values are small numbers well below 1. The reason for this is that the <code>readImage</code> function recognizes that the <code>TIFF</code> images uses 16 bit integers to represent each pixel, and it returns the data – as is common for numeric variables in R – in an array of double precision floating point numbers, with the integer values (whose theoretical range is from 0 to \(2^{16}-1=65535\)) stored in the mantissa of the floating point representation and the exponents chosen so that the theoretical range is mapped to the interval \([0,1]\). However, the scanner that was used to create these images only used the lower 11 or 12 bits, and this explains the small maximum values in the images. We can rescale these data to approximately cover the range \([0,1]\) as follows8.</p>
<p>8 The function <code>normalize</code> provides a more flexible interface to the scaling of images.</p>
<pre><code>cells[,,1]   = 32 * cells[,,1]
cells[,,2:3] = 16 * cells[,,2:3]
apply(cells, 3, range)__


     image-DAPI image-FITC  image-Cy3
[1,] 0.05078202 0.04638743 0.02661173
[2,] 0.99855039 0.99977111 0.89137102</code></pre>
<p>We can keep in mind that these multiplications with a multiple of 2 have no impact on the underlying precision of the stored data.</p>
</section>
<section id="noise-reduction-by-smoothing" class="level3" data-number="13.7.2">
<h3 data-number="13.7.2" class="anchored" data-anchor-id="noise-reduction-by-smoothing"><span class="header-section-number">13.7.2</span> 11.8.2 Noise reduction by smoothing</h3>
<p>Now we are ready to get going with analyzing the images. As our first goal is segmentation of the images to identify the individual cells, we can start by removing local artifacts or noise from the images through smoothing. An intuitive approach is to define a window of a selected size around each pixel and average the values within that window. After applying this procedure to all pixels, the new, smoothed image is obtained. Mathematically, we can express this as</p>
<p>\[ f^*(x,y) = <em>{s=-a}^{a}</em>{t=-a}^{a} f(x+s, y+t), \]</p>
<p>where \(f(x,y)\) is the value of the pixel at position \(x\), \(y\), and \(a\) determines the window size, which is \(2a+1\) in each direction. \(N=(2a+1)^2\) is the number of pixels averaged over, and \(f^*\) is the new, smoothed image.</p>
<p>More generally, we can replace the moving average by a weighted average, using a weight function \(w\), which typically has highest weight at the window midpoint (\(s=t=0\)) and then decreases towards the edges.</p>
<p>\[ (w * f)(x,y) = <em>{s=-}^{+} </em>{t=-}^{+} w(s,t), f(x+s, y+s) \]</p>
<p>For notational convenience, we let the summations range from \(-\) to \(\), even if in practice the sums are finite as \(w\) has only a finite number of non-zero values. In fact, we can think of the weight function \(w\) as another image, and this operation is also called the <em>convolution</em> of the images \(f\) and \(w\), indicated by the the symbol \(*\). In <strong><a href="https://bioconductor.org/packages/EBImage/">EBImage</a></strong> , the 2-dimensional convolution is implemented by the function <code>filter2</code>, and the auxiliary function <code>makeBrush</code> can be used to generate weight functions \(w\).</p>
<pre><code>w = makeBrush(size = 51, shape = "gaussian", sigma = 7)
nucSmooth = filter2(getFrame(cells, 1), w)__</code></pre>
<p><a href="11-chap_files/figure- html/fig-nucSmooth-1.png" title="Figure 11.9: nucSmooth, a smoothed version of the DNA channel in the image object cells (the original version is shown in the leftmost panel of Figure fig-LauferCells)."><img src="11-chap_files/figure-html/fig-nucSmooth-1.png" class="img-fluid"></a></p>
<p>Figure 11.9: <code>nucSmooth</code>, a smoothed version of the DNA channel in the image object <code>cells</code> (the original version is shown in the leftmost panel of Figure 11.8).</p>
<p>__</p>
<p>Question 11.9</p>
<p>How does the weight matrix <code>w</code> look like?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>See Figure 11.10</p>
<pre><code>library("tibble")
library("ggplot2")
tibble(w = w[(nrow(w)+1)/2, ]) |&gt;
  ggplot(aes(y = w, x = seq(along = w))) + geom_point()__</code></pre>
<p><a href="11-chap_files/figure- html/fig-image-filter2-1.png" title="Figure 11.10: The middle row of the weight matrix, w[26, ]."><img src="11-chap_files/figure-html/fig-image-filter2-1.png" class="img-fluid"></a></p>
<p>Figure 11.10: The middle row of the weight matrix, <code>w[</code>26<code>, ]</code>.</p>
<p>In fact, the <code>filter2</code> function does not directly perform the summation indicated in Equation 11.2. Instead, it uses the Fast Fourier Transformation in a way that is mathematically equivalent and computationally more efficient.</p>
<p>The convolution in Equation 11.2 is a <em>linear</em> operation, in the sense that \(w<em>(c_1f_1+c_2f_2)= c_1w</em>f_1 + c_2w*f_2\) for any two images \(f_1\), \(f_2\) and numbers \(c_1\), \(c_2\). There is beautiful and powerful theory underlying linear filters (<a href="16-chap.html#ref-FoundationSignalProcessing">Vetterli, Kovačević, and Goyal 2014</a>).</p>
<p>To proceed we now use smaller smoothing bandwidths than what we displayed in Figure 11.9 for demonstration. Let’s use a <code>sigma</code> of 1 pixel for the DNA channel and 3 pixels for actin and tubulin.</p>
<pre><code>cellsSmooth = Image(dim = dim(cells))
sigma = c(1, 3, 3)
for(i in seq_along(sigma))
  cellsSmooth[,,i] = filter2( cells[,,i],
         filter = makeBrush(size = 51, shape = "gaussian",
                            sigma = sigma[i]) )__</code></pre>
<p>The smoothed images have reduced pixel noise, yet still the needed resolution.</p>
</section>
</section>
<section id="adaptive-thresholding" class="level2" data-number="13.8">
<h2 data-number="13.8" class="anchored" data-anchor-id="adaptive-thresholding"><span class="header-section-number">13.8</span> 11.9 Adaptive thresholding</h2>
<p>The idea of adaptive thresholding is that, compared to straightforward thresholding as we did for Figure 11.6b, the threshold is allowed to be different in different regions of the image. In this way, one can anticipate spatial dependencies of the underlying background signal caused, for instance, by uneven illumination or by stray signal from nearby bright objects. In fact, we have already seen an example for uneven background in the bottom right image of Figure 11.3.</p>
<p>Our colon cancer images (Figure 11.8) do not have such artefacts, but for demonstration, let’s simulate uneven illumination by multiplying the image with a two-dimensional bell function <code>illuminationGradient</code>, which has highest value in the middle and falls off to the sides (Figure 11.11).</p>
<pre><code>py = seq(-1, +1, length.out = dim(cellsSmooth)[1])
px = seq(-1, +1, length.out = dim(cellsSmooth)[2])
illuminationGradient = Image(outer(py, px, function(x, y) exp(-(x^2 + y^2))))
nucBadlyIlluminated = cellsSmooth[,,1] * illuminationGradient __</code></pre>
<p>We now define a smoothing window, <code>disc</code>, whose size is 21 pixels, and therefore bigger than the nuclei we want to detect, but small compared to the length scales of the illumination artifact. We use it to compute the image <code>localBackground</code> (shown in Figure 11.11 (c)) and the thresholded image <code>nucBadThresh</code>.</p>
<pre><code>disc = makeBrush(21, "disc")
disc = disc / sum(disc)
localBackground = filter2(nucBadlyIlluminated, disc)
offset = 0.02
nucBadThresh = (nucBadlyIlluminated - localBackground &gt; offset)__</code></pre>
<p><a href="11-chap_files/figure- html/fig-illumination-1.png" title="Figure 11.11 (a):"><img src="11-chap_files/figure-html/fig-illumination-1.png" class="img-fluid"></a></p>
<ol type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-illumination-2.png" title="Figure 11.11 (b):"><img src="11-chap_files/figure-html/fig-illumination-2.png" class="img-fluid"></a></p>
<ol start="2" type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-illumination-3.png" title="Figure 11.11 (c):"><img src="11-chap_files/figure-html/fig-illumination-3.png" class="img-fluid"></a></p>
<ol start="3" type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-illumination-4.png" title="Figure 11.11 (d):"><img src="11-chap_files/figure-html/fig-illumination-4.png" class="img-fluid"></a></p>
<ol start="4" type="a">
<li></li>
</ol>
<p>Figure 11.11: a: <code>illuminationGradient</code>, a function that has its maximum at the center and falls off towards the sides, and which simulates uneven illumination sometimes seen in images. (b) <code>nucBadlyIlluminated</code>, the image that results from multiplying the DNA channel in <code>cellsSmooth</code> with <code>illuminationGradient</code>. (c) <code>localBackground</code>, the result of applying a linear filter with a bandwidth that is larger than the objects to be detected. (d) <code>nucBadThresh</code>, the result of adaptive thresholding. The nuclei at the periphery of the image are reasonably well identified, despite the drop off in signal strength.</p>
<p>After having seen that this may work, let’s do the same again for the actual (not artificially degraded) image, as we need this for the next steps.</p>
<pre><code>nucThresh = (cellsSmooth[,,1] - filter2(cellsSmooth[,,1], disc) &gt; offset)__</code></pre>
<p>By comparing each pixel’s intensity to a background determined from the values in a local neighborhood, we assume that the objects are relatively sparse distributed in the image, so that the signal distribution in the neighborhood is dominated by background. For the nuclei in our images, this assumption makes sense, for other situations, you may need to make different assumptions. The adaptive thresholding that we have done here uses a linear filter, <code>filter2</code>, and therefore amounts to (weighted) local averaging. Other distribution summaries, e.g.&nbsp;the median or a low quantile, tend to be preferable, even if they are computationally more expensive. For local median filtering, <strong><a href="https://bioconductor.org/packages/EBimage/">EBimage</a></strong> provides the function <code>medianFilter</code>.</p>
</section>
<section id="morphological-operations-on-binary-images" class="level2" data-number="13.9">
<h2 data-number="13.9" class="anchored" data-anchor-id="morphological-operations-on-binary-images"><span class="header-section-number">13.9</span> 11.10 Morphological operations on binary images</h2>
<p>The thresholded image <code>nucThresh</code> (shown in the left panel of Figure [morphop] is not yet satisfactory. The boundaries of the nuclei are slightly rugged, and there is noise at the single-pixel level. An effective and simple way to remove these nuisances is given by a set of morphological operations (<a href="16-chap.html#ref-MathematicalMorphology">Serra 1983</a>).</p>
<p>Provided a binary image (with values, say, 0 and 1, representing back- and foreground pixels), and a binary mask9 (which is sometimes also called the structuring element), these operations work as follows.</p>
<p>9 An example for a mask is a circle with a given radius, or more precisely, the set of pixels within a certain distance from a center pixel.</p>
<ul>
<li><p><code>erode</code>: For every foreground pixel, put the mask around it, and if any pixel under the mask is from the background, then set all these pixels to background.</p></li>
<li><p><code>dilate</code>: For every background pixel, put the mask around it, and if any pixel under the mask is from the foreground, then set all these pixels to foreground.</p></li>
<li><p><code>open</code>: perform <code>erode</code> followed by <code>dilate</code>.</p></li>
</ul>
<p>We can also think of these operations as filters, however, in contrast to the linear filters of Section 11.8 they operate on binary images only, and there is no linearity.</p>
<p>Let us apply morphological opening to our image.</p>
<pre><code>nucOpened = EBImage::opening(nucThresh, kern = makeBrush(5, shape = "disc"))__</code></pre>
<p>The result of this is subtle, and you will have to zoom into the images in Figure 11.12 to spot the differences, but this operation manages to smoothen out some pixel-level features in the binary images that for our application are undesirable.</p>
</section>
<section id="segmentation-of-a-binary-image-into-objects" class="level2" data-number="13.10">
<h2 data-number="13.10" class="anchored" data-anchor-id="segmentation-of-a-binary-image-into-objects"><span class="header-section-number">13.10</span> 11.11 Segmentation of a binary image into objects</h2>
<p>The binary image <code>nucOpened</code> represents a segmentation of the image into foreground and background pixels, but not into individual nuclei. We can take one step further and extract individual objects defined as connected sets of pixels. In <strong><a href="https://bioconductor.org/packages/EBImage/">EBImage</a></strong> , there is a handy function for this purpose, <code>bwlabel</code>.</p>
<pre><code>nucSeed = bwlabel(nucOpened)
table(nucSeed)__


nucSeed
     0      1      2      3      4      5      6      7      8      9     10 
155408    511    330    120    468    222    121    125    159    116    520 
    11     12     13     14     15     16     17     18     19     20     21 
   115    184    179    116    183    187    303    226    164    309    194 
    22     23     24     25     26     27     28     29     30     31     32 
   148    345    287    203    379    371    208    222    320    443    409 
    33     34     35     36     37     38     39     40     41     42     43 
   493    256    169    225    376    214    228    341    269    119    315 </code></pre>
<p>The function returns an image, <code>nucSeed</code>, of integer values, where 0 represents the background, and the numbers from 1 to 43 index the different identified objects.</p>
<p>__</p>
<p>Question 11.10</p>
<p>What are the numbers in the above table?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>They correspond to the area (in pixels) of each of the objects. We could use this information to remove objects that are too large or too small compared to what we expect.</p>
<p>To visualize such images, the function <code>colorLabels</code> is convenient, which converts the (grayscale) integer image into a color image, using distinct, arbitrarily chosen colors for each object.</p>
<pre><code>EBImage::display(colorLabels(nucSeed))__</code></pre>
<p>This is shown in the middle panel of Figure 11.12. The result is already encouraging, although we can spot two types of errors:</p>
<ul>
<li><p>Some neighboring objects were not properly separated.</p></li>
<li><p>Some objects contain holes.</p></li>
</ul>
<p>Indeed, we could change the occurrences of these by playing with the disc size and the parameter <code>offset</code> in Section 11.9: making the offset higher reduces the probability that two neighboring object touch and are seen as one object by <code>bwlabel</code>; on the other hand, that leads to even more and even bigger holes. Vice versa for making it lower.</p>
<p>Segmentation is a rich and diverse field of research and engineering, with a large body of literature, software tools (<a href="16-chap.html#ref-schindelin2012fiji">Schindelin et al. 2012</a>; <a href="16-chap.html#ref-chaumont2012icy">Chaumont et al. 2012</a>; <a href="16-chap.html#ref-carpenter2006cellprofiler">Carpenter et al. 2006</a>; <a href="16-chap.html#ref-held2010cellcognition">Held et al. 2010</a>) and practical experience in the image analysis and machine learning communities. What is the adequate approach to a given task depends hugely on the data and the underlying question, and there is no universally best method. It is typically even difficult to obtain a “ground truth” or “gold standards” by which to evaluate an analysis – relying on manual annotation of a modest number of selected images is not uncommon. Despite the bewildering array of choices, it is easy to get going, and we need not be afraid of starting out with a simple solution, which we can successively refine. Improvements can usually be gained from methods that allow inclusion of more prior knowledge of the expected shapes, sizes and relations between the objects to be identified.</p>
<p>For statistical analyses of high-throughput images, we may choose to be satisfied with a simple method that does not rely on too many parameters or assumptions and results in a perhaps sub-optimal but rapid and good enough result (<a href="16-chap.html#ref-PhenoRipper">Rajaram et al.&nbsp;2012</a>). In this spirit, let us proceed with what we have. We generate a lenient foreground mask, which surely covers all nuclear stained regions, even though it also covers some regions between nuclei. To do so, we simply apply a second, less stringent adaptive thresholding.</p>
<pre><code>nucMask = cellsSmooth[,,1] - filter2(cellsSmooth[,,1], disc) &gt; 0 __</code></pre>
<p>and apply another morphological operation, <code>fillHull</code>, which fills holes that are surrounded by foreground pixels.</p>
<pre><code>nucMask = fillHull(nucMask)__</code></pre>
<p>To improve <code>nucSeed</code>, we can now <em>propagate</em> its segmented objects until they fill the mask defined by <code>nucMask</code>. Boundaries between nuclei, in those places where the mask is connected, can be drawn by Voronoi tessellation, which is implemented in the function <code>propagate</code>, and will be explained in the next section.</p>
<pre><code>nuclei = propagate(cellsSmooth[,,1], nucSeed, mask = nucMask)__</code></pre>
<p>The result is displayed in the rightmost panel of Figure 11.12.</p>
<p><a href="11-chap_files/figure- html/fig-morphop-1.png" title="Figure 11.12 (a):"><img src="11-chap_files/figure-html/fig-morphop-1.png" class="img-fluid"></a></p>
<ol type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-morphop-2.png" title="Figure 11.12 (b):"><img src="11-chap_files/figure-html/fig-morphop-2.png" class="img-fluid"></a></p>
<ol start="2" type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-morphop-3.png" title="Figure 11.12 (c):"><img src="11-chap_files/figure-html/fig-morphop-3.png" class="img-fluid"></a></p>
<ol start="3" type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-morphop-4.png" title="Figure 11.12 (d):"><img src="11-chap_files/figure-html/fig-morphop-4.png" class="img-fluid"></a></p>
<ol start="4" type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-morphop-5.png" title="Figure 11.12 (e):"><img src="11-chap_files/figure-html/fig-morphop-5.png" class="img-fluid"></a></p>
<ol start="5" type="a">
<li></li>
</ol>
<p>Figure 11.12: Different steps in the segmentation of the nuclei. From (a-e): <code>nucThresh</code>, <code>nucOpened</code>, <code>nucSeed</code>, <code>nucMask</code>, <code>nuclei</code>.</p>
</section>
<section id="voronoi-tessellation" class="level2" data-number="13.11">
<h2 data-number="13.11" class="anchored" data-anchor-id="voronoi-tessellation"><span class="header-section-number">13.11</span> 11.12 Voronoi tessellation</h2>
<p>Voronoi tessellation is useful if we have a set of seed points (or regions) and want to partition the space that lies between these seeds in such a way that each point in the space is assigned to its closest seed. As this is an intuitive and powerful idea, we’ll use this section for a short digression on it. Let us consider a basic example. We use the image <code>nuclei</code> as seeds. To call the function <code>propagate</code>, we also need to specify another image: for now we just provide a trivial image of all zeros, and we set the parameter <code>lambda</code> to a large positive value (we will come back to these choices).</p>
<pre><code>zeros        = Image(dim = dim(nuclei))
voronoiExamp = propagate(seeds = nuclei, x = zeros, lambda = 100)
voronoiPaint = paintObjects(voronoiExamp, 1 - nucOpened)__</code></pre>
<p><a href="11-chap_files/figure- html/fig-voronoiPaint-1.png" title="Figure 11.13: Example of a Voronoi segmentation, indicated by the gray lines, using the nuclei (indicated by black regions) as seeds."><img src="11-chap_files/figure-html/fig-voronoiPaint-1.png" class="img-fluid"></a></p>
<p>Figure 11.13: Example of a Voronoi segmentation, indicated by the gray lines, using the nuclei (indicated by black regions) as seeds.</p>
<p>__</p>
<p>Question 11.11</p>
<p>How do you select partition elements from the tessellation?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>The result, <code>voronoiExamp</code>, of the above call to <code>propagate</code> is simply an image of integers whose values indicate the different partitions.</p>
<pre><code>head(table(voronoiExamp))__


voronoiExamp
   1    2    3    4    5    6 
5645 4735  370 5964 3333 1377 


ind = which(voronoiExamp == 13, arr.ind = TRUE)
head(ind, 3)__


     row col
[1,] 112 100
[2,] 113 100
[3,] 114 100</code></pre>
<p>The result is shown in Figure 11.13. This looks interesting, but perhaps not yet as useful as the image <code>nuclei</code> in Figure [morphop]. We note that the basic definition of Voronoi tessellation, which we have given above, allows for two generalizations:</p>
<ul>
<li><p>By default, the space that we partition is the full, rectangular image area – but indeed we could restrict ourselves to any arbitrary subspace. This is akin to finding the shortest distance from each point to the next seed not in a simple flat landscape, but in a landscape that is interspersed by lakes and rivers (which you cannot cross), so that all paths need to remain on the land. <code>propagate</code> allows for this generalization through its <code>mask</code> parameter.</p></li>
<li><p>By default, we think of the space as flat – but in fact it could have hills and canyons, so that the distance between two points in the landscape not only depends on their \(x\)- and \(y\)-positions but also on the ascents and descents, up and down in \(z\)-direction, that lie in between. We can think of \(z\) as an “elevation”. You can specify such a landscape to <code>propagate</code> through its <code>x</code> argument.</p></li>
</ul>
<p>Mathematically, we say that instead of the simple default case (a flat rectangle, or image, with a Euclidean metric on it), we perform the Voronoi segmentation on a Riemann manifold that has a special shape and a special metric. Let us use the notation \(x\) and \(y\) for the column and row coordinates of the image, and \(z\) for the elevation. For two neighboring points, defined by coordinates \((x, y, z)\) and \((x+x, y+y, z+z)\), the distance \(s\) between them is thus not obtained by the usual Euclidean metric on the 2D image,</p>
<p>\[ s^2 = x^2 + y^2 \]</p>
<p>but instead</p>
<p>\[ s^2 = , \]</p>
<p>where the parameter \(\) is a real number \(\). To understand this, lets look at some important cases:</p>
<p>\[ ]</p>
<p>For \(\), the metric becomes the isotropic Euclidean metric, i.e., a movement in \(z\)-direction is equally “expensive” or “far” as in \(x\)- or \(y\)-direction. In the extreme case of \(\), only the \(z\)-movements matter, whereas lateral movements (in \(x\)- or \(y\)-direction) do not contribute to the distance. In the other extreme case, \(\), only lateral movements matter, and movement in \(z\)-direction is “free”. Distances between points further apart are obtained by summing \(s\) along the shortest path between them. The parameter \(\) serves as a convenient control of the relative weighting between sideways movement (along the \(x\) and \(y\) axes) and vertical movement. Intuitively, if you imagine yourself as a hiker in such a landscape, by choosing \(\) you can specify how much you are prepared to climb up and down to overcome a mountain, versus walking around it. When we used <code>lambda = 100</code> in our call to <code>propagate</code> at the begin of this section, this value was effectively infinite, so we were in the third boundary case of Equation 11.5.</p>
<p>For the purpose of cell segmentation, these ideas were put forward by Thouis Jones et al.&nbsp;(<a href="16-chap.html#ref- jones2005voronoi">Jones, Carpenter, and Golland 2005</a>; <a href="16-chap.html#ref- carpenter2006cellprofiler">Carpenter et al.&nbsp;2006</a>), who also wrote the efficient algorithm that is used by <code>propagate</code>.</p>
<p>__</p>
<p>Task</p>
<p>Try out the effect of using different \(\)s.</p>
</section>
<section id="segmenting-the-cell-bodies" class="level2" data-number="13.12">
<h2 data-number="13.12" class="anchored" data-anchor-id="segmenting-the-cell-bodies"><span class="header-section-number">13.12</span> 11.13 Segmenting the cell bodies</h2>
<p><a href="11-chap_files/figure-html/fig-histcellbody-1-1.png &quot;Figure 11.14: Histogram of the actin channel in cellsSmooth, after taking the logarithm.&quot;"><img src="11-chap_files/figure-html/fig- histcellbody-1-1.png" class="img-fluid"></a></p>
<p>Figure 11.14: Histogram of the actin channel in <code>cellsSmooth</code>, after taking the logarithm.</p>
<p><a href="11-chap_files/figure-html/fig-histcellbody-2-1.png &quot;Figure 11.15: Zoom into Figure fig-histcellbody-1.&quot;"><img src="11-chap_files/figure-html/fig- histcellbody-2-1.png" class="img-fluid"></a></p>
<p>Figure 11.15: Zoom into Figure 11.14.</p>
<p>To determine a mask of cytoplasmic area in the images, let us explore a different way of thresholding, this time using a global threshold which we find by fitting a mixture model to the data. The histograms show the distributions of the pixel intensities in the actin image. We look at the data on the logarithmic scale, and in Figure 11.15 zoom into the region where most of the data lie.</p>
<pre><code>hist(log(cellsSmooth[,,3]) )
hist(log(cellsSmooth[,,3]), xlim = -c(3.6, 3.1), breaks = 300)__</code></pre>
<p>Looking at the these histograms for many images, we can set up the following model for the purpose of segmentation: the signal in the cytoplasmic channels of the <em>Image</em> <code>cells</code> is a mixture of two distributions, a log-Normal background and a foreground with another, unspecified, rather flat, but mostly non-overlapping distribution10. Moreover the majority of pixels are from the background. We can then find robust estimates for the location and width parameters of the log-Normal component from the half range mode (implemented in the package <strong><a href="https://bioconductor.org/packages/genefilter/">genefilter</a></strong>) and from the root mean square of the values that lie left of the mode.</p>
<p>10 This is an application of the ideas we saw in <a href="04-chap.html">Chapter 4</a> on mixture models.</p>
<pre><code>library("genefilter")
bgPars = function(x) {
  x    = log(x)
  loc  = half.range.mode( x )
  left = (x - loc)[ x &lt; loc ]
  wid  = sqrt( mean(left^2) )
  c(loc = loc, wid = wid, thr = loc + 6*wid)
}
cellBg = apply(cellsSmooth, MARGIN = 3, FUN = bgPars)
cellBg __


           [,1]        [,2]        [,3]
loc -2.90176965 -2.94427499 -3.52191681
wid  0.00635322  0.01121337  0.01528207
thr -2.86365033 -2.87699477 -3.43022437</code></pre>
<p>The function defines as a threshold the location <code>loc</code> plus 6 widths <code>wid</code>11.</p>
<p>11 The choice of the number 6 here is ad hoc; we could make the choice of threshold more objective by estimating the weights of the two mixture components and assigning each pixel to either fore- or background based on its posterior probability according to the mixture model. More advanced segmentation methods use the fact that this is really a classification problem and include additional features and more complex classifiers to separate foreground and background regions (e.g., (<a href="16-chap.html#ref- ilastik:NatMeth:2019">Berg et al.&nbsp;2019</a>)).</p>
<pre><code>hist(log(cellsSmooth[,,3]), xlim = -c(3.6, 3.1), breaks = 300)
abline(v = cellBg[c("loc", "thr"), 3], col = c("brown", "red"))__</code></pre>
<p><a href="11-chap_files/figure-html/fig-histcellbody-3-1.png &quot;Figure 11.16: As in Figure fig-histcellbody-2, but with loc and thr shown by vertical lines.&quot;"><img src="11-chap_files/figure-html/fig- histcellbody-3-1.png" class="img-fluid"></a></p>
<p>Figure 11.16: As in Figure 11.15, but with <code>loc</code> and <code>thr</code> shown by vertical lines.</p>
<p>We can now define <code>cytoplasmMask</code> by the union of all those pixels that are above the threshold in the actin or tubulin image, or that we have already classified as nuclear in the image <code>nuclei</code>.</p>
<pre><code>cytoplasmMask = (cellsSmooth[,,2] &gt; exp(cellBg["thr", 2])) |
       nuclei | (cellsSmooth[,,3] &gt; exp(cellBg["thr", 3]))__</code></pre>
<p>The result is shown in the left panel of Figure 11.17. To define the cellular bodies, we can now simply extend the nucleus segmentation within this mask by the Voronoi tessellation based propagation algorithm of Section 11.12. This method makes sure that there is exactly one cell body for each nucleus, and the cell bodies are delineated in such a way that a compromise is reached between compactness of cell shape and following the actin and \(\)-tubulin intensity signal in the images. In the terminology of the <code>propagate</code> algorithm, cell shape is kept compact by the \(x\) and \(y\) components of the distance metric 11.4, and the actin signal is used for the \(z\) component. \(\) controls the trade-off.</p>
<pre><code>cellbodies = propagate(x = cellsSmooth[,,3], seeds = nuclei,
                       lambda = 1.0e-2, mask = cytoplasmMask)__</code></pre>
<p>As an alternative representation to the <code>colorLabel</code> plots, we can also display the segmentations of nuclei and cell bodies on top of the original images using the <code>paintObjects</code> function; the Images <code>nucSegOnNuc</code>, <code>nucSegOnAll</code> and <code>cellSegOnAll</code> that are computed below are show in the middle to right panels of Figure 11.17</p>
<pre><code>cellsColor = EBImage::rgbImage(red   = cells[,,3],
                               green = cells[,,2],
                               blue  = cells[,,1])
nucSegOnNuc  = paintObjects(nuclei, tgt = EBImage::toRGB(cells[,,1]), col = "#ffff00")
nucSegOnAll  = paintObjects(nuclei,     tgt = cellsColor,    col = "#ffff00")
cellSegOnAll = paintObjects(cellbodies, tgt = nucSegOnAll,   col = "#ff0080")__</code></pre>
<p><a href="11-chap_files/figure- html/fig-cellbodies-1.png" title="Figure 11.17 (a):"><img src="11-chap_files/figure-html/fig-cellbodies-1.png" class="img-fluid"></a></p>
<ol type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-cellbodies-2.png" title="Figure 11.17 (b):"><img src="11-chap_files/figure-html/fig-cellbodies-2.png" class="img-fluid"></a></p>
<ol start="2" type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-cellbodies-3.png" title="Figure 11.17 (c):"><img src="11-chap_files/figure-html/fig-cellbodies-3.png" class="img-fluid"></a></p>
<ol start="3" type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-cellbodies-4.png" title="Figure 11.17 (d):"><img src="11-chap_files/figure-html/fig-cellbodies-4.png" class="img-fluid"></a></p>
<ol start="4" type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-cellbodies-5.png" title="Figure 11.17 (e):"><img src="11-chap_files/figure-html/fig-cellbodies-5.png" class="img-fluid"></a></p>
<ol start="5" type="a">
<li></li>
</ol>
<p>Figure 11.17: Steps in the segmentation of the cell bodies. From (a-d): <code>cytoplasmMask</code>, <code>cellbodies</code> (blue: DAPI, red: actin, green: alpha-tubulin), <code>nucSegOnNuc</code>, <code>nucSegOnAll</code>, <code>cellSegOnAll</code>.</p>
</section>
<section id="feature-extraction" class="level2" data-number="13.13">
<h2 data-number="13.13" class="anchored" data-anchor-id="feature-extraction"><span class="header-section-number">13.13</span> 11.14 Feature extraction</h2>
<p>Now that we have the segmentations <code>nuclei</code> and <code>cellbodies</code> together with the original image data <code>cells</code>, we can compute various descriptors, or features, for each cell. We already saw in the beginning of Section 11.11 how to use the base R function <code>table</code> to determine the total number and sizes of the objects. Let us now take this further and compute the mean intensity of the DAPI signal (<code>cells[,,1]</code>) in the segmented nuclei, the mean actin intensity (<code>cells[,,3]</code>) in the segmented nuclei and the mean actin intensity in the cell bodies.</p>
<pre><code>meanNucInt       = tapply(cells[,,1], nuclei, mean)
meanActIntInNuc  = tapply(cells[,,3], nuclei, mean)
meanActIntInCell = tapply(cells[,,3], cellbodies, mean)__</code></pre>
<p>We can visualize the features in pairwise scatterplots (Figure 11.18). We see that they are correlated with each other, although each feature also carries independent information.</p>
<pre><code>library("GGally")
ggpairs(tibble(meanNucInt, meanActIntInNuc, meanActIntInCell))__</code></pre>
<p><a href="11-chap_files/figure- html/fig-pairsint-1.png" title="Figure 11.18: Pairwise scatterplots of per-cell intensity descriptors."><img src="11-chap_files/figure-html/fig-pairsint-1.png" class="img-fluid"></a></p>
<p>Figure 11.18: Pairwise scatterplots of per-cell intensity descriptors.</p>
<p>With a little more work, we could also compute more sophisticated summary statistics – e.g., the ratio of nuclei area to cell body area; or entropies, mutual information and correlation of the different fluorescent signals in each cell body, as more or less abstract measures of cellular morphology. Such measures can be used, for instance, to detect subtle drug induced changes of cellular architecture.</p>
<p>While it is easy and intuitive to perform these computations using basic R idioms like in the <code>tapply</code> expressions above, the package <strong><a href="https://bioconductor.org/packages/EBImage/">EBImage</a></strong> also provides the function <code>computeFeatures</code> which efficiently computes a large collection of features that have been commonly used in the literature (a pioneering reference is Boland and Murphy. (<a href="16-chap.html#ref-BolandMurphy">2001</a>)). Details about this function are described in its manual page, and an example application is worked through in the <strong><a href="https://bioconductor.org/packages/HD2013SGI/">HD2013SGI</a></strong> vignette. Below, we compute features for intensity, shape and texture for each cell from the DAPI channel using the nucleus segmentation (<code>nuclei</code>) and from the actin and tubulin channels using the cell body segmentation (<code>cytoplasmRegions</code>).</p>
<pre><code>F1 = computeFeatures(nuclei,     cells[,,1], xname = "nuc",  refnames = "nuc")
F2 = computeFeatures(cellbodies, cells[,,2], xname = "cell", refnames = "tub")
F3 = computeFeatures(cellbodies, cells[,,3], xname = "cell", refnames = "act")
dim(F1)__


[1] 43 89</code></pre>
<p><code>F1</code> is a matrix with 43 rows (one for each cell) and 89 columns, one for each of the computed features.</p>
<pre><code>F1[1:3, 1:5]__


  nuc.0.m.cx nuc.0.m.cy nuc.0.m.majoraxis nuc.0.m.eccentricity nuc.0.m.theta
1   119.5523   17.46895          44.86819            0.8372059     -1.314789
2   143.4511   15.83709          26.15009            0.6627672     -1.213444
3   336.5401   11.48175          18.97424            0.8564444      1.470913</code></pre>
<p>The column names encode the type of feature, as well the color channel(s) and segmentation mask on which it was computed. We can now use multivariate analysis methods – like those we saw in Chapters <a href="05-chap.html">5</a>, <a href="07-chap.html">7</a> and <a href="09-chap.html">9</a> – for many dfferent tasks, such as</p>
<ul>
<li><p>detecting cell subpopulations (clustering)</p></li>
<li><p>classifying cells into pre-defined cell types or phenotypes (classification)</p></li>
<li><p>seeing whether the absolute or relative frequencies of the subpopulations or cell types differ between images that correspond to different biological conditions</p></li>
</ul>
<p>In addition to these “generic” machine learning tasks, we also know the cell’s spatial positions, and in the following we will explore some ways to make use of these in our analyses.</p>
<p>__</p>
<p>Task</p>
<p>Use explorative multivariate methods to visualize the matrices <code>F1</code>, <code>F2</code>, <code>F3</code>: PCA, heatmap. What’s special about the “outlier” cells?</p>
</section>
<section id="spatial-statistics-point-processes" class="level2" data-number="13.14">
<h2 data-number="13.14" class="anchored" data-anchor-id="spatial-statistics-point-processes"><span class="header-section-number">13.14</span> 11.15 Spatial statistics: point processes</h2>
<p>In the previous sections, we have seen ways how to use images of cells to extract their positions and various shape and morphological features. We’ll now explore spatial distributions of the position. In order to have interesting data to work on, we’ll change datasets and look at breast cancer lymph node biopsies.</p>
<section id="a-case-study-interaction-between-immune-cells-and-cancer-cells" class="level3" data-number="13.14.1">
<h3 data-number="13.14.1" class="anchored" data-anchor-id="a-case-study-interaction-between-immune-cells-and-cancer-cells"><span class="header-section-number">13.14.1</span> 11.15.1 A case study: Interaction between immune cells and cancer cells</h3>
<p>The lymph nodes function as an immunologic filter for the bodily fluid known as lymph. Antigens are filtered out of the lymph in the lymph node before returning it to the circulation. Lymph nodes are found throughout the body, and are composed mostly of T cells, B cells, dendritic cells and macrophages. The nodes drain fluid from most of our tissues. The lymph ducts of the breast usually drain to one lymph node first, before draining through the rest of the lymph nodes underneath the arm. That first lymph node is called the <em>sentinel</em> lymph node. In a similar fashion as the spleen, the macrophages and dendritic cells that capture antigens present these foreign materials to T and B cells, consequently initiating an immune response.</p>
<p>T lymphocytes are usually divided into two major subsets that are functionally and phenotypically different.</p>
<ul>
<li><p>CD4+ T-cells, or T helper cells: they are pertinent coordinators of immune regulation. The main function of T helper cells is to augment or potentiate immune responses by the secretion of specialized factors that activate other white blood cells to fight off infection.</p></li>
<li><p>CD8+ T cells, or T killer/suppressor cells: these cells are important in directly killing certain tumor cells, viral-infected cells and sometimes parasites. The CD8+ T cells are also important for the down-regulation of immune responses.</p></li>
</ul>
<p>Both types of T cells can be found throughout the body. They often depend on the secondary lymphoid organs (the lymph nodes and spleen) as sites where activation occurs.</p>
<p>Dendritic Cells or CD1a cells are antigen-presenting cells that process antigen and present peptides to T cells.</p>
<p>Typing the cells can be done by staining the cells with protein antibodies that provide specific signatures. For instance, different types of immune cells have different proteins expressed, mostly in their cell membranes.</p>
<p><a href="imgs/SixPanelsLymphsmall.jpg &quot;Figure 11.19: Biopsy of an enlarged lymph node revealed an intact capsule and obliterated sinuses (upper left panel, stained with hematoxylin and eosin, original magnification \times 100). The infiltrate was composed of an admixture of small lymphocytes, macrophages, and plasma cells (upper right panel, hematoxylin and eosin, original magnification \times 400). The infiltrate was composed of a mixture of CD3 positive T-cells (including both CD4 and CD8 positive cells) and CD20 positive B-cells. Numerous macrophages were also CD4 positive. (From: Hurley et al., Diagnostic Pathology (2008) 3:13)&quot;"><img src="imgs/SixPanelsLymphsmall.jpg" class="img-fluid"></a></p>
<p>Figure 11.19: Biopsy of an enlarged lymph node revealed an intact capsule and obliterated sinuses (upper left panel, stained with hematoxylin and eosin, original magnification \(\) 100). The infiltrate was composed of an admixture of small lymphocytes, macrophages, and plasma cells (upper right panel, hematoxylin and eosin, original magnification \(\) 400). The infiltrate was composed of a mixture of CD3 positive T-cells (including both CD4 and CD8 positive cells) and CD20 positive B-cells. Numerous macrophages were also CD4 positive. (From: Hurley et al., Diagnostic Pathology (2008) 3:13)</p>
<p><a href="imgs/testscan_1_2_RGB99-4525D.jpg &quot;Figure 11.20: A stained lymph node; this image is the basis for the spatial data in brcalymphnode.&quot;"><img src="imgs/testscan_1_2_RGB99-4525D.jpg" class="img-fluid"></a></p>
<p>Figure 11.20: A stained lymph node; this image is the basis for the spatial data in <code>brcalymphnode</code>.</p>
<p>We’ll look at data by Setiadi et al.&nbsp;(<a href="16-chap.html#ref-Setiadi2010">2010</a>). After segmentating the image shown in Figure 11.20 using the segmentation method <em>GemIdent</em> (<a href="16-chap.html#ref- Holmes2009">Holmes, Kapelner, and Lee 2009</a>), the authors obtained the coordinates and the type of all the cells in the image. We call this type of data a <em>marked point process</em> , and it can be seen as a simple table with 3 columns.</p>
<pre><code>library("readr")
library("dplyr")
cellclasses = c("T_cells", "Tumor", "DCs", "other_cells")
brcalymphnode = lapply(cellclasses, function(k) {
    read_csv(file.path("..", "data", sprintf("99_4525D-%s.txt", k))) |&gt;
    transmute(x = globalX, y = globalY, class = k)
}) |&gt; bind_rows() |&gt; mutate(class = factor(class))

brcalymphnode __


# A tibble: 209,462 × 3
       x     y class  
   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;  
 1  6355 10382 T_cells
 2  6356 10850 T_cells
 3  6357 11070 T_cells
 4  6357 11082 T_cells
 5  6358 10600 T_cells
 6  6361 10301 T_cells
 7  6369 10309 T_cells
 8  6374 10395 T_cells
 9  6377 10448 T_cells
10  6379 10279 T_cells
# ℹ 209,452 more rows


table(brcalymphnode$class)__


        DCs other_cells     T_cells       Tumor 
        878       77081      103681       27822 </code></pre>
<p>We see that there are over a 100,000 T cells, around 28,000 tumor cells, and only several hundred dendritic cells. Let’s plot the \(x\)- and \(y\)-positions of the cells (Figure 11.21).</p>
<pre><code>ggplot(filter(brcalymphnode, class %in% c("T_cells", "Tumor")),
   aes(x = x, y = y, col = class)) + geom_point(shape = ".") +
   facet_grid( . ~ class) + guides(col = "none")__</code></pre>
<p><a href="11-chap_files/figure- html/fig-brcalntcells-1.png" title="Figure 11.21: Scatterplot of the x and y positions of the T- and tumor cells in brcalymphnode. The locations were obtained by a segmentation algorithm from a high resolution version of Figure fig-stainedlymphnode. Some rectangular areas in the T-cells plot are suspiciously empty, this could be because the corresponding image tiles within the overall composite image went missing, or were not analyzed."><img src="11-chap_files/figure-html/fig-brcalntcells-1.png" class="img-fluid"></a></p>
<p>Figure 11.21: Scatterplot of the \(x\) and \(y\) positions of the T- and tumor cells in <code>brcalymphnode</code>. The locations were obtained by a segmentation algorithm from a high resolution version of Figure 11.20. Some rectangular areas in the T-cells plot are suspiciously empty, this could be because the corresponding image tiles within the overall composite image went missing, or were not analyzed.</p>
<p>__</p>
<p>Question 11.12</p>
<p>Compare Figures 11.20 and 11.21. Why are the \(y\)-axis inverted relative to each other?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Figure 11.20 follows the convention for image data, where the origin is in the top left corner of the image, while Figure 11.21 follows the convention for Cartesian plots, with the origin at the bottom left.</p>
<p>To use the functionality of the <strong><a href="https://cran.r-project.org/web/packages/spatstat/">spatstat</a></strong> package, it is convenient to convert our data in <code>brcalymphnode</code> into an object of class <em>ppp</em> ; we do this by calling the eponymous function.</p>
<pre><code>library("spatstat")__


ln = with(brcalymphnode, ppp(x = x, y = y, marks = class, 
                             xrange = range(x), yrange = range(y)))
ln __


Marked planar point pattern: 209462 points
Multitype, with levels = DCs, other_cells, T_cells, Tumor 
window: rectangle = [3839, 17276] x [6713, 23006] units</code></pre>
<p><em>ppp</em> objects are designed to capture realizations of a <strong>spatial point process</strong> , that is, a set of isolated points located in a mathematical space; in our case, as you can see above, the space is a two-dimensional rectangle that contains the range of the \(x\)- and \(y\)-coordinates. In addition, the points can be <em>marked</em> with certain properties. In <code>ln</code>, the mark is simply the <em>factor</em> variable <code>class</code>. More generally, it could be several attributes, times, or quantitative data as well. There are similarities between a marked point process and an image, although for the former, the points can lie anywhere within the space, whereas in an image, the pixels are covering the space in regular, rectangular way.</p>
</section>
<section id="convex-hull" class="level3" data-number="13.14.2">
<h3 data-number="13.14.2" class="anchored" data-anchor-id="convex-hull"><span class="header-section-number">13.14.2</span> 11.15.2 Convex hull</h3>
<p>Above, we (implicitly) confined the point process to lie in a rectangle. In fact, the data generating process is more confined, by the shape of the tissue section. We can approximate this and compute a tighter region from the convex hull of the points12.</p>
<p>12 You can use <code>str(cvxhull)</code> to look at the internal structure of this S3 object.</p>
<pre><code>cvxhull = convexhull.xy(cbind(ln$x, ln$y))
ggplot(as_tibble(cvxhull$bdry[[1]]), aes(x = x, y = y)) +
  geom_polygon(fill = NA, col = "black") + geom_point() + coord_fixed()__</code></pre>
<p><a href="11-chap_files/figure- html/fig-convhull-1.png" title="Figure 11.22: Polygon describing the convex hull of the points in ln."><img src="11-chap_files/figure-html/fig-convhull-1.png" class="img-fluid"></a></p>
<p>Figure 11.22: Polygon describing the convex hull of the points in <code>ln</code>.</p>
<p>We can see the polygon in Figure 11.22 and now call <code>ppp</code> again, this time with the polygon.</p>
<pre><code>ln = with(brcalymphnode, ppp(x = x, y = y, marks = class, 
                             poly = cvxhull$bdry[[1]]))
ln __


Marked planar point pattern: 209462 points
Multitype, with levels = DCs, other_cells, T_cells, Tumor 
window: polygonal boundary
enclosing rectangle: [3839, 17276] x [6713, 23006] units</code></pre>
</section>
<section id="other-ways-of-defining-the-space-for-the-point-process" class="level3" data-number="13.14.3">
<h3 data-number="13.14.3" class="anchored" data-anchor-id="other-ways-of-defining-the-space-for-the-point-process"><span class="header-section-number">13.14.3</span> 11.15.3 Other ways of defining the space for the point process</h3>
<p>We do not have to use the convex hull to define the space on which the point process is considered. Alternatively, we could have provided an image mask to <code>ppp</code> that defines the space based on prior knowledge; or we could use density estimation on the sampled points to only identify a region in which there is a high enough point density, ignoring sporadic outliers. These choices are part of the analyst’s job when considering spatial point processes.</p>
</section>
</section>
<section id="first-order-effects-the-intensity" class="level2" data-number="13.15">
<h2 data-number="13.15" class="anchored" data-anchor-id="first-order-effects-the-intensity"><span class="header-section-number">13.15</span> 11.16 First order effects: the intensity</h2>
<p>One of the most basic questions of spatial statistics is whether neighboring points are “clustering”, i.e., whether and to what extent they are closer to each other than expected “by chance”; or perhaps the opposite, whether they seem to repel each other. There are many examples where this kind of question can be asked, for instance</p>
<ul>
<li><p>crime patterns within a city,</p></li>
<li><p>disease patterns within a country,</p></li>
<li><p>soil measurements in a region.</p></li>
</ul>
<p>It is usually not hard to find reasons why such patterns exist: good and bad neighborhoods, local variations in lifestyle or environmental exposure, the common geological history of the soil. Sometimes there may also be mechanisms by which the observed events attract or repel each other – the proverbial “broken windows” in a neighborhood, or the tendency of many cell types to stick close to other cells.</p>
<p>The cell example highlights that spatial clustering (or anticlustering) can depend on the objects’ attributes (or marks, in the parlance of spatial point processes). It also highlights that the answer can depend on the length scale considered. Even if cells attract each other, they have a finite size, and cannot occupy the same space. So there will be some minmal distance between them, on the scale of which they essentially repel each other, while at further distances, they attract.</p>
<p>To attack these questions more quantitatively, we need to define a probabilistic model of what we expect <em>by chance</em>. Let’s count the number of points lying in a subregion, say, a circle of area \(a\) around a point \(p=(x,y)\); call this \(N(p, a)\)13 The mean and covariance of \(N\) provide first and second order properties. The first order is the intensity of the process:</p>
<p>13 As usual, we use the uppercase notation \(N(p, a)\) for the random variable, and the lowercase \(n(p, a)\) for its realizations, or samples.</p>
<p>\[ (p) = _{a} . \]</p>
<p>Here we used infinitesimal calculus to define the local intensity \((p)\). As for time series, a stationary process is one where we have homogeneity all over the region, i.e., \((p) = \); then the intensity in an area \(A\) is proportional to the area: \(E[N(, A)] = A\). Later we’ll also look at higher order statistics, such as the spatial covariance</p>
<p>\[ (p_1, p_2) = _{a } . \]</p>
<p>If the process is stationary, this will only depend on the relative position of the two points (the vector between them). If it only depends on the distance, i.e., only on the length but not on the direction of the vector, it is called second order isotropic.</p>
<p><a href="imgs/Rain-Drops-small.jpg" title="Figure 11.23: Rain drops falling on the floor are modelled by a Poisson process. The number of drops falling on a particular spot only depends on the rate \lambda (and on the size of the spot), but not on what happens at other spots."><img src="imgs/Rain-Drops-small.jpg" class="img-fluid"></a></p>
<p>Figure 11.23: Rain drops falling on the floor are modelled by a Poisson process. The number of drops falling on a particular spot only depends on the rate \(\) (and on the size of the spot), but not on what happens at other spots.</p>
<section id="poisson-process" class="level3" data-number="13.15.1">
<h3 data-number="13.15.1" class="anchored" data-anchor-id="poisson-process"><span class="header-section-number">13.15.1</span> 11.16.1 Poisson Process</h3>
<p>The simplest spatial process is the Poisson process. We will use it as a null model against which to compare our data. It is stationary with intensity \(\), and there are no further dependencies between occurrences of points in non-overlapping regions of the space. Moreover, the number of points in a region of area \(A\) follows a Poisson distribution with rate \(A\).</p>
</section>
<section id="estimating-the-intensity" class="level3" data-number="13.15.2">
<h3 data-number="13.15.2" class="anchored" data-anchor-id="estimating-the-intensity"><span class="header-section-number">13.15.2</span> 11.16.2 Estimating the intensity</h3>
<p>To estimate the intensity, divide up the area into subregions, small enough to see potential local variations of \((p)\), but big enough to contain a sufficient sample of points. This is analogous to 2D density estimation, and instead of hard region boundaries, we can use a smooth kernel function \(K\).</p>
<p>\[ (p) = _i e(p_i) K(p-p_i). \]</p>
<p>The kernel function depends on a smoothing parameter, \(\), the larger it is, the larger the regions over which we compute the local estimate for each \(p\). \(e(p)\) is an edge correction factor, and takes into account the estimation bias caused when the support of the kernel (the “smoothing window”) would fall outside the space on which the point process is defined. The function <code>density</code>, which is defined for <em>ppp</em> objects in the <strong><a href="https://cran.r-project.org/web/packages/spatstat/">spatstat</a></strong> package, implements Equation 11.8.</p>
<pre><code>d = density(subset(ln, marks == "Tumor"), edge=TRUE, diggle=TRUE)
plot(d)__</code></pre>
<p><a href="11-chap_files/figure- html/fig-densityppp1-1.png" title="Figure 11.24: Intensity estimate for the cells marked Tumor in ppp. The support of the estimate is the polygon that we specified earlier on (Figure fig-convhull)."><img src="11-chap_files/figure-html/fig-densityppp1-1.png" class="img-fluid"></a></p>
<p>Figure 11.24: Intensity estimate for the cells marked <code>Tumor</code> in <code>ppp</code>. The support of the estimate is the polygon that we specified earlier on (Figure 11.22).</p>
<p>The plot is shown in Figure 11.24.</p>
<p>__</p>
<p>Question 11.13</p>
<p>How does the estimate look without edge correction?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>d0 = density(subset(ln, marks == "Tumor"), edge = FALSE)
plot(d0)__</code></pre>
<p><a href="11-chap_files/figure- html/fig-densityppp0-1.png" title="Figure 11.25: As Figure fig-densityppp1, but without edge correction."><img src="11-chap_files/figure-html/fig-densityppp0-1.png" class="img-fluid"></a></p>
<p>Figure 11.25: As Figure 11.24, but without edge correction.</p>
<p>Now estimated intensity is smaller towards the edge of the space, reflecting edge bias (Figure 11.25).</p>
<p><code>density</code> gives us as estimate of the <em>intensity</em> of the point process. A related, but different task is the estimation of the (conditional) <em>probability</em> of being a particular cell class. The function <code>relrisk</code> computes a nonparametric estimate of the spatially varying risk of a particular event type. We’re interested in the probability that a cell that is present at particular spatial location will be a tumor cell (Figure 11.26).</p>
<pre><code>rr = relrisk(ln, sigma = 250)__


plot(rr)__</code></pre>
<p><a href="11-chap_files/figure- html/fig-relrisk-1.png" title="Figure 11.26: Estimates of the spatially varying probability of each of the cell classes, conditional on there being cells."><img src="11-chap_files/figure-html/fig-relrisk-1.png" class="img-fluid"></a></p>
<p>Figure 11.26: Estimates of the spatially varying probability of each of the cell classes, conditional on there being cells.</p>
</section>
</section>
<section id="second-order-effects-spatial-dependence" class="level2" data-number="13.16">
<h2 data-number="13.16" class="anchored" data-anchor-id="second-order-effects-spatial-dependence"><span class="header-section-number">13.16</span> 11.17 Second order effects: spatial dependence</h2>
<p>If we pick a point at random in our spatial process, what is the distance \(W\) to its nearest neighbor? For a homogenous Poisson process, the cumulative distribution function of this distance is</p>
<p>\[ G(w) = P(Ww) = 1-e<sup>{-w</sup>2}. \]</p>
<p>Plotting \(G\) gives a way of noticing departure from the homogenous Poisson process. An estimator of \(G\), which also takes into account edge effects (<a href="16-chap.html#ref-Baddeley1998">A. J. Baddeley 1998</a>; <a href="16-chap.html#ref-RipleySISS1988">Ripley 1988</a>), is provided by the function <code>Gest</code> of the <strong><a href="https://cran.r-project.org/web/packages/spatstat/">spatstat</a></strong> package.</p>
<pre><code>gln = Gest(ln)
gln __


Function value object (class 'fv')
for the function r -&gt; G(r)
.....................................................................
        Math.label      Description                                  
r       r               distance argument r                          
theo    G[pois](r)      theoretical Poisson G(r)                     
han     hat(G)[han](r)  Hanisch estimate of G(r)                     
rs      hat(G)[bord](r) border corrected estimate of G(r)            
km      hat(G)[km](r)   Kaplan-Meier estimate of G(r)                
hazard  hat(h)[km](r)   Kaplan-Meier estimate of hazard function h(r)
theohaz h[pois](r)      theoretical Poisson hazard function h(r)     
.....................................................................
Default plot formula:  .~r
where "." stands for 'km', 'rs', 'han', 'theo'
Recommended range of argument r: [0, 20.998]
Available range of argument r: [0, 52.443]


library("RColorBrewer")
plot(gln, xlim = c(0, 10), lty = 1, col = brewer.pal(4, "Set1"))__</code></pre>
<p><a href="11-chap_files/figure-html/fig- Gest-1.png" title="Figure 11.27: Estimates of G, using three different edge effect corrections –which here happen to essentially lie on top of each other– and the theoretical distribution for a homogenous Poisson process."><img src="11-chap_files/figure-html/fig-Gest-1.png" class="img-fluid"></a></p>
<p>Figure 11.27: Estimates of \(G\), using three different edge effect corrections –which here happen to essentially lie on top of each other– and the theoretical distribution for a homogenous Poisson process.</p>
<p>The printed summary of the object <code>gln</code> gives an overview of the computed estimates; further explanations are in the manual page of <code>Gest</code>. In Figure 11.27 we see that the empirical distribution function and that of our null model, a homogenous Poisson process with a suitably chosen intensity, cross at around 4.5 units. Cell to cell distances that are shorter than this value are less likely than for the null model, in particular, there are essentially no distances below around 2; this, of course, reflects the fact that our cells have finite size and cannot overlap the same space. There seems to be trend to avoid very large distances –compared to the Poisson process–, perhaps indicative of a tendency of the cells to cluster.</p>
<section id="ripleys-k-function" class="level3" data-number="13.16.1">
<h3 data-number="13.16.1" class="anchored" data-anchor-id="ripleys-k-function"><span class="header-section-number">13.16.1</span> 11.17.1 Ripley’s \(K\) function</h3>
<p>In homogeneous spatial Poisson process, if we randomly pick any point and count the number of points within a distance of at most \(r\), we expect this number to grow as the area of the circle, \(r^2\). For a given dataset, we can compare this expectation to the observed number of neighbors within distance \(r\), averaged across all points.</p>
<p>The \(K\) function (variously called <em>Ripley’s \(K\)-function</em> or the <em>reduced second moment function</em>) of a stationary point process is defined so that \(K(r)\) is the expected number of (additional) points within a distance \(r\) of a given, randomly picked point. Remember that \(\) is the intensity of the process, i.e., the expected number of points per unit area. The \(K\) function is a second order moment property of the process.</p>
<p>The definition of \(K\) can be generalized to inhomogeneous point processes and written as in (<a href="16-chap.html#ref-Baddeley2000">A. Baddeley, Moller, and Waagepetersen 2000</a>),</p>
<p>\[ K_{}(r)= <em>{i,j} 𝟙</em>{d(p_i, p_j) r} { (x_i) (x_j) }, \]</p>
<p>where \(d(p_i, p_j)\) is the distance between points \(p_i\) and \(p_j\), and \(e(p_i, p_j, r)\) is an edge correction factor14. For estimation and visualisation, it is useful to consider a transformation of \(K\) (and analogously, of \(K_{}\)), the so- called \(L\) function.</p>
<p>14 See the manual page of <code>Kinhom</code> for more.</p>
<p>\[ L(r)=. \]</p>
<p>For a homogeneous spatial Poisson process, the theoretical value is \(L(r) = r\). By comparing that to the estimate of \(L\) for a dataset, we can learn about inter-point dependence and spatial clustering. The square root in Equation 11.11 has the effect of stabilising the variance of the estimator, so that compared to \(K\), \(L\) is more appropriate for data analysis and simulations. The computations in the function <code>Linhom</code> of the <strong><a href="https://cran.r-project.org/web/packages/spatstat/">spatstat</a></strong> package take a few minutes for our data (Figure 11.28).</p>
<pre><code>Lln = Linhom(subset(ln, marks == "T_cells"))__


Lln __


Function value object (class 'fv')


for the function r -&gt; L[inhom](r)


................................................................................
           Math.label                
r          r                         
theo       L[pois](r)                
border     {hat(L)[inhom]^{bord}}(r) 
bord.modif {hat(L)[inhom]^{bordm}}(r)
           Description                                      
r          distance argument r                              
theo       theoretical Poisson L[inhom](r)                  
border     border-corrected estimate of L[inhom](r)         
bord.modif modified border-corrected estimate of L[inhom](r)
................................................................................
Default plot formula:  .~.x
where "." stands for 'bord.modif', 'border', 'theo'
Recommended range of argument r: [0, 694.7]
Available range of argument r: [0, 694.7]


plot(Lln, lty = 1, col = brewer.pal(3, "Set1"))__</code></pre>
<p><a href="11-chap_files/figure- html/fig-images-Lln-1.png" title="Figure 11.28: Estimate of L_{\scriptsize \mbox{inhom}}, Equations eq-kinhom and eq-Lest, of the T cell pattern."><img src="11-chap_files/figure-html/fig-images-Lln-1.png" class="img-fluid"></a></p>
<p>Figure 11.28: Estimate of \(L_{}\), Equations 11.10 and 11.11, of the T cell pattern.</p>
<p>We could now proceed with looking at the \(L\) function also for other cell types, and for different tumors as well as for healthy lymph nodes. This is what Setiadi and colleagues did in their report (<a href="16-chap.html#ref-Setiadi2010">Setiadi et al. 2010</a>), where by comparing the spatial grouping patterns of T and B cells between healthy and breast cancer lymph nodes they saw that B cells appeared to lose their normal localization in the extrafollicular region of the lymph nodes in some tumors.</p>
<section id="the-pair-correlation-function" class="level4" data-number="13.16.1.1">
<h4 data-number="13.16.1.1" class="anchored" data-anchor-id="the-pair-correlation-function"><span class="header-section-number">13.16.1.1</span> The pair correlation function</h4>
<p>describes how point density varies as a function of distance from a reference point. It provides a perspective inspired by physics for looking at spatial clustering. For a stationary point process, it is defined as</p>
<p>\[ g(r)=(r). \]</p>
<p>For a stationary Poisson process, the pair correlation function is identically equal to 1. Values \(g(r) &lt; 1\) suggest inhibition between points; values greater than 1 suggest clustering.</p>
<p>The <strong><a href="https://cran.r-project.org/web/packages/spatstat/">spatstat</a></strong> package allows computing estimates of \(g\) even for inhomogeneous processes, if we call <code>pcf</code> as below, the definition 11.12 is applied to the estimate of \(K_{}\).</p>
<pre><code>pcfln = pcf(Kinhom(subset(ln, marks == "T_cells")))__


plot(pcfln, lty = 1)
plot(pcfln, lty = 1, xlim = c(0, 10))__</code></pre>
<p><a href="11-chap_files/figure- html/fig-images-pcf-1.png" title="Figure 11.29 (a):"><img src="11-chap_files/figure-html/fig-images-pcf-1.png" class="img-fluid"></a></p>
<ol type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-images-pcf-2.png" title="Figure 11.29 (b):"><img src="11-chap_files/figure-html/fig-images-pcf-2.png" class="img-fluid"></a></p>
<ol start="2" type="a">
<li></li>
</ol>
<p>Figure 11.29: Estimate of the pair correlation function, Equation 11.12, of the T cell pattern.</p>
<p>As we see in Figure 11.29, the T cells cluster, although at very short distances, there is also evidence for avoidance.</p>
<p>__</p>
<p>Question 11.14</p>
<p>The sampling resolution in the plot of the pair correlation function in the bottom panel of Figure 11.29 is low; how can it be increased?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>The answer lies in the <code>r</code> argument of the <code>Kinhom</code> function; see Figure 11.30.</p>
<pre><code>pcfln2 = pcf(Kinhom(subset(ln, marks == "T_cells"),
                    r = seq(0, 10, by = 0.2)))
plot(pcfln2, lty = 1)__</code></pre>
<p><a href="11-chap_files/figure- html/fig-samplingpcf-1.png" title="Figure 11.30: Answer to Question wrn-images- samplingpcf: as in the bottom panel of Figure fig-images-pcf, but with denser sampling."><img src="11-chap_files/figure-html/fig-samplingpcf-1.png" class="img-fluid"></a></p>
<p>Figure 11.30: Answer to Question 11.14: as in the bottom panel of Figure 11.29, but with denser sampling.</p>
</section>
</section>
</section>
<section id="summary-of-this-chapter" class="level2" data-number="13.17">
<h2 data-number="13.17" class="anchored" data-anchor-id="summary-of-this-chapter"><span class="header-section-number">13.17</span> 11.18 Summary of this chapter</h2>
<p>We learned to work with image data in R. Images are basically just arrays, and we can use familiar idioms to manipulate them. We can extract quantitative features from images, and then many of the analytical questions are not unlike those with other high-throughput data: we summarize the features into statistics such as means and variances, do hypothesis testing for differences between conditions, perform analysis of variance, apply dimension reduction, clustering and classification.</p>
<p>Often we want to compute such quantitative features not on the whole image, but for individual objects shown in the image, and then we need to first segment the image to demarcate the boundaries of the objects of interest. We saw how to do this for images of nuclei and cells.</p>
<p>When the interest is on the positions of the objects and how these positions relate to each other, we enter the realm of <em>spatial statistics</em>. We have explored some of the functionality of the <strong><a href="https://cran.r-project.org/web/packages/spatstat/">spatstat</a></strong> package, have encountered the point process class, and we learned some of the specific diagnostic statistics used for point patterns, like Ripley’s \(K\) function.</p>
</section>
<section id="further-reading" class="level2" data-number="13.18">
<h2 data-number="13.18" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">13.18</span> 11.19 Further reading</h2>
<ul>
<li><p>There is a vast amount of literature on image analysis. When navigating it, it is helpful to realize that the field is driven by two forces: specific application domains (we saw the analysis of high-throughput cell-based assays) and available computer hardware. Some algorithms and concepts that were developed in the 1970s are still relevant, others have been superseeded by more systematic and perhaps computationally more intensive methods. Many algorithms imply certain assumptions about the nature of the data and and scientific questions asked, which may be fine for one application, but need a fresh look in another. A classic introduction is <em>The Image Processing Handbook</em> (<a href="16-chap.html#ref-RussImageProcessingHandbook">Russ and Neal 2015</a>), which now is its seventh edition.</p></li>
<li><p>For spatial point pattern analysis, Diggle (<a href="16-chap.html#ref-DiggleSPP">2013</a>; <a href="16-chap.html#ref-RipleySISS1988">Ripley 1988</a>; <a href="16-chap.html#ref-CressieSSD1991">Cressie 1991</a>; <a href="16-chap.html#ref-MeckeSG2013">Chiu et al.&nbsp;2013</a>).</p></li>
</ul>
</section>
<section id="exercises" class="level2" data-number="13.19">
<h2 data-number="13.19" class="anchored" data-anchor-id="exercises"><span class="header-section-number">13.19</span> 11.20 Exercises</h2>
<p>__</p>
<p>Exercise 11.1</p>
<p>Load some images from your personal photo library into R and try out the manipulations from Section 11.6 on them.</p>
<p>__</p>
<p>Exercise 11.2</p>
<p>Explore the effect of the parameter <code>lambda</code> in the <code>propagate</code> function (Sections 11.12, 11.13) using a <strong><a href="https://cran.r-project.org/web/packages/shiny">shiny</a></strong> app that displays the <code>cellbodies</code> image as in Figure 11.17.</p>
<p>__</p>
<p>Exercise 11.3</p>
<p>Consider the two-dimensional empirical autocorrelation function,</p>
<p>\[ a(v_x, v_y) = _{(x,y)I} B(x, y);B(x+v_x, , y+v_y), \]</p>
<p>where \(B\) is an image, i.e., a function over the set of pixels \(I\), the tuple \((x,y)\) runs over all the pixel coordinates, and \(v=(v_x, v_y)\) is the offset vector. Using the <a href="https://mathworld.wolfram.com/Wiener-KhinchinTheorem.html">Wiener–Khinchin theorem</a>, we can compute this function efficiently using the Fast Fourier Transformation.</p>
<pre><code>autocorr2d = function(x) {
  y = fft(x/sum(x))
  abs(gsignal::fftshift(fft(y * Conj(y), inverse = TRUE), MARGIN = 1:2)) 
}__</code></pre>
<p>Below, we’ll use this little helper function, which shows a matrix as a heatmap with <strong><a href="https://cran.r-project.org/web/packages/ggplot2">ggplot2</a></strong> (similar to base R’s <code>image</code>).</p>
<pre><code>matrix_as_heatmap = function(m)
  ggplot(reshape2::melt(m), aes(x = Var1, y = Var2, fill = value)) + 
    geom_tile() + coord_fixed() +
    scale_fill_continuous(type = "viridis") +
    scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))__</code></pre>
<p>Now let’s apply <code>autocorr2d</code> to each of the three color channels separately. The result is shown in Figure 11.31.</p>
<pre><code>nm = dimnames(cells)[[3]]
ac = lapply(nm, function(i) autocorr2d(cells[,, i])) |&gt; setNames(sub("^image-", "", nm))

for (w in names(ac)) 
  print(matrix_as_heatmap(ac[[w]]) + ggtitle(w))

cy = dim(cells)[1] / 2
cx = dim(cells)[2] / 2
r  = round(sqrt((col(cells[,,1]) - cx)^2 + (row(cells[,,1]) - cy)^2))

matrix_as_heatmap(r) + ggtitle("radius r")__</code></pre>
<p><a href="11-chap_files/figure- html/fig-autocorr2d-1.png" title="Figure 11.31 (a):"><img src="11-chap_files/figure-html/fig-autocorr2d-1.png" class="img-fluid"></a></p>
<ol type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-autocorr2d-2.png" title="Figure 11.31 (b):"><img src="11-chap_files/figure-html/fig-autocorr2d-2.png" class="img-fluid"></a></p>
<ol start="2" type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-autocorr2d-3.png" title="Figure 11.31 (c):"><img src="11-chap_files/figure-html/fig-autocorr2d-3.png" class="img-fluid"></a></p>
<ol start="3" type="a">
<li></li>
</ol>
<p><a href="11-chap_files/figure- html/fig-autocorr2d-4.png" title="Figure 11.31 (d):"><img src="11-chap_files/figure-html/fig-autocorr2d-4.png" class="img-fluid"></a></p>
<ol start="4" type="a">
<li></li>
</ol>
<p>Figure 11.31: Autocorrelation functions of the three color channels of the <code>cells</code> image, shown as heatmaps. The peaks in the centres correspond to signal correlations over short distances. Also shown is the radial coordinate <code>r</code>.</p>
<p>Since the images are (or should be) isotropic, i.e., there is no preferred direction, we can average over the angular coordinate. The result is shown in Figure 11.32. We can see that the signals in the different color channels have different length scales.</p>
<pre><code>aggregate_by_radius = function(x, r)
  tibble(x = as.vector(x),
         r = as.vector(r)) |&gt;
  group_by(r) |&gt;
  summarize(value = mean(x))

lapply(names(ac), function(w) 
  cbind(channel = w, 
        aggregate_by_radius(ac[[w]], r))
  ) |&gt; 
  bind_rows() |&gt; 
  dplyr::filter(r &lt;= 50) |&gt;
  ggplot(aes(x = r, y = value, col = channel)) + geom_line() + 
    scale_color_manual(values = c(`Cy3` = "red", `FITC` = "green", `DAPI` = "blue"))__</code></pre>
<p>Extend the <code>autocorr2d</code> function to also compute the cross-correlation between different channels.</p>
<ul>
<li>What is the motivation behind the <code>sum</code> normalization in the above implementation <code>autocorr2d</code>?</li>
<li>Would it make sense to subtract the mean of <code>x</code> before the other computations?</li>
<li>What is the relation between this function and the usual empirical variance or correlation, i.e.&nbsp;the functions <code>var</code> and <code>sd</code> in base R?</li>
<li>How might plots such as Figure 11.32 be used for the construction of quality metrics in a high-throughput screening setting, i.e., when thousands or millions of images need to be analyzed?</li>
<li>How would a 3- or \(n\)-dimensional extension of <code>autocorr2d</code> look like? What would it be good for?</li>
</ul>
<p><a href="11-chap_files/figure- html/fig-autocorr1d-1.png" title="Figure 11.32: Autocorrelation functions of the three color channels of the cells image, aggregated by radius."><img src="11-chap_files/figure-html/fig-autocorr1d-1.png" class="img-fluid"></a></p>
<p>Figure 11.32: Autocorrelation functions of the three color channels of the <code>cells</code> image, aggregated by radius.</p>
<p>__</p>
<p>Exercise 11.4</p>
<p>Have a look at the workshop “Working with Image Data” <a href="https://github.com/wolfganghuber/WorkingWithImageData" class="uri">https://github.com/wolfganghuber/WorkingWithImageData</a>, which goes through some of the same content as this chapter, but on different images, and also has additional examples on segmentation and optical flow.</p>
<p>__</p>
<p>Exercise 11.5</p>
<p>Compute and display the Voronoi tessellation for the Ukrainian cities from <a href="09-chap.html">Chapter 9</a>. Either use their MDS-coordinates in the 2D plane with Euclidean distances, or the latitudes and longitudes using the great circle distance (Haversine formula).</p>
<p>__</p>
<p>Exercise 11.6</p>
<p>Download 3D image data from light sheet microscopy15, load it into an <strong><a href="https://bioconductor.org/packages/EBImage/">EBImage</a></strong> <em>Image</em> object and explore the data.</p>
<p>15 For instance, <a href="http://www.digital-embryo.org" class="uri">http://www.digital-embryo.org</a></p>
<p>Baddeley, Adrain, Jesper Moller, and Rasmus Waagepetersen. 2000. “Non- and Semiparametric Estimation of Interaction in Inhomogeneous Point Patterns.” <em>Statistica Neerlandica</em> 54: 329–50.</p>
<p>Baddeley, Adrian J. 1998. “Spatial Sampling and Censoring.” In <em>Stochastic Geometry: Likelihood and Computation</em> , edited by O. E. Barndorff-Nielsen, W. S. Kendall, and M. N. M. van Lieshout, 37–78. Chapman; Hall.</p>
<p>Berg, Stuart, Dominik Kutra, Thorben Kroeger, Christoph N Straehle, Bernhard X Kausler, Carsten Haubold, Martin Schiegg, et al.&nbsp;2019. “Ilastik: Interactive Machine Learning for (Bio)image Analysis.” <em>Nature Methods</em> 16 (12): 1226–32.</p>
<p>Boland, Michael V., and Robert F. Murphy. 2001. “A neural network classifier capable of recognizing the patterns of all major subcellular structures in fluorescence microscope images of HeLa cells.” <em>Bioinformatics</em> 17 (12): 1213–23.</p>
<p>Carpenter, Anne E, Thouis R Jones, Michael R Lamprecht, Colin Clarke, In Han Kang, Ola Friman, David A Guertin, Joo Han Chang, Robert A Lindquist, and Jason Moffat. 2006. “CellProfiler: Image Analysis Software for Identifying and Quantifying Cell Phenotypes.” <em>Genome Biology</em> 7: R100.</p>
<p>Chaumont, Fabrice de, Stéphane Dallongeville, Nicolas Chenouard, Nicolas Hervé, Sorin Pop, Thomas Provoost, Vannary Meas-Yedid, et al.&nbsp;2012. “Icy: an open bioimage informatics platform for extended reproducible research.” <em>Nature Methods</em> 9: 690–96.</p>
<p>Chiu, Sung Nok, Dietrich Stoyan, Wilfrid S. Kendall, and Joseph Mecke. 2013. <em>Stochastic Geometry and Its Applications</em>. Springer.</p>
<p>Cressie, Noel A. 1991. <em>Statistics for Spatial Data</em>. John Wiley; Sons.</p>
<p>Diggle, Peter J. 2013. <em>Statistical Analysis of Spatial and Spatio-Temporal Point Patterns</em>. Chapman; Hall/CRCs.</p>
<p>Held, M., M. H. A. Schmitz, B. Fischer, T. Walter, B. Neumann, M. H. Olma, M. Peter, J. Ellenberg, and D. W. Gerlich. 2010. “CellCognition: Time-Resolved Phenotype Annotation in High-Throughput Live Cell Imaging.” <em>Nature Methods</em> 7: 747.</p>
<p>Holmes, Susan, Adam Kapelner, and Peter P Lee. 2009. “An Interactive Java Statistical Image Segmentation System: GemIdent.” <em>Journal of Statistical Software</em> 30 (10).</p>
<p>Jones, T., A. Carpenter, and P. Golland. 2005. “Voronoi-Based Segmentation of Cells on Image Manifolds.” <em>Computer Vision for Biomedical Image Applications</em> , 535.</p>
<p>Laufer, Christina, Bernd Fischer, Maximilian Billmann, Wolfgang Huber, and Michael Boutros. 2013. “Mapping genetic interactions in human cancer cells with RNAi and multiparametric phenotyping.” <em>Nature Methods</em> 10: 427–31.</p>
<p>Pau, Grégoire, Florian Fuchs, Oleg Sklyar, Michael Boutros, and Wolfgang Huber. 2010. “EBImage R Package for Image Processing with Applications to Cellular Phenotypes.” <em>Bioinformatics</em> 26 (7): 979–81.</p>
<p>Rajaram, S., B. Pavie, L. F. Wu, and S. J. Altschuler. 2012. “PhenoRipper: software for rapidly profiling microscopy images.” <em>Nature Methods</em> 9: 635–37.</p>
<p>Ripley, B. D. 1988. <em>Statistical Inference for Spatial Processes.</em> Cambridge University Press.</p>
<p>Russ, John C., and F. Brent Neal. 2015. <em>The Image Processing Handbook</em>. 7th ed.&nbsp;CRC Press;</p>
<p>Schindelin, Johannes, Ignacio Arganda-Carreras, Erwin Frise, Verena Kaynig, Mark Longair, Tobias Pietzsch, Stephan Preibisch, et al.&nbsp;2012. “Fiji: an open- source platform for biological-image analysis.” <em>Nature Methods</em> 9: 676–82.</p>
<p>Serra, Jean. 1983. <em>Image Analysis and Mathematical Morphology</em>. Academic Press.</p>
<p>Setiadi, A Francesca, Nelson C Ray, Holbrook E Kohrt, Adam Kapelner, Valeria Carcamo-Cavazos, Edina B Levic, Sina Yadegarynia, et al.&nbsp;2010. “Quantitative, Architectural Analysis of Immune Cell Subsets in Tumor-Draining Lymph Nodes from Breast Cancer Patients and Healthy Lymph Nodes.” <em>PLoS One</em> 5 (8): e12420.</p>
<p>Vetterli, Martin, Jelena Kovačević, and Vivek Goyal. 2014. <em>Foundations of Signal Processing</em>. Cambridge University Press.</p>
<p>Page built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./10-chap.html" class="pagination-link" aria-label="10.1 Goals for this chapter">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">10.1 Goals for this chapter</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./12-chap.html" class="pagination-link" aria-label="12.1 Goals for this chapter">
        <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">12.1 Goals for this chapter</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>