<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; 4.1 이 장의 목표 – Modern Statistics for Modern Biology</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./05-chap.html" rel="next">
<link href="./03-chap.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-7bf12d62aa84b4fa538b342f1416a45b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="msmb.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./04-chap.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">4.1 이 장의 목표</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Modern Statistics for Modern Biology</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">홈</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">주제: 이질성</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">1.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">2.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">3.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-chap.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">4.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">5.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">6.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">7.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">8.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">9.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">10.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">11.1 이 장의 목표</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">12.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">13.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">14-chap.html</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">15-chap.html</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">16-chap.html</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#유한-혼합finite-mixtures" id="toc-유한-혼합finite-mixtures" class="nav-link active" data-scroll-target="#유한-혼합finite-mixtures"><span class="header-section-number">6.1</span> 4.2 유한 혼합(Finite mixtures)</a>
  <ul class="collapse">
  <li><a href="#단순한-예제와-컴퓨터-실험" id="toc-단순한-예제와-컴퓨터-실험" class="nav-link" data-scroll-target="#단순한-예제와-컴퓨터-실험"><span class="header-section-number">6.1.1</span> 4.2.1 단순한 예제와 컴퓨터 실험</a></li>
  <li><a href="#discovering-the-hidden-class-labels" id="toc-discovering-the-hidden-class-labels" class="nav-link" data-scroll-target="#discovering-the-hidden-class-labels"><span class="header-section-number">6.1.2</span> 4.2.2 Discovering the hidden class labels</a></li>
  <li><a href="#영-과잉-데이터zero-inflated-data를-위한-모델" id="toc-영-과잉-데이터zero-inflated-data를-위한-모델" class="nav-link" data-scroll-target="#영-과잉-데이터zero-inflated-data를-위한-모델"><span class="header-section-number">6.1.3</span> 4.2.3 영-과잉 데이터(zero inflated data)를 위한 모델</a></li>
  <li><a href="#more-than-two-components" id="toc-more-than-two-components" class="nav-link" data-scroll-target="#more-than-two-components"><span class="header-section-number">6.1.4</span> 4.2.4 More than two components</a></li>
  </ul></li>
  <li><a href="#empirical-distributions-and-the-nonparametric-bootstrap" id="toc-empirical-distributions-and-the-nonparametric-bootstrap" class="nav-link" data-scroll-target="#empirical-distributions-and-the-nonparametric-bootstrap"><span class="header-section-number">6.2</span> 4.3 Empirical distributions and the nonparametric bootstrap</a></li>
  <li><a href="#infinite-mixtures" id="toc-infinite-mixtures" class="nav-link" data-scroll-target="#infinite-mixtures"><span class="header-section-number">6.3</span> 4.4 Infinite mixtures</a>
  <ul class="collapse">
  <li><a href="#infinite-mixture-of-normals" id="toc-infinite-mixture-of-normals" class="nav-link" data-scroll-target="#infinite-mixture-of-normals"><span class="header-section-number">6.3.1</span> 4.4.1 Infinite mixture of normals</a></li>
  <li><a href="#infinite-mixtures-of-poisson-variables." id="toc-infinite-mixtures-of-poisson-variables." class="nav-link" data-scroll-target="#infinite-mixtures-of-poisson-variables."><span class="header-section-number">6.3.2</span> 4.4.2 Infinite mixtures of Poisson variables.</a></li>
  <li><a href="#gamma-distribution-two-parameters-shape-and-scale" id="toc-gamma-distribution-two-parameters-shape-and-scale" class="nav-link" data-scroll-target="#gamma-distribution-two-parameters-shape-and-scale"><span class="header-section-number">6.3.3</span> 4.4.3 Gamma distribution: two parameters (shape and scale)</a></li>
  <li><a href="#variance-stabilizing-transformations" id="toc-variance-stabilizing-transformations" class="nav-link" data-scroll-target="#variance-stabilizing-transformations"><span class="header-section-number">6.3.4</span> 4.4.4 Variance stabilizing transformations</a></li>
  </ul></li>
  <li><a href="#summary-of-this-chapter" id="toc-summary-of-this-chapter" class="nav-link" data-scroll-target="#summary-of-this-chapter"><span class="header-section-number">6.4</span> 4.5 Summary of this chapter</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">6.5</span> 4.6 Further reading</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">6.6</span> 4.7 Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">4.1 이 장의 목표</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><img src="imgs/t_distribution.png" class="img-fluid"></p>
<p>생물학적 데이터 분석의 주요 과제 중 하나는 이질성(heterogeneity)을 다루는 것입니다. 우리가 관심을 갖는 수치들은 종종 단순하고 단봉형(unimodal)인 “교과서적인 분포”를 따르지 않습니다. 예를 들어, <a href="02-chap.html">2장</a>의 마지막 부분에서 <a href="02-chap.html#fig-ScoreMixture-1">그림 2.27</a>의 서열 점수 히스토그램이 CpG 섬과 비섬(non-islands)이라는 두 개의 별개 최빈값(mode)을 갖는 것을 보았습니다. 우리는 이 데이터를 몇 가지(이 경우 두 개) 성분의 단순한 혼합으로 볼 수 있습니다. 이를 <strong>유한 혼합(finite mixtures)</strong>이라고 부릅니다. 다른 혼합물은 관측치 수만큼이나 많은 성분을 포함할 수 있는데, 이를 <strong>무한 혼합(infinite mixtures)</strong>이라고 부릅니다1.</p>
<p>1 모델링 선택의 많은 부분이 그러하듯이, 혼합물의 적절한 복잡성은 보는 사람의 관점에 달려 있으며, 종종 데이터의 양과 우리가 달성하고자 하는 해상도 및 매끄러움에 따라 달라집니다.</p>
<p><a href="01-chap.html">1장</a>에서 포아송 분포를 이용한 단순한 생성 모델이 에피토프(epitope) 검출에서 어떻게 유용한 추론으로 이어지는지 보았습니다. 불행히도, 이러한 단순한 모델로 실제 데이터를 만족스럽게 피팅하는 것은 종종 어렵습니다. 그러나 정규 분포나 포아송 분포와 같은 단순한 모델은 이 장에서 다룰 혼합 프레임워크를 사용하여 더 현실적인 모델을 구축하기 위한 구성 요소 역할을 할 수 있습니다. 혼합물은 유세포 분석 데이터, 생체 측정값, RNA-Seq, ChIP-Seq, 마이크로바이옴 및 현대 생명공학 기술을 사용하여 수집된 다른 많은 유형의 데이터에서 자연스럽게 발생합니다. 이 장에서는 단순한 예제들을 통해 혼합물을 사용하여 더 현실적인 분포 모델을 구축하는 방법을 배울 것입니다.</p>
<p>이 장에서 우리는 다음을 수행할 것입니다:</p>
<ul>
<li><p>두 개의 정규 모집단으로 구성된 분포로부터 우리만의 혼합 모델 데이터를 생성합니다.</p></li>
<li><p>기댓값-최대화(Expectation-Maximization, EM) 알고리즘을 통해 데이터 세트의 기저에 깔린 혼합물을 어떻게 “역설계(reverse engineer)”할 수 있는지 살펴봅니다.</p></li>
<li><p>ChIP-Seq 데이터와 같이 0이 아주 많은 데이터를 위해 영-과잉(zero-inflation)이라고 불리는 특수한 유형의 혼합 모델을 사용합니다.</p></li>
<li><p>경험적 누적 분포(empirical cumulative distribution)를 발견합니다: 관측된 데이터로부터 구축할 수 있는 특수한 혼합물입니다. 이를 통해 붓스트랩(bootstrap)을 사용하여 추정치의 가변성을 어떻게 시뮬레이션할 수 있는지 살펴볼 것입니다.</p></li>
<li><p>많은 성분을 가진 무한 혼합 모델의 사례로서 라플라스 분포(Laplace distribution)를 구축합니다. 이를 사용하여 프로모터 길이와 마이크로어레이 강도를 모델링할 것입니다.</p></li>
<li><p>RNA-Seq 데이터에 유용한 계층적 모델인 감마-포아송(gamma-Poisson) 분포를 처음으로 접하게 됩니다. 이것이 서로 다른 포아송 분포 소스들을 혼합함으로써 자연스럽게 발생한다는 것을 보게 될 것입니다.</p></li>
<li><p>혼합 모델을 통해 데이터 변환을 선택하는 방법을 살펴봅니다.</p></li>
</ul>
<section id="유한-혼합finite-mixtures" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="유한-혼합finite-mixtures"><span class="header-section-number">6.1</span> 4.2 유한 혼합(Finite mixtures)</h2>
<section id="단순한-예제와-컴퓨터-실험" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="단순한-예제와-컴퓨터-실험"><span class="header-section-number">6.1.1</span> 4.2.1 단순한 예제와 컴퓨터 실험</h3>
<p>여기에 두 개의 동일한 크기의 성분으로 구성된 혼합 모델의 첫 번째 예제가 있습니다. 생성 과정은 두 단계로 이루어집니다:</p>
<p><strong>공정한 동전을 던집니다.</strong></p>
<p>앞면이 나오면: 평균 1, 분산 0.25인 정규 분포에서 난수를 생성합니다.</p>
<p>뒷면이 나오면: 평균 3, 분산 0.25인 정규 분포에서 난수를 생성합니다. 그림 4.1에 표시된 히스토그램은 다음 코드를 사용하여 이 두 단계를 10,000번 반복하여 생성되었습니다.</p>
<pre><code>coinflips = (runif(10000) &gt; 0.5)
table(coinflips)__


coinflips
FALSE  TRUE 
 5003  4997 


oneFlip = function(fl, mean1 = 1, mean2 = 3, sd1 = 0.5, sd2 = 0.5) {
  if (fl) {
   rnorm(1, mean1, sd1)
  } else {
   rnorm(1, mean2, sd2)
  }
}
fairmix = vapply(coinflips, oneFlip, numeric(1))
library("ggplot2")
library("dplyr")
ggplot(tibble(value = fairmix), aes(x = value)) +
     geom_histogram(fill = "purple", binwidth = 0.1)__</code></pre>
<p><a href="04-chap_files/figure- html/fig-twocoins-1.png" title="그림 4.1: 두 정규 분포의 공정한 혼합물로부터 얻은 10,000개의 난수 히스토그램. 히스토그램의 왼쪽 부분은 (A)에서 생성된 숫자가 지배적이고, 오른쪽은 (B)에서 생성된 숫자가 지배적입니다."><img src="04-chap_files/figure-html/fig-twocoins-1.png" class="img-fluid"></a></p>
<p>그림 4.1: 두 정규 분포의 공정한 혼합물로부터 얻은 10,000개의 난수 히스토그램. 히스토그램의 왼쪽 부분은 (A)에서 생성된 숫자가 지배적이고, 오른쪽은 (B)에서 생성된 숫자가 지배적입니다.</p>
<p>__</p>
<p>질문 4.1</p>
<p>R의 벡터화된 구문을 사용하여 <code>vapply</code> 루프를 제거하고 <code>fairmix</code> 벡터를 더 효율적으로 생성하려면 어떻게 해야 할까요?</p>
<p>__</p>
<p>해결책</p>
<p>__</p>
<pre><code>means = c(1, 3)
sds   = c(0.5, 0.5)
values = rnorm(length(coinflips),
               mean = ifelse(coinflips, means[1], means[2]),
               sd   = ifelse(coinflips, sds[1],   sds[2]))__</code></pre>
<p>__</p>
<p>질문 4.2</p>
<p>개선된 코드를 사용하여 백만 번의 동전 던지기를 수행하고 200개의 빈(bin)을 가진 히스토그램을 만들어 보세요. 무엇을 알 수 있나요?</p>
<p>__</p>
<p>해결책</p>
<p>__</p>
<pre><code>fair = tibble(
  coinflips = (runif(1e6) &gt; 0.5),
  values = rnorm(length(coinflips),
                 mean = ifelse(coinflips, means[1], means[2]),
                 sd   = ifelse(coinflips, sds[1],   sds[2])))
ggplot(fair, aes(x = values)) +
     geom_histogram(fill = "purple", bins = 200)__</code></pre>
<p><a href="04-chap_files/figure-html/fig- limitinghistogram-1.png" title="그림 4.2: 그림 4.1과 유사하지만 백만 개의 관측치를 사용한 경우."><img src="04-chap_files/figure-html/fig- limitinghistogram-1.png" class="img-fluid"></a></p>
<p>그림 4.2: 그림 4.1과 유사하지만 백만 개의 관측치를 사용한 경우.</p>
<p>그림 4.2는 빈의 수와 빈당 관측치 수를 늘림에 따라 히스토그램이 매끄러운 곡선에 가까워지는 것을 보여줍니다. 이 매끄러운 한계 곡선을 확률 변수 <code>fair$values</code>의 <strong>밀도(density)</strong> 함수라고 부릅니다.</p>
<p>정규 분포 \(N(,)\) 확률 변수의 밀도 함수는 다음과 같이 명시적으로 쓸 수 있습니다. 우리는 보통 이를 다음과 같이 부릅니다.</p>
<p>\[ (x)=e<sup>{-()</sup>2}. \]</p>
<p>__</p>
<p>질문 4.3</p>
<ol type="1">
<li><code>coinflips</code>가 <code>TRUE</code>인 <code>fair$values</code> 값들에 대한 히스토그램을 그리세요. 힌트: <code>aes</code> 호출 시 <code>y = after_stat(density)</code>를 사용하고(이는 수직축이 비율을 나타냄을 의미함), binwidth를 0.01로 설정하세요.</li>
<li>\((z)\)에 해당하는 선을 겹쳐서 그리세요.</li>
</ol>
<p>__</p>
<p>해결책</p>
<p>__</p>
<pre><code>ggplot(dplyr::filter(fair, coinflips), aes(x = values)) +
  geom_histogram(aes(y = after_stat(density)), fill = "purple", binwidth = 0.01) +
  stat_function(fun = dnorm, color = "red",
                args = list(mean = means[1], sd = sds[1]))__</code></pre>
<p><a href="04-chap_files/figure-html/fig-overlaydensity-1.png &quot;그림 4.3: 정규 분포 N(\mu=1,\sigma^2=0.5^2)로부터 얻은 50만 개의 난수 히스토그램. 곡선은 dnorm 함수를 사용하여 계산된 이론적 밀도 \phi(x)입니다.&quot;"><img src="04-chap_files/figure-html/fig- overlaydensity-1.png" class="img-fluid"></a></p>
<p>그림 4.3: 정규 분포 \(N(,<sup>2=0.5</sup>2)\)로부터 얻은 50만 개의 난수 히스토그램. 곡선은 <code>dnorm</code> 함수를 사용하여 계산된 이론적 밀도 \((x)\)입니다.</p>
<p>사실 우리는 <code>fair$values</code> 전체의 밀도(히스토그램이 따라가는 한계 곡선)에 대한 수학적 공식을 두 밀도의 합으로 쓸 수 있습니다.</p>
<p>\[ f(x)=_1(x)+_2(x), \]</p>
<p>여기서 \(_1\)은 정규 분포 \(N(_1=1,^2=0.25)\)의 밀도이고, \(_2\)는 정규 분포 \(N(_2=3,^2=0.25)\)의 밀도입니다. 그림 4.4는 다음 코드를 통해 생성되었습니다.</p>
<pre><code>fairtheory = tibble(
  x = seq(-1, 5, length.out = 1000),
  f = 0.5 * dnorm(x, mean = means[1], sd = sds[1]) +
      0.5 * dnorm(x, mean = means[2], sd = sds[2]))
ggplot(fairtheory, aes(x = x, y = f)) +
  geom_line(color = "red", linewidth = 1.5) + ylab("mixture density")__</code></pre>
<p><a href="04-chap_files/figure- html/fig-twodensity-1.png" title="그림 4.4: 혼합물의 이론적 밀도."><img src="04-chap_files/figure-html/fig-twodensity-1.png" class="img-fluid"></a></p>
<p>그림 4.4: 혼합물의 이론적 밀도.</p>
<p>이 경우, 두 성분 분포의 겹침이 거의 없기 때문에 혼합 모델이 매우 뚜렷하게 보입니다. 그림 4.4는 두 개의 뚜렷한 정점을 보여줍니다. 우리는 이를 <strong>이봉(bimodal)</strong> 분포라고 부릅니다. 실제로는 많은 경우 혼합 성분 사이의 분리가 그렇게 명확하지 않지만, 그럼에도 불구하고 이는 중요합니다.</p>
<p><a href="04-chap_files/figure- html/fig-histmystery-1.png" title="그림 4.5: 인식하기 더 어려운 두 정규 분포의 혼합물."><img src="04-chap_files/figure-html/fig-histmystery-1.png" class="img-fluid"></a></p>
<p>그림 4.5: 인식하기 더 어려운 두 정규 분포의 혼합물.</p>
<p>__</p>
<p>질문 4.4</p>
<p>그림 4.5는 분산이 동일한 두 정규 분포의 공정한 혼합물 히스토그램입니다. 성분 분포의 두 <em>평균</em> 매개변수를 추측할 수 있나요? 힌트: 시행착오법을 사용하여 다양한 혼합물을 시뮬레이션하여 일치하는 히스토그램을 만들 수 있는지 확인할 수 있습니다. 이 장의 R 코드를 살펴보면 데이터가 정확히 어떻게 생성되었는지 알 수 있습니다.</p>
<p>__</p>
<p>해결책</p>
<p>__</p>
<p>다음 코드는 동전의 _앞면_에서 생성된 점은 빨간색으로, _뒷면_에서 생성된 점은 파란색으로 표시합니다. 그림 4.6에 표시된 출력 결과는 두 기저 분포를 보여줍니다.</p>
<pre><code>head(mystery, 3)__


# A tibble: 3 × 2
  coinflips values
  &lt;lgl&gt;      &lt;dbl&gt;
1 FALSE       2.40
2 FALSE       1.66
3 TRUE        1.22


br = with(mystery, seq(min(values), max(values), length.out = 30))
ggplot(mystery, aes(x = values)) +
  geom_histogram(data = dplyr::filter(mystery, coinflips),
     fill = "red", alpha = 0.2, breaks = br) +
  geom_histogram(data = dplyr::filter(mystery, !coinflips),
     fill = "darkblue", alpha = 0.2, breaks = br) __</code></pre>
<p><a href="04-chap_files/figure-html/fig- betterhistogram-1-1.png" title="그림 4.6: 그림 4.5의 혼합물이지만, 두 성분을 빨간색과 파란색으로 표시한 경우."><img src="04-chap_files/figure-html/fig- betterhistogram-1-1.png" class="img-fluid"></a></p>
<p>그림 4.6: 그림 4.5의 혼합물이지만, 두 성분을 빨간색과 파란색으로 표시한 경우.</p>
<p>그림 4.6에서 두 성분 분포의 막대는 서로 겹쳐서 표시됩니다. 성분을 표시하는 다른 방법은 아래 코드로 생성된 그림 4.7입니다.</p>
<pre><code>ggplot(mystery, aes(x = values, fill = coinflips)) +
  geom_histogram(data = dplyr::filter(mystery, coinflips),
     fill = "red", alpha = 0.2, breaks = br) +
  geom_histogram(data = dplyr::filter(mystery, !coinflips),
     fill = "darkblue", alpha = 0.2, breaks = br) +
  geom_histogram(fill = "purple", breaks = br, alpha = 0.2)__</code></pre>
<p><a href="04-chap_files/figure-html/fig- comparecomponents-1-1.png" title="그림 4.7: 그림 4.6과 유사하지만, 두 혼합 성분에 대해 누적 막대(stacked bars)를 사용한 경우."><img src="04-chap_files/figure-html/fig- comparecomponents-1-1.png" class="img-fluid"></a></p>
<p>그림 4.7: 그림 4.6과 유사하지만, 두 혼합 성분에 대해 누적 막대(stacked bars)를 사용한 경우.</p>
<p>__</p>
<p>질문 4.5</p>
<p>왜 그림 4.7의 막대 높이는 그림 4.5와 같지만, 그림 4.6의 막대 높이는 그렇지 않을까요?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>In Figures 4.7 and 4.5, each count occupies a different piece of vertical space in a bin. In Figure 4.6, in the overlapping (darker) region, some of the counts falling within the same bin are overplotted.</p>
<p>In Figures 4.6 and 4.7, we were able to use the <code>coinflips</code> column in the data to disentangle the components. In real data, this information is missing.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/book_icon.png" title="A book-long treatment on the subject of finite mixtures [@mclachlan2004]."><img src="imgs/book_icon.png" class="img-fluid figure-img"></a></p>
<figcaption>A book-long treatment on the subject of finite mixtures (McLachlan and Peel 2004).</figcaption>
</figure>
</div>
<p>A book-long treatment on the subject of finite mixtures (<a href="16-chap.html#ref-mclachlan2004">McLachlan and Peel 2004</a>).</p>
</section>
<section id="discovering-the-hidden-class-labels" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="discovering-the-hidden-class-labels"><span class="header-section-number">6.1.2</span> 4.2.2 Discovering the hidden class labels</h3>
<p>We use a method called the <em>expectation-maximization (EM) algorithm</em> to infer the value of the hidden groupings. The EM algorithm is a popular iterative procedure that alternates between pretending we know one part of the solution to compute the other part, and pretending the other part is known and computing the first part, and so on, until convergence. More concretely, it alternates between</p>
<ul>
<li><p>pretending we know the probabilities with which each observation belongs to the different mixture components and, from this, estimating the parameters of the components, and</p></li>
<li><p>pretending we know the parameters of the mixture components and estimating the probabilities with which each observation belongs to the components.</p></li>
</ul>
<p>Let’s take a simple example. We measure a variable \(x\) on a series of objects that we think come from two groups, although we do not know the group labels. We start by <em>augmenting</em> 2 the data with the unobserved (latent) group label, which we will call \(U\).</p>
<p>2 Adding another variable which was not measured, called a hidden or <strong>latent variable</strong>.</p>
<p>We are interested in finding the values of \(U\) and the unknown parameters \(\) of the underlying distribution of the groups. We will use the maximum likelihood approach introduced in <a href="02-chap.html">Chapter 2</a> to estimate the parameters that make the data \(x\) the most likely. We can write the probability densities</p>
<p>\[ p(x,u,|,) = p(x,|,u,),p(u,|,). \]</p>
<section id="mixture-of-normals" class="level4" data-number="6.1.2.1">
<h4 data-number="6.1.2.1" class="anchored" data-anchor-id="mixture-of-normals"><span class="header-section-number">6.1.2.1</span> Mixture of normals</h4>
<p>For instance, we could generalize our previous mixture model with two normal distributions Equation 4.1 by allowing non-equal mixture fractions,</p>
<p>\[ f(x)=_1(x)+(1-)_2(x), \]</p>
<p>for \(\). Similarly as above, \(_k\) is the density of the normal \(N(_k,_k^2)\) for \(k=1\) and \(k=2\), respectively. Then, the parameter vector \(\) is a five-tuple of the two means, the two standard deviations, and the mixture parameter \(\). In other words, \(=(_1,_2,_1,_2,)\). Here is an example of 이러한 모델에 따라 생성된 데이터입니다. 레이블은 \(u\)로 표시됩니다.</p>
<pre><code>mus = c(-0.5, 1.5)
lambda = 0.5
u = sample(2, size = 100, replace = TRUE, prob = c(lambda, 1-lambda))
x = rnorm(length(u), mean = mus[u])
dux = tibble(u, x)
head(dux)__


# A tibble: 6 × 2
      u     x
  &lt;int&gt; &lt;dbl&gt;
1     2 0.303
2     2 2.65 
3     1 0.484
4     2 3.04 
5     2 1.10 
6     2 1.96 </code></pre>
<p>만약 레이블 \(u\)를 알고 있다면, 각 그룹에 대해 별도의 MLE를 사용하여 매개변수를 추정할 수 있습니다. 전체 가능도(likelihood)는 다음과 같습니다.</p>
<p>\[ p(x, u ,|, ) = ( _{\{i:,u_i=1\}} <em>1(x_i) ) ( </em>{\{i:,u_i=2\}} _2(x_i) ). \]</p>
<p>이 식을 최대화하는 작업은 세 가지 독립적인 부분으로 나눌 수 있습니다: 식 4.4의 우변에 있는 첫 번째 괄호 안의 식을 최대화하여 \(_1\)과 \(_1\)을 찾고, 두 번째 괄호를 최대화하여 \(_2\)와 \(_2\)를 찾으며, 레이블의 경험적 빈도로부터 \(\)를 찾습니다.</p>
<pre><code>group_by(dux, u) |&gt; summarize(mu = mean(x), sigma = sd(x))__


# A tibble: 2 × 3
      u     mu sigma
  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;
1     1 -0.558  1.05
2     2  1.41   1.04


table(dux$u) / nrow(dux)__


   1    2 
0.55 0.45 </code></pre>
<p>__</p>
<p>질문 4.6</p>
<p>혼합 비율 \(=\)은 알고 있지만 \(u_i\)는 모른다고 가정해 봅시다. 이 경우 밀도는 \(_1(x)+_2(x)\)입니다. (로그) 가능도를 직접 써보세요. 여기에서 MLE를 명시적으로 구하는 것을 방해하는 요인은 무엇인가요?</p>
<p>__</p>
<p>해결책</p>
<p>__</p>
<p>정규 혼합물의 가능도 계산에 대해서는 Shalizi (<a href="16-chap.html#ref-CosmaShalizi2017">2017</a>)의 <em>혼합 모델(Mixture Models)</em> 장을 참조하십시오. “혼합 모델을 추정하려고 하면, 사후 레이블 확률(posterior label probabilities)에 의해 주어진 가중치를 사용하여 가중 최대 가능도(weighted maximum likelihood)를 수행하게 됩니다. 거듭 강조하지만, 이 확률들은 우리가 추정하고자 하는 매개변수에 의존하므로, 이는 일종의 악순환처럼 보입니다.”</p>
<p>대부분의 경우 우리는 \(u\) 레이블이나 혼합 비율을 알지 못합니다. 우리가 할 수 있는 일은 레이블에 대한 초기 추측에서 시작하여 이를 알고 있다고 가정하고 위와 같이 매개변수를 추정한 다음, 추정치가 실질적으로 변하지 않을 때까지(즉, 수렴할 때까지) 그리고 가능도가 최적값에 도달할 때까지 매 단계마다 레이블과 매개변수에 대한 현재의 최선의 추측을 업데이트하는 반복적인 사이클을 거치는 것입니다.</p>
<p>사실 우리는 각 관측치에 대해 “딱딱한(hard)” 레이블 \(u\)(그룹 1 아니면 2에 속함)를 부여하는 대신, 합이 1이 되는 멤버십 가중치(membership weights)로 대체하는 더 정교한 작업을 수행할 수 있습니다. 혼합 모델 4.3은 다음 식을 제안합니다.</p>
<p>\[ w(x, 1) = \]</p>
<p>이 식은 값 \(x\)를 갖는 관측치가 첫 번째 혼합 성분에 의해 생성되었을 확률로 해석될 수 있으며, 두 번째 성분에 대해서는 유사하게 \(w(x, 2) = 1 - w(x, 1)\)이 됩니다. 즉, \(\)가 아직 보지 못한 관측치가 혼합 성분 1에서 나올 사전 확률(prior probability)이라면, \(w(x,1)\)은 그 값 \(x\)를 관찰한 후의 상응하는 사후 확률(posterior probability)입니다. 이는 다음과 같은 반복 알고리즘을 제안합니다:</p>
<p><strong>E 단계</strong> : \(\)(즉, 평균, 표준 편차 및 \(\))가 알려져 있다고 가정하고, 멤버십 가중치 4.5를 평가합니다.</p>
<p><strong>M 단계</strong> : 각 관측치 \(x_i\)의 멤버십 가중치가 주어졌을 때, \(\)의 새롭고 개선된 최대 가능도 추정치를 결정합니다.</p>
<p>\(\)와 가능도가 수렴할 때까지 반복합니다. 이 시점에서 코드가 포함된 데모인 연습 문제 4.1을 확인해 보시기 바랍니다. 사실, 이 알고리즘은 여기서 다루는 특정 응용 예제보다 훨씬 더 일반적입니다. (<a href="16-chap.html#ref-Bishop:PRML">Bishop 2006</a>)은 매우 읽기 쉬운 설명을 제시하고 있으며, 주요 내용은 다음과 같습니다:</p>
<p>우리의 목표는 관측 변수 \(x\), 관측되지 않은 변수 \(u\) 및 일부 매개변수 \(\)를 포함하는 확률 모델의 주변 가능도(marginal likelihood)를 최대화하는 것입니다. 우리의 단순한 예제에서 \(u\)는 두 가지 가능한 값을 갖는 범주형 변수이고, \(x\)는 실수입니다. 일반적으로 \(x\)와 \(u\) 모두 모든 유형의 개별 변수들의 튜플(즉, 다변량)일 수 있습니다. 주변 가능도는 \(u\)의 모든 가능한 값에 대해 기댓값(즉, 가중 평균)을 취하여 계산됩니다:</p>
<p>\[ L_(; x) = _U p(x, u,|,) , U. \]</p>
<p>우리의 구체적인 예제에서, 적분은 가능한 모든 멤버십 구성에 대해 (확률적으로) 평균을 내는 것에 해당하며, 따라서 멤버십 가중치를 고려한 가중 합이 됩니다.</p>
<p>\[ L_(; x) = <em>{i=1}^n </em>{u=1}^2 p(x_i, u,|,) , w(x_i, u,|,). \]</p>
<p>이 수치를 직접 최대화하는 것은 다루기 힘든(intractable) 일입니다.</p>
<p>데이터와 현재의 매개변수 추정치 \(_t\)가 주어졌을 때 우리가 파악할 수 있는 것은 잠재 변수의 조건부 분포 \(p(u,|,x, _t)\)입니다. 우리는 이를 사용하여 일반적인 매개변수 값 \(\)에 대해 평가된 완전 데이터 로그 가능도(complete data log likelihood) \(p(x, u,|,)\)의 기댓값을 찾을 수 있습니다. 이 기댓값은 종종 다음과 같이 표시됩니다.</p>
<p>\[ Q(, _t) = _U p(x, u,|,) , p(u,|, x, _t) , U. \]</p>
<p>M 단계에서 우리는 이 함수를 최대화하여 수정된 매개변수 추정치 \(_{t+1}\)을 결정합니다.</p>
<p>\[ <em>{t+1} = </em>, Q(, _t). \]</p>
<p>E 단계는 \(Q\)의 핵심 요소인 \(p(u,|, x, _t)\)를 계산하는 것으로 구성됩니다.</p>
<p>이 두 단계(E와 M)는 개선 사항이 작아질 때까지 반복됩니다. 이는 우리가 가능도의 평평한 부분에 가까워졌고 최대값에 도달했음을 나타내는 수치적 지표입니다. 반복 궤적은 시작 지점에 따라 달라지겠지만, 도달하는 지점은 달라지지 않기를 바랍니다. 이는 산 정상에 오르는 것과 비슷합니다. 산 정상에 오르는 길은 시작 지점에 따라 다를 수 있고 경로도 다를 수 있지만, 산봉우리가 하나만 있고 여러 개가 아닌 이상 항상 정상으로 이어집니다. 따라서 예방 조치로서, 이러한 절차를 서로 다른 시작 지점에서 여러 번 반복하여 항상 동일한 답을 얻는지 확인하는 것이 좋습니다.</p>
<p>__</p>
<p>질문 4.7</p>
<p>여러 R 패키지에서 <strong><a href="https://cran.r-project.org/web/packages/mclust/">mclust</a></strong>, <strong><a href="https://cran.r-project.org/web/packages/EMcluster/">EMcluster</a></strong>, <strong><a href="https://cran.r-project.org/web/packages/EMMIXskew/">EMMIXskew</a></strong>를 포함한 EM 구현을 제공합니다. 하나를 선택하여 서로 다른 시작 값으로 EM 함수를 여러 번 실행해 보세요. 그런 다음 <strong><a href="https://cran.r-project.org/web/packages/mixtools/">mixtools</a></strong> 패키지의 <code>normalmixEM</code> 함수를 사용하여 출력을 비교해 보세요.</p>
<p>__</p>
<p>해결책</p>
<p>__</p>
<p>여기서는 <strong><a href="https://cran.r-project.org/web/packages/mixtools/">mixtools</a></strong>의 출력을 보여줍니다.</p>
<pre><code>library("mixtools")
y = c(rnorm(100, mean = -0.2, sd = 0.5),
      rnorm( 50, mean =  0.5, sd =   1))
gm = normalmixEM(y, k = 2, 
                    lambda = c(0.5, 0.5),
                    mu = c(-0.01, 0.01), 
                    sigma = c(3, 3))__


number of iterations= 134 


with(gm, c(lambda, mu, sigma, loglik))__


[1]    0.9263    0.0737   -0.1859    1.9216    0.6134    0.4746 -170.5662</code></pre>
<p>EM 알고리즘은 매우 유익합니다:</p>
<ol type="1">
<li><p>우리는 관측치가 한 그룹에 속하는지 다른 그룹에 속하는지 결정하지 않고, 멤버십 확률을 가중치로 사용하여 여러 그룹에 참여할 수 있게 함으로써 더 미묘한 추정치를 얻는 “부드러운(soft)” 평균화의 첫 번째 사례를 보았습니다 (<a href="16-chap.html#ref-Slonim:2005">Slonim et al.&nbsp;2005</a>).</p></li>
<li><p>미지수가 너무 많은 어려운 문제를 더 간단한 문제들을 번갈아 가며 해결함으로써 어떻게 다룰 수 있는지 보여줍니다.</p></li>
<li><p>숨겨진 변수(hidden variables)가 있는 데이터 생성 모델을 고려하면서도 그 매개변수를 추정할 수 있었습니다. 숨겨진 변수의 값을 명시적으로 확정하지 않고도 그렇게 할 수 있었습니다: 식 4.8의 기댓값 단계에서 멤버십 확률로 구체화된 이들에 대해 (가중) 평균을 취했습니다. 이 기본 아이디어는 매우 강력하여 머신러닝의 많은 고급 알고리즘의 출발점이 됩니다 (<a href="16-chap.html#ref-Bishop:PRML">Bishop 2006</a>).</p></li>
<li><p>이는 모델 평균화(model averaging, <a href="16-chap.html#ref-hoeting1999">Hoeting et al.&nbsp;1999</a>)의 더 일반적인 경우로 확장될 수 있습니다. 우리 데이터에 어떤 모델이 적합한지 확신할 수 없는 경우 여러 모델을 동시에 고려하는 것이 때때로 유익할 수 있습니다. 우리는 이들을 가중 모델로 결합할 수 있습니다. 가중치는 모델의 가능도에 의해 제공됩니다.</p></li>
</ol>
</section>
</section>
<section id="영-과잉-데이터zero-inflated-data를-위한-모델" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="영-과잉-데이터zero-inflated-data를-위한-모델"><span class="header-section-number">6.1.3</span> 4.2.3 영-과잉 데이터(zero inflated data)를 위한 모델</h3>
<p>생태학적 및 분자적 데이터는 종종 카운트(counts)의 형태로 나타납니다. 예를 들어, 여러 장소 각각에 있는 여러 종 각각의 개체 수일 수 있습니다. 이러한 데이터는 종종 두 가지 시나리오의 혼합으로 볼 수 있습니다: 종이 존재하지 않으면 카운트는 반드시 0이지만, 종이 존재한다면 우리가 관찰하는 개체 수는 무작위 샘플링 분포에 따라 달라지며, 이 분포에도 0이 포함될 수 있습니다. 우리는 이를 다음과 같은 혼합 모델로 모델링합니다:</p>
<p>\[ f_{}(x) = , (x) + (1-) , f_{}(x), \]</p>
<p>여기서 \(\)는 모든 질량이 0에 있는 확률 분포를 나타내는 디랙 델타 함수입니다. 첫 번째 혼합 성분에서 발생하는 0은 “구조적(structural)”이라고 불립니다. 우리의 예제에서 이는 특정 종이 특정 서식지에 살지 않기 때문에 발생합니다. 두 번째 성분인 \(f_{}\) 역시 단순히 무작위 샘플링으로 인해 0 및 다른 작은 숫자를 포함할 수 있습니다. R 패키지 <strong><a href="https://cran.r-project.org/web/packages/pscl/">pscl</a></strong> (<a href="16-chap.html#ref-Zeileis:2008">Zeileis, Kleiber, and Jackman 2008</a>)과 <strong><a href="https://cran.r-project.org/web/packages/zicounts/">zicounts</a></strong>는 <strong>영-과잉(zero inflated)</strong> 카운트를 다루기 위한 많은 예제와 함수를 제공합니다.</p>
<section id="예시-chip-seq-데이터" class="level4" data-number="6.1.3.1">
<h4 data-number="6.1.3.1" class="anchored" data-anchor-id="예시-chip-seq-데이터"><span class="header-section-number">6.1.3.1</span> 예시: ChIP-Seq 데이터</h4>
<p>ChIP-Seq 데이터의 예를 들어보겠습니다. 이 데이터는 염색질 면역 침전(chromatin immunoprecipitation, ChIP)을 통해 얻은 DNA 조각의 서열입니다. 이 기술을 사용하면 전사 인자, 뉴클레오솜, 히스톤 수정, 염색질 리모델링 효소, 샤페론, 중합효소 및 기타 단백질의 게놈 DNA 상의 위치를 매핑할 수 있습니다. 이는 DNA 요소 백과사전(ENCODE) 프로젝트에서 사용된 주요 기술이었습니다. 여기서는 <strong><a href="https://bioconductor.org/packages/mosaicsExample/">mosaicsExample</a></strong> 패키지의 예제(<a href="16-chap.html#ref-Kuan2011statistical">Kuan et al.&nbsp;2011</a>)를 사용합니다. 이 예제는 GM12878 세포주에 적용된 STAT1 단백질 및 H3K4me3 히스톤 수정에 대한 항체의 ChIP-Seq으로부터 22번 염색체에서 측정된 데이터를 보여줍니다. 여기서는 <code>binTFBS</code> 객체를 생성하는 코드는 보여주지 않지만, 이 장의 소스 코드 파일에 나와 있으며 <strong><a href="https://bioconductor.org/packages/mosaics/">mosaics</a></strong> 패키지의 비네트를 따릅니다.</p>
<pre><code>binTFBS __


Summary: bin-level data (class: BinData)
----------------------------------------
- # of chromosomes in the data: 1
- total effective tag counts: 462479
  (sum of ChIP tag counts of all bins)
- control sample is incorporated
- mappability score is NOT incorporated
- GC content score is NOT incorporated
- uni-reads are assumed
----------------------------------------</code></pre>
<p>이 객체로부터 빈(bin)당 카운트의 히스토그램을 생성할 수 있습니다.</p>
<pre><code>bincts = print(binTFBS)
ggplot(bincts, aes(x = tagCount)) +
  geom_histogram(binwidth = 1, fill = "forestgreen")__</code></pre>
<p><a href="04-chap_files/figure- html/fig-chipseqzeros-1.png" title="그림 4.8: ChIP-Seq 데이터 세트에서 22번 염색체를 따라 200nt 창(window)에서 발견된 결합 부위의 수입니다."><img src="04-chap_files/figure-html/fig-chipseqzeros-1.png" class="img-fluid"></a></p>
<p>그림 4.8: ChIP-Seq 데이터 세트에서 22번 염색체를 따라 200nt 창(window)에서 발견된 결합 부위의 수.</p>
<p>그림 4.8을 보면 0이 아주 많다는 것을 알 수 있는데, 이 플롯만으로는 다른 작은 숫자들(\(1, 2, …\))의 빈도를 고려할 때 0의 개수가 정말로 특별한지는 즉각적으로 명확하지 않습니다.</p>
<p>__</p>
<p>질문 4.8</p>
<ol type="1">
<li><p>(y)축에 로그(밑 10) 스케일을 사용하여 카운트 히스토그램을 다시 그리세요.</p></li>
<li><p>카운트가 0인 빈의 비율인 (_0)을 추정하세요.</p></li>
</ol>
<p>__</p>
<p>해결책</p>
<p>__</p>
<pre><code>ggplot(bincts, aes(x = tagCount)) + scale_y_log10() +
   geom_histogram(binwidth = 1, fill = "forestgreen")__</code></pre>
<p><a href="04-chap_files/figure-html/fig-ChipseqHistlogY-1.png &quot;Figure 4.9: As Figure fig-chipseqzeros, but using a logarithm base 10 scale on the y-axis. The fraction of zeros seems elevated compared to that of ones, twos, …&quot;"><img src="04-chap_files/figure-html/fig- ChipseqHistlogY-1.png" class="img-fluid"></a></p>
<p>Figure 4.9: As Figure 4.8, but using a logarithm base 10 scale on the \(y\)-axis. The fraction of zeros seems elevated compared to that of ones, twos, …</p>
</section>
</section>
<section id="more-than-two-components" class="level3" data-number="6.1.4">
<h3 data-number="6.1.4" class="anchored" data-anchor-id="more-than-two-components"><span class="header-section-number">6.1.4</span> 4.2.4 More than two components</h3>
<p>So far we have looked at mixtures of two components. We can extend our description to cases where there may be more. For instance, when weighing N=7,000 nucleotides obtained from mixtures of deoxyribonucleotide monophosphates (each type has a different weight, measured with the same standard deviation sd=3), we might observe the histogram (shown in Figure 4.10) generated by the following code.</p>
<pre><code>masses = c(A =  331, C =  307, G =  347, T =  322)
probs  = c(A = 0.12, C = 0.38, G = 0.36, T = 0.14)
N  = 7000
sd = 3
nuclt   = sample(length(probs), N, replace = TRUE, prob = probs)
quadwts = rnorm(length(nuclt),
                mean = masses[nuclt],
                sd   = sd)
ggplot(tibble(quadwts = quadwts), aes(x = quadwts)) +
  geom_histogram(bins = 100, fill = "purple")__</code></pre>
<p><a href="04-chap_files/figure-html/fig- nucleotideweights-1-1.png" title="Figure 4.10: Simulation of 7,000 nucleotide mass measurements."><img src="04-chap_files/figure-html/fig- nucleotideweights-1-1.png" class="img-fluid"></a></p>
<p>Figure 4.10: Simulation of 7,000 nucleotide mass measurements.</p>
<p>__</p>
<p>Question 4.9</p>
<p>Repeat this simulation experiment with \(N=1000\) nucleotide measurements. What do you notice in the histogram?</p>
<p>__</p>
<p>Question 4.10</p>
<p>What happens when \(N=7000\) but the standard deviation is 10?</p>
<p>__</p>
<p>Question 4.11</p>
<p>Plot the theoretical density curve for the distribution simulated in Figure 4.10.</p>
<p>In this case, as we have enough measurements with good enough precision, we are able to distinguish the four nucleotides and decompose the distribution shown in Figure 4.10. With fewer data and/or more noisy measurements, the four modes and the distribution component might be less clear.</p>
</section>
</section>
<section id="empirical-distributions-and-the-nonparametric-bootstrap" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="empirical-distributions-and-the-nonparametric-bootstrap"><span class="header-section-number">6.2</span> 4.3 Empirical distributions and the nonparametric bootstrap</h2>
<p>In this section, we will consider an extreme case of mixture models, where we model our sample of \(n\) data points as a mixture of \(n\) point masses. We could use almost any set of data here; to be concrete, we use Darwin’s <em>Zea Mays</em> data3 in which he compared the heights of 15 pairs of <em>Zea Mays</em> plants (15 self-hybridized versus 15 crossed). The data are available in the <strong><a href="https://cran.r-project.org/web/packages/HistData/">HistData</a></strong> package, and we plot the distribution of the 15 differences in height:</p>
<p>3 They were collected by Darwin who asked his cousin, Francis Galton to analyse them. R.A. Fisher re-analysed the same data using a paired t-test (<a href="16-chap.html#ref-bulmer2003">Bulmer 2003</a>). We will get back to this example in <a href="13-chap.html">Chapter 13</a>.</p>
<pre><code>library("HistData")
ZeaMays$diff __


 [1]  6.125 -8.375  1.000  2.000  0.750  2.875  3.500  5.125  1.750  3.625
[11]  7.000  3.000  9.375  7.500 -6.000


ggplot(ZeaMays, aes(x = diff, ymax = 1/15, ymin = 0)) +
  geom_linerange(linewidth = 1, col = "forestgreen") + ylim(0, 0.1)__</code></pre>
<p><a href="04-chap_files/figure- html/fig-ecdfZea-1.png" title="Figure 4.11: The observed sample can be seen as a mixture of point masses at each of the values (real point masses would be bars without any width whatsoever)."><img src="04-chap_files/figure-html/fig-ecdfZea-1.png" class="img-fluid"></a></p>
<p>Figure 4.11: The observed sample can be seen as a mixture of point masses at each of the values (real point masses would be bars without any width whatsoever).</p>
<p><a href="03-chap.html#sec-graphics-ecdf">Section 3.6.7</a>에서 우리는 크기 \(n\)인 표본에 대한 경험적 누적 분포 함수(empirical cumulative distribution function, ECDF)가 다음과 같음을 보았습니다.</p>
<p>\[ <em>n(x)= </em>{i=1}^n {}_{x x_i}, \]</p>
<p>그리고 <a href="03-chap.html#fig-graphics-onedecdf">그림 3.24</a>에서 ECDF 플롯을 보았습니다. 우리는 또한 우리 표본의 _밀도_를 다음과 같이 쓸 수 있습니다.</p>
<p>\[ <em>n(x) =</em>{i=1}^n _{x_i}(x) \]</p>
<p>일반적으로 확률 분포의 밀도는 (존재한다면) 분포 함수의 도함수입니다. 우리는 여기서 이 원리를 적용했습니다: 식 4.11로 정의된 분포의 밀도는 식 4.12입니다. 함수 \(<em>a\)를 계단 함수(step function) \({}</em>{x a}\)의 “도함수”로 간주할 수 있기 때문에 이렇게 할 수 있었습니다: 이 함수는 계단이 있는 한 점 \(a\)를 제외하고는 거의 모든 곳에서 완전히 평평하며, 그 점에서의 값은 “무한”입니다. 식 4.12는 우리의 데이터 표본이 그림 4.11에서와 같이 관측된 값 \(x_1, x_2, …, x_n\)에서의 \(n\)개 <strong>점 질량(point masses)</strong>의 혼합물로 간주될 수 있음을 강조합니다.</p>
<p>이것이 타당하기 위해서는 (표준 미적분학을 넘어서는) 약간의 고급 수학이 필요하지만, 여기서는 다루지 않겠습니다.</p>
<p><a href="imgs/BootstrapPrincipleNew.png &quot;그림 4.12: 통계량 \tau의 값은 기저 분포 F로부터 생성된 데이터(회색 행렬들)로부터 추정됩니다. F로부터 얻은 서로 다른 표본들은 서로 다른 데이터를 생성하고, 따라서 추정치 \hat{\tau}의 값도 달라집니다: 이를 표집 가변성(sampling variability)이라고 합니다. 모든 \hat{\tau}들의 분포가 표집 분포(sampling distribution)입니다.&quot;"><img src="imgs/BootstrapPrincipleNew.png" class="img-fluid"></a></p>
<p>그림 4.12: 통계량 \(\)의 값은 기저 분포 \(F\)로부터 생성된 데이터(회색 행렬들)로부터 추정됩니다. \(F\)로부터 얻은 서로 다른 표본들은 서로 다른 데이터를 생성하고, 따라서 추정치 \(\)의 값도 달라집니다: 이를 <strong>표집 가변성(sampling variability)</strong>이라고 합니다. 모든 \(\)들의 분포가 <strong>표집 분포(sampling distribution)</strong>입니다.</p>
<p>평균, 최솟값 또는 중앙값과 같은 우리 표본의 통계량은 이제 ECDF의 함수로 쓰여질 수 있습니다. 예를 들어, \({x} = <em>{x_i}(x),x\)입니다. 또 다른 예로, 만약 \(n\)이 홀수라면 중앙값은 정렬된 리스트의 정중앙에 있는 값인 \(x</em>{()}\)입니다.</p>
<p>통계량 \(\)의 실제 <strong>표집 분포</strong>는 통계량을 계산하기 위한 많은 서로 다른 데이터 표본을 필요로 하기 때문에 알기 어려운 경우가 많습니다. 이는 그림 4.12에 나와 있습니다.</p>
<p><strong>붓스트랩(bootstrap)</strong> 원리는 원래 표본으로부터 구축된 경험적 분포에서 뽑은 새로운 표본들을 생성함으로써 \(\)의 실제 표집 분포를 근사합니다(그림 4.13). 우리는 데이터를 (\(\)들의 혼합 분포로 간주하여) _재사용_하여 표본을 추출하고 그로부터 계산된 통계량 \(^*\)의 표집 분포를 살펴봄으로써 새로운 “데이터 세트”를 만듭니다. 이를 <strong>비모수적 붓스트랩(nonparametric bootstrap)</strong> 재표본 추출 접근법이라고 하며, 완전한 참고 문헌으로는 Bradley Efron과 Tibshirani (<a href="16-chap.html#ref-Efron:1994">1994</a>)를 참조하십시오. 이는 아무리 복잡하더라도 기본적으로 모든 통계량에 적용할 수 있는 매우 다재다능하고 강력한 방법입니다. <a href="05-chap.html">5장</a>에서 이 방법의 응용 예시, 특히 클러스터링에 대한 적용 사례를 살펴볼 것입니다.</p>
<p><a href="imgs/BootstrapPrinciple2New.png &quot;그림 4.13: 붓스트랩은 (그림 4.12에서처럼) 기저의 실제 분포 F가 아니라 경험적 분포 함수 \hat{F}_n으로부터 표본을 추출함으로써 표집 가변성을 시뮬레이션합니다.&quot;"><img src="imgs/BootstrapPrinciple2New.png" class="img-fluid"></a></p>
<p>그림 4.13: 붓스트랩은 (그림 4.12에서처럼) 기저의 실제 분포 \(F\)가 아니라 경험적 분포 함수 \(_n\)으로부터 표본을 추출함으로써 표집 가변성을 시뮬레이션합니다.</p>
<p>이러한 아이디어를 사용하여 그림 4.11에서 보았던 Zea Mays 차이값의 중앙값에 대한 표집 분포를 추정해 봅시다. 이전 섹션들과 유사한 시뮬레이션을 사용합니다: 15개의 값(각각이 15개 성분 혼합물의 한 성분임)으로부터 크기가 15인 표본을 \(B=1000\)번 추출합니다. 그런 다음 이 15개 값으로 구성된 1000개 표본 각각의 중앙값을 계산하고 그 분포를 살펴봅니다. 이것이 중앙값의 붓스트랩 표집 분포입니다.</p>
<pre><code>B = 1000
meds = replicate(B, {
  i = sample(15, 15, replace = TRUE)
  median(ZeaMays$diff[i])
})
ggplot(tibble(medians = meds), aes(x = medians)) +
  geom_histogram(bins = 30, fill = "purple")__</code></pre>
<p><a href="04-chap_files/figure- html/fig-bootmedian-1.png" title="그림 4.14: Zea Mays 차이값의 중앙값에 대한 붓스트랩 표집 분포."><img src="04-chap_files/figure-html/fig-bootmedian-1.png" class="img-fluid"></a></p>
<p>그림 4.14: Zea Mays 차이값의 중앙값에 대한 붓스트랩 표집 분포.</p>
<p>__</p>
<p>질문 4.12</p>
<p>이 시뮬레이션을 바탕으로 중앙값에 대한 99% 신뢰 구간을 추정해 보세요. 이 구간과 0 사이의 겹침을 보고 무엇을 결론지을 수 있나요?</p>
<p>__</p>
<p>질문 4.13</p>
<p><strong><a href="https://cran.r-project.org/web/packages/bootstrap/">bootstrap</a></strong> 패키지의 <code>bootstrap</code> 함수를 사용하여 <code>median</code>과 <code>mean</code> 모두에 대해 동일한 분석을 다시 수행해 보세요. 평균과 중앙값의 표집 분포 사이에서 어떤 차이점을 발견했나요?</p>
<p>__</p>
<p>해결책</p>
<p>__</p>
<pre><code>library("bootstrap")
bootstrap(ZeaMays$diff, B, mean)
bootstrap(ZeaMays$diff, B, median)__</code></pre>
<section id="왜-비모수적nonparametric인가요" class="level4" data-number="6.2.0.1">
<h4 data-number="6.2.0.1" class="anchored" data-anchor-id="왜-비모수적nonparametric인가요"><span class="header-section-number">6.2.0.1</span> 왜 비모수적(nonparametric)인가요?</h4>
<p>이론 통계학에서 비모수적 방법이란 무한히 많은 자유도나 알 수 없는 매개변수의 수를 가진 방법을 말합니다.</p>
<p><img src="imgs/devil.png" class="img-fluid"></p>
<p>실제로 우리는 무한대까지 기다리지 않습니다. 매개변수의 수가 가용 데이터의 양만큼 많거나 그보다 많아지면 그 방법을 비모수적이라고 부릅니다. 붓스트랩은 \(n\)개의 성분을 가진 혼합물을 사용하므로, 크기가 \(n\)인 표본의 경우 비모수적 방법의 자격을 갖춥니다.</p>
<p>그 이름에도 불구하고, 비모수적 방법이 매개변수를 사용하지 않는 방법은 아닙니다. 모든 통계적 방법은 알 수 없는 수치를 추정합니다.</p>
<p>__</p>
<p>질문 4.14</p>
<p>표본이 \(n=3\)개의 서로 다른 값으로 구성되어 있다면, 몇 가지의 서로 다른 붓스트랩 재표본이 가능할까요? \(n=15\)인 경우에 대해서도 답해 보세요.</p>
<p>__</p>
<p>해결책</p>
<p>__</p>
<p>모든 붓스트랩 재표본의 집합은 합이 \(n\)인 \(n\)개 정수 벡터의 집합과 동일합니다. 관측치 \(x_1, x_2, …, x_n\)이 붓스트랩 표본에 나타나는 횟수를 \( = (k_1, k_2, …, k_n)\)이라고 합시다. 각 \(k_i\)를 (다항 분포에서처럼) 상자로 생각할 수 있고, \(n\)개의 공을 떨어뜨릴 \(n\)개의 상자가 있습니다. 구성을 세는 방법은 \(n\)개의 공을 상자에 나누는 방법의 수를 세는 것입니다. 즉, 공을 나타내는 <code>o</code>를 \(n\)번 쓰고 그 사이에 구분선 <code>|</code>를 \(n-1\)번 쓰는 것입니다. 따라서 우리는 <code>o</code>(공) 또는 <code>|</code>(구분선) 중 하나를 선택해야 하는 \(2n-1\)개의 자리를 채워야 합니다. \(n=3\)인 경우, 가능한 배치는 <code>oo||o</code>일 수 있으며, 이는 \( = (2,0,1)\)에 해당합니다. 일반적으로 이 숫자는 \({2n-1} \)이며, 따라서 \(n=3\)과 \(15\)에 대한 답은 다음과 같습니다.</p>
<pre><code>c(N3 = choose(5, 3), N15 = choose(29, 15))__


      N3      N15 
      10 77558760 </code></pre>
<p>__</p>
<p>질문 4.15</p>
<p><strong><a href="https://cran.r-project.org/web/packages/bootstrap/">bootstrap</a></strong> 패키지에 구현된 붓스트랩을 사용할 때 발생할 수 있는 두 가지 유형의 오류는 무엇인가요? 그중 하나를 개선하기 위해 어떤 매개변수를 수정할 수 있나요?</p>
<p>__</p>
<p>해결책</p>
<p>__</p>
<p>무작위 재표본 추출을 통한 데이터 하위 집합의 몬테카를로 시뮬레이션은 전수 붓스트랩(exhaustive bootstrap)을 근사합니다(<a href="16-chap.html#ref-Diaconis1994">Diaconis and Holmes 1994</a>). <code>bootstrap</code> 함수의 <code>nboot</code> 인수의 크기를 늘리면 몬테카를로 오류를 줄일 수 있지만, 전수 붓스트랩도 여전히 정확하지는 않습니다. 우리는 여전히 실제 분포 대신 데이터의 근사 분포 함수를 사용하고 있기 때문입니다. 표본 크기가 작거나 원래 표본에 편향이 있는 경우, <code>nboot</code>를 아무리 크게 선택하더라도 근사는 여전히 상당히 좋지 않을 수 있습니다.</p>
</section>
</section>
<section id="infinite-mixtures" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="infinite-mixtures"><span class="header-section-number">6.3</span> 4.4 Infinite mixtures</h2>
<p>Sometimes mixtures can be useful even if we don’t aim to assign a label to each observation or, to put it differently, if we allow as many `labels’ as there are observations. If the number of mixture components is as big as (or bigger than) the number of observations, we say we have an <strong>infinite mixture</strong>. Let’s look at some examples.</p>
<section id="infinite-mixture-of-normals" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="infinite-mixture-of-normals"><span class="header-section-number">6.3.1</span> 4.4.1 Infinite mixture of normals</h3>
<p><a href="imgs/LaplacePortrait_web.png" title="Figure 4.15: Laplace knew already that the probability density f_X(y)=\frac{1}{2\phi}\exp\left(-\frac{|y-\theta|}{\phi}\right),\qquad\phi>0 has the median as its location parameter \theta and the median absolute deviation (MAD) as its scale parameter \phi."><img src="imgs/LaplacePortrait_web.png" class="img-fluid"></a></p>
<p>Figure 4.15: Laplace knew already that the probability density \[f_X(y)=(-),&gt;0\] has the median as its location parameter \(\) and the median absolute deviation (MAD) as its scale parameter \(\).</p>
<p>Consider the following two-level data generating scheme:</p>
<p><strong>Level 1</strong> Create a sample of <code>W</code>s from an exponential distribution.</p>
<pre><code>w = rexp(10000, rate = 1)__</code></pre>
<p><strong>Level 2</strong> The \(w\)s serve as the variances of normal variables with mean \(\) generated using <code>rnorm</code>.</p>
<pre><code>mu  = 0.3
lps = rnorm(length(w), mean = mu, sd = sqrt(w))
ggplot(data.frame(lps), aes(x = lps)) +
  geom_histogram(fill = "purple", binwidth = 0.1)__</code></pre>
<p><a href="04-chap_files/figure-html/fig- Laplacedistribution-1.png" title="Figure 4.16: Data sampled from a Laplace distribution."><img src="04-chap_files/figure-html/fig- Laplacedistribution-1.png" class="img-fluid"></a></p>
<p>Figure 4.16: Data sampled from a Laplace distribution.</p>
<p>This turns out to be a rather useful distribution. It has well-understood properties and is named after Laplace, who proved that the median is a good estimator of its location parameter \(\) and that the median absolute deviation can be used to estimate its scale parameter \(\). From the formula in the caption of Figure 4.15 we see that the \(L_1\) distance (absolute value of the difference) holds a similar position in the Laplace density as the \(L_2\) (square of the difference) does for the normal density.</p>
<p>Conversely, in Bayesian regression4, having a Laplace distribution as a prior on the coefficients amounts to an \(L_1\) penalty, called the <em>lasso</em> (<a href="16-chap.html#ref-Tibshirani1996">Tibshirani 1996</a>), while a normal distribution as a prior leads to an \(L_2\) penalty, called ridge regression.</p>
<p>4 Don’t worry if you are not familiar with this, in that case just skip this sentence.</p>
<p>__</p>
<p>Question 4.16</p>
<p>Write a random variable whose distribution is the symmetric Laplace as a function of normal and exponential random variables.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>We can write the hierarchical model with variances generated as exponential variables, \(W\), as:</p>
<p>\[ X = Z, W Exp(1), Z N(0,1). \]</p>
<section id="asymmetric-laplace" class="level4" data-number="6.3.1.1">
<h4 data-number="6.3.1.1" class="anchored" data-anchor-id="asymmetric-laplace"><span class="header-section-number">6.3.1.1</span> Asymmetric Laplace</h4>
<p>In the Laplace distribution, the variances of the normal components depend on \(W\), while the means are unaffected. A useful extension adds another parameter \(\) that controls the locations or centers of the components. We generate data <code>alps</code> from a hierarchical model with \(W\) an exponential variable; the output shown in Figure 4.17 is a histogram of normal \(N(+w,w)\) random numbers, where the \(w\)’s themselves were randomly generated from an exponential distribution with mean \(1\) as shown in the code:</p>
<pre><code>mu = 0.3; sigma = 0.4; theta = -1
w  = rexp(10000, 1)
alps = rnorm(length(w), theta + mu * w, sigma * sqrt(w))
ggplot(tibble(alps), aes(x = alps)) +
  geom_histogram(fill = "purple", binwidth = 0.1)__</code></pre>
<p><a href="04-chap_files/figure-html/fig- ALaplacedistribution-1.png" title="Figure 4.17: Histogram of data generated from an asymmetric Laplace distribution – a scale mixture of many normals whose means and variances are dependent. We write X \sim AL(\theta, \mu, \sigma)."><img src="04-chap_files/figure-html/fig- ALaplacedistribution-1.png" class="img-fluid"></a></p>
<p>Figure 4.17: Histogram of data generated from an asymmetric Laplace distribution – a scale mixture of many normals whose means and variances are dependent. We write \(X AL(, , )\).</p>
<p>Such hierarchical mixture distributions, where every instance of the data has its own mean and variance, are useful models in many biological settings. Examples are shown in Figure 4.18.</p>
<p><a href="imgs/LaplaceMixturePromoterLengths.png &quot;Figure 4.18 (a): The lengths of the promoters shorter than 2000bp from Saccharomyces cerevisiae as studied by @Kristiansson2009.&quot;"><img src="imgs/LaplaceMixturePromoterLengths.png" class="img-fluid"></a></p>
<ol type="a">
<li>The lengths of the promoters shorter than 2000bp from Saccharomyces cerevisiae as studied by Kristiansson et al.&nbsp;(<a href="16-chap.html#ref- Kristiansson2009">2009</a>).</li>
</ol>
<p><img src="imgs/tcellhist.png" class="img-fluid">: The log- ratios of microarray gene expression measurements for 20,000 genes [<span class="citation" data-cites="Purdom2005">@Purdom2005</span>].”)</p>
<ol start="2" type="a">
<li>The log-ratios of microarray gene expression measurements for 20,000 genes (<a href="16-chap.html#ref-Purdom2005">Purdom and Holmes 2005</a>).</li>
</ol>
<p>Figure 4.18: Histogram of real data. Both distributions can be modeled by asymmetric Laplace distributions.</p>
<p>__</p>
<p>Question 4.17</p>
<p>Looking at the log-ratio of gene expression values from a microarray, one gets a distribution as shown on the right of Figure 4.18. How would one explain that the data have a histogram of this form?</p>
<p>The Laplace distribution is an example of where the consideration of the generative process indicates how the variance and mean are linked. The expectation value and variance of an asymmetric Laplace distribution \(AL(, , )\) are</p>
<p>\[ E(X)=+(X)=<sup>2+</sup>2. \]</p>
<p>Note the variance is dependent on the mean, unless \(= 0\) (the case of the symmetric Laplace Distribution). This is the feature of the distribution that makes it useful. Having a mean-variance dependence is very common for physical measurements, be they microarray fluorescence intensities, peak heights from a mass spectrometer, or reads counts from high-throughput sequencing, as we’ll see in the next section.</p>
</section>
</section>
<section id="infinite-mixtures-of-poisson-variables." class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="infinite-mixtures-of-poisson-variables."><span class="header-section-number">6.3.2</span> 4.4.2 Infinite mixtures of Poisson variables.</h3>
<p><a href="imgs/three-worlds_web.jpg" title="Figure 4.19: How to count the fish in a lake? MC Escher."><img src="imgs/three-worlds_web.jpg" class="img-fluid"></a></p>
<p>Figure 4.19: How to count the fish in a lake? MC Escher.</p>
<p>A similar two-level hierarchical model is often also needed to model real- world count data. At the lower level, simple Poisson and binomial distributions serve as the building blocks, but their parameters may depend on some underlying (latent) process. In ecology, for instance, we might be interested in variations of fish species in all the lakes in a region. We sample the fish species in each lake to estimate their true abundances, and that could be modeled by a Poisson. But the true abundances will vary from lake to lake. And if we want to see whether, for instance, changes in climate or altitude play a role, we need to disentangle such systematic effects from random lake-to-lake variation. The different Poisson rate parameters \(\) can be modeled as coming from a distribution of rates. Such a hierarchical model also enables us to add supplementary steps in the hierarchy, for instance we could be interested in many different types of fish, model altitude and other environmental factors separately, etc.</p>
<p>Further examples of sampling schemes that are well modeled by mixtures of Poisson variables include applications of high-throughput sequencing, such as RNA-Seq, which we will cover in detail in <a href="08-chap.html">Chapter 8</a>, or 16S rRNA-Seq data used in microbial ecology.</p>
</section>
<section id="gamma-distribution-two-parameters-shape-and-scale" class="level3" data-number="6.3.3">
<h3 data-number="6.3.3" class="anchored" data-anchor-id="gamma-distribution-two-parameters-shape-and-scale"><span class="header-section-number">6.3.3</span> 4.4.3 Gamma distribution: two parameters (shape and scale)</h3>
<p>Now we are getting to know a new distribution that we haven’t seen before. The gamma distribution is an extension of the (one-parameter) exponential distribution, but it has two parameters, which makes it more flexible. It is often useful as a building block for the upper level of a hierarchical model. The gamma distribution is positive-valued and continuous. While the density of the exponential has its maximum at zero and then simply decreases towards 0 as the value goes to infinity, the density of the gamma distribution has its maximum at some finite value. Let’s explore it by simulation examples. The histograms in Figure 4.20 were generated by the following lines of code:</p>
<pre><code>ggplot(tibble(x = rgamma(10000, shape = 2, rate = 1/3)),
   aes(x = x)) + geom_histogram(bins = 100, fill= "purple")
ggplot(tibble(x = rgamma(10000, shape = 10, rate = 3/2)),
   aes(x = x)) + geom_histogram(bins = 100, fill= "purple")__</code></pre>
<p><a href="04-chap_files/figure- html/fig-gammahist1-1.png" title="Figure 4.20 (a): gamma(2,\frac{1}{3})"><img src="04-chap_files/figure-html/fig-gammahist1-1.png" class="img-fluid"></a></p>
<ol type="a">
<li>gamma\((2,)\)</li>
</ol>
<p><a href="04-chap_files/figure- html/fig-gammahist1-2.png" title="Figure 4.20 (b): gamma(10,\frac{3}{2})"><img src="04-chap_files/figure-html/fig-gammahist1-2.png" class="img-fluid"></a></p>
<ol start="2" type="a">
<li>gamma\((10,)\)</li>
</ol>
<p>Figure 4.20: Histograms of random samples of gamma distributions. The gamma is a flexible two parameter distribution: <a href="http://en.wikipedia.org/wiki/Gamma_distribution">see Wikipedia</a>.</p>
<section id="gammapoisson-mixture-a-hierarchical-model" class="level4" data-number="6.3.3.1">
<h4 data-number="6.3.3.1" class="anchored" data-anchor-id="gammapoisson-mixture-a-hierarchical-model"><span class="header-section-number">6.3.3.1</span> Gamma–Poisson mixture: a hierarchical model</h4>
<ol type="1">
<li><p>Generate a set of parameters: \(_1,_2,…\) from a gamma distribution.</p></li>
<li><p>Use these to generate a set of Poisson(\(_i\)) random variables, one for each \(_1\).</p></li>
</ol>
<pre><code>lambda = rgamma(10000, shape = 10, rate = 3/2)
gp = rpois(length(lambda), lambda = lambda)
ggplot(tibble(x = gp), aes(x = x)) +
  geom_histogram(bins = 100, fill= "purple")__</code></pre>
<p><a href="04-chap_files/figure-html/fig- generatepoissongamma-1.png" title="Figure 4.21: Histogram of gp, generated via a gamma-Poisson hierachical model."><img src="04-chap_files/figure-html/fig- generatepoissongamma-1.png" class="img-fluid"></a></p>
<p>Figure 4.21: Histogram of <code>gp</code>, generated via a gamma-Poisson hierachical model.</p>
<p>The resulting values are said to come from a gamma–Poisson mixture. Figure 4.21 shows the histogram of <code>gp</code>.</p>
<p>__</p>
<p>Question 4.18</p>
<ol type="1">
<li><p>Are the values generated from such a gamma–Poisson mixture continuous or discrete ?</p></li>
<li><p>What is another name for this distribution? Hint: Try the different distributions provided by the <code>goodfit</code> function from the <strong><a href="https://cran.r-project.org/web/packages/vcd/">vcd</a></strong> package.</p></li>
</ol>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>library("vcd")
ofit = goodfit(gp, "nbinomial")
plot(ofit, xlab = "")
ofit$par __


$size
[1] 9.911829

$prob
[1] 0.5963857</code></pre>
<p><a href="04-chap_files/figure- html/fig-goofy-1.png" title="Figure 4.22: Goodness of fit plot. The rootogram shows the theoretical probabilities of the gamma-Poisson distribution (a.k.a. negative binomial) as red dots and the square roots of the observed frequencies as the height of the rectangular bars. The bars all end close to the horizontal axis, which indicates a good fit to the negative binomial distribution."><img src="04-chap_files/figure-html/fig-goofy-1.png" class="img-fluid"></a></p>
<p>Figure 4.22: Goodness of fit plot. The <strong>rootogram</strong> shows the theoretical probabilities of the gamma-Poisson distribution (a.k.a. negative binomial) as red dots and the square roots of the observed frequencies as the height of the rectangular bars. The bars all end close to the horizontal axis, which indicates a good fit to the negative binomial distribution.</p>
<p>In R, and in some other places, the gamma-Poisson distribution travels under the alias name of <strong>negative binomial distribution</strong>. The two names are synonyms; the second one alludes to the fact that Equation 4.15 bears some formal similarities to the probabilities of a binomial distribution. The first name, gamma–Poisson distribution, is more indicative of its generating mechanism, and that’s what we will use in the rest of the book. It is a discrete distribution, that means that it takes values only on the natural numbers (in contrast to the gamma distribution, which covers the whole positive real axis). Its probability distribution is</p>
<p>\[ (K=k)=(]</p>
<p>which depends on the two parameters \(a^+\) and \(p\). Equivalently, the two parameters can be expressed by the mean \(=pa/(1-p)\) and a parameter called the <strong>dispersion</strong> \(/a\). The variance of the distribution depends on these parameters, and is \(+^2\).</p>
<p><a href="04-chap_files/figure-html/fig-mixtures-dgammapois-1.png &quot;Figure 4.23: Visualization of the hierarchical model that generates the gamma-Poisson distribution. The top panel shows the density of a gamma distribution with mean 50 (vertical black line) and variance 30. Assume that in one particular experimental replicate, the value 60 is realized. This is our latent variable. The observable outcome is distributed according to the Poisson distribution with that rate parameter, shown in the middle panel. In one particular experiment the outcome may be, say, 55, indicated by the dashed green line. Overall, if we repeat these two subsequent random process many times, the outcomes will be distributed as shown in the bottom panel – the gamma-Poisson distribution.&quot;"><img src="04-chap_files/figure-html/fig-mixtures- dgammapois-1.png" class="img-fluid"></a></p>
<p>Figure 4.23: Visualization of the hierarchical model that generates the gamma- Poisson distribution. The top panel shows the density of a gamma distribution with mean 50 (vertical black line) and variance 30. Assume that in one particular experimental replicate, the value 60 is realized. This is our latent variable. The observable outcome is distributed according to the Poisson distribution with that rate parameter, shown in the middle panel. In one particular experiment the outcome may be, say, 55, indicated by the dashed green line. Overall, if we repeat these two subsequent random process many times, the outcomes will be distributed as shown in the bottom panel – the gamma-Poisson distribution.</p>
<p>__</p>
<p>Question 4.19</p>
<p>If you are more interested in analytical derivations than illustrative simulations, try writing out the mathematical derivation of the gamma-Poisson probability distribution.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Recall that the final distribution is the result of a two step process:</p>
<ol type="1">
<li>Generate a \((a,b)\) distributed number, call it \(x\), from the density</li>
</ol>
<p>\[ f_(x, a, b)=,x<sup>{a-1},e</sup>{-b x}, \]</p>
<p>where \(\) is the so-called \(\)-function, \((a)=_0<sup>x</sup>{a-1},e^{-x},x\) (not to be confused with the gamma distribution, even though there is this incidental relation).</p>
<ol start="2" type="1">
<li>Generate a number \(k\) from the Poisson distribution with rate \(x\). The probability distribution is</li>
</ol>
<p>\[ f_{}(k, =x)= \]</p>
<p>If \(x\) only took on a finite set of values, we could solve the problem simply by summing over all the possible cases, each weighted by their probability according to \(f_\). But \(x\) is continuous, so we have to write the sum out as an integral instead of a discrete sum. We call the distribution of \(K\) the marginal. Its probability mass function is</p>
<p>\[ ]</p>
<p>Collect terms and move terms independent of \(x\) outside the integral</p>
<p>\[ P(K=k)= _{x=0}^{} x<sup>{k+a-1}e</sup>{-(b+1)x} dx \]</p>
<p>Because we know the gamma density sums to one: \(_0^ x<sup>{k+a-1}e</sup>{-(b+1)x} dx = \)</p>
<p>\[ ]</p>
<p>where in the last line we used that \((v+1)=v!\). This is the same as Equation (4.15), the gamma-Poisson with size parameter \(a\) and probability \(p=\).</p>
</section>
</section>
<section id="variance-stabilizing-transformations" class="level3" data-number="6.3.4">
<h3 data-number="6.3.4" class="anchored" data-anchor-id="variance-stabilizing-transformations"><span class="header-section-number">6.3.4</span> 4.4.4 Variance stabilizing transformations</h3>
<p>A key issue we need to control when we analyse experimental data is how much variability there is between repeated measurements of the same underlying true value, i.e., between replicates. This will determine whether and how well we can see any true differences, i.e., between different conditions. Data that arise through the type of hierarchical models we have studied in this chapter often turn out to have very heterogeneous variances, and this can be a challenge. We will see how in such cases <strong>variance-stabilizing transformations</strong> (<a href="16-chap.html#ref-Anscombe1948">Anscombe 1948</a>) can help. Let’s start with a series of Poisson variables with rates from 10 to 100:</p>
<p>Note how we construct the dataframe (or, more precisely, the <em>tibble</em>) <code>simdat</code>: the output of the <code>lapply</code> loop is a list of <em>tibble</em> s, one for each value of <code>lam</code>. With the pipe operator <code>|&gt;</code> we send it to the function <code>bind_rows</code> (from the <strong><a href="https://cran.r-project.org/web/packages/dplyr/">dplyr</a></strong> package). The result is a dataframe of all the list elements neatly stacked on top of each other.</p>
<pre><code>simdat = lapply(seq(10, 100, by = 10), function(lam)
    tibble(n = rpois(200, lambda = lam),
           `sqrt(n)` = sqrt(n),
       lambda = lam)) |&gt;
  bind_rows() |&gt;
  tidyr::pivot_longer(cols = !lambda)
ggplot(simdat, aes(x = factor(lambda), y = value)) + xlab(expression(lambda)) +
  geom_violin() + facet_grid(rows = vars(name), scales = "free")__</code></pre>
<p><a href="04-chap_files/figure-html/fig-seriesofpoisson-1.png &quot;Figure 4.24: Poisson distributed measurement data, for eight different choices of the mean lambda. In the upper panel, the y-axis is proportional to the data; in the lower panel, it is on a square-root scale. Note how the distribution widths change in the first case, but less so in the second.&quot;"><img src="04-chap_files/figure-html/fig- seriesofpoisson-1.png" class="img-fluid"></a></p>
<p>Figure 4.24: Poisson distributed measurement data, for eight different choices of the mean <code>lambda</code>. In the upper panel, the \(y\)-axis is proportional to the data; in the lower panel, it is on a square-root scale. Note how the distribution widths change in the first case, but less so in the second.</p>
<p>The data that we see in the upper panel of Figure 4.24 are an example of what is called <strong>heteroscedasticity</strong> : the standard deviations (or, equivalently, the variance) of our data is different in different regions of our data space. In particular, it increases along the \(x\)-axis, with the mean. For the Poisson distribution, we indeed know that the standard deviation is the square root of the mean; for other types of data, there may be other dependencies. This can be a problem if we want to apply subsequent analysis techniques (for instance, regression, or a statistical test) that are based on assuming that the variances are the same. In Figure 4.24, the numbers of replicates for each value of lambda are quite large. In practice, this is not always the case. Moreover, the data are usually not explicitly stratified by a known mean as in our simulation, so the heteroskedasticity may be harder to see, even though it is there. However, as we see in the lower panel of Figure 4.24, if we simply apply the square root transformation, then the transformed variables will have approximately the same variance. This works even if we do not know the underlying mean for each observation, the square root transformation does not need this information. We can verify this more quantitatively, as in the following code, which shows the standard deviations of the sampled values <code>n</code> and <code>sqrt(n)</code> for the difference choices of <code>lambda</code>.</p>
<p>The standard deviation of the square root transformed values is consistently around 0.5, so we would use the transformation <code>2*sqrt(n)</code> to achieve unit variance.</p>
<pre><code>summarise(group_by(simdat, name, lambda), sd(value)) |&gt; tidyr::pivot_wider(values_from = `sd(value)`)__


# A tibble: 10 × 3
   lambda     n `sqrt(n)`
    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;
 1     10  2.95     0.478
 2     20  4.19     0.470
 3     30  5.62     0.521
 4     40  5.99     0.473
 5     50  7.69     0.546
 6     60  7.59     0.492
 7     70  8.69     0.520
 8     80  8.99     0.505
 9     90  9.44     0.498
10    100  9.84     0.495</code></pre>
<p>Another example, now using the gamma-Poisson distribution, is shown in Figure 4.25. We generate gamma-Poisson variables <code>u</code>5 and plot the 95% confidence intervals around the mean.</p>
<p>5 To catch a greater range of values for the mean value <code>mu</code>, without creating too dense a sequence, we use a geometric series: \(_{i+1} = 2_i\).</p>
<pre><code>muvalues = 2^seq(0, 10, by = 1)
simgp = lapply(muvalues, function(mu) {
  u = rnbinom(n = 1e4, mu = mu, size = 4)
  tibble(mean = mean(u), sd = sd(u),
         lower = quantile(u, 0.025),
         upper = quantile(u, 0.975),
         mu = mu)
  } ) |&gt; bind_rows()
head(as.data.frame(simgp), 2)__


    mean       sd lower upper mu
1 0.9965 1.106440     0     4  1
2 2.0233 1.748503     0     6  2


ggplot(simgp, aes(x = mu, y = mean, ymin = lower, ymax = upper)) +
  geom_point() + geom_errorbar()__</code></pre>
<p><a href="04-chap_files/figure- html/fig-seriesofnb-1.png" title="Figure 4.25: gamma-Poisson distributed measurement data, for a range of \mu from 1 to 1024."><img src="04-chap_files/figure-html/fig-seriesofnb-1.png" class="img-fluid"></a></p>
<p>Figure 4.25: gamma-Poisson distributed measurement data, for a range of \(\) from 1 to 1024.</p>
<p>__</p>
<p>Question 4.20</p>
<p>How can we find a transformation for these data that stabilizes the variance, similar to the square root function for the Poisson distributed data?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>If we divide the values that correspond to <code>mu[1]</code> (and which are centered around <code>simgp$mean[1]</code>) by their standard deviation <code>simgp$sd[1]</code>, the values that correspond to <code>mu[2]</code> (and which are centered around <code>simgp$mean[2]</code>) by their standard deviation <code>simgp$sd[2]</code>, and so on, then the resulting values will have, by construction, a standard deviation (and thus variance) of 1. And rather than defining 11 separate transformations, we can achieve our goal by defining one single piecewise linear <em>and</em> continuous function that has the appropriate slopes at the appropriate values.</p>
<pre><code>simgp = mutate(simgp,
  slopes = 1 / sd,
  trsf   = cumsum(slopes * mean))
ggplot(simgp, aes(x = mean, y = trsf)) +
  geom_point() + geom_line() + xlab("")__</code></pre>
<p><a href="04-chap_files/figure- html/fig-pcwlin-1-1.png" title="Figure 4.26: Piecewise linear function that stabilizes the variance of the data in Figure fig-seriesofnb."><img src="04-chap_files/figure-html/fig-pcwlin-1-1.png" class="img-fluid"></a></p>
<p>Figure 4.26: Piecewise linear function that stabilizes the variance of the data in Figure 4.25.</p>
<p>We see in Figure 4.26 that this function has some resemblance to a square root function in particular at its lower end. At the upper end, it seems to look more like a logarithm. The more mathematically inclined will see that an elegant extension of these numerical calculations can be done through a little calculus known as the <strong>delta method</strong> , as follows.</p>
<p>Call our transformation function \(g\), and assume it’s differentiable (that’s not a very strong assumption: pretty much any function that we might consider reasonable here is differentiable). Also call our random variables \(X_i\), with means \(_i\) and variances \(v_i\), and we assume that \(v_i\) and \(_i\) are related by a functional relationship \(v_i = v(_i)\). Then, for values of \(X_i\) in the neighborhood of its mean \(_i\),</p>
<p>\[ g(X_i) = g(_i) + g’(_i) (X_i-_i) + … \]</p>
<p>where the dots stand for higher order terms that we can neglect. The variances of the transformed values are then</p>
<p>\[ <span class="math display">\[\begin{align} \text{Var}(g(X_i)) &amp;\simeq g'(\mu_i)^2\\\ \text{Var}(X_i) &amp;=
g'(\mu_i)^2 \, v(\mu_i), \end{align}\]</span> \]</p>
<p>where we have used the rules \((X-c)=(X)\) and \((cX)=c^2,(X)\) that hold whenever \(c\) is a constant number. Requiring that this be constant leads to the differential equation</p>
<p>\[ g’(x) = . \]</p>
<p>For a given mean-variance relationship \(v()\), we can solve this for the function \(g\). Let’s check this for some simple cases:</p>
<ul>
<li><p>if \(v()=\) (Poisson), we recover \(g(x)=\), the square root transformation.</p></li>
<li><p>If \(v()=,^2\), solving the differential equation 4.19 gives \(g(x)=(x)\). This explains why the logarithm transformation is so popular in many data analysis applications: it acts as a variance stabilizing transformation whenever the data have a constant coefficient of variation, that is, when the standard deviation is proportional to the mean.</p></li>
</ul>
<p>__</p>
<p>Question 4.21</p>
<p>What is the variance-stabilizing transformation associated with \(v() = + ,^2\)?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>To solve the differential equation 4.19 with this function \(v()\), we need to compute the integral</p>
<p>\[ . \]</p>
<p>A closed form expression can be looked up in a reference table such as (<a href="16-chap.html#ref-BronsteinSemendjajew">Bronštein and Semendjajew 1979</a>). These authors provide the general solution</p>
<p>\[ = (2+2ax+b) + , \]</p>
<p>into which we can plug in our special case \(a=\), \(b=1\), \(c=0\), to obtain the variance-stabilizing transformation</p>
<p>\[ <span class="math display">\[\begin{align} g_\alpha(x) &amp;= \frac{1}{2\sqrt{\alpha}}
\ln\left(2\sqrt{\alpha x (\alpha x+1)} + 2\alpha x + 1\right) \\\ &amp;=
\frac{1}{2\sqrt{\alpha}} {\displaystyle \operatorname {arcosh}} (2\alpha
x+1).\\\ \end{align}\]</span> \]</p>
<p>For the second line in Equation 4.22, we used the identity \({ }(z) = (z+)\). In the limit of \(\), we can use the linear approximation \((1+)=+O(^2)\) to see that \(g_0(x)=\). Note that if \(g_\) is a variance-stabilizing transformation, then so is \(ug_+v\) for any pair of numbers \(u\) and \(v\), and we have used this freedom to insert an extra factor \(\) for reasons that become apparent in the following. You can verify that the function \(g_\) from Equation 4.22 fulfills condition 4.19 by computing its derivative, which is an elementary calculation. We can plot it:</p>
<pre><code>f = function(x, a) 
  ifelse (a==0, 
    sqrt(x), 
    log(2*sqrt(a) * sqrt(x*(a*x+1)) + 2*a*x+1) / (2*sqrt(a)))
x  = seq(0, 24, by = 0.1)
df = lapply(c(0, 0.05*2^(0:5)), function(a) 
  tibble(x = x, a = a, y = f(x, a))) %&gt;% bind_rows()
ggplot(df, aes(x = x, y = y, col = factor(a))) + 
  geom_line() + labs(col = expression(alpha))__</code></pre>
<p><a href="04-chap_files/figure-html/fig- plotvstgammapoisson-1-1.png" title="Figure 4.27: Graph of the function Equation eq- mixtures-vstgammapoisson for different choices of \alpha."><img src="04-chap_files/figure-html/fig- plotvstgammapoisson-1-1.png" class="img-fluid"></a></p>
<p>Figure 4.27: Graph of the function Equation 4.22 for different choices of \(\).</p>
<p>and empirically verify the equivalence of two terms in Equation 4.22:</p>
<pre><code>f2 = function(x, a) ifelse (a==0, sqrt(x), acosh(2*a*x + 1) / (2*sqrt(a)))  
with(df, max(abs(f2(x,a) - y)))__


[1] 8.881784e-16</code></pre>
<p>As we see in Figure 4.27, for small values of \(x\), \(g_(x) \) (independently of \(\)), whereas for large values (\(x\)) and \(&gt;0\), it behaves like a logarithm:</p>
<p>\[ <span class="math display">\[\begin{align} &amp;\frac{1}{2\sqrt{\alpha}}\ln\left(2\sqrt{\alpha\left(\alpha
x^2+x\right)}+2\alpha x+1\right)\\\ \approx&amp;\frac{1}{2\sqrt{\alpha}}
\ln\left(2\sqrt{\alpha^2x^2}+2\alpha x\right)\\\
=&amp;\frac{1}{2\sqrt{\alpha}}\ln\left(4\alpha x\right)\\\
=&amp;\frac{1}{2\sqrt{\alpha}}\ln x+\text{const.} \end{align}\]</span>=&amp;(4x)\<br>
=&amp;x+ \end{align} \]</p>
<p>We can verify this empirically by, say,</p>
<pre><code>  a = c(0.2, 0.5, 1)
  f(1e6, a) __


[1] 15.196731 10.259171  7.600903


  1/(2*sqrt(a)) * (log(1e6) + log(4*a))__


[1] 15.196728 10.259170  7.600902</code></pre>
</section>
</section>
<section id="summary-of-this-chapter" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="summary-of-this-chapter"><span class="header-section-number">6.4</span> 4.5 Summary of this chapter</h2>
<p>We have given motivating examples and ways of using mixtures to model biological data. We saw how the EM algorithm is an interesting example of fitting a difficult-to-estimate probabilistic model to data by iterating between partial, simpler problems.</p>
<section id="finite-mixture-models" class="level4" data-number="6.4.0.1">
<h4 data-number="6.4.0.1" class="anchored" data-anchor-id="finite-mixture-models"><span class="header-section-number">6.4.0.1</span> Finite mixture models</h4>
<p>We have seen how to model mixtures of two or more normal distributions with different means and variances. We have seen how to decompose a given sample of data from such a mixture, even without knowing the latent variable, using the EM algorithm. The EM approach requires that we know the parametric form of the distributions and the number of components. In <a href="05-chap.html">Chapter 5</a>, we will see how we can find groupings in data even without relying on such information – this is then called clustering. We can keep in mind that there is a strong conceptual relationship between clustering and mixture modeling.</p>
</section>
<section id="common-infinite-mixture-models" class="level4" data-number="6.4.0.2">
<h4 data-number="6.4.0.2" class="anchored" data-anchor-id="common-infinite-mixture-models"><span class="header-section-number">6.4.0.2</span> Common infinite mixture models</h4>
<p>Infinite mixture models are good for constructing new distributions (such as the gamma-Poisson or the Laplace) out of more basic ones (such as binomial, normal, Poisson). Common examples are</p>
<ul>
<li><p>mixtures of normals (often with a hierarchical model on the means and the variances);</p></li>
<li><p>beta-binomial mixtures – where the probability \(p\) in the binomial is generated according to a \((a, b)\) distribution;</p></li>
<li><p>gamma-Poisson for read counts (see <a href="08-chap.html">Chapter 8</a>);</p></li>
<li><p>gamma-exponential for PCR.</p></li>
</ul>
</section>
<section id="applications" class="level4" data-number="6.4.0.3">
<h4 data-number="6.4.0.3" class="anchored" data-anchor-id="applications"><span class="header-section-number">6.4.0.3</span> Applications</h4>
<p>Mixture models are useful whenever there are several layers of experimental variability. For instance, at the lowest layer, our measurement precision may be limited by basic physical detection limits, and these may be modeled by a Poisson distribution in the case of a counting-based assay, or a normal distribution in the case of the continuous measurement. On top of there may be one (or more) layers of instrument-to-instrument variation, variation in the reagents, operator variaton etc.</p>
<p>Mixture models reflect that there is often heterogeneous amounts of variability (variances) in the data. In such cases, suitable data transformations, i.e., variance stabilizing transformations, are necessary before subsequent visualization or analysis. We’ll study in depth an example for RNA-Seq in <a href="08-chap.html">Chapter 8</a>, and this also proves useful in the normalization of next generation reads in microbial ecology (<a href="16-chap.html#ref-mcmurdie2014">McMurdie and Holmes 2014</a>).</p>
<p>Another important application of mixture modeling is the two-component model in multiple testing – we will come back to this in <a href="06-chap.html">Chapter 6</a>.</p>
</section>
<section id="the-ecdf-and-bootstrapping" class="level4" data-number="6.4.0.4">
<h4 data-number="6.4.0.4" class="anchored" data-anchor-id="the-ecdf-and-bootstrapping"><span class="header-section-number">6.4.0.4</span> The ECDF and bootstrapping</h4>
<p>We saw that by using the observed sample as a mixture we could generate many simulated samples that inform us about the sampling distribution of an estimate. This method is called the bootstrap and we will return to it several times, as it provides a way of evaluating estimates even when a closed form expression is not available (we say it is non-parametric).</p>
</section>
</section>
<section id="further-reading" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">6.5</span> 4.6 Further reading</h2>
<p>A useful book-long treatment of finite mixture models is by McLachlan and Peel (<a href="16-chap.html#ref-mclachlan2004">2004</a>); for the EM algorithm, see also the book by McLachlan and Krishnan (<a href="16-chap.html#ref- mclachlan2007algorithm">2007</a>). A recent book that presents all EM type algorithms within the Majorize-Minimization (MM) framework is by Lange (<a href="16-chap.html#ref-lange2016mm">2016</a>).</p>
<p>There are in fact mathematical reasons why many natural phenomena can be seen as mixtures: this occurs when the observed events are exchangeable (the order in which they occur doesn’t matter). The theory underlying this is quite mathematical, a good way to start is to look at the Wikipedia entry and the paper by Diaconis and Freedman (<a href="16-chap.html#ref-diaconis1980finite">1980</a>).</p>
<p>In particular, we use mixtures for high-throughput data. You will see examples in Chapters <a href="08-chap.html">8</a> and <a href="11-chap.html">11</a>.</p>
<p>The bootstrap can be used in many situations and is a very useful tool to know about, a friendly treatment is given in (<a href="16-chap.html#ref-efront">B. Efron and Tibshirani 1993</a>).</p>
<p>A historically interesting paper is the original article on variance stabilization by Anscombe (<a href="16-chap.html#ref-Anscombe1948">1948</a>), who proposed ways of making variance stabilizing transformations for Poisson and gamma-Poisson random variables. Variance stabilization is explained using the delta method in many standard texts in theoretical statistics, e.g., those by Rice (<a href="16-chap.html#ref-Rice:2007">2006, chap.&nbsp;6</a>) and Kéry and Royle (<a href="16-chap.html#ref-Kery2015">2015, 35</a>).</p>
<p>Kéry and Royle (<a href="16-chap.html#ref-Kery2015">2015</a>) provide a nice exploration of using R to build hierarchical models for abundance estimation in niche and spatial ecology.</p>
</section>
<section id="exercises" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="exercises"><span class="header-section-number">6.6</span> 4.7 Exercises</h2>
<p>__</p>
<p>Exercise 4.1</p>
<p><strong>The EM algorithm step by step.</strong> As an example dataset, we use the values in the file <code>Myst.rds</code>. As always, it is a good idea to first visualize the data. The histogram is shown in Figure 4.28. We are going to model these data as a mixture of two normal distributions with unknown means and standard deviations, and unknown mixture fraction. We’ll call the two components A and B.</p>
<pre><code>mx = readRDS("../data/Myst.rds")$yvar
str(mx)__


 num [1:1800] 0.3038 0.0596 -0.0204 0.1849 0.2842 ...


ggplot(tibble(mx), aes(x = mx)) + geom_histogram(binwidth = 0.025)__</code></pre>
<p>We start by randomly assigning the membership weights for each of the values in <code>mx</code> for each of the components</p>
<pre><code>wA = runif(length(mx))
wB = 1 - wA __</code></pre>
<p>We also need to set up some housekeeping variables: <code>iter</code> counts over the iterations of the EM algorithm; <code>loglik</code> stores the current log-likelihood; <code>delta</code> stores the change in the log-likelihood from the previous iteration to the current one. We also define the parameters <code>tolerance</code>, <code>miniter</code> and <code>maxiter</code> of the algorithm.</p>
<pre><code>iter      = 0
loglik    = -Inf
delta     = +Inf
tolerance = 1e-12
miniter   = 50
maxiter   = 1000 __</code></pre>
<p>Study the code below and answer the following questions:</p>
<ol type="1">
<li><p>Which lines correspond to the E-step, which to the M-step?</p></li>
<li><p>What is the role of <code>tolerance</code>, <code>miniter</code> and <code>maxiter</code>?</p></li>
<li><p>Compare the result of what we are doing here to the output of the <code>normalmixEM</code> function from the <strong><a href="https://cran.r-project.org/web/packages/mixtools/">mixtools</a></strong> package.</p></li>
</ol>
<pre><code>while((delta &gt; tolerance) &amp;&amp; (iter &lt;= maxiter) || (iter &lt; miniter)) {
  lambda = mean(wA)
  muA = weighted.mean(mx, wA)
  muB = weighted.mean(mx, wB)
  sdA = sqrt(weighted.mean((mx - muA)^2, wA))
  sdB = sqrt(weighted.mean((mx - muB)^2, wB))

  pA   =    lambda    * dnorm(mx, mean = muA, sd = sdA)
  pB   = (1 - lambda) * dnorm(mx, mean = muB, sd = sdB)
  ptot = pA + pB
  wA   = pA / ptot
  wB   = pB / ptot

  loglikOld = loglik
  loglik = sum(log(pA + pB))
  delta = abs(loglikOld - loglik)
  iter = iter + 1
}
iter __


[1] 447


c(lambda, muA, muB, sdA, sdB)__


[1]  0.4756 -0.1694  0.1473  0.0983  0.1498</code></pre>
<p><a href="04-chap_files/figure-html/fig-EMillustrate-1-1.png &quot;Figure 4.28: Histogram of mx, our example data for the EM algorithm.&quot;"><img src="04-chap_files/figure-html/fig- EMillustrate-1-1.png" class="img-fluid"></a></p>
<p>Figure 4.28: Histogram of <code>mx</code>, our example data for the EM algorithm.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>The first five lines in the <code>while</code> loop implement the <em>Maximization step</em>. Given the current values of <code>wA</code> and <code>wB</code>, we estimate the parameters of the mixture model using the maximum-likelihood estimators: the mixture fraction <code>lambda</code> by the mean of <code>wA</code>, and the parameters of the two normal distribution components (<code>muA</code>, <code>sdA</code>) and (<code>muB</code>, <code>sdB</code>) by the sample means and the sample standard deviations. To take into account the membership weights, we use the weighted mean (function <code>weighted.mean</code>) and standard deviation.</p>
<p>Next comes the <em>Expectation step</em>. For each of the elements in the data vector <code>mx</code>, we compute the probability densities <code>pA</code> and <code>pB</code> for the generative distribution models A and B, using the normal density function <code>dnorm</code>, weighted by the mixture fractions <code>lambda</code> and <code>(1-lambda)</code>, respectively. From this, we compute the updated membership weights <code>wA</code> and <code>wB</code>, according to Equation 4.5.</p>
<p>Given the membership weights and the parameters, the logarithmic likelihood <code>loglik</code> is easily computed, and the <code>while</code> loop iterates these steps.</p>
<p>The termination criterion for the loop is based on <code>delta</code>, the change in the likelihood. The loop can end if this becomes smaller than <code>tolerance</code>. This is a simple way of checking whether the algorithm has converged. The additional conditions on <code>iter</code> make sure that at least <code>miniter</code> iterations are run, and that the loop always stops after <code>maxiter</code> iterations. The latter is to make sure that the loop terminates in finite time no matter what. (“Professional” implementations of such iterative algorithms typically work a bit harder to decide what is the best time to stop.)</p>
<p>Finally, let’s compare our estimates to those from the function <code>normalmixEM</code> from the <strong><a href="https://cran.r-project.org/web/packages/mixtools/">mixtools</a></strong> package.</p>
<pre><code>gm = mixtools::normalmixEM(mx, k = 2)__


number of iterations= 215 


with(gm, c(lambda[1], mu, sigma))__


[1]  0.4757 -0.1694  0.1473  0.0983  0.1498</code></pre>
<p>__</p>
<p>Exercise 4.2</p>
<p>Why do we often consider the logarithm of the likelihood rather than the likelihood? E.g., in the EM code above, why did we work with the probabilities on the logarithmic scale?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Likelihoods often (whenever the data points are sampled independently) take the form of a product. This is, for instance, the case in Equation 4.4. Calculating the derivative, for likelihood optimisation, would then require application of the product rule. On the logarithmic scale, the product turns into a sum, and the derivative of a sum is simply the sum of the derivatives of the individual summands.</p>
<p>An additional reason comes from the way computers implement arithmetic. They commonly use a floating point representation of numbers with a finite number of bits. E.g., the IEEE 754-2008 standard uses 64 bits for a <em>double- precision</em> number: 1 bit for the sign, 52 for the mantissa (also called significand), 11 for the exponent. Multiplication between such numbers implies addition of the exponents, but the range of the exponent is only \(0\) to \(2^{11}-1=2047\). Even likelihoods that involve only a few hundred data points can lead to arithmetic overflow or other problems with precision. On the logarithmic scale, where the product is a sum, the workload tends to be better distributed between mantissa and exponent, and log-likelihoods even with millions of data points to be handled with reasonable precision.</p>
<p>See also Gregory Gundersen’s post on the <a href="https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/">Log-Sum-Exp Trick for normalizing vectors of log probabilities</a>.</p>
<p>__</p>
<p>Exercise 4.3</p>
<p>Compare the theoretical values of the gamma-Poisson distribution with parameters given by the estimates in <code>ofit$par</code> in Section 4.4.3 to the data used for the estimation using a QQ-plot.</p>
<p>__</p>
<p>Exercise 4.4</p>
<p><strong>Mixture modeling examples for regression</strong>. The <strong><a href="https://cran.r-project.org/web/packages/flexmix/">flexmix</a></strong> package (<a href="16-chap.html#ref-Grun2012">Grün, Scharl, and Leisch 2012</a>) enables us to cluster and fit regressions to the data at the same time. The standard M-step <code>FLXMRglm</code> of <strong><a href="https://cran.r-project.org/web/packages/flexmix/">flexmix</a></strong> is an interface to R’s generalized linear modeling facilities (the <code>glm</code> function). Load the package and an example dataset.</p>
<pre><code>library("flexmix")
data("NPreg")__</code></pre>
<ol type="1">
<li><p>First, plot the data and try to guess how the points were generated.</p></li>
<li><p>Fit a two component mixture model using the commands</p></li>
</ol>
<pre><code>m1 = flexmix(yn ~ x + I(x^2), data = NPreg, k = 2)__</code></pre>
<ol start="3" type="1">
<li><p>Look at the estimated parameters of the mixture components and make a truth table that cross-classifies true classes versus cluster memberships. What does the summary of the object <code>m1</code> show us?</p></li>
<li><p>Plot the data again, this time coloring each point according to its estimated class.</p></li>
</ol>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>ggplot(NPreg, aes(x = x, y = yn)) + geom_point()__</code></pre>
<p><a href="04-chap_files/figure- html/fig-npreg-1.png" title="Figure 4.29: The points seem to come from two different generative processes, one is linear; the other quadratic."><img src="04-chap_files/figure-html/fig-npreg-1.png" class="img-fluid"></a></p>
<p>Figure 4.29: The points seem to come from two different generative processes, one is linear; the other quadratic.</p>
<p>The components are:</p>
<pre><code>modeltools::parameters(m1, component = 1)__


                      Comp.1
coef.(Intercept) -0.20998685
coef.x            4.81807854
coef.I(x^2)       0.03613061
sigma             3.47665584


modeltools::parameters(m1, component = 2)__


                     Comp.2
coef.(Intercept) 14.7167886
coef.x            9.8468507
coef.I(x^2)      -0.9683734
sigma             3.4795657</code></pre>
<p>The parameter estimates of both components are close to the true values. A cross-tabulation of true classes and cluster memberships can be obtained by</p>
<pre><code>table(NPreg$class, modeltools::clusters(m1))__


   
     1  2
  1 95  5
  2  5 95</code></pre>
<p>For our example data, the ratios of both components are approximately 0.7, indicating the overlap of the classes at the cross-section of line and parabola.</p>
<pre><code>summary(m1)__</code></pre>
<p>The summary shows the estimated prior probabilities \(<em>k\), the number of observations assigned to the two clusters, the number of observations where \(p</em>{nk}&gt;\) (with a default of \(^{-4}\)), and the ratio of the latter two numbers. For well- separated components, a large proportion of observations with non-vanishing posteriors \(p_{nk}\) should be assigned to their cluster, giving a ratio close to 1.</p>
<pre><code>NPreg = mutate(NPreg, gr = factor(class))
ggplot(NPreg, aes(x = x, y = yn, group = gr)) +
   geom_point(aes(colour = gr, shape = gr)) +
   scale_colour_hue(l = 40, c = 180)__</code></pre>
<p><a href="04-chap_files/figure- html/fig-npregC-1.png" title="Figure 4.30: Regression example using flexmix with the points colored according to their estimated class. You can see that at the intersection we have an `identifiability’ problem: we cannot distinguish points that belong to the straight line from ones that belong to the parabole."><img src="04-chap_files/figure-html/fig-npregC-1.png" class="img-fluid"></a></p>
<p>Figure 4.30: Regression example using <code>flexmix</code> with the points colored according to their estimated class. You can see that at the intersection we have an `identifiability’ problem: we cannot distinguish points that belong to the straight line from ones that belong to the parabole.</p>
<p>__</p>
<p>Exercise 4.5</p>
<p><strong>Other hierarchical noise models:</strong><br>
Find two papers that explore the use of other infinite mixtures for modeling molecular biology technological variation.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>The paper by Chen, Xie, and Story (<a href="16-chap.html#ref-Chen2011">2011</a>) explores an exponential-Poisson model for modeling background noise in bead arrays. Wills et al.&nbsp;(<a href="16-chap.html#ref-Wills2013">2013</a>) compares several Poisson mixture models.</p>
<p>Anscombe, Francis J. 1948. “The Transformation of Poisson, Binomial and Negative-Binomial Data.” <em>Biometrika</em> , 246–54.</p>
<p>Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. Springer.</p>
<p>Bronštein, Il’ja N., and Konstantin A Semendjajew. 1979. <em>Taschenbuch Der Mathematik</em>. B.G. Teubner Verlagsgesellschaft, Leipzig; Verlag Nauka, Moscow.</p>
<p>Bulmer, Michael George. 2003. <em>Francis Galton: Pioneer of Heredity and Biometry</em>. JHU Press.</p>
<p>Chen, Min, Yang Xie, and Michael Story. 2011. “An Exponential-Gamma Convolution Model for Background Correction of Illumina BeadArray Data.” <em>Communications in Statistics-Theory and Methods</em> 40 (17): 3055–69.</p>
<p>Diaconis, Persi, and David Freedman. 1980. “Finite Exchangeable Sequences.” <em>The Annals of Probability</em> , 745–64.</p>
<p>Diaconis, Persi, and Susan Holmes. 1994. “Gray Codes for Randomization Procedures.” <em>Statistics and Computing</em> 4 (4): 287–302.</p>
<p>Efron, Bradley, and Robert J Tibshirani. 1994. <em>An Introduction to the Bootstrap</em>. CRC press.</p>
<p>Efron, B., and R. Tibshirani. 1993. <em>An Introduction to the Bootstrap</em>. Chapman &amp; Hall/CRC.</p>
<p>Grün, Bettina, Theresa Scharl, and Friedrich Leisch. 2012. “Modelling Time Course Gene Expression Data with Finite Mixtures of Linear Additive Models.” <em>Bioinformatics</em> 28 (2): 222–28. <a href="https://doi.org/10.1093/bioinformatics/btr653" class="uri">https://doi.org/10.1093/bioinformatics/btr653</a>.</p>
<p>Hoeting, Jennifer A, David Madigan, Adrian E Raftery, and Chris T Volinsky. 1999. “Bayesian Model Averaging: A Tutorial.” <em>Statistical Science</em> , 382–401.</p>
<p>Kéry, Marc, and J Andrew Royle. 2015. <em>Applied Hierarchical Modeling in Ecology: Analysis of Distribution, Abundance and Species Richness in r and BUGS: Volume 1: Prelude and Static Models</em>. Academic Press.</p>
<p>Kristiansson, Erik, Michael Thorsen, Markus J Tamás, and Olle Nerman. 2009. “Evolutionary Forces Act on Promoter Length: Identification of Enriched Cis- Regulatory Elements.” <em>Molecular Biology and Evolution</em> 26 (6): 1299–1307.</p>
<p>Kuan, Pei Fen, Dongjun Chung, Guangjin Pan, James A Thomson, Ron Stewart, and Sündüz Keleş. 2011. “A Statistical Framework for the Analysis of ChIP-Seq Data.” <em>Journal of the American Statistical Association</em> 106 (495): 891–903.</p>
<p>Lange, Kenneth. 2016. <em>MM Optimization Algorithms</em>. SIAM.</p>
<p>McLachlan, Geoffrey, and Thriyambakam Krishnan. 2007. <em>The EM Algorithm and Extensions</em>. Vol. 382. John Wiley &amp; Sons.</p>
<p>McLachlan, Geoffrey, and David Peel. 2004. <em>Finite Mixture Models</em>. John Wiley &amp; Sons.</p>
<p>McMurdie, Paul J, and Susan Holmes. 2014. “Waste Not, Want Not: Why Rarefying Microbiome Data Is Inadmissible.” <em>PLoS Computational Biology</em> 10 (4): e1003531.</p>
<p>Purdom, Elizabeth, and Susan P Holmes. 2005. “Error Distribution for Gene Expression Data.” <em>Statistical Applications in Genetics and Molecular Biology</em> 4 (1).</p>
<p>Rice, John. 2006. <em>Mathematical Statistics and Data Analysis</em>. Cengage Learning.</p>
<p>Shalizi, Cosma. 2017. <em>Advanced Data Analysis from an Elementary Point of View</em>. Cambridge University Press. <a href="https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf" class="uri">https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf</a>.</p>
<p>Slonim, Noam, Gurinder Singh Atwal, Gašper Tkačik, and William Bialek. 2005. “Information-Based Clustering.” <em>PNAS</em> 102 (51): 18297–302.</p>
<p>Tibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> , 267–88.</p>
<p>Wills, Quin F, Kenneth J Livak, Alex J Tipping, Tariq Enver, Andrew J Goldson, Darren W Sexton, and Chris Holmes. 2013. “Single-Cell Gene Expression Analysis Reveals Genetic Associations Masked in Whole-Tissue Experiments.” <em>Nature Biotechnology</em> 31 (8): 748–52.</p>
<p>Zeileis, Achim, Christian Kleiber, and Simon Jackman. 2008. “Regression Models for Count Data in R.” <em>Journal of Statistical Software</em> 27 (8). <a href="http://www.jstatsoft.org/v27/i08/" class="uri">http://www.jstatsoft.org/v27/i08/</a>.</p>
<p>Page built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./03-chap.html" class="pagination-link" aria-label="3.1 이 장의 목표">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">3.1 이 장의 목표</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./05-chap.html" class="pagination-link" aria-label="5.1 이 장의 목표">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">5.1 이 장의 목표</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>