<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; 4.1 Goals for this chapter – Modern Statistics for Modern Biology</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./05-chap.html" rel="next">
<link href="./03-chap.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-22854ec117201859c8a7ba6f538122c9.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="msmb.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./04-chap.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">4.1 Goals for this chapter</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Modern Statistics for Modern Biology</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Home</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The challenge: heterogeneity</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">1.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">2.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">3.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-chap.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">4.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">5.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">6.1 Goals for this Chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">7.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">8.1 Goals of this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">9.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">10.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">11.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">12.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">13.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">14-chap.html</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">15-chap.html</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">16-chap.html</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#finite-mixtures" id="toc-finite-mixtures" class="nav-link active" data-scroll-target="#finite-mixtures"><span class="header-section-number">6.1</span> 4.2 Finite mixtures</a>
  <ul class="collapse">
  <li><a href="#simple-examples-and-computer-experiments" id="toc-simple-examples-and-computer-experiments" class="nav-link" data-scroll-target="#simple-examples-and-computer-experiments"><span class="header-section-number">6.1.1</span> 4.2.1 Simple examples and computer experiments</a></li>
  <li><a href="#discovering-the-hidden-class-labels" id="toc-discovering-the-hidden-class-labels" class="nav-link" data-scroll-target="#discovering-the-hidden-class-labels"><span class="header-section-number">6.1.2</span> 4.2.2 Discovering the hidden class labels</a></li>
  <li><a href="#models-for-zero-inflated-data" id="toc-models-for-zero-inflated-data" class="nav-link" data-scroll-target="#models-for-zero-inflated-data"><span class="header-section-number">6.1.3</span> 4.2.3 Models for zero inflated data</a></li>
  <li><a href="#more-than-two-components" id="toc-more-than-two-components" class="nav-link" data-scroll-target="#more-than-two-components"><span class="header-section-number">6.1.4</span> 4.2.4 More than two components</a></li>
  </ul></li>
  <li><a href="#empirical-distributions-and-the-nonparametric-bootstrap" id="toc-empirical-distributions-and-the-nonparametric-bootstrap" class="nav-link" data-scroll-target="#empirical-distributions-and-the-nonparametric-bootstrap"><span class="header-section-number">6.2</span> 4.3 Empirical distributions and the nonparametric bootstrap</a></li>
  <li><a href="#infinite-mixtures" id="toc-infinite-mixtures" class="nav-link" data-scroll-target="#infinite-mixtures"><span class="header-section-number">6.3</span> 4.4 Infinite mixtures</a>
  <ul class="collapse">
  <li><a href="#infinite-mixture-of-normals" id="toc-infinite-mixture-of-normals" class="nav-link" data-scroll-target="#infinite-mixture-of-normals"><span class="header-section-number">6.3.1</span> 4.4.1 Infinite mixture of normals</a></li>
  <li><a href="#infinite-mixtures-of-poisson-variables." id="toc-infinite-mixtures-of-poisson-variables." class="nav-link" data-scroll-target="#infinite-mixtures-of-poisson-variables."><span class="header-section-number">6.3.2</span> 4.4.2 Infinite mixtures of Poisson variables.</a></li>
  <li><a href="#gamma-distribution-two-parameters-shape-and-scale" id="toc-gamma-distribution-two-parameters-shape-and-scale" class="nav-link" data-scroll-target="#gamma-distribution-two-parameters-shape-and-scale"><span class="header-section-number">6.3.3</span> 4.4.3 Gamma distribution: two parameters (shape and scale)</a></li>
  <li><a href="#variance-stabilizing-transformations" id="toc-variance-stabilizing-transformations" class="nav-link" data-scroll-target="#variance-stabilizing-transformations"><span class="header-section-number">6.3.4</span> 4.4.4 Variance stabilizing transformations</a></li>
  </ul></li>
  <li><a href="#summary-of-this-chapter" id="toc-summary-of-this-chapter" class="nav-link" data-scroll-target="#summary-of-this-chapter"><span class="header-section-number">6.4</span> 4.5 Summary of this chapter</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">6.5</span> 4.6 Further reading</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">6.6</span> 4.7 Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">4.1 Goals for this chapter</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><a href="imgs/t_distribution.png"><img src="imgs/t_distribution.png" class="img-fluid"></a></p>
<p>One of the main challenges of biological data analysis is dealing with heterogeneity. The quantities we are interested in often do not show a simple, unimodal “textbook distribution”. For example, in the last part of <a href="02-chap.html">Chapter 2</a> we saw how the histogram of sequence scores in <a href="02-chap.html#fig-ScoreMixture-1">Figure 2.27</a> had two separate modes, one for CpG- islands and one for non-islands. We can see the data as a simple mixture of a few (in this case: two) components. We call these <em>finite mixtures</em>. Other mixtures can involve almost as many components as we have observations. These we call <em>infinite mixtures</em> 1.</p>
<p>1 We will see that—as for so many modeling choices–the right complexity of the mixture is in the eye of the beholder and often depends on the amount of data and the resolution and smoothness we want to attain.</p>
<p>In <a href="01-chap.html">Chapter 1</a> we saw how a simple generative model with a Poisson distribution led us to make useful inference in the detection of an epitope. Unfortunately, a satisfactory fit to real data with such a simple model is often out of reach. However, simple models such as the normal or Poisson distributions can serve as building blocks for more realistic models using the mixing framework that we cover in this chapter. Mixtures occur naturally for flow cytometry data, biometric measurements, RNA-Seq, ChIP-Seq, microbiome and many other types of data collected using modern biotechnologies. In this chapter we will learn from simple examples how to build more realistic models of distributions using mixtures.</p>
<p>In this chapter we will:</p>
<ul>
<li><p>Generate our own mixture model data from distributions composed of two normal populations.</p></li>
<li><p>See how the Expectation-Maximization (EM) algorithm enables us to “reverse engineer” the underlying mixtures in dataset.</p></li>
<li><p>Use a special type of mixture called zero-inflation for data such as ChIP-Seq data that have many extra zeros.</p></li>
<li><p>Discover the empirical cumulative distribution: a special mixture that we can build from observed data. This will enable us to see how we can simulate the variability of our estimates using the bootstrap.</p></li>
<li><p>Build the Laplace distribution as an instance of an infinite mixture model with many components. We will use it to model promoter lengths and microarray intensities.</p></li>
<li><p>Have our first encounter with the gamma-Poisson distribution, a hierarchical model useful for RNA-Seq data. We will see it comes naturally from mixing different Poisson distributed sources.</p></li>
<li><p>See how mixture models enable us to choose data transformations.</p></li>
</ul>
<section id="finite-mixtures" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="finite-mixtures"><span class="header-section-number">6.1</span> 4.2 Finite mixtures</h2>
<section id="simple-examples-and-computer-experiments" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="simple-examples-and-computer-experiments"><span class="header-section-number">6.1.1</span> 4.2.1 Simple examples and computer experiments</h3>
<p>Here is a first example of a mixture model with two equal-sized components. The generating process has two steps:</p>
<p><strong>Flip a fair coin.</strong></p>
<p>If it comes up heads: Generate a random number from a normal distribution with mean 1 and variance 0.25.</p>
<p>If it comes up tails: Generate a random number from a normal distribution with mean 3 and variance 0.25. The histogram shown in Figure 4.1 was produced by repeating these two steps 10000 times using the following code.</p>
<pre><code>coinflips = (runif(10000) &gt; 0.5)
table(coinflips)__


coinflips
FALSE  TRUE 
 5003  4997 


oneFlip = function(fl, mean1 = 1, mean2 = 3, sd1 = 0.5, sd2 = 0.5) {
  if (fl) {
   rnorm(1, mean1, sd1)
  } else {
   rnorm(1, mean2, sd2)
  }
}
fairmix = vapply(coinflips, oneFlip, numeric(1))
library("ggplot2")
library("dplyr")
ggplot(tibble(value = fairmix), aes(x = value)) +
     geom_histogram(fill = "purple", binwidth = 0.1)__</code></pre>
<p><a href="04-chap_files/figure- html/fig-twocoins-1.png" title="Figure 4.1: Histogram of 10,000 random draws from a fair mixture of two normals. The left hand part of the histogram is dominated by numbers generated from (A), on the right from (B)."><img src="04-chap_files/figure-html/fig-twocoins-1.png" class="img-fluid"></a></p>
<p>Figure 4.1: Histogram of 10,000 random draws from a fair mixture of two normals. The left hand part of the histogram is dominated by numbers generated from (A), on the right from (B).</p>
<p>__</p>
<p>Question 4.1</p>
<p>How can you use R’s vectorized syntax to remove the <code>vapply</code>-loop and generate the <code>fairmix</code> vector more efficiently?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>means = c(1, 3)
sds   = c(0.5, 0.5)
values = rnorm(length(coinflips),
               mean = ifelse(coinflips, means[1], means[2]),
               sd   = ifelse(coinflips, sds[1],   sds[2]))__</code></pre>
<p>__</p>
<p>Question 4.2</p>
<p>Using your improved code, use one million coin flips and make a histogram with 200 bins. What do you notice?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>fair = tibble(
  coinflips = (runif(1e6) &gt; 0.5),
  values = rnorm(length(coinflips),
                 mean = ifelse(coinflips, means[1], means[2]),
                 sd   = ifelse(coinflips, sds[1],   sds[2])))
ggplot(fair, aes(x = values)) +
     geom_histogram(fill = "purple", bins = 200)__</code></pre>
<p><a href="04-chap_files/figure-html/fig- limitinghistogram-1.png" title="Figure 4.2: Similar to Figure fig-twocoins, but with one million observations."><img src="04-chap_files/figure-html/fig- limitinghistogram-1.png" class="img-fluid"></a></p>
<p>Figure 4.2: Similar to Figure 4.1, but with one million observations.</p>
<p>Figure 4.2 illustrates that as we increase the number of bins and the number of observations per bin, the histogram approaches a smooth curve. This smooth limiting curve is called the <strong>density</strong> function of the random variable <code>fair$values</code>.</p>
<p>The density function for a normal \(N(,)\) random variable can be written explicitly. We usually call it</p>
<p>\[ (x)=e<sup>{-()</sup>2}. \]</p>
<p>__</p>
<p>Question 4.3</p>
<ol type="1">
<li>Plot a histogram of those values of <code>fair$values</code> for which <code>coinflips</code> is <code>TRUE</code>. Hint: use <code>y = ..density..</code> in the call to <code>aes</code> (which means that the vertical axis is to show the proportion of counts) and set the binwidth to 0.01.</li>
<li>Overlay the line corresponding to \((z)\).</li>
</ol>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>ggplot(dplyr::filter(fair, coinflips), aes(x = values)) +
  geom_histogram(aes(y = after_stat(density)), fill = "purple", binwidth = 0.01) +
  stat_function(fun = dnorm, color = "red",
                args = list(mean = means[1], sd = sds[1]))__</code></pre>
<p><a href="04-chap_files/figure-html/fig-overlaydensity-1.png &quot;Figure 4.3: Histogram of half a million draws from the normal distribution N(\mu=1,\sigma^2=0.5^2). The curve is the theoretical density \phi(x) calculated using the function dnorm.&quot;"><img src="04-chap_files/figure-html/fig- overlaydensity-1.png" class="img-fluid"></a></p>
<p>Figure 4.3: Histogram of half a million draws from the normal distribution \(N(,<sup>2=0.5</sup>2)\). The curve is the theoretical density \((x)\) calculated using the function <code>dnorm</code>.</p>
<p>In fact we can write the mathematical formula for the density of all of <code>fair$values</code> (the limiting curve that the histograms tend to look like) as a sum of the two densities.</p>
<p>\[ f(x)=_1(x)+_2(x), \]</p>
<p>where \(_1\) is the density of the normal \(N(_1=1,^2=0.25)\) and \(_2\) is the density of the normal \(N(_2=3,^2=0.25)\). Figure 4.4 was generated by the following code.</p>
<pre><code>fairtheory = tibble(
  x = seq(-1, 5, length.out = 1000),
  f = 0.5 * dnorm(x, mean = means[1], sd = sds[1]) +
      0.5 * dnorm(x, mean = means[2], sd = sds[2]))
ggplot(fairtheory, aes(x = x, y = f)) +
  geom_line(color = "red", linewidth = 1.5) + ylab("mixture density")__</code></pre>
<p><a href="04-chap_files/figure- html/fig-twodensity-1.png" title="Figure 4.4: The theoretical density of the mixture."><img src="04-chap_files/figure-html/fig-twodensity-1.png" class="img-fluid"></a></p>
<p>Figure 4.4: The theoretical density of the mixture.</p>
<p>In this case, the mixture model is extremely visible as the two component distributions have little overlap. Figure 4.4 shows two distinct peaks: we call this a <strong>bimodal</strong> distribution. In practice, in many cases the separation between the mixture components is not so obvious—but nevertheless relevant.</p>
<p><a href="04-chap_files/figure- html/fig-histmystery-1.png" title="Figure 4.5: A mixture of two normals that is harder to recognize."><img src="04-chap_files/figure-html/fig-histmystery-1.png" class="img-fluid"></a></p>
<p>Figure 4.5: A mixture of two normals that is harder to recognize.</p>
<p>__</p>
<p>Question 4.4</p>
<p>Figure 4.5 is a histogram of a fair mixture of two normals with the same variances. Can you guess the two <em>mean</em> parameters of the component distributions? Hint: you can use trial and error, and simulate various mixtures to see if you can make a matching histogram. Looking at the R code for the chapter will show you exactly how the data were generated.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>The following code colors in red the points that were generated from the <em>heads</em> coin flips and in blue those from the <em>tails</em>. Its output, in Figure 4.6, shows the two underlying distributions.</p>
<pre><code>head(mystery, 3)__


# A tibble: 3 × 2
  coinflips values
  &lt;lgl&gt;      &lt;dbl&gt;
1 FALSE       2.40
2 FALSE       1.66
3 TRUE        1.22


br = with(mystery, seq(min(values), max(values), length.out = 30))
ggplot(mystery, aes(x = values)) +
  geom_histogram(data = dplyr::filter(mystery, coinflips),
     fill = "red", alpha = 0.2, breaks = br) +
  geom_histogram(data = dplyr::filter(mystery, !coinflips),
     fill = "darkblue", alpha = 0.2, breaks = br) __</code></pre>
<p><a href="04-chap_files/figure-html/fig- betterhistogram-1-1.png" title="Figure 4.6: The mixture from Figure fig-histmystery, but with the two components colored in red and blue."><img src="04-chap_files/figure-html/fig- betterhistogram-1-1.png" class="img-fluid"></a></p>
<p>Figure 4.6: The mixture from Figure 4.5, but with the two components colored in red and blue.</p>
<p>In Figure 4.6, the bars from the two component distributions are plotted on top of each other. A different way of showing the components is Figure 4.7, produced by the code below.</p>
<pre><code>ggplot(mystery, aes(x = values, fill = coinflips)) +
  geom_histogram(data = dplyr::filter(mystery, coinflips),
     fill = "red", alpha = 0.2, breaks = br) +
  geom_histogram(data = dplyr::filter(mystery, !coinflips),
     fill = "darkblue", alpha = 0.2, breaks = br) +
  geom_histogram(fill = "purple", breaks = br, alpha = 0.2)__</code></pre>
<p><a href="04-chap_files/figure-html/fig- comparecomponents-1-1.png" title="Figure 4.7: As Figure fig-betterhistogram-1, with stacked bars for the two mixture components."><img src="04-chap_files/figure-html/fig- comparecomponents-1-1.png" class="img-fluid"></a></p>
<p>Figure 4.7: As Figure 4.6, with stacked bars for the two mixture components.</p>
<p>__</p>
<p>Question 4.5</p>
<p>Why do the bars in Figure 4.7, but not those in Figure 4.6 have the same heights as in Figure 4.5?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>In Figures 4.7 and 4.5, each count occupies a different piece of vertical space in a bin. In Figure 4.6, in the overlapping (darker) region, some of the counts falling within the same bin are overplotted.</p>
<p>In Figures 4.6 and 4.7, we were able to use the <code>coinflips</code> column in the data to disentangle the components. In real data, this information is missing.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/book_icon.png" title="A book-long treatment on the subject of finite mixtures [@mclachlan2004]."><img src="imgs/book_icon.png" class="img-fluid figure-img"></a></p>
<figcaption>A book-long treatment on the subject of finite mixtures (McLachlan and Peel 2004).</figcaption>
</figure>
</div>
<p>A book-long treatment on the subject of finite mixtures (<a href="16-chap.html#ref-mclachlan2004">McLachlan and Peel 2004</a>).</p>
</section>
<section id="discovering-the-hidden-class-labels" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="discovering-the-hidden-class-labels"><span class="header-section-number">6.1.2</span> 4.2.2 Discovering the hidden class labels</h3>
<p>We use a method called the <em>expectation-maximization (EM) algorithm</em> to infer the value of the hidden groupings. The EM algorithm is a popular iterative procedure that alternates between pretending we know one part of the solution to compute the other part, and pretending the other part is known and computing the first part, and so on, until convergence. More concretely, it alternates between</p>
<ul>
<li><p>pretending we know the probabilities with which each observation belongs to the different mixture components and, from this, estimating the parameters of the components, and</p></li>
<li><p>pretending we know the parameters of the mixture components and estimating the probabilities with which each observation belongs to the components.</p></li>
</ul>
<p>Let’s take a simple example. We measure a variable \(x\) on a series of objects that we think come from two groups, although we do not know the group labels. We start by <em>augmenting</em> 2 the data with the unobserved (latent) group label, which we will call \(U\).</p>
<p>2 Adding another variable which was not measured, called a hidden or <strong>latent variable</strong>.</p>
<p>We are interested in finding the values of \(U\) and the unknown parameters \(\) of the underlying distribution of the groups. We will use the maximum likelihood approach introduced in <a href="02-chap.html">Chapter 2</a> to estimate the parameters that make the data \(x\) the most likely. We can write the probability densities</p>
<p>\[ p(x,u,|,) = p(x,|,u,),p(u,|,). \]</p>
<section id="mixture-of-normals" class="level4" data-number="6.1.2.1">
<h4 data-number="6.1.2.1" class="anchored" data-anchor-id="mixture-of-normals"><span class="header-section-number">6.1.2.1</span> Mixture of normals</h4>
<p>For instance, we could generalize our previous mixture model with two normal distributions Equation 4.1 by allowing non-equal mixture fractions,</p>
<p>\[ f(x)=_1(x)+(1-)_2(x), \]</p>
<p>for \(\). Similarly as above, \(_k\) is the density of the normal \(N(_k,_k^2)\) for \(k=1\) and \(k=2\), respectively. Then, the parameter vector \(\) is a five-tuple of the two means, the two standard deviations, and the mixture parameter \(\). In other words, \(=(_1,_2,_1,_2,)\). Here is an example of data generated according to such a model. The labels are denoted by \(u\).</p>
<pre><code>mus = c(-0.5, 1.5)
lambda = 0.5
u = sample(2, size = 100, replace = TRUE, prob = c(lambda, 1-lambda))
x = rnorm(length(u), mean = mus[u])
dux = tibble(u, x)
head(dux)__


# A tibble: 6 × 2
      u     x
  &lt;int&gt; &lt;dbl&gt;
1     2 0.303
2     2 2.65 
3     1 0.484
4     2 3.04 
5     2 1.10 
6     2 1.96 </code></pre>
<p>If we know the labels \(u\), we can estimate the parameters using separate MLEs for each group. The overall likelihood is</p>
<p>\[ p(x, u ,|, ) = ( _{\{i:,u_i=1\}} <em>1(x_i) ) ( </em>{\{i:,u_i=2\}} _2(x_i) ). \]</p>
<p>Its maximization can be split into three independent pieces: we find \(_1\) and \(_1\) by maximizing the first bracketed expression on the right hand side of Equation 4.4, \(_2\) and \(_2\) by maximizing the second, and \(\) from the empirical frequencies of the labels:</p>
<pre><code>group_by(dux, u) |&gt; summarize(mu = mean(x), sigma = sd(x))__


# A tibble: 2 × 3
      u     mu sigma
  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;
1     1 -0.558  1.05
2     2  1.41   1.04


table(dux$u) / nrow(dux)__


   1    2 
0.55 0.45 </code></pre>
<p>__</p>
<p>Question 4.6</p>
<p>Suppose we knew the mixing proportion \(=\), but not the \(u_i\), so that the density is \(_1(x)+_2(x)\). Try writing out the (log) likelihood. What prevents us from solving for the MLE explicitly here?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>See, e.g., the chapter on <em>Mixture Models</em> in Shalizi (<a href="16-chap.html#ref-CosmaShalizi2017">2017</a>) for the computation of the likelihood of a finite mixture of normals. “If we try to estimate the mixture model, then, we’re doing weighted maximum likelihood, with weights given by the posterior label probabilities. These, to repeat, depend on the parameters we are trying to estimate, so there seems to be a vicious circle.”</p>
<p>In many cases we neither know the \(u\) labels nor the mixture proportions. What we can do is start with an initial guess for the labels and pretend to know them, estimate the parameters as above, and then go through an iterative cycle, updating at each step our current best guess of the labels and the parameters until our estimates no longer substantially change (i.e., until they seem to have converged) and the likelihood has reached an optimum.</p>
<p>In fact we can do something more elaborate and replace the “hard” labels \(u\) for each observation (it is either in group 1 or 2) by membership weights that sum up to 1. The mixture model 4.3 suggests that we interpret</p>
<p>\[ w(x, 1) = \]</p>
<p>as the probability that an observation with value \(x\) was generated by the first mixture component, and analogously \(w(x, 2) = 1 - w(x, 1)\) for the second component. In other words, while \(\) is the prior probability that an observation that we have not yet seen comes from mixture component 1, \(w(x,1)\) is the corresponding posterior probability once we have observed its value \(x\). This suggests the following iterative algorithm:</p>
<p><strong>E step</strong> : Assuming that \(\) (i.e., the means, standard deviations and \(\)) is known, evaluate the membership weights 4.5.</p>
<p><strong>M step</strong> : Given the membership weights of each observation \(x_i\), determine a new, improved maximum likelihood estimate of \(\).</p>
<p>Iterate until \(\) and the likelihood have converged. At this point, please check out Exercise 4.1 for a worked out demonstration with code. In fact, this algorithm is far more general than our particular application example here. A very readable exposition is presented by (<a href="16-chap.html#ref-Bishop:PRML">Bishop 2006</a>), here some of the highlights:</p>
<p>Our goal is to maximize the marginal likelihood of a probabilistic model that involves an observed variable \(x\), an unobserved variable \(u\) and some parameter \(\). In our simple example, \(u\) is a categorical variable with two possible values, and \(x\) a real number. In general, both \(x\) and \(u\) can be a tuple of multiple individual variables (i.e., they can be multivariate) of any variable type. The marginal likelihood is computed by taking the expectation (i.e., a weighted average) across all possible values of \(u\):</p>
<p>\[ L_(; x) = _U p(x, u,|,) , U. \]</p>
<p>In our specific example, the integration amounts to (probabilistically) averaging over all possible membership configurations, and thus, to the weighted sum taking into account the membership weights,</p>
<p>\[ L_(; x) = <em>{i=1}^n </em>{u=1}^2 p(x_i, u,|,) , w(x_i, u,|,). \]</p>
<p>Directly maximizing this quantity is intractable.</p>
<p>Something we do have a grip on, given the data and our current parameter estimate \(_t\), is the conditional distribution of the latent variable, \(p(u,|,x, _t)\). We can use it to find the expectation of the complete data log likelihood \(p(x, u,|,)\) evaluated for some general parameter value \(\). This expectation is often denoted as</p>
<p>\[ Q(, _t) = _U p(x, u,|,) , p(u,|, x, _t) , U. \]</p>
<p>In the M step, we determine the revised parameter estimate \(_{t+1}\) by maximizing this function,</p>
<p>\[ <em>{t+1} = </em>, Q(, _t). \]</p>
<p>The E step consists of computing \(p(u,|, x, _t)\), the essential ingredient of \(Q\).</p>
<p>These two steps (E and M) are iterated until the improvements are small; this is a numerical indication that we are close to a flattening of the likelihood and have reached a maximum. The iteration trajectory, but hopefully not the point that we arrive at, will depend on the starting point. This is analogous to a hike to the top of a mountain, which can start at different places and may take different routes, but always leads to the top of the hill—as long as there is only one hilltop and not several. Thus, as a precaution, it is good practice to repeat such a procedure several times from different starting points and make sure that we always get the same answer.</p>
<p>__</p>
<p>Question 4.7</p>
<p>Several R packages provide EM implementations, including <strong><a href="https://cran.r-project.org/web/packages/mclust/">mclust</a></strong> , <strong><a href="https://cran.r-project.org/web/packages/EMcluster/">EMcluster</a></strong> and <strong><a href="https://cran.r-project.org/web/packages/EMMIXskew/">EMMIXskew</a></strong>. Choose one and run the EM function several times with different starting values. Then use the function <code>normalmixEM</code> from the <strong><a href="https://cran.r-project.org/web/packages/mixtools/">mixtools</a></strong> package to compare the outputs.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Here we show the output from <strong><a href="https://cran.r-project.org/web/packages/mixtools/">mixtools</a></strong>.</p>
<pre><code>library("mixtools")
y = c(rnorm(100, mean = -0.2, sd = 0.5),
      rnorm( 50, mean =  0.5, sd =   1))
gm = normalmixEM(y, k = 2, 
                    lambda = c(0.5, 0.5),
                    mu = c(-0.01, 0.01), 
                    sigma = c(3, 3))__


number of iterations= 134 


with(gm, c(lambda, mu, sigma, loglik))__


[1]    0.9263    0.0737   -0.1859    1.9216    0.6134    0.4746 -170.5662</code></pre>
<p>The EM algorithm is very instructive:</p>
<ol type="1">
<li><p>We saw a first example of <em>soft</em> averaging, where we don’t decide whether an observation belongs to one group or another, but allow it to participate in several groups by using membership probabilities as weights, and thus obtain more nuanced estimates (<a href="16-chap.html#ref-Slonim:2005">Slonim et al.&nbsp;2005</a>).</p></li>
<li><p>It shows us how we can tackle a difficult problem with too many unknowns by alternating between solving simpler problems.</p></li>
<li><p>We were able to consider a data generating model with hidden variables, and still estimate its parameters. We could do so even without explicitly commiting on the values of the hidden variables: we took a (weighted) average over them, in the Expecation step of Equation 4.8, embodied by the membership probabilities. This basic idea is so powerful that it is the starting point for many more advanced algorithms in machine learning (<a href="16-chap.html#ref-Bishop:PRML">Bishop 2006</a>).</p></li>
<li><p>It can be extended to the more general case of model averaging (<a href="16-chap.html#ref-hoeting1999">Hoeting et al.&nbsp;1999</a>). It can be sometimes beneficial to consider several models simultaneously if we are unsure which one is relevant for our data. We can combine them together into a weighted model. The weights are provided by the likelihoods of the models.</p></li>
</ol>
</section>
</section>
<section id="models-for-zero-inflated-data" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="models-for-zero-inflated-data"><span class="header-section-number">6.1.3</span> 4.2.3 Models for zero inflated data</h3>
<p>Ecological and molecular data often come in the form of counts. For instance, this may be the number of individuals from each of several species at each of several locations. Such data can often be seen as a mixture of two scenarios: if the species is not present, the count is necessarily zero, but <em>if</em> the species is present, the number of individuals we observe varies, with a random sampling distribution; this distribution may also include zeros. We model this as a mixture:</p>
<p>\[ f_{}(x) = , (x) + (1-) , f_{}(x), \]</p>
<p>where \(\) is Dirac’s delta function, which represents a probability distribution that has all its mass at 0. The zeros from the first mixture component are called “structural”: in our example, they occur because certain species do not live in certain habitats. The second component, \(f_{}\) may also include zeros and other small numbers, simply due to random sampling. The R packages <strong><a href="https://cran.r-project.org/web/packages/pscl/">pscl</a></strong> (<a href="16-chap.html#ref-Zeileis:2008">Zeileis, Kleiber, and Jackman 2008</a>) and <strong><a href="https://cran.r-project.org/web/packages/zicounts/">zicounts</a></strong> provide many examples and functions for working with <strong>zero inflated</strong> counts.</p>
<section id="example-chip-seq-data" class="level4" data-number="6.1.3.1">
<h4 data-number="6.1.3.1" class="anchored" data-anchor-id="example-chip-seq-data"><span class="header-section-number">6.1.3.1</span> Example: ChIP-Seq data</h4>
<p>Let’s consider the example of ChIP-Seq data. These data are sequences of pieces of DNA that are obtained from chromatin immunoprecipitation (ChIP). This technology enables the mapping of the locations along genomic DNA of transcription factors, nucleosomes, histone modifications, chromatin remodeling enzymes, chaperones, polymerases and other protein. It was the main technology used by the Encyclopedia of DNA Elements (ENCODE) Project. Here we use an example (<a href="16-chap.html#ref-Kuan2011statistical">Kuan et al.&nbsp;2011</a>) from the <strong><a href="https://bioconductor.org/packages/mosaicsExample/">mosaicsExample</a></strong> package, which shows data measured on chromosome 22 from ChIP-Seq of antibodies for the STAT1 protein and the H3K4me3 histone modification applied to the GM12878 cell line. Here we do not show the code to construct the <code>binTFBS</code> object; it is shown in the source code file for this chapter and follows the vignette of the <strong><a href="https://bioconductor.org/packages/mosaics/">mosaics</a></strong> package.</p>
<pre><code>binTFBS __


Summary: bin-level data (class: BinData)
----------------------------------------
- # of chromosomes in the data: 1
- total effective tag counts: 462479
  (sum of ChIP tag counts of all bins)
- control sample is incorporated
- mappability score is NOT incorporated
- GC content score is NOT incorporated
- uni-reads are assumed
----------------------------------------</code></pre>
<p>From this object, we can create the histogram of per-bin counts.</p>
<pre><code>bincts = print(binTFBS)
ggplot(bincts, aes(x = tagCount)) +
  geom_histogram(binwidth = 1, fill = "forestgreen")__</code></pre>
<p><a href="04-chap_files/figure- html/fig-chipseqzeros-1.png" title="Figure 4.8: The number of binding sites found in 200nt windows along chromosome 22 in a ChIP-Seq dataset."><img src="04-chap_files/figure-html/fig-chipseqzeros-1.png" class="img-fluid"></a></p>
<p>Figure 4.8: The number of binding sites found in 200nt windows along chromosome 22 in a ChIP-Seq dataset.</p>
<p>We see in Figure 4.8 that there are many zeros, although from this plot it is not immediately obvious whether the number of zeros is really extraordinary, given the frequencies of the other small numbers (\(1,2,…\)).</p>
<p>__</p>
<p>Question 4.8</p>
<ol type="1">
<li><p>Redo the histogram of the counts using a logarithm base 10 scale on the \(y\)-axis.</p></li>
<li><p>Estimate \(_0\), the proportion of bins with zero counts.</p></li>
</ol>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>ggplot(bincts, aes(x = tagCount)) + scale_y_log10() +
   geom_histogram(binwidth = 1, fill = "forestgreen")__</code></pre>
<p><a href="04-chap_files/figure-html/fig-ChipseqHistlogY-1.png &quot;Figure 4.9: As Figure fig-chipseqzeros, but using a logarithm base 10 scale on the y-axis. The fraction of zeros seems elevated compared to that of ones, twos, …&quot;"><img src="04-chap_files/figure-html/fig- ChipseqHistlogY-1.png" class="img-fluid"></a></p>
<p>Figure 4.9: As Figure 4.8, but using a logarithm base 10 scale on the \(y\)-axis. The fraction of zeros seems elevated compared to that of ones, twos, …</p>
</section>
</section>
<section id="more-than-two-components" class="level3" data-number="6.1.4">
<h3 data-number="6.1.4" class="anchored" data-anchor-id="more-than-two-components"><span class="header-section-number">6.1.4</span> 4.2.4 More than two components</h3>
<p>So far we have looked at mixtures of two components. We can extend our description to cases where there may be more. For instance, when weighing N=7,000 nucleotides obtained from mixtures of deoxyribonucleotide monophosphates (each type has a different weight, measured with the same standard deviation sd=3), we might observe the histogram (shown in Figure 4.10) generated by the following code.</p>
<pre><code>masses = c(A =  331, C =  307, G =  347, T =  322)
probs  = c(A = 0.12, C = 0.38, G = 0.36, T = 0.14)
N  = 7000
sd = 3
nuclt   = sample(length(probs), N, replace = TRUE, prob = probs)
quadwts = rnorm(length(nuclt),
                mean = masses[nuclt],
                sd   = sd)
ggplot(tibble(quadwts = quadwts), aes(x = quadwts)) +
  geom_histogram(bins = 100, fill = "purple")__</code></pre>
<p><a href="04-chap_files/figure-html/fig- nucleotideweights-1-1.png" title="Figure 4.10: Simulation of 7,000 nucleotide mass measurements."><img src="04-chap_files/figure-html/fig- nucleotideweights-1-1.png" class="img-fluid"></a></p>
<p>Figure 4.10: Simulation of 7,000 nucleotide mass measurements.</p>
<p>__</p>
<p>Question 4.9</p>
<p>Repeat this simulation experiment with \(N=1000\) nucleotide measurements. What do you notice in the histogram?</p>
<p>__</p>
<p>Question 4.10</p>
<p>What happens when \(N=7000\) but the standard deviation is 10?</p>
<p>__</p>
<p>Question 4.11</p>
<p>Plot the theoretical density curve for the distribution simulated in Figure 4.10.</p>
<p>In this case, as we have enough measurements with good enough precision, we are able to distinguish the four nucleotides and decompose the distribution shown in Figure 4.10. With fewer data and/or more noisy measurements, the four modes and the distribution component might be less clear.</p>
</section>
</section>
<section id="empirical-distributions-and-the-nonparametric-bootstrap" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="empirical-distributions-and-the-nonparametric-bootstrap"><span class="header-section-number">6.2</span> 4.3 Empirical distributions and the nonparametric bootstrap</h2>
<p>In this section, we will consider an extreme case of mixture models, where we model our sample of \(n\) data points as a mixture of \(n\) point masses. We could use almost any set of data here; to be concrete, we use Darwin’s <em>Zea Mays</em> data3 in which he compared the heights of 15 pairs of <em>Zea Mays</em> plants (15 self-hybridized versus 15 crossed). The data are available in the <strong><a href="https://cran.r-project.org/web/packages/HistData/">HistData</a></strong> package, and we plot the distribution of the 15 differences in height:</p>
<p>3 They were collected by Darwin who asked his cousin, Francis Galton to analyse them. R.A. Fisher re-analysed the same data using a paired t-test (<a href="16-chap.html#ref-bulmer2003">Bulmer 2003</a>). We will get back to this example in <a href="13-chap.html">Chapter 13</a>.</p>
<pre><code>library("HistData")
ZeaMays$diff __


 [1]  6.125 -8.375  1.000  2.000  0.750  2.875  3.500  5.125  1.750  3.625
[11]  7.000  3.000  9.375  7.500 -6.000


ggplot(ZeaMays, aes(x = diff, ymax = 1/15, ymin = 0)) +
  geom_linerange(linewidth = 1, col = "forestgreen") + ylim(0, 0.1)__</code></pre>
<p><a href="04-chap_files/figure- html/fig-ecdfZea-1.png" title="Figure 4.11: The observed sample can be seen as a mixture of point masses at each of the values (real point masses would be bars without any width whatsoever)."><img src="04-chap_files/figure-html/fig-ecdfZea-1.png" class="img-fluid"></a></p>
<p>Figure 4.11: The observed sample can be seen as a mixture of point masses at each of the values (real point masses would be bars without any width whatsoever).</p>
<p>In <a href="03-chap.html#sec-graphics-ecdf">Section 3.6.7</a> we saw that the empirical cumulative distribution function (ECDF) for a sample of size \(n\) is</p>
<p>\[ <em>n(x)= </em>{i=1}^n {}_{x x_i}, \]</p>
<p>and we saw ECDF plots in <a href="03-chap.html#fig-graphics-onedecdf">Figure 3.24</a>. We can also write the <em>density</em> of our sample as</p>
<p>\[ <em>n(x) =</em>{i=1}^n _{x_i}(x) \]</p>
<p>In general, the density of a probability distribution is the derivative (if it exists) of the distribution function. We have applied this principle here: the density of the distribution defined by Equation 4.11 is Equation 4.12. We could do this since one can consider the function \(<em>a\) the “derivative” of the step function \({}</em>{x a}\): it is completely flat almost everywhere, except at the one point \(a\) where there is the step, and where its value is “infinite” . Equation 4.12 highlights that our data sample can be considered a mixture of \(n\) <strong>point masses</strong> at the observed values \(x_1,x_2,…, x_{n}\), such as in Figure 4.11.</p>
<p>There is a bit of advanced mathematics (beyond standard calculus) required for this to make sense, which is however beyond the scope of our treatment here.</p>
<p><a href="imgs/BootstrapPrincipleNew.png &quot;Figure 4.12: The value of a statistic \tau is estimated from data (the grey matrices) generated from an underlying distribution F. Different samples from F lead to different data, and so to different values of the estimate \hat{\tau}: this is called sampling variability. The distribution of all the \hat{\tau}’s is the sampling distribution.&quot;"><img src="imgs/BootstrapPrincipleNew.png" class="img-fluid"></a></p>
<p>Figure 4.12: The value of a statistic \(\) is estimated from data (the grey matrices) generated from an underlying distribution \(F\). Different samples from \(F\) lead to different data, and so to different values of the estimate \(\): this is called <strong>sampling variability</strong>. The distribution of all the \(\)’s is the <strong>sampling distribution</strong>.</p>
<p>Statistics of our sample, such as the mean, minimum or median, can now be written as a function of the ECDF, for instance, \({x} = <em>{x_i}(x),x\). As another instance, if \(n\) is an odd number, the median is \(x</em>{()}\), the value right in the middle of the ordered list.</p>
<p>The true <strong>sampling distribution</strong> of a statistic \(\) is often hard to know as it requires many different data samples from which to compute the statistic; this is shown in Figure 4.12.</p>
<p>The <strong>bootstrap</strong> principle approximates the true sampling distribution of \(\) by creating new samples drawn from the empirical distribution built from the original sample (Figure 4.13). We <em>reuse</em> the data (by considering it a mixture distribution of \(\)s) to create new “datasets” by taking samples and looking at the sampling distribution of the statistics \(^*\) computed on them. This is called the <strong>nonparametric bootstrap</strong> resampling approach, see Bradley Efron and Tibshirani (<a href="16-chap.html#ref-Efron:1994">1994</a>) for a complete reference. It is very versatile and powerful method that can be applied to basically any statistic, no matter how complicated it is. We will see example applications of this method, in particular to clustering, in <a href="05-chap.html">Chapter 5</a>.</p>
<p><a href="imgs/BootstrapPrinciple2New.png &quot;Figure 4.13: The bootstrap simulates the sampling variability by drawing samples not from the underlying true distribution F (as in Figure fig- samplingdist), but from the empirical distribution function \hat{F}_n.&quot;"><img src="imgs/BootstrapPrinciple2New.png" class="img-fluid"></a></p>
<p>Figure 4.13: The bootstrap simulates the sampling variability by drawing samples not from the underlying true distribution \(F\) (as in Figure 4.12), but from the empirical distribution function \(_n\).</p>
<p>Using these ideas, let’s try to estimate the sampling distribution of the median of the Zea Mays differences we saw in Figure 4.11. We use similar simulations to those in the previous sections: Draw \(B=1000\) samples of size 15 from the 15 values (each being a component in the 15-component mixture). Then compute the 1000 medians of each of these samples of 15 values and look at their distribution: this is the bootstrap sampling distribution of the median.</p>
<pre><code>B = 1000
meds = replicate(B, {
  i = sample(15, 15, replace = TRUE)
  median(ZeaMays$diff[i])
})
ggplot(tibble(medians = meds), aes(x = medians)) +
  geom_histogram(bins = 30, fill = "purple")__</code></pre>
<p><a href="04-chap_files/figure- html/fig-bootmedian-1.png" title="Figure 4.14: The bootstrap sampling distribution of the median of the Zea Mays differences."><img src="04-chap_files/figure-html/fig-bootmedian-1.png" class="img-fluid"></a></p>
<p>Figure 4.14: The bootstrap sampling distribution of the median of the Zea Mays differences.</p>
<p>__</p>
<p>Question 4.12</p>
<p>Estimate a 99% confidence interval for the median based on these simulations. What can you conclude from looking at the overlap between this interval and 0?</p>
<p>__</p>
<p>Question 4.13</p>
<p>Use the <strong><a href="https://cran.r-project.org/web/packages/bootstrap/">bootstrap</a></strong> package to redo the same analysis using the function <code>bootstrap</code> for both <code>median</code> and <code>mean</code>. What differences do you notice between the sampling distribution of the mean and the median?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>library("bootstrap")
bootstrap(ZeaMays$diff, B, mean)
bootstrap(ZeaMays$diff, B, median)__</code></pre>
<section id="why-nonparametric" class="level4" data-number="6.2.0.1">
<h4 data-number="6.2.0.1" class="anchored" data-anchor-id="why-nonparametric"><span class="header-section-number">6.2.0.1</span> Why nonparametric?</h4>
<p>In theoretical statistics, nonparametric methods are those that have infinitely many degrees of freedom or numbers of unknown parameters.</p>
<p><a href="imgs/devil.png"><img src="imgs/devil.png" class="img-fluid"></a></p>
<p>In practice, we do not wait for infinity; when the number of parameters becomes as large or larger than the amount of data available, we say the method is nonparametric. The bootstrap uses a mixture with \(n\) components, so with a sample of size \(n\), it qualifies as a nonparametric method.</p>
<p>Despite their name, nonparametric methods are not methods that do not use parameters: all statistical methods estimate unknown quantities.</p>
<p>__</p>
<p>Question 4.14</p>
<p>If the sample is composed of \(n=3\) different values, how many different bootstrap resamples are possible? Answer the same question with \(n=15\).</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>The set of all bootstrap resamples is equivalent to the set of all vectors of \(n\) integers whose sum is \(n\). Denote by \( = (k_1,k_2,…,k_n)\) the number of times the observations \(x_1,x_2,…, x_n\) occur in a bootstrap sample. We can think of each \(k_i\) as a box (as in the multinomial distribution), and there are \(n\) boxes in which to drop \(n\) balls. We can count the number of configurations by counting the number of ways of separating \(n\) balls into the boxes, i.e.&nbsp;by writing down \(n\) times an <code>o</code> (for the balls) and \(n-1\) times a separator <code>|</code> between them. So we have \(2n-1\) positions to fill, for which we must choose either <code>o</code> (a ball) or <code>|</code> (a separator). For \(n=3\), a possible placement would be <code>oo||o</code>, which corresponds to \( = (2,0,1)\). In general, this number is \({2n-1} \), and thus the answers for \(n=3\) and \(15\) are:</p>
<pre><code>c(N3 = choose(5, 3), N15 = choose(29, 15))__


      N3      N15 
      10 77558760 </code></pre>
<p>__</p>
<p>Question 4.15</p>
<p>What are the two types of error that can occur when using the bootstrap as it is implemented in the <strong><a href="https://cran.r-project.org/web/packages/bootstrap/">bootstrap</a></strong> package? Which parameter can you modify to improve one of them?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Monte Carlo simulations of subsets of the data by resampling randomly approximate the exhaustive bootstrap (<a href="16-chap.html#ref-Diaconis1994">Diaconis and Holmes 1994</a>). Increasing the size of <code>nboot</code> argument in the <code>bootstrap</code> function reduces the Monte Carlo error, however, the exhaustive bootstrap is still not exact: we are still using an approximate distribution function, that of the data instead of the true distribution. If the sample size is small or the original sample biased, the approximation can still be quite poor, no matter how large we choose <code>nboot</code>.</p>
</section>
</section>
<section id="infinite-mixtures" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="infinite-mixtures"><span class="header-section-number">6.3</span> 4.4 Infinite mixtures</h2>
<p>Sometimes mixtures can be useful even if we don’t aim to assign a label to each observation or, to put it differently, if we allow as many `labels’ as there are observations. If the number of mixture components is as big as (or bigger than) the number of observations, we say we have an <strong>infinite mixture</strong>. Let’s look at some examples.</p>
<section id="infinite-mixture-of-normals" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="infinite-mixture-of-normals"><span class="header-section-number">6.3.1</span> 4.4.1 Infinite mixture of normals</h3>
<p><a href="imgs/LaplacePortrait_web.png" title="Figure 4.15: Laplace knew already that the probability density f_X(y)=\frac{1}{2\phi}\exp\left(-\frac{|y-\theta|}{\phi}\right),\qquad\phi>0 has the median as its location parameter \theta and the median absolute deviation (MAD) as its scale parameter \phi."><img src="imgs/LaplacePortrait_web.png" class="img-fluid"></a></p>
<p>Figure 4.15: Laplace knew already that the probability density \[f_X(y)=(-),&gt;0\] has the median as its location parameter \(\) and the median absolute deviation (MAD) as its scale parameter \(\).</p>
<p>Consider the following two-level data generating scheme:</p>
<p><strong>Level 1</strong> Create a sample of <code>W</code>s from an exponential distribution.</p>
<pre><code>w = rexp(10000, rate = 1)__</code></pre>
<p><strong>Level 2</strong> The \(w\)s serve as the variances of normal variables with mean \(\) generated using <code>rnorm</code>.</p>
<pre><code>mu  = 0.3
lps = rnorm(length(w), mean = mu, sd = sqrt(w))
ggplot(data.frame(lps), aes(x = lps)) +
  geom_histogram(fill = "purple", binwidth = 0.1)__</code></pre>
<p><a href="04-chap_files/figure-html/fig- Laplacedistribution-1.png" title="Figure 4.16: Data sampled from a Laplace distribution."><img src="04-chap_files/figure-html/fig- Laplacedistribution-1.png" class="img-fluid"></a></p>
<p>Figure 4.16: Data sampled from a Laplace distribution.</p>
<p>This turns out to be a rather useful distribution. It has well-understood properties and is named after Laplace, who proved that the median is a good estimator of its location parameter \(\) and that the median absolute deviation can be used to estimate its scale parameter \(\). From the formula in the caption of Figure 4.15 we see that the \(L_1\) distance (absolute value of the difference) holds a similar position in the Laplace density as the \(L_2\) (square of the difference) does for the normal density.</p>
<p>Conversely, in Bayesian regression4, having a Laplace distribution as a prior on the coefficients amounts to an \(L_1\) penalty, called the <em>lasso</em> (<a href="16-chap.html#ref-Tibshirani1996">Tibshirani 1996</a>), while a normal distribution as a prior leads to an \(L_2\) penalty, called ridge regression.</p>
<p>4 Don’t worry if you are not familiar with this, in that case just skip this sentence.</p>
<p>__</p>
<p>Question 4.16</p>
<p>Write a random variable whose distribution is the symmetric Laplace as a function of normal and exponential random variables.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>We can write the hierarchical model with variances generated as exponential variables, \(W\), as:</p>
<p>\[ X = Z, W Exp(1), Z N(0,1). \]</p>
<section id="asymmetric-laplace" class="level4" data-number="6.3.1.1">
<h4 data-number="6.3.1.1" class="anchored" data-anchor-id="asymmetric-laplace"><span class="header-section-number">6.3.1.1</span> Asymmetric Laplace</h4>
<p>In the Laplace distribution, the variances of the normal components depend on \(W\), while the means are unaffected. A useful extension adds another parameter \(\) that controls the locations or centers of the components. We generate data <code>alps</code> from a hierarchical model with \(W\) an exponential variable; the output shown in Figure 4.17 is a histogram of normal \(N(+w,w)\) random numbers, where the \(w\)’s themselves were randomly generated from an exponential distribution with mean \(1\) as shown in the code:</p>
<pre><code>mu = 0.3; sigma = 0.4; theta = -1
w  = rexp(10000, 1)
alps = rnorm(length(w), theta + mu * w, sigma * sqrt(w))
ggplot(tibble(alps), aes(x = alps)) +
  geom_histogram(fill = "purple", binwidth = 0.1)__</code></pre>
<p><a href="04-chap_files/figure-html/fig- ALaplacedistribution-1.png" title="Figure 4.17: Histogram of data generated from an asymmetric Laplace distribution – a scale mixture of many normals whose means and variances are dependent. We write X \sim AL(\theta, \mu, \sigma)."><img src="04-chap_files/figure-html/fig- ALaplacedistribution-1.png" class="img-fluid"></a></p>
<p>Figure 4.17: Histogram of data generated from an asymmetric Laplace distribution – a scale mixture of many normals whose means and variances are dependent. We write \(X AL(, , )\).</p>
<p>Such hierarchical mixture distributions, where every instance of the data has its own mean and variance, are useful models in many biological settings. Examples are shown in Figure 4.18.</p>
<p><a href="imgs/LaplaceMixturePromoterLengths.png &quot;Figure 4.18 (a): The lengths of the promoters shorter than 2000bp from Saccharomyces cerevisiae as studied by @Kristiansson2009.&quot;"><img src="imgs/LaplaceMixturePromoterLengths.png" class="img-fluid"></a></p>
<ol type="a">
<li>The lengths of the promoters shorter than 2000bp from Saccharomyces cerevisiae as studied by Kristiansson et al.&nbsp;(<a href="16-chap.html#ref- Kristiansson2009">2009</a>).</li>
</ol>
<p><a href="imgs/tcellhist.png" title="Figure 4.18 (b): The log- ratios of microarray gene expression measurements for 20,000 genes [@Purdom2005]."><img src="imgs/tcellhist.png" class="img-fluid"></a></p>
<ol start="2" type="a">
<li>The log-ratios of microarray gene expression measurements for 20,000 genes (<a href="16-chap.html#ref-Purdom2005">Purdom and Holmes 2005</a>).</li>
</ol>
<p>Figure 4.18: Histogram of real data. Both distributions can be modeled by asymmetric Laplace distributions.</p>
<p>__</p>
<p>Question 4.17</p>
<p>Looking at the log-ratio of gene expression values from a microarray, one gets a distribution as shown on the right of Figure 4.18. How would one explain that the data have a histogram of this form?</p>
<p>The Laplace distribution is an example of where the consideration of the generative process indicates how the variance and mean are linked. The expectation value and variance of an asymmetric Laplace distribution \(AL(, , )\) are</p>
<p>\[ E(X)=+(X)=<sup>2+</sup>2. \]</p>
<p>Note the variance is dependent on the mean, unless \(= 0\) (the case of the symmetric Laplace Distribution). This is the feature of the distribution that makes it useful. Having a mean-variance dependence is very common for physical measurements, be they microarray fluorescence intensities, peak heights from a mass spectrometer, or reads counts from high-throughput sequencing, as we’ll see in the next section.</p>
</section>
</section>
<section id="infinite-mixtures-of-poisson-variables." class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="infinite-mixtures-of-poisson-variables."><span class="header-section-number">6.3.2</span> 4.4.2 Infinite mixtures of Poisson variables.</h3>
<p><a href="imgs/three-worlds_web.jpg" title="Figure 4.19: How to count the fish in a lake? MC Escher."><img src="imgs/three-worlds_web.jpg" class="img-fluid"></a></p>
<p>Figure 4.19: How to count the fish in a lake? MC Escher.</p>
<p>A similar two-level hierarchical model is often also needed to model real- world count data. At the lower level, simple Poisson and binomial distributions serve as the building blocks, but their parameters may depend on some underlying (latent) process. In ecology, for instance, we might be interested in variations of fish species in all the lakes in a region. We sample the fish species in each lake to estimate their true abundances, and that could be modeled by a Poisson. But the true abundances will vary from lake to lake. And if we want to see whether, for instance, changes in climate or altitude play a role, we need to disentangle such systematic effects from random lake-to-lake variation. The different Poisson rate parameters \(\) can be modeled as coming from a distribution of rates. Such a hierarchical model also enables us to add supplementary steps in the hierarchy, for instance we could be interested in many different types of fish, model altitude and other environmental factors separately, etc.</p>
<p>Further examples of sampling schemes that are well modeled by mixtures of Poisson variables include applications of high-throughput sequencing, such as RNA-Seq, which we will cover in detail in <a href="08-chap.html">Chapter 8</a>, or 16S rRNA-Seq data used in microbial ecology.</p>
</section>
<section id="gamma-distribution-two-parameters-shape-and-scale" class="level3" data-number="6.3.3">
<h3 data-number="6.3.3" class="anchored" data-anchor-id="gamma-distribution-two-parameters-shape-and-scale"><span class="header-section-number">6.3.3</span> 4.4.3 Gamma distribution: two parameters (shape and scale)</h3>
<p>Now we are getting to know a new distribution that we haven’t seen before. The gamma distribution is an extension of the (one-parameter) exponential distribution, but it has two parameters, which makes it more flexible. It is often useful as a building block for the upper level of a hierarchical model. The gamma distribution is positive-valued and continuous. While the density of the exponential has its maximum at zero and then simply decreases towards 0 as the value goes to infinity, the density of the gamma distribution has its maximum at some finite value. Let’s explore it by simulation examples. The histograms in Figure 4.20 were generated by the following lines of code:</p>
<pre><code>ggplot(tibble(x = rgamma(10000, shape = 2, rate = 1/3)),
   aes(x = x)) + geom_histogram(bins = 100, fill= "purple")
ggplot(tibble(x = rgamma(10000, shape = 10, rate = 3/2)),
   aes(x = x)) + geom_histogram(bins = 100, fill= "purple")__</code></pre>
<p><a href="04-chap_files/figure- html/fig-gammahist1-1.png" title="Figure 4.20 (a): gamma(2,\frac{1}{3})"><img src="04-chap_files/figure-html/fig-gammahist1-1.png" class="img-fluid"></a></p>
<ol type="a">
<li>gamma\((2,)\)</li>
</ol>
<p><a href="04-chap_files/figure- html/fig-gammahist1-2.png" title="Figure 4.20 (b): gamma(10,\frac{3}{2})"><img src="04-chap_files/figure-html/fig-gammahist1-2.png" class="img-fluid"></a></p>
<ol start="2" type="a">
<li>gamma\((10,)\)</li>
</ol>
<p>Figure 4.20: Histograms of random samples of gamma distributions. The gamma is a flexible two parameter distribution: <a href="http://en.wikipedia.org/wiki/Gamma_distribution">see Wikipedia</a>.</p>
<section id="gammapoisson-mixture-a-hierarchical-model" class="level4" data-number="6.3.3.1">
<h4 data-number="6.3.3.1" class="anchored" data-anchor-id="gammapoisson-mixture-a-hierarchical-model"><span class="header-section-number">6.3.3.1</span> Gamma–Poisson mixture: a hierarchical model</h4>
<ol type="1">
<li><p>Generate a set of parameters: \(_1,_2,…\) from a gamma distribution.</p></li>
<li><p>Use these to generate a set of Poisson(\(_i\)) random variables, one for each \(_1\).</p></li>
</ol>
<pre><code>lambda = rgamma(10000, shape = 10, rate = 3/2)
gp = rpois(length(lambda), lambda = lambda)
ggplot(tibble(x = gp), aes(x = x)) +
  geom_histogram(bins = 100, fill= "purple")__</code></pre>
<p><a href="04-chap_files/figure-html/fig- generatepoissongamma-1.png" title="Figure 4.21: Histogram of gp, generated via a gamma-Poisson hierachical model."><img src="04-chap_files/figure-html/fig- generatepoissongamma-1.png" class="img-fluid"></a></p>
<p>Figure 4.21: Histogram of <code>gp</code>, generated via a gamma-Poisson hierachical model.</p>
<p>The resulting values are said to come from a gamma–Poisson mixture. Figure 4.21 shows the histogram of <code>gp</code>.</p>
<p>__</p>
<p>Question 4.18</p>
<ol type="1">
<li><p>Are the values generated from such a gamma–Poisson mixture continuous or discrete ?</p></li>
<li><p>What is another name for this distribution? Hint: Try the different distributions provided by the <code>goodfit</code> function from the <strong><a href="https://cran.r-project.org/web/packages/vcd/">vcd</a></strong> package.</p></li>
</ol>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>library("vcd")
ofit = goodfit(gp, "nbinomial")
plot(ofit, xlab = "")
ofit$par __


$size
[1] 9.911829

$prob
[1] 0.5963857</code></pre>
<p><a href="04-chap_files/figure- html/fig-goofy-1.png" title="Figure 4.22: Goodness of fit plot. The rootogram shows the theoretical probabilities of the gamma-Poisson distribution (a.k.a. negative binomial) as red dots and the square roots of the observed frequencies as the height of the rectangular bars. The bars all end close to the horizontal axis, which indicates a good fit to the negative binomial distribution."><img src="04-chap_files/figure-html/fig-goofy-1.png" class="img-fluid"></a></p>
<p>Figure 4.22: Goodness of fit plot. The <strong>rootogram</strong> shows the theoretical probabilities of the gamma-Poisson distribution (a.k.a. negative binomial) as red dots and the square roots of the observed frequencies as the height of the rectangular bars. The bars all end close to the horizontal axis, which indicates a good fit to the negative binomial distribution.</p>
<p>In R, and in some other places, the gamma-Poisson distribution travels under the alias name of <strong>negative binomial distribution</strong>. The two names are synonyms; the second one alludes to the fact that Equation 4.15 bears some formal similarities to the probabilities of a binomial distribution. The first name, gamma–Poisson distribution, is more indicative of its generating mechanism, and that’s what we will use in the rest of the book. It is a discrete distribution, that means that it takes values only on the natural numbers (in contrast to the gamma distribution, which covers the whole positive real axis). Its probability distribution is</p>
<p>\[ (K=k)=(]</p>
<p>which depends on the two parameters \(a^+\) and \(p\). Equivalently, the two parameters can be expressed by the mean \(=pa/(1-p)\) and a parameter called the <strong>dispersion</strong> \(/a\). The variance of the distribution depends on these parameters, and is \(+^2\).</p>
<p><a href="04-chap_files/figure-html/fig-mixtures-dgammapois-1.png &quot;Figure 4.23: Visualization of the hierarchical model that generates the gamma-Poisson distribution. The top panel shows the density of a gamma distribution with mean 50 (vertical black line) and variance 30. Assume that in one particular experimental replicate, the value 60 is realized. This is our latent variable. The observable outcome is distributed according to the Poisson distribution with that rate parameter, shown in the middle panel. In one particular experiment the outcome may be, say, 55, indicated by the dashed green line. Overall, if we repeat these two subsequent random process many times, the outcomes will be distributed as shown in the bottom panel – the gamma-Poisson distribution.&quot;"><img src="04-chap_files/figure-html/fig-mixtures- dgammapois-1.png" class="img-fluid"></a></p>
<p>Figure 4.23: Visualization of the hierarchical model that generates the gamma- Poisson distribution. The top panel shows the density of a gamma distribution with mean 50 (vertical black line) and variance 30. Assume that in one particular experimental replicate, the value 60 is realized. This is our latent variable. The observable outcome is distributed according to the Poisson distribution with that rate parameter, shown in the middle panel. In one particular experiment the outcome may be, say, 55, indicated by the dashed green line. Overall, if we repeat these two subsequent random process many times, the outcomes will be distributed as shown in the bottom panel – the gamma-Poisson distribution.</p>
<p>__</p>
<p>Question 4.19</p>
<p>If you are more interested in analytical derivations than illustrative simulations, try writing out the mathematical derivation of the gamma-Poisson probability distribution.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Recall that the final distribution is the result of a two step process:</p>
<ol type="1">
<li>Generate a \((a,b)\) distributed number, call it \(x\), from the density</li>
</ol>
<p>\[ f_(x, a, b)=,x<sup>{a-1},e</sup>{-b x}, \]</p>
<p>where \(\) is the so-called \(\)-function, \((a)=_0<sup>x</sup>{a-1},e^{-x},x\) (not to be confused with the gamma distribution, even though there is this incidental relation).</p>
<ol start="2" type="1">
<li>Generate a number \(k\) from the Poisson distribution with rate \(x\). The probability distribution is</li>
</ol>
<p>\[ f_{}(k, =x)= \]</p>
<p>If \(x\) only took on a finite set of values, we could solve the problem simply by summing over all the possible cases, each weighted by their probability according to \(f_\). But \(x\) is continuous, so we have to write the sum out as an integral instead of a discrete sum. We call the distribution of \(K\) the marginal. Its probability mass function is</p>
<p>\[ ]</p>
<p>Collect terms and move terms independent of \(x\) outside the integral</p>
<p>\[ P(K=k)= _{x=0}^{} x<sup>{k+a-1}e</sup>{-(b+1)x} dx \]</p>
<p>Because we know the gamma density sums to one: \(_0^ x<sup>{k+a-1}e</sup>{-(b+1)x} dx = \)</p>
<p>\[ ]</p>
<p>where in the last line we used that \((v+1)=v!\). This is the same as Equation (4.15), the gamma-Poisson with size parameter \(a\) and probability \(p=\).</p>
</section>
</section>
<section id="variance-stabilizing-transformations" class="level3" data-number="6.3.4">
<h3 data-number="6.3.4" class="anchored" data-anchor-id="variance-stabilizing-transformations"><span class="header-section-number">6.3.4</span> 4.4.4 Variance stabilizing transformations</h3>
<p>A key issue we need to control when we analyse experimental data is how much variability there is between repeated measurements of the same underlying true value, i.e., between replicates. This will determine whether and how well we can see any true differences, i.e., between different conditions. Data that arise through the type of hierarchical models we have studied in this chapter often turn out to have very heterogeneous variances, and this can be a challenge. We will see how in such cases <strong>variance-stabilizing transformations</strong> (<a href="16-chap.html#ref-Anscombe1948">Anscombe 1948</a>) can help. Let’s start with a series of Poisson variables with rates from 10 to 100:</p>
<p>Note how we construct the dataframe (or, more precisely, the <em>tibble</em>) <code>simdat</code>: the output of the <code>lapply</code> loop is a list of <em>tibble</em> s, one for each value of <code>lam</code>. With the pipe operator <code>|&gt;</code> we send it to the function <code>bind_rows</code> (from the <strong><a href="https://cran.r-project.org/web/packages/dplyr/">dplyr</a></strong> package). The result is a dataframe of all the list elements neatly stacked on top of each other.</p>
<pre><code>simdat = lapply(seq(10, 100, by = 10), function(lam)
    tibble(n = rpois(200, lambda = lam),
           `sqrt(n)` = sqrt(n),
       lambda = lam)) |&gt;
  bind_rows() |&gt;
  tidyr::pivot_longer(cols = !lambda)
ggplot(simdat, aes(x = factor(lambda), y = value)) + xlab(expression(lambda)) +
  geom_violin() + facet_grid(rows = vars(name), scales = "free")__</code></pre>
<p><a href="04-chap_files/figure-html/fig-seriesofpoisson-1.png &quot;Figure 4.24: Poisson distributed measurement data, for eight different choices of the mean lambda. In the upper panel, the y-axis is proportional to the data; in the lower panel, it is on a square-root scale. Note how the distribution widths change in the first case, but less so in the second.&quot;"><img src="04-chap_files/figure-html/fig- seriesofpoisson-1.png" class="img-fluid"></a></p>
<p>Figure 4.24: Poisson distributed measurement data, for eight different choices of the mean <code>lambda</code>. In the upper panel, the \(y\)-axis is proportional to the data; in the lower panel, it is on a square-root scale. Note how the distribution widths change in the first case, but less so in the second.</p>
<p>The data that we see in the upper panel of Figure 4.24 are an example of what is called <strong>heteroscedasticity</strong> : the standard deviations (or, equivalently, the variance) of our data is different in different regions of our data space. In particular, it increases along the \(x\)-axis, with the mean. For the Poisson distribution, we indeed know that the standard deviation is the square root of the mean; for other types of data, there may be other dependencies. This can be a problem if we want to apply subsequent analysis techniques (for instance, regression, or a statistical test) that are based on assuming that the variances are the same. In Figure 4.24, the numbers of replicates for each value of lambda are quite large. In practice, this is not always the case. Moreover, the data are usually not explicitly stratified by a known mean as in our simulation, so the heteroskedasticity may be harder to see, even though it is there. However, as we see in the lower panel of Figure 4.24, if we simply apply the square root transformation, then the transformed variables will have approximately the same variance. This works even if we do not know the underlying mean for each observation, the square root transformation does not need this information. We can verify this more quantitatively, as in the following code, which shows the standard deviations of the sampled values <code>n</code> and <code>sqrt(n)</code> for the difference choices of <code>lambda</code>.</p>
<p>The standard deviation of the square root transformed values is consistently around 0.5, so we would use the transformation <code>2*sqrt(n)</code> to achieve unit variance.</p>
<pre><code>summarise(group_by(simdat, name, lambda), sd(value)) |&gt; tidyr::pivot_wider(values_from = `sd(value)`)__


# A tibble: 10 × 3
   lambda     n `sqrt(n)`
    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;
 1     10  2.95     0.478
 2     20  4.19     0.470
 3     30  5.62     0.521
 4     40  5.99     0.473
 5     50  7.69     0.546
 6     60  7.59     0.492
 7     70  8.69     0.520
 8     80  8.99     0.505
 9     90  9.44     0.498
10    100  9.84     0.495</code></pre>
<p>Another example, now using the gamma-Poisson distribution, is shown in Figure 4.25. We generate gamma-Poisson variables <code>u</code>5 and plot the 95% confidence intervals around the mean.</p>
<p>5 To catch a greater range of values for the mean value <code>mu</code>, without creating too dense a sequence, we use a geometric series: \(_{i+1} = 2_i\).</p>
<pre><code>muvalues = 2^seq(0, 10, by = 1)
simgp = lapply(muvalues, function(mu) {
  u = rnbinom(n = 1e4, mu = mu, size = 4)
  tibble(mean = mean(u), sd = sd(u),
         lower = quantile(u, 0.025),
         upper = quantile(u, 0.975),
         mu = mu)
  } ) |&gt; bind_rows()
head(as.data.frame(simgp), 2)__


    mean       sd lower upper mu
1 0.9965 1.106440     0     4  1
2 2.0233 1.748503     0     6  2


ggplot(simgp, aes(x = mu, y = mean, ymin = lower, ymax = upper)) +
  geom_point() + geom_errorbar()__</code></pre>
<p><a href="04-chap_files/figure- html/fig-seriesofnb-1.png" title="Figure 4.25: gamma-Poisson distributed measurement data, for a range of \mu from 1 to 1024."><img src="04-chap_files/figure-html/fig-seriesofnb-1.png" class="img-fluid"></a></p>
<p>Figure 4.25: gamma-Poisson distributed measurement data, for a range of \(\) from 1 to 1024.</p>
<p>__</p>
<p>Question 4.20</p>
<p>How can we find a transformation for these data that stabilizes the variance, similar to the square root function for the Poisson distributed data?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>If we divide the values that correspond to <code>mu[1]</code> (and which are centered around <code>simgp$mean[1]</code>) by their standard deviation <code>simgp$sd[1]</code>, the values that correspond to <code>mu[2]</code> (and which are centered around <code>simgp$mean[2]</code>) by their standard deviation <code>simgp$sd[2]</code>, and so on, then the resulting values will have, by construction, a standard deviation (and thus variance) of 1. And rather than defining 11 separate transformations, we can achieve our goal by defining one single piecewise linear <em>and</em> continuous function that has the appropriate slopes at the appropriate values.</p>
<pre><code>simgp = mutate(simgp,
  slopes = 1 / sd,
  trsf   = cumsum(slopes * mean))
ggplot(simgp, aes(x = mean, y = trsf)) +
  geom_point() + geom_line() + xlab("")__</code></pre>
<p><a href="04-chap_files/figure- html/fig-pcwlin-1-1.png" title="Figure 4.26: Piecewise linear function that stabilizes the variance of the data in Figure fig-seriesofnb."><img src="04-chap_files/figure-html/fig-pcwlin-1-1.png" class="img-fluid"></a></p>
<p>Figure 4.26: Piecewise linear function that stabilizes the variance of the data in Figure 4.25.</p>
<p>We see in Figure 4.26 that this function has some resemblance to a square root function in particular at its lower end. At the upper end, it seems to look more like a logarithm. The more mathematically inclined will see that an elegant extension of these numerical calculations can be done through a little calculus known as the <strong>delta method</strong> , as follows.</p>
<p>Call our transformation function \(g\), and assume it’s differentiable (that’s not a very strong assumption: pretty much any function that we might consider reasonable here is differentiable). Also call our random variables \(X_i\), with means \(_i\) and variances \(v_i\), and we assume that \(v_i\) and \(_i\) are related by a functional relationship \(v_i = v(_i)\). Then, for values of \(X_i\) in the neighborhood of its mean \(_i\),</p>
<p>\[ g(X_i) = g(_i) + g’(_i) (X_i-_i) + … \]</p>
<p>where the dots stand for higher order terms that we can neglect. The variances of the transformed values are then</p>
<p>\[ <span class="math display">\[\begin{align} \text{Var}(g(X_i)) &amp;\simeq g'(\mu_i)^2\\\ \text{Var}(X_i) &amp;=
g'(\mu_i)^2 \, v(\mu_i), \end{align}\]</span> \]</p>
<p>where we have used the rules \((X-c)=(X)\) and \((cX)=c^2,(X)\) that hold whenever \(c\) is a constant number. Requiring that this be constant leads to the differential equation</p>
<p>\[ g’(x) = . \]</p>
<p>For a given mean-variance relationship \(v()\), we can solve this for the function \(g\). Let’s check this for some simple cases:</p>
<ul>
<li><p>if \(v()=\) (Poisson), we recover \(g(x)=\), the square root transformation.</p></li>
<li><p>If \(v()=,^2\), solving the differential equation 4.19 gives \(g(x)=(x)\). This explains why the logarithm transformation is so popular in many data analysis applications: it acts as a variance stabilizing transformation whenever the data have a constant coefficient of variation, that is, when the standard deviation is proportional to the mean.</p></li>
</ul>
<p>__</p>
<p>Question 4.21</p>
<p>What is the variance-stabilizing transformation associated with \(v() = + ,^2\)?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>To solve the differential equation 4.19 with this function \(v()\), we need to compute the integral</p>
<p>\[ . \]</p>
<p>A closed form expression can be looked up in a reference table such as (<a href="16-chap.html#ref-BronsteinSemendjajew">Bronštein and Semendjajew 1979</a>). These authors provide the general solution</p>
<p>\[ = (2+2ax+b) + , \]</p>
<p>into which we can plug in our special case \(a=\), \(b=1\), \(c=0\), to obtain the variance-stabilizing transformation</p>
<p>\[ <span class="math display">\[\begin{align} g_\alpha(x) &amp;= \frac{1}{2\sqrt{\alpha}}
\ln\left(2\sqrt{\alpha x (\alpha x+1)} + 2\alpha x + 1\right) \\\ &amp;=
\frac{1}{2\sqrt{\alpha}} {\displaystyle \operatorname {arcosh}} (2\alpha
x+1).\\\ \end{align}\]</span> \]</p>
<p>For the second line in Equation 4.22, we used the identity \({ }(z) = (z+)\). In the limit of \(\), we can use the linear approximation \((1+)=+O(^2)\) to see that \(g_0(x)=\). Note that if \(g_\) is a variance-stabilizing transformation, then so is \(ug_+v\) for any pair of numbers \(u\) and \(v\), and we have used this freedom to insert an extra factor \(\) for reasons that become apparent in the following. You can verify that the function \(g_\) from Equation 4.22 fulfills condition 4.19 by computing its derivative, which is an elementary calculation. We can plot it:</p>
<pre><code>f = function(x, a) 
  ifelse (a==0, 
    sqrt(x), 
    log(2*sqrt(a) * sqrt(x*(a*x+1)) + 2*a*x+1) / (2*sqrt(a)))
x  = seq(0, 24, by = 0.1)
df = lapply(c(0, 0.05*2^(0:5)), function(a) 
  tibble(x = x, a = a, y = f(x, a))) %&gt;% bind_rows()
ggplot(df, aes(x = x, y = y, col = factor(a))) + 
  geom_line() + labs(col = expression(alpha))__</code></pre>
<p><a href="04-chap_files/figure-html/fig- plotvstgammapoisson-1-1.png" title="Figure 4.27: Graph of the function Equation eq- mixtures-vstgammapoisson for different choices of \alpha."><img src="04-chap_files/figure-html/fig- plotvstgammapoisson-1-1.png" class="img-fluid"></a></p>
<p>Figure 4.27: Graph of the function Equation 4.22 for different choices of \(\).</p>
<p>and empirically verify the equivalence of two terms in Equation 4.22:</p>
<pre><code>f2 = function(x, a) ifelse (a==0, sqrt(x), acosh(2*a*x + 1) / (2*sqrt(a)))  
with(df, max(abs(f2(x,a) - y)))__


[1] 8.881784e-16</code></pre>
<p>As we see in Figure 4.27, for small values of \(x\), \(g_(x) \) (independently of \(\)), whereas for large values (\(x\)) and \(&gt;0\), it behaves like a logarithm:</p>
<p>\[ <span class="math display">\[\begin{align} &amp;\frac{1}{2\sqrt{\alpha}}\ln\left(2\sqrt{\alpha\left(\alpha
x^2+x\right)}+2\alpha x+1\right)\\\ \approx&amp;\frac{1}{2\sqrt{\alpha}}
\ln\left(2\sqrt{\alpha^2x^2}+2\alpha x\right)\\\
=&amp;\frac{1}{2\sqrt{\alpha}}\ln\left(4\alpha x\right)\\\
=&amp;\frac{1}{2\sqrt{\alpha}}\ln x+\text{const.} \end{align}\]</span>=&amp;(4x)\<br>
=&amp;x+ \end{align} \]</p>
<p>We can verify this empirically by, say,</p>
<pre><code>  a = c(0.2, 0.5, 1)
  f(1e6, a) __


[1] 15.196731 10.259171  7.600903


  1/(2*sqrt(a)) * (log(1e6) + log(4*a))__


[1] 15.196728 10.259170  7.600902</code></pre>
</section>
</section>
<section id="summary-of-this-chapter" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="summary-of-this-chapter"><span class="header-section-number">6.4</span> 4.5 Summary of this chapter</h2>
<p>We have given motivating examples and ways of using mixtures to model biological data. We saw how the EM algorithm is an interesting example of fitting a difficult-to-estimate probabilistic model to data by iterating between partial, simpler problems.</p>
<section id="finite-mixture-models" class="level4" data-number="6.4.0.1">
<h4 data-number="6.4.0.1" class="anchored" data-anchor-id="finite-mixture-models"><span class="header-section-number">6.4.0.1</span> Finite mixture models</h4>
<p>We have seen how to model mixtures of two or more normal distributions with different means and variances. We have seen how to decompose a given sample of data from such a mixture, even without knowing the latent variable, using the EM algorithm. The EM approach requires that we know the parametric form of the distributions and the number of components. In <a href="05-chap.html">Chapter 5</a>, we will see how we can find groupings in data even without relying on such information – this is then called clustering. We can keep in mind that there is a strong conceptual relationship between clustering and mixture modeling.</p>
</section>
<section id="common-infinite-mixture-models" class="level4" data-number="6.4.0.2">
<h4 data-number="6.4.0.2" class="anchored" data-anchor-id="common-infinite-mixture-models"><span class="header-section-number">6.4.0.2</span> Common infinite mixture models</h4>
<p>Infinite mixture models are good for constructing new distributions (such as the gamma-Poisson or the Laplace) out of more basic ones (such as binomial, normal, Poisson). Common examples are</p>
<ul>
<li><p>mixtures of normals (often with a hierarchical model on the means and the variances);</p></li>
<li><p>beta-binomial mixtures – where the probability \(p\) in the binomial is generated according to a \((a, b)\) distribution;</p></li>
<li><p>gamma-Poisson for read counts (see <a href="08-chap.html">Chapter 8</a>);</p></li>
<li><p>gamma-exponential for PCR.</p></li>
</ul>
</section>
<section id="applications" class="level4" data-number="6.4.0.3">
<h4 data-number="6.4.0.3" class="anchored" data-anchor-id="applications"><span class="header-section-number">6.4.0.3</span> Applications</h4>
<p>Mixture models are useful whenever there are several layers of experimental variability. For instance, at the lowest layer, our measurement precision may be limited by basic physical detection limits, and these may be modeled by a Poisson distribution in the case of a counting-based assay, or a normal distribution in the case of the continuous measurement. On top of there may be one (or more) layers of instrument-to-instrument variation, variation in the reagents, operator variaton etc.</p>
<p>Mixture models reflect that there is often heterogeneous amounts of variability (variances) in the data. In such cases, suitable data transformations, i.e., variance stabilizing transformations, are necessary before subsequent visualization or analysis. We’ll study in depth an example for RNA-Seq in <a href="08-chap.html">Chapter 8</a>, and this also proves useful in the normalization of next generation reads in microbial ecology (<a href="16-chap.html#ref-mcmurdie2014">McMurdie and Holmes 2014</a>).</p>
<p>Another important application of mixture modeling is the two-component model in multiple testing – we will come back to this in <a href="06-chap.html">Chapter 6</a>.</p>
</section>
<section id="the-ecdf-and-bootstrapping" class="level4" data-number="6.4.0.4">
<h4 data-number="6.4.0.4" class="anchored" data-anchor-id="the-ecdf-and-bootstrapping"><span class="header-section-number">6.4.0.4</span> The ECDF and bootstrapping</h4>
<p>We saw that by using the observed sample as a mixture we could generate many simulated samples that inform us about the sampling distribution of an estimate. This method is called the bootstrap and we will return to it several times, as it provides a way of evaluating estimates even when a closed form expression is not available (we say it is non-parametric).</p>
</section>
</section>
<section id="further-reading" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">6.5</span> 4.6 Further reading</h2>
<p>A useful book-long treatment of finite mixture models is by McLachlan and Peel (<a href="16-chap.html#ref-mclachlan2004">2004</a>); for the EM algorithm, see also the book by McLachlan and Krishnan (<a href="16-chap.html#ref- mclachlan2007algorithm">2007</a>). A recent book that presents all EM type algorithms within the Majorize-Minimization (MM) framework is by Lange (<a href="16-chap.html#ref-lange2016mm">2016</a>).</p>
<p>There are in fact mathematical reasons why many natural phenomena can be seen as mixtures: this occurs when the observed events are exchangeable (the order in which they occur doesn’t matter). The theory underlying this is quite mathematical, a good way to start is to look at the Wikipedia entry and the paper by Diaconis and Freedman (<a href="16-chap.html#ref-diaconis1980finite">1980</a>).</p>
<p>In particular, we use mixtures for high-throughput data. You will see examples in Chapters <a href="08-chap.html">8</a> and <a href="11-chap.html">11</a>.</p>
<p>The bootstrap can be used in many situations and is a very useful tool to know about, a friendly treatment is given in (<a href="16-chap.html#ref-efront">B. Efron and Tibshirani 1993</a>).</p>
<p>A historically interesting paper is the original article on variance stabilization by Anscombe (<a href="16-chap.html#ref-Anscombe1948">1948</a>), who proposed ways of making variance stabilizing transformations for Poisson and gamma-Poisson random variables. Variance stabilization is explained using the delta method in many standard texts in theoretical statistics, e.g., those by Rice (<a href="16-chap.html#ref-Rice:2007">2006, chap.&nbsp;6</a>) and Kéry and Royle (<a href="16-chap.html#ref-Kery2015">2015, 35</a>).</p>
<p>Kéry and Royle (<a href="16-chap.html#ref-Kery2015">2015</a>) provide a nice exploration of using R to build hierarchical models for abundance estimation in niche and spatial ecology.</p>
</section>
<section id="exercises" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="exercises"><span class="header-section-number">6.6</span> 4.7 Exercises</h2>
<p>__</p>
<p>Exercise 4.1</p>
<p><strong>The EM algorithm step by step.</strong> As an example dataset, we use the values in the file <code>Myst.rds</code>. As always, it is a good idea to first visualize the data. The histogram is shown in Figure 4.28. We are going to model these data as a mixture of two normal distributions with unknown means and standard deviations, and unknown mixture fraction. We’ll call the two components A and B.</p>
<pre><code>mx = readRDS("../data/Myst.rds")$yvar
str(mx)__


 num [1:1800] 0.3038 0.0596 -0.0204 0.1849 0.2842 ...


ggplot(tibble(mx), aes(x = mx)) + geom_histogram(binwidth = 0.025)__</code></pre>
<p>We start by randomly assigning the membership weights for each of the values in <code>mx</code> for each of the components</p>
<pre><code>wA = runif(length(mx))
wB = 1 - wA __</code></pre>
<p>We also need to set up some housekeeping variables: <code>iter</code> counts over the iterations of the EM algorithm; <code>loglik</code> stores the current log-likelihood; <code>delta</code> stores the change in the log-likelihood from the previous iteration to the current one. We also define the parameters <code>tolerance</code>, <code>miniter</code> and <code>maxiter</code> of the algorithm.</p>
<pre><code>iter      = 0
loglik    = -Inf
delta     = +Inf
tolerance = 1e-12
miniter   = 50
maxiter   = 1000 __</code></pre>
<p>Study the code below and answer the following questions:</p>
<ol type="1">
<li><p>Which lines correspond to the E-step, which to the M-step?</p></li>
<li><p>What is the role of <code>tolerance</code>, <code>miniter</code> and <code>maxiter</code>?</p></li>
<li><p>Compare the result of what we are doing here to the output of the <code>normalmixEM</code> function from the <strong><a href="https://cran.r-project.org/web/packages/mixtools/">mixtools</a></strong> package.</p></li>
</ol>
<pre><code>while((delta &gt; tolerance) &amp;&amp; (iter &lt;= maxiter) || (iter &lt; miniter)) {
  lambda = mean(wA)
  muA = weighted.mean(mx, wA)
  muB = weighted.mean(mx, wB)
  sdA = sqrt(weighted.mean((mx - muA)^2, wA))
  sdB = sqrt(weighted.mean((mx - muB)^2, wB))

  pA   =    lambda    * dnorm(mx, mean = muA, sd = sdA)
  pB   = (1 - lambda) * dnorm(mx, mean = muB, sd = sdB)
  ptot = pA + pB
  wA   = pA / ptot
  wB   = pB / ptot

  loglikOld = loglik
  loglik = sum(log(pA + pB))
  delta = abs(loglikOld - loglik)
  iter = iter + 1
}
iter __


[1] 447


c(lambda, muA, muB, sdA, sdB)__


[1]  0.4756 -0.1694  0.1473  0.0983  0.1498</code></pre>
<p><a href="04-chap_files/figure-html/fig-EMillustrate-1-1.png &quot;Figure 4.28: Histogram of mx, our example data for the EM algorithm.&quot;"><img src="04-chap_files/figure-html/fig- EMillustrate-1-1.png" class="img-fluid"></a></p>
<p>Figure 4.28: Histogram of <code>mx</code>, our example data for the EM algorithm.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>The first five lines in the <code>while</code> loop implement the <em>Maximization step</em>. Given the current values of <code>wA</code> and <code>wB</code>, we estimate the parameters of the mixture model using the maximum-likelihood estimators: the mixture fraction <code>lambda</code> by the mean of <code>wA</code>, and the parameters of the two normal distribution components (<code>muA</code>, <code>sdA</code>) and (<code>muB</code>, <code>sdB</code>) by the sample means and the sample standard deviations. To take into account the membership weights, we use the weighted mean (function <code>weighted.mean</code>) and standard deviation.</p>
<p>Next comes the <em>Expectation step</em>. For each of the elements in the data vector <code>mx</code>, we compute the probability densities <code>pA</code> and <code>pB</code> for the generative distribution models A and B, using the normal density function <code>dnorm</code>, weighted by the mixture fractions <code>lambda</code> and <code>(1-lambda)</code>, respectively. From this, we compute the updated membership weights <code>wA</code> and <code>wB</code>, according to Equation 4.5.</p>
<p>Given the membership weights and the parameters, the logarithmic likelihood <code>loglik</code> is easily computed, and the <code>while</code> loop iterates these steps.</p>
<p>The termination criterion for the loop is based on <code>delta</code>, the change in the likelihood. The loop can end if this becomes smaller than <code>tolerance</code>. This is a simple way of checking whether the algorithm has converged. The additional conditions on <code>iter</code> make sure that at least <code>miniter</code> iterations are run, and that the loop always stops after <code>maxiter</code> iterations. The latter is to make sure that the loop terminates in finite time no matter what. (“Professional” implementations of such iterative algorithms typically work a bit harder to decide what is the best time to stop.)</p>
<p>Finally, let’s compare our estimates to those from the function <code>normalmixEM</code> from the <strong><a href="https://cran.r-project.org/web/packages/mixtools/">mixtools</a></strong> package.</p>
<pre><code>gm = mixtools::normalmixEM(mx, k = 2)__


number of iterations= 215 


with(gm, c(lambda[1], mu, sigma))__


[1]  0.4757 -0.1694  0.1473  0.0983  0.1498</code></pre>
<p>__</p>
<p>Exercise 4.2</p>
<p>Why do we often consider the logarithm of the likelihood rather than the likelihood? E.g., in the EM code above, why did we work with the probabilities on the logarithmic scale?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Likelihoods often (whenever the data points are sampled independently) take the form of a product. This is, for instance, the case in Equation 4.4. Calculating the derivative, for likelihood optimisation, would then require application of the product rule. On the logarithmic scale, the product turns into a sum, and the derivative of a sum is simply the sum of the derivatives of the individual summands.</p>
<p>An additional reason comes from the way computers implement arithmetic. They commonly use a floating point representation of numbers with a finite number of bits. E.g., the IEEE 754-2008 standard uses 64 bits for a <em>double- precision</em> number: 1 bit for the sign, 52 for the mantissa (also called significand), 11 for the exponent. Multiplication between such numbers implies addition of the exponents, but the range of the exponent is only \(0\) to \(2^{11}-1=2047\). Even likelihoods that involve only a few hundred data points can lead to arithmetic overflow or other problems with precision. On the logarithmic scale, where the product is a sum, the workload tends to be better distributed between mantissa and exponent, and log-likelihoods even with millions of data points to be handled with reasonable precision.</p>
<p>See also Gregory Gundersen’s post on the <a href="https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/">Log-Sum-Exp Trick for normalizing vectors of log probabilities</a>.</p>
<p>__</p>
<p>Exercise 4.3</p>
<p>Compare the theoretical values of the gamma-Poisson distribution with parameters given by the estimates in <code>ofit$par</code> in Section 4.4.3 to the data used for the estimation using a QQ-plot.</p>
<p>__</p>
<p>Exercise 4.4</p>
<p><strong>Mixture modeling examples for regression</strong>. The <strong><a href="https://cran.r-project.org/web/packages/flexmix/">flexmix</a></strong> package (<a href="16-chap.html#ref-Grun2012">Grün, Scharl, and Leisch 2012</a>) enables us to cluster and fit regressions to the data at the same time. The standard M-step <code>FLXMRglm</code> of <strong><a href="https://cran.r-project.org/web/packages/flexmix/">flexmix</a></strong> is an interface to R’s generalized linear modeling facilities (the <code>glm</code> function). Load the package and an example dataset.</p>
<pre><code>library("flexmix")
data("NPreg")__</code></pre>
<ol type="1">
<li><p>First, plot the data and try to guess how the points were generated.</p></li>
<li><p>Fit a two component mixture model using the commands</p></li>
</ol>
<pre><code>m1 = flexmix(yn ~ x + I(x^2), data = NPreg, k = 2)__</code></pre>
<ol start="3" type="1">
<li><p>Look at the estimated parameters of the mixture components and make a truth table that cross-classifies true classes versus cluster memberships. What does the summary of the object <code>m1</code> show us?</p></li>
<li><p>Plot the data again, this time coloring each point according to its estimated class.</p></li>
</ol>
<p>__</p>
<p>Solution</p>
<p>__</p>
<pre><code>ggplot(NPreg, aes(x = x, y = yn)) + geom_point()__</code></pre>
<p><a href="04-chap_files/figure- html/fig-npreg-1.png" title="Figure 4.29: The points seem to come from two different generative processes, one is linear; the other quadratic."><img src="04-chap_files/figure-html/fig-npreg-1.png" class="img-fluid"></a></p>
<p>Figure 4.29: The points seem to come from two different generative processes, one is linear; the other quadratic.</p>
<p>The components are:</p>
<pre><code>modeltools::parameters(m1, component = 1)__


                      Comp.1
coef.(Intercept) -0.20998685
coef.x            4.81807854
coef.I(x^2)       0.03613061
sigma             3.47665584


modeltools::parameters(m1, component = 2)__


                     Comp.2
coef.(Intercept) 14.7167886
coef.x            9.8468507
coef.I(x^2)      -0.9683734
sigma             3.4795657</code></pre>
<p>The parameter estimates of both components are close to the true values. A cross-tabulation of true classes and cluster memberships can be obtained by</p>
<pre><code>table(NPreg$class, modeltools::clusters(m1))__


   
     1  2
  1 95  5
  2  5 95</code></pre>
<p>For our example data, the ratios of both components are approximately 0.7, indicating the overlap of the classes at the cross-section of line and parabola.</p>
<pre><code>summary(m1)__</code></pre>
<p>The summary shows the estimated prior probabilities \(<em>k\), the number of observations assigned to the two clusters, the number of observations where \(p</em>{nk}&gt;\) (with a default of \(^{-4}\)), and the ratio of the latter two numbers. For well- separated components, a large proportion of observations with non-vanishing posteriors \(p_{nk}\) should be assigned to their cluster, giving a ratio close to 1.</p>
<pre><code>NPreg = mutate(NPreg, gr = factor(class))
ggplot(NPreg, aes(x = x, y = yn, group = gr)) +
   geom_point(aes(colour = gr, shape = gr)) +
   scale_colour_hue(l = 40, c = 180)__</code></pre>
<p><a href="04-chap_files/figure- html/fig-npregC-1.png" title="Figure 4.30: Regression example using flexmix with the points colored according to their estimated class. You can see that at the intersection we have an `identifiability’ problem: we cannot distinguish points that belong to the straight line from ones that belong to the parabole."><img src="04-chap_files/figure-html/fig-npregC-1.png" class="img-fluid"></a></p>
<p>Figure 4.30: Regression example using <code>flexmix</code> with the points colored according to their estimated class. You can see that at the intersection we have an `identifiability’ problem: we cannot distinguish points that belong to the straight line from ones that belong to the parabole.</p>
<p>__</p>
<p>Exercise 4.5</p>
<p><strong>Other hierarchical noise models:</strong><br>
Find two papers that explore the use of other infinite mixtures for modeling molecular biology technological variation.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>The paper by Chen, Xie, and Story (<a href="16-chap.html#ref-Chen2011">2011</a>) explores an exponential-Poisson model for modeling background noise in bead arrays. Wills et al.&nbsp;(<a href="16-chap.html#ref-Wills2013">2013</a>) compares several Poisson mixture models.</p>
<p>Anscombe, Francis J. 1948. “The Transformation of Poisson, Binomial and Negative-Binomial Data.” <em>Biometrika</em> , 246–54.</p>
<p>Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. Springer.</p>
<p>Bronštein, Il’ja N., and Konstantin A Semendjajew. 1979. <em>Taschenbuch Der Mathematik</em>. B.G. Teubner Verlagsgesellschaft, Leipzig; Verlag Nauka, Moscow.</p>
<p>Bulmer, Michael George. 2003. <em>Francis Galton: Pioneer of Heredity and Biometry</em>. JHU Press.</p>
<p>Chen, Min, Yang Xie, and Michael Story. 2011. “An Exponential-Gamma Convolution Model for Background Correction of Illumina BeadArray Data.” <em>Communications in Statistics-Theory and Methods</em> 40 (17): 3055–69.</p>
<p>Diaconis, Persi, and David Freedman. 1980. “Finite Exchangeable Sequences.” <em>The Annals of Probability</em> , 745–64.</p>
<p>Diaconis, Persi, and Susan Holmes. 1994. “Gray Codes for Randomization Procedures.” <em>Statistics and Computing</em> 4 (4): 287–302.</p>
<p>Efron, Bradley, and Robert J Tibshirani. 1994. <em>An Introduction to the Bootstrap</em>. CRC press.</p>
<p>Efron, B., and R. Tibshirani. 1993. <em>An Introduction to the Bootstrap</em>. Chapman &amp; Hall/CRC.</p>
<p>Grün, Bettina, Theresa Scharl, and Friedrich Leisch. 2012. “Modelling Time Course Gene Expression Data with Finite Mixtures of Linear Additive Models.” <em>Bioinformatics</em> 28 (2): 222–28. <a href="https://doi.org/10.1093/bioinformatics/btr653" class="uri">https://doi.org/10.1093/bioinformatics/btr653</a>.</p>
<p>Hoeting, Jennifer A, David Madigan, Adrian E Raftery, and Chris T Volinsky. 1999. “Bayesian Model Averaging: A Tutorial.” <em>Statistical Science</em> , 382–401.</p>
<p>Kéry, Marc, and J Andrew Royle. 2015. <em>Applied Hierarchical Modeling in Ecology: Analysis of Distribution, Abundance and Species Richness in r and BUGS: Volume 1: Prelude and Static Models</em>. Academic Press.</p>
<p>Kristiansson, Erik, Michael Thorsen, Markus J Tamás, and Olle Nerman. 2009. “Evolutionary Forces Act on Promoter Length: Identification of Enriched Cis- Regulatory Elements.” <em>Molecular Biology and Evolution</em> 26 (6): 1299–1307.</p>
<p>Kuan, Pei Fen, Dongjun Chung, Guangjin Pan, James A Thomson, Ron Stewart, and Sündüz Keleş. 2011. “A Statistical Framework for the Analysis of ChIP-Seq Data.” <em>Journal of the American Statistical Association</em> 106 (495): 891–903.</p>
<p>Lange, Kenneth. 2016. <em>MM Optimization Algorithms</em>. SIAM.</p>
<p>McLachlan, Geoffrey, and Thriyambakam Krishnan. 2007. <em>The EM Algorithm and Extensions</em>. Vol. 382. John Wiley &amp; Sons.</p>
<p>McLachlan, Geoffrey, and David Peel. 2004. <em>Finite Mixture Models</em>. John Wiley &amp; Sons.</p>
<p>McMurdie, Paul J, and Susan Holmes. 2014. “Waste Not, Want Not: Why Rarefying Microbiome Data Is Inadmissible.” <em>PLoS Computational Biology</em> 10 (4): e1003531.</p>
<p>Purdom, Elizabeth, and Susan P Holmes. 2005. “Error Distribution for Gene Expression Data.” <em>Statistical Applications in Genetics and Molecular Biology</em> 4 (1).</p>
<p>Rice, John. 2006. <em>Mathematical Statistics and Data Analysis</em>. Cengage Learning.</p>
<p>Shalizi, Cosma. 2017. <em>Advanced Data Analysis from an Elementary Point of View</em>. Cambridge University Press. <a href="https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf" class="uri">https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf</a>.</p>
<p>Slonim, Noam, Gurinder Singh Atwal, Gašper Tkačik, and William Bialek. 2005. “Information-Based Clustering.” <em>PNAS</em> 102 (51): 18297–302.</p>
<p>Tibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> , 267–88.</p>
<p>Wills, Quin F, Kenneth J Livak, Alex J Tipping, Tariq Enver, Andrew J Goldson, Darren W Sexton, and Chris Holmes. 2013. “Single-Cell Gene Expression Analysis Reveals Genetic Associations Masked in Whole-Tissue Experiments.” <em>Nature Biotechnology</em> 31 (8): 748–52.</p>
<p>Zeileis, Achim, Christian Kleiber, and Simon Jackman. 2008. “Regression Models for Count Data in R.” <em>Journal of Statistical Software</em> 27 (8). <a href="http://www.jstatsoft.org/v27/i08/" class="uri">http://www.jstatsoft.org/v27/i08/</a>.</p>
<p>Page built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./03-chap.html" class="pagination-link" aria-label="3.1 Goals for this chapter">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">3.1 Goals for this chapter</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./05-chap.html" class="pagination-link" aria-label="5.1 Goals for this chapter">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">5.1 Goals for this chapter</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>