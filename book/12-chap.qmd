# 지도 학습 (Supervised Learning)

![](imgs/BuildWall.png)

![](imgs/EWall.png)

지도 학습(Supervised Learning) 환경에서는 우리가 얼마나 잘하고 있는지를 판단할 수 있는 잣대, 바로 반응 변수(response) 그 자체가 존재합니다.

생물학 및 생물의학 응용 분야에서 자주 묻는 질문은 관심 있는 속성(예: 질병 유형, 세포 유형, 환자의 예후)을 다른 하나 이상의 속성, 즉 **예측 변수(predictors)**를 통해 "예측"할 수 있는지 여부입니다. 종종 우리는 예측해야 할 속성은 알 수 없지만(미래에 있거나 측정하기 어려운 경우), 예측 변수는 알고 있는 상황에 직면합니다. 중요한 점은 관심 있는 속성 또한 알고 있는 **학습 데이터(training data)** 집합으로부터 예측 규칙을 _학습_ 한다는 것입니다. 규칙을 확보하고 나면, 이를 새로운 데이터에 적용하여 알 수 없는 결과를 실제로 예측하거나, 기저에 있는 생물학적 원리를 더 잘 이해하기 위해 규칙 자체를 분석할 수 있습니다.

우리가 무엇을 찾고 있는지, 또는 결과가 "맞는지" 어떻게 결정해야 할지 모르는 비지도 학습(Unsupervised Learning)이나 [5장](05-chap.html), [7장](07-chap.html), [9장](09-chap.html)에서 본 내용과 비교할 때, 지도 학습은 훨씬 더 확고한 기반 위에 있습니다. 목표가 명확하게 명시되어 있고, 우리가 얼마나 잘하고 있는지 측정할 수 있는 간단한 기준이 있기 때문입니다.

**지도 학습** 1의 핵심 이슈는 **과적합(overfitting)**과 **일반화 가능성(generalizability)**입니다. 훈련 데이터에 대해 100% 정확도를 가지지만 새로운 데이터에 대해서는 형편없는 성능을 보이는 규칙을 만들어 훈련 데이터를 단순히 "암기"한 것일까요? 아니면 우리의 규칙이 연구 중인 시스템의 적절한 패턴을 실제로 포착하여 아직 보지 못한 새로운 데이터에도 적용될 수 있을까요? (그림 12.1)

1 때때로 **통계적 학습(statistical learning)**이라는 용어가 거의 같은 의미로 사용됩니다.

[![](12-chap_files/figure-html/fig-overfitting-1-1.png)](12-chap_files/figure-html/fig-overfitting-1-1.png "그림 12.1: 과적합의 예: 두 회귀선이 \(x, y\)-평면의 데이터(검은 점)에 적합되었습니다. 우리는 이러한 선을 \(x\) 값이 주어졌을 때 \(y\) 값을 예측하는 규칙으로 생각할 수 있습니다. 두 선 모두 부드럽지만, 적합(fit)은 대역폭(bandwidth)이라고 불리는 것에서 차이가 나며, 이는 직관적으로 경직성(stiffness)으로 해석될 수 있습니다. 파란색 선은 데이터의 사소한 움직임까지 지나치게 따라가려는 것처럼 보이는 반면, 주황색 선은 전반적인 추세는 포착하지만 세부적인 것은 덜합니다. 파란색 선을 설명하는 데 필요한 유효 매개변수(effective parameters)의 수는 주황색 선보다 훨씬 높습니다. 또한, 추가 데이터를 얻게 된다면 파란색 선은 주황색 선보다 새로운 데이터를 모델링하는 데 더 나쁜 성능을 보일 가능성이 높습니다. 이 장의 뒷부분에서 이러한 개념들 – 훈련 오차(training error)와 테스트 세트 오차(test set error) – 을 공식화할 것입니다. 여기서는 선 적합(line fitting)으로 예시를 들었지만, 이 개념은 예측 모델에 더 일반적으로 적용됩니다.")

그림 12.1: **과적합(overfitting)**의 예: 두 회귀선이 \((x, y)\)-평면의 데이터(검은 점)에 적합되었습니다. 우리는 이러한 선을 \(x\) 값이 주어졌을 때 \(y\) 값을 예측하는 규칙으로 생각할 수 있습니다. 두 선 모두 부드럽지만, 적합(fit)은 **대역폭(bandwidth)**이라고 불리는 것에서 차이가 나며, 이는 직관적으로 경직성(stiffness)으로 해석될 수 있습니다. 파란색 선은 데이터의 사소한 움직임까지 지나치게 따라가려는 것처럼 보이는 반면, 주황색 선은 전반적인 추세는 포착하지만 세부적인 것은 덜합니다. 파란색 선을 설명하는 데 필요한 유효 매개변수(effective parameters)의 수는 주황색 선보다 훨씬 높습니다. 또한, 추가 데이터를 얻게 된다면 파란색 선은 주황색 선보다 새로운 데이터를 모델링하는 데 더 **나쁜** 성능을 보일 가능성이 높습니다. 이 장의 뒷부분에서 이러한 개념들 – 훈련 오차(training error)와 테스트 세트 오차(test set error) – 을 공식화할 것입니다. 여기서는 선 적합(line fitting)으로 예시를 들었지만, 이 개념은 예측 모델에 더 일반적으로 적용됩니다.

## 12.1 이 장의 목표

이 장에서는 다음을 수행합니다:

  * 지도 학습 방법의 사용 동기가 되는 예시적인 응용 사례를 살펴봅니다.

  * 판별 분석(discriminant analysis)이 무엇을 하는지 배웁니다.

  * 성능 척도(measures of performance)를 정의합니다.

  * 차원의 저주(curse of dimensionality)를 접하고 과적합(overfitting)이 무엇인지 알아봅니다.

  * 정규화(regularization) – 특히 벌점화(penalization) – 에 대해 알아보고 일반화 가능성(generalizability)과 모델 복잡성(model complexity)의 개념을 이해합니다.

  * 교차 검증(cross-validation)을 사용하여 알고리즘의 매개변수를 조정하는 방법을 살펴봅니다.

  * 방법론 해킹(method hacking)에 대해 논의합니다.

## 12.2 데이터란 무엇인가?

지도 학습과 비지도 학습 모두에 대한 기본적인 데이터 구조는 (적어도 개념적으로는) 데이터프레임입니다. 여기서 각 행은 객체(object)에 해당하고 열은 객체의 다른 특징(feature, 주로 숫자 값)입니다2. 비지도 학습에서는 특징 값을 기반으로 객체 간의 (비)유사성 관계를 찾는 것(예: 군집화 또는 정렬)을 목표로 하는 반면, 지도 학습에서는 다른 특징들로부터 특징 중 하나의 값을 예측하는 수학적 함수(또는 계산 알고리즘)를 찾는 것을 목표로 합니다. 많은 구현체들은 결측치(missing values)가 없어야 작동하지만, 일부 방법은 약간의 결측 데이터가 있어도 작동하도록 만들 수 있습니다.

2 이것은 단순화된 설명입니다. 기계 학습(Machine Learning)은 거대한 분야이며, 이 단순한 개념적 그림에 대한 많은 일반화가 이루어졌습니다. 이미 관련 특징을 구축하는 것 자체가 하나의 예술입니다. [11장](11-chap.html)에서 세포 이미지의 예를 보았고, 더 일반적으로 이미지, 소리, 영화, 자유 텍스트 등에서 특징을 추출할 수 있는 많은 가능성이 있습니다 … 또한, 특징이 전혀 필요 없는 **커널 방법(kernel methods)**이라는 기계 학습 방법의 변형도 있습니다. 대신 커널 방법은 객체 간의 거리 또는 유사성 척도를 사용합니다. 예를 들어, 두 자연어텍스트 객체를 나타낼 관련 수치 특징을 찾는 것보다 두 객체 간의 유사성 척도를 정의하는 것이 더 쉬울 수 있습니다. 커널 방법은 이 책의 범위를 벗어납니다.

예측을 목표로 다른 모든 특징들 중에서 선택한 특징을 **목표(objective)** 또는 **반응(response)**이라고 합니다. 때로는 선택이 자연스럽지만, 때로는 역할을 뒤집는 것이 유익할 수 있습니다. 특히 생물학적 이해를 위해 예측 함수를 분석하거나 상관관계와 인과관계를 구분하는 데 관심이 있다면 더욱 그렇습니다.

지도 학습의 프레임워크는 연속형 및 범주형 반응 변수를 모두 포괄합니다. 연속형인 경우 **회귀(regression)**라고 하고, 범주형인 경우 **분류(classification)**라고 합니다. 이 구분은 사소한 것이 아닙니다. 손실 함수 (12.5절) 선택과 그에 따른 알고리즘 선택에 꽤 광범위한 영향을 미치기 때문입니다 ([Friedman 1997](16-chap.html#ref-friedmanbiasvariance01)).

모든 지도 학습 작업에서 고려해야 할 첫 번째 질문은 객체의 수가 예측 변수의 수와 어떻게 비교되는지입니다. 객체가 많을수록 좋으며, 지도 학습에서의 많은 힘든 작업은 유한한(그리고 일반적으로 너무 작은) 훈련 세트가 가진 한계를 극복하는 것과 관련이 있습니다.

[![](imgs/fourquad.png)](imgs/fourquad.png "그림 12.2: 지도 학습에서 우리는 변수에 두 가지 다른 역할을 부여합니다. 설명 변수를 X, 반응 변수(들)를 Y라고 라벨링했습니다. 또한 두 가지 다른 관측값 세트가 있습니다: 훈련 세트 X_\ell과 Y_\ell, 그리고 테스트 세트 X_v와 Y_v.(아래첨자는 두 세트의 다른 이름인 “learning”과 “validation”을 나타냅니다.)")

그림 12.2: 지도 학습에서 우리는 변수에 두 가지 다른 역할을 부여합니다. 설명 변수를 \(X\), 반응 변수(들)를 \(Y\)라고 라벨링했습니다. 또한 두 가지 다른 관측값 세트가 있습니다: 훈련 세트 \(X_\ell\)과 \(Y_\ell\), 그리고 테스트 세트 \(X_v\)와 \(Y_v\). (아래첨자는 두 세트의 다른 이름인 “learning”과 “validation”을 나타냅니다.)

__

일

이 책에서 범주형 반응 변수를 가진 지도 학습 사례를 접한 예시를 드시오.

### 12.2.1 동기 부여 예시

#### 당뇨병 유형 예측

`diabetes` 데이터셋([Reaven and Miller 1979](16-chap.html#ref-diabetes))은 세 가지 다른 당뇨병 환자 그룹과 그들에게서 측정된 5가지 임상 변수를 제공합니다.

    
    
    data("diabetes", package = "rrcov")
    head(diabetes)__
    
    
        rw fpg glucose insulin sspg  group
    1 0.81  80     356     124   55 normal
    2 0.95  97     289     117   76 normal
    3 0.94 105     319     143  105 normal
    4 1.04  90     356     199  108 normal
    5 1.00  90     323     240  143 normal
    6 0.76  86     381     157  165 normal

일변량 분포(더 정확하게는 그것들의 밀도 추정치)는 그림 12.3에 나와 있습니다.

    
    
    library("reshape2")
    ggplot(melt(diabetes, id.vars = "group"), aes(x = value, col = group)) +
     geom_density() + facet_wrap( ~variable, ncol = 1, scales = "free") +
     theme(legend.position = "bottom")__

[![](12-chap_files/figure-html/fig-ldagroups-1-1.png)](12-chap_files/figure-html/fig-ldagroups-1-1.png "그림 12.3: 1차원 분포에서 이미 개별 변수 중 일부가 환자가 어떤 그룹에 속할 가능성이 높은지 잠재적으로 예측할 수 있음을 알 수 있습니다. 우리의 목표는 변수들을 결합하여 이러한 1차원 예측 모델보다 개선하는 것입니다.")

그림 12.3: 1차원 분포에서 이미 개별 변수 중 일부가 환자가 어떤 그룹에 속할 가능성이 높은지 잠재적으로 예측할 수 있음을 알 수 있습니다. 우리의 목표는 변수들을 결합하여 이러한 1차원 예측 모델보다 개선하는 것입니다.

변수들은 데이터셋의 매뉴얼 페이지와 논문([Reaven and Miller 1979](16-chap.html#ref-diabetes))에 설명되어 있습니다:

  * rw: 상대 체중 (relative weight)

  * fpg: 공복 혈장 포도당 (fasting plasma glucose)

  * glucose: 3시간 경구 포도당 내성 검사(OGTT)에 대한 혈장 포도당 곡선 아래 면적

  * insulin: OGTT에 대한 혈장 인슐린 곡선 아래 면적

  * sspg: 정상 상태 혈장 포도당 반응 (steady state plasma glucose response)

  * group: 정상(normal), 화학적 당뇨병(chemical diabetes), 명백한 당뇨병(overt diabetes)

#### 세포 표현형 예측

Neumann 등([2010](16-chap.html#ref-Neumann:2010))은 라이브 셀 이미징(live-cell imaging)을 사용하여 인간 암세포를 관찰했습니다. 세포의 히스톤이 녹색 형광 단백질(GFP)로 태그되도록 유전적으로 조작되었습니다. 게놈 전체 RNAi 라이브러리를 세포에 적용했고, 각 siRNA 교란에 대해 며칠 동안 수백 개의 세포 영상을 기록하여 각 유전자의 고갈이 세포 주기, 핵 형태 및 세포 증식에 어떤 영향을 미치는지 확인했습니다. 그들의 논문은 각 세포 핵의 시각적 외관을 정량화하고 정상적인 유사 분열 상태 또는 비정상적인 핵을 예측할 수 있게 해주는 자동화된 이미지 분류 알고리즘의 사용을 보고합니다. 이 알고리즘은 인간 전문가가 주석을 단 약 3000개의 세포 데이터로 훈련되었습니다. 그런 다음 거의 20억 개의 핵 이미지에 적용되었습니다(그림 12.4). 자동화된 이미지 분류를 사용하여 확장성(20억 개의 이미지를 수동으로 주석 처리하는 데는 오랜 시간이 걸릴 것입니다)과 객관성을 제공했습니다.

[![](imgs/Neumann2010Fig1b_web.jpg)](imgs/Neumann2010Fig1b_web.jpg "그림 12.4: 데이터는 영상에서 나온 2\times10^9핵의 이미지였습니다. 이미지는 핵을 식별하기 위해 분할되었고, 각 핵에 대해 크기, 모양, 밝기 및 픽셀 강도의 결합 분포에 대한 다소 추상적인 정량적 요약에 해당하는 수치 특징이 계산되었습니다. 특징으로부터 세포는 16가지 다른 핵 형태 클래스로 분류되었으며, 막대 그래프의 행으로 표시됩니다. 각 클래스에 대한 대표 이미지는 가운데 열에 흑백으로 표시됩니다. 클래스 빈도는 막대의 길이로 표시되며 매우 불균형합니다.")

그림 12.4: 데이터는 영상에서 나온 \(2\times10^9\)개 핵의 이미지였습니다. 이미지는 핵을 식별하기 위해 분할되었고, 각 핵에 대해 크기, 모양, 밝기 및 픽셀 강도의 결합 분포에 대한 다소 추상적인 정량적 요약에 해당하는 수치 특징이 계산되었습니다. 특징으로부터 세포는 16가지 다른 핵 형태 클래스로 분류되었으며, 막대 그래프의 행으로 표시됩니다. 각 클래스에 대한 대표 이미지는 가운데 열에 흑백으로 표시됩니다. 클래스 빈도는 막대의 길이로 표시되며 매우 불균형합니다.

#### 배아 세포 상태 예측

우리는 [3장](03-chap.html), [5장](05-chap.html), [7장](07-chap.html)에서 이미 본 마우스 배아 데이터([Ohnishi et al. 2014](16-chap.html#ref-Ohnishi2014))를 다시 살펴볼 것입니다. 12.3.2절과 12.6.3절에서 유전자 발현 측정값으로부터 세포 상태와 유전자형을 예측해 보겠습니다.

## 12.3 선형 판별 (Linear discrimination)

가장 간단한 판별 문제 중 하나부터 시작해 보겠습니다3. 두 개의 연속형 특징으로 설명되는(따라서 객체는 2D 평면의 점으로 생각할 수 있는) 객체가 있고, 이들은 세 그룹으로 나뉩니다. 우리의 목표는 2D 공간의 선인 클래스 경계를 정의하는 것입니다.

3 틀림없이 가장 간단한 문제는 단일 연속형 특징, 두 개의 클래스, 그리고 두 그룹을 구별하기 위한 단일 임계값을 찾는 작업입니다 – [그림 6.2](06-chap.html#fig-testing-FDRvspstatic1)와 같이.

### 12.3.1 당뇨병 데이터

`diabetes` 데이터의 `sspg`와 `glucose` 변수로부터 `group`을 예측할 수 있는지 알아봅시다. 먼저 데이터를 시각화하는 것이 항상 좋은 아이디어입니다(그림 12.5).

    
    
    ggdb = ggplot(mapping = aes(x = sspg, y = glucose)) +
      geom_point(aes(colour = group), data = diabetes)
    ggdb __

[![](12-chap_files/figure-html/fig-scatterdiabetes-1-1.png)](12-chap_files/figure-html/fig-scatterdiabetes-1-1.png "그림 12.5: 당뇨병 데이터의 두 변수에 대한 산점도. 각 점은 샘플이며, 색상은 group 변수에 인코딩된 당뇨병 유형을 나타냅니다.")

그림 12.5: `diabetes` 데이터의 두 변수에 대한 산점도. 각 점은 샘플이며, 색상은 `group` 변수에 인코딩된 당뇨병 유형을 나타냅니다.

**선형 판별 분석(Linear Discriminant Analysis, LDA)**이라는 방법으로 시작하겠습니다. 이 방법은 분류의 초석이며, 더 복잡한(그리고 때로는 더 강력한) 알고리즘 중 많은 것들이 실제로는 LDA의 일반화일 뿐입니다.

    
    
    library("MASS")
    diabetes_lda = lda(group ~ sspg + glucose, data = diabetes)
    diabetes_lda __
    
    
    Call:
    lda(group ~ sspg + glucose, data = diabetes)
    
    Prior probabilities of groups:
       normal  chemical     overt 
    0.5241379 0.2482759 0.2275862 
    
    Group means:
                 sspg   glucose
    normal   114.0000  349.9737
    chemical 208.9722  493.9444
    overt    318.8788 1043.7576
    
    Coefficients of linear discriminants:
                    LD1         LD2
    sspg    0.005036943 -0.01539281
    glucose 0.005461400  0.00449050
    
    Proportion of trace:
       LD1    LD2 
    0.9683 0.0317 
    
    
    ghat = predict(diabetes_lda)$class
    table(ghat, diabetes$group)__
    
    
              
    ghat       normal chemical overt
      normal       69       12     1
      chemical      7       24     6
      overt         0        0    26
    
    
    mean(ghat != diabetes$group)__
    
    
    [1] 0.1793103

__

질문 12.1

위 출력의 다른 부분들은 무엇을 의미합니까?

이제 LDA 결과를 시각화해 보겠습니다. 각 세 그룹에 대한 예측 영역을 그릴 것입니다. 이를 위해 점 그리드를 만들고 각각에 대해 예측 규칙을 사용합니다. 그런 다음 LDA의 메커니즘을 좀 더 깊이 파고들어 클래스 중심(`diabetes_lda$means`)과 적합된 공분산 행렬(`diabetes_lda$scaling`)에 해당하는 타원을 그릴 것입니다. 이 시각화를 조립하려면 약간의 코드를 작성해야 합니다.

    
    
    make1Dgrid = function(x) {
      rg = grDevices::extendrange(x)
      seq(from = rg[1], to = rg[2], length.out = 100)
    }__

데이터 범위를 커버하는 \(100 \times 100\) 그리드인 예측용 점들을 설정합니다.

    
    
    diabetes_grid = with(diabetes,
      expand.grid(sspg = make1Dgrid(sspg),
                  glucose = make1Dgrid(glucose)))

예측을 수행합니다.

    
    
    diabetes_grid$ghat =
      predict(diabetes_lda, newdata = diabetes_grid)$class __

그룹 중심입니다.

    
    
    centers = diabetes_lda$means __

타원을 계산합니다. 단위원(360개의 변을 가진 다각형으로 근사)에서 시작하여 LDA 출력의 해당 아핀 변환(affine transformation)을 적용합니다.

    
    
    unitcircle = exp(1i * seq(0, 2*pi, length.out = 360)) |>
              (\(z) cbind(Re(z), Im(z)))() 
    ellipse = unitcircle %*% solve(diabetes_lda$scaling) |> as_tibble()__

세 개의 타원 모두, 각 그룹 중심에 하나씩입니다.

    
    
    library("dplyr")
    ellipses = lapply(rownames(centers), function(gr) {
      mutate(ellipse, 
         sspg    = sspg    + centers[gr, "sspg"],
         glucose = glucose + centers[gr, "glucose"],
         group   = gr)
    }) |> bind_rows()__

이제 그릴 준비가 되었습니다 (그림 12.6).

    
    
    ggdb + geom_raster(aes(fill = ghat),
                data = diabetes_grid, alpha = 0.25, interpolate = TRUE) +
        geom_point(data = as_tibble(centers), pch = "+", size = 8) +
        geom_path(aes(colour = group), data = ellipses) +
        scale_x_continuous(expand = c(0, 0)) +
        scale_y_continuous(expand = c(0, 0))__

[![](12-chap_files/figure-html/fig-modeldiabetes-1-1.png)](12-chap_files/figure-html/fig-modeldiabetes-1-1.png "그림 12.6: 그림 12.5와 동일하지만, LDA 모델의 분류 영역이 표시되어 있습니다. 세 개의 타원은 클래스 중심과 LDA 모델의 공분산 행렬을 나타냅니다. 공분산 행렬은 단 하나뿐이며 세 클래스 모두에 대해 동일하다는 점에 유의하십시오. 따라서 타원의 크기와 방향도 세 클래스 모두에 대해 동일하며 중심만 다릅니다. 이들은 동일한 클래스 멤버십 확률의 등고선을 나타냅니다.")

그림 12.6: 그림 12.5와 동일하지만, LDA 모델의 분류 영역이 표시되어 있습니다. 세 개의 타원은 클래스 중심과 LDA 모델의 공분산 행렬을 나타냅니다. 공분산 행렬은 단 하나뿐이며 세 클래스 모두에 대해 동일하다는 점에 유의하십시오. 따라서 타원의 크기와 방향도 세 클래스 모두에 대해 동일하며 중심만 다릅니다. 이들은 동일한 클래스 멤버십 확률의 등고선을 나타냅니다.

__

질문 12.2

_chemical_ 과 _overt_ 사이의 예측 영역 경계가 그룹 중심 사이의 선과 수직이 아닌 이유는 무엇입니까?

__

해결책

__

타원이 원이라면 경계는 수직일 것입니다. 일반적으로 경계는 동일 클래스 확률의 등고선에 접하며, 등고선의 타원 형태로 인해 경계는 일반적으로 중심 사이의 선과 수직이 아닙니다.

__

질문 12.3

모든 군집 중심에서 멀리 떨어진 2D 평면 영역에서의 예측에 대해 얼마나 확신할 수 있습니까?

__

해결책

__

어떤 군집 중심에서도 멀리 떨어진 예측은 비판적으로 평가해야 합니다. 이는 LDA 모델이 좋지 않을 수 있거나 예측을 뒷받침할 훈련 데이터가 근처에 없을 수 있는 영역으로의 외삽(extrapolation)에 해당하기 때문입니다. 가장 가까운 중심까지의 거리를 특정 지점에 대한 예측 신뢰도의 척도로 사용할 수 있습니다. 그러나 리샘플링 및 교차 검증 기반 방법이 더 일반적이고 보통 더 신뢰할 수 있는 척도를 제공한다는 것을 보게 될 것입니다.

__

질문 12.4

_normal_ 과 _chemical_ 사이의 예측 영역 경계가 중심 사이의 중간이 아니라 _normal_ 에 유리하게 이동된 이유는 무엇입니까? 힌트: `lda`의 `prior` 인수를 살펴보세요. 균일 사전 확률(uniform prior)로 다시 시도해 보세요.

__

해결책

__

다음 코드 청크의 결과는 그림 12.7에 나와 있습니다. 접미사 `_up`은 "uniform prior"의 줄임말입니다.

    
    
    diabetes_up = lda(group ~ sspg + glucose, data = diabetes,
      prior = (\(n) rep(1/n, n)) (nlevels(diabetes$group)))
    
    diabetes_grid$ghat_up =
      predict(diabetes_up, newdata = diabetes_grid)$class
    
    stopifnot(all.equal(diabetes_up$means, diabetes_lda$means))
    
    ellipse_up  = unitcircle %*% solve(diabetes_up$scaling) |> as_tibble()
    ellipses_up = lapply(rownames(centers), function(gr) {
      mutate(ellipse_up, 
         sspg    = sspg    + centers[gr, "sspg"],
         glucose = glucose + centers[gr, "glucose"],
         group   = gr)
    }) |> bind_rows()
    
    ggdb + geom_raster(aes(fill = ghat_up),
                data = diabetes_grid, alpha = 0.4, interpolate = TRUE) +
        geom_point(data = data.frame(centers), pch = "+", size = 8) +
        geom_path(aes(colour = group), data = ellipses_up) +
        scale_x_continuous(expand = c(0, 0)) +
        scale_y_continuous(expand = c(0, 0))__

[![](12-chap_files/figure-html/fig-diabetes-lda-uniform-prior-1-1.png)](12-chap_files/figure-html/fig-diabetes-lda-uniform-prior-1-1.png "그림 12.7: 그림 12.6과 동일하지만, 균일 클래스 사전 확률을 사용했습니다.")

그림 12.7: 그림 12.6과 동일하지만, 균일 클래스 사전 확률을 사용했습니다.

`stopifnot` 라인은 클래스 중심이 사전 확률과 독립적이기 때문에 동일함을 확인해 줍니다. 결합 공분산은 그렇지 않습니다.

__

질문 12.5

그림 12.6과 12.7은 타원을 통한 적합된 LDA 모델과 영역 색칠을 통한 예측 영역을 모두 보여줍니다. 이 시각화의 어떤 부분이 모든 종류의 분류 방법에 대해 일반적이며, 어떤 부분이 방법 특정적입니까?

__

해결책

__

예측 영역은 "블랙박스" 방법을 포함한 모든 분류 방법에 대해 표시될 수 있습니다. 그림 12.6과 12.7의 군집 중심과 타원은 방법 특정적입니다.

__

질문 12.6

`glucose`와 `sspg`만 사용하는 대신 5개의 변수를 모두 사용하면 예측 정확도에 어떤 차이가 있습니까?

__

해결책

__

    
    
    diabetes_lda5 = lda(group ~ rw + fpg + glucose + sspg + insulin, data = diabetes)
    diabetes_lda5 __
    
    
    Call:
    lda(group ~ rw + fpg + glucose + sspg + insulin, data = diabetes)
    
    Prior probabilities of groups:
       normal  chemical     overt 
    0.5241379 0.2482759 0.2275862 
    
    Group means:
                    rw       fpg   glucose     sspg  insulin
    normal   0.9372368  91.18421  349.9737 114.0000 172.6447
    chemical 1.0558333  99.30556  493.9444 208.9722 288.0000
    overt    0.9839394 217.66667 1043.7576 318.8788 106.0000
    
    Coefficients of linear discriminants:
                      LD1          LD2
    rw       1.3624356881 -3.784142444
    fpg     -0.0336487883  0.036633317
    glucose  0.0125763942 -0.007092017
    sspg     0.0042431866  0.001134070
    insulin -0.0001022245 -0.006173424
    
    Proportion of trace:
       LD1    LD2 
    0.8812 0.1188 
    
    
    ghat5 = predict(diabetes_lda5)$class
    table(ghat5, diabetes$group)__
    
    
              
    ghat5      normal chemical overt
      normal       73        5     1
      chemical      3       31     5
      overt         0        0    27
    
    
    mean(ghat5 != diabetes$group)__
    
    
    [1] 0.09655172

__

Question 12.7

점 그리드로부터 분류하여 예측 영역을 근사하는 대신, 선형 판별 계수로부터 구분선을 명시적으로 계산하세요.

__

Solution

__

([Hastie, Tibshirani, and Friedman 2008](16-chap.html#ref-HastieTibshiraniFriedman))의 4.3절, 식 (4.10)을 참조하십시오.