{"title":"4.1 Goals for this chapter","markdown":{"headingText":"4.1 Goals for this chapter","containsRefs":false,"markdown":"![](imgs/t_distribution.png)\n\nOne of the main challenges of biological data analysis is dealing with\nheterogeneity. The quantities we are interested in often do not show a simple,\nunimodal “textbook distribution”. For example, in the last part of [Chapter\n2](02-chap.html) we saw how the histogram of sequence scores in [Figure\n2.27](02-chap.html#fig-ScoreMixture-1) had two separate modes, one for CpG-\nislands and one for non-islands. We can see the data as a simple mixture of a\nfew (in this case: two) components. We call these _finite mixtures_. Other\nmixtures can involve almost as many components as we have observations. These\nwe call _infinite mixtures_ 1.\n\n1 We will see that—as for so many modeling choices–the right complexity of the\nmixture is in the eye of the beholder and often depends on the amount of data\nand the resolution and smoothness we want to attain.\n\nIn [Chapter 1](01-chap.html) we saw how a simple generative model with a\nPoisson distribution led us to make useful inference in the detection of an\nepitope. Unfortunately, a satisfactory fit to real data with such a simple\nmodel is often out of reach. However, simple models such as the normal or\nPoisson distributions can serve as building blocks for more realistic models\nusing the mixing framework that we cover in this chapter. Mixtures occur\nnaturally for flow cytometry data, biometric measurements, RNA-Seq, ChIP-Seq,\nmicrobiome and many other types of data collected using modern\nbiotechnologies. In this chapter we will learn from simple examples how to\nbuild more realistic models of distributions using mixtures.\n\n\nIn this chapter we will:\n\n  * Generate our own mixture model data from distributions composed of two normal populations.\n\n  * See how the Expectation-Maximization (EM) algorithm enables us to “reverse engineer” the underlying mixtures in dataset.\n\n  * Use a special type of mixture called zero-inflation for data such as ChIP-Seq data that have many extra zeros.\n\n  * Discover the empirical cumulative distribution: a special mixture that we can build from observed data. This will enable us to see how we can simulate the variability of our estimates using the bootstrap.\n\n  * Build the Laplace distribution as an instance of an infinite mixture model with many components. We will use it to model promoter lengths and microarray intensities.\n\n  * Have our first encounter with the gamma-Poisson distribution, a hierarchical model useful for RNA-Seq data. We will see it comes naturally from mixing different Poisson distributed sources.\n\n  * See how mixture models enable us to choose data transformations.\n\n## 4.2 Finite mixtures\n\n### 4.2.1 Simple examples and computer experiments\n\nHere is a first example of a mixture model with two equal-sized components.\nThe generating process has two steps:\n\n**Flip a fair coin.**\n\nIf it comes up heads: Generate a random number from a normal distribution with\nmean 1 and variance 0.25.\n\nIf it comes up tails: Generate a random number from a normal distribution with\nmean 3 and variance 0.25. The histogram shown in Figure 4.1 was produced by\nrepeating these two steps 10000 times using the following code.\n\n    \n    \n    coinflips = (runif(10000) > 0.5)\n    table(coinflips)__\n    \n    \n    coinflips\n    FALSE  TRUE \n     5003  4997 \n    \n    \n    oneFlip = function(fl, mean1 = 1, mean2 = 3, sd1 = 0.5, sd2 = 0.5) {\n      if (fl) {\n       rnorm(1, mean1, sd1)\n      } else {\n       rnorm(1, mean2, sd2)\n      }\n    }\n    fairmix = vapply(coinflips, oneFlip, numeric(1))\n    library(\"ggplot2\")\n    library(\"dplyr\")\n    ggplot(tibble(value = fairmix), aes(x = value)) +\n         geom_histogram(fill = \"purple\", binwidth = 0.1)__\n\n[![](04-chap_files/figure-html/fig-twocoins-1.png)](04-chap_files/figure-\nhtml/fig-twocoins-1.png \"Figure 4.1: Histogram of 10,000 random draws from a\nfair mixture of two normals. The left hand part of the histogram is dominated\nby numbers generated from \\(A\\), on the right from \\(B\\).\")\n\nFigure 4.1: Histogram of 10,000 random draws from a fair mixture of two\nnormals. The left hand part of the histogram is dominated by numbers generated\nfrom (A), on the right from (B).\n\n__\n\nQuestion 4.1\n\nHow can you use R’s vectorized syntax to remove the `vapply`-loop and generate\nthe `fairmix` vector more efficiently?\n\n__\n\nSolution\n\n__\n\n    \n    \n    means = c(1, 3)\n    sds   = c(0.5, 0.5)\n    values = rnorm(length(coinflips),\n                   mean = ifelse(coinflips, means[1], means[2]),\n                   sd   = ifelse(coinflips, sds[1],   sds[2]))__\n\n__\n\nQuestion 4.2\n\nUsing your improved code, use one million coin flips and make a histogram with\n200 bins. What do you notice?\n\n__\n\nSolution\n\n__\n\n    \n    \n    fair = tibble(\n      coinflips = (runif(1e6) > 0.5),\n      values = rnorm(length(coinflips),\n                     mean = ifelse(coinflips, means[1], means[2]),\n                     sd   = ifelse(coinflips, sds[1],   sds[2])))\n    ggplot(fair, aes(x = values)) +\n         geom_histogram(fill = \"purple\", bins = 200)__\n\n[![](04-chap_files/figure-html/fig-\nlimitinghistogram-1.png)](04-chap_files/figure-html/fig-\nlimitinghistogram-1.png \"Figure 4.2: Similar to Figure fig-twocoins, but with\none million observations.\")\n\nFigure 4.2: Similar to Figure 4.1, but with one million observations.\n\nFigure 4.2 illustrates that as we increase the number of bins and the number\nof observations per bin, the histogram approaches a smooth curve. This smooth\nlimiting curve is called the **density** function of the random variable\n`fair$values`.\n\nThe density function for a normal \\\\(N(\\mu,\\sigma)\\\\) random variable can be\nwritten explicitly. We usually call it\n\n\\\\[ \\phi(x)=\\frac{1}{\\sigma\n\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}. \\\\]\n\n__\n\nQuestion 4.3\n\n  1. Plot a histogram of those values of `fair$values` for which `coinflips` is `TRUE`. Hint: use `y = ..density..` in the call to `aes` (which means that the vertical axis is to show the proportion of counts) and set the binwidth to 0.01.\n  2. Overlay the line corresponding to \\\\(\\phi(z)\\\\).\n\n__\n\nSolution\n\n__\n\n    \n    \n    ggplot(dplyr::filter(fair, coinflips), aes(x = values)) +\n      geom_histogram(aes(y = after_stat(density)), fill = \"purple\", binwidth = 0.01) +\n      stat_function(fun = dnorm, color = \"red\",\n                    args = list(mean = means[1], sd = sds[1]))__\n\n[![](04-chap_files/figure-html/fig-\noverlaydensity-1.png)](04-chap_files/figure-html/fig-overlaydensity-1.png\n\"Figure 4.3: Histogram of half a million draws from the normal distribution\nN\\(\\\\mu=1,\\\\sigma^2=0.5^2\\). The curve is the theoretical density \\\\phi\\(x\\)\ncalculated using the function dnorm.\")\n\nFigure 4.3: Histogram of half a million draws from the normal distribution\n\\\\(N(\\mu=1,\\sigma^2=0.5^2)\\\\). The curve is the theoretical density\n\\\\(\\phi(x)\\\\) calculated using the function `dnorm`.\n\nIn fact we can write the mathematical formula for the density of all of\n`fair$values` (the limiting curve that the histograms tend to look like) as a\nsum of the two densities.\n\n\\\\[ f(x)=\\frac{1}{2}\\phi_1(x)+\\frac{1}{2}\\phi_2(x), \\tag{4.1}\\\\]\n\nwhere \\\\(\\phi_1\\\\) is the density of the normal \\\\(N(\\mu_1=1,\\sigma^2=0.25)\\\\)\nand \\\\(\\phi_2\\\\) is the density of the normal \\\\(N(\\mu_2=3,\\sigma^2=0.25)\\\\).\nFigure 4.4 was generated by the following code.\n\n    \n    \n    fairtheory = tibble(\n      x = seq(-1, 5, length.out = 1000),\n      f = 0.5 * dnorm(x, mean = means[1], sd = sds[1]) +\n          0.5 * dnorm(x, mean = means[2], sd = sds[2]))\n    ggplot(fairtheory, aes(x = x, y = f)) +\n      geom_line(color = \"red\", linewidth = 1.5) + ylab(\"mixture density\")__\n\n[![](04-chap_files/figure-html/fig-twodensity-1.png)](04-chap_files/figure-\nhtml/fig-twodensity-1.png \"Figure 4.4: The theoretical density of the\nmixture.\")\n\nFigure 4.4: The theoretical density of the mixture.\n\nIn this case, the mixture model is extremely visible as the two component\ndistributions have little overlap. Figure 4.4 shows two distinct peaks: we\ncall this a **bimodal** distribution. In practice, in many cases the\nseparation between the mixture components is not so obvious—but nevertheless\nrelevant.\n\n[![](04-chap_files/figure-html/fig-histmystery-1.png)](04-chap_files/figure-\nhtml/fig-histmystery-1.png \"Figure 4.5: A mixture of two normals that is\nharder to recognize.\")\n\nFigure 4.5: A mixture of two normals that is harder to recognize.\n\n__\n\nQuestion 4.4\n\nFigure 4.5 is a histogram of a fair mixture of two normals with the same\nvariances. Can you guess the two _mean_ parameters of the component\ndistributions? Hint: you can use trial and error, and simulate various\nmixtures to see if you can make a matching histogram. Looking at the R code\nfor the chapter will show you exactly how the data were generated.\n\n__\n\nSolution\n\n__\n\nThe following code colors in red the points that were generated from the\n_heads_ coin flips and in blue those from the _tails_. Its output, in Figure\n4.6, shows the two underlying distributions.\n\n    \n    \n    head(mystery, 3)__\n    \n    \n    # A tibble: 3 × 2\n      coinflips values\n      <lgl>      <dbl>\n    1 FALSE       2.40\n    2 FALSE       1.66\n    3 TRUE        1.22\n    \n    \n    br = with(mystery, seq(min(values), max(values), length.out = 30))\n    ggplot(mystery, aes(x = values)) +\n      geom_histogram(data = dplyr::filter(mystery, coinflips),\n         fill = \"red\", alpha = 0.2, breaks = br) +\n      geom_histogram(data = dplyr::filter(mystery, !coinflips),\n         fill = \"darkblue\", alpha = 0.2, breaks = br) __\n\n[![](04-chap_files/figure-html/fig-\nbetterhistogram-1-1.png)](04-chap_files/figure-html/fig-\nbetterhistogram-1-1.png \"Figure 4.6: The mixture from Figure fig-histmystery,\nbut with the two components colored in red and blue.\")\n\nFigure 4.6: The mixture from Figure 4.5, but with the two components colored\nin red and blue.\n\nIn Figure 4.6, the bars from the two component distributions are plotted on\ntop of each other. A different way of showing the components is Figure 4.7,\nproduced by the code below.\n\n    \n    \n    ggplot(mystery, aes(x = values, fill = coinflips)) +\n      geom_histogram(data = dplyr::filter(mystery, coinflips),\n         fill = \"red\", alpha = 0.2, breaks = br) +\n      geom_histogram(data = dplyr::filter(mystery, !coinflips),\n         fill = \"darkblue\", alpha = 0.2, breaks = br) +\n      geom_histogram(fill = \"purple\", breaks = br, alpha = 0.2)__\n\n[![](04-chap_files/figure-html/fig-\ncomparecomponents-1-1.png)](04-chap_files/figure-html/fig-\ncomparecomponents-1-1.png \"Figure 4.7: As Figure fig-betterhistogram-1, with\nstacked bars for the two mixture components.\")\n\nFigure 4.7: As Figure 4.6, with stacked bars for the two mixture components.\n\n__\n\nQuestion 4.5\n\nWhy do the bars in Figure 4.7, but not those in Figure 4.6 have the same\nheights as in Figure 4.5?\n\n__\n\nSolution\n\n__\n\nIn Figures 4.7 and 4.5, each count occupies a different piece of vertical\nspace in a bin. In Figure 4.6, in the overlapping (darker) region, some of the\ncounts falling within the same bin are overplotted.\n\nIn Figures 4.6 and 4.7, we were able to use the `coinflips` column in the data\nto disentangle the components. In real data, this information is missing.\n\n[![A book-long treatment on the subject of finite mixtures \\(McLachlan and\nPeel 2004\\).](imgs/book_icon.png)](imgs/book_icon.png \"A book-long treatment\non the subject of finite mixtures \\[@mclachlan2004\\].\")\n\nA book-long treatment on the subject of finite mixtures ([McLachlan and Peel\n2004](16-chap.html#ref-mclachlan2004)).\n\n### 4.2.2 Discovering the hidden class labels\n\nWe use a method called the _expectation-maximization (EM) algorithm_ to infer\nthe value of the hidden groupings. The EM algorithm is a popular iterative\nprocedure that alternates between pretending we know one part of the solution\nto compute the other part, and pretending the other part is known and\ncomputing the first part, and so on, until convergence. More concretely, it\nalternates between\n\n  * pretending we know the probabilities with which each observation belongs to the different mixture components and, from this, estimating the parameters of the components, and\n\n  * pretending we know the parameters of the mixture components and estimating the probabilities with which each observation belongs to the components.\n\nLet’s take a simple example. We measure a variable \\\\(x\\\\) on a series of\nobjects that we think come from two groups, although we do not know the group\nlabels. We start by _augmenting_ 2 the data with the unobserved (latent) group\nlabel, which we will call \\\\(U\\\\).\n\n2 Adding another variable which was not measured, called a hidden or **latent\nvariable**.\n\nWe are interested in finding the values of \\\\(U\\\\) and the unknown parameters\n\\\\(\\theta\\\\) of the underlying distribution of the groups. We will use the\nmaximum likelihood approach introduced in [Chapter 2](02-chap.html) to\nestimate the parameters that make the data \\\\(x\\\\) the most likely. We can\nwrite the probability densities\n\n\\\\[ p(x,u\\,|\\,\\theta) = p(x\\,|\\,u,\\theta)\\,p(u\\,|\\,\\theta). \\tag{4.2}\\\\]\n\n#### Mixture of normals\n\nFor instance, we could generalize our previous mixture model with two normal\ndistributions Equation 4.1 by allowing non-equal mixture fractions,\n\n\\\\[ f(x)=\\lambda\\phi_1(x)+(1-\\lambda)\\phi_2(x), \\tag{4.3}\\\\]\n\nfor \\\\(\\lambda\\in[0,1]\\\\). Similarly as above, \\\\(\\phi_k\\\\) is the density of\nthe normal \\\\(N(\\mu_k,\\sigma_k^2)\\\\) for \\\\(k=1\\\\) and \\\\(k=2\\\\),\nrespectively. Then, the parameter vector \\\\(\\theta\\\\) is a five-tuple of the\ntwo means, the two standard deviations, and the mixture parameter\n\\\\(\\lambda\\\\). In other words,\n\\\\(\\theta=(\\mu_1,\\mu_2,\\sigma_1,\\sigma_2,\\lambda)\\\\). Here is an example of\ndata generated according to such a model. The labels are denoted by \\\\(u\\\\).\n\n    \n    \n    mus = c(-0.5, 1.5)\n    lambda = 0.5\n    u = sample(2, size = 100, replace = TRUE, prob = c(lambda, 1-lambda))\n    x = rnorm(length(u), mean = mus[u])\n    dux = tibble(u, x)\n    head(dux)__\n    \n    \n    # A tibble: 6 × 2\n          u     x\n      <int> <dbl>\n    1     2 0.303\n    2     2 2.65 \n    3     1 0.484\n    4     2 3.04 \n    5     2 1.10 \n    6     2 1.96 \n\nIf we know the labels \\\\(u\\\\), we can estimate the parameters using separate\nMLEs for each group. The overall likelihood is\n\n\\\\[ p(x, u \\,|\\, \\theta) = \\left( \\prod_{\\\\{i:\\,u_i=1\\\\}} \\phi_1(x_i) \\right)\n\\times \\left( \\prod_{\\\\{i:\\,u_i=2\\\\}} \\phi_2(x_i) \\right). \\tag{4.4}\\\\]\n\nIts maximization can be split into three independent pieces: we find\n\\\\(\\mu_1\\\\) and \\\\(\\sigma_1\\\\) by maximizing the first bracketed expression on\nthe right hand side of Equation 4.4, \\\\(\\mu_2\\\\) and \\\\(\\sigma_2\\\\) by\nmaximizing the second, and \\\\(\\lambda\\\\) from the empirical frequencies of the\nlabels:\n\n    \n    \n    group_by(dux, u) |> summarize(mu = mean(x), sigma = sd(x))__\n    \n    \n    # A tibble: 2 × 3\n          u     mu sigma\n      <int>  <dbl> <dbl>\n    1     1 -0.558  1.05\n    2     2  1.41   1.04\n    \n    \n    table(dux$u) / nrow(dux)__\n    \n    \n       1    2 \n    0.55 0.45 \n\n__\n\nQuestion 4.6\n\nSuppose we knew the mixing proportion \\\\(\\lambda=\\frac{1}{2}\\\\), but not the\n\\\\(u_i\\\\), so that the density is\n\\\\(\\frac{1}{2}\\phi_1(x)+\\frac{1}{2}\\phi_2(x)\\\\). Try writing out the (log)\nlikelihood. What prevents us from solving for the MLE explicitly here?\n\n__\n\nSolution\n\n__\n\nSee, e.g., the chapter on _Mixture Models_ in Shalizi\n([2017](16-chap.html#ref-CosmaShalizi2017)) for the computation of the\nlikelihood of a finite mixture of normals. “If we try to estimate the mixture\nmodel, then, we’re doing weighted maximum likelihood, with weights given by\nthe posterior label probabilities. These, to repeat, depend on the parameters\nwe are trying to estimate, so there seems to be a vicious circle.”\n\nIn many cases we neither know the \\\\(u\\\\) labels nor the mixture proportions.\nWhat we can do is start with an initial guess for the labels and pretend to\nknow them, estimate the parameters as above, and then go through an iterative\ncycle, updating at each step our current best guess of the labels and the\nparameters until our estimates no longer substantially change (i.e., until\nthey seem to have converged) and the likelihood has reached an optimum.\n\nIn fact we can do something more elaborate and replace the “hard” labels\n\\\\(u\\\\) for each observation (it is either in group 1 or 2) by membership\nweights that sum up to 1. The mixture model 4.3 suggests that we interpret\n\n\\\\[ w(x, 1) = \\frac{\\lambda\\phi_1(x)}{\\lambda\\phi_1(x)+(1-\\lambda)\\phi_2(x)}\n\\tag{4.5}\\\\]\n\nas the probability that an observation with value \\\\(x\\\\) was generated by the\nfirst mixture component, and analogously \\\\(w(x, 2) = 1 - w(x, 1)\\\\) for the\nsecond component. In other words, while \\\\(\\lambda\\\\) is the prior probability\nthat an observation that we have not yet seen comes from mixture component 1,\n\\\\(w(x,1)\\\\) is the corresponding posterior probability once we have observed\nits value \\\\(x\\\\). This suggests the following iterative algorithm:\n\n**E step** : Assuming that \\\\(\\theta\\\\) (i.e., the means, standard deviations\nand \\\\(\\lambda\\\\)) is known, evaluate the membership weights 4.5.\n\n**M step** : Given the membership weights of each observation \\\\(x_i\\\\),\ndetermine a new, improved maximum likelihood estimate of \\\\(\\theta\\\\).\n\nIterate until \\\\(\\theta\\\\) and the likelihood have converged. At this point,\nplease check out Exercise 4.1 for a worked out demonstration with code. In\nfact, this algorithm is far more general than our particular application\nexample here. A very readable exposition is presented by ([Bishop\n2006](16-chap.html#ref-Bishop:PRML)), here some of the highlights:\n\nOur goal is to maximize the marginal likelihood of a probabilistic model that\ninvolves an observed variable \\\\(x\\\\), an unobserved variable \\\\(u\\\\) and some\nparameter \\\\(\\theta\\\\). In our simple example, \\\\(u\\\\) is a categorical\nvariable with two possible values, and \\\\(x\\\\) a real number. In general, both\n\\\\(x\\\\) and \\\\(u\\\\) can be a tuple of multiple individual variables (i.e.,\nthey can be multivariate) of any variable type. The marginal likelihood is\ncomputed by taking the expectation (i.e., a weighted average) across all\npossible values of \\\\(u\\\\):\n\n\\\\[ L_\\text{marg}(\\theta; x) = \\int_U p(x, u\\,|\\,\\theta) \\, \\text{d}U.\n\\tag{4.6}\\\\]\n\nIn our specific example, the integration amounts to (probabilistically)\naveraging over all possible membership configurations, and thus, to the\nweighted sum taking into account the membership weights,\n\n\\\\[ L_\\text{marg}(\\theta; x) = \\prod_{i=1}^n \\sum_{u=1}^2 p(x_i, u\\,|\\,\\theta)\n\\, w(x_i, u\\,|\\,\\theta). \\tag{4.7}\\\\]\n\nDirectly maximizing this quantity is intractable.\n\nSomething we do have a grip on, given the data and our current parameter\nestimate \\\\(\\theta_t\\\\), is the conditional distribution of the latent\nvariable, \\\\(p(u\\,|\\,x, \\theta_t)\\\\). We can use it to find the expectation of\nthe complete data log likelihood \\\\(\\log p(x, u\\,|\\,\\theta)\\\\) evaluated for\nsome general parameter value \\\\(\\theta\\\\). This expectation is often denoted\nas\n\n\\\\[ Q(\\theta, \\theta_t) = \\int_U \\log p(x, u\\,|\\,\\theta) \\, p(u\\,|\\, x,\n\\theta_t) \\, \\text{d}U. \\tag{4.8}\\\\]\n\nIn the M step, we determine the revised parameter estimate \\\\(\\theta_{t+1}\\\\)\nby maximizing this function,\n\n\\\\[ \\theta_{t+1} = \\arg \\max_\\theta \\, Q(\\theta, \\theta_t). \\tag{4.9}\\\\]\n\nThe E step consists of computing \\\\(p(u\\,|\\, x, \\theta_t)\\\\), the essential\ningredient of \\\\(Q\\\\).\n\nThese two steps (E and M) are iterated until the improvements are small; this\nis a numerical indication that we are close to a flattening of the likelihood\nand have reached a maximum. The iteration trajectory, but hopefully not the\npoint that we arrive at, will depend on the starting point. This is analogous\nto a hike to the top of a mountain, which can start at different places and\nmay take different routes, but always leads to the top of the hill—as long as\nthere is only one hilltop and not several. Thus, as a precaution, it is good\npractice to repeat such a procedure several times from different starting\npoints and make sure that we always get the same answer.\n\n__\n\nQuestion 4.7\n\nSeveral R packages provide EM implementations, including\n**[mclust](https://cran.r-project.org/web/packages/mclust/)** ,\n**[EMcluster](https://cran.r-project.org/web/packages/EMcluster/)** and\n**[EMMIXskew](https://cran.r-project.org/web/packages/EMMIXskew/)**. Choose\none and run the EM function several times with different starting values. Then\nuse the function `normalmixEM` from the\n**[mixtools](https://cran.r-project.org/web/packages/mixtools/)** package to\ncompare the outputs.\n\n__\n\nSolution\n\n__\n\nHere we show the output from\n**[mixtools](https://cran.r-project.org/web/packages/mixtools/)**.\n\n    \n    \n    library(\"mixtools\")\n    y = c(rnorm(100, mean = -0.2, sd = 0.5),\n          rnorm( 50, mean =  0.5, sd =   1))\n    gm = normalmixEM(y, k = 2, \n                        lambda = c(0.5, 0.5),\n                        mu = c(-0.01, 0.01), \n                        sigma = c(3, 3))__\n    \n    \n    number of iterations= 134 \n    \n    \n    with(gm, c(lambda, mu, sigma, loglik))__\n    \n    \n    [1]    0.9263    0.0737   -0.1859    1.9216    0.6134    0.4746 -170.5662\n\nThe EM algorithm is very instructive:\n\n  1. We saw a first example of _soft_ averaging, where we don’t decide whether an observation belongs to one group or another, but allow it to participate in several groups by using membership probabilities as weights, and thus obtain more nuanced estimates ([Slonim et al. 2005](16-chap.html#ref-Slonim:2005)).\n\n  2. It shows us how we can tackle a difficult problem with too many unknowns by alternating between solving simpler problems.\n\n  3. We were able to consider a data generating model with hidden variables, and still estimate its parameters. We could do so even without explicitly commiting on the values of the hidden variables: we took a (weighted) average over them, in the Expecation step of Equation 4.8, embodied by the membership probabilities. This basic idea is so powerful that it is the starting point for many more advanced algorithms in machine learning ([Bishop 2006](16-chap.html#ref-Bishop:PRML)).\n\n  4. It can be extended to the more general case of model averaging ([Hoeting et al. 1999](16-chap.html#ref-hoeting1999)). It can be sometimes beneficial to consider several models simultaneously if we are unsure which one is relevant for our data. We can combine them together into a weighted model. The weights are provided by the likelihoods of the models.\n\n### 4.2.3 Models for zero inflated data\n\nEcological and molecular data often come in the form of counts. For instance,\nthis may be the number of individuals from each of several species at each of\nseveral locations. Such data can often be seen as a mixture of two scenarios:\nif the species is not present, the count is necessarily zero, but _if_ the\nspecies is present, the number of individuals we observe varies, with a random\nsampling distribution; this distribution may also include zeros. We model this\nas a mixture:\n\n\\\\[ f_{\\text{zi}}(x) = \\lambda \\, \\delta(x) + (1-\\lambda) \\,\nf_{\\text{count}}(x), \\tag{4.10}\\\\]\n\nwhere \\\\(\\delta\\\\) is Dirac’s delta function, which represents a probability\ndistribution that has all its mass at 0. The zeros from the first mixture\ncomponent are called “structural”: in our example, they occur because certain\nspecies do not live in certain habitats. The second component,\n\\\\(f_{\\text{count}}\\\\) may also include zeros and other small numbers, simply\ndue to random sampling. The R packages\n**[pscl](https://cran.r-project.org/web/packages/pscl/)** ([Zeileis, Kleiber,\nand Jackman 2008](16-chap.html#ref-Zeileis:2008)) and\n**[zicounts](https://cran.r-project.org/web/packages/zicounts/)** provide many\nexamples and functions for working with **zero inflated** counts.\n\n#### Example: ChIP-Seq data\n\nLet’s consider the example of ChIP-Seq data. These data are sequences of\npieces of DNA that are obtained from chromatin immunoprecipitation (ChIP).\nThis technology enables the mapping of the locations along genomic DNA of\ntranscription factors, nucleosomes, histone modifications, chromatin\nremodeling enzymes, chaperones, polymerases and other protein. It was the main\ntechnology used by the Encyclopedia of DNA Elements (ENCODE) Project. Here we\nuse an example ([Kuan et al. 2011](16-chap.html#ref-Kuan2011statistical)) from\nthe **[mosaicsExample](https://bioconductor.org/packages/mosaicsExample/)**\npackage, which shows data measured on chromosome 22 from ChIP-Seq of\nantibodies for the STAT1 protein and the H3K4me3 histone modification applied\nto the GM12878 cell line. Here we do not show the code to construct the\n`binTFBS` object; it is shown in the source code file for this chapter and\nfollows the vignette of the\n**[mosaics](https://bioconductor.org/packages/mosaics/)** package.\n\n    \n    \n    binTFBS __\n    \n    \n    Summary: bin-level data (class: BinData)\n    ----------------------------------------\n    - # of chromosomes in the data: 1\n    - total effective tag counts: 462479\n      (sum of ChIP tag counts of all bins)\n    - control sample is incorporated\n    - mappability score is NOT incorporated\n    - GC content score is NOT incorporated\n    - uni-reads are assumed\n    ----------------------------------------\n\nFrom this object, we can create the histogram of per-bin counts.\n\n    \n    \n    bincts = print(binTFBS)\n    ggplot(bincts, aes(x = tagCount)) +\n      geom_histogram(binwidth = 1, fill = \"forestgreen\")__\n\n[![](04-chap_files/figure-html/fig-chipseqzeros-1.png)](04-chap_files/figure-\nhtml/fig-chipseqzeros-1.png \"Figure 4.8: The number of binding sites found in\n200nt windows along chromosome 22 in a ChIP-Seq dataset.\")\n\nFigure 4.8: The number of binding sites found in 200nt windows along\nchromosome 22 in a ChIP-Seq dataset.\n\nWe see in Figure 4.8 that there are many zeros, although from this plot it is\nnot immediately obvious whether the number of zeros is really extraordinary,\ngiven the frequencies of the other small numbers (\\\\(1,2,...\\\\)).\n\n__\n\nQuestion 4.8\n\n  1. Redo the histogram of the counts using a logarithm base 10 scale on the \\\\(y\\\\)-axis.  \n\n  2. Estimate \\\\(\\pi_0\\\\), the proportion of bins with zero counts.\n\n__\n\nSolution\n\n__\n\n    \n    \n    ggplot(bincts, aes(x = tagCount)) + scale_y_log10() +\n       geom_histogram(binwidth = 1, fill = \"forestgreen\")__\n\n[![](04-chap_files/figure-html/fig-\nChipseqHistlogY-1.png)](04-chap_files/figure-html/fig-ChipseqHistlogY-1.png\n\"Figure 4.9: As Figure fig-chipseqzeros, but using a logarithm base 10 scale\non the y-axis. The fraction of zeros seems elevated compared to that of ones,\ntwos, …\")\n\nFigure 4.9: As Figure 4.8, but using a logarithm base 10 scale on the\n\\\\(y\\\\)-axis. The fraction of zeros seems elevated compared to that of ones,\ntwos, …\n\n### 4.2.4 More than two components\n\nSo far we have looked at mixtures of two components. We can extend our\ndescription to cases where there may be more. For instance, when weighing\nN=7,000 nucleotides obtained from mixtures of deoxyribonucleotide\nmonophosphates (each type has a different weight, measured with the same\nstandard deviation sd=3), we might observe the histogram (shown in Figure\n4.10) generated by the following code.\n\n    \n    \n    masses = c(A =  331, C =  307, G =  347, T =  322)\n    probs  = c(A = 0.12, C = 0.38, G = 0.36, T = 0.14)\n    N  = 7000\n    sd = 3\n    nuclt   = sample(length(probs), N, replace = TRUE, prob = probs)\n    quadwts = rnorm(length(nuclt),\n                    mean = masses[nuclt],\n                    sd   = sd)\n    ggplot(tibble(quadwts = quadwts), aes(x = quadwts)) +\n      geom_histogram(bins = 100, fill = \"purple\")__\n\n[![](04-chap_files/figure-html/fig-\nnucleotideweights-1-1.png)](04-chap_files/figure-html/fig-\nnucleotideweights-1-1.png \"Figure 4.10: Simulation of 7,000 nucleotide mass\nmeasurements.\")\n\nFigure 4.10: Simulation of 7,000 nucleotide mass measurements.\n\n__\n\nQuestion 4.9\n\nRepeat this simulation experiment with \\\\(N=1000\\\\) nucleotide measurements.\nWhat do you notice in the histogram?\n\n__\n\nQuestion 4.10\n\nWhat happens when \\\\(N=7000\\\\) but the standard deviation is 10?\n\n__\n\nQuestion 4.11\n\nPlot the theoretical density curve for the distribution simulated in Figure\n4.10.\n\nIn this case, as we have enough measurements with good enough precision, we\nare able to distinguish the four nucleotides and decompose the distribution\nshown in Figure 4.10. With fewer data and/or more noisy measurements, the four\nmodes and the distribution component might be less clear.\n\n## 4.3 Empirical distributions and the nonparametric bootstrap\n\nIn this section, we will consider an extreme case of mixture models, where we\nmodel our sample of \\\\(n\\\\) data points as a mixture of \\\\(n\\\\) point masses.\nWe could use almost any set of data here; to be concrete, we use Darwin’s _Zea\nMays_ data3 in which he compared the heights of 15 pairs of _Zea Mays_ plants\n(15 self-hybridized versus 15 crossed). The data are available in the\n**[HistData](https://cran.r-project.org/web/packages/HistData/)** package, and\nwe plot the distribution of the 15 differences in height:\n\n3 They were collected by Darwin who asked his cousin, Francis Galton to\nanalyse them. R.A. Fisher re-analysed the same data using a paired t-test\n([Bulmer 2003](16-chap.html#ref-bulmer2003)). We will get back to this example\nin [Chapter 13](13-chap.html).\n\n    \n    \n    library(\"HistData\")\n    ZeaMays$diff __\n    \n    \n     [1]  6.125 -8.375  1.000  2.000  0.750  2.875  3.500  5.125  1.750  3.625\n    [11]  7.000  3.000  9.375  7.500 -6.000\n    \n    \n    ggplot(ZeaMays, aes(x = diff, ymax = 1/15, ymin = 0)) +\n      geom_linerange(linewidth = 1, col = \"forestgreen\") + ylim(0, 0.1)__\n\n[![](04-chap_files/figure-html/fig-ecdfZea-1.png)](04-chap_files/figure-\nhtml/fig-ecdfZea-1.png \"Figure 4.11: The observed sample can be seen as a\nmixture of point masses at each of the values \\(real point masses would be\nbars without any width whatsoever\\).\")\n\nFigure 4.11: The observed sample can be seen as a mixture of point masses at\neach of the values (real point masses would be bars without any width\nwhatsoever).\n\nIn [Section 3.6.7](03-chap.html#sec-graphics-ecdf) we saw that the empirical\ncumulative distribution function (ECDF) for a sample of size \\\\(n\\\\) is\n\n\\\\[ \\hat{F}_n(x)= \\frac{1}{n}\\sum_{i=1}^n {\\mathbb 1}_{x \\leq x_i},\n\\tag{4.11}\\\\]\n\nand we saw ECDF plots in [Figure 3.24](03-chap.html#fig-graphics-onedecdf). We\ncan also write the _density_ of our sample as\n\n\\\\[ \\hat{f}_n(x) =\\frac{1}{n}\\sum_{i=1}^n \\delta_{x_i}(x) \\tag{4.12}\\\\]\n\nIn general, the density of a probability distribution is the derivative (if it\nexists) of the distribution function. We have applied this principle here: the\ndensity of the distribution defined by Equation 4.11 is Equation 4.12. We\ncould do this since one can consider the function \\\\(\\delta_a\\\\) the\n“derivative” of the step function \\\\({\\mathbb 1}_{x \\leq a}\\\\): it is\ncompletely flat almost everywhere, except at the one point \\\\(a\\\\) where there\nis the step, and where its value is “infinite” . Equation 4.12 highlights that\nour data sample can be considered a mixture of \\\\(n\\\\) **point masses** at the\nobserved values \\\\(x_1,x_2,..., x_{n}\\\\), such as in Figure 4.11.\n\nThere is a bit of advanced mathematics (beyond standard calculus) required for\nthis to make sense, which is however beyond the scope of our treatment here.\n\n[![](imgs/BootstrapPrincipleNew.png)](imgs/BootstrapPrincipleNew.png\n\"Figure 4.12: The value of a statistic \\\\tau is estimated from data \\(the grey\nmatrices\\) generated from an underlying distribution F. Different samples from\nF lead to different data, and so to different values of the estimate\n\\\\hat{\\\\tau}: this is called sampling variability. The distribution of all the\n\\\\hat{\\\\tau}’s is the sampling distribution.\")\n\nFigure 4.12: The value of a statistic \\\\(\\tau\\\\) is estimated from data (the\ngrey matrices) generated from an underlying distribution \\\\(F\\\\). Different\nsamples from \\\\(F\\\\) lead to different data, and so to different values of the\nestimate \\\\(\\hat{\\tau}\\\\): this is called **sampling variability**. The\ndistribution of all the \\\\(\\hat{\\tau}\\\\)’s is the **sampling distribution**.\n\nStatistics of our sample, such as the mean, minimum or median, can now be\nwritten as a function of the ECDF, for instance, \\\\(\\bar{x} = \\int\n\\delta_{x_i}(x)\\,\\text{d}x\\\\). As another instance, if \\\\(n\\\\) is an odd\nnumber, the median is \\\\(x_{(\\frac{n+1}{2})}\\\\), the value right in the middle\nof the ordered list.\n\nThe true **sampling distribution** of a statistic \\\\(\\hat{\\tau}\\\\) is often\nhard to know as it requires many different data samples from which to compute\nthe statistic; this is shown in Figure 4.12.\n\nThe **bootstrap** principle approximates the true sampling distribution of\n\\\\(\\hat{\\tau}\\\\) by creating new samples drawn from the empirical distribution\nbuilt from the original sample (Figure 4.13). We _reuse_ the data (by\nconsidering it a mixture distribution of \\\\(\\delta\\\\)s) to create new\n“datasets” by taking samples and looking at the sampling distribution of the\nstatistics \\\\(\\hat{\\tau}^*\\\\) computed on them. This is called the\n**nonparametric bootstrap** resampling approach, see Bradley Efron and\nTibshirani ([1994](16-chap.html#ref-Efron:1994)) for a complete reference. It\nis very versatile and powerful method that can be applied to basically any\nstatistic, no matter how complicated it is. We will see example applications\nof this method, in particular to clustering, in [Chapter 5](05-chap.html).\n\n[![](imgs/BootstrapPrinciple2New.png)](imgs/BootstrapPrinciple2New.png\n\"Figure 4.13: The bootstrap simulates the sampling variability by drawing\nsamples not from the underlying true distribution F \\(as in Figure fig-\nsamplingdist\\), but from the empirical distribution function \\\\hat{F}_n.\")\n\nFigure 4.13: The bootstrap simulates the sampling variability by drawing\nsamples not from the underlying true distribution \\\\(F\\\\) (as in Figure 4.12),\nbut from the empirical distribution function \\\\(\\hat{F}_n\\\\).\n\nUsing these ideas, let’s try to estimate the sampling distribution of the\nmedian of the Zea Mays differences we saw in Figure 4.11. We use similar\nsimulations to those in the previous sections: Draw \\\\(B=1000\\\\) samples of\nsize 15 from the 15 values (each being a component in the 15-component\nmixture). Then compute the 1000 medians of each of these samples of 15 values\nand look at their distribution: this is the bootstrap sampling distribution of\nthe median.\n\n    \n    \n    B = 1000\n    meds = replicate(B, {\n      i = sample(15, 15, replace = TRUE)\n      median(ZeaMays$diff[i])\n    })\n    ggplot(tibble(medians = meds), aes(x = medians)) +\n      geom_histogram(bins = 30, fill = \"purple\")__\n\n[![](04-chap_files/figure-html/fig-bootmedian-1.png)](04-chap_files/figure-\nhtml/fig-bootmedian-1.png \"Figure 4.14: The bootstrap sampling distribution of\nthe median of the Zea Mays differences.\")\n\nFigure 4.14: The bootstrap sampling distribution of the median of the Zea Mays\ndifferences.\n\n__\n\nQuestion 4.12\n\nEstimate a 99% confidence interval for the median based on these simulations.\nWhat can you conclude from looking at the overlap between this interval and 0?\n\n__\n\nQuestion 4.13\n\nUse the **[bootstrap](https://cran.r-project.org/web/packages/bootstrap/)**\npackage to redo the same analysis using the function `bootstrap` for both\n`median` and `mean`. What differences do you notice between the sampling\ndistribution of the mean and the median?\n\n__\n\nSolution\n\n__\n\n    \n    \n    library(\"bootstrap\")\n    bootstrap(ZeaMays$diff, B, mean)\n    bootstrap(ZeaMays$diff, B, median)__\n\n#### Why nonparametric?\n\nIn theoretical statistics, nonparametric methods are those that have\ninfinitely many degrees of freedom or numbers of unknown parameters.\n\n![](imgs/devil.png)\n\nIn practice, we do not wait for infinity; when the number of parameters\nbecomes as large or larger than the amount of data available, we say the\nmethod is nonparametric. The bootstrap uses a mixture with \\\\(n\\\\) components,\nso with a sample of size \\\\(n\\\\), it qualifies as a nonparametric method.\n\nDespite their name, nonparametric methods are not methods that do not use\nparameters: all statistical methods estimate unknown quantities.\n\n__\n\nQuestion 4.14\n\nIf the sample is composed of \\\\(n=3\\\\) different values, how many different\nbootstrap resamples are possible? Answer the same question with \\\\(n=15\\\\).\n\n__\n\nSolution\n\n__\n\nThe set of all bootstrap resamples is equivalent to the set of all vectors of\n\\\\(n\\\\) integers whose sum is \\\\(n\\\\). Denote by \\\\(\\mathbf{k} =\n(k_1,k_2,...,k_n)\\\\) the number of times the observations \\\\(x_1,x_2,...,\nx_n\\\\) occur in a bootstrap sample. We can think of each \\\\(k_i\\\\) as a box\n(as in the multinomial distribution), and there are \\\\(n\\\\) boxes in which to\ndrop \\\\(n\\\\) balls. We can count the number of configurations by counting the\nnumber of ways of separating \\\\(n\\\\) balls into the boxes, i.e. by writing\ndown \\\\(n\\\\) times an `o` (for the balls) and \\\\(n-1\\\\) times a separator `|`\nbetween them. So we have \\\\(2n-1\\\\) positions to fill, for which we must\nchoose either `o` (a ball) or `|` (a separator). For \\\\(n=3\\\\), a possible\nplacement would be `oo||o`, which corresponds to \\\\(\\mathbf{k} = (2,0,1)\\\\).\nIn general, this number is \\\\({2n-1} \\choose {n-1}\\\\), and thus the answers\nfor \\\\(n=3\\\\) and \\\\(15\\\\) are:\n\n    \n    \n    c(N3 = choose(5, 3), N15 = choose(29, 15))__\n    \n    \n          N3      N15 \n          10 77558760 \n\n__\n\nQuestion 4.15\n\nWhat are the two types of error that can occur when using the bootstrap as it\nis implemented in the\n**[bootstrap](https://cran.r-project.org/web/packages/bootstrap/)** package?\nWhich parameter can you modify to improve one of them?\n\n__\n\nSolution\n\n__\n\nMonte Carlo simulations of subsets of the data by resampling randomly\napproximate the exhaustive bootstrap ([Diaconis and Holmes\n1994](16-chap.html#ref-Diaconis1994)). Increasing the size of `nboot` argument\nin the `bootstrap` function reduces the Monte Carlo error, however, the\nexhaustive bootstrap is still not exact: we are still using an approximate\ndistribution function, that of the data instead of the true distribution. If\nthe sample size is small or the original sample biased, the approximation can\nstill be quite poor, no matter how large we choose `nboot`.\n\n## 4.4 Infinite mixtures\n\nSometimes mixtures can be useful even if we don’t aim to assign a label to\neach observation or, to put it differently, if we allow as many `labels’ as\nthere are observations. If the number of mixture components is as big as (or\nbigger than) the number of observations, we say we have an **infinite\nmixture**. Let’s look at some examples.\n\n### 4.4.1 Infinite mixture of normals\n\n[![](imgs/LaplacePortrait_web.png)](imgs/LaplacePortrait_web.png \"Figure 4.15:\nLaplace knew already that the probability density\nf_X\\(y\\)=\\\\frac{1}{2\\\\phi}\\\\exp\\\\left\\(-\\\\frac{|y-\\\\theta|}{\\\\phi}\\\\right\\),\\\\qquad\\\\phi>0\nhas the median as its location parameter \\\\theta and the median absolute\ndeviation \\(MAD\\) as its scale parameter \\\\phi.\")\n\nFigure 4.15: Laplace knew already that the probability density\n\\\\[f_X(y)=\\frac{1}{2\\phi}\\exp\\left(-\\frac{|y-\\theta|}{\\phi}\\right),\\qquad\\phi>0\\\\]\nhas the median as its location parameter \\\\(\\theta\\\\) and the median absolute\ndeviation (MAD) as its scale parameter \\\\(\\phi\\\\).\n\nConsider the following two-level data generating scheme:\n\n**Level 1** Create a sample of `W`s from an exponential distribution.\n\n    \n    \n    w = rexp(10000, rate = 1)__\n\n**Level 2** The \\\\(w\\\\)s serve as the variances of normal variables with mean\n\\\\(\\mu\\\\) generated using `rnorm`.\n\n    \n    \n    mu  = 0.3\n    lps = rnorm(length(w), mean = mu, sd = sqrt(w))\n    ggplot(data.frame(lps), aes(x = lps)) +\n      geom_histogram(fill = \"purple\", binwidth = 0.1)__\n\n[![](04-chap_files/figure-html/fig-\nLaplacedistribution-1.png)](04-chap_files/figure-html/fig-\nLaplacedistribution-1.png \"Figure 4.16: Data sampled from a Laplace\ndistribution.\")\n\nFigure 4.16: Data sampled from a Laplace distribution.\n\nThis turns out to be a rather useful distribution. It has well-understood\nproperties and is named after Laplace, who proved that the median is a good\nestimator of its location parameter \\\\(\\theta\\\\) and that the median absolute\ndeviation can be used to estimate its scale parameter \\\\(\\phi\\\\). From the\nformula in the caption of Figure 4.15 we see that the \\\\(L_1\\\\) distance\n(absolute value of the difference) holds a similar position in the Laplace\ndensity as the \\\\(L_2\\\\) (square of the difference) does for the normal\ndensity.\n\nConversely, in Bayesian regression4, having a Laplace distribution as a prior\non the coefficients amounts to an \\\\(L_1\\\\) penalty, called the _lasso_\n([Tibshirani 1996](16-chap.html#ref-Tibshirani1996)), while a normal\ndistribution as a prior leads to an \\\\(L_2\\\\) penalty, called ridge\nregression.\n\n4 Don’t worry if you are not familiar with this, in that case just skip this\nsentence.\n\n__\n\nQuestion 4.16\n\nWrite a random variable whose distribution is the symmetric Laplace as a\nfunction of normal and exponential random variables.\n\n__\n\nSolution\n\n__\n\nWe can write the hierarchical model with variances generated as exponential\nvariables, \\\\(W\\\\), as:\n\n\\\\[ X = \\sqrt{W} \\cdot Z, \\qquad W \\sim Exp(1), \\qquad Z \\sim N(0,1).\n\\tag{4.13}\\\\]\n\n#### Asymmetric Laplace\n\nIn the Laplace distribution, the variances of the normal components depend on\n\\\\(W\\\\), while the means are unaffected. A useful extension adds another\nparameter \\\\(\\theta\\\\) that controls the locations or centers of the\ncomponents. We generate data `alps` from a hierarchical model with \\\\(W\\\\) an\nexponential variable; the output shown in Figure 4.17 is a histogram of normal\n\\\\(N(\\theta+w\\mu,\\sigma w)\\\\) random numbers, where the \\\\(w\\\\)’s themselves\nwere randomly generated from an exponential distribution with mean \\\\(1\\\\) as\nshown in the code:\n\n    \n    \n    mu = 0.3; sigma = 0.4; theta = -1\n    w  = rexp(10000, 1)\n    alps = rnorm(length(w), theta + mu * w, sigma * sqrt(w))\n    ggplot(tibble(alps), aes(x = alps)) +\n      geom_histogram(fill = \"purple\", binwidth = 0.1)__\n\n[![](04-chap_files/figure-html/fig-\nALaplacedistribution-1.png)](04-chap_files/figure-html/fig-\nALaplacedistribution-1.png \"Figure 4.17: Histogram of data generated from an\nasymmetric Laplace distribution – a scale mixture of many normals whose means\nand variances are dependent. We write X \\\\sim AL\\(\\\\theta, \\\\mu, \\\\sigma\\).\")\n\nFigure 4.17: Histogram of data generated from an asymmetric Laplace\ndistribution – a scale mixture of many normals whose means and variances are\ndependent. We write \\\\(X \\sim AL(\\theta, \\mu, \\sigma)\\\\).\n\nSuch hierarchical mixture distributions, where every instance of the data has\nits own mean and variance, are useful models in many biological settings.\nExamples are shown in Figure 4.18.\n\n[![](imgs/LaplaceMixturePromoterLengths.png)](imgs/LaplaceMixturePromoterLengths.png\n\"Figure 4.18 \\(a\\): The lengths of the promoters shorter than 2000bp from\nSaccharomyces cerevisiae as studied by @Kristiansson2009.\")\n\n(a) The lengths of the promoters shorter than 2000bp from Saccharomyces\ncerevisiae as studied by Kristiansson et al. ([2009](16-chap.html#ref-\nKristiansson2009)).\n\n![](imgs/tcellhist.png): The log-\nratios of microarray gene expression measurements for 20,000 genes\n\\[@Purdom2005\\].\")\n\n(b) The log-ratios of microarray gene expression measurements for 20,000 genes\n([Purdom and Holmes 2005](16-chap.html#ref-Purdom2005)).\n\nFigure 4.18: Histogram of real data. Both distributions can be modeled by\nasymmetric Laplace distributions.\n\n__\n\nQuestion 4.17\n\nLooking at the log-ratio of gene expression values from a microarray, one gets\na distribution as shown on the right of Figure 4.18. How would one explain\nthat the data have a histogram of this form?\n\nThe Laplace distribution is an example of where the consideration of the\ngenerative process indicates how the variance and mean are linked. The\nexpectation value and variance of an asymmetric Laplace distribution\n\\\\(AL(\\theta, \\mu, \\sigma)\\\\) are\n\n\\\\[ E(X)=\\theta+\\mu\\quad\\quad\\text{and}\\quad\\quad\\text{var}(X)=\\sigma^2+\\mu^2.\n\\tag{4.14}\\\\]\n\nNote the variance is dependent on the mean, unless \\\\(\\mu = 0\\\\) (the case of\nthe symmetric Laplace Distribution). This is the feature of the distribution\nthat makes it useful. Having a mean-variance dependence is very common for\nphysical measurements, be they microarray fluorescence intensities, peak\nheights from a mass spectrometer, or reads counts from high-throughput\nsequencing, as we’ll see in the next section.\n\n### 4.4.2 Infinite mixtures of Poisson variables.\n\n[![](imgs/three-worlds_web.jpg)](imgs/three-worlds_web.jpg \"Figure 4.19: How\nto count the fish in a lake? MC Escher.\")\n\nFigure 4.19: How to count the fish in a lake? MC Escher.\n\nA similar two-level hierarchical model is often also needed to model real-\nworld count data. At the lower level, simple Poisson and binomial\ndistributions serve as the building blocks, but their parameters may depend on\nsome underlying (latent) process. In ecology, for instance, we might be\ninterested in variations of fish species in all the lakes in a region. We\nsample the fish species in each lake to estimate their true abundances, and\nthat could be modeled by a Poisson. But the true abundances will vary from\nlake to lake. And if we want to see whether, for instance, changes in climate\nor altitude play a role, we need to disentangle such systematic effects from\nrandom lake-to-lake variation. The different Poisson rate parameters\n\\\\(\\lambda\\\\) can be modeled as coming from a distribution of rates. Such a\nhierarchical model also enables us to add supplementary steps in the\nhierarchy, for instance we could be interested in many different types of\nfish, model altitude and other environmental factors separately, etc.\n\nFurther examples of sampling schemes that are well modeled by mixtures of\nPoisson variables include applications of high-throughput sequencing, such as\nRNA-Seq, which we will cover in detail in [Chapter 8](08-chap.html), or 16S\nrRNA-Seq data used in microbial ecology.\n\n### 4.4.3 Gamma distribution: two parameters (shape and scale)\n\nNow we are getting to know a new distribution that we haven’t seen before. The\ngamma distribution is an extension of the (one-parameter) exponential\ndistribution, but it has two parameters, which makes it more flexible. It is\noften useful as a building block for the upper level of a hierarchical model.\nThe gamma distribution is positive-valued and continuous. While the density of\nthe exponential has its maximum at zero and then simply decreases towards 0 as\nthe value goes to infinity, the density of the gamma distribution has its\nmaximum at some finite value. Let’s explore it by simulation examples. The\nhistograms in Figure 4.20 were generated by the following lines of code:\n\n    \n    \n    ggplot(tibble(x = rgamma(10000, shape = 2, rate = 1/3)),\n       aes(x = x)) + geom_histogram(bins = 100, fill= \"purple\")\n    ggplot(tibble(x = rgamma(10000, shape = 10, rate = 3/2)),\n       aes(x = x)) + geom_histogram(bins = 100, fill= \"purple\")__\n\n[![](04-chap_files/figure-html/fig-gammahist1-1.png)](04-chap_files/figure-\nhtml/fig-gammahist1-1.png \"Figure 4.20 \\(a\\): gamma\\(2,\\\\frac{1}{3}\\)\")\n\n(a) gamma\\\\((2,\\frac{1}{3})\\\\)\n\n[![](04-chap_files/figure-html/fig-gammahist1-2.png)](04-chap_files/figure-\nhtml/fig-gammahist1-2.png \"Figure 4.20 \\(b\\): gamma\\(10,\\\\frac{3}{2}\\)\")\n\n(b) gamma\\\\((10,\\frac{3}{2})\\\\)\n\nFigure 4.20: Histograms of random samples of gamma distributions. The gamma is\na flexible two parameter distribution: [see\nWikipedia](http://en.wikipedia.org/wiki/Gamma_distribution).\n\n#### Gamma–Poisson mixture: a hierarchical model\n\n  1. Generate a set of parameters: \\\\(\\lambda_1,\\lambda_2,...\\\\) from a gamma distribution.\n\n  2. Use these to generate a set of Poisson(\\\\(\\lambda_i\\\\)) random variables, one for each \\\\(\\lambda_1\\\\).\n\n    \n    \n    lambda = rgamma(10000, shape = 10, rate = 3/2)\n    gp = rpois(length(lambda), lambda = lambda)\n    ggplot(tibble(x = gp), aes(x = x)) +\n      geom_histogram(bins = 100, fill= \"purple\")__\n\n[![](04-chap_files/figure-html/fig-\ngeneratepoissongamma-1.png)](04-chap_files/figure-html/fig-\ngeneratepoissongamma-1.png \"Figure 4.21: Histogram of gp, generated via a\ngamma-Poisson hierachical model.\")\n\nFigure 4.21: Histogram of `gp`, generated via a gamma-Poisson hierachical\nmodel.\n\nThe resulting values are said to come from a gamma–Poisson mixture. Figure\n4.21 shows the histogram of `gp`.\n\n__\n\nQuestion 4.18\n\n  1. Are the values generated from such a gamma–Poisson mixture continuous or discrete ?  \n\n  2. What is another name for this distribution? Hint: Try the different distributions provided by the `goodfit` function from the **[vcd](https://cran.r-project.org/web/packages/vcd/)** package.\n\n__\n\nSolution\n\n__\n\n    \n    \n    library(\"vcd\")\n    ofit = goodfit(gp, \"nbinomial\")\n    plot(ofit, xlab = \"\")\n    ofit$par __\n    \n    \n    $size\n    [1] 9.911829\n    \n    $prob\n    [1] 0.5963857\n\n[![](04-chap_files/figure-html/fig-goofy-1.png)](04-chap_files/figure-\nhtml/fig-goofy-1.png \"Figure 4.22: Goodness of fit plot. The rootogram shows\nthe theoretical probabilities of the gamma-Poisson distribution \\(a.k.a.\nnegative binomial\\) as red dots and the square roots of the observed\nfrequencies as the height of the rectangular bars. The bars all end close to\nthe horizontal axis, which indicates a good fit to the negative binomial\ndistribution.\")\n\nFigure 4.22: Goodness of fit plot. The **rootogram** shows the theoretical\nprobabilities of the gamma-Poisson distribution (a.k.a. negative binomial) as\nred dots and the square roots of the observed frequencies as the height of the\nrectangular bars. The bars all end close to the horizontal axis, which\nindicates a good fit to the negative binomial distribution.\n\nIn R, and in some other places, the gamma-Poisson distribution travels under\nthe alias name of **negative binomial distribution**. The two names are\nsynonyms; the second one alludes to the fact that Equation 4.15 bears some\nformal similarities to the probabilities of a binomial distribution. The first\nname, gamma–Poisson distribution, is more indicative of its generating\nmechanism, and that’s what we will use in the rest of the book. It is a\ndiscrete distribution, that means that it takes values only on the natural\nnumbers (in contrast to the gamma distribution, which covers the whole\npositive real axis). Its probability distribution is\n\n\\\\[ \\text{P}(K=k)=\\left(\\begin{array}{c}k+a-1\\\\\\k\\end{array}\\right) \\, p^a \\,\n(1-p)^k, \\tag{4.15}\\\\]\n\nwhich depends on the two parameters \\\\(a\\in\\mathbb{R}^+\\\\) and\n\\\\(p\\in[0,1]\\\\). Equivalently, the two parameters can be expressed by the mean\n\\\\(\\mu=pa/(1-p)\\\\) and a parameter called the **dispersion** \\\\(\\alpha=1/a\\\\).\nThe variance of the distribution depends on these parameters, and is\n\\\\(\\mu+\\alpha\\mu^2\\\\).\n\n[![](04-chap_files/figure-html/fig-mixtures-\ndgammapois-1.png)](04-chap_files/figure-html/fig-mixtures-dgammapois-1.png\n\"Figure 4.23: Visualization of the hierarchical model that generates the\ngamma-Poisson distribution. The top panel shows the density of a gamma\ndistribution with mean 50 \\(vertical black line\\) and variance 30. Assume that\nin one particular experimental replicate, the value 60 is realized. This is\nour latent variable. The observable outcome is distributed according to the\nPoisson distribution with that rate parameter, shown in the middle panel. In\none particular experiment the outcome may be, say, 55, indicated by the dashed\ngreen line. Overall, if we repeat these two subsequent random process many\ntimes, the outcomes will be distributed as shown in the bottom panel – the\ngamma-Poisson distribution.\")\n\nFigure 4.23: Visualization of the hierarchical model that generates the gamma-\nPoisson distribution. The top panel shows the density of a gamma distribution\nwith mean 50 (vertical black line) and variance 30. Assume that in one\nparticular experimental replicate, the value 60 is realized. This is our\nlatent variable. The observable outcome is distributed according to the\nPoisson distribution with that rate parameter, shown in the middle panel. In\none particular experiment the outcome may be, say, 55, indicated by the dashed\ngreen line. Overall, if we repeat these two subsequent random process many\ntimes, the outcomes will be distributed as shown in the bottom panel – the\ngamma-Poisson distribution.\n\n__\n\nQuestion 4.19\n\nIf you are more interested in analytical derivations than illustrative\nsimulations, try writing out the mathematical derivation of the gamma-Poisson\nprobability distribution.\n\n__\n\nSolution\n\n__\n\nRecall that the final distribution is the result of a two step process:\n\n  1. Generate a \\\\(\\text{gamma}(a,b)\\\\) distributed number, call it \\\\(x\\\\), from the density\n\n\\\\[ f_\\Gamma(x, a, b)=\\frac{b^a}{\\Gamma(a)}\\,x^{a-1}\\,e^{-b x}, \\tag{4.16}\\\\]\n\nwhere \\\\(\\Gamma\\\\) is the so-called \\\\(\\Gamma\\\\)-function,\n\\\\(\\Gamma(a)=\\int_0^\\infty x^{a-1}\\,e^{-x}\\,\\text{d}x\\\\) (not to be confused\nwith the gamma distribution, even though there is this incidental relation).\n\n  2. Generate a number \\\\(k\\\\) from the Poisson distribution with rate \\\\(x\\\\). The probability distribution is\n\n\\\\[ f_{\\text{Pois}}(k, \\lambda=x)=\\frac{x^{k}e^{-x}}{k!} \\\\]\n\nIf \\\\(x\\\\) only took on a finite set of values, we could solve the problem\nsimply by summing over all the possible cases, each weighted by their\nprobability according to \\\\(f_\\Gamma\\\\). But \\\\(x\\\\) is continuous, so we have\nto write the sum out as an integral instead of a discrete sum. We call the\ndistribution of \\\\(K\\\\) the marginal. Its probability mass function is\n\n\\\\[ \\begin{aligned} P(K=k)&=&\\int_{x=0}^{\\infty} \\, f_{\\text{Pois}}(k,\n\\lambda=x)\\; f_\\Gamma(x, a, b) \\;dx\\\\\\ &=& \\int_{x=0}^{\\infty}\n\\frac{x^{k}e^{-x}}{k!}\\;\\frac{b^a}{\\Gamma(a)} x^{a-1}e^{-bx}\\; dx\n\\end{aligned} \\\\]\n\nCollect terms and move terms independent of \\\\(x\\\\) outside the integral\n\n\\\\[ P(K=k)=\\frac{b^a}{\\Gamma(a)\\,k!} \\int_{x=0}^{\\infty} x^{k+a-1}e^{-(b+1)x}\ndx \\\\]\n\nBecause we know the gamma density sums to one: \\\\(\\int_0^\\infty\nx^{k+a-1}e^{-(b+1)x} dx = \\frac{\\Gamma(k+a)}{(b+1)^{k+a}}\\\\)\n\n\\\\[ \\begin{aligned} P(K=k)\n&=&\\frac{\\Gamma(k+a)}{\\Gamma(a)\\Gamma(k+1)}\\frac{b^a}{(b+1)^{a}(b+1)^k}\n={k+a-1\\choose k}\\left(\\frac{b}{b+1}\\right)^a\\left(1-\\frac{b}{b+1}\\right)^k\n\\end{aligned} \\\\]\n\nwhere in the last line we used that \\\\(\\Gamma(v+1)=v!\\\\). This is the same as\nEquation (4.15), the gamma-Poisson with size parameter \\\\(a\\\\) and probability\n\\\\(p=\\frac{b}{b+1}\\\\).\n\n### 4.4.4 Variance stabilizing transformations\n\nA key issue we need to control when we analyse experimental data is how much\nvariability there is between repeated measurements of the same underlying true\nvalue, i.e., between replicates. This will determine whether and how well we\ncan see any true differences, i.e., between different conditions. Data that\narise through the type of hierarchical models we have studied in this chapter\noften turn out to have very heterogeneous variances, and this can be a\nchallenge. We will see how in such cases **variance-stabilizing\ntransformations** ([Anscombe 1948](16-chap.html#ref-Anscombe1948)) can help.\nLet’s start with a series of Poisson variables with rates from 10 to 100:\n\nNote how we construct the dataframe (or, more precisely, the _tibble_)\n`simdat`: the output of the `lapply` loop is a list of _tibble_ s, one for\neach value of `lam`. With the pipe operator `|>` we send it to the function\n`bind_rows` (from the\n**[dplyr](https://cran.r-project.org/web/packages/dplyr/)** package). The\nresult is a dataframe of all the list elements neatly stacked on top of each\nother.\n\n    \n    \n    simdat = lapply(seq(10, 100, by = 10), function(lam)\n        tibble(n = rpois(200, lambda = lam),\n               `sqrt(n)` = sqrt(n),\n           lambda = lam)) |>\n      bind_rows() |>\n      tidyr::pivot_longer(cols = !lambda)\n    ggplot(simdat, aes(x = factor(lambda), y = value)) + xlab(expression(lambda)) +\n      geom_violin() + facet_grid(rows = vars(name), scales = \"free\")__\n\n[![](04-chap_files/figure-html/fig-\nseriesofpoisson-1.png)](04-chap_files/figure-html/fig-seriesofpoisson-1.png\n\"Figure 4.24: Poisson distributed measurement data, for eight different\nchoices of the mean lambda. In the upper panel, the y-axis is proportional to\nthe data; in the lower panel, it is on a square-root scale. Note how the\ndistribution widths change in the first case, but less so in the second.\")\n\nFigure 4.24: Poisson distributed measurement data, for eight different choices\nof the mean `lambda`. In the upper panel, the \\\\(y\\\\)-axis is proportional to\nthe data; in the lower panel, it is on a square-root scale. Note how the\ndistribution widths change in the first case, but less so in the second.\n\nThe data that we see in the upper panel of Figure 4.24 are an example of what\nis called **heteroscedasticity** : the standard deviations (or, equivalently,\nthe variance) of our data is different in different regions of our data space.\nIn particular, it increases along the \\\\(x\\\\)-axis, with the mean. For the\nPoisson distribution, we indeed know that the standard deviation is the square\nroot of the mean; for other types of data, there may be other dependencies.\nThis can be a problem if we want to apply subsequent analysis techniques (for\ninstance, regression, or a statistical test) that are based on assuming that\nthe variances are the same. In Figure 4.24, the numbers of replicates for each\nvalue of lambda are quite large. In practice, this is not always the case.\nMoreover, the data are usually not explicitly stratified by a known mean as in\nour simulation, so the heteroskedasticity may be harder to see, even though it\nis there. However, as we see in the lower panel of Figure 4.24, if we simply\napply the square root transformation, then the transformed variables will have\napproximately the same variance. This works even if we do not know the\nunderlying mean for each observation, the square root transformation does not\nneed this information. We can verify this more quantitatively, as in the\nfollowing code, which shows the standard deviations of the sampled values `n`\nand `sqrt(n)` for the difference choices of `lambda`.\n\nThe standard deviation of the square root transformed values is consistently\naround 0.5, so we would use the transformation `2*sqrt(n)` to achieve unit\nvariance.\n\n    \n    \n    summarise(group_by(simdat, name, lambda), sd(value)) |> tidyr::pivot_wider(values_from = `sd(value)`)__\n    \n    \n    # A tibble: 10 × 3\n       lambda     n `sqrt(n)`\n        <dbl> <dbl>     <dbl>\n     1     10  2.95     0.478\n     2     20  4.19     0.470\n     3     30  5.62     0.521\n     4     40  5.99     0.473\n     5     50  7.69     0.546\n     6     60  7.59     0.492\n     7     70  8.69     0.520\n     8     80  8.99     0.505\n     9     90  9.44     0.498\n    10    100  9.84     0.495\n\nAnother example, now using the gamma-Poisson distribution, is shown in Figure\n4.25. We generate gamma-Poisson variables `u`5 and plot the 95% confidence\nintervals around the mean.\n\n5 To catch a greater range of values for the mean value `mu`, without creating\ntoo dense a sequence, we use a geometric series: \\\\(\\mu_{i+1} = 2\\mu_i\\\\).\n\n    \n    \n    muvalues = 2^seq(0, 10, by = 1)\n    simgp = lapply(muvalues, function(mu) {\n      u = rnbinom(n = 1e4, mu = mu, size = 4)\n      tibble(mean = mean(u), sd = sd(u),\n             lower = quantile(u, 0.025),\n             upper = quantile(u, 0.975),\n             mu = mu)\n      } ) |> bind_rows()\n    head(as.data.frame(simgp), 2)__\n    \n    \n        mean       sd lower upper mu\n    1 0.9965 1.106440     0     4  1\n    2 2.0233 1.748503     0     6  2\n    \n    \n    ggplot(simgp, aes(x = mu, y = mean, ymin = lower, ymax = upper)) +\n      geom_point() + geom_errorbar()__\n\n[![](04-chap_files/figure-html/fig-seriesofnb-1.png)](04-chap_files/figure-\nhtml/fig-seriesofnb-1.png \"Figure 4.25: gamma-Poisson distributed measurement\ndata, for a range of \\\\mu from 1 to 1024.\")\n\nFigure 4.25: gamma-Poisson distributed measurement data, for a range of\n\\\\(\\mu\\\\) from 1 to 1024.\n\n__\n\nQuestion 4.20\n\nHow can we find a transformation for these data that stabilizes the variance,\nsimilar to the square root function for the Poisson distributed data?\n\n__\n\nSolution\n\n__\n\nIf we divide the values that correspond to `mu[1]` (and which are centered\naround `simgp$mean[1]`) by their standard deviation `simgp$sd[1]`, the values\nthat correspond to `mu[2]` (and which are centered around `simgp$mean[2]`) by\ntheir standard deviation `simgp$sd[2]`, and so on, then the resulting values\nwill have, by construction, a standard deviation (and thus variance) of 1. And\nrather than defining 11 separate transformations, we can achieve our goal by\ndefining one single piecewise linear _and_ continuous function that has the\nappropriate slopes at the appropriate values.\n\n    \n    \n    simgp = mutate(simgp,\n      slopes = 1 / sd,\n      trsf   = cumsum(slopes * mean))\n    ggplot(simgp, aes(x = mean, y = trsf)) +\n      geom_point() + geom_line() + xlab(\"\")__\n\n[![](04-chap_files/figure-html/fig-pcwlin-1-1.png)](04-chap_files/figure-\nhtml/fig-pcwlin-1-1.png \"Figure 4.26: Piecewise linear function that\nstabilizes the variance of the data in Figure fig-seriesofnb.\")\n\nFigure 4.26: Piecewise linear function that stabilizes the variance of the\ndata in Figure 4.25.\n\nWe see in Figure 4.26 that this function has some resemblance to a square root\nfunction in particular at its lower end. At the upper end, it seems to look\nmore like a logarithm. The more mathematically inclined will see that an\nelegant extension of these numerical calculations can be done through a little\ncalculus known as the **delta method** , as follows.\n\nCall our transformation function \\\\(g\\\\), and assume it’s differentiable\n(that’s not a very strong assumption: pretty much any function that we might\nconsider reasonable here is differentiable). Also call our random variables\n\\\\(X_i\\\\), with means \\\\(\\mu_i\\\\) and variances \\\\(v_i\\\\), and we assume that\n\\\\(v_i\\\\) and \\\\(\\mu_i\\\\) are related by a functional relationship \\\\(v_i =\nv(\\mu_i)\\\\). Then, for values of \\\\(X_i\\\\) in the neighborhood of its mean\n\\\\(\\mu_i\\\\),\n\n\\\\[ g(X_i) = g(\\mu_i) + g'(\\mu_i) (X_i-\\mu_i) + ... \\tag{4.17}\\\\]\n\nwhere the dots stand for higher order terms that we can neglect. The variances\nof the transformed values are then\n\n\\\\[ \\begin{align} \\text{Var}(g(X_i)) &\\simeq g'(\\mu_i)^2\\\\\\ \\text{Var}(X_i) &=\ng'(\\mu_i)^2 \\, v(\\mu_i), \\end{align} \\tag{4.18}\\\\]\n\nwhere we have used the rules \\\\(\\text{Var}(X-c)=\\text{Var}(X)\\\\) and\n\\\\(\\text{Var}(cX)=c^2\\,\\text{Var}(X)\\\\) that hold whenever \\\\(c\\\\) is a\nconstant number. Requiring that this be constant leads to the differential\nequation\n\n\\\\[ g'(x) = \\frac{1}{\\sqrt{v(x)}}. \\tag{4.19}\\\\]\n\nFor a given mean-variance relationship \\\\(v(\\mu)\\\\), we can solve this for the\nfunction \\\\(g\\\\). Let’s check this for some simple cases:\n\n  * if \\\\(v(\\mu)=\\mu\\\\) (Poisson), we recover \\\\(g(x)=\\sqrt{x}\\\\), the square root transformation.\n\n  * If \\\\(v(\\mu)=\\alpha\\,\\mu^2\\\\), solving the differential equation 4.19 gives \\\\(g(x)=\\log(x)\\\\). This explains why the logarithm transformation is so popular in many data analysis applications: it acts as a variance stabilizing transformation whenever the data have a constant coefficient of variation, that is, when the standard deviation is proportional to the mean.\n\n__\n\nQuestion 4.21\n\nWhat is the variance-stabilizing transformation associated with \\\\(v(\\mu) =\n\\mu + \\alpha\\,\\mu^2\\\\)?\n\n__\n\nSolution\n\n__\n\nTo solve the differential equation 4.19 with this function \\\\(v(\\cdot)\\\\), we\nneed to compute the integral\n\n\\\\[\\label{eq:} \\int \\frac{dx}{\\sqrt{x + \\alpha x^2}}. \\tag{4.20}\\\\]\n\nA closed form expression can be looked up in a reference table such as\n([Bronštein and Semendjajew 1979](16-chap.html#ref-BronsteinSemendjajew)).\nThese authors provide the general solution\n\n\\\\[ \\int \\frac{dx}{\\sqrt{ax^2+bx+c}} = \\frac{1}{\\sqrt{a}}\n\\ln\\left(2\\sqrt{a(ax^2+bx+c)}+2ax+b\\right) + \\text{const.}, \\tag{4.21}\\\\]\n\ninto which we can plug in our special case \\\\(a=\\alpha\\\\), \\\\(b=1\\\\),\n\\\\(c=0\\\\), to obtain the variance-stabilizing transformation\n\n\\\\[ \\begin{align} g_\\alpha(x) &= \\frac{1}{2\\sqrt{\\alpha}}\n\\ln\\left(2\\sqrt{\\alpha x (\\alpha x+1)} + 2\\alpha x + 1\\right) \\\\\\ &=\n\\frac{1}{2\\sqrt{\\alpha}} {\\displaystyle \\operatorname {arcosh}} (2\\alpha\nx+1).\\\\\\ \\end{align} \\tag{4.22}\\\\]\n\nFor the second line in Equation 4.22, we used the identity \\\\({\\displaystyle\n\\operatorname {arcosh}}(z) = \\ln\\left(z+\\sqrt{z^2-1}\\right)\\\\). In the limit\nof \\\\(\\alpha\\to0\\\\), we can use the linear approximation\n\\\\(\\ln(1+\\varepsilon)=\\varepsilon+O(\\varepsilon^2)\\\\) to see that\n\\\\(g_0(x)=\\sqrt{x}\\\\). Note that if \\\\(g_\\alpha\\\\) is a variance-stabilizing\ntransformation, then so is \\\\(ug_\\alpha+v\\\\) for any pair of numbers \\\\(u\\\\)\nand \\\\(v\\\\), and we have used this freedom to insert an extra factor\n\\\\(\\frac{1}{2}\\\\) for reasons that become apparent in the following. You can\nverify that the function \\\\(g_\\alpha\\\\) from Equation 4.22 fulfills condition\n4.19 by computing its derivative, which is an elementary calculation. We can\nplot it:\n\n    \n    \n    f = function(x, a) \n      ifelse (a==0, \n        sqrt(x), \n        log(2*sqrt(a) * sqrt(x*(a*x+1)) + 2*a*x+1) / (2*sqrt(a)))\n    x  = seq(0, 24, by = 0.1)\n    df = lapply(c(0, 0.05*2^(0:5)), function(a) \n      tibble(x = x, a = a, y = f(x, a))) %>% bind_rows()\n    ggplot(df, aes(x = x, y = y, col = factor(a))) + \n      geom_line() + labs(col = expression(alpha))__\n\n[![](04-chap_files/figure-html/fig-\nplotvstgammapoisson-1-1.png)](04-chap_files/figure-html/fig-\nplotvstgammapoisson-1-1.png \"Figure 4.27: Graph of the function Equation eq-\nmixtures-vstgammapoisson for different choices of \\\\alpha.\")\n\nFigure 4.27: Graph of the function Equation 4.22 for different choices of\n\\\\(\\alpha\\\\).\n\nand empirically verify the equivalence of two terms in Equation 4.22:\n\n    \n    \n    f2 = function(x, a) ifelse (a==0, sqrt(x), acosh(2*a*x + 1) / (2*sqrt(a)))  \n    with(df, max(abs(f2(x,a) - y)))__\n    \n    \n    [1] 8.881784e-16\n\nAs we see in Figure 4.27, for small values of \\\\(x\\\\), \\\\(g_\\alpha(x)\\approx\n\\sqrt{x}\\\\) (independently of \\\\(\\alpha\\\\)), whereas for large values\n(\\\\(x\\to\\infty\\\\)) and \\\\(\\alpha>0\\\\), it behaves like a logarithm:\n\n\\\\[ \\begin{align} &\\frac{1}{2\\sqrt{\\alpha}}\\ln\\left(2\\sqrt{\\alpha\\left(\\alpha\nx^2+x\\right)}+2\\alpha x+1\\right)\\\\\\ \\approx&\\frac{1}{2\\sqrt{\\alpha}}\n\\ln\\left(2\\sqrt{\\alpha^2x^2}+2\\alpha x\\right)\\\\\\\n=&\\frac{1}{2\\sqrt{\\alpha}}\\ln\\left(4\\alpha x\\right)\\\\\\\n=&\\frac{1}{2\\sqrt{\\alpha}}\\ln x+\\text{const.} \\end{align} \\\\]\n\nWe can verify this empirically by, say,\n\n    \n    \n      a = c(0.2, 0.5, 1)\n      f(1e6, a) __\n    \n    \n    [1] 15.196731 10.259171  7.600903\n    \n    \n      1/(2*sqrt(a)) * (log(1e6) + log(4*a))__\n    \n    \n    [1] 15.196728 10.259170  7.600902\n\n## 4.5 Summary of this chapter\n\nWe have given motivating examples and ways of using mixtures to model\nbiological data. We saw how the EM algorithm is an interesting example of\nfitting a difficult-to-estimate probabilistic model to data by iterating\nbetween partial, simpler problems.\n\n#### Finite mixture models\n\nWe have seen how to model mixtures of two or more normal distributions with\ndifferent means and variances. We have seen how to decompose a given sample of\ndata from such a mixture, even without knowing the latent variable, using the\nEM algorithm. The EM approach requires that we know the parametric form of the\ndistributions and the number of components. In [Chapter 5](05-chap.html), we\nwill see how we can find groupings in data even without relying on such\ninformation – this is then called clustering. We can keep in mind that there\nis a strong conceptual relationship between clustering and mixture modeling.\n\n#### Common infinite mixture models\n\nInfinite mixture models are good for constructing new distributions (such as\nthe gamma-Poisson or the Laplace) out of more basic ones (such as binomial,\nnormal, Poisson). Common examples are\n\n  * mixtures of normals (often with a hierarchical model on the means and the variances);\n\n  * beta-binomial mixtures – where the probability \\\\(p\\\\) in the binomial is generated according to a \\\\(\\text{beta}(a, b)\\\\) distribution;\n\n  * gamma-Poisson for read counts (see [Chapter 8](08-chap.html));\n\n  * gamma-exponential for PCR.\n\n#### Applications\n\nMixture models are useful whenever there are several layers of experimental\nvariability. For instance, at the lowest layer, our measurement precision may\nbe limited by basic physical detection limits, and these may be modeled by a\nPoisson distribution in the case of a counting-based assay, or a normal\ndistribution in the case of the continuous measurement. On top of there may be\none (or more) layers of instrument-to-instrument variation, variation in the\nreagents, operator variaton etc.\n\nMixture models reflect that there is often heterogeneous amounts of\nvariability (variances) in the data. In such cases, suitable data\ntransformations, i.e., variance stabilizing transformations, are necessary\nbefore subsequent visualization or analysis. We’ll study in depth an example\nfor RNA-Seq in [Chapter 8](08-chap.html), and this also proves useful in the\nnormalization of next generation reads in microbial ecology ([McMurdie and\nHolmes 2014](16-chap.html#ref-mcmurdie2014)).\n\nAnother important application of mixture modeling is the two-component model\nin multiple testing – we will come back to this in [Chapter 6](06-chap.html).\n\n#### The ECDF and bootstrapping\n\nWe saw that by using the observed sample as a mixture we could generate many\nsimulated samples that inform us about the sampling distribution of an\nestimate. This method is called the bootstrap and we will return to it several\ntimes, as it provides a way of evaluating estimates even when a closed form\nexpression is not available (we say it is non-parametric).\n\n## 4.6 Further reading\n\nA useful book-long treatment of finite mixture models is by McLachlan and Peel\n([2004](16-chap.html#ref-mclachlan2004)); for the EM algorithm, see also the\nbook by McLachlan and Krishnan ([2007](16-chap.html#ref-\nmclachlan2007algorithm)). A recent book that presents all EM type algorithms\nwithin the Majorize-Minimization (MM) framework is by Lange\n([2016](16-chap.html#ref-lange2016mm)).\n\nThere are in fact mathematical reasons why many natural phenomena can be seen\nas mixtures: this occurs when the observed events are exchangeable (the order\nin which they occur doesn’t matter). The theory underlying this is quite\nmathematical, a good way to start is to look at the Wikipedia entry and the\npaper by Diaconis and Freedman ([1980](16-chap.html#ref-diaconis1980finite)).\n\nIn particular, we use mixtures for high-throughput data. You will see examples\nin Chapters [8](08-chap.html) and [11](11-chap.html).\n\nThe bootstrap can be used in many situations and is a very useful tool to know\nabout, a friendly treatment is given in ([B. Efron and Tibshirani\n1993](16-chap.html#ref-efront)).\n\nA historically interesting paper is the original article on variance\nstabilization by Anscombe ([1948](16-chap.html#ref-Anscombe1948)), who\nproposed ways of making variance stabilizing transformations for Poisson and\ngamma-Poisson random variables. Variance stabilization is explained using the\ndelta method in many standard texts in theoretical statistics, e.g., those by\nRice ([2006, chap. 6](16-chap.html#ref-Rice:2007)) and Kéry and Royle ([2015,\n35](16-chap.html#ref-Kery2015)).\n\nKéry and Royle ([2015](16-chap.html#ref-Kery2015)) provide a nice exploration\nof using R to build hierarchical models for abundance estimation in niche and\nspatial ecology.\n\n## 4.7 Exercises\n\n__\n\nExercise 4.1\n\n**The EM algorithm step by step.** As an example dataset, we use the values in\nthe file `Myst.rds`. As always, it is a good idea to first visualize the data.\nThe histogram is shown in Figure 4.28. We are going to model these data as a\nmixture of two normal distributions with unknown means and standard\ndeviations, and unknown mixture fraction. We’ll call the two components A and\nB.\n\n    \n    \n    mx = readRDS(\"../data/Myst.rds\")$yvar\n    str(mx)__\n    \n    \n     num [1:1800] 0.3038 0.0596 -0.0204 0.1849 0.2842 ...\n    \n    \n    ggplot(tibble(mx), aes(x = mx)) + geom_histogram(binwidth = 0.025)__\n\nWe start by randomly assigning the membership weights for each of the values\nin `mx` for each of the components\n\n    \n    \n    wA = runif(length(mx))\n    wB = 1 - wA __\n\nWe also need to set up some housekeeping variables: `iter` counts over the\niterations of the EM algorithm; `loglik` stores the current log-likelihood;\n`delta` stores the change in the log-likelihood from the previous iteration to\nthe current one. We also define the parameters `tolerance`, `miniter` and\n`maxiter` of the algorithm.\n\n    \n    \n    iter      = 0\n    loglik    = -Inf\n    delta     = +Inf\n    tolerance = 1e-12\n    miniter   = 50\n    maxiter   = 1000 __\n\nStudy the code below and answer the following questions:\n\n  1. Which lines correspond to the E-step, which to the M-step?\n\n  2. What is the role of `tolerance`, `miniter` and `maxiter`?\n\n  3. Compare the result of what we are doing here to the output of the `normalmixEM` function from the **[mixtools](https://cran.r-project.org/web/packages/mixtools/)** package.\n\n    \n    \n    while((delta > tolerance) && (iter <= maxiter) || (iter < miniter)) {\n      lambda = mean(wA)\n      muA = weighted.mean(mx, wA)\n      muB = weighted.mean(mx, wB)\n      sdA = sqrt(weighted.mean((mx - muA)^2, wA))\n      sdB = sqrt(weighted.mean((mx - muB)^2, wB))\n    \n      pA   =    lambda    * dnorm(mx, mean = muA, sd = sdA)\n      pB   = (1 - lambda) * dnorm(mx, mean = muB, sd = sdB)\n      ptot = pA + pB\n      wA   = pA / ptot\n      wB   = pB / ptot\n    \n      loglikOld = loglik\n      loglik = sum(log(pA + pB))\n      delta = abs(loglikOld - loglik)\n      iter = iter + 1\n    }\n    iter __\n    \n    \n    [1] 447\n    \n    \n    c(lambda, muA, muB, sdA, sdB)__\n    \n    \n    [1]  0.4756 -0.1694  0.1473  0.0983  0.1498\n\n[![](04-chap_files/figure-html/fig-\nEMillustrate-1-1.png)](04-chap_files/figure-html/fig-EMillustrate-1-1.png\n\"Figure 4.28: Histogram of mx, our example data for the EM algorithm.\")\n\nFigure 4.28: Histogram of `mx`, our example data for the EM algorithm.\n\n__\n\nSolution\n\n__\n\nThe first five lines in the `while` loop implement the _Maximization step_.\nGiven the current values of `wA` and `wB`, we estimate the parameters of the\nmixture model using the maximum-likelihood estimators: the mixture fraction\n`lambda` by the mean of `wA`, and the parameters of the two normal\ndistribution components (`muA`, `sdA`) and (`muB`, `sdB`) by the sample means\nand the sample standard deviations. To take into account the membership\nweights, we use the weighted mean (function `weighted.mean`) and standard\ndeviation.\n\nNext comes the _Expectation step_. For each of the elements in the data vector\n`mx`, we compute the probability densities `pA` and `pB` for the generative\ndistribution models A and B, using the normal density function `dnorm`,\nweighted by the mixture fractions `lambda` and `(1-lambda)`, respectively.\nFrom this, we compute the updated membership weights `wA` and `wB`, according\nto Equation 4.5.\n\nGiven the membership weights and the parameters, the logarithmic likelihood\n`loglik` is easily computed, and the `while` loop iterates these steps.\n\nThe termination criterion for the loop is based on `delta`, the change in the\nlikelihood. The loop can end if this becomes smaller than `tolerance`. This is\na simple way of checking whether the algorithm has converged. The additional\nconditions on `iter` make sure that at least `miniter` iterations are run, and\nthat the loop always stops after `maxiter` iterations. The latter is to make\nsure that the loop terminates in finite time no matter what. (“Professional”\nimplementations of such iterative algorithms typically work a bit harder to\ndecide what is the best time to stop.)\n\nFinally, let’s compare our estimates to those from the function `normalmixEM`\nfrom the **[mixtools](https://cran.r-project.org/web/packages/mixtools/)**\npackage.\n\n    \n    \n    gm = mixtools::normalmixEM(mx, k = 2)__\n    \n    \n    number of iterations= 215 \n    \n    \n    with(gm, c(lambda[1], mu, sigma))__\n    \n    \n    [1]  0.4757 -0.1694  0.1473  0.0983  0.1498\n\n__\n\nExercise 4.2\n\nWhy do we often consider the logarithm of the likelihood rather than the\nlikelihood? E.g., in the EM code above, why did we work with the probabilities\non the logarithmic scale?\n\n__\n\nSolution\n\n__\n\nLikelihoods often (whenever the data points are sampled independently) take\nthe form of a product. This is, for instance, the case in Equation 4.4.\nCalculating the derivative, for likelihood optimisation, would then require\napplication of the product rule. On the logarithmic scale, the product turns\ninto a sum, and the derivative of a sum is simply the sum of the derivatives\nof the individual summands.\n\nAn additional reason comes from the way computers implement arithmetic. They\ncommonly use a floating point representation of numbers with a finite number\nof bits. E.g., the IEEE 754-2008 standard uses 64 bits for a _double-\nprecision_ number: 1 bit for the sign, 52 for the mantissa (also called\nsignificand), 11 for the exponent. Multiplication between such numbers implies\naddition of the exponents, but the range of the exponent is only \\\\(0\\\\) to\n\\\\(2^{11}-1=2047\\\\). Even likelihoods that involve only a few hundred data\npoints can lead to arithmetic overflow or other problems with precision. On\nthe logarithmic scale, where the product is a sum, the workload tends to be\nbetter distributed between mantissa and exponent, and log-likelihoods even\nwith millions of data points to be handled with reasonable precision.\n\nSee also Gregory Gundersen’s post on the [Log-Sum-Exp Trick for normalizing\nvectors of log\nprobabilities](https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/).\n\n__\n\nExercise 4.3\n\nCompare the theoretical values of the gamma-Poisson distribution with\nparameters given by the estimates in `ofit$par` in Section 4.4.3 to the data\nused for the estimation using a QQ-plot.\n\n__\n\nExercise 4.4\n\n**Mixture modeling examples for regression**. The\n**[flexmix](https://cran.r-project.org/web/packages/flexmix/)** package\n([Grün, Scharl, and Leisch 2012](16-chap.html#ref-Grun2012)) enables us to\ncluster and fit regressions to the data at the same time. The standard M-step\n`FLXMRglm` of **[flexmix](https://cran.r-project.org/web/packages/flexmix/)**\nis an interface to R’s generalized linear modeling facilities (the `glm`\nfunction). Load the package and an example dataset.\n\n    \n    \n    library(\"flexmix\")\n    data(\"NPreg\")__\n\n  1. First, plot the data and try to guess how the points were generated.\n\n  2. Fit a two component mixture model using the commands\n\n    \n    \n    m1 = flexmix(yn ~ x + I(x^2), data = NPreg, k = 2)__\n\n  3. Look at the estimated parameters of the mixture components and make a truth table that cross-classifies true classes versus cluster memberships. What does the summary of the object `m1` show us?\n\n  4. Plot the data again, this time coloring each point according to its estimated class.\n\n__\n\nSolution\n\n__\n\n    \n    \n    ggplot(NPreg, aes(x = x, y = yn)) + geom_point()__\n\n[![](04-chap_files/figure-html/fig-npreg-1.png)](04-chap_files/figure-\nhtml/fig-npreg-1.png \"Figure 4.29: The points seem to come from two different\ngenerative processes, one is linear; the other quadratic.\")\n\nFigure 4.29: The points seem to come from two different generative processes,\none is linear; the other quadratic.\n\nThe components are:\n\n    \n    \n    modeltools::parameters(m1, component = 1)__\n    \n    \n                          Comp.1\n    coef.(Intercept) -0.20998685\n    coef.x            4.81807854\n    coef.I(x^2)       0.03613061\n    sigma             3.47665584\n    \n    \n    modeltools::parameters(m1, component = 2)__\n    \n    \n                         Comp.2\n    coef.(Intercept) 14.7167886\n    coef.x            9.8468507\n    coef.I(x^2)      -0.9683734\n    sigma             3.4795657\n\nThe parameter estimates of both components are close to the true values. A\ncross-tabulation of true classes and cluster memberships can be obtained by\n\n    \n    \n    table(NPreg$class, modeltools::clusters(m1))__\n    \n    \n       \n         1  2\n      1 95  5\n      2  5 95\n\nFor our example data, the ratios of both components are approximately 0.7,\nindicating the overlap of the classes at the cross-section of line and\nparabola.\n\n    \n    \n    summary(m1)__\n\nThe summary shows the estimated prior probabilities \\\\(\\hat\\pi_k\\\\), the\nnumber of observations assigned to the two clusters, the number of\nobservations where \\\\(p_{nk}>\\delta\\\\) (with a default of\n\\\\(\\delta=10^{-4}\\\\)), and the ratio of the latter two numbers. For well-\nseparated components, a large proportion of observations with non-vanishing\nposteriors \\\\(p_{nk}\\\\) should be assigned to their cluster, giving a ratio\nclose to 1.\n\n    \n    \n    NPreg = mutate(NPreg, gr = factor(class))\n    ggplot(NPreg, aes(x = x, y = yn, group = gr)) +\n       geom_point(aes(colour = gr, shape = gr)) +\n       scale_colour_hue(l = 40, c = 180)__\n\n[![](04-chap_files/figure-html/fig-npregC-1.png)](04-chap_files/figure-\nhtml/fig-npregC-1.png \"Figure 4.30: Regression example using flexmix with the\npoints colored according to their estimated class. You can see that at the\nintersection we have an `identifiability’ problem: we cannot distinguish\npoints that belong to the straight line from ones that belong to the\nparabole.\")\n\nFigure 4.30: Regression example using `flexmix` with the points colored\naccording to their estimated class. You can see that at the intersection we\nhave an `identifiability’ problem: we cannot distinguish points that belong to\nthe straight line from ones that belong to the parabole.\n\n__\n\nExercise 4.5\n\n**Other hierarchical noise models:**  \nFind two papers that explore the use of other infinite mixtures for modeling\nmolecular biology technological variation.\n\n__\n\nSolution\n\n__\n\nThe paper by Chen, Xie, and Story ([2011](16-chap.html#ref-Chen2011)) explores\nan exponential-Poisson model for modeling background noise in bead arrays.\nWills et al. ([2013](16-chap.html#ref-Wills2013)) compares several Poisson\nmixture models.\n\nAnscombe, Francis J. 1948. “The Transformation of Poisson, Binomial and\nNegative-Binomial Data.” _Biometrika_ , 246–54.\n\nBishop, Christopher M. 2006. _Pattern Recognition and Machine Learning_.\nSpringer.\n\nBronštein, Il’ja N., and Konstantin A Semendjajew. 1979. _Taschenbuch Der\nMathematik_. B.G. Teubner Verlagsgesellschaft, Leipzig; Verlag Nauka, Moscow.\n\nBulmer, Michael George. 2003. _Francis Galton: Pioneer of Heredity and\nBiometry_. JHU Press.\n\nChen, Min, Yang Xie, and Michael Story. 2011. “An Exponential-Gamma\nConvolution Model for Background Correction of Illumina BeadArray Data.”\n_Communications in Statistics-Theory and Methods_ 40 (17): 3055–69.\n\nDiaconis, Persi, and David Freedman. 1980. “Finite Exchangeable Sequences.”\n_The Annals of Probability_ , 745–64.\n\nDiaconis, Persi, and Susan Holmes. 1994. “Gray Codes for Randomization\nProcedures.” _Statistics and Computing_ 4 (4): 287–302.\n\nEfron, Bradley, and Robert J Tibshirani. 1994. _An Introduction to the\nBootstrap_. CRC press.\n\nEfron, B., and R. Tibshirani. 1993. _An Introduction to the Bootstrap_.\nChapman & Hall/CRC.\n\nGrün, Bettina, Theresa Scharl, and Friedrich Leisch. 2012. “Modelling Time\nCourse Gene Expression Data with Finite Mixtures of Linear Additive Models.”\n_Bioinformatics_ 28 (2): 222–28.\n<https://doi.org/10.1093/bioinformatics/btr653>.\n\nHoeting, Jennifer A, David Madigan, Adrian E Raftery, and Chris T Volinsky.\n1999. “Bayesian Model Averaging: A Tutorial.” _Statistical Science_ , 382–401.\n\nKéry, Marc, and J Andrew Royle. 2015. _Applied Hierarchical Modeling in\nEcology: Analysis of Distribution, Abundance and Species Richness in r and\nBUGS: Volume 1: Prelude and Static Models_. Academic Press.\n\nKristiansson, Erik, Michael Thorsen, Markus J Tamás, and Olle Nerman. 2009.\n“Evolutionary Forces Act on Promoter Length: Identification of Enriched Cis-\nRegulatory Elements.” _Molecular Biology and Evolution_ 26 (6): 1299–1307.\n\nKuan, Pei Fen, Dongjun Chung, Guangjin Pan, James A Thomson, Ron Stewart, and\nSündüz Keleş. 2011. “A Statistical Framework for the Analysis of ChIP-Seq\nData.” _Journal of the American Statistical Association_ 106 (495): 891–903.\n\nLange, Kenneth. 2016. _MM Optimization Algorithms_. SIAM.\n\nMcLachlan, Geoffrey, and Thriyambakam Krishnan. 2007. _The EM Algorithm and\nExtensions_. Vol. 382. John Wiley & Sons.\n\nMcLachlan, Geoffrey, and David Peel. 2004. _Finite Mixture Models_. John Wiley\n& Sons.\n\nMcMurdie, Paul J, and Susan Holmes. 2014. “Waste Not, Want Not: Why Rarefying\nMicrobiome Data Is Inadmissible.” _PLoS Computational Biology_ 10 (4):\ne1003531.\n\nPurdom, Elizabeth, and Susan P Holmes. 2005. “Error Distribution for Gene\nExpression Data.” _Statistical Applications in Genetics and Molecular Biology_\n4 (1).\n\nRice, John. 2006. _Mathematical Statistics and Data Analysis_. Cengage\nLearning.\n\nShalizi, Cosma. 2017. _Advanced Data Analysis from an Elementary Point of\nView_. Cambridge University Press.\n<https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf>.\n\nSlonim, Noam, Gurinder Singh Atwal, Gašper Tkačik, and William Bialek. 2005.\n“Information-Based Clustering.” _PNAS_ 102 (51): 18297–302.\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.”\n_Journal of the Royal Statistical Society. Series B (Methodological)_ ,\n267–88.\n\nWills, Quin F, Kenneth J Livak, Alex J Tipping, Tariq Enver, Andrew J Goldson,\nDarren W Sexton, and Chris Holmes. 2013. “Single-Cell Gene Expression Analysis\nReveals Genetic Associations Masked in Whole-Tissue Experiments.” _Nature\nBiotechnology_ 31 (8): 748–52.\n\nZeileis, Achim, Christian Kleiber, and Simon Jackman. 2008. “Regression Models\nfor Count Data in R.” _Journal of Statistical Software_ 27 (8).\n<http://www.jstatsoft.org/v27/i08/>.\n\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["msmb.css"],"toc":true,"output-file":"04-chap.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}