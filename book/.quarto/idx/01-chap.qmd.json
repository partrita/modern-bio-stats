{"title":"1.1 Goals for this chapter","markdown":{"headingText":"1.1 Goals for this chapter","containsRefs":false,"markdown":"![](imgs/Pile_ou_face.png)\n\nIn molecular biology, many situations involve counting events: how many codons\nuse a certain spelling, how many reads of DNA match a reference, how many CG\ndigrams are observed in a DNA sequence. These counts give us _discrete_\nvariables, as opposed to quantities such as mass and intensity that are\nmeasured on _continuous_ scales.\n\nIf we know the rules that the mechanisms under study follow, even if the\noutcomes are random, we can generate the probabilities of any events we are\ninterested in by computations and standard probability laws. This is a _top-\ndown_ approach based on deduction and our knowledge of how to manipulate\nprobabilities. In [Chapter 2](02-chap.html), you will see how to combine this\nwith data-driven (_bottom-up_) statistical modeling.\n\n\nIn this chapter we will:\n\n  * Learn how to obtain the probabilities of all possible outcomes from a given model and see how we can compare the theoretical frequencies with those observed in real data.\n\n  * Explore a complete example of how to use the Poisson distribution to analyse data on epitope detection.\n\n  * See how we can experiment with the most useful generative models for discrete data: Poisson, binomial, multinomial.\n\n  * Use the R functions for computing probabilities and counting rare events.\n\n  * Generate random numbers from specified distributions.\n\n## 1.2 A real example\n\nLet’s dive into an example where we have a probability model for the data\ngenerating process. Our model says that mutations along the genome of HIV\n(Human Immunodeficiency Virus) occur with a rate of \\\\(5 \\times 10^{-4}\\\\) per\nnucleotide per replication cycle. The rate is the same at each nucleotide\nposition, and mutations at one position happen independently of what happens\nat other positions1. The genome size of HIV is about \\\\(10^4=10,000\\\\)\nnucleotides, thus, after one cycle, the total number of mutations will follow\na **Poisson** distribution2 with rate \\\\(5 \\times 10^{-4} \\times 10^4 = 5\\\\).\nWhat does that tell us?\n\n1 In practice, and strictly speaking, complete and utter independence will\nrarely hold in reality, if you look close enough. Thus, what modellers usually\nmean with such assertions is that any possible correlations or dependencies\nare so weak and rare that ignoring them is a good enough approximation.\n\n2 We will give more details later about this type of probability distribution\n\nThis probability model predicts that the number of mutations over one\nreplication cycle will be close to 5, and that the variability of this\nestimate is \\\\(\\sqrt{5}\\\\) (the standard error). We now have baseline\nreference values for both the number of mutations we expect to see in a\ntypical HIV strain and its variability.\n\nIn fact, we can deduce even more detailed information. If we want to know how\noften 3 mutations could occur under the Poisson(5) model, we can use an R\nfunction to generate the probability of seeing \\\\(x=3\\\\) events, taking the\nvalue of the **rate parameter** of the Poisson distribution, called lambda\n(\\\\(\\lambda\\\\)), to be \\\\(5\\\\).  \n\nGreek letters such as \\\\(\\lambda\\\\) and \\\\(\\mu\\\\) often denote important\nparameters that characterize the probability distributions we use.\n\n    \n    \n    dpois(x = 3, lambda = 5)__\n    \n    \n    [1] 0.1403739\n\nThis says the chance of seeing exactly three events is around 0.14, or about 1\nin 7.\n\nIf we want to generate the probabilities of all values from 0 to 12, we do not\nneed to write a loop. We can simply set the first argument to be the\n**vector** of these 13 values, using R’s sequence operator, the colon “`:`”.\nWe can see the probabilities by plotting them (Figure 1.1). As with this\nfigure, most figures in the margins of this book are created by the code shown\nin the text.\n\n[![Note how the output from R is formatted: the first line begins with the\nfirst item in the vector, hence the \\[1\\], and the second line begins with the\n9th item, hence the \\[9\\]. This helps you keep track of elements in long\nvectors. The term vector is R parlance for an ordered list of elements of the\nsame type \\(in this case, numbers\\).](imgs/devil.png)](imgs/devil.png \"Note\nhow the output from R is formatted: the first line begins with the first item\nin the vector, hence the \\[1\\], and the second line begins with the 9th item,\nhence the \\[9\\]. This helps you keep track of elements in long vectors. The\nterm vector is R parlance for an ordered list of elements of the same type\n\\(in this case, numbers\\).\")\n\nNote how the output from R is formatted: the first line begins with the first\nitem in the vector, hence the [1], and the second line begins with the 9th\nitem, hence the [9]. This helps you keep track of elements in long vectors.\nThe term _vector_ is R parlance for an ordered list of elements of the same\ntype (in this case, numbers).\n\n    \n    \n    0:12 __\n    \n    \n     [1]  0  1  2  3  4  5  6  7  8  9 10 11 12\n    \n    \n    dpois(x = 0:12, lambda = 5)__\n    \n    \n     [1] 0.0067 0.0337 0.0842 0.1404 0.1755 0.1755 0.1462 0.1044 0.0653 0.0363\n    [11] 0.0181 0.0082 0.0034\n    \n    \n    barplot(dpois(0:12, 5), names.arg = 0:12, col = \"red\")__\n\n[![](01-chap_files/figure-html/fig-Poisson5-1.png)](01-chap_files/figure-\nhtml/fig-Poisson5-1.png \"Figure 1.1: Probabilities of seeing 0,1,2,…,12\nmutations, as modeled by the Poisson\\(5\\) distribution. The plot shows that we\nwill often see 4 or 5 mutations but rarely as many as 12. The distribution\ncontinues to higher numbers \\(13,...\\), but the probabilities will be\nsuccessively smaller, and here we don’t visualize them.\")\n\nFigure 1.1: Probabilities of seeing 0,1,2,…,12 mutations, as modeled by the\nPoisson(5) distribution. The plot shows that we will often see 4 or 5\nmutations but rarely as many as 12. The distribution continues to higher\nnumbers (\\\\(13,...\\\\)), but the probabilities will be successively smaller,\nand here we don’t visualize them.\n\nMathematical theory tells us that the Poisson probability of seeing the value\n\\\\(x\\\\) is given by the formula \\\\(e^{-\\lambda} \\lambda^x / x!\\\\). In this\nbook, we’ll discuss theory from time to time, but give preference to\ndisplaying concrete numeric examples and visualizations like Figure 1.1.\n\nThe Poisson distribution is a good model for rare events such as mutations.\nOther useful probability models for **discrete events** are the Bernoulli,\nbinomial and multinomial distributions. We will explore these models in this\nchapter.\n\n## 1.3 Using discrete probability models\n\n[![Think of a categorical variable as having different alternative values.\nThese are the levels, similar to the different alternatives at a gene locus:\nalleles.](imgs/devil.png)](imgs/devil.png \"Think of a categorical variable as\nhaving different alternative values. These are the levels, similar to the\ndifferent alternatives at a gene locus: alleles.\")\n\nThink of a categorical variable as having different alternative values. These\nare the levels, similar to the different alternatives at a gene locus:\n_alleles_.\n\nA point mutation can either occur or not; it is a binary event. The two\npossible outcomes (yes, no) are called the **levels** of the categorical\nvariable.\n\nNot all events are binary. For example, the genotypes in a diploid organism\ncan take three levels (AA, Aa, aa).\n\nSometimes the number of levels in a categorical variable is very large;\nexamples include the number of different types of bacteria in a biological\nsample (hundreds or thousands) and the number of codons formed of 3\nnucleotides (64 levels).\n\nWhen we measure a categorical variable on a sample, we often want to tally the\nfrequencies of the different levels in a vector of counts. R has a special\nencoding for categorical variables and calls them **factors** 3. Here we\ncapture the different blood genotypes for 19 subjects in a vector which we\ntabulate.\n\n3 R makes sure that the factor variable will accept no other, “illegal”\nvalues, and this is useful for keeping your calculations safe.\n\n[![c\\(\\) is one of the most basic functions. It collates elements of the same\ntype into a vector. In the code shown here, the elements of genotype are\ncharacter strings.](imgs/devil.png)](imgs/devil.png \"c\\(\\) is one of the most\nbasic functions. It collates elements of the same type into a vector. In the\ncode shown here, the elements of genotype are character strings.\")\n\n`c()` is one of the most basic functions. It collates elements of the same\ntype into a vector. In the code shown here, the elements of `genotype` are\ncharacter strings.\n\n    \n    \n    genotype = c(\"AA\",\"AO\",\"BB\",\"AO\",\"OO\",\"AO\",\"AA\",\"BO\",\"BO\",\n                 \"AO\",\"BB\",\"AO\",\"BO\",\"AB\",\"OO\",\"AB\",\"BB\",\"AO\",\"AO\")\n    table(genotype)__\n    \n    \n    genotype\n    AA AB AO BB BO OO \n     2  2  7  3  3  2 \n\nOn creating a _factor_ , R automatically detects the levels. You can access\nthe levels with the `levels` function.\n\n[![It is not obvious from the output of the table function that the input was\na factor; however if there had been another level with no instances, the table\nwould also have contained that level, with a zero\ncount.](imgs/devil.png)](imgs/devil.png \"It is not obvious from the output of\nthe table function that the input was a factor; however if there had been\nanother level with no instances, the table would also have contained that\nlevel, with a zero count.\")\n\nIt is not obvious from the output of the `table` function that the input was a\nfactor; however if there had been another level with no instances, the table\nwould also have contained that level, with a zero count.\n\n    \n    \n    genotypeF = factor(genotype)\n    levels(genotypeF)__\n    \n    \n    [1] \"AA\" \"AB\" \"AO\" \"BB\" \"BO\" \"OO\"\n    \n    \n    table(genotypeF)__\n    \n    \n    genotypeF\n    AA AB AO BB BO OO \n     2  2  7  3  3  2 \n\n__\n\nQuestion 1.1\n\nWhat if you want to create a _factor_ that has some levels not yet in your\ndata?\n\n__\n\nSolution\n\n__\n\nLook at the manual page of the `factor` function.\n\nIf the order in which the data are observed doesn’t matter, we call the random\nvariable **exchangeable**. In that case, all the information available in the\nfactor is summarized by the counts of the factor levels. We then say that the\nvector of frequencies is **sufficient** to capture all the relevant\ninformation in the data, thus providing an effective way of compressing the\ndata.\n\n### 1.3.1 Bernoulli trials\n\n[![](imgs/BallsinBoxes2.png)](imgs/BallsinBoxes2.png \"Figure 1.2: Two possible\nevents with unequal probabilities. We model this by a Bernoulli distribution\nwith probability parameter p=2/3.\")\n\nFigure 1.2: Two possible events with unequal probabilities. We model this by a\nBernoulli distribution with probability parameter \\\\(p=2/3\\\\).\n\nTossing a coin has two possible outcomes. This simple experiment, called a\nBernoulli trial, is modeled using a so-called Bernoulli random variable.\nUnderstanding this building block will take you surprisingly far. We can use\nit to build more complex models.\n\nLet’s try a few experiments to see what some of these random variables look\nlike. We use special R functions tailored to generate outcomes for each type\nof distribution. They all start with the letter `r`, followed by a\nspecification of the model, here `rbinom`, where `binom` is the abbreviation\nused for binomial.\n\nSuppose we want to simulate a sequence of 15 fair coin tosses. To get the\noutcome of 15 Bernoulli trials with a probability of success equal to 0.5 (a\nfair coin), we write\n\n    \n    \n    rbinom(15, prob = 0.5, size = 1)__\n    \n    \n     [1] 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0\n\nWe use the `rbinom` function with a specific set of **parameters** 4: the\nfirst parameter is the number of trials we want to observe; here we chose 15.\nWe designate by `prob` the probability of success. By `size=1` we declare that\neach individual trial consists of just one single coin toss.\n\n4 For R functions, parameters are also called **argument** s.\n\n__\n\nQuestion 1.2\n\nRepeat this function call a number of times. Why isn’t the answer always the\nsame?\n\nSuccess and failure can have unequal probabilities in a Bernoulli trial, as\nlong as the probabilities sum to one5. To simulate twelve trials of throwing a\nball into the two boxes as shown in Figure 1.2, with probability of falling in\nthe right-hand box \\\\(\\frac{2}{3}\\\\) and in the left-hand box\n\\\\(\\frac{1}{3}\\\\), we write\n\n5 We call such events **complementary**.\n\n    \n    \n    rbinom(12, prob = 2/3, size = 1)__\n    \n    \n     [1] 1 1 1 0 0 0 1 0 1 0 1 0\n\nThe 1 indicates success, meaning that the ball fell in the right-hand box, 0\nmeans the ball fell in the left-hand box.\n\n### 1.3.2 Binomial success counts\n\nIf we only care how many balls go in the right-hand box, then the order of the\nthrows doesn’t matter6, and we can get this number by just taking the sum of\nthe cells in the output vector. Therefore, instead of the binary vector we saw\nabove, we only need to report a single number. In R, we can do this using one\ncall to the `rbinom` function with the parameter `size` set to 12.\n\nTwo outcomes and a size of 1 or more makes it a binomial trial. If the size is\n1, then this is the special case of the Bernoulli trial.\n\n6 The exchangeability property.\n\n    \n    \n    rbinom(1, prob = 2/3, size = 12)__\n    \n    \n    [1] 9\n\nThis output tells us how many of the twelve balls fell into the right-hand box\n(the outcome that has probability 2/3). We use a random two-box model when we\nhave only two possible outcomes such as heads or tails, success or failure,\nCpG or non-CpG, M or F, Y = pyrimidine or R = purine, diseased or healthy,\ntrue or false. We only need the probability of “success” \\\\(p\\\\), because\n“failure” (the _complementary_ event) will occur with probability \\\\(1-p\\\\).\nWhen looking at the result of several such trials, if they are exchangeable7,\nwe record only the number of successes. Therefore, SSSSSFSSSSFFFSF is\nsummarized as (#Successes=10, #Failures=5), or as \\\\(x=10\\\\), \\\\(n=15\\\\).\n\n7 One situation in which trials are exchangeable is if they are\n**independent** of each other.\n\nThe number of successes in 15 Bernoulli trials with a probability of success\nof 0.3 is called a **binomial** random variable or a random variable that\nfollows the \\\\(B(15,0.3)\\\\) distribution. To generate samples, we use a call\nto the `rbinom` function with the number of trials set to 15:\n\n[![What does set.seed do here?](imgs/devil.png)](imgs/devil.png \"What does\nset.seed do here?\")\n\nWhat does `set.seed` do here?\n\n    \n    \n    set.seed(235569515)\n    rbinom(1, prob = 0.3, size = 15)__\n    \n    \n    [1] 5\n\n__\n\nQuestion 1.3\n\nRepeat this function call ten times. What seems to be the most common outcome?\n\n__\n\nSolution\n\n__\n\nThe most frequent value is 4. In fact, the theoretical proportion of times\nthat we expect 4 to appear is the value of the probability that \\\\(X=4\\\\) if\n\\\\(X\\\\) follows \\\\(B(15, 0.3)\\\\).\n\nThe complete **probability mass distribution** is available by typing:\n\nWe use the function `round` to keep the number of printed decimal digits down\nto 2.\n\n    \n    \n    probabilities = dbinom(0:15, prob = 0.3, size = 15)\n    round(probabilities, 2)__\n    \n    \n     [1] 0.00 0.03 0.09 0.17 0.22 0.21 0.15 0.08 0.03 0.01 0.00 0.00 0.00 0.00 0.00\n    [16] 0.00\n\nWe can produce a bar plot of this distribution, shown in Figure 1.3.\n\n    \n    \n    barplot(probabilities, names.arg = 0:15, col = \"red\")__\n\n[![](01-chap_files/figure-html/fig-binombarplot-1.png)](01-chap_files/figure-\nhtml/fig-binombarplot-1.png \"Figure 1.3: Theoretical distribution of\nB\\(15,0.3\\) . The highest bar is at x=4. We have chosen to represent\ntheoretical values in red throughout.\")\n\nFigure 1.3: Theoretical distribution of \\\\(B(15,0.3)\\\\) . The highest bar is\nat \\\\(x=4\\\\). We have chosen to represent theoretical values in red\nthroughout.\n\nThe number of trials is the number we input in R as the `size` parameter and\nis often written \\\\(n\\\\), while the probability of success is \\\\(p\\\\).\nMathematical theory tells us that for \\\\(X\\\\) distributed as a binomial\ndistribution with parameters \\\\((n,p)\\\\) written \\\\(X \\sim B(n,p)\\\\), the\nprobability of seeing \\\\(X=k\\\\) successes is\n\n[![Instead of \\\\frac{n!}{\\(n-k\\)!k!} we can use the special notation {n\n\\\\choose k} as a shortcut.](imgs/devil.png)](imgs/devil.png \"Instead of\n\\\\frac{n!}{\\(n-k\\)!k!} we can use the special notation {n \\\\choose k} as a\nshortcut.\")\n\nInstead of \\\\(\\frac{n!}{(n-k)!k!}\\\\) we can use the special notation \\\\({n\n\\choose k}\\\\) as a shortcut.\n\n\\\\[ \\begin{aligned} P(X=k) &=&\\frac{n\\times (n-1)... (n-k+1)}{k\\times(k-1)...\n1}\\; p^k\\, (1-p)^{n-k}\\\\\\ &=&\\frac{n!}{(n-k)!k!}\\;p^k\\, (1-p)^{n-k}\\\\\\ &=&{ n\n\\choose k}\\; p^k\\, (1-p)^{n-k}. \\end{aligned} \\\\]\n\n__\n\nQuestion 1.4\n\nWhat is the output of the formula for \\\\(k=3\\\\), \\\\(p=2/3\\\\), \\\\(n=4\\\\)?\n\n### 1.3.3 Poisson distributions\n\n[![](imgs/Simeon_Poisson.jpg)](imgs/Simeon_Poisson.jpg \"Figure 1.4: Simeon\nPoisson, after whom the Poisson distribution is named \\(this is why it always\nhas a capital letter, except in our R code\\). \")\n\nFigure 1.4: Simeon Poisson, after whom the Poisson distribution is named (this\nis why it always has a capital letter, except in our R code).\n\nWhen the probability of success \\\\(p\\\\) is small and the number of trials\n\\\\(n\\\\) large, the binomial distribution \\\\(B(n, p)\\\\) can be faithfully\napproximated by a simpler distribution, the **Poisson distribution** with rate\nparameter \\\\(\\lambda=np\\\\). We already used this fact, and this distribution,\nin the HIV example (Figure 1.1).\n\n__\n\nQuestion 1.5\n\nWhat is the probability mass distribution of observing `0:12` mutations in a\ngenome of \\\\(n = 10^4\\\\) nucleotides, when the probability is \\\\(p = 5 \\times\n10^{-4}\\\\) per nucleotide? Is it similar when modeled by the binomial\n\\\\(B(n,p)\\\\) distribution and by the Poisson\\\\((\\lambda=np)\\\\) distribution?\n\n__\n\nSolution\n\n__\n\nNote that, unlike the binomial distribution, the Poisson no longer depends on\ntwo separate parameters \\\\(n\\\\) and \\\\(p\\\\), but only on their product\n\\\\(np\\\\). As in the case of the binomial distribution, we also have a\nmathematical formula for computing Poisson probabilities:\n\n\\\\[ P(X=k)= \\frac{\\lambda^k\\;e^{-\\lambda}}{k!}. \\\\]\n\nFor instance, let’s take \\\\(\\lambda=5\\\\) and compute \\\\(P(X=3)\\\\):\n\n    \n    \n    5^3 * exp(-5) / factorial(3)__\n    \n    \n    [1] 0.1403739\n\nwhich we can compare with what we computed above using `dpois`.\n\n__\n\nTask\n\nSimulate a mutation process along 10,000 positions with a mutation rate of\n\\\\(5\\times10^{-4}\\\\) and count the number of mutations. Repeat this many times\nand plot the distribution with the `barplot` function (see Figure 1.5).\n\n    \n    \n    rbinom(1, prob = 5e-4, size = 10000)__\n    \n    \n    [1] 6\n    \n    \n    simulations = rbinom(n = 300000, prob = 5e-4, size = 10000)\n    barplot(table(simulations), col = \"lavender\")__\n\n[![](01-chap_files/figure-html/fig-gen-\nsimpoisson-1.png)](01-chap_files/figure-html/fig-gen-simpoisson-1.png\n\"Figure 1.5: Simulated distribution of B\\(10000, 10^{-4}\\) for 300000\nsimulations.\")\n\nFigure 1.5: Simulated distribution of B(10000, \\\\(10^{-4}\\\\)) for 300000\nsimulations.\n\nNow we are ready to use probability calculations in a case study.\n\n### 1.3.4 A generative model for epitope detection\n\nWhen testing certain pharmaceutical compounds, it is important to detect\nproteins that provoke an allergic reaction. The molecular sites that are\nresponsible for such reactions are called **epitopes**. The technical\ndefinition of an epitope is:\n\n> A specific portion of a macromolecular antigen to which an antibody binds.\n> In the case of a protein antigen recognized by a T-cell, the epitope or\n> determinant is the peptide portion or site that binds to a Major\n> Histocompatibility Complex (MHC) molecule for recognition by the T cell\n> receptor (TCR).\n\nAnd in case you’re not so familiar with immunology: an **antibody** (as\nschematized in Figure 1.6) is a type of protein made by certain white blood\ncells in response to a foreign substance in the body, which is called the\n**antigen**.\n\n[![](imgs/Antibody_IgG2.png)](imgs/Antibody_IgG2.png \"Figure 1.6: A diagram of\nan antibody showing several immunoglobulin domains in color.\")\n\nFigure 1.6: A diagram of an antibody showing several immunoglobulin domains in\ncolor.\n\nAn antibody binds (with more or less specificity) to its antigen. The purpose\nof the binding is to help destroy the antigen. Antibodies can work in several\nways, depending on the nature of the antigen. Some antibodies destroy antigens\ndirectly. Others help recruit white blood cells to destroy the antigen. An\nepitope, also known as antigenic determinant, is the part of an antigen that\nis recognized by the immune system, specifically by antibodies, B cells or T\ncells.\n\n#### ELISA error model with known parameters\n\nELISA8 assays are used to detect specific epitopes at different positions\nalong a protein. Suppose the following facts hold for an ELISA array we are\nusing:\n\n8 **E** nzyme-**L** inked **I** mmuno**S** orbent **A** ssay ([Wikipedia link\nELISA](http://en.wikipedia.org/wiki/ELISA)).\n\n  * The baseline noise level per position, or more precisely the false positive rate, is 1%. This is the probability of declaring a hit – we think we have an epitope – when there is none. We write this \\\\(P(\\text{declare epitope}|\\text{no epitope})\\\\)9.\n\n  * The protein is tested at 100 different positions, supposed to be independent.\n\n  * We are going to examine a collection of 50 patient samples.\n\n9 The vertical bar in expressions such as \\\\(X|Y\\\\) means “\\\\(X\\\\) happens\n_conditional on_ \\\\(Y\\\\) being the case”.\n\n#### One patient’s data\n\nThe data for one patient’s assay look like this:\n\n    \n    \n      [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n     [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n     [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\nwhere the 1 signifies a hit (and thus the potential for an allergic reaction),\nand the zeros signify no reaction at that position.\n\n__\n\nTask\n\nVerify by simulation that the sum of 50 independent Bernoulli variables with\n\\\\(p=0.01\\\\) is –to good enough approximation– the same as a\nPoisson(\\\\(0.5\\\\)) random variable.\n\n#### Results from the 50 assays\n\nWe’re going to study the data for all 50 patients tallied at each of the 100\npositions. If there are no allergic reactions, the false positive rate means\nthat for one patient, each individual position has a probability of 1 in 100\nof being a 1. So, after tallying 50 patients, we expect at any given position\nthe sum of the 50 observed \\\\((0,1)\\\\) variables to have a Poisson\ndistribution with parameter 0.5. A typical result may look like Figure 1.7.\nNow suppose we see actual data as shown in Figure 1.8, loaded as R object\n`e100` from the data file `e100.RData`.\n\n[![](01-chap_files/figure-html/fig-typicalP-1.png)](01-chap_files/figure-\nhtml/fig-typicalP-1.png \"Figure 1.7: Plot of typical data from our generative\nmodel for the background, i.,e., for the false positive hits: 100 positions\nalong the protein, at each position the count is drawn from a Poisson\\(0.5\\)\nrandom variable.\")\n\nFigure 1.7: Plot of typical data from our generative model for the background,\ni.,e., for the false positive hits: 100 positions along the protein, at each\nposition the count is drawn from a Poisson(0.5) random variable.\n\n    \n    \n    load(\"../data/e100.RData\")\n    barplot(e100, ylim = c(0, 7), width = 0.7, xlim = c(-0.5, 100.5),\n      names.arg = seq(along = e100), col = \"darkolivegreen\")__\n\n[![](01-chap_files/figure-html/fig-epitopedata-1.png)](01-chap_files/figure-\nhtml/fig-epitopedata-1.png \"Figure 1.8: Output of the ELISA array results for\n50 patients in the 100 positions.\")\n\nFigure 1.8: Output of the ELISA array results for 50 patients in the 100\npositions.\n\nThe spike in Figure 1.8 is striking. _What are the chances of seeing a value\nas large as 7, if no epitope is present?_  \nIf we look for the probability of seeing a number as big as 7 (or larger) when\nconsidering one Poisson(\\\\(0.5\\\\)) random variable, the answer can be\ncalculated in closed form as\n\n\\\\[ P(X\\geq 7)= \\sum_{k=7}^\\infty P(X=k). \\\\]\n\nThis is, of course, the same as \\\\(1-P(X\\leq 6)\\\\). The probability \\\\(P(X\\leq\n6)\\\\) is the so-called **cumulative distribution** function at 6, and R has\nthe function `ppois` for computing it, which we can use in either of the\nfollowing two ways:10\n\n10 Besides the convenience of not having to do the subtraction from one, the\nsecond of these computations also tends to be more accurate when the\nprobability is small. This has to do with limitations of floating point\narithmetic.\n\n    \n    \n    1 - ppois(6, 0.5)__\n    \n    \n    [1] 1.00238e-06\n    \n    \n    ppois(6, 0.5, lower.tail = FALSE)__\n    \n    \n    [1] 1.00238e-06\n\n__\n\nTask\n\nCheck the manual page of `ppois` for the meaning of the `lower.tail` argument.\n\nWe denote this number by \\\\(\\epsilon\\\\), the Greek letter epsilon11. We have\nshown that the probability of seeing a count as large as \\\\(7\\\\), assuming no\nepitope reactions, is:\n\n11 Mathematicians often call small numbers (and children) \\\\(\\epsilon\\\\)s.\n\n\\\\[ \\epsilon=P(X\\geq 7)=1-P(X\\leq 6)\\simeq10^{-6}. \\\\]\n\n#### Extreme value analysis for the Poisson distribution\n\nStop! The above calculation is _not_ the correct computation in this case.\n\n__\n\nQuestion 1.6\n\nCan you spot the flaw in our reasoning if we want to compute the probability\nthat we observe these data if there is no epitope?\n\n__\n\nSolution\n\n__\n\nWe looked at all 100 positions, looked for the largest value and found that it\nwas 7. Due to this selection, a value as large as 7 is more likely to occur\nthan if we only looked at one position.\n\nSo instead of asking what the chances are of seeing a Poisson(0.5) as large as\n7, we should ask ourselves, what are the chances that the maximum of 100\nPoisson(0.5) trials is as large as 7? We use **extreme value** analysis\nhere12. We order the data values \\\\(x_1,x_2,... ,x_{100}\\\\) and rename them\n\\\\(x_{(1)},x_{(2)},x_{(3)},... ,x_{(100)}\\\\), so that \\\\(x_{(1)}\\\\) denotes\nthe smallest and \\\\(x_{(100)}\\\\) the largest of the counts over the 100\npositions. Together, \\\\(x_{(1)},... x_{(100)}\\\\) are called the **rank\nstatistic** of this sample of 100 values.\n\n12 Meaning that we’re interested in the behavior of the very large or very\nsmall values of a random distribution, for instance the maximum or the\nminimum.\n\n13 The notation with the \\\\(\\prod\\\\) is just a compact way to write the\nproduct of a series of terms, analogous to the \\\\(\\sum\\\\) for sums.\n\nThe maximum value being as large as 7 is the **complementary event** of having\nall 100 counts be smaller than or equal to 6. Two complementary events have\nprobabilities that sum to 1. Because the positions are supposed to be\nindependent, we can now do the computation13:\n\n\\\\[ \\begin{aligned} P(x_{(100)}\\geq7) &=&1-P(x_{(100)}\\leq6)\\\\\\\n&=&1-P(x_{(1)}\\leq6)\\times P(x_{(2)}\\leq6)\\times \\cdots \\times\nP(x_{(100)}\\leq6)\\\\\\ &=&1-P(x_1\\leq6)\\times P(x_2\\leq6)\\times \\cdots \\times\nP(x_{100}\\leq6)\\\\\\ &=&1-\\prod_{i=1}^{100} P(x_i\\leq6). \\end{aligned} \\\\]\n\nBecause we suppose each of these 100 events are independent, we can use our\nresult from above:\n\n\\\\[ \\prod_{i=1}^{100} P(x_i \\leq 6)= \\left(P(x_i \\leq 6)\\right)^{100}=\n\\left(1-\\epsilon\\right)^{100}. \\\\]\n\n#### Actually computing the numbers\n\nWe could just let R compute the value of this number,\n\\\\(\\left(1-\\epsilon\\right)^{100}\\\\). For those interested in how such\ncalculations can be shortcut through approximation, we give some details.\nThese can be skipped on a first reading.\n\nWe recall from above that \\\\(\\epsilon\\simeq 10^{-6}\\\\) is much smaller than 1.\nTo compute the value of \\\\(\\left(1-\\epsilon\\right)^{100}\\\\) approximately, we\ncan use the binomial theorem and drop all “higher order” terms of\n\\\\(\\epsilon\\\\), i.e., all terms with \\\\(\\epsilon^2, \\epsilon^3, ...\\\\),\nbecause they are negligibly small compared to the remaining (“leading”) terms.\n\n\\\\[ (1-\\epsilon)^n = \\sum_{k=0}^n {n\\choose k} \\, 1^{n-k} \\, (-\\epsilon)^k =\n1-n\\epsilon+{n\\choose 2} \\epsilon^2 - {n\\choose 3} \\epsilon^3 + ... \\simeq\n1-n\\epsilon \\simeq 1 - 10^{-4} \\\\]\n\nAnother, equivalent, route goes by using the approximation \\\\(e^{-\\epsilon}\n\\simeq 1-\\epsilon\\\\), which is the same as \\\\(\\log(1-\\epsilon)\\simeq\n-\\epsilon\\\\). Hence\n\n\\\\[ (1-\\epsilon)^{100} = e^{\\log\\left((1-\\epsilon)^{100}\\right)} = e^{ 100\n\\log (1-\\epsilon)} \\simeq e^{-100 \\epsilon} \\simeq e^{-10^{-4}} \\simeq 1 -\n10^{-4}. \\\\]\n\nThus the correct probability of seeing a number of hits as large or larger\nthan 7 in the 100 positions, if there is no epitope, is about 100 times the\nprobability we wrongly calculated previously.\n\nBoth computed probabilities \\\\(10^{-6}\\\\) and \\\\(10^{-4}\\\\) are smaller than\nstandard significance thresholds (say, \\\\(0.05, 0.01\\\\) or \\\\(0.001\\\\)). The\ndecision to reject the null of no epitope would have been the same. However if\none has to stand up in court and defend the p-value to 8 significant digits as\nin some forensic court cases:14 that is another matter. The adjusted p-value\nthat takes into account the multiplicity of the test is the one that should be\nreported, and we will return to this important issue in [Chapter\n6](06-chap.html).\n\n14 This occurred in the examination of the forensic evidence in the OJ Simpson\ncase.\n\n#### Computing probabilities by simulation\n\nIn the case we just saw, the theoretical probability calculation was quite\nsimple and we could figure out the result by an explicit calculation. In\npractice, things tend to be more complicated, and we are better to compute our\nprobabilities using the **Monte Carlo** method: a computer simulation based on\nour generative model that finds the probabilities of the events we’re\ninterested in. Below, we generate 100,000 instances of picking the maximum\nfrom 100 Poisson distributed numbers.\n\n    \n    \n    maxes = replicate(100000, {\n      max(rpois(100, 0.5))\n    })\n    table(maxes)__\n    \n    \n    maxes\n        1     2     3     4     5     6     7     9 \n        7 23028 60840 14364  1604   141    15     1 \n\nIn 16 of 100000 trials, the maximum was 7 or larger. This gives the following\napproximation for \\\\(P(X_{\\text{max}}\\geq 7)\\\\)15:\n\n15 In R, the expression `maxes >= 7` evaluates into a logical vector of the\nsame length as `maxes`, but with values of `TRUE` and `FALSE`. If we apply the\nfunction `mean` to it, that vector is converted into 0s and 1s, and the result\nof the computation is the fraction of 1s, which is the same as the fraction of\n`TRUE`s.\n\n    \n    \n    mean( maxes >= 7 )__\n    \n    \n    [1] 0.00016\n\nwhich more or less agrees with our theoretical calculation. We already see one\nof the potential limitations of Monte Carlo simulations: the “granularity” of\nthe simulation result is determined by the inverse of the number of\nsimulations (100000) and so will be around 10^{-5}. Any estimated probability\ncannot be more precise than this granularity, and indeed the precision of our\nestimate will be a few multiples of that. Everything we have done up to now is\nonly possible because we know the false positive rate per position, we know\nthe number of patients assayed and the length of the protein, we suppose we\nhave identically distributed independent draws from the model, and there are\nno unknown parameters. This is an example of **probability or generative\nmodeling** : all the parameters are known and the mathematical theory allows\nus to work by **deduction** in a **top-down** fashion.\n\nWe postulated the Poisson distribution for the noise, pretending we knew all\nthe parameters and were able to conclude through mathematical deduction.\n\nIf instead we are in the more realistic situation of knowing the number of\npatients and the length of the proteins, but don’t know the distribution of\nthe data, then we have to use **statistical modeling**. This approach will be\ndeveloped in [Chapter 2](02-chap.html). We will see that if we have only the\ndata to start with, we first need to **fit** a reasonable distribution to\ndescribe it. However, before we get to this harder problem, let’s extend our\nknowledge of discrete distributions to more than binary, success-or-failure\noutcomes.\n\n## 1.4 Multinomial distributions: the case of DNA\n\n#### More than two outcomes.\n\nWhen modeling four possible outcomes, as for instance the boxes in Figure 1.9\nor when studying counts of the four nucleotides [A,C,G] and [T], we need to\nextend the [binomial] model.\n\n[![](imgs/BallsinBoxes4.png)](imgs/BallsinBoxes4.png \"Figure 1.9: The boxes\nrepresent four outcomes or levels of a discrete categorical variable. The box\non the right represents the more likely outcome.\")\n\nFigure 1.9: The boxes represent four outcomes or levels of a discrete\n**categorical** variable. The box on the right represents the more likely\noutcome.\n\nRecall that when using the binomial, we can consider unequal probabilities for\nthe two outcomes by assigning a probability \\\\(p=P(1)=p_1\\\\) to the outcome 1\nand \\\\(1-p=p(0)=p_0\\\\) to the outcome 0. When there are more than two possible\noutcomes, say [A,C,G] and [T], we can think of throwing balls into boxes of\ndiffering sizes corresponding to different probabilities, and we can label\nthese probabilities \\\\(p_A,p_C,p_G,p_T\\\\). Just as in the binomial case the\nsum of the probabilities of all possible outcomes is 1,\n\\\\(p_A+p_C+p_G+p_T=1\\\\).\n\n[![You are secretly meeting a continuous distribution here, the uniform\ndistribution: runif.](imgs/devil.png)](imgs/devil.png \"You are secretly\nmeeting a continuous distribution here, the uniform distribution: runif.\")\n\nYou are secretly meeting a continuous distribution here, the uniform\ndistribution: `runif`.\n\n__\n\nTask\n\nExperiment with the random number generator that generates all possible\nnumbers between \\\\(0\\\\) and \\\\(1\\\\) through the function called `runif`. Use\nit to generate a random variable with 4 levels (A, C, G, T) where\n\\\\(p_{\\text{A}}=\\frac{1}{8}, p_{\\text{C}}=\\frac{3}{8},\np_{\\text{G}}=\\frac{3}{8}, p_{\\text{T}}=\\frac{1}{8}\\\\).\n\n**Mathematical formulation.** Multinomial distributions are the most important\nmodel for tallying counts and R uses a general formula to compute the\nprobability of a **multinomial** vector of counts \\\\((x_1,...,x_m)\\\\) for\noutcomes of \\\\(n\\\\) draws from \\\\(m\\\\) boxes with probabilities\n\\\\(p_1,...,p_m\\\\):  \n\nThe first term reads: the joint probability of observing count \\\\(x_1\\\\) in\nbox 1 and \\\\(x_2\\\\) in 2 and … \\\\(x_m\\\\) in box m, given that box 1 has\nprobability \\\\(p_1\\\\), box 2 has probability \\\\(p_2\\\\), … and box \\\\(m\\\\) has\nprobability \\\\(p_m\\\\).\n\n\\\\[\\begin{align} P(x_1,x_2,...,x_m) &=\\frac{n!}{\\prod_{i=1}^m x_i!}\n\\prod_{i=1}^m p_i^{x_i}\\\\\\ &={{n}\\choose{x_1,x_2,...,x_m}} \\;\np_1^{x_1}\\,p_2^{x_2}\\cdots p_m^{x_m}. \\end{align}\\\\]\n\nThe term in brackets is called the multinomial coefficient and is an\nabbreviation for \\\\[{n\\choose x_1,x_2,...,x_m}=\\frac{n!}{x_1!x_2!\\cdots\nx_m!}.\\\\] So this is a generalization of the binomial coefficient – for\n\\\\(m=2\\\\) it is the same as the binomial coefficient.\n\n__\n\nQuestion 1.7\n\nSuppose we have four boxes that are equally likely. Using the formula, what is\nthe probability of observing 4 in the first box, 2 in the second box, and none\nin the two other boxes?\n\n__\n\nSolution\n\n__\n\n\\\\[ P(4,2,0,0)=\\frac{6\\times 5\\times 4\\times 3\\times 2}{4\\times 3\\times 2\n\\times 2} \\frac{1}{4^6} =\\frac{15}{4^6}\\simeq 0.0037. \\\\]\n\n    \n    \n    dmultinom(c(4, 2, 0, 0), prob = rep(1/4, 4))__\n    \n    \n    [1] 0.003662109\n\nWe often run simulation experiments to check whether the data we see are\nconsistent with the simplest possible four-box model where each box has the\nsame probability 1/4. In some sense it is the strawman (nothing interesting is\nhappening). We’ll see more examples of this in [Chapter 2](02-chap.html). Here\nwe use a few R commands to generate such vectors of counts. First suppose we\nhave 8 characters of four different, equally likely types:\n\n    \n    \n    pvec = rep(1/4, 4)\n    t(rmultinom(1, prob = pvec, size = 8))__\n    \n    \n         [,1] [,2] [,3] [,4]\n    [1,]    1    3    1    3\n\n__\n\nQuestion 1.8\n\nTry the code without using the `t()` function; what does `t` stand for?\n\n__\n\nQuestion 1.9\n\nHow do you interpret the difference between `rmultinom(n = 8, prob = pvec,\nsize = 1)` and `rmultinom(n = 1, prob = pvec, size = 8)`? Hint: remember what\nwe did in Sections 1.3.1 and 1.3.2.\n\n### 1.4.1 Simulating for power\n\nLet’s see an example of using Monte Carlo for the **multinomial** in a way\nwhich is related to a problem scientists often have to solve when planning\ntheir experiments: how big a sample size do I need?\n\n![](imgs/SampleSize.png)\n\nAsk a statistician about sample size, they will always tell you they need more\ndata. The larger the sample size, the more sensitive the results. However lab\nwork is expensive, so there is a tricky cost-benefit tradeoff to be\nconsidered. This is such an important problem that we have dedicated a whole\nchapter to it at the end of the book ([Chapter 13](13-chap.html)).\n\nThe term **power** has a special meaning in statistics. It is the probability\nof detecting something if it _is_ there, also called the **true positive\nrate**.\n\nConventionally, experimentalists aim for a power of 80% (or more) when\nplanning experiments. This means that if the same experiment is run many\ntimes, about 20% of the time it will fail to yield significant results even\nthough it should.\n\nLet’s call \\\\(H_0\\\\) the null hypothesis that the DNA data we have collected\ncomes from a _fair_ process, where each of the 4 nucleotides is equally likely\n\\\\((p_A,p_C,p_G,p_T)=(0.25,0.25,0.25,0.25)\\\\). Null here just means: the\nbaseline, where nothing interesting is going on. It’s the strawman that we are\ntrying to disprove (or “reject”, in the lingo of statisticians), so the null\nhypothesis should be such that deviations from it are interesting16.\n\n16 If you know a little biology, you will know that DNA of living organisms\nrarely follows that null hypothesis – so disproving it may not be all that\ninteresting. Here we proceed with this null hypothesis because it allows us to\nillustrate the calculations, but it can also serve us as a reminder that the\nchoice of a good null hypothesis (one whose rejection is interesting) requires\nscientific input.\n\nAs you saw by running the R commands for 8 characters and 4 equally likely\noutcomes, represented by equal-sized boxes, we do not always get 2 in each\nbox. It is impossible to say, from looking at just 8 characters, whether the\nnucleotides come from a fair process or not.\n\nLet’s determine if, by looking at a sequence of length \\\\(n=20\\\\), we can\ndetect whether the original distribution of nucleotides is fair or whether it\ncomes from some other (“alternative”) process.\n\nWe generate 1000 simulations from the null hypothesis using the `rmultinom`\nfunction. We display only the first 11 columns to save space.\n\n    \n    \n    obsunder0 = rmultinom(1000, prob = pvec, size = 20)\n    dim(obsunder0)__\n    \n    \n    [1]    4 1000\n    \n    \n    obsunder0[, 1:11]__\n    \n    \n         [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]\n    [1,]    6    5    6    8    4    6    2    7    5     4     4\n    [2,]    6    6    3    7    3    3    8    4    3     3     5\n    [3,]    3    3    6    2    8    3    5    7    4     7     6\n    [4,]    5    6    5    3    5    8    5    2    8     6     5\n\n[![Notice that the top of every column there is an index of the form\n\\[,1\\]\\[,2\\]... These are the column indices. The rows are labeled\n\\[1,\\]\\[2,\\].... The object obsunder0 is not a simple vector as those we have\nseen before, but an array of numbers in a\nmatrix.](imgs/devil.png)](imgs/devil.png \"Notice that the top of every column\nthere is an index of the form \\[,1\\]\\[,2\\]... These are the column indices.\nThe rows are labeled \\[1,\\]\\[2,\\].... The object obsunder0 is not a simple\nvector as those we have seen before, but an array of numbers in a matrix.\")\n\nNotice that the top of every column there is an index of the form\n`[,1][,2]...` These are the column indices. The rows are labeled\n`[1,][2,]...`. The object `obsunder0` is not a simple vector as those we have\nseen before, but an array of numbers in a matrix.\n\nEach column in the matrix `obsunder0` is a simulated instance. You can see\nthat the numbers in the boxes vary a lot: some are as big as 8, whereas the\nexpected value is 5=20/4.\n\n#### Creating a test\n\nRemember: we know these values come from a fair process. Clearly, knowing the\nprocess’ expected values isn’t enough. We also need a measure of variability\nthat will enable us to describe how much variability is expected and how much\nis too much. We use as our measure the following statistic, which is computed\nas the sum of the squares of the differences between the observed values and\nthe expected values relative to the expected values. Thus, for each instance,\n\nThis measure weights each of the square residuals relative to their expected\nvalues.\n\n\\\\[ {\\tt stat}=\\frac{(E_A-x_A)^2}{E_A}+ \\frac{(E_C-x_C)^2}{E_C}+ \\frac{(E_G-\nx_G)^2}{E_G}+ \\frac{(E_T-x_T)^2}{E_T} =\\sum_i\\frac{(E_i-x_i)^2}{E_i}\n\\tag{1.1}\\\\]\n\nHow much do the first three columns of the generated data differ from what we\nexpect? We get:\n\n    \n    \n    expected0 = pvec * 20\n    sum((obsunder0[, 1] - expected0)^2 / expected0)__\n    \n    \n    [1] 1.2\n    \n    \n    sum((obsunder0[, 2] - expected0)^2 / expected0)__\n    \n    \n    [1] 1.2\n    \n    \n    sum((obsunder0[, 3] - expected0)^2 / expected0)__\n    \n    \n    [1] 1.2\n\nThe values of the measure can differ- you can look at a few more than 3\ncolumns, we’re going to see how to study all 1,000 of them. To avoid\nrepetitive typing, we encapsulate the formula for `stat`, Equation 1.1, in a\nfunction:\n\n    \n    \n    stat = function(obsvd, exptd = 20 * pvec) {\n      sum((obsvd - exptd)^2 / exptd)\n    }\n    stat(obsunder0[, 1])__\n    \n    \n    [1] 1.2\n\nTo get a more complete picture of this variation, we compute the measure for\nall 1000 instances and store these values in a vector we call `S0`: it\ncontains values generated under \\\\(H_0\\\\). We can consider the histogram of\nthe `S0` values shown in Figure 1.10 an estimate of our **null distribution**.\n\n    \n    \n    S0 = apply(obsunder0, 2, stat)\n    summary(S0)__\n    \n    \n       Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0.000   1.200   2.800   3.126   4.400  17.600 \n    \n    \n    hist(S0, breaks = 25, col = \"lavender\", main = \"\")__\n\n[![](01-chap_files/figure-html/fig-histS0-1.png)](01-chap_files/figure-\nhtml/fig-histS0-1.png \"Figure 1.10: The histogram of simulated values S0 of\nthe statistic stat under the null \\(fair\\) distribution provides an\napproximation of the sampling distribution of the statistic stat.\")\n\nFigure 1.10: The histogram of simulated values `S0` of the statistic `stat`\nunder the null (fair) distribution provides an approximation of the **sampling\ndistribution** of the statistic `stat`.\n\n[![The apply function is shorthand for a loop over the rows or columns of an\narray. Here the second argument, 2, indicates looping over the\ncolumns.](imgs/devil.png)](imgs/devil.png \"The apply function is shorthand for\na loop over the rows or columns of an array. Here the second argument, 2,\nindicates looping over the columns.\")\n\nThe `apply` function is shorthand for a loop over the rows or columns of an\narray. Here the second argument, 2, indicates looping over the columns.\n\nThe summary function shows us that `S0` takes on a spread of different values.\nFrom the simulated data we can approximate, for instance, the 95% quantile (a\nvalue that separates the smaller 95% of the values from the 5% largest\nvalues).\n\n    \n    \n    q95 = quantile(S0, probs = 0.95)\n    q95 __\n    \n    \n    95% \n    7.6 \n\nSo we see that 5% of the `S0` values are larger than 7.6. We’ll propose this\nas our critical value for testing data and will reject the hypothesis that the\ndata come from a fair process, with equally likely nucleotides, if the\nweighted sum of squares `stat` is larger than 7.6.\n\n#### Determining our test’s power\n\nWe must compute the probability that our test—based on the weighted sum-of-\nsquare differences—will detect that the data in fact do not come from the null\nhypothesis. We compute the probability of rejecting by simulation. We generate\n1000 simulated instances from an alternative process, parameterized by\n`pvecA`.\n\n![](imgs/roulette.png)\n\n    \n    \n    pvecA = c(3/8, 1/4, 1/4, 1/8)\n    observed = rmultinom(1000, prob = pvecA, size = 20)\n    dim(observed)__\n    \n    \n    [1]    4 1000\n    \n    \n    observed[, 1:7]__\n    \n    \n         [,1] [,2] [,3] [,4] [,5] [,6] [,7]\n    [1,]   10    4    8    8    4    7    7\n    [2,]    3   10    5    6    6    7    2\n    [3,]    5    3    5    6    4    2    6\n    [4,]    2    3    2    0    6    4    5\n    \n    \n    apply(observed, 1, mean)__\n    \n    \n    [1] 7.469 4.974 5.085 2.472\n    \n    \n    expectedA = pvecA * 20\n    expectedA __\n    \n    \n    [1] 7.5 5.0 5.0 2.5\n\nAs with the simulation from the null hypothesis, the observed values vary\nconsiderably. The question is: how often (out of 1000 instances) will our test\ndetect that the data depart from the null?\n\nThe test doesn’t reject the first observation, (10, 3, 5, 2), because the\nvalue of the statistic is within the 95th percentile.\n\n    \n    \n    stat(observed[, 1])__\n    \n    \n    [1] 7.6\n    \n    \n    S1 = apply(observed, 2, stat)\n    q95 __\n    \n    \n    95% \n    7.6 \n    \n    \n    sum(S1 > q95)__\n    \n    \n    [1] 199\n    \n    \n    power = mean(S1 > q95)\n    power __\n    \n    \n    [1] 0.199\n\n[![We read the vertical line as given or conditional\non.](imgs/devil.png)](imgs/devil.png \"We read the vertical line as given or\nconditional on.\")\n\nWe read the vertical line as **given** or **conditional on**.\n\nRun across 1000 simulations, the test identified 199 as coming from an\nalternative distribution. We have thus computed that the probability\n\\\\(P(\\text{reject }H_0 \\;|\\; H_A)\\\\) is 0.199.\n\nWith a sequence length of \\\\(n = 20\\\\) we have a _power_ of about 20% to\ndetect the difference between the fair generating process and our\n**alternative**.\n\n__\n\nTask\n\nIn practice, as we mentioned, an acceptable value of power is \\\\(0.8\\\\) or\nmore. Repeat the simulation experiments and suggest a new sequence length\n\\\\(n\\\\) that will ensure that the power is acceptable.\n\n#### Classical statistics for classical data\n\nWe didn’t need to simulate the data using Monte Carlo to compute the 95th\npercentiles; there is an adequate theory to help us with the computations.\n\nOur statistic `stat` actually has a well-known distribution called the chi-\nsquare distribution (with 3 degrees of freedom) and written \\\\({\\chi}^2_3\\\\).\n\n[![](imgs/ProbaDiagram.png)](imgs/ProbaDiagram.png \"Figure 1.11: We have\nstudied how a probability model has a distribution, which we call F. F often\ndepends on parameters, which are–by convention–denoted by Greek letters, such\nas \\\\theta. The observed data are generated via the brown arrow and are\nrepresented by Latin letters, such as x. The vertical bar in the probability\ncomputation stands for supposing that or conditional on\")\n\nFigure 1.11: We have studied how a probability model has a distribution, which\nwe call \\\\(F\\\\). \\\\(F\\\\) often depends on parameters, which are–by\nconvention–denoted by Greek letters, such as \\\\(\\theta\\\\). The observed data\nare generated via the brown arrow and are represented by Latin letters, such\nas \\\\(x\\\\). The vertical bar in the probability computation stands for\n**supposing that** or **conditional on**\n\nWe will see in [Chapter 2](02-chap.html) how to compare distributions using\nQ-Q plots (see [Figure 2.8](02-chap.html#fig-qqplot3-1)). We could have used a\nmore standard test instead of running a hand-made simulation. However, the\nprocedure we’ve learned extends to many situations in which the chi-square\ndistribution doesn’t apply. For instance, when some of the boxes have\nextremely low probabilities and their counts are mostly zero.\n\n## 1.5 Summary of this chapter\n\nWe have used mathematical formulæ and R to compute probabilities of various\ndiscrete _events_ , using a few basic distributions:\n\n**The _Bernoulli_ distribution** was our most basic building block – it is\nused to represent a single binary trial such as a coin flip. We can code the\noutcomes as 0 and 1. We call \\\\(p\\\\) the probability of success (the 1\noutcome).\n\n**The _binomial_ distribution** is used for the number of 1s in \\\\(n\\\\) binary\ntrials, and we can compute the probabilities of seeing \\\\(k\\\\) successes using\nthe R function `dbinom`. We also saw how to simulate a binomial eperiment with\n\\\\(n\\\\) trials using the function `rbinom`.\n\n**The _Poisson_ distribution** is most appropriate for cases when \\\\(p\\\\) is\nsmall (the 1s are rare). It has only one parameter \\\\(\\lambda\\\\), and the\nPoisson distribution for \\\\(\\lambda=np\\\\) is approximately the same as the\nbinomial distribution for \\\\((n,p)\\\\) if \\\\(p\\\\) is small. We used the Poisson\ndistribution to model the number of randomly occurring false positives in an\nassay that tested for epitopes along a sequence, presuming that the per-\nposition false positive rate \\\\(p\\\\) was small. We saw how such a parametric\nmodel enabled us to compute the probabilities of extreme events, as long as we\nknew all the parameters.\n\n**The _multinomial_ distribution** is used for discrete events that have more\nthan two possible outcomes or **levels**. The power example showed us how to\nuse Monte Carlo simulations to decide how much data we need to collect if we\nwant to test whether a multinomial model with equal probabilities is\nconsistent with the data. We used probability distributions and probabilistic\nmodels to evaluate hypotheses about how our data were generated, by making\nassumptions about the generative models. We term the probability of seeing the\ndata, given a hypothesis, a **p-value**. This is not the same as the\nprobability that the hypothesis is true!\n\n[![P\\(H_0\\\\;|\\\\;\\\\text{data}\\) is not the same as a p-value\nP\\(\\\\text{data}\\\\;|\\\\;H_0\\).](imgs/devil.png)](imgs/devil.png\n\"P\\(H_0\\\\;|\\\\;\\\\text{data}\\) is not the same as a p-value\nP\\(\\\\text{data}\\\\;|\\\\;H_0\\).\")\n\n\\\\(P(H_0\\;|\\;\\text{data})\\\\) is not the same as a p-value\n\\\\(P(\\text{data}\\;|\\;H_0)\\\\).\n\n## 1.6 Further reading\n\n  * The elementary book by Freedman, Pisani, and Purves ([1997](16-chap.html#ref-Freedman:1997)) provides the best introduction to probability through the type of box models we mention here.\n\n  * The book by Durbin et al. ([1998](16-chap.html#ref-DEKM)) covers many useful probability distributions and provides in its appendices a more complete view of the theoretical background in probability theory and its applications to sequences in biology.\n\n  * Monte Carlo methods are used extensively in modern statistics. Robert and Casella ([2009](16-chap.html#ref-Casella2009)) provides an introduction to these methods using R.\n\n  * [Chapter 6](06-chap.html) will cover the subject of hypothesis testing. We also suggest Rice ([2006](16-chap.html#ref-Rice:2007)) for more advanced material useful for the type of more advanced probability distributions, beta, gamma, exponentials we often use in data analyses.\n\n## 1.7 Exercises\n\n__\n\nExercise 1.1\n\nR can generate numbers from all known distributions. We now know how to\ngenerate random discrete data using the specialized R functions tailored for\neach type of distribution. We use the functions that start with an `r` as in\n`rXXXX`, where `XXXX` could be `pois`, `binom`, `multinom`. If we need a\ntheoretical computation of a probability under one of these models, we use the\nfunctions `dXXXX`, such as `dbinom`, which computes the probabilities of\nevents in the discrete binomial distribution, and `dnorm`, which computes the\nprobability density function for the continuous normal distribution. When\ncomputing tail probabilities such as \\\\(P(X>a)\\\\) it is convenient to use the\ncumulative distribution functions, which are called `pXXXX`. Find two other\ndiscrete distributions that could replace the `XXXX` above.\n\n__\n\nExercise 1.2\n\nIn this chapter we have concentrated on _discrete_ random variables, where the\nprobabilities are concentrated on a countable set of values. How would you\ncalculate the _probability mass_ at the value \\\\(X=2\\\\) for a binomial\n\\\\(B(10, 0.3)\\\\) with `dbinom`? Use `dbinom` to compute the _cumulative_\ndistribution at the value 2, corresponding to \\\\(P(X\\leq 2)\\\\), and check your\nanswer with another R function.\n\n__\n\nSolution\n\n__\n\n    \n    \n    dbinom(2, size = 10, prob = 0.3)__\n    \n    \n    [1] 0.2334744\n    \n    \n    pbinom(2, size = 10, prob = 0.3)__\n    \n    \n    [1] 0.3827828\n    \n    \n    sum(dbinom(0:2, size = 10, prob = 0.3)) __\n    \n    \n    [1] 0.3827828\n\n__\n\nExercise 1.3\n\nWhenever we note that we keep needing a certain sequence of commands, it’s\ngood to put them into a function. The function body contains the instructions\nthat we want to do over and over again, the function arguments take those\nthings that we may want to vary. Write a function to compute the probability\nof having a maximum as big as `m` when looking across `n` Poisson variables\nwith rate `lambda`.\n\n__\n\nSolution\n\n__\n\n    \n    \n    poismax = function(lambda, n, m) {\n      epsilon = 1 - ppois(m - 1, lambda)\n      1 - exp( -n * epsilon)\n    }\n    poismax(lambda = 0.5, n = 100, m = 7)__\n    \n    \n    [1] 0.0001002329\n    \n    \n    poismax(lambda = mean(e100), n = 100, m = 7)__\n    \n    \n    [1] 0.0001870183\n\n__\n\nExercise 1.4\n\nRewrite the function to have default values for its arguments (i.e., values\nthat are used by it if the argument is not specified in a call to the\nfunction).\n\n__\n\nSolution\n\n__\n\n    \n    \n    poismax = function(lambda, n = 100, m = 7) {\n      1 - exp( -n * (1 - ppois(m - 1, lambda)))\n    }\n    poismax(0.5)__\n    \n    \n    [1] 0.0001002329\n    \n    \n    poismax(0.5, m = 9)__\n    \n    \n    [1] 3.43549e-07\n\n__\n\nExercise 1.5\n\nIn the epitope example, use a simulation to find the probability of having a\nmaximum of 9 or larger in 100 trials. How many simulations do you need if you\nwould like to prove that “the probability is smaller than 0.000001”?\n\n__\n\nExercise 1.6\n\nUse `?Distributions` in R to get a list of available distributions17. Make\nplots of the probability mass or density functions for various distributions\n(using the functions named `dXXXX`), and list five distributions that are not\ndiscrete.\n\n17 These are just the ones that come with a basic R installation. There are\nmore in additional packages, see the [CRAN task view: Probability\nDistributions](https://cran.r-project.org/web/views/Distributions.html).\n\n__\n\nExercise 1.7\n\nGenerate 100 instances of a Poisson(3) random variable. What is the mean? What\nis the variance as computed by the R function `var`?\n\n__\n\nExercise 1.8\n\n_C. elegans_ genome nucleotide frequency: Is the mitochondrial sequence of _C.\nelegans_ consistent with a model of equally likely nucleotides?\n\n  1. Explore the nucleotide frequencies of chromosome M by using a dedicated function in the **[Biostrings](https://bioconductor.org/packages/Biostrings/)** package from Bioconductor.\n\n  2. Test whether the _C. elegans_ data is consistent with the uniform model (all nucleotide frequencies the same) using a simulation. Hint: This is our opportunity to use Bioconductor for the first time. Since Bioconductor’s package management is more tightly controlled than CRAN’s, we need to use a special `install` function (from the **[BiocManager](https://cran.r-project.org/web/packages/BiocManager/)** package) to install Bioconductor packages.\n\n    \n    \n    if (!requireNamespace(\"BiocManager\", quietly = TRUE))\n        install.packages(\"BiocManager\")\n    BiocManager::install(c(\"Biostrings\", \"BSgenome.Celegans.UCSC.ce2\"))__\n\nAfter that, we can load the genome sequence package as we load any other R\npackages.\n\n__\n\nSolution\n\n__\n\n    \n    \n    library(\"BSgenome.Celegans.UCSC.ce2\")\n    Celegans __\n    \n    \n    | BSgenome object for Worm\n    | - organism: Caenorhabditis elegans\n    | - provider: UCSC\n    | - genome: ce2\n    | - release date: Mar. 2004\n    | - 7 sequence(s):\n    |     chrI   chrII  chrIII chrIV  chrV   chrX   chrM                       \n    | \n    | Tips: call 'seqnames()' on the object to get all the sequence names, call\n    | 'seqinfo()' to get the full sequence info, use the '$' or '[[' operator to\n    | access a given sequence, see '?BSgenome' for more information.\n    \n    \n    seqnames(Celegans)__\n    \n    \n    [1] \"chrI\"   \"chrII\"  \"chrIII\" \"chrIV\"  \"chrV\"   \"chrX\"   \"chrM\"  \n    \n    \n    Celegans$chrM __\n    \n    \n    13794-letter DNAString object\n    seq: CAGTAAATAGTTTAATAAAAATATAGCATTTGGGTT...TATTTATAGATATATACTTTGTATATATCTATATTA\n    \n    \n    class(Celegans$chrM)__\n    \n    \n    [1] \"DNAString\"\n    attr(,\"package\")\n    [1] \"Biostrings\"\n    \n    \n    length(Celegans$chrM)__\n    \n    \n    [1] 13794\n    \n    \n    library(\"Biostrings\")\n    lfM = letterFrequency(Celegans$chrM, letters=c(\"A\", \"C\", \"G\", \"T\"))\n    lfM __\n    \n    \n       A    C    G    T \n    4335 1225 2055 6179 \n    \n    \n    sum(lfM)__\n    \n    \n    [1] 13794\n    \n    \n    lfM / sum(lfM)__\n    \n    \n             A          C          G          T \n    0.31426707 0.08880673 0.14897782 0.44794838 \n\nCreate a random (each letter with equal probability) sequence of the same\nlength as the _C. elegans_ chromosome M:\n\n    \n    \n    t(rmultinom(1, length(Celegans$chrM), p = rep(1/4, 4)))__\n    \n    \n         [,1] [,2] [,3] [,4]\n    [1,] 3409 3486 3476 3423\n\nThe expected frequencies are just\n\n    \n    \n    length(Celegans$chrM) / 4 __\n    \n    \n    [1] 3448.5\n\nWe’re going to compute a statistic that measures how close two multinomial\noutputs are to each other. We’ll take the average squared difference between\nobserved (`o`) and expected (`e`) counts, scaled by `e`. We will call the\nfunction `oestat`.\n\n    \n    \n    oestat = function(o, e) {\n      sum((o-e)^2 / e)\n    }\n    oe = oestat(o = lfM, e = length(Celegans$chrM) / 4)\n    oe __\n    \n    \n    [1] 4386.634\n\nIs this larger than what randomness could explain? We already saw above a set\nof typical counts we could expect under the null model. But we need a whole\nset (distribution) of values. We compute these using the replicate function\nthat evaluates a function many times. We run the following:\n\n    \n    \n    B = 10000\n    n = length(Celegans$chrM)\n    expected = rep(n / 4, 4)\n    oenull = replicate(B,\n      oestat(e = expected, o = rmultinom(1, n, p = rep(1/4, 4))))__\n\nDurbin, Richard, Sean Eddy, Anders Krogh, and Graeme Mitchison. 1998.\n_Biological Sequence Analysis_. Cambridge University Press.\n\nFreedman, David, Robert Pisani, and Roger Purves. 1997. _Statistics_. New\nYork, NY: WW Norton.\n\nRice, John. 2006. _Mathematical Statistics and Data Analysis_. Cengage\nLearning.\n\nRobert, Christian, and George Casella. 2009. _Introducing Monte Carlo Methods\nwith R_. Springer Science & Business Media.\n\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["msmb.css"],"toc":true,"output-file":"01-chap.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}