{"title":"13.1 Goals for this chapter","markdown":{"headingText":"13.1 Goals for this chapter","containsRefs":false,"markdown":"![](imgs/RAFisherSmoking.png)\n\n![](imgs/dailies_icon.png)\n\nIn the same way a film director will view daily takes to correct potential\nlighting or shooting issues before they affect too much footage, it is a good\nidea not to wait until all the runs of an experiment have been finished before\nlooking at the data. Intermediate data analyses and visualizations will track\nunexpected sources of variation and enable you to adjust the protocol. Much is\nknown about sequential design of experiments ([Mead 1990](16-chap.html#ref-\nmead1990design)), but even in a more pragmatic setting it is important to be\naware of sources of variation as they occur and adjust for them.\n\nWe have now seen many different biological datasets and data types, and\nmethods for analyzing them. To conclude this book, we recapitulate some of the\ngeneral lessons we learned. Three great pieces of good advice are:\n\n  * **How to analyze?** R.A. Fisher, one of the fathers of experimental design ([Fisher 1935](16-chap.html#ref-fisher1935design)) is quoted as saying: _To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of_ 1. So it is important to design an experiment already with the analysis in mind. Do not wait with thinking about how to analyze the data only once they have been acquired.\n\n  * **When?** Dailies: start with the analysis as soon as you have acquired the first data. Don’t wait until everything is collected and it’s too late to troubleshoot.\n\n  * **What?** Start writing the paper while you’re analyzing the data. Only once you’re writing and trying to present your results and conclusions, you realize what you should have done to properly support them.\n\n1 Presidential Address to the First Indian Statistical Congress, 1938. Sankhya\n4, 14-17.\n\n\nIn this chapter we will:\n\n  * Develop a simple categorization of what types of experiments there are and the varying amounts of control we have with each of them.\n\n  * Recap how to distinguish the different types of variability: error, noise and bias.\n\n  * Discuss the things that we need to worry about: confounding, dependencies, batch effects. We’ll ask the famous question: _how many replicates?_.\n\n  * Recap the essential ideas behind mean-variance relationships and how they inform us on whether and how to transform our data.\n\n  * Computational techniques and tools are essential for getting the job done. We will discuss efficient workflow design, data representation and computation.\n\n  * Try to be aware of data summarization steps and questions of sufficiency in our analytical workflows - so that we don’t throw away important information in some step “upstream”, which is then missing and making us trouble downstream.\n\n## 13.2 Types of experiments\n\n#### The art of “good enough”.\n\nWe need experimental design in order to deal with the fact that our resources\nare finite, our instruments not perfect, and that the real world is\ncomplicated. We want to get the the best possible outcome nonetheless. This\ninvariably results in hard decisions and tradeoffs. **Experimental design**\naims to rationalize such decisions. Our experimental interventions and our\nmeasurement instruments have limited precision and accuracy; often we don’t\nknow these limitations at the outset and have to collect **preliminary data**\nto estimate them. We may only be able to observe the phenomenon of interest\nindirectly rather than directly. Our treatment conditions may have undesired\nbut hard to avoid side effects, our measurements may be overlaid with\ninterfering signals or “background noise”. Sample sizes are limited for\npractical and economic reasons. There is little point in prescribing\nunrealistic ideals – we need to make choices that are pragmatic and feasible.\nA quote from ([Bacher and Kendziorski 2016](16-chap.html#ref-Bacher:GB:2016))\nexplains this clearly: “Generally speaking, a well-designed experiment is one\nthat is sufficiently powered and one in which technical artifacts and\nbiological features that may systematically affect measurements are balanced,\nrandomized or controlled in some other way in order to minimize opportunities\nfor multiple explanations for the effect(s) under study.”\n\nTo start with, let us discuss the major different types of experiments, since\neach of them requires different approaches.\n\nIn a **controlled experiment** , we have control over all relevant variables:\nthe (model) system under study, the environmental conditions, the experimental\nreadout. For instance, we could have a well-characterized cell line growing in\nlaboratory conditions on defined media, temperature and atmosphere, we’ll\nadminister a precise amount of a drug, and after 72h we measure the activity\nof a specific pathway reporter.\n\nIn a **study** , we have less control: important conditions that may affect\nthe measured outcome are not under control of the researcher, usually because\nof ethical concerns or logistical constraints. For instance, in an ecological\nfield study, this could be the weather, the availability of nutrition\nresources or the activity of predators. In an **observational study** , even\nthe variable of interest is not controlled by the researcher. For instance, in\na clinical trial, this might be the assignment of the individual subjects to\ngroups. Since there are many possibilities for confounding (Section 13.4.1),\ninterpretation of an observational study can be difficult. Here’s where the\nold adage “correlation is not causation” appertains.\n\nIn a **randomized controlled trial** , we still have to deal with lack of\ncontrol over many of the factors that impact the outcome, but we control\nassignment of the variable of interest (say, the type of treatment in a\nclinical trial), therefore we can expect that –with high enough sample size–\nall the nuisance effects average out and the observed effect can really be\ncausally assigned to the intervention. Such trials are usually **prospective**\n2, i.e., the outcome is not known at the time of the assignment of the\npatients to the groups.\n\n2 The antonym is retrospective; observational studies can be prospective or\nretrospective.\n\nA **meta-analysis** is an observational study on several previous experiments\nor studies. One motivation of a meta-analysis is to increase power by\nincreasing effective sample size. Another is to overcome the limitations of\nindividual experiments or studies, which might suffer from researcher bias or\nother biases, be underpowered, or can otherwise be flawed or random. The hope\nis that by pooling results from many studies, such “study-level” problems\naverage out.\n\n## 13.3 Partitioning error: bias and noise\n\n[![Statisticians use the term error for any deviations of a measured value\nfrom the true value. This is different from the everyday use of the word. In\nstatistics, error is an unavoidable aspect of life. It is not “bad”, it is\nsomething to be cherished, reckoned with, tamed and\ncontrolled.](imgs/devil.png)](imgs/devil.png \"Statisticians use the term error\nfor any deviations of a measured value from the true value. This is different\nfrom the everyday use of the word. In statistics, error is an unavoidable\naspect of life. It is not “bad”, it is something to be cherished, reckoned\nwith, tamed and controlled.\")\n\nStatisticians use the term **error** for any deviations of a measured value\nfrom the true value. This is different from the everyday use of the word. In\nstatistics, error is an unavoidable aspect of life. It is not “bad”, it is\nsomething to be cherished, reckoned with, tamed and controlled.\n\nWe broadly distinguish between two types of error. The first, which we call\n**noise** , “averages out” if we just perform enough replicates. The second,\nwhich we call **bias** , remains; it even becomes more apparent with more\nreplication. Recall the bull’s eye in [Figure 12.18](12-chap.html#fig-\nsupervised-bullseye): in the lower panel, there is a lot of noise, but no\nbias, and the center of the cloud of points is in the right place. In the\nupper panel, there is much less noise, but bias. No amount of replication will\nremedy the fact that the center of the points is in the wrong place.\n\nBias is more difficult to deal with than noise: noise is easily recognized\njust from looking at replicates, and it averages out as we analyze more and\nmore replicates. With bias, it can be hard to even recognize that it is there,\nand then we need to find ways to measure it and adjust for it, usually with\nsome quantitative model.\n\n__\n\nQuestion 13.1\n\nGive two examples in previous chapters where we have modeled bias in high\nthroughput data.\n\n__\n\nSolution\n\n__\n\nFor instance, in [Chapter 8](08-chap.html), we modeled the sampling noise with\nthe gamma-Poisson distribution, we estimated sequencing depth bias with the\nlibrary size factors and took it into account when testing for differential\nexpression. We also modeled sampling biases caused by the two different\nprotocols used (single-end, paired-end) by introducing a blocking factor into\nour generalized linear model.\n\n### 13.3.1 Error models: noise is in the eye of the beholder\n\nThe efficiency of most biochemical or physical processes involving DNA-\npolymers depends on their sequence content. For instance, occurrences of long\nhomopolymer stretches, palindromes, overall or local GC content can modify the\nefficiency of PCR, or the dynamics of how the polymer is being pulled through\na nanopore. The size and nature of such effects is challenging to model. They\ndepend in subtle ways on factors like concentration, temperature, enzyme used,\netc. So: when looking at RNA-Seq data, should we treat GC content as noise or\nas bias?\n\n__\n\nQuestion 13.2\n\nHow does the **[DESeq2](https://bioconductor.org/packages/DESeq2/)** method\naddress this issue?\n\n__\n\nSolution\n\n__\n\n**[DESeq2](https://bioconductor.org/packages/DESeq2/)** offers both options.\nIf size factors are used to model per-sample sampling bias, then such effects\nare not explicitly modeled.\n\n_Note:_ The assumption is then that, for each gene, any such bias would effect\nthe counts in the same way across all samples, so that for the purpose of\ndifferential expression analysis, it cancels out. To the extent that such\neffects are sample-specific, they are treated as noise. However, as described\nin its vignette, **[DESeq2](https://bioconductor.org/packages/DESeq2/)** also\nallows specifying sample- _and_ gene-dependent normalization factors for a\nmatrix, and these are intended to contain explicit estimates of such biases.\n\nRemember that the noun _sample_ here, by convention, refers to one column of\nthe count matrix, e.g., one sequencing library corresponding to one replicate\nof one biological condition. The same term (here as the verb form _sampling_)\nis also used in its more general, statistical sense, as in “a sample of data\nfrom a distribution”. There is no easy way around this ambiguity, so we just\nneed to be aware of it.\n\nFormal error models can help us decompose the variability into noise and bias.\nA standard decomposition you may have encountered is called ANOVA (ANalysis Of\nVAriance). In these types of models, variability is measured by sums of\nsquares and aportioned according to its origin. For instance, when doing\nsupervised classification in a linear discriminant analysis (LDA) in [Chapter\n12](12-chap.html), we computed the total sum of squares \\\\(C\\\\) as\n\n\\\\[ C_{\\text{total}} = C_{\\text{within groups}} + C_{\\text{between groups}}.\n\\tag{13.1}\\\\]\n\nHowever, there are usually multiple ways of doing such a decomposition: an\neffect that at one stage is considered within-group variation (noise) might be\nconsidered a between-groups effect once the right (sub)groups are assigned.\n\n[![Maybe this is akin to the vision of “personalized medicine”: better patient\nstratification that converts within group variation \\(incl. unsuccessful or\nunnecessary treatments\\) into between groups variation \\(where every group\ngets exactly what they need\\).](imgs/devil.png)](imgs/devil.png \"Maybe this is\nakin to the vision of “personalized medicine”: better patient stratification\nthat converts within group variation \\(incl. unsuccessful or unnecessary\ntreatments\\) into between groups variation \\(where every group gets exactly\nwhat they need\\).\")\n\nMaybe this is akin to the vision of “personalized medicine”: better patient\nstratification that converts within group variation (incl. unsuccessful or\nunnecessary treatments) into between groups variation (where every group gets\nexactly what they need).\n\n#### Determinism versus chance.\n\n[![](imgs/cointosser3_web.jpg)](imgs/cointosser3_web.jpg \"Figure 13.1: A\ncarefully constructed coin tossing machine can be made to provide\ndeterministic coin flips.\")\n\nFigure 13.1: A carefully constructed coin tossing machine can be made to\nprovide deterministic coin flips.\n\nEveryone thinks of the outcome of a coin toss as random, thus a perfect\nexample of noise. But if we meticulously registered the initial conditions of\nthe coin flip and solved the mechanical equations, we could predict which side\nhas a higher probability of coming up ([Diaconis, Holmes, and Montgomery\n2007](16-chap.html#ref-Diaconis-Montgomery-Holmes-2007)).\n\nSo, rather than asking whether a certain effect or process _is_ random or\ndeterministic, it is more fruitful to say whether _we care_ to model it\ndeterministically (as bias), or whether we ignore the details, treat it as\nstochastic, and use probabilistic modeling (noise). In this sense,\nprobabilistic models are a way of quantifying our ignorance, taming our\nuncertainty.\n\n#### Latent factors.\n\nSometimes we explicitly know about factors that cause bias, for instance, when\na different reagent batch was used in different phases of the experiments. We\ncall this **batch effects** ([Jeffrey T. Leek et al. 2010](16-chap.html#ref-\nLeek:2010:batch)). At other times, we may expect that such factors are at work\nbut have no explicit record of them. We call these **latent factors**. We can\ntreat them as adding to the noise, and in [Chapter 4](04-chap.html) we saw how\nto use mixture models to do so. But this may not be enough: with high-\ndimensional data, noise caused by latent factors tends to be correlated, and\nthis can lead to faulty inference ([Jeffrey T. Leek et al.\n2010](16-chap.html#ref-Leek:2010:batch)). The good news is that these same\ncorrelations can be exploited to estimate latent factors from the data, model\nthem as bias and thus reduce the noise ([Jeffrey T. Leek and Storey\n2007](16-chap.html#ref-LeekStorey:2007); [Stegle et al.\n2010](16-chap.html#ref-Stegle:2010)).\n\n### 13.3.2 Biological versus technical replicates\n\n__\n\nQuestion 13.3\n\nImagine you want to test whether a weight loss drug works. Which of the\nfollowing study designs would you use:\n\n  * A person is weighed on milligram precision scales, with 20 replicates. He follows the diet, and four weeks later, he is weighed again, with 20 replicates.\n\n  * Ten people weigh themselves once on their bathroom scales and report the number. Four weeks later, they weigh themselves and report again.\n\nSurely the first option must be better since it has 20 replicates on a very\nprecise instrument rather than only ten on an older piece of equipment?\n\n__\n\nSolution\n\n__\n\nWhat we have here is a (placative) instance of the difference between\n**technical** versus **biological replicates**. The number of replicates is\nless important than what types of variation are allowed to affect them. The 20\nreplicates in the first design are wasted on re-measuring something that we\nalready know with more than enough precision. Whereas the far more important\nquestion –how does the effect generalize to different people– starts to be\naddressed with the second design, although in practice more people would be\nneeded.\n\n_Note:_ Inference or generalizations can only be made to a wider population if\nwe have a representative, randomized sample of that population in our study.\nIn the first case if a weight loss occurs, one could only infer about that\nperson at that time.\n\nAnalogous questions arise in biological experimentation, e.g., do you rather\ndo five replicates on the same cell line, or one replicate each on three\ndifferent cell lines?\n\n__\n\nQuestion 13.4\n\nFor reliable variant calling with the sequencing technology used by the 1000\nGenomes project, one needs about \\\\(30\\times\\\\) coverage per genome. However,\nthe average depth of the data produced was 5.1 for 1,092 individuals ([1000\nGenomes Project Consortium 2012](16-chap.html#ref-1000Genomes)). Why was that\nstudy design chosen?\n\n__\n\nSolution\n\n__\n\nThe project’s aim was finding common genetic variants, i.e., finding variants\nthat have a prevalence of more than, say, 1% in the population. It was not to\ncall high-confidence genotypes of individual people. Therefore, it was more\ncost-efficient to sample more individuals each with low coverage (say, 1092\nindividuals at 5x) than fewer individuals with high coverage (say, 182 at\n30x). In this way, common variants would still be found with \\\\(>=30\\\\)\ncoverage (\\\\(1092 \\times 1\\% \\times 5 = 55\\\\)), since they would be present in\nseveral of the 1000 people, but more of them would be found, and there would\nbe more precise estimates of their population frequency.\n\nThe technical versus biological replicates terminology has some value, but is\noften too coarse. The observed effect may or may not be generalizable at many\ndifferent levels: different labs, different operators within one lab,\ndifferent technologies, different machines from the same technology, different\nvariants of the protocol, different strains, litters, sexes, individual\nanimals, and so forth. It’s better to name the levels of replication more\nexplicitly.\n\n### 13.3.3 Units vs. fold-changes\n\nMeasurements in physics are usually reported as multiples of SI3 units, such\nas meters, kilograms, seconds. A length measured in meters by a lab in\nAustralia using one instrument is directly comparable to one measured a year\nlater by a lab in Canada using a different instrument, or by alien scientists\nin a far-away galaxy. In biology, it is rarely possible or practical to make\nmeasurements that are as standardized. The situation here is more like that\nwhere human body parts (feets, inches, etc.) are used for length measurements,\nand where the size of these body parts is even different in different towns\nand countries, let alone galaxies.\n\n3 International System of Units (French: Système International d’Unités)\n\nBiologists often report measurements as multipes of (i.e., fold changes with\nregard to) some local, more or less ad hoc reference. The challenge with this\nis that fold changes and proportions are ratios. The denominator is a random\nvariable (as it changes from lab to lab and probably from experiment to\nexperiment), which can create high instability and very unequal variances\nbetween experiments; see the sections on transformations and sufficiency a\nlittle later in this chapter. Even when seemingly absolute values exist (e.g.,\nTPKM values in an RNA-Seq experiment), due to experiment-specific sampling\nbiases they do not translate into universal units, and they often lack an\nindication of their precision.\n\n### 13.3.4 Regular and catastrophic noise\n\nRegular noise can be modelled by simple probability models such as independent\nnormal distributions, Poissons, or mixtures such as gamma–Poisson or Laplace.\nWe can use relatively straightforward methods to take such noise into account\nin our data analyses and to compute the probability of extraordinarily large\nor small values. In the real world, this is only part of the story:\nmeasurements can be completely off scale (a sample swap, a contamination or a\nsoftware bug), and they can go awry all at the same time (a whole microtiter\nplate went bad, affecting all data measured from it). Such events are hard to\nmodel or even correct for – our best chance to deal with them is data quality\nassessment, outlier detection and documented removal.\n\n## 13.4 Basic principles in the design of experiments\n\n### 13.4.1 Confounding\n\n[![](13-chap_files/figure-html/fig-confounding-1-1.png)](13-chap_files/figure-\nhtml/fig-confounding-1-1.png \"Figure 13.2: Comparison of a \\(hypothetical\\)\nbiomarker between samples from disease and healthy states. If we are only\ngiven the information shown in the left panel, we might conclude that this\nbiomarker performs well in detecting the disease. If, in addition, we are told\nthat the data were acquired in two separate batches \\(e.g., different labs,\ndifferent machines, different time points\\) as indicated in the panel on the\nright hand side, the conclusion will be different.\")\n\nFigure 13.2: Comparison of a (hypothetical) biomarker between samples from\ndisease and healthy states. If we are only given the information shown in the\nleft panel, we might conclude that this biomarker performs well in detecting\nthe disease. If, in addition, we are told that the data were acquired in two\nseparate batches (e.g., different labs, different machines, different time\npoints) as indicated in the panel on the right hand side, the conclusion will\nbe different.\n\n__\n\nQuestion 13.5\n\nConsider the data shown in Figure 13.2. How can we decide whether the observed\ndifferences in the biomarker level are due to disease vs. healthy, or due to\nthe batch?\n\n__\n\nSolution\n\n__\n\nIt is impossible to know from these data: the two variables are confounded.\n\n**Confounding** need not only be between a biological and a technical\nvariable, it can also be more subtle. For instance, the biomarker might have\nnothing to do with the disease directly – it might just be a marker of a life\nstyle that causes the disease (as well as other things), or of an inflammation\nthat is caused by the disease (as well as by many other things), etc.\n\n### 13.4.2 Effect size and replicates\n\n[![](imgs/Avicenna.png)](imgs/Avicenna.png \"Figure 13.3: Confounding is the\nreason that one of the seven rules of experimental design listed by the\nPersian physician-scientist Abu ’Ali al-Husayn ibn Sina \\(Avicenna\\) around AD\n1020 was “to study one possible cause of a disease at a time”\n\\[@Stigler:sevenpillars\\].\")\n\nFigure 13.3: Confounding is the reason that one of the seven rules of\nexperimental design listed by the Persian physician-scientist [Abu ’Ali al-\nHusayn ibn Sina (Avicenna)](https://en.wikipedia.org/wiki/Avicenna) around AD\n1020 was “to study one possible cause of a disease at a time” ([Stigler\n2016](16-chap.html#ref-Stigler:sevenpillars)).\n\nThe effect size is the difference between the group centers, as shown by the\nred arrow in Figure 13.4. A larger sample size in each group increases the\nprecision with which the locations of each group and the effect size are\nknown, thus increasing our power to detect a difference (Figure 13.5). On the\nother hand, the performance of the biomarker as a diagnostic for\ndistinguishing individual samples between healthy and disease states depends\non the within-group distributions (and the relative prevalences of both\nstates), and is not improved by replication.\n\n[![](13-chap_files/figure-html/fig-effectsize-1.png)](13-chap_files/figure-\nhtml/fig-effectsize-1.png \"Figure 13.4: The red arrow shows the effect size,\nas measured by the difference between the centers of the two groups. Here we\nlocate the centers by the medians; sometimes the mean is used.\")\n\nFigure 13.4: The red arrow shows the effect size, as measured by the\ndifference between the centers of the two groups. Here we locate the centers\nby the medians; sometimes the mean is used.\n\n[![](13-chap_files/figure-html/fig-\ncomparesamplesize-1.png)](13-chap_files/figure-html/fig-\ncomparesamplesize-1.png \"Figure 13.5: On the left, the boxplot was created\nwith samples of size 6. On the right the sample sizes are 60. The measurements\nhave the same underlying error distribution in both cases.\")\n\nFigure 13.5: On the left, the boxplot was created with samples of size 6. On\nthe right the sample sizes are 60. The measurements have the same underlying\nerror distribution in both cases.\n\n### 13.4.3 Clever combinations: Hotelling’s weighting example\n\nTo get the best data out of available resources, capitalizing on cancellations\nand symmetries is an important aspect. Here is a famous illustration of how\nHotelling devised an improved weighing scheme. Suppose we are given a set of\neight unknown weights \\\\(\\theta = (\\theta_1, ...,\\theta_8)\\\\). In the\nfollowing code, we simulate such a set of true weights using R’s random number\ngenerator.\n\n[![](imgs/balancechem.png)](imgs/balancechem.png \"Figure 13.6: The example in\nthis section uses the pharmacist’s balance weighing analogy introduced by\nYates and developed by @Hotelling1944 and @Mood1946.\")\n\nFigure 13.6: The example in this section uses the pharmacist’s balance\nweighing analogy introduced by Yates and developed by Hotelling\n([1944](16-chap.html#ref-Hotelling1944)) and Mood ([1946](16-chap.html#ref-\nMood1946)).\n\n    \n    \n    theta = round((2 * sample(8, 8) + rnorm(8)), 1)\n    theta __\n    \n    \n    [1] 10.7 13.4 16.4  3.9  8.5 16.0  1.2  4.4\n\n**Method 1** : Naïve method, using eight weighings. Suppose we use a\npharmacist’s balance (Figure 13.6) that weighs each weight \\\\(\\theta_i\\\\)\nindividually, with errors distributed normally with a standard deviation of\n0.1. We compute the vector of errors `errors1` and their sum of squares as\nfollows:\n\n    \n    \n    X = theta + rnorm(length(theta), 0, 0.1)\n    X __\n    \n    \n    [1] 10.513279 13.268145 16.507673  3.881881  8.395974 16.073952  1.131341\n    [8]  4.289040\n    \n    \n    errors1 = X - theta\n    errors1 __\n    \n    \n    [1] -0.18672051 -0.13185519  0.10767279 -0.01811869 -0.10402607  0.07395242\n    [7] -0.06865871 -0.11095993\n    \n    \n    sum(errors1^2)__\n    \n    \n    [1] 0.09748857\n\n**Method 2** : Hotelling’s method, also using eight weighings. The method is\nbased on a Hadamard matrix, which we compute here.\n\n    \n    \n    library(\"survey\")\n    h8 = hadamard(6)\n    coef8 = 2*h8 - 1\n    coef8 __\n    \n    \n         [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n    [1,]    1    1    1    1    1    1    1    1\n    [2,]    1   -1    1   -1    1   -1    1   -1\n    [3,]    1    1   -1   -1    1    1   -1   -1\n    [4,]    1   -1   -1    1    1   -1   -1    1\n    [5,]    1    1    1    1   -1   -1   -1   -1\n    [6,]    1   -1    1   -1   -1    1   -1    1\n    [7,]    1    1   -1   -1   -1   -1    1    1\n    [8,]    1   -1   -1    1   -1    1    1   -1\n\nWe use `coef8` as the coefficients in a new weighing scheme, as follows: the\nfirst column of the matrix tells us to put all the weights on one side of the\nbalance and to weigh that. Call the result `Y[1]`. The second column tell us\nto place weights 1, 3, 5, 7 on one side of the balance and weights 2, 4, 6, 8\non the other. We then measure the difference and call the result `Y[2]`. And\nso forth, for all eight columns of `coef8`. We can express the necessary\ncomputations in matrix multiplication form as below.\n\n    \n    \n    Y = theta  %*% coef8 + rnorm(length(theta), 0, 0.1)__\n\nAs in the first method, each of the eight weight measurements has a normal\nerror with standard deviation of 0.1.\n\n__\n\nQuestion 13.6\n\n  1. Check that `coef8` is -up to an overall factor- an orthogonal matrix (\\\\(C^t C = \\lambda\\mathbb{1}\\\\) for some \\\\(\\lambda\\in\\mathbb{R}\\\\)).\n\n  2. Check that if we multiply `theta` with `coef8` times `coef8` transposed and divide by 8, we obtain `theta` again.\n\n__\n\nSolution\n\n__\n\n    \n    \n    coef8 %*% t(coef8)__\n    \n    \n         [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n    [1,]    8    0    0    0    0    0    0    0\n    [2,]    0    8    0    0    0    0    0    0\n    [3,]    0    0    8    0    0    0    0    0\n    [4,]    0    0    0    8    0    0    0    0\n    [5,]    0    0    0    0    8    0    0    0\n    [6,]    0    0    0    0    0    8    0    0\n    [7,]    0    0    0    0    0    0    8    0\n    [8,]    0    0    0    0    0    0    0    8\n    \n    \n    theta %*% coef8 %*% t(coef8) / ncol(coef8)__\n    \n    \n         [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n    [1,] 10.7 13.4 16.4  3.9  8.5   16  1.2  4.4\n\nWe combine these results to estimate `theta` using the orthogonality of\n`coef8`.\n\n    \n    \n    thetahat = Y %*% t(coef8) / ncol(coef8)__\n\nSince we know the true \\\\(\\theta\\\\), we can compute the errors and their sum\nof squares.\n\n    \n    \n    errors2 = as.vector(thetahat) - theta\n    errors2 __\n    \n    \n    [1] -0.005213746  0.025216488  0.003201562  0.033880188 -0.029459127\n    [6] -0.043173774  0.083202870 -0.025818188\n    \n    \n    sum(errors2^2)__\n    \n    \n    [1] 0.01214228\n\nWe see that the sum of squares here is substantially smaller than that of the\nfirst procedure. Were we just lucky?\n\n__\n\nQuestion 13.7\n\n  1. Repeat the above experiment B = 10000 times, each time using a different `theta`, and look at the sampling distributions of sum of squared errors in both schemes.\n\n  2. What do you think the relationship between the two variances is?\n\n__\n\nSolution\n\n__\n\n    \n    \n    B  = 10000\n    tc = t(coef8) / ncol(coef8)\n    sse = replicate(B, {\n      theta = round((2 * sample(8, 8)) + rnorm(8), 1)\n      X = theta + rnorm(length(theta), 0, 0.1)\n      err1 = sum((X - theta)^2)\n      Y = coef8 %*% theta + rnorm(length(theta), 0, 0.1)\n      thetahat = tc %*% Y\n      err2 = sum((thetahat - theta)^2)\n      c(err1, err2)\n    })\n    rowMeans(sse)__\n    \n    \n    [1] 0.079591221 0.009954419\n    \n    \n    ggplot(tibble(lr = log2(sse[1, ] / sse[2, ])), aes(x = lr)) +\n      geom_histogram(bins = 50) +\n      geom_vline(xintercept = log2(8), col = \"orange\") +\n      xlab(\"log2 ratio of SSE, Method 1 vs 2\")__\n\n[![](13-chap_files/figure-html/fig-logsseratios-1.png)](13-chap_files/figure-\nhtml/fig-logsseratios-1.png \"Figure 13.7: Logarithm \\(base 2\\) of the ratios\nof sum of squared error for the two methods. The vertical orange line\ncorresponds to 8.\")\n\nFigure 13.7: Logarithm (base 2) of the ratios of sum of squared error for the\ntwo methods. The vertical orange line corresponds to 8.\n\nThe second scheme is more **efficient** than the first by a factor of 8\nbecause the errors generated by the measurement have a sum of squares that is\n8 times lower (Figure 13.7).\n\nThis example shows us that when several quantities are to be ascertained there\nis an opportunity to increase the accuracy and reduce the cost by combining\nmeasurements in one experiment and making comparisons between similar groups.\n\nIbn Sina’s rule that an optimal design can only vary one factor at a time was\nsuperseded in the 20th century by RA Fisher. He realized that one could modify\nthe factors in combinations and still come to a conclusion—sometimes, an even\nbetter conclusion, as in the weighing example—as long as the contrasts were\ncarefully designed.\n\n[![](13-chap_files/figure-html/fig-blockbox-1.png)](13-chap_files/figure-\nhtml/fig-blockbox-1.png \"Figure 13.8: On the left, two samples each of size 6\nare being compared. On the right, the same data are shown, but colored by the\ntime of data collection. We note a tendency of the data to fall into blocks\naccording to these times. Because of this, comparison between the groups is\ndiluted. This effect can be mitigated by comparing within times, i.,e., by\nblocking into three groups. Paired analysis, such as demonstrated in Questions\nwrn-design-paired—wrn-design-powerPairedUnpaired, is a special case of\nblocking.\")\n\nFigure 13.8: On the left, two samples each of size 6 are being compared. On\nthe right, the same data are shown, but colored by the time of data\ncollection. We note a tendency of the data to fall into blocks according to\nthese times. Because of this, comparison between the groups is diluted. This\neffect can be mitigated by comparing within times, i.,e., by blocking into\nthree groups. Paired analysis, such as demonstrated in Questions 13.8—13.10,\nis a special case of blocking.\n\n### 13.4.4 Blocking and pairing\n\nDarwin suspected that corn growth is affected by the composition of the soil\nand the humidity in the pots. For this reason, when he wanted to compare\nplants grown from cross-pollinated seeds to plants grown from self-pollinated\nseeds, he planted one seedling of each type in each of 15 pots. Each pot in\nDarwin’s _Zea Mays_ experiment is a block, only the factor of interest\n(pollination method), called the **treatment** , is different within each\nblock (Figure 13.9).\n\n[![](imgs/maizeDarwin.png)](imgs/maizeDarwin.png \"Figure 13.9: A paired\nexperiment is the simplest case of blocking.\")\n\nFigure 13.9: A paired experiment is the simplest case of blocking.\n\nIn fact, RA Fisher criticized Darwin’s experiment because he systematically\nput the cross-pollinated plants on the same side of the pot. This could have\ninduced confounding of a “side” effect with the cross effect, if one side of\nthe pot received more sunlight for instance. It would have been preferable to\nrandomize the side of the pot, e.,g., by flipping a coin.\n\n_Block what you can, randomize what you cannot._  \n(George Box, 1978)\n\n#### Comparing a paired versus an unpaired design\n\nWhen comparing various possible designs, we do **power simulations** similar\nto what we saw in [Chapter 1](01-chap.html). Let’s suppose the sample size is\n15 in each group and the **effect size** is 0.2. We also need to make\nassumptions about the standard deviations of the measurements, here we suppose\nboth groups have the same sd=0.25 and simulate data:\n\n    \n    \n    n = 15\n    effect = 0.2\n    pots   = rnorm(n, 0, 1)\n    noiseh = rnorm(n, 0, 0.25)\n    noisea = rnorm(n, 0, 0.25)\n    hybrid = pots + effect + noiseh\n    autoz  = pots + noisea __\n\n__\n\nQuestion 13.8\n\nPerform both a simple \\\\(t\\\\)-test and a paired \\\\(t\\\\)-test. Which is more\npowerful in this case?\n\n__\n\nSolution\n\n__\n\n    \n    \n    t.test(hybrid, autoz, paired = FALSE)__\n    \n    \n        Welch Two Sample t-test\n    \n    data:  hybrid and autoz\n    t = 0.77183, df = 26.012, p-value = 0.4472\n    alternative hypothesis: true difference in means is not equal to 0\n    95 percent confidence interval:\n     -0.3145706  0.6928591\n    sample estimates:\n    mean of x mean of y \n    0.5073519 0.3182076 \n    \n    \n    t.test(hybrid, autoz, paired = TRUE)__\n    \n    \n        Paired t-test\n    \n    data:  hybrid and autoz\n    t = 1.8783, df = 14, p-value = 0.08133\n    alternative hypothesis: true mean difference is not equal to 0\n    95 percent confidence interval:\n     -0.02683705  0.40512561\n    sample estimates:\n    mean difference \n          0.1891443 \n\nMaybe we were just lucky with our simulated data here?\n\n__\n\nQuestion 13.9\n\nCheck which method is generally more powerful. Repeat the above computations\n\\\\(1000\\\\) times and compute the average probability of rejection for these\n1000 trials, using a false positive rate \\\\(\\alpha=0.05\\\\).\n\n__\n\nSolution\n\n__\n\n    \n    \n    B     = 1000\n    alpha = 0.05\n    what  = c(FALSE, TRUE)\n    pvs = replicate(B, {\n      pots   = rnorm(n, 0, 1)\n      noiseh = rnorm(n, 0, 0.25)\n      noisea = rnorm(n, 0, 0.25)\n      hybrid = pots + effect + noiseh\n      autoz  = pots + noisea\n      vapply(what,\n        function(paired)\n          t.test(hybrid, autoz, paired = paired)$p.value,\n        double(1)) |> setNames(paste(what))\n    })\n    rowMeans(pvs <= alpha)__\n    \n    \n    FALSE  TRUE \n    0.000 0.532 \n\nWe can compare the p-values obtained using both methods (Figure 13.10).\n\n    \n    \n    tidyr::pivot_longer(as.data.frame(t(pvs)), cols = everything(), names_to = \"paired\") |>\n      ggplot(aes(x = value, fill = paired)) +\n      geom_histogram(binwidth = 0.01, boundary = 0, alpha = 1/3)__\n\n[![](13-chap_files/figure-html/fig-\npvaluescompare-1-1.png)](13-chap_files/figure-html/fig-pvaluescompare-1-1.png\n\"Figure 13.10: Results from the power calculation, comparing the p-value\ndistributions from the ordinary unpaired and the paired t-test.\")\n\nFigure 13.10: Results from the power calculation, comparing the p-value\ndistributions from the ordinary unpaired and the paired \\\\(t\\\\)-test.\n\n__\n\nQuestion 13.10\n\n  * Write a function that compares the power of the two types of tests for different values of the effect size, sample size, size of the pot effects (as measured by their standard deviation), noise standard deviation and sample size.\n\n  * Use your function to find out which of the standard deviations (pots or noise) has the largest effect on the improvement produced by pairing for \\\\(n=15\\\\).\n\n  * How big should \\\\(n\\\\) be to attain a power of 80% if the two standard deviations are both 0.5?\n\n__\n\nSolution\n\n__\n\n    \n    \n    powercomparison = function(effect = 0.2, n = 15, alpha = 0.05,\n                    sdnoise, sdpots, B = 1000) {\n      what = c(FALSE, TRUE)\n      pvs = replicate(B, {\n        pots   = rnorm(n, 0, sdpots)\n        noiseh = rnorm(n, 0, sdnoise)\n        noisea = rnorm(n, 0, sdnoise)\n        hybrid = pots + effect + noiseh\n        autoz  = pots + noisea\n        vapply(what,\n          function(paired)\n            t.test(hybrid, autoz, paired = paired)$p.value,\n          double(1)) |> setNames(paste(what))\n      })\n      rowMeans(pvs <= alpha)\n    }__\n\nHere are a few simulations showing that when the pot effects are small\ncompared to the noise standard deviation, pairing hardly makes a difference.\nIf the pot effects are large, then pairing does make a big difference.\n\n    \n    \n    powercomparison(sdpots = 0.5,  sdnoise = 0.25)__\n    \n    \n    FALSE  TRUE \n    0.034 0.533 \n    \n    \n    powercomparison(sdpots = 0.25, sdnoise = 0.25)__\n    \n    \n    FALSE  TRUE \n    0.242 0.524 \n    \n    \n    powercomparison(sdpots = 0.1,  sdnoise = 0.25)__\n    \n    \n    FALSE  TRUE \n    0.510 0.534 \n\nFor 100 plants of each type and both standard deviation at 0.5, the power of\nthe paired test is about 80%.\n\n    \n    \n    powercomparison(sdpots = 0.5, sdnoise = 0.5, n = 100)__\n    \n    \n    FALSE  TRUE \n    0.513 0.796 \n\n__\n\nQuestion 13.11\n\n**Paired designs** take into account a natural pairing of the observations —\nfor instance, twin studies, or studies of patients before and after a\ntreatment. What can be done when pairing is not available?\n\n__\n\nSolution\n\n__\n\n**Matched designs** try to create pairs of subjects that have as much\nsimilarity as possible through matching age, sex, background health etc. One\nis treated, the other serves as a control.\n\nA **balanced design** is an experimental design where all the different factor\ncombinations have the same number of observation replicates. The effect of\neach factor is identifiable. If there are nuisance factors, it is good to make\nsure they are balanced with the factors of interest. Sometimes this is\ninconvenient or impractical for logistic or economic reasons – but in such\ncases analysts are on thin ice and need to proceed with caution.\n\n#### Randomization\n\nOften we don’t know which nuisance factors will be important, or we cannot\nplan for them ahead of time. In such cases, randomization is a practical\nstrategy: at least in the limit of large enough sample size, the effect of any\nnuisance factor should average out.\n\nRandomization can also help reduce unconscious bias. For instance, if the\nsamples from one of the groups are extremely hard to come by, we might be\ntempted to be extra careful when handling them, compared to samples from the\nother groups. Unfortunately this might bias the measurement outcomes and thus\ninvalidate the comparison. See Senn ([2004](16-chap.html#ref-\nsenn2004randomization)) for an extensive discussion of some of the pitfalls\nthat occur when trying to improve on simple randomization.\n\n### 13.4.5 How many replicates do I need?\n\n[![Beware of underpowered me-too studies.](imgs/devil.png)](imgs/devil.png\n\"Beware of underpowered me-too studies.\")\n\nBeware of underpowered me-too studies.\n\nIn [Section 1.4.1](01-chap.html#sec-generative-SimulatingForPower) we showed a\nsimulation experiment calculating how many nucleotides were necessary to\nachieve a 80% true positive rate, given that we knew the alternative. Now,\nrecall the discussion of experiments versus studies from Section 13.2. For the\ncell line experiment, we might get the correct result already from one\nreplicate; usually we’ll do two or three to be sure. On the other hand, for a\nstudy comparing the effect of two alternative drugs on patients, our intuition\ntells us that there is so much uncontrolled variability that we’ll likely need\ndozens (if not more) patients until we can be sure about the result. The\nnumber of replicates needed is highly context specific. It depends on the\namount of uncontrolled variability and the **effect size**. A pragmatic\napproach is to check out previous successful (or unsuccessful) experiments or\nstudies that did something comparable and use simulations, subsampling or\nbootstrapping to get an estimate of the planned study’s power.\n\n#### Power depends on sample sizes, effect sizes and variability.\n\n[![](imgs/African_Bush_Elephant.jpg)](imgs/African_Bush_Elephant.jpg\n\"Figure 13.11: The elephant in the room with power calculations is the effect\nsize. Especially in ’omics studies, when we are screening thousands of genes\n\\(or other features\\) for differences, we rarely have a precise idea of what\neffect size to expect. However, even so, power calculations are useful for\norder-of-magnitude calculations, or for qualitative comparisons such as shown\nin this section for paired versus unpaired tests. Source: Wikimedia CH.\")\n\nFigure 13.11: The elephant in the room with power calculations is the effect\nsize. Especially in ’omics studies, when we are screening thousands of genes\n(or other features) for differences, we rarely have a precise idea of what\neffect size to expect. However, even so, power calculations are useful for\norder-of-magnitude calculations, or for qualitative comparisons such as shown\nin this section for paired versus unpaired tests. Source: [Wikimedia\nCH](https://en.wikipedia.org/wiki/Elephant).\n\nThe package **[pwr](https://cran.r-project.org/web/packages/pwr/)** provides\nfunctions for doing the standard **power calculations**. There are always four\nquantities involved in these computations: sample size, effect size,\nsignificance level (false positive rate) and the power itself which is the\nprobability of rejecting a hypothesis when you should (true positive rate).\nThe functions `pwr.2p.test`, `pwr.chisq.test`, `pwr.f2.test` provide the\ncalculations for tests of two proportions, the chisquared test and general\nlinear tests respectively.\n\nHere is an example of the power calculcation for a two sample \\\\(t\\\\)-test\nwith \\\\(n=15\\\\). The function requires several arguments:\n\n    \n    \n    library(\"pwr\")\n    str(pwr.t.test)__\n    \n    \n    function (n = NULL, d = NULL, sig.level = 0.05, power = NULL, type = c(\"two.sample\", \n        \"one.sample\", \"paired\"), alternative = c(\"two.sided\", \"less\", \"greater\"))  \n\nIf you call the function with a value for power and effect size, it will\nreturn the sample size needed, or if you specify the sample size and effect\nsize, it returns the power.\n\n    \n    \n    pwr.t.test(n = 15, d = 0.4, sig.level = 0.05, type = \"two.sample\")__\n    \n    \n         Two-sample t test power calculation \n    \n                  n = 15\n                  d = 0.4\n          sig.level = 0.05\n              power = 0.1848496\n        alternative = two.sided\n    \n    NOTE: n is number in *each* group\n    \n    \n    pwr.t.test(n = 15, d = 0.4, sig.level = 0.05, type = \"paired\")__\n    \n    \n         Paired t test power calculation \n    \n                  n = 15\n                  d = 0.4\n          sig.level = 0.05\n              power = 0.3031649\n        alternative = two.sided\n    \n    NOTE: n is number of *pairs*\n\nIf we want to know what sample size would be required to detect a given effect\nsize:\n\n    \n    \n    pwr.t.test(d = 0.4, sig.level = 0.05, type = \"two.sample\", power=0.8)__\n    \n    \n         Two-sample t test power calculation \n    \n                  n = 99.08032\n                  d = 0.4\n          sig.level = 0.05\n              power = 0.8\n        alternative = two.sided\n    \n    NOTE: n is number in *each* group\n    \n    \n    pwr.t.test(d = 0.4, sig.level = 0.05, type = \"paired\", power=0.8)__\n    \n    \n         Paired t test power calculation \n    \n                  n = 51.00945\n                  d = 0.4\n          sig.level = 0.05\n              power = 0.8\n        alternative = two.sided\n    \n    NOTE: n is number of *pairs*\n\nWe see that we would need about twice as many observations for the same power\nwhen not using a paired test.\n\n#### Effective sample size\n\nA sample of independent observations is more informative than the same number\nof dependent observations. Suppose you want to do an opinion poll by knocking\nat people’s doors and asking them a question. In the first scenario, you pick\n\\\\(n\\\\) people at \\\\(n\\\\) random places throughout the country. In the second\nscenario, to save travel time, you pick \\\\(n/3\\\\) random places and then at\neach of these interview three people who live next door to each other. In both\ncases, the number of people polled is \\\\(n\\\\), but if we assume that people\nliving in the same neighborhood are more likely to have the same opinion, the\ndata from the second scenario are (positively) correlated. To explore this,\nlet’s do a simulation.\n\n    \n    \n    doPoll = function(n = 100, numPeoplePolled = 12) {\n      opinion = sort(rnorm(n))\n      i1 = sample(n, numPeoplePolled)\n      i2 = sample(seq(3, n, by = 3), numPeoplePolled / 3)\n      i2 = c(i2, i2 - 1, i2 - 2)\n      c(independent = mean(opinion[i1]), correlated = mean(opinion[i2]))\n    }\n    responses = replicate(5000, doPoll())\n    \n    tidyr::pivot_longer(as.data.frame(t(responses)), \n            cols = everything(), names_to = \"design\") |>\n    ggplot(aes(x = value, col = design)) + geom_density() +\n      geom_vline(xintercept = 0) + xlab(\"Opinion poll result\")__\n\n[![](13-chap_files/figure-html/fig-effective-sample-size-\nsim-1-1.png)](13-chap_files/figure-html/fig-effective-sample-size-sim-1-1.png\n\"Figure 13.12: Density estimates for the polling result using the two sampling\nmethods. The correlated method has higher spread. The truth is indicated by\nthe vertical line.\")\n\nFigure 13.12: Density estimates for the polling result using the two sampling\nmethods. The correlated method has higher spread. The truth is indicated by\nthe vertical line.\n\nThere are 100 people in the country, of which in the first approach (`i1`) we\nrandomly sample 12. In the second approach, we sample 4 people as well as two\nneighbors for each (`i2`). The “opinion” in our case is a real number,\nnormally distributed in the population with mean 0 and standard deviation 1.\nWe model the spatio-sociological structure of our country by sorting the\nhouses from most negative to most positive opinion in the first line of the\n`doPoll` function. The output is shown in Figure 13.12.\n\n## 13.5 Mean-variance relationships and variance-stabilizing transformations\n\nIn Chapters [4](04-chap.html) and [8](08-chap.html) we saw examples for data\ntransformations that compress or stretch the space of quantitative\nmeasurements in such a way that the measurements’ variance is more similar\nthroughout. Thus the variance between replicate measurements is no longer\nhighly dependent on the mean value.\n\nThe mean-variance relationship of our data _before_ transformation can in\nprinciple be any function, but in many cases, the following prototypic\nrelationships are found, at least approximately:\n\n  1. constant: the variance is independent of the mean, \\\\(v(m)=c\\\\).\n\n  2. Poisson: the variance is proportional to to the mean, \\\\(v(m)=am\\\\).\n\n  3. quadratic: the standard deviation is proportional to the mean, therefore the variance grows quadratically, \\\\(v(m)=bm^2\\\\).\n\nHere \\\\(v(m)\\\\) is the function that describes the trend of the variance\n\\\\(v\\\\) as a function of the mean \\\\(m\\\\). The real numbers \\\\(a, b, c\\ge0\\\\)\nparameterize factors affecting the variance besides the mean.\n\n__\n\nQuestion 13.12\n\nGive examples for biological assays or measurement technologies whose data\nshow these types of mean-variance relationships.\n\nReal data can also be affected by a combination of these. For instance, with\nDNA microarrays, the fluorescence intensities are subject to a combination of\nbackground noise that is largely independent of the signal, and multiplicative\nnoise whose standard deviation is proportional to the signal ([Rocke and\nDurbin 2001](16-chap.html#ref-RockeDurbin:2001)). Therefore, the mean-variance\nrelationship is \\\\(v(m)=bm^2+c\\\\). For bright spots (large \\\\(m\\\\)), the\nmultiplicative noise dominates (\\\\(bm^2\\\\)), whereas for faint ones, the\nbackground \\\\(c\\\\).\n\n__\n\nQuestion 13.13\n\nWhat is the point of applying a variance-stabilizing transformation?\n\n__\n\nSolution\n\n__\n\nAnalyzing the data on the transformed scale tends to:\n\n  * Improve visualization, since the physical space on the plot is used more “fairly” throughout the range of the data. A similar argument applies to the color space in the case of a heatmap.\n\n  * Improve the outcome of ordination methods such as PCA or clustering based on correlation, as the results are not so much dominated by the signal from a few very highly expressed genes, but more uniformly from many genes throughout the dynamic range.\n\n  * Improve the estimates and inference from statistical models that are based on assuming identically distributed (and hence, homoskedastic) noise.\n\n## 13.6 Data quality assessment and quality control\n\nWe distinguish between data quality assessment (QA) –steps taken to measure\nand monitor data quality– and quality control (QC) –removing bad data. These\nactivities pervade all phases of an analysis, from assembling the raw data\nover transformation, summarization, model fitting, hypothesis testing or\nscreening for “hits” to interpretation. QA-related questions include:\n\n  * How do the marginal distributions of the variables look (histograms, ECDF plots)?\n\n  * How do their joint distributions look (scatter plots, pairs plot)?\n\n  * How well do replicates agree (as compared to different biological conditions)? Are the magnitudes of the differences between several conditions plausible?\n\n  * Is there evidence of batch effects? These could be of a categorical (stepwise) or continuous (gradual) nature, e.g.. due to changes in experimental reagents, protocols or environmental factors. Factors associated with such effects may be explicitly known, or unkown and latent , and often they are somewhere in between (e.g., when a measurement apparatus slowly degrades over time, and we have recorded the times, but don’t really know exactly at what time the degradation is how bad).\n\nFor the last two sets of questions, heatmaps, principal component plots and\nother ordination plots (as we have seen in Chapters [7](07-chap.html) and\n[9](09-chap.html)) are useful.\n\n[![](imgs/1896_Ford_Quadricycle.jpeg)](imgs/1896_Ford_Quadricycle.jpeg\n\"Figure 13.13: Henry Ford’s \\(possibly apocryphal\\) quote: “If I had asked\npeople what they wanted, they would have said faster horses.” expresses the\nview of quality as fitness for purpose, versus adherence to specifications.\n\\(Source: Ford\\)\")\n\nFigure 13.13: Henry Ford’s (possibly apocryphal) quote: “If I had asked people\nwhat they wanted, they would have said faster horses.” expresses the view of\nquality as **fitness for purpose** , versus adherence to specifications.\n([Source: Ford](https://corporate.ford.com/history.html))\n\nIt’s not easy to define **quality** , and the word is used with many meanings.\nThe most pertinent for us is **fitness for purpose** 4, and this contrasts to\nother definitions of quality that are based on normative specifications. For\ninstance, in differential expression analysis with RNA-Seq data, our purpose\nmay be the detection of differentially expressed genes between two biological\nconditions. We can check specifications such as the number of reads, read\nlength, base calling quality, fraction of aligned reads, but ultimately these\nmeasures in isolation have little bearing on our purpose. More to the point\nwill be the identification of samples that are not behaving as expected, e.g.,\nbecause of a sample swap or degradation; or genes that were not measured\nproperly. We saw an example for this in [Section 8.10.3](08-chap.html#sec-\ncountdata-dealingCooks). Useful plots include ordination plots, such as\n[Figure 8.6](08-chap.html#fig-countdata-PCA), and heatmaps, such as [Figure\n8.7](08-chap.html#fig-figHeatmap-1). A **quality metric** is any value that we\nuse to measure quality, and having explicit quality metrics helps automating\nQA/QC.\n\n4 <http://en.wikipedia.org/wiki/Quality_%28business%29>\n\n## 13.7 Longitudinal data\n\n**Longitudinal data** 5 have time as a covariate. The first question is\nwhether we are looking at a handful of time points –say, the response of a\ncell line measured 48h, 72h and 96h after exposure to a drug– or a long and\ndensely sampled time series –say, patch clamp data in electrophysiology or a\nmovie from life cell microscopy.\n\n5 A related but different concept is _survival data_ , where time is the\noutcome variable.\n\nIn the first case, time is usually best thought of as just another discrete\nexperimental factor. Perhaps the multiple time points were chosen because the\nexperimenter was not sure which one would give the most useful results. One\ncan then try to identify the best time point and focus on that. Depending on\nthe data, the other time points could serve for validation, as “more-or-less”\nreplicates. When designing the experiment, we’ll try to cover those time\nperiods more densely when we expect most to happen, e.g., directly after a\nperturbation.\n\nIn a screening context, we can ask whether there is any effect at all,\nregardless of which time point and which shape, using something like an\n\\\\(F\\\\)-test. We then just need to make sure that we account for the\ndependencies between the measurements at the different time points and\ndetermine the null distribution accordingly.\n\nIn the second case, with time series, we may want to fit dynamical models to\nthe data. We can write \\\\(X(t)\\\\) for the _state_ of our system at time\n\\\\(t\\\\), and we have many choices, depending on whether\n\n  * \\\\(X\\\\) is continuous or discrete,\n\n  * the _dynamics_ of \\\\(X\\\\)6 are deterministic or stochastic,\n\n  * the dynamics are smooth and/or jumpy,\n\n  * we observe \\\\(X\\\\) directly or only some noisy and/or reduced version \\\\(Y = g(X)+\\varepsilon\\\\)7 of it.\n\n6 The value of \\\\(X(t+\\Delta t)\\\\), given \\\\(X(t)\\\\), in other words, the\ntemporal evolution\n\n7 Here \\\\(g\\\\) denotes a function that looses information, e.g., by dropping\nsome of the variables of a vector-valued \\\\(X\\\\), and \\\\(\\varepsilon\\\\) is a\nnoise term.\n\nWe have many modeling tools at hand, including\n\n  * Markov Models: discrete state space; the dynamics are stochastic and occur by jumping between states.\n\n  * Ordinary or partial differential equations: continuous state space; the dynamics are deterministic and smooth and are described by a differential equation, possibly derived from first principles rooted in physics or chemistry.\n\n  * Master equation, Fokker-Planck equation: the dynamics are stochastic and are described by (partial) differential equations for the probability distribution of \\\\(X\\\\) in space and time.\n\n  * Piece-wise deterministic stochastic processes: a combination of the above, samples from the process involve deterministic, smooth movements as well as occasional jumps.\n\nIf we don’t observe \\\\(X\\\\) directly, but only a noisy and/or summarized\nversion \\\\(Y\\\\), then in the case of Markov models, the formalism of **Hidden\nMarkov Models** ([Durbin et al. 1998](16-chap.html#ref-DEKM)) makes it\nrelatively straightforward to fit such models. For the other types of\nprocesses, analogous approaches are possible, but these are technically more\ndemanding, and we refer to specialized literature.\n\nTaking a more data-driven (rather than model-driven) view, methods for\nanalyzing time series data include:\n\n  * Non-parametric smoothing followed by clustering or classification into prototypic shapes\n\n  * Change point detection\n\n  * Autoregressive models\n\n  * Fourier and wavelet decomposition\n\nIt’s outside the scope of this book to go into details, and there is a huge\nnumber of choices8. Many methods originated in physics, econometrics or signal\nprocessing, so it’s worthwhile to scan the literature in these fields.\n\n8 One start point is the CRAN taskview\n<https://cran.r-project.org/web/views/TimeSeries.html>.\n\n## 13.8 Data integration: use everything you (could) know\n\n[![Don’t pretend you are dumb.](imgs/devil.png)](imgs/devil.png \"Don’t pretend\nyou are dumb.\")\n\nDon’t pretend you are dumb.\n\nThere is an attraction to seemingly “unbiased” approaches that analyse the\ndata at hand without reference to what is already known. Such tendencies are\nreinforced by the fact that statistical methods have often been developed to\nbe generic and self-contained, for instance, to work of a general matrix\nwithout specific reference to what the rows and column mean in an application,\nor what other, more or less relevant data might be around.\n\nGeneric approaches are a good way to get started, and for analyses that are\nstraightforward and highly powered, such an approach might work out. But\noften, it is wasteful. Recall the example of an RNA-Seq experiment for\ndifferential expression. As we saw in Chapters [6](06-chap.html) and\n[8](08-chap.html), we could perform a hypothesis test for each recorded gene,\nregardless of its signal strength9 or anything else, and then run a multiple\ntesting method that treats all tests the same (i.e., as exchangeable). But\nthis is inefficient: we can improve our detection power by filtering out or\ndownweighting hypotheses with lower power or with higher prior probability\n\\\\(\\pi_0\\\\) of being true.\n\n9 i.e., average read counts\n\nSimilarly, in the interpretation of single p-values, we don’t need to ignore\neverything else we know, and for instance, blindly stick to an arbitrary 5%\ncutoff no matter what, but rather, we can let prior knowledge on the test’s\npower and on \\\\(\\pi_0\\\\) guide our interpretation ([Altman and Krzywinski\n2017](16-chap.html#ref-Altman:PoS:2017)).\n\nOther potential examples of misplaced objectivity include:\n\n  * Penalization or feature selection in high-dimensional regression or classification. It is easy to use schemes that treat all features the same, for instance, standardize all of them to zero mean and unit variance. But sometime we know that some classes of features are likely to be more or less informative than others ([Wiel et al. 2016](16-chap.html#ref-Wiel:StatMed:2016)). We can also use graphs or networks to represent “other” data and use approaches like the group or graph lasso ([Jacob, Obozinski, and Vert 2009](16-chap.html#ref-Jacob:GroupLasso:2009)) to structure your penalties in high-dimensional modeling.\n\n  * Unsupervised clustering of our objects of interest (samples, genes or sequences) and subsequent search for over-represented annotations. We can be better off by incorporating the different uncertainties with which these were measured as well as their different frequencies into the clustering algorithm. We can use probabilities and similarities to check whether the members of clusters are more similar than two randomly picked objects ([Callahan et al. 2016](16-chap.html#ref-dada2)).\n\nWhen embarking on an analysis, it’s important to anticipate that rarely we’ll\nbe done by applying a single method and getting a straightforward result. We\nneed to dig out other, related datasets, look for confirmations (or else) of\nour results, get further interpretation. An example is gene set enrichment\nanalysis: after we’ve analyzed our data and found a list of genes that appear\nto be related to our comparison of interest, we’ll overlap them with other\ngene lists, such as those from the [Molecular Signatures\nDatabase](http://software.broadinstitute.org/gsea/msigdb) ([Liberzon et al.\n2011](16-chap.html#ref-MSigDB)) in order to explore the broader biological\nprocesses involved; or we might load up datasets looking at levels of\nregulation10 up-stream or down-stream of ours in search for context.\n\n10 Genome, chromatin state, transcription, mRNA life cycle, translation,\nprotein life cycle, localization and interactions; metabolites, \\\\(...\\\\)\n\n## 13.9 Sharpen your tools: reproducible research\n\nAnalysis projects often begin with a simple script, perhaps to try out a few\ninitial ideas and explore the quality of the pilot data. Then more ideas are\nadded, more data come in, other datasets are integrated, more people become\ninvolved. Eventually the paper needs to be written, figures be done\n‘properly’, and the analysis be saved for the scientific record and to\ndocument its integrity. Here are a few principles that can help with such a\nprocess11.\n\n11 An excellent and very readable outline of good computing practices for\nresearchers, including data management, programming, collaborating with\ncolleagues, organizing projects, tracking work and writing manuscripts, is\ngiven by Wilson et al. ([2017](16-chap.html#ref-Wilson:Goodenough:2017)).\n\n**Use an integrated development environment.** **RStudio** is a great choice;\nthere are also other platforms such as Emacs or Eclipse.\n\n**Use literate programming** tools such as **Rmarkdown** or Jupyter. This is\nmore readable (for yourself and for others) than burying explanations and\nusage instructions in comments in the source code or in separate README files,\nin addition you can directly embed figures and tables in these documents. Such\ndocuments are good starting points for the supplementary material of your\npaper. Moreover, they’re great for reporting analyses to your collaborators.\n**Anticipate re-engineering of the data formats and the software.** The first\nversion of how you represent the data and structure the analysis workflow will\nrarely be capable of supporting the project as it evolves. Don’t be afraid12\nto make a clean cut and redesign as soon as you notice that you are doing a\nlot of awkward data manipulations or repetitive steps. This is time well-\ninvested. Almost always it also helps to unearth bugs.\n\n12 The professionals do it, too: “Most software at Google gets rewritten every\nfew years.” ([Henderson 2017](16-chap.html#ref-\nHenderson:2017:SoftwareEngineeringGoogle))\n\n**Reuse existing tools.** Don’t reinvent the wheel; your time is better spent\non things that are actually new. Before using a self-made “heuristic” or a\ntemporary “short-cut”, spend a couple of minutes researching to see if\nsomething like this hasn’t been done before. More often than not, it has, and\nsometimes there is a clean, scalable and already tested solution.\n\n**Use version control** , such as . This takes time to learn, but this time is\nwell-invested. In the long run it will be infinitely better than all your\nself-grown attempts at managing evolving code with version numbers, switches\nand the like. Moreover, this is the sanest option for collaborative work on\ncode, and it provides an extra backup of your codebase, especially if the\nserver is distinct from your personal computer.\n\n**Use functions** rather than copy-pasting (or repeatedly `source`-ing)\nstretches of code.\n\n**Use the R package system.** Soon you’ll note recurring function or variable\ndefinitions that you want to share between your different scripts. It is fine\nto use the R function `source` to manage them initially, but it is never too\nearly to move them into your own package – at the latest when you find\nyourself starting to write emails or code comments explaining others (or\nyourself) how to use some functionality. Assembling existing code into an R\npackage is not hard, and it offers you many goodies including standardized\nways of documentation, showing code usage examples, code testing, versioning\nand provision to others. And quite likely you’ll soon appreciate the benefits\nof using namespaces.\n\n**Centralize the location of the raw data files and automate the derivation of\nintermediate data.** Store the input data at a centralized file server that is\nprofessionally backed up. Mark the files as read-only. Have a clear and linear\nworkflow for computing the derived data (e.g., normalized, summarized,\ntransformed etc.) from the raw files, and store these in a separate directory.\nAnticipate that this workflow will need to be run several times13, and version\nit. Use the\n**[BiocFileCache](https://bioconductor.org/packages/BiocFileCache/)** package\nto mirror these files on your personal computer14.\n\n13 Always once more than the final, final time before the final data freeze…\n\n14 A more basic alternative is the utility. A popular solution offered by some\norganizations is based on [ownCloud](https://owncloud.org). Commercial options\ninclude Dropbox, Google Drive, and the like.\n\n15 In computer science, the term _data warehouse_ is sometimes used for such a\nconcept.\n\n**Think in terms of cooking recipes and try to automate them.** When\ndeveloping downstream analysis ideas that bring together several different\ndata types, you don’t want to do the conversion from data type specific\nformats into a representation suitable for machine learning or generic\nstatistical method each time anew, on an ad hoc basis. Have a _recipe_ script\nthat assembles the different ingredients and cooks them up as an easily\nconsumable15 matrix, data frame or Bioconductor _SummarizedExperiment_.\n\n**Keep a hyperlinked webpage with an index of all analyses.** This is helpful\nfor collaborators (especially if the page and the analysis can be accessed via\na web browser) and also a good starting point for the methods part of your\npaper. Structure it in chronological or logical order, or a combination of\nboth.\n\n## 13.10 Data representation\n\nGetting data ready for analysis or visualization often involves a lot of\nshuffling until they are in the right shape and format for an analytical\nalgorithm or a graphics routine. As we saw in [Chapter 3](03-chap.html),\n**[ggplot2](https://cran.r-project.org/web/packages/ggplot2/)** likes its data\nin dataframe objects, with one row per measurement record. The reasons behind\nthis choice are well explained in Hadley Wickham’s paper on **tidy data**\n([Wickham 2014](16-chap.html#ref-Wickham:TidyData)).\n\n### 13.10.1 Wide vs long table format\n\nRecall the Hiiragi data (for space reasons we select only four genes, and\nprint only the first five columns of `xwdf`):\n\n    \n    \n    library(\"magrittr\")\n    data(\"x\", package = \"Hiiragi2013\")\n    xwdf = tibble(\n      probe  = c(\"1420085_at\", \"1418863_at\", \"1425463_at\", \"1416967_at\"),\n      symbol = c(      \"Fgf4\",      \"Gata4\",      \"Gata6\",       \"Sox2\"))\n    xwdf %<>% bind_cols(as_tibble(Biobase::exprs(x)[xwdf$probe, ]))\n    dim(xwdf)__\n    \n    \n    [1]   4 103\n    \n    \n    xwdf[, 1:5]__\n    \n    \n    # A tibble: 4 × 5\n      probe      symbol `1 E3.25` `2 E3.25` `3 E3.25`\n      <chr>      <chr>      <dbl>     <dbl>     <dbl>\n    1 1420085_at Fgf4        3.03      9.29      2.94\n    2 1418863_at Gata4       4.84      5.53      4.42\n    3 1425463_at Gata6       5.50      6.16      4.58\n    4 1416967_at Sox2        1.73      9.70      4.16\n\nEach row of this dataframe corresponds to one of the selected genes. The first\ntwo column contain the Affymetrix probe identifier and the gene symbol. The\nremaining 101 columns report the measured expression values, one for each\nsample. The sample identifiers, together with information on the time point\nwhen the sample was taken, are recorded in the column names as a concatenated\nstring. This is an example for a data table in **wide format**. Now let us\ncall the `pivot_longer` function from the\n**[tidyr](https://cran.r-project.org/web/packages/tidyr/)** package and have a\nlook at its output.\n\n    \n    \n    library(\"tidyr\")\n    xldf = pivot_longer(xwdf, cols = !all_of(c(\"probe\", \"symbol\")),\n                              names_to = \"sample\")\n    dim(xldf)__\n    \n    \n    [1] 404   4\n    \n    \n    head(xldf)__\n    \n    \n    # A tibble: 6 × 4\n      probe      symbol sample  value\n      <chr>      <chr>  <chr>   <dbl>\n    1 1420085_at Fgf4   1 E3.25  3.03\n    2 1420085_at Fgf4   2 E3.25  9.29\n    3 1420085_at Fgf4   3 E3.25  2.94\n    4 1420085_at Fgf4   4 E3.25  9.72\n    5 1420085_at Fgf4   5 E3.25  8.92\n    6 1420085_at Fgf4   6 E3.25 11.3 \n\nIn `xldf`, each row corresponds to exactly one of the 404 measured values,\nstored in the column `value`. Then there are additional columns `probe`,\n`symbol` and `sample`, which store the associated covariates. This is an\ninstance of **long format**.\n\nIn `xwdf`, some columns refer to data from all the samples (namely, `probe`\nand `symbol`), whereas other columns (those with the expression measurements)\ncontain information that is sample-specific. We somehow have to “know” this\nwhen interpreting the dataframe. This is what Hadley Wickham calls **untidy\ndata** 16. In contrast, in the tidy dataframe `xldf` each row forms exactly\none observation, its value is in the column named `value`, and all other\ninformation associated with that observation is in the other colums of the\nsame row. If we want to add additional columns, say, Ensembl gene identifiers\nor chromosome locations, we can simply add them. Similarly, if we want to add\ndata from further genes or additional samples, we can simply add the\ncorresponding rows to `xldf`. In either, we can assume that we will not break\nexisting code. This is in contrast to `xwdf`, adding columns might invalidate\nexisting code, as we cannot be sure how it differentiates between data columns\n(with measured values) and covariate columns.\n\n16 [Recall the Anna Karenina principle: there are many different ways for data\nto be untidy.](http://en.wikipedia.org/wiki/Anna_Karenina_principle)\n\nAlso, subsetting by probe identifier, by gene symbol, or by samples, or indeed\nby any other covariate, is straightforward and can always use the same\n`dplyr::filter` syntax. In contrast, for `xwdf`, we need to remember that\nsubsetting samples amounts to column subsetting, whereas subsetting genes to\nrow subsetting.\n\nThe Hiiragi data have another natural wide format representation besides\n`xwdf`: instead of one row per gene and columns for the different samples, we\ncould also have the data in a dataframe with one row per sample and columns\nfor the different genes. Both of these wide representations can be useful. For\ninstance, if we want to produce scatterplots using `ggplot2` of the expression\nvalues of all genes between two samples, or all samples between two genes, we\nneed to use one or the other of the two wide formats.\n\nTo transform from the long format into the wide format (either of them), you\ncan use the `pivot_wider` function from the\n**[tidyr](https://cran.r-project.org/web/packages/tidyr/)** package—the\ncomplement of the `pivot_longer` function that we already used above.\n\n## 13.11 Tidy data – using it wisely\n\nIn tidy data ([Wickham 2014](16-chap.html#ref-Wickham:TidyData)),\n\n  1. each variable forms a column,\n\n  2. each observation forms a row,\n\n  3. each type of observational unit forms a table.\n\nThe success of the [tidyverse](https://www.tidyverse.org) attests to the power\nof its underlying ideas and the quality of its implementation. Much of the\ncode for this book has adopted these ideas and uses the tidyverse.\n\nNevertheless, dataframes in the long format are not a panacea. Here are some\nthings to keep in mind:\n\n**Efficiency and integrity.** Even though there are only 4 probe-gene symbol\nrelationships, we are repeatedly storing them 404 times in the rows of `xldf`.\nIn this instance, the extra storage cost is negligible. In other cases it\ncould be more considerable. More important is the diffusion of information:\nwhen we are given an object like `xldf` and want to know all the probe-gene\nsymbol relationships it uses, we have to gather this information back from the\nmany copies of it in the dataframe; we cannot be sure, without further\nchecking, that the redundant copies of the information are consistent with\neach other; if we want to update the information, we have to change it in many\nplaces. This speaks for workflow designs in which an object like `xldf` is not\nused for long term data storage, but is assembled at a relatively late stage\nof analysis from more normalized17 data containers that contain the primary\ndata objects.\n\n17 Data normalization is the process of organizing a database to reduce\nredundancy and improve integrity; see e.g.\n<https://en.wikipedia.org/wiki/Database_normalization>.\n\n**Lack of contracts and standardization.** When we write a function that\nexpects to work on an object like `xldf`, we have no guarantee that the column\n`probe` does indeed contain valid probe identifiers; nor that such a column\neven exists. There is not even a direct way to express programmatically what\n“an object like `xldf`” means in the tidyverse. Object oriented (OO)\nprogramming, and its incarnation S4 in R, solves such questions. For instance,\nthe above-mentioned checks could be performed by a `validObject` method for a\nsuitably defined class, and the class definition would formalize the notion of\n“an object like `xldf`”. Addressing such issues is behind the object-oriented\ndesign of the data structures in Bioconductor, such as the\n_SummarizedExperiment_ class. Other potentially useful features of OO data\nrepresentations include\n\n  * Abstraction of interface from implementation and encapsulation: the user accesses the data only through defined channels and does not need to see how the data are stored “inside” – which means the inside can be changed and optimized without breaking user-level code.\n\n  * Polymorphism: you can have different functions with the same name, such as `plot` or `filter`, for different classes of objects, and R figures out for you which one to call.\n\n  * Inheritance: you can build up more complex data representations from simpler ones.\n\n  * Reflection and self-documentation: you can send programmatic queries to an object to ask for information about itself.\n\nAll of these make it easier to write high-level code that focuses on the big\npicture functionality rather than on implementation details of the building\nblocks – albeit at the cost of more initial investment in infrastructure and\n“bureaucracy”.\n\n**Data provenance and metadata.** There is no obvious place in an object like\n`xldf` to add information about data provenance, e.g., who performed the\nexperiment, where it was published, where the data were downloaded from or\nwhich version of the data we’re looking at (data bugs exist \\\\(...\\\\)).\nNeither are there any explanations of the columns, such as units and assay\ntype. Again, the data classes in Bioconductor try to address this need.\n\n[![](imgs/leakypipeline.png)](imgs/leakypipeline.png \"Figure 13.14: Sequential\ndata analyses workflows can be leaky. If insufficient information is passed\nfrom one stage to the next, the procedure can end up being suboptimal and\nlosing power.\")\n\nFigure 13.14: Sequential data analyses workflows can be leaky. If insufficient\ninformation is passed from one stage to the next, the procedure can end up\nbeing suboptimal and losing power.\n\n**Matrix-like data.** Many datasets in biology have a natural matrix-like\nstructure, since a number of features (e.g., genes; conventionally the rows of\nthe matrix) were assayed on several samples (conventionally, columns of the\nmatrix). Unrolling the matrix into a long form like `xldf` makes some\noperations (say, PCA, SVD, clustering of features or samples) more awkward.\n\n## 13.12 Leaky pipelines and statistical sufficiency\n\nData analysis pipelines in high-throughput biology often work as ‘funnels’\nthat successively summarise and compress the data. In high-throughput\nsequencing, we may start with microscopy images of a flow cell, perform base\ncalling to derive sequencing reads, then align them to a reference, then only\ncount the aligned reads for each position, summarise positions to genes (or\nother kinds of regions), then “normalize” these numbers by library size to\nmake them comparable across libraries, etc. At each step, we loose\ninformation, yet it is important to make sure we still have enough information\nfor the task at hand18. The problem is particularly acute if we build our data\npipeline from a series of components from separate developers.\n\n18 For instance, for the RNA-Seq differential expression analysis that we saw\nin [Chapter 8](08-chap.html), we needed the actual read counts, not\n“normalized” versions; for some analyses, gene-level summaries might suffice,\nfor others, we’ll want to look at the exon or isoform level.\n\nStatisticians have a concept for whether certain summaries enable the\nreconstruction of all the relevant information in the data: **sufficiency**.\nIn a Bernoulli random experiment with a known number of trials, \\\\(n\\\\), the\nnumber of successes is a sufficient statistic for estimating the probability\nof success \\\\(p\\\\).\n\n__\n\nQuestion 13.14\n\nIn a 4 state Markov chain (A, C, G, T) such as the one we saw in Chapter 13,\nwhat are the sufficient statistics for the estimation of the transition\nprobabilities?\n\nIterative approaches akin to what we saw when we used the EM algorithm can\nsometimes help avoid information loss. For instance, when analyzing mass\nspectroscopy data, a first run guesses at peaks individually for every sample.\nAfter this preliminary spectra-spotting, another iteration allows us to borrow\nstrength from the other samples to spot spectra that may have been overlooked\n(looked like noise) before.\n\n## 13.13 Efficient computing\n\nThe rapid progress in data acquisition technologies leads to ever large\ndatasets, and dealing with these is a challenge. It is tempting to jump right\ninto software technologies that are designed for big data and scalability. But\nusually it is more helpful to first take a step back. Software engineers know\nthe risks of **premature optimization** , or to paraphrase John Tukey19: “A\nslow and clumsy solution to the right problem is worth a good deal more than a\nfast and scalable solution to the wrong problem.” Sometimes, a good strategy\nis to figure out what is the right solution on a subset of the data before\nembarking on the quest for scalability and performance.\n\n19 <http://stats.stackexchange.com/a/744>\n\nIt’s also good to keep in mind the value of your own time, versus CPU time. If\nyou can save some of your time developing code, even at the cost of longer\ncomputations, that can be a worthwhile trade-off.\n\nHaving considered all that, let’s talk about performance. R has a reputation\nfor being slow and wasteful of memory, and that perception is sometimes\ninvoked to motivate choosing other platforms. In some cases, this is\njustified: nobody would advocate writing a short read aligner, or the steering\nlogic of a self-driving car in R. For statistical analyses, however, it is\npossible to write very efficient code using one or more of these concepts:\n\n**Vectorization**. Consider the following alternative choices of computing the\nsame result.\n\n    \n    \n    a = runif(1e6)\n    b = runif(length(a))\n    system.time({\n      z1 = numeric(length(a))\n      for (i in seq(along = a))\n        z1[i] = a[i]^2 * b[i]\n    })__\n    \n    \n       user  system elapsed \n      0.076   0.001   0.076 \n    \n    \n    system.time({\n      z2 = a^2 * b\n    })__\n    \n    \n       user  system elapsed \n      0.003   0.000   0.003 \n    \n    \n    identical(z1, z2)__\n    \n    \n    [1] TRUE\n\nThe vectorized version (`z2`) is many times faster than the explicitly indexed\none (`z1`) and even easier to read. Sometimes, translating an algorithm that\nis formulated with indices is a little harder — say, if there are\n`if`-conditions, or if the computation for index `i` involves results from\nindex `i-1`. Language constructs such as vectorized conditionals with\n`ifelse`, shifting of vectors with functions such as `lead` and `lag` in the\n**[dplyr](https://cran.r-project.org/web/packages/dplyr/)** package, and\ngenerally the infrastructure of\n**[dplyr](https://cran.r-project.org/web/packages/dplyr/)** , which is\ndesigned to express computations on whole dataframes (rather than row by row),\ncan help.\n\n**Parallelization**. Parallelizing computations with R is easy, not least\nbecause it is a functional language in which it is natural to express\ncomputations as functions with explicit input, output, and no side effects.\nThe landscape of R packages and functionality to support parallized computing\nis fast-moving; the [CRAN task view “High-Performance and Parallel\nComputing”](https://cran.r-project.org/web/views/HighPerformanceComputing.html)\nand the package\n**[BiocParallel](https://bioconductor.org/packages/BiocParallel/)** are good\nstarting points.\n\n**Out-of-memory-data and chunking**. Some datasets are too big to load into\nrandom access memory (RAM) and manipulate all at once. Chunking means\nsplitting the data into manageable portions (“chunks”) and then sequentially\nloading each portion from mass storage, computing on it, storing the result\nand removing the portion from RAM before loading the next one. R also offers\ninfrastructure for working with large datasets that are stored on disk in a\nrelational database management systems (the\n**[DBI](https://cran.r-project.org/web/packages/DBI/)** package) or in\n[HDF5](https://support.hdfgroup.org/HDF5) (the\n**[rhdf5](https://bioconductor.org/packages/rhdf5/)** package). The\nBioconductor project provides the class _SummarizedExperiment_ , which can\nstore big data matrices either in RAM or in an HDF5 backend in a manner that\nis transparent to the user of objects of this class.\n\n**Judicious use of lower level languages**. The\n**[Rcpp](https://cran.r-project.org/web/packages/Rcpp/)** package makes it\neasy to write portions of your code in C++ and include them seamlessly within\nyour R code. Many convenient wrappers are provided, such as below the C++\nclass `NumericVector` that wraps the R class _numeric_ vector.\n\n    \n    \n    library(\"Rcpp\")\n    cppFunction(\"\n      NumericVector myfun(NumericVector x, NumericVector y) {\n        int n = x.size();\n        NumericVector out(n);\n        for(int i = 0; i < n; ++i) {\n          out[i] = pow(x[i], 2) * y[i];\n        }\n        return out;\n      }\")\n    z3 = myfun(a, b)\n    identical(z1, z3)__\n    \n    \n    [1] TRUE\n\nIn practice, the above code should also contain a check on the length of `y`.\nHere, we provided the C++ code to\n**[Rcpp](https://cran.r-project.org/web/packages/Rcpp/)** as an R character\nvector, and this is convenient for short injections. For larger functions, you\ncan store the C++ code in an extra file. The idea is, of course, not to write\na lot of code in C++, but only the most time critical parts.\n\n## 13.14 Summary of this chapter\n\nIn this last chapter, we have tried to collect, generalize and sort some of\nthe concepts and ideas that popped up throughout the book, and that can help\nyou design informative experiments or studies and analyze them effectively.\nSome of these ideas are intuitive and natural. Others are perhaps less\nintuitive, such as Hotelling’s weighting example in Section 13.4.3. It\nrequires formal mathematical reasoning. Even when you cannot do an analytical\ncomputation, you might be able to do simulations or compute on existing,\nsimilar data to benchmark different, non-obvious design choices.\n\nYet again other ideas require discipline and foresight: for instance, the\n“dailies” might be easily forgotten or rationalized away in the heat of an\nexperimental campaign, with so many other concerns competing for our time and\nattention. You might get away with skipping on keeping your kitchen tidy or\neating healthily on individual occasions – as a general approach, it is not\nrecommended.\n\nWe emphasized the importance of computing practices. Throughout the book, with\nits quantity of interweaved code and almost all “live” data visualizations, we\nhave seen many examples of how to set up computational analyses. Nevertheless,\nrunning your own analysis on your own data is something very different from\nfollowing the computations in a book – just like reading a cookbook is very\ndifferent from preparing a banquet, or even just one dish. To equip you\nfurther, we highly recommend the resources mentioned in Section 13.15. And we\nwish you good cooking!\n\n## 13.15 Further reading\n\n  * This chapter presented merely a pragmatic and brief introduction to _experimental design_. There are many book-long treatments that offer detailed advice on setting up experiments to _avoid confounding_ and _optimize power_ ([Wu and Hamada 2011](16-chap.html#ref-wu2011experiments); [Box, Hunter, and Hunter 1978](16-chap.html#ref-box1978statistics); [Glass 2007](16-chap.html#ref-glass2007experimental)).\n\n  * We have not scratched the surface of more sophisticated procedures. For instance if you have the possibility of setting up a sequence of experiments that you might stop once you can make a decision, you will need to study **sequential design** ([Lai 2001](16-chap.html#ref-Lai:2001)). Exploring _complex response surfaces_ by choosing “good” starting points and then using successive results to choose further points can be very effective; Box, Draper, et al. ([1987](16-chap.html#ref-Box:1987)) is an invaluable resource.\n\n  * Gentleman et al. ([2004](16-chap.html#ref-Bioconductor)) explain the ideas behind _Bioconductor_ data structures and software design, and Huber et al. ([2015](16-chap.html#ref-Huber:2015)) give an update on how Bioconductor supports collaborative software development for users and developers.\n\n  * **Git and GitHub**. Jenny Bryan’s website [Happy Git and GitHub for the useR](http://happygitwithr.com) is a great introduction to using version control with R.\n\n  * Wickham ([2014](16-chap.html#ref-Wickham:TidyData)) explains the principles of _tidy data_.\n\n  * _Good enough practices_. Wilson et al. ([2017](16-chap.html#ref-Wilson:Goodenough:2017)) give a pragmatic and wise set of recommendations for how to be successful in scientific computing.\n\n  * The manual [Writing R Extensions](https://cran.r-project.org/doc/manuals/r-release/R-exts.html) is the ultimate reference for _R package authoring_. It can be consumed in conjunction with the Bioconductor [package guidelines](https://www.bioconductor.org/developers/package-guidelines).\n\n## 13.16 Exercises\n\n__\n\nExercise 13.1\n\nSet up a simulation experiment to decide how many subjects you need, given\nthat you know your measurements will be affected by noise that follows a\nsymmetric Laplace distribution (infinite mixture of normal distributions as\ndefined in [Chapter 4](04-chap.html)). You will need to set up a table with\ndifferent possible noise levels and effect sizes.\n\n__\n\nExercise 13.2\n\nUse the Bioconductor package\n**[PROPER](https://bioconductor.org/packages/PROPER/)** to decide the number\nof samples for an RNA-Seq experiment, and compare the results to those from\nthe **[RNASeqPower](https://bioconductor.org/packages/RNASeqPower/)**\nBioconductor package.\n\n__\n\nExercise 13.3\n\nCheck out R’s `model.matrix` function. Read its manual page and explore the\nexamples given there.\n\n__\n\nExercise 13.4\n\nGo back to one of your recent data analyses and assemble it into an R package.\n\n__\n\nSolution\n\n__\n\n  * Collect one or more recurrent operations (e.g., plots) into functions and document them with manual pages (you may use **[roxygen2](https://cran.r-project.org/web/packages/roxygen2/)**).\n\n  * Add the dataset under the `data` or `inst/extdata` directories.\n\n  * If it is not already in that format, convert your analysis script to Rmarkdown.\n\n  * Run `R CMD build` and `R CMD check` until all errors and warnings disappear.\n\nA simple intro is given here: <https://hilaryparker.com/2014/04/29/writing-an-\nr-package-from-scratch>, futher details are in the manual [Writing R\nExtensions](https://cran.r-project.org/doc/manuals/r-release/R-exts.html) that\ncomes with every installation of R.\n\n__\n\nExercise 13.5\n\nOpen an account at GitHub and upload your package. Hint: follow the\ninstructions at Jenny Bryan’s [Happy Git and GitHub for the\nuseR](http://happygitwithr.com) site.\n\n__\n\nExercise 13.6\n\nCheck out the _renjin_ project and the **renjin** package. Compare code\ncompiled with `renjin` with native R code, and with code translated into C/C++\nwith **[Rcpp](https://cran.r-project.org/web/packages/Rcpp/)** as above.\n\n__\n\nSolution\n\n__\n\nSee the Gist at\n<https://gist.github.com/wolfganghuber/909e14e45af6888eec384b82682b3766>.\n\n1000 Genomes Project Consortium. 2012. “An Integrated Map of Genetic Variation\nfrom 1,092 Human Genomes.” _Nature_ 491 (7422): 56–65.\n\nAltman, Naomi, and Martin Krzywinski. 2017. “Points of Significance:\nInterpreting p Values.” _Nature Methods_ 14 (3): 213–14.\n<https://doi.org/10.1038/nmeth.4210>.\n\nBacher, Rhonda, and Christina Kendziorski. 2016. “Design and Computational\nAnalysis of Single-Cell RNA-Sequencing Experiments.” _Genome Biology_ 17 (1):\n1.\n\nBox, George EP, Norman Richard Draper, et al. 1987. _Empirical Model-Building\nand Response Surfaces_. Vol. 424. Wiley New York.\n\nBox, George EP, William G Hunter, and J Stuart Hunter. 1978. _Statistics for\nExperimenters: An Introduction to Design, Data Analysis, and Model Building_.\nJohn Wiley & Sons.\n\nCallahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy J\nJohnson, and Susan P Holmes. 2016. “DADA2: High Resolution Sample Inference\nfrom Amplicon Data.” _Nature Methods_ , 1–4.\n\nDiaconis, Persi, Susan Holmes, and Richard Montgomery. 2007. “Dynamical Bias\nin the Coin Toss.” _SIAM Review_ 49 (2): 211–35.\n\nDurbin, Richard, Sean Eddy, Anders Krogh, and Graeme Mitchison. 1998.\n_Biological Sequence Analysis_. Cambridge University Press.\n\nFisher, Ronald Aylmer. 1935. _The Design of Experiments_. Oliver & Boyd.\n\nGentleman, Robert C, Vincent J Carey, Douglas M Bates, Ben Bolstad, Marcel\nDettling, Sandrine Dudoit, Byron Ellis, et al. 2004. “Bioconductor: Open\nSoftware Development for Computational Biology and Bioinformatics.” _Genome\nBiology_ 5 (10): R80. <https://doi.org/10.1186/gb-2004-5-10-r80>.\n\nGlass, David J. 2007. _Experimental Design for Biologists_. Cold Spring Harbor\nLaboratory Press.\n\nHenderson, Fergus. 2017. “Software Engineering at Google.” _ArXiv e-Prints_.\n<https://arxiv.org/abs/1702.01715>.\n\nHotelling, Harold. 1944. “Some Improvements in Weighing and Other Experimental\nTechniques.” _The Annals of Mathematical Statistics_ 15 (3): 297–306.\n\nHuber, Wolfgang, Vincent J Carey, Robert Gentleman, Simon Anders, Marc\nCarlson, Benilton S Carvalho, Hector Corrada Bravo, et al. 2015.\n“Orchestrating High-Throughput Genomic Analysis with Bioconductor.” _Nature\nMethods_ 12 (2): 115–21.\n\nJacob, Laurent, Guillaume Obozinski, and Jean-Philippe Vert. 2009. “Group\nLasso with Overlap and Graph Lasso.” In _Proceedings of the 26th Annual\nInternational Conference on Machine Learning_ , 433–40. ACM.\n\nLai, Tze Leung. 2001. _Sequential Analysis_. Wiley Online Library.\n\nLeek, Jeffrey T, Robert B Scharpf, Héctor Corrada Bravo, David Simcha,\nBenjamin Langmead, W Evan Johnson, Donald Geman, Keith Baggerly, and Rafael A\nIrizarry. 2010. “Tackling the Widespread and Critical Impact of Batch Effects\nin High-Throughput Data.” _Nature Reviews Genetics_ 11 (10): 733–39.\n\nLeek, Jeffrey T., and John D. Storey. 2007. “Capturing heterogeneity in gene\nexpression studies by surrogate variable analysis.” _PLoS Genetics_ 3 (9):\n1724–35.\n\nLiberzon, Arthur, Aravind Subramanian, Reid Pinchback, Helga Thorvaldsdóttir,\nPablo Tamayo, and Jill P Mesirov. 2011. “Molecular Signatures Database\n(MSigDB) 3.0.” _Bioinformatics_ 27 (12): 1739–40.\n\nMead, Roger. 1990. _The Design of Experiments: Statistical Principles for\nPractical Applications_. Cambridge University Press.\n\nMood, Alexander M. 1946. “On Hotelling’s Weighing Problem.” _The Annals of\nMathematical Statistics_ , 432–46.\n\nRocke, David M, and Blythe Durbin. 2001. “A Model for Measurement Error for\nGene Expression Arrays.” _Journal of Computational Biology_ 8 (6): 557–69.\n\nSenn, Stephen. 2004. “Controversies Concerning Randomization and Additivity in\nClinical Trials.” _Statistics in Medicine_ 23: 3729–53.\n\nStegle, O., L. Parts, R. Durbin, and J. Winn. 2010. “A Bayesian framework to\naccount for complex non-genetic factors in gene expression levels greatly\nincreases power in eQTL studies.” _PLoS Computational Biology_ 6 (5):\ne1000770.\n\nStigler, Stephen M. 2016. _The Seven Pillars of Statistical Wisdom_. Harvard\nUniversity Press.\n\nWickham, Hadley. 2014. “Tidy Data.” _Journal of Statistical Software_ 59 (10).\n\nWiel, Mark A, Tonje G Lien, Wina Verlaat, Wessel N Wieringen, and Saskia M\nWilting. 2016. “Better Prediction by Use of Co-Data: Adaptive Group-\nRegularized Ridge Regression.” _Statistics in Medicine_ 35 (3): 368–81.\n\nWilson, Greg, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt,\nand Tracy K. Teal. 2017. “Good Enough Practices in Scientific Computing.”\nEdited by Francis Ouellette. _PLOS Computational Biology_ 13 (6): e1005510.\n<https://doi.org/10.1371/journal.pcbi.1005510>.\n\nWu, CF Jeff, and Michael S Hamada. 2011. _Experiments: Planning, Analysis, and\nOptimization_. Vol. 552. John Wiley & Sons.\n\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["msmb.css"],"toc":true,"output-file":"13-chap.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}