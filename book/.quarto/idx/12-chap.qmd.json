{"title":"12.1 Goals for this chapter","markdown":{"headingText":"12.1 Goals for this chapter","containsRefs":false,"markdown":"![](imgs/BuildWall.png)\n\n![](imgs/EWall.png)\n\nIn a supervised learning setting, we have a yardstick or plumbline to judge\nhow well we are doing: the response itself.\n\nA frequent question in biological and biomedical applications is whether a\nproperty of interest (say, disease type, cell type, the prognosis of a\npatient) can be ‚Äúpredicted‚Äù, given one or more other properties, called the\n**predictors**. Often we are motivated by a situation in which the property to\nbe predicted is unknown (it lies in the future, or is hard to measure), while\nthe predictors are known. The crucial point is that we _learn_ the prediction\nrule from a set of _training data_ in which the property of interest is also\nknown. Once we have the rule, we can either apply it to new data, and make\nactual predictions of unknown outcomes; or we can dissect the rule with the\naim of better understanding the underlying biology.\n\nCompared to unsupervised learning and what we have seen in Chapters\n[5](05-chap.html), [7](07-chap.html) and [9](09-chap.html), where we do not\nknow what we are looking for or how to decide whether our result is ‚Äúright‚Äù,\nwe are on much more solid ground with supervised learning: the objective is\nclearly stated, and there are straightforward criteria to measure how well we\nare doing.\n\nThe central issues in **supervised learning** 1 are **overfitting** and\n**generalizability** : did we just learn the training data ‚Äúby heart‚Äù by\nconstructing a rule that has 100% accuracy on the training data, but would\nperform poorly on any new data? Or did our rule indeed pick up some of the\npertinent patterns in the system being studied, which will also apply to yet\nunseen new data? (Figure 12.1)\n\n1 Sometimes the term **statistical learning** is used, more or less\nexchangeably.\n\n[![](12-chap_files/figure-html/fig-overfitting-1-1.png)](12-chap_files/figure-\nhtml/fig-overfitting-1-1.png \"Figure¬†12.1: An example for overfitting: two\nregression lines are fit to data in the \\(x, y\\)-plane \\(black points\\). We\ncan think of such a line as a rule that predicts the y-value, given an\nx-value. Both lines are smooth, but the fits differ in what is called their\nbandwidth, which intuitively can be interpreted their stiffness. The blue line\nseems overly keen to follow minor wiggles in the data, while the orange line\ncaptures the general trend but is less detailed. The effective number of\nparameters needed to describe the blue line is much higher than for the orange\nline. Also, if we were to obtain additional data, it is likely that the blue\nline would do a worse job than the orange line in modeling the new data. We‚Äôll\nformalize these concepts ‚Äìtraining error and test set error‚Äì later in this\nchapter. Although exemplified here with line fitting, the concept applies more\ngenerally to prediction models.\")\n\nFigure 12.1: An example for **overfitting** : two regression lines are fit to\ndata in the \\\\((x, y)\\\\)-plane (black points). We can think of such a line as\na rule that predicts the \\\\(y\\\\)-value, given an \\\\(x\\\\)-value. Both lines are\nsmooth, but the fits differ in what is called their **bandwidth** , which\nintuitively can be interpreted their stiffness. The blue line seems overly\nkeen to follow minor wiggles in the data, while the orange line captures the\ngeneral trend but is less detailed. The effective number of parameters needed\nto describe the blue line is much higher than for the orange line. Also, if we\nwere to obtain additional data, it is likely that the blue line would do a\n**worse** job than the orange line in modeling the new data. We‚Äôll formalize\nthese concepts ‚Äìtraining error and test set error‚Äì later in this chapter.\nAlthough exemplified here with line fitting, the concept applies more\ngenerally to prediction models.\n\n\nIn this chapter we will:\n\n  * See exemplary applications that motivate the use of supervised learning methods.\n\n  * Learn what discriminant analysis does.\n\n  * Define measures of performance.\n\n  * Encounter the curse of dimensionality and see what overfitting is.\n\n  * Find out about regularization ‚Äì in particular, penalization ‚Äì and understand the concepts of generalizability and model complexity.\n\n  * See how to use cross-validation to tune parameters of algorithms.\n\n  * Discuss method hacking.\n\n## 12.2 What are the data?\n\nThe basic data structure for both supervised and unsupervised learning is (at\nleast conceptually) a dataframe, where each row corresponds to an object and\nthe columns are different features (usually numerical values) of the objects2.\nWhile in unsupervised learning we aim to find (dis)similarity relationships\nbetween the objects based on their feature values (e.g., by clustering or\nordination), in supervised learning we aim to find a mathematical function (or\na computational algorithm) that predicts the value of one of the features from\nthe other features. Many implementations require that there are no missing\nvalues, whereas other methods can be made to work with some amount of missing\ndata.\n\n2 This is a simplified description. Machine learning is a huge field, and lots\nof generalizations of this simple conceptual picture have been made. Already\nthe construction of relevant features is an art by itself ‚Äî we have seen\nexamples with images of cells in [Chapter 11](11-chap.html), and more\ngenerally there are lots of possibilities to extract features from images,\nsounds, movies, free text, \\\\(...\\\\) Moreover, there is a variant of machine\nlearning methods called **kernel methods** that do not need features at all;\ninstead, kernel methods use distances or measures of similarity between\nobjects. It may be easier, for instance, to define a measure of similarity\nbetween two natural language text objects than to find relevant numerical\nfeatures to represent them. Kernel methods are beyond the scope of this book.\n\nThe feature that we select over all the others with the aim of predicting is\ncalled the **objective** or the **response**. Sometimes the choice is natural,\nbut sometimes it is also instructive to reverse the roles, especially if we\nare interested in dissecting the prediction function for the purpose of\nbiological understanding, or in disentangling correlations from causation.\n\nThe framework for supervised learning covers both continuous and categorical\nresponse variables. In the continuous case we also call it **regression** , in\nthe categorical case, **classification**. It turns out that this distinction\nis not a detail, as it has quite far-reaching consequences for the choice of\nloss function (Section 12.5) and thus the choice of algorithm ([Friedman\n1997](16-chap.html#ref-friedmanbiasvariance01)).\n\nThe first question to consider in any supervised learning task is how the\nnumber of objects compares to the number of predictors. The more objects, the\nbetter, and much of the hard work in supervised learning has to do with\novercoming the limitations of having a finite (and typically, too small)\ntraining set.\n\n[![](imgs/fourquad.png)](imgs/fourquad.png \"Figure¬†12.2: In supervised\nlearning, we assign two different roles to our variables. We have labeled the\nexplanatory variables X and the response variable\\(s\\) Y. There are also two\ndifferent sets of observations: the training set X_\\\\ell and Y_\\\\ell and the\ntest set X_v and Y_v. \\(The subscripts refer to alternative names for the two\nsets: ‚Äúlearning‚Äù and ‚Äúvalidation‚Äù.\\)\")\n\nFigure 12.2: In supervised learning, we assign two different roles to our\nvariables. We have labeled the explanatory variables \\\\(X\\\\) and the response\nvariable(s) \\\\(Y\\\\). There are also two different sets of observations: the\ntraining set \\\\(X_\\ell\\\\) and \\\\(Y_\\ell\\\\) and the test set \\\\(X_v\\\\) and\n\\\\(Y_v\\\\). (The subscripts refer to alternative names for the two sets:\n‚Äúlearning‚Äù and ‚Äúvalidation‚Äù.)\n\n__\n\nTask\n\nGive examples where we have encountered instances of supervised learning with\na categorical response in this book.\n\n### 12.2.1 Motivating examples\n\n#### Predicting diabetes type\n\nThe `diabetes` dataset ([Reaven and Miller 1979](16-chap.html#ref-diabetes))\npresents three different groups of diabetes patients and five clinical\nvariables measured on them.\n\n    \n    \n    data(\"diabetes\", package = \"rrcov\")\n    head(diabetes)__\n    \n    \n        rw fpg glucose insulin sspg  group\n    1 0.81  80     356     124   55 normal\n    2 0.95  97     289     117   76 normal\n    3 0.94 105     319     143  105 normal\n    4 1.04  90     356     199  108 normal\n    5 1.00  90     323     240  143 normal\n    6 0.76  86     381     157  165 normal\n\nThe univariate distributions (more precisely, some density estimates of them)\nare shown in Figure 12.3.\n\n    \n    \n    library(\"reshape2\")\n    ggplot(melt(diabetes, id.vars = \"group\"), aes(x = value, col = group)) +\n     geom_density() + facet_wrap( ~variable, ncol = 1, scales = \"free\") +\n     theme(legend.position = \"bottom\")__\n\n[![](12-chap_files/figure-html/fig-ldagroups-1-1.png)](12-chap_files/figure-\nhtml/fig-ldagroups-1-1.png \"Figure¬†12.3: We see already from the one-\ndimensional distributions that some of the individual variables could\npotentially predict which group a patient is more likely to belong to. Our\ngoal is to combine variables to improve over such one-dimensional prediction\nmodels.\")\n\nFigure 12.3: We see already from the one-dimensional distributions that some\nof the individual variables could potentially predict which group a patient is\nmore likely to belong to. Our goal is to combine variables to improve over\nsuch one-dimensional prediction models.\n\nThe variables are explained in the manual page of the dataset, and in the\npaper ([Reaven and Miller 1979](16-chap.html#ref-diabetes)):\n\n  * rw: relative weight\n\n  * fpg: fasting plasma glucose\n\n  * glucose: area under the plasma glucose curve for the three hour oral glucose tolerance test (OGTT)\n\n  * insulin: area under the plasma insulin curve for the OGTT\n\n  * sspg: steady state plasma glucose response\n\n  * group: normal, chemical diabetes and overt diabetes\n\n#### Predicting cellular phenotypes\n\nNeumann et al. ([2010](16-chap.html#ref-Neumann:2010)) observed human cancer\ncells using live-cell imaging. The cells were genetically engineered so that\ntheir histones were tagged with a green fluorescent protein (GFP). A genome-\nwide RNAi library was applied to the cells, and for each siRNA perturbation,\nmovies of a few hundred cells were recorded for about two days, to see what\neffect the depletion of each gene had on cell cycle, nuclear morphology and\ncell proliferation. Their paper reports the use of an automated image\nclassification algorithm that quantified the visual appearance of each cell‚Äôs\nnucleus and enabled the prediction of normal mitosis states or aberrant\nnuclei. The algorithm was trained on the data from around 3000 cells that were\nannotated by a human expert. It was then applied to almost 2 billions images\nof nuclei (Figure 12.4). Using automated image classification provided\nscalability (annotating 2 billion images manually would take a long time) and\nobjectivity.\n\n[![](imgs/Neumann2010Fig1b_web.jpg)](imgs/Neumann2010Fig1b_web.jpg\n\"Figure¬†12.4: The data were images of 2\\\\times10^9 nuclei from movies. The\nimages were segmented to identify the nuclei, and numeric features were\ncomputed for each nucleus, corresponding to size, shape, brightness and lots\nof other more or less abstract quantitative summaries of the joint\ndistribution of pixel intensities. From the features, the cells were\nclassified into 16 different nuclei morphology classes, represented by the\nrows of the barplot. Representative images for each class are shown in black\nand white in the center column. The class frequencies, which are very\nunbalanced, are shown by the lengths of the bars.\")\n\nFigure 12.4: The data were images of \\\\(2\\times10^9\\\\) nuclei from movies. The\nimages were segmented to identify the nuclei, and numeric features were\ncomputed for each nucleus, corresponding to size, shape, brightness and lots\nof other more or less abstract quantitative summaries of the joint\ndistribution of pixel intensities. From the features, the cells were\nclassified into 16 different nuclei morphology classes, represented by the\nrows of the barplot. Representative images for each class are shown in black\nand white in the center column. The class frequencies, which are very\nunbalanced, are shown by the lengths of the bars.\n\n#### Predicting embryonic cell states\n\nWe will revisit the mouse embryo data ([Ohnishi et al. 2014](16-chap.html#ref-\nOhnishi2014)), which we have already seen in Chapters [3](03-chap.html),\n[5](05-chap.html) and [7](07-chap.html). We‚Äôll try to predict cell state and\ngenotype from the gene expression measurements in Sections 12.3.2 and 12.6.3.\n\n## 12.3 Linear discrimination\n\nWe start with one of the simplest possible discrimination problems3: we have\nobjects described by two continuous features (so the objects can be thought of\nas points in the 2D plane) and falling into three groups. Our aim is to define\nclass boundaries, which are lines in the 2D space.\n\n3 Arguably the simplest possible problem is a single continuous feature, two\nclasses, and the task of finding a single threshold to discriminate between\nthe two groups ‚Äì as in [Figure 6.2](06-chap.html#fig-testing-FDRvspstatic1).\n\n### 12.3.1 Diabetes data\n\nLet‚Äôs see whether we can predict the `group` from the `sspg` and `glucose`\nvariables in the `diabetes` data. It‚Äôs always a good idea to first visualise\nthe data (Figure 12.5).\n\n    \n    \n    ggdb = ggplot(mapping = aes(x = sspg, y = glucose)) +\n      geom_point(aes(colour = group), data = diabetes)\n    ggdb __\n\n[![](12-chap_files/figure-html/fig-\nscatterdiabetes-1-1.png)](12-chap_files/figure-html/fig-\nscatterdiabetes-1-1.png \"Figure¬†12.5: Scatterplot of two of the variables in\nthe diabetes data. Each point is a sample, and the color indicates the\ndiabetes type as encoded in the group variable.\")\n\nFigure 12.5: Scatterplot of two of the variables in the `diabetes` data. Each\npoint is a sample, and the color indicates the diabetes type as encoded in the\n`group` variable.\n\nWe‚Äôll start with a method called **linear discriminant analysis** (**LDA**).\nThis method is a foundation stone of classification, many of the more\ncomplicated (and sometimes more powerful) algorithms are really just\ngeneralizations of LDA.\n\n    \n    \n    library(\"MASS\")\n    diabetes_lda = lda(group ~ sspg + glucose, data = diabetes)\n    diabetes_lda __\n    \n    \n    Call:\n    lda(group ~ sspg + glucose, data = diabetes)\n    \n    Prior probabilities of groups:\n       normal  chemical     overt \n    0.5241379 0.2482759 0.2275862 \n    \n    Group means:\n                 sspg   glucose\n    normal   114.0000  349.9737\n    chemical 208.9722  493.9444\n    overt    318.8788 1043.7576\n    \n    Coefficients of linear discriminants:\n                    LD1         LD2\n    sspg    0.005036943 -0.01539281\n    glucose 0.005461400  0.00449050\n    \n    Proportion of trace:\n       LD1    LD2 \n    0.9683 0.0317 \n    \n    \n    ghat = predict(diabetes_lda)$class\n    table(ghat, diabetes$group)__\n    \n    \n              \n    ghat       normal chemical overt\n      normal       69       12     1\n      chemical      7       24     6\n      overt         0        0    26\n    \n    \n    mean(ghat != diabetes$group)__\n    \n    \n    [1] 0.1793103\n\n__\n\nQuestion 12.1\n\nWhat do the different parts of the above output mean?\n\nNow, let‚Äôs visualise the LDA result. We are going to plot the prediction\nregions for each of the three groups. We do this by creating a grid of points\nand using our prediction rule on each of them. We‚Äôll then also dig a bit\ndeeper into the mechanics of LDA and plot the class centers\n(`diabetes_lda$means`) and ellipses that correspond to the fitted covariance\nmatrix (`diabetes_lda$scaling`). Assembling this visualization requires us to\nwrite a bit of code.\n\n    \n    \n    make1Dgrid = function(x) {\n      rg = grDevices::extendrange(x)\n      seq(from = rg[1], to = rg[2], length.out = 100)\n    }__\n\nSet up the points for prediction, a \\\\(100 \\times 100\\\\) grid that covers the\ndata range.\n\n    \n    \n    diabetes_grid = with(diabetes,\n      expand.grid(sspg = make1Dgrid(sspg),\n                  glucose = make1Dgrid(glucose)))__\n\nDo the predictions.\n\n    \n    \n    diabetes_grid$ghat =\n      predict(diabetes_lda, newdata = diabetes_grid)$class __\n\nThe group centers.\n\n    \n    \n    centers = diabetes_lda$means __\n\nCompute the ellipse. We start from a unit circle (approximated by a polygon\nwith 360 sides) and apply the corresponding affine transformation from the LDA\noutput.\n\n    \n    \n    unitcircle = exp(1i * seq(0, 2*pi, length.out = 360)) |>\n              (\\(z) cbind(Re(z), Im(z)))() \n    ellipse = unitcircle %*% solve(diabetes_lda$scaling) |> as_tibble()__\n\nAll three ellipses, one for each group center.\n\n    \n    \n    library(\"dplyr\")\n    ellipses = lapply(rownames(centers), function(gr) {\n      mutate(ellipse,\n         sspg    = sspg    + centers[gr, \"sspg\"],\n         glucose = glucose + centers[gr, \"glucose\"],\n         group   = gr)\n    }) |> bind_rows()__\n\nNow we are ready to plot (Figure 12.6).\n\n    \n    \n    ggdb + geom_raster(aes(fill = ghat),\n                data = diabetes_grid, alpha = 0.25, interpolate = TRUE) +\n        geom_point(data = as_tibble(centers), pch = \"+\", size = 8) +\n        geom_path(aes(colour = group), data = ellipses) +\n        scale_x_continuous(expand = c(0, 0)) +\n        scale_y_continuous(expand = c(0, 0))__\n\n[![](12-chap_files/figure-html/fig-\nmodeldiabetes-1-1.png)](12-chap_files/figure-html/fig-modeldiabetes-1-1.png\n\"Figure¬†12.6: As Figure¬†fig-scatterdiabetes-1, with the classification regions\nfrom the LDA model shown. The three ellipses represent the class centers and\nthe covariance matrix of the LDA model; note that there is only one covariance\nmatrix, which is the same for all three classes. Therefore also the sizes and\norientations of the ellipses are the same for the three classes, only their\ncenters differ. They represent contours of equal class membership\nprobability.\")\n\nFigure 12.6: As Figure 12.5, with the classification regions from the LDA\nmodel shown. The three ellipses represent the class centers and the covariance\nmatrix of the LDA model; note that there is only one covariance matrix, which\nis the same for all three classes. Therefore also the sizes and orientations\nof the ellipses are the same for the three classes, only their centers differ.\nThey represent contours of equal class membership probability.\n\n__\n\nQuestion 12.2\n\nWhy is the boundary between the prediction regions for _chemical_ and _overt_\nnot perpendicular to the line between the group centers?\n\n__\n\nSolution\n\n__\n\nThe boundaries would be perpendicular if the ellipses were circles. In\ngeneral, a boundary is tangential to the contours of equal class\nprobabilities, and due the elliptic shape of the contours, a boundary is in\ngeneral not perpendicular to the line between centers.\n\n__\n\nQuestion 12.3\n\nHow confident would you be about the predictions in those areas of the 2D\nplane that are far from all of the cluster centers?\n\n__\n\nSolution\n\n__\n\nPredictions that are far from any cluster center should be assessed\ncritically, as this amounts to an extrapolation into regions where the LDA\nmodel may not be very good and/or there may be no training data nearby to\nsupport the prediction. We could use the distance to the nearest center as a\nmeasure of confidence in the prediction for any particular point; although we\nwill see that resampling and cross-validation based methods offer more generic\nand usually more reliable measures.\n\n__\n\nQuestion 12.4\n\nWhy is the boundary between the prediction regions for _normal_ and _chemical_\nnot half-way between the centers, but shifted in favor of _normal_? Hint: have\na look at the `prior` argument of `lda`. Try again with uniform prior.\n\n__\n\nSolution\n\n__\n\nThe result of the following code chunk is shown in Figure 12.7. The suffix\n`_up` is short for ‚Äúuniform prior‚Äù.\n\n    \n    \n    diabetes_up = lda(group ~ sspg + glucose, data = diabetes,\n      prior = (\\(n) rep(1/n, n)) (nlevels(diabetes$group)))\n    \n    diabetes_grid$ghat_up =\n      predict(diabetes_up, newdata = diabetes_grid)$class\n    \n    stopifnot(all.equal(diabetes_up$means, diabetes_lda$means))\n    \n    ellipse_up  = unitcircle %*% solve(diabetes_up$scaling) |> as_tibble()\n    ellipses_up = lapply(rownames(centers), function(gr) {\n      mutate(ellipse_up,\n         sspg    = sspg    + centers[gr, \"sspg\"],\n         glucose = glucose + centers[gr, \"glucose\"],\n         group   = gr)\n    }) |> bind_rows()\n    \n    ggdb + geom_raster(aes(fill = ghat_up),\n                data = diabetes_grid, alpha = 0.4, interpolate = TRUE) +\n        geom_point(data = data.frame(centers), pch = \"+\", size = 8) +\n        geom_path(aes(colour = group), data = ellipses_up) +\n        scale_x_continuous(expand = c(0, 0)) +\n        scale_y_continuous(expand = c(0, 0))__\n\n[![](12-chap_files/figure-html/fig-diabetes-lda-uniform-\nprior-1-1.png)](12-chap_files/figure-html/fig-diabetes-lda-uniform-\nprior-1-1.png \"Figure¬†12.7: As Figure¬†fig-modeldiabetes-1, but with uniform\nclass priors.\")\n\nFigure 12.7: As Figure 12.6, but with uniform class priors.\n\nThe `stopifnot` line confirms that the class centers are the same, as they are\nindependent of the prior. The joint covariance is not.\n\n__\n\nQuestion 12.5\n\nFigures 12.6 and 12.7 show both the fitted LDA model, through the ellipses,\nand the prediction regions, through the area coloring. What part of this\nvisualization is generic for all sorts of classification methods, what part is\nmethod-specific?\n\n__\n\nSolution\n\n__\n\nThe prediction regions can be shown for any classification method, including a\n‚Äúblack box‚Äù method. The cluster centers and ellipses in Figures 12.6 and 12.7\nare method-specific.\n\n__\n\nQuestion 12.6\n\nWhat is the difference in the prediction accuracy if we use all 5 variables\ninstead of just `glucose` and `sspg`?\n\n__\n\nSolution\n\n__\n\n    \n    \n    diabetes_lda5 = lda(group ~ rw + fpg + glucose + sspg + insulin, data = diabetes)\n    diabetes_lda5 __\n    \n    \n    Call:\n    lda(group ~ rw + fpg + glucose + sspg + insulin, data = diabetes)\n    \n    Prior probabilities of groups:\n       normal  chemical     overt \n    0.5241379 0.2482759 0.2275862 \n    \n    Group means:\n                    rw       fpg   glucose     sspg  insulin\n    normal   0.9372368  91.18421  349.9737 114.0000 172.6447\n    chemical 1.0558333  99.30556  493.9444 208.9722 288.0000\n    overt    0.9839394 217.66667 1043.7576 318.8788 106.0000\n    \n    Coefficients of linear discriminants:\n                      LD1          LD2\n    rw       1.3624356881 -3.784142444\n    fpg     -0.0336487883  0.036633317\n    glucose  0.0125763942 -0.007092017\n    sspg     0.0042431866  0.001134070\n    insulin -0.0001022245 -0.006173424\n    \n    Proportion of trace:\n       LD1    LD2 \n    0.8812 0.1188 \n    \n    \n    ghat5 = predict(diabetes_lda5)$class\n    table(ghat5, diabetes$group)__\n    \n    \n              \n    ghat5      normal chemical overt\n      normal       73        5     1\n      chemical      3       31     5\n      overt         0        0    27\n    \n    \n    mean(ghat5 != diabetes$group)__\n    \n    \n    [1] 0.09655172\n\n__\n\nQuestion 12.7\n\nInstead of approximating the prediction regions by classification from a grid\nof points, compute the separating lines explicitly from the linear determinant\ncoefficients.\n\n__\n\nSolution\n\n__\n\nSee Section 4.3, Equation (4.10) in ([Hastie, Tibshirani, and Friedman\n2008](16-chap.html#ref-HastieTibshiraniFriedman)).\n\n### 12.3.2 Predicting embryonic cell state from gene expression\n\nAssume that we already know that the four genes _FN1_ , _TIMD2_ , _GATA4_ and\n_SOX7_ are relevant to the classification task4. We want to build a classifier\nthat predicts the developmental time (embryonic days: E3.25, E3.5, E4.5). We\nload the data and select four corresponding probes.\n\n4 Later in this chapter we will see methods that can drop this assumption and\nscreen all available features.\n\n    \n    \n    library(\"Hiiragi2013\")__\n    \n    \n    In chunk 'loadHiiragi2': Warning: replacing previous import 'boot::logit' by 'gtools::logit' whenloading 'Hiiragi2013'\n    \n    \n    In chunk 'loadHiiragi2': Warning: replacing previous import 'boot::inv.logit' by 'gtools::inv.logit'when loading 'Hiiragi2013'\n    \n    \n    data(\"x\")\n    probes = c(\"1426642_at\", \"1418765_at\", \"1418864_at\", \"1416564_at\")\n    embryoCells = t(Biobase::exprs(x)[probes, ]) |> as_tibble() |>\n      mutate(Embryonic.day = x$Embryonic.day) |>\n      dplyr::filter(x$genotype == \"WT\")__\n\nWe can use the Bioconductor annotation package associated with the microarray\nto verify that the probes correspond to the intended genes.\n\n    \n    \n    annotation(x)__\n    \n    \n    [1] \"mouse4302\"\n    \n    \n    library(\"mouse4302.db\")\n    anno = AnnotationDbi::select(mouse4302.db, keys = probes,\n                                 columns = c(\"SYMBOL\", \"GENENAME\"))\n    anno __\n    \n    \n         PROBEID SYMBOL                                            GENENAME\n    1 1426642_at    Fn1                                       fibronectin 1\n    2 1418765_at  Timd2 T cell immunoglobulin and mucin domain containing 2\n    3 1418864_at  Gata4                              GATA binding protein 4\n    4 1416564_at   Sox7                SRY (sex determining region Y)-box 7\n    \n    \n    mt = match(anno$PROBEID, colnames(embryoCells))\n    colnames(embryoCells)[mt] = anno$SYMBOL __\n\nNow we are ready to visualize the data in a pairs plot (Figure 12.8).\n\n    \n    \n    library(\"GGally\")\n    ggpairs(embryoCells, mapping = aes(col = Embryonic.day),\n      columns = anno$SYMBOL, upper = list(continuous = \"points\"))__\n\n[![](12-chap_files/figure-html/fig-\nHiiragiFourGenesPairs-1-1.png)](12-chap_files/figure-html/fig-\nHiiragiFourGenesPairs-1-1.png \"Figure¬†12.8: Expression values of the\ndiscriminating genes, with the prediction target Embryonic.day shown by\ncolor.\")\n\nFigure 12.8: Expression values of the discriminating genes, with the\nprediction target Embryonic.day shown by color.\n\nWe can now call `lda` on these data. The linear combinations `LD1` and `LD2`\nthat serve as discriminating variables are given in the slot `ed_lda$scaling`\nof the output from `lda`.\n\n    \n    \n    ec_lda = lda(Embryonic.day ~ Fn1 + Timd2 + Gata4 + Sox7,\n                 data = embryoCells)\n    round(ec_lda$scaling, 1)__\n    \n    \n           LD1  LD2\n    Fn1   -0.2  0.4\n    Timd2  0.5  0.0\n    Gata4 -0.1  0.6\n    Sox7  -0.7 -0.5\n\nFor the visualization of the learned model in Figure 12.9, we need to build\nthe prediction regions and their boundaries by expanding the grid in the space\nof the two new coordinates LD1 and LD2.\n\n    \n    \n    ec_rot = predict(ec_lda)$x |> as_tibble() |>\n               mutate(ed = embryoCells$Embryonic.day)\n    ec_lda2 = lda(ec_rot[, 1:2], predict(ec_lda)$class)\n    ec_grid = with(ec_rot, expand.grid(\n      LD1 = make1Dgrid(LD1),\n      LD2 = make1Dgrid(LD2)))\n    ec_grid$edhat = predict(ec_lda2, newdata = ec_grid)$class\n    ggplot() +\n      geom_point(aes(x = LD1, y = LD2, colour = ed), data = ec_rot) +\n      geom_raster(aes(x = LD1, y = LD2, fill = edhat),\n                data = ec_grid, alpha = 0.4, interpolate = TRUE) +\n      scale_x_continuous(expand = c(0, 0)) +\n      scale_y_continuous(expand = c(0, 0)) +\n      coord_fixed()__\n\n[![](12-chap_files/figure-html/fig-edcontour-1-1.png)](12-chap_files/figure-\nhtml/fig-edcontour-1-1.png \"Figure¬†12.9: LDA classification regions for\nEmbryonic.day.\")\n\nFigure 12.9: LDA classification regions for Embryonic.day.\n\n__\n\nQuestion 12.8\n\nRepeat these analyses using quadratic discriminant analysis (`qda`). What\ndifference do you see in the shape of the boundaries?\n\n__\n\nSolution\n\n__\n\nSee code below and Figure 12.10.\n\n    \n    \n    library(\"gridExtra\")\n    \n    ec_qda = qda(Embryonic.day ~ Fn1 + Timd2 + Gata4 + Sox7,\n                 data = embryoCells)\n    \n    variables = colnames(ec_qda$means)\n    pairs = combn(variables, 2)\n    lapply(seq_len(ncol(pairs)), function(i) {\n      grid = with(embryoCells,\n        expand.grid(x = make1Dgrid(get(pairs[1, i])),\n                    y = make1Dgrid(get(pairs[2, i])))) |>\n        `colnames<-`(pairs[, i])\n    \n      for (v in setdiff(variables, pairs[, i]))\n        grid[[v]] = median(embryoCells[[v]])\n    \n      grid$edhat = predict(ec_qda, newdata = grid)$class\n    \n      x <- pairs[1,i]\n      y <- pairs[2,i]\n      ggplot() + \n        geom_point(\n          data = embryoCells,\n          aes(x = .data[[x]], y = .data[[y]], colour = Embryonic.day)\n        ) +\n        geom_raster(\n          aes(x = .data[[x]], y = .data[[y]], fill = edhat),\n          data = grid, alpha = 0.4, interpolate = TRUE\n        ) +\n        scale_x_continuous(expand = c(0, 0)) +\n        scale_y_continuous(expand = c(0, 0)) +\n        coord_fixed() +\n        if (i != ncol(pairs)) theme(legend.position = \"none\")\n    }) |> (\\(g) grid.arrange(grobs = g, ncol = 2))()__\n\n[![](12-chap_files/figure-html/fig-qdamouse-1-1.png)](12-chap_files/figure-\nhtml/fig-qdamouse-1-1.png \"Figure¬†12.10: QDA for the mouse cell data. Shown\nare all pairwise plots of the four features. In each plot, the other two\nfeatures are set to the median.\")\n\nFigure 12.10: QDA for the mouse cell data. Shown are all pairwise plots of the\nfour features. In each plot, the other two features are set to the median.\n\n__\n\nQuestion 12.9\n\nWhat happens if you call `lda` or `qda` with a lot more genes, say the first\n1000, in the Hiiragi dataset?\n\n__\n\nSolution\n\n__\n\n    \n    \n    lda(t(Biobase::exprs(x))[, 1:1000], x$Embryonic.day)__\n    \n    \n     warnings()\n    qda(t(Biobase::exprs(x))[, 1:1000], x$Embryonic.day)__\n    \n    \n    Error in qda.default(x, grouping, ...): some group is too small for 'qda'\n\nThe `lda` function manages to fit a model, but complains (with the warning)\nabout the fact that there are more variables than replicates, which means that\nthe variables are not linearly independent, and thus are redundant of each\nother. The `qda` function aborts with an error, since the QDA model with so\nmany parameters cannot be fitted from the available data (at least, without\nmaking further assumptions, such as some sort of regularization, which it is\nnot equipped for).\n\n## 12.4 Machine learning vs rote learning\n\nComputers are really good at memorizing facts. In the worst case, a machine\nlearning algorithm is a roundabout way of doing this5. The central goal in\nstatistical learning, however, is _generalizability_. We want an algorithm\nthat is able to generalize, i.e., interpolate and extrapolate from given data\nto make good predictions about future data.\n\n5 The not-so roundabout way is database technologies.\n\nLet‚Äôs look at the following example. We generate random data (`rnorm`) for `n`\nobjects, with different numbers of features (given by `p`). We train a LDA on\nthese data and compute the **misclassification rate** , i.e., the fraction of\ntimes the prediction is wrong (`pred != resp`).\n\n    \n    \n    p = 2:21\n    n = 20\n    \n    mcl = lapply(p, function(pp) {\n      replicate(100, {\n        xmat = matrix(rnorm(n * pp), nrow = n)\n        resp = sample(c(\"apple\", \"orange\"), n, replace = TRUE)\n        fit  = lda(xmat, resp)\n        pred = predict(fit)$class\n        mean(pred != resp)\n      }) |> mean() |> (\\(x) tibble(mcl = x, p = pp))()\n    }) |> bind_rows()__\n    \n    \n     ggplot(mcl, aes(x = p, y = mcl)) + \n      geom_line() + geom_point() +\n      ylab(\"Misclassification rate\")__\n\n[![](12-chap_files/figure-html/fig-\nlearnbyheart-1-1.png)](12-chap_files/figure-html/fig-learnbyheart-1-1.png\n\"Figure¬†12.11: Misclassification rate of LDA applied to random data. While the\nnumber of observations n is held constant \\(at 20\\), we are increasing the\nnumber of features p starting from 2 up to 21. The misclassification rate\nbecomes almost zero as p approaches 20. The LDA model becomes so elaborate and\nover-parameterized that it manages to learn the random labels ‚Äúby heart‚Äù. \\(As\np becomes even larger, the ‚Äúperformance‚Äù degrades again somewhat, apparently\ndue to numerical properties of the lda implementation used here.\\)\")\n\nFigure 12.11: Misclassification rate of LDA applied to random data. While the\nnumber of observations `n` is held constant (at 20), we are increasing the\nnumber of features `p` starting from 2 up to 21. The misclassification rate\nbecomes almost zero as `p` approaches 20. The LDA model becomes so elaborate\nand over-parameterized that it manages to learn the random labels ‚Äúby heart‚Äù.\n(As `p` becomes even larger, the ‚Äúperformance‚Äù degrades again somewhat,\napparently due to numerical properties of the `lda` implementation used here.)\n\n__\n\nQuestion 12.10\n\nWhat is the purpose of the `replicate` loop in the above code? What happens if\nyou omit it (or replace the 100 by 1)?\n\n__\n\nSolution\n\n__\n\nFor each single replicate, the curve is a noisier version of Figure 12.11.\nAveraging the measured misclassifications rate over 100 replicates makes the\nestimate more stable. We can do this since we are working with simulated data.\n\nFigure 12.11 seems to imply that we can perfectly predict random labels from\nrandom data, if we only fit a complex enough model, i.e., one with many\nparameters. How can we overcome such an absurd conclusion? The problem with\nthe above code is that the model performance is evaluated on the same data on\nwhich it was trained. This generally leads to positive bias, as you see in\nthis crass example. How can we overcome this problem? The key idea is to\nassess model performance on different data than those on which the model was\ntrained.\n\n### 12.4.1 Cross-validation\n\nA naive approach might be to split the data in two halves, and use the first\nhalf for learning (‚Äútraining‚Äù) and the second half for assessment (‚Äútesting‚Äù).\nIt turns out that this is needlessly variable and needlessly inefficient. It\nis needlessly variable, since by splitting the data only once, our results can\nbe quite affected by how the split happens to fall. It seems better to do the\nsplitting many times, and average. This will give us more stable results. It\nis needlessly inefficient, since the performance of machine learning\nalgorithms depends on the number of observations, and the performance measured\non half the data is likely6 to be worse than what it is with all the data. For\nthis reason, it is better to use unequal sizes of training and test data. In\nthe extreme case, we‚Äôll use as much as \\\\(n-1\\\\) observations for training,\nand the remaining one for testing. After we‚Äôve done this likewise for all\nobservations, we can average our performance metric. This is called **leave-\none-out cross-validation**.\n\n6 Unless we have such an excess of data that it doesn‚Äôt matter.\n\n![](imgs/book_icon.png)\n\nSee Chapter _Model Assessment and Selection_ in the book by Hastie,\nTibshirani, and Friedman ([2008](16-chap.html#ref-HastieTibshiraniFriedman))\nfor further discussion on these trade-offs.\n\nAn alternative is **\\\\(k\\\\) -fold cross-validation**, where the observations\nare repeatedly split into a training set of size of around \\\\(n(k-1)/k\\\\) and\na test set of size of around \\\\(n/k\\\\). Both alternatives have pros and\ncontras, and there is not a universally best choice. An advantage of leave-\none-out is that the amount of data used for training is close to the maximally\navailable data; this is especially important if the sample size is limiting\nand ‚Äúevery little matters‚Äù for the algorithm. A drawback of leave-one-out is\nthat the training sets are all very similar, so they may not model\nsufficiently well the kind of sampling changes to be expected if a new dataset\ncame along. For large \\\\(n\\\\), leave-one-out cross-validation can be\nneedlessly time-consuming.\n\n    \n    \n    estimate_mcl_loocv = function(x, resp) {\n      vapply(seq_len(nrow(x)), function(i) {\n        fit  = lda(x[-i, ], resp[-i])\n        ptrn = predict(fit, newdata = x[-i,, drop = FALSE])$class\n        ptst = predict(fit, newdata = x[ i,, drop = FALSE])$class\n        c(train = mean(ptrn != resp[-i]), test = (ptst != resp[i]))\n      }, FUN.VALUE = numeric(2)) |> rowMeans() |> t() |> as_tibble()\n    }\n    \n    xmat = matrix(rnorm(n * last(p)), nrow = n)\n    resp = sample(c(\"apple\", \"orange\"), n, replace = TRUE)\n    \n    mcl = lapply(p, function(k) {\n      estimate_mcl_loocv(xmat[, 1:k], resp)\n    }) |> bind_rows() |> data.frame(p) |> melt(id.var = \"p\")__\n    \n    \n     ggplot(mcl, aes(x = p, y = value, col = variable)) + geom_line() +\n      geom_point() + ylab(\"Misclassification rate\")__\n\n[![](12-chap_files/figure-html/fig-mclcv-1-1.png)](12-chap_files/figure-\nhtml/fig-mclcv-1-1.png \"Figure¬†12.12: Cross-validation: the misclassification\nrate of LDA applied to random data, when evaluated on test data that were not\nused for learning, hovers around 0.5 independent of p. The misclassification\nrate on the training data is also shown. It behaves similar to what we already\nsaw in Figure¬†fig-learnbyheart-1.\")\n\nFigure 12.12: Cross-validation: the misclassification rate of LDA applied to\nrandom data, when evaluated on test data that were not used for learning,\nhovers around 0.5 independent of `p`. The misclassification rate on the\ntraining data is also shown. It behaves similar to what we already saw in\nFigure 12.11.\n\nThe result is show in Figure 12.12.\n\n__\n\nQuestion 12.11\n\nWhy are the curves in Figure 12.12 more variable (‚Äúwiggly‚Äù) than in Figure\n12.11? How can you overcome this?\n\n__\n\nSolution\n\n__\n\nOnly one dataset (`xmat`, `resp`) was used to calculate Figure 12.12, whereas\nfor Figure 12.11, we had the data generated within a `replicate` loop. You\ncould similarly extend the above code to average the misclassification rate\ncurves over many replicate simulated datasets.\n\n### 12.4.2 The curse of dimensionality\n\nIn Section 12.4.1 we have seen overfitting and cross-validation on random\ndata, but how does it look if there is in fact a relevant class separation?\n\n    \n    \n    p   = 2:20\n    mcl = replicate(100, {\n      xmat = matrix(rnorm(n * last(p)), nrow = n)\n      resp = sample(c(\"apple\", \"orange\"), n, replace = TRUE)\n      xmat[, 1:6] = xmat[, 1:6] + as.integer(factor(resp))\n    \n      lapply(p, function(k) {\n        estimate_mcl_loocv(xmat[, 1:k], resp)\n      }) |> bind_rows() |> cbind(p = p) |> melt(id.var = \"p\")\n    }, simplify = FALSE) |> bind_rows()__\n    \n    \n    mcl = group_by(mcl, p, variable) |> summarise(value = mean(value))\n    \n    ggplot(mcl, aes(x = p, y = value, col = variable)) + geom_line() +\n       geom_point() + ylab(\"Misclassification rate\")__\n\n[![](12-chap_files/figure-html/fig-curseofdim-1.png)](12-chap_files/figure-\nhtml/fig-curseofdim-1.png \"Figure¬†12.13: As we increase the number of features\nincluded in the model, the misclassification rate initially improves; as we\nstart including more and more irrelevant features, it increases again, as we\nare fitting noise.\")\n\nFigure 12.13: As we increase the number of features included in the model, the\nmisclassification rate initially improves; as we start including more and more\nirrelevant features, it increases again, as we are fitting noise.\n\n[![](imgs/BiasVarianceTradeoff.png)](imgs/BiasVarianceTradeoff.png\n\"Figure¬†12.14: Idealized version of Figure¬†fig-curseofdim, from\n@HastieTibshiraniFriedman. A recurrent goal in machine learning is finding the\nsweet spot in the variance <- bias trade-off.\")\n\nFigure 12.14: Idealized version of Figure 12.13, from Hastie, Tibshirani, and\nFriedman ([2008](16-chap.html#ref-HastieTibshiraniFriedman)). A recurrent goal\nin machine learning is finding the sweet spot in the variance <\\- bias trade-\noff.\n\nThe result is shown in Figure 12.13. The group centers are the vectors (in\n\\\\(\\mathbb{R}^{20}\\\\)) given by the coordinates \\\\((1, 1, 1, 1, 1, 1, 0, 0, 0,\n...)\\\\) (apples) and \\\\((2, 2, 2, 2, 2, 2, 0, 0, 0, ...)\\\\) (oranges), and the\noptimal decision boundary is the hyperplane orthogonal to the line between\nthem. For \\\\(p\\\\) smaller than \\\\(6\\\\), the decision rule cannot reach this\nhyperplane ‚Äì it is biased. As a result, the misclassification rate is\nsuboptimal, and it decreases with \\\\(p\\\\). But what happens for \\\\(p\\\\) larger\nthan \\\\(6\\\\)? The algorithm is, in principle, able to model the optimal\nhyperplane, and it should not be distracted by the additional features. The\nproblem is that it is. The more additional features enter the dataset, the\nhigher the probability that one or more of them happen to fall in a way that\nthey _look like_ good, discriminating features in the training data ‚Äì only to\nmislead the classifier and degrade its performance in the test data. Shortly\nwe‚Äôll see how to use penalization to (try to) control this problem.\n\nThe term **curse of dimensionality** was coined by Bellman\n([1961](16-chap.html#ref-Bellman:1961)). It refers to the fact that high-\ndimensional spaces are very hard, if not impossible, to sample thoroughly: for\ninstance, to cover a 2-dimensional square of side length 1 with grid points\nthat are 0.1 apart, we need \\\\(10^2=100\\\\) points. In 100 dimensions, we need\n\\\\(10^{100}\\\\) ‚Äì which is more than [the number of protons in the\nuniverse](https://en.wikipedia.org/wiki/Eddington_number). In genomics, we\noften aim to fit models to data with thousands of features. Also our\nintuitions about distances between points or about the relationship between a\nvolume and its surface break down in a high-dimensional settings. We‚Äôll\nexplore some of the weirdnesses of high-dimensional spaces in the next few\nquestions.\n\n__\n\nQuestion 12.12\n\nAssume you have a dataset with 1 000 000 data points in \\\\(p\\\\) dimensions.\nThe data are uniformly distributed in the unit hybercube (i.e., all features\nlie in the interval \\\\([0,1]\\\\)). What‚Äôs the side length of a hybercube that\ncan be expected to contain just 10 of the points, as a function of \\\\(p\\\\)?\n\n__\n\nSolution\n\n__\n\nSee Figure 12.15.\n\n    \n    \n    sideLength = function(p, pointDensity = 1e6, pointsNeeded = 10)\n      (pointsNeeded / pointDensity) ^ (1 / p)\n    ggplot(tibble(p = 1:400, sideLength = sideLength(p)),\n           aes(x = p, y = sideLength)) + geom_line(col = \"red\") +\n      geom_hline(aes(yintercept = 1), linetype = 2)__\n\n[![](12-chap_files/figure-html/fig-\ncursedimans1-1-1.png)](12-chap_files/figure-html/fig-cursedimans1-1-1.png\n\"Figure¬†12.15: Side length of a p-dimensional hybercube expected to contain 10\npoints out of 1 million uniformly distributed ones, as a function of the p.\nWhile for p=1, this length is conveniently small, namely 10/10^6=10^{-5}, for\nlarger p it approaches 1, i.,e., becomes the same as the range of each the\nfeatures. This means that a ‚Äúlocal neighborhood‚Äù of 10 points encompasses\nalmost the same data range as the whole dataset.\")\n\nFigure 12.15: Side length of a \\\\(p\\\\)-dimensional hybercube expected to\ncontain 10 points out of 1 million uniformly distributed ones, as a function\nof the \\\\(p\\\\). While for \\\\(p=1\\\\), this length is conveniently small, namely\n\\\\(10/10^6=10^{-5}\\\\), for larger \\\\(p\\\\) it approaches 1, i.,e., becomes the\nsame as the range of each the features. This means that a ‚Äúlocal neighborhood‚Äù\nof 10 points encompasses almost the same data range as the whole dataset.\n\nNext, let‚Äôs look at the relation between inner regions of the feature space\nversus its boundary regions. Generally speaking, prediction at the boundaries\nof feature space is more difficult than in its interior, as it tends to\ninvolve extrapolation, rather than interpolation. In the next question you‚Äôll\nsee how this difficulty explodes with feature space dimension.\n\n__\n\nQuestion 12.13\n\nWhat fraction of a unit cube‚Äôs total volume is closer than 0.01 to any of its\nsurfaces, as a function of the dimension?\n\n__\n\nSolution\n\n__\n\nSee code below and Figure 12.16.\n\n    \n    \n    tibble(\n      p = 1:400,\n      volOuterCube = 1 ^ p,\n      volInnerCube = 0.98 ^ p,  # 0.98 = 1 - 2 * 0.01\n      `V(shell)` = volOuterCube - volInnerCube) |>\n    ggplot(aes(x = p, y =`V(shell)`)) + geom_line(col = \"blue\")__\n\n[![](12-chap_files/figure-html/fig-\ncursedimans2-1-1.png)](12-chap_files/figure-html/fig-cursedimans2-1-1.png\n\"Figure¬†12.16: Fraction of a unit cube‚Äôs total volume that is in its ‚Äúshell‚Äù\n\\(here operationalised as those points that are closer than 0.01 to its\nsurface\\) as a function of the dimension p.\")\n\nFigure 12.16: Fraction of a unit cube‚Äôs total volume that is in its ‚Äúshell‚Äù\n(here operationalised as those points that are closer than 0.01 to its\nsurface) as a function of the dimension \\\\(p\\\\).\n\n__\n\nQuestion 12.14\n\nWhat is the coefficient of variation (ratio of standard deviation over\naverage) of the distance between two randomly picked points in the unit\nhypercube, as a function of the dimension?\n\n__\n\nSolution\n\n__\n\nWe solve this one by simulation. We generate `n` pairs of random points in the\nhypercube (`x1`, `x2`) and compute their Euclidean distances. See Figure\n12.17. This result can also be predicted from the central limit theorem.\n\n    \n    \n    n = 1000\n    df = tibble(\n      p = round(10 ^ seq(0, 4, by = 0.25)),\n      cv = vapply(p, function(k) {\n        x1 = matrix(runif(k * n), nrow = n)\n        x2 = matrix(runif(k * n), nrow = n)\n        d = sqrt(rowSums((x1 - x2)^2))\n        sd(d) / mean(d)\n      }, FUN.VALUE = numeric(1)))\n    ggplot(df, aes(x = log10(p), y = cv)) + geom_line(col = \"orange\") +\n      geom_point()__\n\n[![](12-chap_files/figure-html/fig-\ncursedimans3-1-1.png)](12-chap_files/figure-html/fig-cursedimans3-1-1.png\n\"Figure¬†12.17: Coefficient of variation \\(CV\\) of the distance between\nrandomly picked points in the unit hypercube, as a function of the dimension.\nAs the dimension increases, everybody is equally far away from everyone else:\nthere is almost no variation in the distances any more.\")\n\nFigure 12.17: Coefficient of variation (CV) of the distance between randomly\npicked points in the unit hypercube, as a function of the dimension. As the\ndimension increases, everybody is equally far away from everyone else: there\nis almost no variation in the distances any more.\n\n## 12.5 Objective functions\n\nWe‚Äôve already seen the **misclassification rate** (MCR) used to assess our\nclassification performance in Figures 12.11‚Äì12.13. Its population version is\ndefined as\n\n\\\\[ \\text{MCR} = \\text{E}\\left[ ùüô_{\\hat{y} \\neq y} \\right], \\text{MCR} =\n\\text{E}\\left[ ùüô_{\\hat{y} \\neq y} \\right], \\tag{12.1}\\\\]\n\nand for a finite sample\n\n\\\\[ \\widehat{\\text{MCR}} = \\frac{1}{n}\\sum_{i=1}^n ùüô_{\\hat{y_i} \\neq y_i}.\n\\tag{12.2}\\\\]\n\nThis is not the only choice we could make. Perhaps we care more about the\nmisclassification of apples as oranges than vice versa, and we can reflect\nthis by introducing weights that depend on the type of error made into the sum\nof Equation 12.2 (or the integral of Equation 12.1). This can get even more\nelaborate if we have more than two classes. Often we want to see the whole\n**confusion table** , which we can get via\n\n    \n    \n    table(truth, response)__\n\nAn important special case is binary classification with asymmetric costs ‚Äì\nthink about, say, a medical test. Here, the **sensitivity** (a.k.a. **true\npositive rate** or **recall**) is related to the misclassification of healthy\nas ill, and the **specificity** (or **true negative rate**) depends on the\nprobability of misclassification of ill as healthy. Often, there is a single\nparameter (e.g., a threshold) that can be moved up and down, allowing a trade-\noff between sensitivity and specificity (and thus, equivalently, between the\ntwo types of misclassification). In those cases, we usually are not content to\nknow the classifier performance at one single choice of threshold, but at many\n(or all) of them. This leads to **receiver operating characteristic**\n(**ROC**) or **precision-recall** curves.\n\n__\n\nQuestion 12.15\n\nWhat are the exact relationships between the per-class misclassification rates\nand sensitivity and specificity?\n\n__\n\nSolution\n\n__\n\nThe sensitivity or true positive rate is\n\n\\\\[ \\text{TPR} = \\frac{\\text{TP}}{\\text{P}}, \\\\]\n\nwhere \\\\(\\text{TP}\\\\) is the number of true positives and \\\\(\\text{P}\\\\) the\nnumber of all positives. The specificity or true negative rate is\n\n\\\\[ \\text{SPC} = \\frac{\\text{TN}}{\\text{N}}, \\\\]\n\nwhere \\\\(\\text{TN}\\\\) is the number of true negatives and \\\\(\\text{N}\\\\) the\nnumber of all negatives. See also\n<https://en.wikipedia.org/wiki/Sensitivity_and_specificity>\n\nAnother cost function can be computed from the **Jaccard index** , which we\nalready saw in [Chapter 5](05-chap.html).\n\n\\\\[ J(A,B) = \\frac{|\\,A\\cap B\\,|}{|\\,A\\cup B\\,|}, \\tag{12.3}\\\\]\n\nwhere \\\\(A\\\\) is the set of observations for which the true class is 1\n(\\\\(A=\\\\{i\\,|\\,y_i=1\\\\}\\\\)) and \\\\(B\\\\) is the set of observations for which\nthe predicted class is 1. The number \\\\(J\\\\) is between 0 and 1, and when\n\\\\(J\\\\) is large, it indicates high overlap of the two sets. Note that \\\\(J\\\\)\ndoes not depend on the number of observations for which both true and\npredicted class is 0 ‚Äì so it is particularly suitable for measuring the\nperformance of methods that try to find rare events.\n\nWe can also consider probabilistic class predictions, which come in the form\n\\\\(\\hat{P}(Y\\,|\\,X)\\\\). In this case, a possible risk function would be\nobtained by looking at distances between the true probability distribution and\nthe estimated probability distributions. For two classes, the finite sample\nversion of the \\\\(\\log \\text{loss}\\\\) is\n\n\\\\[ \\log \\text{loss} = -\\frac{1}{n}\\sum_{i=1}^n y_i\\log(\\hat{p}_i) + (1 -\ny_i)\\log(1 - \\hat{p}_i), \\tag{12.4}\\\\]\n\nwhere \\\\(\\hat{p}_i \\in [0,1]\\\\) is the prediction, and \\\\(y_i\\in\\\\{0,1\\\\}\\\\)\nis the truth.\n\n[![Note that the \\\\log\\\\text{loss} will be infinite if a prediction is totally\nconfident \\(\\\\hat{p}_i is exactly 0 or 1\\) but\nwrong.](imgs/devil.png)](imgs/devil.png \"Note that the \\\\log\\\\text{loss} will\nbe infinite if a prediction is totally confident \\(\\\\hat{p}_i is exactly 0 or\n1\\) but wrong.\")\n\nNote that the \\\\(\\log\\text{loss}\\\\) will be infinite if a prediction is\ntotally confident (\\\\(\\hat{p}_i\\\\) is exactly \\\\(0\\\\) or \\\\(1\\\\)) but wrong.\n\nFor continuous continuous response variables (regression), a natural choice is\nthe **mean squared error** (**MSE**). It is the average squared error,\n\n\\\\[ \\widehat{\\text{MSE}} = \\frac{1}{n}\\sum_{i=1}^n ( \\hat{Y}_i - Y_i )^2.\n\\tag{12.5}\\\\]\n\nThe population version is defined analogously, by turning the summation into\nan integral as in Equations 12.1 and 12.2.\n\nStatisticians call functions like Equations 12.1‚Äî12.5 variously (and depending\non context and predisposition) **risk function** , **cost function** ,\n**objective function** 7.\n\n7 There is even an R package dedicated to evaluation of statistical learners\ncalled **[metrics](https://cran.r-project.org/web/packages/metrics/)**.\n\n## 12.6 Variance‚Äìbias trade-off\n\n![Figure¬†12.18¬†\\(a\\):](imgs/TargetBias.png)\n\n(a)\n\n![Figure¬†12.18¬†\\(b\\):](imgs/TargetVariance.png)\n\n(b)\n\nFigure 12.18: In bull‚Äôs eye (a), the estimates are systematically off target,\nbut in a quite reproducible manner. The green segment represents the bias. In\nbull‚Äôs eye (b), the estimates are not biased, as they are centered in the\nright place, however they have high variance. We can distinguish the two\nscenarios since we see the result from many shots. If we only had one shot and\nmissed the bull‚Äôs eye, we could not easily know whether that‚Äôs because of bias\nor variance.\n\nAn important fact that helps us understand the tradeoffs when picking a\nstatistical learning model is that the MSE is the sum of two terms, and often\nthe choices we can make are such that one of those terms goes down while the\nother one goes up. The bias measures how different the average of all the\ndifferent estimates is from the truth, and variance, how much an individual\none might scatter from the average value (Figure 12.18). In applications, we\noften only get one shot, therefore being reliably almost on target can beat\nbeing right on the long term average but really off today. The decomposition\n\n\\\\[ \\text{MSE} = \\underbrace{\\text{Var}(\\hat{Y})}_{\\text{variance}} \\+\n\\underbrace{\\mathbb{E}[\\hat{Y}-Y]^2}_{\\text{bias}} \\tag{12.6}\\\\]\n\nfollows by straightforward algebra.\n\nWhen trying to minimize the MSE, it is important to realize that sometimes we\ncan pay the price of a small bias to greatly reduce variance, and thus overall\nimprove MSE. We already encountered shrinkage estimation in [Chapter\n8](08-chap.html). In classification (i.e., when we have categorical response\nvariables), different objective functions than the MSE are used, and there is\nusually no such straightforward decomposition as in Equation 12.6. The good\nnews is that we can usually go even much further than in the case of\ncontinuous responses with our trading biases for variance. This is because the\ndiscreteness of the response absorbs certain biases ([Friedman\n1997](16-chap.html#ref-friedmanbiasvariance01)), so that the cost of higher\nbias is almost zero, while we still get the benefit of better (smaller)\nvariance.\n\n### 12.6.1 Penalization\n\nIn high-dimensional statistics, we are constantly plagued by variance: there\nis just not enough data to fit all the possible parameters. One of the most\nfruitful ideas in high-dimensional statistics is **penalization** : a tool to\nactively control and exploit the variance-bias tradeoff. Penalization is part\nof a larger class of regularization methods that are used to ensure stable\nestimates.\n\nAlthough generalization of LDA to high-dimensional settings is possible\n([Clemmensen et al. 2011](16-chap.html#ref-clemmensen2012sparse); [Witten and\nTibshirani 2011](16-chap.html#ref-witten2011penalized)), it turns out that\nlogistic regression is a more general approach8, and therefore we‚Äôll now\nswitch to that, using the\n**[glmnet](https://cran.r-project.org/web/packages/glmnet/)** package.\n\n8 It fits into the framework of generalized linear models, which we\nencountered in [Chapter 8](08-chap.html).\n\nFor multinomial‚Äîor, for the special case of two classes, binomial‚Äîlogistic\nregression models, the posterior log-odds between \\\\(k\\\\) classes and can be\nwritten in the form (see the section on _Logistic Regression_ in the book by\nHastie, Tibshirani, and Friedman ([2008](16-chap.html#ref-\nHastieTibshiraniFriedman)) for a more complete presentation):\n\n\\\\[ \\log \\frac{P(Y=i\\,|\\,X=x)}{P(Y=k\\,|\\,X=x)} = \\beta^0_i + \\beta_i x,\n\\tag{12.7}\\\\]\n\nwhere \\\\(i=1,...,k-1\\\\) enumerates the different classes and the \\\\(k\\\\)-th\nclass is chosen as a reference. The data matrix \\\\(x\\\\) has dimensions\n\\\\(n\\times p\\\\), where \\\\(n\\\\) is number of observations and \\\\(p\\\\) the\nnumber of features. The \\\\(p\\\\)-dimensional vector \\\\(\\beta_i\\\\) determines\nhow the classification odds for class \\\\(i\\\\) versus class \\\\(k\\\\) depend on\n\\\\(x\\\\). The numbers \\\\(\\beta^0_i\\\\) are intercepts and depend, among other\nthings, on the classes‚Äô prior probabilities. Instead of the log odds 12.7\n(i.e., ratios of class probabilities), we can also write down an equivalent\nmodel for the class probabilities themselves, and the fact that we here used\nthe \\\\(k\\\\)-th class as a reference is an arbitrary choice, as the model\nestimates are equivariant under this choice ([Hastie, Tibshirani, and Friedman\n2008](16-chap.html#ref-HastieTibshiraniFriedman)). The model is fit by\nmaximising the log-likelihood \\\\(\\mathcal{l}(\\beta, \\beta^0; x)\\\\), where\n\\\\(\\beta=(\\beta_1,...,\\beta_{k-1})\\\\) and analogously for \\\\(\\beta^0\\\\).\n\nSo far, so good. But as \\\\(p\\\\) gets larger, there is an increasing chance\nthat some of the estimates go wildly off the mark, due to random sampling\nhappenstances in the data (remember Figure 12.1). This is true even if for\neach individual coordinate of the vector \\\\(\\beta_i\\\\), the error distribution\nis bounded: the probabilty of there being one coordinate that is in the far\ntails increases the more coordiates there are, i.e., the larger \\\\(p\\\\) is.\n\nA related problem can also occur, not in 12.7, but in other, non-linear\nmodels, as the model dimension \\\\(p\\\\) increases while the sample size \\\\(n\\\\)\nremains the same: the likelihood landscape around its maximum becomes\nincreasingly flat, and the maximum-likelihood estimate of the model parameters\nbecomes more and more variable. Eventually, the maximum is no longer a point,\nbut a submanifold, and the maximum likelihood estimate is unidentifiable. Both\nof these limitations can be overcome with a modification of the objective:\ninstead of maximising the bare log-likelihood, we maximise a penalized version\nof it,\n\n\\\\[ \\hat{\\beta}= \\arg\\max_\\beta \\mathcal{l}(\\beta, \\beta^0; x) + \\lambda\n\\operatorname{pen}(\\beta), \\tag{12.8}\\\\]\n\nwhere \\\\(\\lambda\\ge0\\\\) is a real number, and \\\\(\\operatorname{pen}\\\\) is a\nconvex function, called the **penalty function**. Popular choices are\n\\\\(\\operatorname{pen}(\\beta)=|\\beta|^2\\\\) (**ridge regression**) and\n\\\\(\\operatorname{pen}(\\beta)=|\\beta|^1\\\\) (**lasso**).\n\n[![Here, |\\\\beta|^\\\\nu=\\\\sum_i\\\\beta_i^\\\\nu is the L_\\\\nu-norm of the vector\n\\\\beta. Variations are possible, for instead we could include in this\nsummation only some but not all of the elements of \\\\beta; or we could scale\ndifferent elements differently, for instance based on some prior belief of\ntheir scale and importance.](imgs/devil.png)](imgs/devil.png \"Here,\n|\\\\beta|^\\\\nu=\\\\sum_i\\\\beta_i^\\\\nu is the L_\\\\nu-norm of the vector \\\\beta.\nVariations are possible, for instead we could include in this summation only\nsome but not all of the elements of \\\\beta; or we could scale different\nelements differently, for instance based on some prior belief of their scale\nand importance.\")\n\nHere, \\\\(|\\beta|^\\nu=\\sum_i\\beta_i^\\nu\\\\) is the \\\\(L_\\nu\\\\)-norm of the\nvector \\\\(\\beta\\\\). Variations are possible, for instead we could include in\nthis summation only some but not all of the elements of \\\\(\\beta\\\\); or we\ncould scale different elements differently, for instance based on some prior\nbelief of their scale and importance.\n\nIn the **elastic net** , ridge and lasso are hybridized by using the penalty\nfunction \\\\(\\operatorname{pen}(\\beta)=(1-\\alpha)|\\beta|^1+\\alpha|\\beta|^2\\\\)\nwith some further parameter \\\\(\\alpha\\in[0,1]\\\\). The crux is, of course, how\nto choose the right \\\\(\\lambda\\\\), and we will discuss that in the following.\n\n### 12.6.2 Example: predicting colon cancer from stool microbiome composition\n\nZeller et al. ([2014](16-chap.html#ref-Zeller:MSB:2014)) studied metagenome\nsequencing data from fecal samples of 156 humans that included colorectal\ncancer patients and tumor-free controls. Their aim was to see whether they\ncould identify biomarkers (presence or abundance of certain taxa) that could\nhelp with early tumor detection. The data are available from\n[Bioconductor](https://www.bioconductor.org) through its **ExperimentHub**\nservice under the identifier EH361.\n\n    \n    \n    library(\"ExperimentHub\")\n    eh = ExperimentHub()\n    zeller = eh[[\"EH361\"]]__\n    \n    \n    table(zeller$disease)__\n    \n    \n           cancer large_adenoma             n small_adenoma \n               53            15            61            27 \n\n__\n\nQuestion 12.16\n\nExplore the `eh` object to see what other datasets there are.\n\n__\n\nSolution\n\n__\n\nType `eh` into the R prompt and study the output.\n\nFor the following, let‚Äôs focus on the normal and cancer samples and set the\nadenomas aside.\n\n    \n    \n    zellerNC = zeller[, zeller$disease %in% c(\"n\", \"cancer\")]__\n\nBefore jumping into model fitting, as always it‚Äôs a good idea to do some\nexploration of the data. First, let‚Äôs look at the sample annotations. The\nfollowing code prints the data from three randomly picked samples. (Only\nlooking at the first ones, say with the R function `head`, is also an option,\nbut may not be representative of the whole dataset).\n\n    \n    \n    pData(zellerNC)[ sample(ncol(zellerNC), 3), ]__\n    \n    \n                       subjectID age gender bmi country disease tnm_stage\n    CCIS50148151ST-4-0    FR-503  87 female  15  france  cancer    t2n1m0\n    CCIS16383318ST-4-0    FR-139  61 female  24  france       n      <NA>\n    CCIS95097901ST-4-0    FR-696  52   male  24  france       n      <NA>\n                       ajcc_stage localization     fobt wif-1_gene_methylation_test\n    CCIS50148151ST-4-0        iii           rc negative                    negative\n    CCIS16383318ST-4-0       <NA>         <NA> negative                    negative\n    CCIS95097901ST-4-0       <NA>         <NA> negative                    negative\n                         group bodysite ethnicity number_reads\n    CCIS50148151ST-4-0     crc    stool     white     54709150\n    CCIS16383318ST-4-0 control    stool     white     78085760\n    CCIS95097901ST-4-0 control    stool     white     51567166\n\nNext, let‚Äôs explore the feature names:\n\n[![We define the helper function formatfn to line wrap these long character\nstrings for the available space here.](imgs/devil.png)](imgs/devil.png \"We\ndefine the helper function formatfn to line wrap these long character strings\nfor the available space here.\")\n\nWe define the helper function `formatfn` to line wrap these long character\nstrings for the available space here.\n\n    \n    \n    formatfn = function(x)\n       gsub(\"|\", \"| \", x, fixed = TRUE) |> lapply(strwrap)\n    \n    rownames(zellerNC)[1:4]__\n    \n    \n    [1] \"k__Bacteria\"                  \"k__Viruses\"                  \n    [3] \"k__Bacteria|p__Firmicutes\"    \"k__Bacteria|p__Bacteroidetes\"\n    \n    \n    rownames(zellerNC)[nrow(zellerNC) + (-2:0)] |> formatfn()__\n    \n    \n    [[1]]\n    [1] \"k__Bacteria| p__Proteobacteria| c__Deltaproteobacteria|\"         \n    [2] \"o__Desulfovibrionales| f__Desulfovibrionaceae| g__Desulfovibrio|\"\n    [3] \"s__Desulfovibrio_termitidis\"                                     \n    \n    [[2]]\n    [1] \"k__Viruses| p__Viruses_noname| c__Viruses_noname| o__Viruses_noname|\"\n    [2] \"f__Baculoviridae| g__Alphabaculovirus|\"                              \n    [3] \"s__Bombyx_mori_nucleopolyhedrovirus|\"                                \n    [4] \"t__Bombyx_mori_nucleopolyhedrovirus_unclassified\"                    \n    \n    [[3]]\n    [1] \"k__Bacteria| p__Proteobacteria| c__Deltaproteobacteria|\"         \n    [2] \"o__Desulfovibrionales| f__Desulfovibrionaceae| g__Desulfovibrio|\"\n    [3] \"s__Desulfovibrio_termitidis| t__GCF_000504305\"                   \n\nAs you can see, the features are a mixture of abundance quantifications at\ndifferent taxonomic levels, from _k_ ingdom over _p_ hylum to _s_ pecies. We\ncould select only some of these, but here we continue with all of them. Next,\nlet‚Äôs look at the distribution of some of the features. Here, we show an\narbitrary choice of two, number 510 and 527; in practice, it is helpful to\nscroll through many such plots quickly to get an impression (Figure 12.19).\n\n    \n    \n    ggplot(melt(Biobase::exprs(zellerNC)[c(510, 527), ]), aes(x = value)) +\n        geom_histogram(bins = 25) +\n        facet_wrap( ~ Var1, ncol = 1, scales = \"free\")__\n\n[![](12-chap_files/figure-html/fig-zellerHist-1-1.png)](12-chap_files/figure-\nhtml/fig-zellerHist-1-1.png \"Figure¬†12.19: Histograms of the distributions for\ntwo randomly selected features. The distributions are highly skewed, with many\nzero values and a thin, long tail of non-zero values.\")\n\nFigure 12.19: Histograms of the distributions for two randomly selected\nfeatures. The distributions are highly skewed, with many zero values and a\nthin, long tail of non-zero values.\n\nIn the simplest case, we fit model 12.7 as follows.\n\n    \n    \n    library(\"glmnet\")\n    glmfit = glmnet(x = t(Biobase::exprs(zellerNC)),\n                    y = factor(zellerNC$disease),\n                    family = \"binomial\")__\n\nA remarkable feature of the `glmnet` function is that it fits 12.7 not only\nfor one choice of \\\\(\\lambda\\\\), but for all possible \\\\(\\lambda\\\\)s at once.\nFor now, let‚Äôs look at the prediction performance for, say,\n\\\\(\\lambda=0.04\\\\). The name of the function parameter is `s`:\n\n    \n    \n    predTrsf = predict(glmfit, newx = t(Biobase::exprs(zellerNC)),\n                       type = \"class\", s = 0.04)\n    table(predTrsf, zellerNC$disease)__\n    \n    \n            \n    predTrsf cancer  n\n      cancer     51  0\n      n           2 61\n\nNot bad ‚Äì but remember that this is on the training data, without cross-\nvalidation. Let‚Äôs have a closer look at `glmfit`. The\n**[glmnet](https://cran.r-project.org/web/packages/glmnet/)** package offers a\na diagnostic plot that is worth looking at (Figure 12.20).\n\n    \n    \n    plot(glmfit, col = RColorBrewer::brewer.pal(8, \"Dark2\"), lwd = sqrt(3), ylab = \"\")__\n\n[![](12-chap_files/figure-html/fig-plotglmfit-1-1.png)](12-chap_files/figure-\nhtml/fig-plotglmfit-1-1.png \"Figure¬†12.20: Regularization paths for glmfit.\")\n\nFigure 12.20: Regularization paths for `glmfit`.\n\n__\n\nQuestion 12.17\n\nWhat are the \\\\(x\\\\)\\- and \\\\(y\\\\)-axes in Figure 12.20? What are the\ndifferent lines?\n\n__\n\nSolution\n\n__\n\nConsult the manual page of the function `plot.glmnet` in the\n**[glmnet](https://cran.r-project.org/web/packages/glmnet/)** package.\n\nLet‚Äôs get back to the question of how to choose the parameter \\\\(\\lambda\\\\).\nWe could try many different choices ‚Äìand indeed, all possible choices‚Äì of\n\\\\(\\lambda\\\\), assess classification performance in each case using cross-\nvalidation, and then choose the best \\\\(\\lambda\\\\).\n\n[![You‚Äôll already realize from the description of this strategy that if we\noptimize \\\\lambda in this way, the resulting apparent classification\nperformance will likely be exaggerated. We need a truly independent dataset,\nor at least another, outer cross-validation loop to get a more realistic\nimpression of the generalizability. We will get back to this question at the\nend of the chapter.](imgs/devil.png)](imgs/devil.png \"You‚Äôll already realize\nfrom the description of this strategy that if we optimize \\\\lambda in this\nway, the resulting apparent classification performance will likely be\nexaggerated. We need a truly independent dataset, or at least another, outer\ncross-validation loop to get a more realistic impression of the\ngeneralizability. We will get back to this question at the end of the\nchapter.\")\n\nYou‚Äôll already realize from the description of this strategy that if we\noptimize \\\\(\\lambda\\\\) in this way, the resulting apparent classification\nperformance will likely be exaggerated. We need a truly independent dataset,\nor at least another, outer cross-validation loop to get a more realistic\nimpression of the generalizability. We will get back to this question at the\nend of the chapter.\n\nWe could do so by writing a loop as we did in the `estimate_mcl_loocv`\nfunction in Section 12.4.1. It turns out that the\n**[glmnet](https://cran.r-project.org/web/packages/glmnet/)** package already\nhas built-in functionality for that, with the function `cv.glmnet`, which we\ncan use instead.\n\n    \n    \n    cvglmfit = cv.glmnet(x = t(Biobase::exprs(zellerNC)),\n                         y = factor(zellerNC$disease),\n                         family = \"binomial\")\n    plot(cvglmfit)__\n\n[![](12-chap_files/figure-html/fig-colonCV-1-1.png)](12-chap_files/figure-\nhtml/fig-colonCV-1-1.png \"Figure¬†12.21: Diagnostic plot for cv.glmnet: shown\nis a measure of cross-validated prediction performance, the deviance, as a\nfunction of \\\\lambda. The dashed vertical lines show lambda.min and\nlambda.1se.\")\n\nFigure 12.21: Diagnostic plot for `cv.glmnet`: shown is a measure of cross-\nvalidated prediction performance, the deviance, as a function of\n\\\\(\\lambda\\\\). The dashed vertical lines show `lambda.min` and `lambda.1se`.\n\nThe diagnostic plot is shown in Figure 12.21. We can access the optimal value\nwith\n\n    \n    \n    cvglmfit$lambda.min __\n    \n    \n    [1] 0.0529391\n\nAs this value results from finding a minimum in an estimated curve, it turns\nout that it is often too small, i.e., that the implied penalization is too\nweak. A heuristic recommended by the authors of the\n**[glmnet](https://cran.r-project.org/web/packages/glmnet/)** package is to\nuse a somewhat larger value instead, namely the largest value of \\\\(\\lambda\\\\)\nsuch that the performance measure is within 1 standard error of the minimum.\n\n    \n    \n    cvglmfit$lambda.1se __\n    \n    \n    [1] 0.08830775\n\n__\n\nQuestion 12.18\n\nHow does the confusion table look like for \\\\(\\lambda=\\;\\\\)`lambda.1se`?\n\n__\n\nSolution\n\n__\n\n    \n    \n    s0 = cvglmfit$lambda.1se\n    predict(glmfit, newx = t(Biobase::exprs(zellerNC)),type = \"class\", s = s0) |>\n        table(zellerNC$disease)__\n    \n    \n            \n             cancer  n\n      cancer     38  5\n      n          15 56\n\n__\n\nQuestion 12.19\n\nWhat features drive the classification?\n\n__\n\nSolution\n\n__\n\n    \n    \n    coefs = coef(glmfit)[, which.min(abs(glmfit$lambda - s0))]\n    topthree = order(abs(coefs), decreasing = TRUE)[1:3]\n    as.vector(coefs[topthree])__\n    \n    \n    [1] -71.471393  -8.770704  -1.465249\n    \n    \n    formatfn(names(coefs)[topthree])__\n    \n    \n    [[1]]\n    [1] \"k__Bacteria| p__Candidatus_Saccharibacteria|\"      \n    [2] \"c__Candidatus_Saccharibacteria_noname|\"            \n    [3] \"o__Candidatus_Saccharibacteria_noname|\"            \n    [4] \"f__Candidatus_Saccharibacteria_noname|\"            \n    [5] \"g__Candidatus_Saccharibacteria_noname|\"            \n    [6] \"s__candidate_division_TM7_single_cell_isolate_TM7b\"\n    \n    [[2]]\n    [1] \"k__Bacteria| p__Firmicutes| c__Clostridia| o__Clostridiales|\"        \n    [2] \"f__Ruminococcaceae| g__Subdoligranulum| s__Subdoligranulum_variabile\"\n    \n    [[3]]\n    [1] \"k__Bacteria| p__Firmicutes| c__Clostridia| o__Clostridiales|\"\n    [2] \"f__Lachnospiraceae| g__Lachnospiraceae_noname|\"              \n    [3] \"s__Lachnospiraceae_bacterium_7_1_58FAA\"                      \n\n__\n\nQuestion 12.20\n\nHow do the results change if we transform the data, say, with the `asinh`\ntransformation as we saw in [Chapter 5](05-chap.html)?\n\n__\n\nSolution\n\n__\n\nSee Figure 12.22.\n\n    \n    \n    cv.glmnet(x = t(asinh(Biobase::exprs(zellerNC))),\n              y = factor(zellerNC$disease),\n              family = \"binomial\") |> plot()__\n\n[![](12-chap_files/figure-html/fig-colonCVTrsf-1-1.png)](12-chap_files/figure-\nhtml/fig-colonCVTrsf-1-1.png \"Figure¬†12.22: like Figure¬†fig-colonCV-1, but\nusing an \\\\text{asinh} transformation of the data.\")\n\nFigure 12.22: like Figure 12.21, but using an \\\\(\\text{asinh}\\\\)\ntransformation of the data.\n\n__\n\nQuestion 12.21\n\nWould a good classification performance on these data mean that this assay is\nready for screening and early cancer detection?\n\n__\n\nSolution\n\n__\n\nNo. The performance here is measured on a set of samples in which the cases\nhave similar prevalence as the controls. This serves well enough to explore\nthe biology. However, in a real-life application, the cases will be much less\nfrequent. To be practically useful, the assay must have a much higher\nspecificity, i.e., rarely diagnose disease where there is none. To establish\nspecificity, a much larger set of normal samples need to be tested.\n\n### 12.6.3 Example: classifying mouse cells from their expression profiles\n\nFigures 12.21 and 12.22 are textbook examples of how we expect the dependence\nof (cross-validated) classification performance versus model complexity\n(\\\\(\\lambda\\\\)) to look. Now let‚Äôs get back to the mouse embryo cells data.\nWe‚Äôll try to classify the cells from embryonic day `E3.25` with respect to\ntheir genotype.\n\n    \n    \n    sx = x[, x$Embryonic.day == \"E3.25\"]\n    embryoCellsClassifier = cv.glmnet(t(Biobase::exprs(sx)), sx$genotype,\n                    family = \"binomial\", type.measure = \"class\")\n    plot(embryoCellsClassifier)__\n\n[![](12-chap_files/figure-html/fig-\nmousecvglmfit-1-1.png)](12-chap_files/figure-html/fig-mousecvglmfit-1-1.png\n\"Figure¬†12.23: Cross-validated misclassification error versus penalty\nparameter for the mouse cells data.\")\n\nFigure 12.23: Cross-validated misclassification error versus penalty parameter\nfor the mouse cells data.\n\nIn Figure 12.23 we see that the misclassification error is (essentially)\nmonotonously increasing with \\\\(\\lambda\\\\), and is smallest for \\\\(\\lambda\\to\n0\\\\), i.e., if we apply no penalization at all.\n\n__\n\nQuestion 12.22\n\nWhat is going on with these data?\n\n__\n\nSolution\n\n__\n\nIt looks that inclusion of more, and even of all features, does not harm the\nclassification performance. In a way, these data are ‚Äútoo easy‚Äù. Let‚Äôs do a\n\\\\(t\\\\)-test for all features:\n\n    \n    \n    mouse_de = rowttests(sx, \"genotype\")\n    ggplot(mouse_de, aes(x = p.value)) +\n      geom_histogram(boundary = 0, breaks = seq(0, 1, by = 0.01))__\n\n[![](12-chap_files/figure-html/fig-\nmousecellsrowttst-1-1.png)](12-chap_files/figure-html/fig-\nmousecellsrowttst-1-1.png \"Figure¬†12.24: Histogram of p-values for the per-\nfeature t-tests between genotypes in the E3.25 cells.\")\n\nFigure 12.24: Histogram of p-values for the per-feature \\\\(t\\\\)-tests between\ngenotypes in the E3.25 cells.\n\nThe result, shown in Figure 12.24, shows that large number of genes are\ndifferentially expressed, and thus informative for the class distinction. We\ncan also compute the pairwise distances between all cells, using all features.\n\n    \n    \n    dists = as.matrix(dist(scale(t(Biobase::exprs(x)))))\n    diag(dists) = +Inf __\n\nand then for each cell determine the class of its nearest neighbor\n\n    \n    \n    nn = sapply(seq_len(ncol(dists)), function(i) which.min(dists[, i]))\n    table(x$sampleGroup, x$sampleGroup[nn]) |> `colnames<-`(NULL)__\n    \n    \n                     \n                      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n      E3.25             33    0    0    0    3    0    0    0\n      E3.25 (FGF4-KO)    1   15    0    1    0    0    0    0\n      E3.5 (EPI)         2    0    3    0    6    0    0    0\n      E3.5 (FGF4-KO)     0    0    0    8    0    0    0    0\n      E3.5 (PE)          0    0    0    0   11    0    0    0\n      E4.5 (EPI)         0    0    0    0    2    2    0    0\n      E4.5 (FGF4-KO)     1    0    0    0    0    0    9    0\n      E4.5 (PE)          0    0    0    0    2    0    0    2\n\nUsing all features, the 1 nearest-neighbor classifier is correct in almost all\ncases, including for the E3.25 wildtype vs FGF4-KO distinction. This means\nthat for these data, there is no apparent benefit in regularization or feature\nselection. Limitations of using all features might become apparent with truly\nnew data, but that is out of reach for cross-validation.\n\n## 12.7 A large choice of methods\n\nWe have now seen three classification methods: linear discriminant analysis\n(`lda`), quadratic discriminant analysis (`qda`) and logistic regression using\nelastic net penalization (`glmnet`). In fact, there are hundreds of different\nlearning algorithms9 available in R and its add-on packages. You can get an\noverview in the CRAN task view [Machine Learning & Statistical\nLearning](https://cran.r-project.org/web/views/MachineLearning.html). Some\nexamples are:\n\n9 For an introduction to the subject that uses R and provides many examples\nand exercises, we recommend ([James et al. 2013](16-chap.html#ref-\nJames:2013)).\n\n  * Support vector machines: the function `svm` in the package **[e1071](https://cran.r-project.org/web/packages/e1071/)** ; `ksvm` in **[kernlab](https://cran.r-project.org/web/packages/kernlab/)**\n\n  * Tree based methods in the packages **[rpart](https://cran.r-project.org/web/packages/rpart/)** , **[tree](https://cran.r-project.org/web/packages/tree/)** , **[randomForest](https://cran.r-project.org/web/packages/randomForest/)**\n\n  * Boosting methods: the functions `glmboost` and `gamboost` in package **[mboost](https://cran.r-project.org/web/packages/mboost/)**\n\n  * `PenalizedLDA` in the package **[PenalizedLDA](https://cran.r-project.org/web/packages/PenalizedLDA/)** , `dudi.discr` and `dist.pcaiv` in **[ade4](https://cran.r-project.org/web/packages/ade4/)**).\n\nThe complexity and heterogeneity of choices of learning strategies, tuning\nparameters and evaluation criteria in each of these packages can be confusing.\nYou will already have noted differences in the interfaces of the `lda`, `qda`\nand `glmnet` functions, i.e., in how they expect their input data to presented\nand what they return. There is even greater diversity across all the other\npackages and functions. At the same time, there are common tasks such as\ncross-validation, parameter tuning and performance assessment that are more or\nless the same no matter what specific method is used. As you have seen, e.g.,\nin our `estimate_mcl_loocv` function, the looping and data shuffling involved\nled to rather verbose code.\n\nSo what to do if you want to try out and explore different learning\nalgorithms? Fortunately, there are several projects that provide unified\ninterfaces to the large number of different machine learning interfaces in R,\nand also try to provide ‚Äúbest practice‚Äù implementations of the common tasks\nsuch as parameter tuning and performance assessment. The two most well-known\nones are the packages\n**[caret](https://cran.r-project.org/web/packages/caret/)** and\n**[mlr](https://cran.r-project.org/web/packages/mlr/)**. Here were have a look\nat **[caret](https://cran.r-project.org/web/packages/caret/)**. You can get a\nlist of supported methods through its `getModelInfo` function. There are quite\na few, here we just show the first 8.\n\n    \n    \n    library(\"caret\")\n    caretMethods = names(getModelInfo())\n    head(caretMethods, 8)__\n    \n    \n    [1] \"ada\"         \"AdaBag\"      \"AdaBoost.M1\" \"adaboost\"    \"amdai\"      \n    [6] \"ANFIS\"       \"avNNet\"      \"awnb\"       \n    \n    \n    length(caretMethods)__\n    \n    \n    [1] 239\n\nWe will check out a neural network method, the `nnet` function from the\neponymous package. The `parameter` slot informs us on the the available tuning\nparameters10.\n\n10 They are described in the manual of the `nnet` function.\n\n    \n    \n    getModelInfo(\"nnet\", regex = FALSE)[[1]]$parameter __\n    \n    \n      parameter   class         label\n    1      size numeric #Hidden Units\n    2     decay numeric  Weight Decay\n\nLet‚Äôs try it out.\n\n    \n    \n    trnCtrl = trainControl(\n      method = \"repeatedcv\",\n      repeats = 3,\n      classProbs = TRUE)\n    tuneGrid = expand.grid(\n      size = c(2, 4, 8),\n      decay = c(0, 1e-2, 1e-1))\n    nnfit = train(\n      Embryonic.day ~ Fn1 + Timd2 + Gata4 + Sox7,\n      data = embryoCells,\n      method = \"nnet\",\n      tuneGrid  = tuneGrid,\n      trControl = trnCtrl,\n      metric = \"Accuracy\")__\n\nThat‚Äôs quite a mouthful, but the nice thing is that this syntax is\nstandardized and applies across many different methods. All you need to do\nspecify the name of the method and the grid of tuning parameters that should\nbe explored via the `tuneGrid` argument.\n\nNow we can have a look at the output (Figure 12.25).\n\n    \n    \n    nnfit __\n    \n    \n    Neural Network \n    \n    66 samples\n     4 predictor\n     3 classes: 'E3.25', 'E3.5', 'E4.5' \n    \n    No pre-processing\n    Resampling: Cross-Validated (10 fold, repeated 3 times) \n    Summary of sample sizes: 60, 59, 60, 60, 59, 59, ... \n    Resampling results across tuning parameters:\n    \n      size  decay  Accuracy   Kappa    \n      2     0.00   0.7083333  0.4279755\n      2     0.01   0.7676587  0.5858922\n      2     0.10   0.7681349  0.5767463\n      4     0.00   0.7515476  0.5638426\n      4     0.01   0.8004762  0.6486256\n      4     0.10   0.7638889  0.5676798\n      8     0.00   0.7385714  0.5393148\n      8     0.01   0.7348016  0.5281220\n      8     0.10   0.7532540  0.5525435\n    \n    Accuracy was used to select the optimal model using the largest value.\n    The final values used for the model were size = 4 and decay = 0.01.\n    \n    \n    plot(nnfit)\n    predict(nnfit) |> head(10)__\n    \n    \n     [1] E3.25 E3.25 E3.25 E3.25 E3.25 E3.25 E3.25 E3.25 E3.25 E3.25\n    Levels: E3.25 E3.5 E4.5\n\n[![](12-chap_files/figure-html/fig-ML-nnfit-1.png)](12-chap_files/figure-\nhtml/fig-ML-nnfit-1.png \"Figure¬†12.25: Parameter tuning of the neural net by\ncross-validation.\")\n\nFigure 12.25: Parameter tuning of the neural net by cross-validation.\n\n__\n\nQuestion 12.23\n\nWill the accuracy that we obtained above for the optimal tuning parameters\ngeneralize to a new dataset? What could you do to address that?\n\n__\n\nSolution\n\n__\n\nNo, it is likely to be too optimistic, as we have picked the optimum. To get a\nsomewhat more realistic estimate of prediction performance when generalized,\nwe could formalize (into computer code) all our data preprocessing choices and\nthe above parameter tuning procedure, and embed this in another, outer cross-\nvalidation loop ([Ambroise and McLachlan 2002](16-chap.html#ref-\nambroise2002selection)). However, this is likely still not enough, as we\ndiscuss in the next section.\n\n### 12.7.1 Method hacking\n\nIn [Chapter 6](06-chap.html) we encountered _p-value hacking_. A similar\nphenomenon exists in statistical learning: given a dataset, we explore various\ndifferent methods of preprocessing (such as normalization, outlier detection,\ntransformation, feature selection), try out different machine learning\nalgorithms and tune their parameters until we are content with the result. The\nmeasured accuracy is likely to be too optimistic, i.e., will not generalize to\na new dataset. Embedding as many of our methodical choices into a\ncomputational formalism and having an outer cross-validation loop (not to be\nconfused with the inner loop that does the parameter tuning) will ameliorate\nthe problem. But is unlikely to address it completely, since not all our\nchoices can be formalized.\n\nThe gold standard remains validation on truly unseen data. In addition, it is\nnever a bad thing if the classifier is not a black box but can be interpreted\nin terms of domain knowledge. Finally, report not just summary statistics,\nsuch as misclassification rates, but lay open the complete computational\nworkflow, so that anyone (including your future self) can convince themselves\nof the robustness of the result or of the influence of the preprocessing,\nmodel selection and tuning choices ([Holmes 2018](16-chap.html#ref-\nHolmes2017)).\n\n## 12.8 Summary of this chapter\n\nWe have seen examples of machine learning applications; we have focused on\npredicting categorical variables (like diabetes type or cell class).\nPredicting continuous outcomes is also part of machine learning, although we\nhave not considered it here. There are many parallels and overlaps between\n_machine learning_ and _statistical regression_ (which we studied in [Chapter\n8](08-chap.html)). One can consider them two different names for pretty much\nthe same activity, although each has its own flavors: in machine learning, the\nemphasis is on the prediction of the outcome variables, whereas in regression\nwe often care at least as much about the role of the covariates ‚Äì which of\nthem have an effect on the outcome, and what is the nature of these effects?\nIn other words, we do not only want predictions, we also want to understand\nthem.\n\nWe saw linear and quadratic discriminant analysis, two intuitive methods for\npartitioning a two-dimensional data plane (or a \\\\(p\\\\)-dimensional space)\ninto regions using either linear or quadratic separation lines (or\nhypersurfaces). We also saw logistic regression, which takes a slightly\ndifferent approach but is more amenable to operating in higher dimensions and\nto regularization.\n\nWe encountered the main challenge of machine learning: how to avoid\noverfitting? We explored why overfitting happens in the context of the so-\ncalled curse of dimensionality, and we learned how it may be overcome using\nregularization.\n\nIn other words, machine learning would be easy if we had infinite amounts of\ndata representatively covering the whole space of possible inputs and\noutputs11. The challenge is to make the best out of a finite amount of\ntraining data, and to generalize these to new, unseen inputs. There is a\nvigorous trade-off between the amount, resolution and coverage of training\ndata and the complexity of the model. Many models have continuous parameters\nthat enable us to ‚Äútune‚Äù their complexity or the strength of their\nregularization. Cross-validation can help us with such tuning, although it is\nnot a panacea, and caveats apply, as we saw in Section 12.6.3.\n\n11 It would ‚Äújust‚Äù be a formidable database / data management problem.\n\n## 12.9 Further reading\n\n  * An introduction to statistical learning that employs many concrete data examples and uses little mathematical formalism is given by James et al. ([2013](16-chap.html#ref-James:2013)). An extension, with more mathematical background, is the textbook by Hastie, Tibshirani, and Friedman ([2008](16-chap.html#ref-HastieTibshiraniFriedman)).\n\n  * The [CRAN task view on machine learning](https://cran.r-project.org/web/views/MachineLearning.html) gives an overview over machine learning software in R.\n\n  * [RStudio‚Äôs API for the ‚Äúdeep learning‚Äù platforms Keras and TensorFlow](https://tensorflow.rstudio.com/keras) and the associated teaching materials and demos are a good place to try out some of the recent developments in this field.\n\n## 12.10 Exercises\n\n__\n\nExercise 12.1\n\nApply a _kernel support vector machine_ , available in the\n**[kernlab](https://cran.r-project.org/web/packages/kernlab/)** package, to\nthe `zeller` microbiome data. What kernel function works well?\n\n__\n\nExercise 12.2\n\nUse `glmnet` for a _prediction of a continuous variable_ , i.e., for\nregression. Use the prostate cancer data from Chapter 3 of ([Hastie,\nTibshirani, and Friedman 2008](16-chap.html#ref-HastieTibshiraniFriedman)).\nThe data are available in the CRAN package\n**[ElemStatLearn](https://cran.r-project.org/web/packages/ElemStatLearn/)**.\nExplore the effects of using ridge versus lasso penalty.\n\n__\n\nExercise 12.3\n\nConsider _smoothing as a regression and model selection problem_ (remember\nFigure 12.1). What is the equivalent quantity to the penalization parameter\n\\\\(\\lambda\\\\) in Equation 12.8? How do you choose it?\n\n__\n\nSolution\n\n__\n\nWe refer to Chapter 5 of ([Hastie, Tibshirani, and Friedman\n2008](16-chap.html#ref-HastieTibshiraniFriedman))\n\n__\n\nExercise 12.4\n\n_Scale invariance_. Consider a rescaling of one of the features in the\n(generalized) linear model 12.7. For instance, denote the \\\\(\\nu\\\\)-th column\nof \\\\(x\\\\) by \\\\(x_{\\cdot\\nu}\\\\), and suppose that \\\\(p\\ge2\\\\) and that we\nrescale \\\\(x_{\\cdot\\nu} \\mapsto s\\, x_{\\cdot\\nu}\\\\) with some number\n\\\\(s\\neq0\\\\). What will happen to the estimate \\\\(\\hat{\\beta}\\\\) from Equation\n12.8 in (a) the unpenalized case (\\\\(\\lambda=0\\\\)) and (b) the penalized case\n(\\\\(\\lambda>0\\\\))?\n\n__\n\nSolution\n\n__\n\nIn the unpenalized case, the estimates will be scaled by \\\\(1/s\\\\), so that\nthe resulting model is, in effect, the same. In the penalized case, the\npenalty from the \\\\(\\nu\\\\)-th component of \\\\(\\beta\\\\) will be different. If\n\\\\(|s|>1\\\\), the amplitude of the feature is increased, smaller\n\\\\(\\beta\\\\)-components are required for it to have the same effect in the\nprediction, and therefore the feature is more likely to receive a non-zero\nand/or larger estimate, possibly on the cost of the other features; conversely\nfor \\\\(|s|<1\\\\). Regular linear regression is scale-invariant, whereas\npenalized regression is scale-dependent. It‚Äôs important to remember this when\ninterpreting penalized model fits.\n\n__\n\nExercise 12.5\n\nIt has been quipped that all classification methods are just refinements of\n_two archetypal ideas_ : discriminant analysis and \\\\(k\\\\) nearest neighbors.\nIn what sense might that be a useful classification?\n\n__\n\nSolution\n\n__\n\nIn linear discriminant analysis, we consider our objects as elements of\n\\\\(\\mathbb{R}^p\\\\), and the learning task is to define regions in this space,\nor boundary hyperplanes between them, which we use to predict the class\nmembership of new objects. This is archetypal for _classification by\npartition_. Generalizations of linear discriminant analysis permit more\ngeneral spaces and more general boundary shapes.\n\nIn \\\\(k\\\\) nearest neighbors, no embedding into a coordinate space is needed,\nbut instead we require a distance (or dissimilarity) measure that can be\ncomputed between each pair of objects, and the classification decision for a\nnew object depends on its distances to the training objects and their classes.\nThis is archetypal for _kernel-based_ methods.\n\nAmbroise, Christophe, and Geoffrey J. McLachlan. 2002. ‚ÄúSelection Bias in Gene\nExtraction on the Basis of Microarray Gene-Expression Data.‚Äù _PNAS_ 99 (10):\n6562‚Äì66.\n\nBellman, Richard Ernest. 1961. _Adaptive Control Processes: A Guided Tour_.\nPrinceton University Press.\n\nClemmensen, Line, Trevor Hastie, Daniela Witten, and Bjarne Ersb√∏ll. 2011.\n‚ÄúSparse Discriminant Analysis.‚Äù _Technometrics_ 53: 406‚Äì13.\n\nFriedman, Jerome H. 1997. ‚ÄúOn Bias, Variance, 0/1‚ÄîLoss, and the Curse-of-\nDimensionality.‚Äù _Data Mining and Knowledge Discovery_ 1: 55‚Äì77.\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2008. _The Elements of\nStatistical Learning_. 2^{\\text{nd}} ed. Springer.\n\nHolmes, Susan. 2018. ‚ÄúStatistical Proof? The Problem of Irreproducibility.‚Äù\n_Bulletin of the AMS_ 55 (1): 31‚Äì55.\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. _An\nIntroduction to Statistical Learning_. Springer.\n\nNeumann, B., T. Walter, J. K. Heriche, J. Bulkescher, H. Erfle, C. Conrad, P.\nRogers, et al. 2010. ‚ÄúPhenotypic profiling of the human genome by time-lapse\nmicroscopy reveals cell division genes.‚Äù _Nature_ 464 (7289): 721‚Äì27.\n\nOhnishi, Y., W. Huber, A. Tsumura, M. Kang, P. Xenopoulos, K. Kurimoto, A. K.\nOles, et al. 2014. ‚ÄúCell-to-Cell Expression Variability Followed by Signal\nReinforcement Progressively Segregates Early Mouse Lineages.‚Äù _Nature Cell\nBiology_ 16 (1): 27‚Äì37.\n\nReaven, GM, and RG Miller. 1979. ‚ÄúAn Attempt to Define the Nature of Chemical\nDiabetes Using a Multidimensional Analysis.‚Äù _Diabetologia_ 16 (1): 17‚Äì24.\n\nWitten, Daniela M, and Robert Tibshirani. 2011. ‚ÄúPenalized Classification\nUsing Fisher‚Äôs Linear Discriminant.‚Äù _JRSSB_ 73 (5): 753‚Äì72.\n\nZeller, Georg, Julien Tap, Anita Y Voigt, Shinichi Sunagawa, Jens Roat\nKultima, Paul I Costea, Aur√©lien Amiot, et al. 2014. ‚ÄúPotential of Fecal\nMicrobiota for Early-Stage Detection of Colorectal Cancer.‚Äù _Molecular Systems\nBiology_ 10 (11): 766. <https://doi.org/10.15252/msb.20145645>.\n\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["msmb.css"],"toc":true,"output-file":"12-chap.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}