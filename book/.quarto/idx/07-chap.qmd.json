{"title":"7.1 Goals for this chapter","markdown":{"headingText":"7.1 Goals for this chapter","containsRefs":false,"markdown":"![](imgs/TheMatrix.jpg)\n\nMany datasets consist of several variables measured on the same set of\nsubjects: patients, samples, or organisms. For instance, we may have biometric\ncharacteristics such as height, weight, age as well as clinical variables such\nas blood pressure, blood sugar, heart rate, and genetic data for, say, a\nthousand patients. The _raison d’être_ for multivariate analysis is the\ninvestigation of connections or associations between the different variables\nmeasured. Usually the data are reported in a tabular data structure with one\nrow for each subject and one column for each variable. In the following, we\nwill focus on the special case where each of the variables is numeric, so we\ncan represent the data structure as a _matrix_ in R.\n\nIf the columns of the matrix are all independent of each other (unrelated), we\ncan simply study each column separately and do standard “univariate”\nstatistics on them one by one; there would be no benefit in studying them as a\nmatrix.\n\nMore often, there will be patterns and dependencies. For instance, in the\nbiology of cells, we know that the proliferation rate will influence the\nexpression of many genes simultaneously. Studying the expression of 25,000\ngene (columns) on many samples (rows) of patient-derived cells, we notice that\nmany of the genes act together, either that they are positively correlated or\nthat they are anti-correlated. We would miss a lot of important information if\nwe were to only study each gene separately. Important connections between\ngenes are detectable only if we consider the data as a whole: each row\nrepresenting the many measurements made on the same observational unit.\nHowever, having 25,000 dimensions of variation to consider at once is\ndaunting; we will show how to reduce our data to a smaller number of most\nimportant dimensions1 without losing too much information.\n\n1 We will elaborate this idea of dimension reduction in much more detail\nbelow. For the time being, remember we live in a four-dimensional world.\n\nThis chapter presents many examples of multivariate data matrices that we\nencounter in high-throughput experiments, as well as some more elementary\nexamples that we hope will enhance your intuition. We will focus in this\nchapter on **P** rincipal **C** omponent **A** nalysis, abbreviated as **PCA**\n, a **dimension reduction** method. We will provide geometric explanations of\nthe algorithm as well as visualizations that help interprete the output of PCA\nanalyses.\n\n\nIn this chapter we will:\n\n  * See examples of matrices that come up in the study of biological data.\n\n  * Perform dimension reduction to understand correlations between variables.\n\n  * Preprocess, rescale and center the data before starting a multivariate analysis.\n\n  * Build new variables, called principal components (PC), that are more useful than the original measurements.\n\n  * See what is “under the hood” of PCA: the singular value decomposition of a matrix.\n\n  * Visualize what this decomposition achieves and learn how to choose the number of principal components.\n\n  * Run through a complete PCA analysis from start to finish.\n\n  * Project factor covariates onto the PCA map to enable a more useful interpretation of the results.\n\n## 7.2 What are the data? Matrices and their motivation\n\nFirst, let’s look at a set of examples of rectangular **matrices** used to\nrepresent tables of measurements. In each matrix, the rows and columns\nrepresent specific entities.\n\n**Turtles:** A simple data set that will help us understand the basic\nprinciples is a matrix of three dimensions of biometric measurements on\npainted turtles ([Jolicoeur and Mosimann 1960](16-chap.html#ref-\nJolicoeur1960)).\n\n    \n    \n    turtles = read.table(\"../data/PaintedTurtles.txt\", header = TRUE)\n    turtles[1:4, ]__\n    \n    \n      sex length width height\n    1   f     98    81     38\n    2   f    103    84     38\n    3   f    103    86     42\n    4   f    105    86     40\n\nThe last three columns are length measurements (in millimetres), whereas the\nfirst column is a factor variable that tells us the sex of each animal.\n\n**Athletes:** This matrix is an interesting example from the sports world. It\nreports the performances for 33 athletes in the ten disciplines of the\ndecathlon: `m100`, `m400` and `m1500` are times in seconds for the 100 meters,\n400 meters, and 1500 meters respectively; `m110` is the time to finish the 110\nmeters hurdles; `pole` is the pole-vault height, and `high` and `long` are the\nresults of the high and long jumps, all in meters; `weight`, `disc`, and\n`javel` are the lengths in meters the athletes were able to throw the weight,\ndiscus and javelin. Here are these variables for the first three athletes:\n\n    \n    \n    data(\"olympic\", package = \"ade4\")\n    athletes = setNames(olympic$tab, \n      c(\"m100\", \"long\", \"weight\", \"high\", \"m400\", \"m110\", \"disc\", \"pole\", \"javel\", \"m1500\"))\n    athletes[1:3, ]__\n    \n    \n       m100 long weight high  m400  m110  disc pole javel  m1500\n    1 11.25 7.43  15.48 2.27 48.90 15.13 49.28  4.7 61.32 268.95\n    2 10.87 7.45  14.97 1.97 47.71 14.46 44.36  5.1 61.76 273.02\n    3 11.18 7.44  14.20 1.97 48.29 14.81 43.66  5.2 64.16 263.20\n\n**Cell Types:** Holmes et al. ([2005](16-chap.html#ref-holmes2005memory))\nstudied gene expression profiles of sorted T-cell populations from different\nsubjects. The columns are a subset of gene expression measurements, they\ncorrespond to 156 genes that show differential expression between cell types.\n\n    \n    \n    load(\"../data/Msig3transp.RData\")\n    round(Msig3transp,2)[1:5, 1:6]__\n    \n    \n                 X3968 X14831 X13492 X5108 X16348  X585\n    HEA26_EFFE_1 -2.61  -1.19  -0.06 -0.15   0.52 -0.02\n    HEA26_MEM_1  -2.26  -0.47   0.28  0.54  -0.37  0.11\n    HEA26_NAI_1  -0.27   0.82   0.81  0.72  -0.90  0.75\n    MEL36_EFFE_1 -2.24  -1.08  -0.24 -0.18   0.64  0.01\n    MEL36_MEM_1  -2.68  -0.15   0.25  0.95  -0.20  0.17\n\n**Bacterial Species Abundances:** Matrices of counts are used in microbial\necology studies (as we saw in [Chapter 4](04-chap.html)). Here the columns\nrepresent different species (or operational taxonomic units, OTUs) of\nbacteria, which are identified by numerical tags. The rows are labeled\naccording to the samples in which they were measured, and the (integer)\nnumbers represent the number of times of each of the OTUs was observed in each\nof the samples.\n\n    \n    \n    data(\"GlobalPatterns\", package = \"phyloseq\")\n    GPOTUs = as.matrix(t(phyloseq::otu_table(GlobalPatterns)))\n    GPOTUs[1:4, 6:13]__\n    \n    \n    OTU Table:          [4 taxa and 8 samples]\n                         taxa are rows\n            246140 143239 244960 255340 144887 141782 215972 31759\n    CL3          0      7      0    153      3      9      0     0\n    CC1          0      1      0    194      5     35      3     1\n    SV1          0      0      0      0      0      0      0     0\n    M31Fcsw      0      0      0      0      0      0      0     0\n\n[![Notice the propensity of the matrix entries to be zero; we call such data\nsparse.](imgs/devil.png)](imgs/devil.png \"Notice the propensity of the matrix\nentries to be zero; we call such data sparse.\")\n\nNotice the propensity of the matrix entries to be zero; we call such data\n**sparse**.\n\n**mRNA reads:** RNA-Seq transcriptome data report the number of sequence reads\nmatching each gene2 in each of several biological samples. We will study this\ntype of data in detail in [Chapter 8](08-chap.html)\n\n2 Or sub-gene structures, such as exons.\n\n    \n    \n    library(\"SummarizedExperiment\")\n    data(\"airway\", package = \"airway\")\n    assay(airway)[1:3, 1:4]__\n    \n    \n                    SRR1039508 SRR1039509 SRR1039512 SRR1039513\n    ENSG00000000003        679        448        873        408\n    ENSG00000000005          0          0          0          0\n    ENSG00000000419        467        515        621        365\n\nIt is customary in the RNA-Seq field—and so it is for the `airway` data\nabove—to report the genes in the rows and the samples in the columns. Compared\nto the other matrices we look at here, this is _transposed_ : rows and columns\nare swapped. Such different conventions easily lead to errors, so they are\nworthwhile paying attention to3. **Proteomic profiles:** Here, the columns are\naligned **mass spectroscopy** peaks or molecules identified through their\n\\\\(m/z\\\\)-ratios; the entries in the matrix are the measured intensities4.\n\n3 The Bioconductor project tries to help users and developers to avoid such\nambiguities by defining data containers in which such conventions are\nexplicitly fixed. In [Chapter 8](08-chap.html), we will see the example of the\n_SummarizedExperiment_ class.\n\n4 More details can be found, e.g., on\n[Wikipedia](https://en.wikipedia.org/wiki/Mass_spectrum).\n\n    \n    \n    metab = t(as.matrix(read.csv(\"../data/metabolites.csv\", row.names = 1)))\n    metab[1:4, 1:4]__\n    \n    \n             146.0985388 148.7053275 310.1505057 132.4512963\n    KOGCHUM1    29932.36    17055.70     1132.82    785.5129\n    KOGCHUM2    94067.61    74631.69    28240.85   5232.0499\n    KOGCHUM3   146411.33   147788.71    64950.49  10283.0037\n    WTGCHUM1   229912.57   384932.56   220730.39  26115.2007\n\n[![In many of the matrices we have seen here, important information about the\nsamples \\(subjects\\) and the measured features is stored in the row or column\nnames, often through some ad hoc string concatenation. This is not the best\nplace to store all available information, and quickly becomes limiting and\nerror-prone. A much better approach is the Bioconductor SummarizedExperiment\nclass.](imgs/devil.png)](imgs/devil.png \"In many of the matrices we have seen\nhere, important information about the samples \\(subjects\\) and the measured\nfeatures is stored in the row or column names, often through some ad hoc\nstring concatenation. This is not the best place to store all available\ninformation, and quickly becomes limiting and error-prone. A much better\napproach is the Bioconductor SummarizedExperiment class.\")\n\nIn many of the matrices we have seen here, important information about the\nsamples (subjects) and the measured features is stored in the row or column\nnames, often through some ad hoc string concatenation. This is not the best\nplace to store all available information, and quickly becomes limiting and\nerror-prone. A much better approach is the Bioconductor _SummarizedExperiment_\nclass.\n\n__\n\nTask\n\nWhen a peak was not detected for a particular \\\\(m/z\\\\) score in the mass\nspectrometry run, a zero was recorded in `metab`. Similarly, zeros in `GPOTUs`\nor in the `airway` object occur when there were no matching sequence reads\ndetected. Tabulate the frequencies of zeros in these data matrices.\n\n__\n\nQuestion 7.1\n\n  1. What are the columns of these data matrices usually called?\n\n  2. In each of these examples, what are the rows of the matrix?\n\n  3. What does a cell in a matrix represent?\n\n  4. If the data matrix is called `athletes` and you want to see the value of the third variable for the fifth athlete, what do you type into R?\n\n### 7.2.1 Low-dimensional data summaries and preparation\n\n[![](imgs/flatland.png)](imgs/flatland.png \"Figure 7.1: xkcd: What do we mean\nby low-dimensional? We live in 3 dimensions, or 4 if you count time, a plane\nhas 2 dimensions, a line has one dimension. A point is said to be zero-\ndimensional. For the amusing novel referenced in the cartoon see\n@Abbott:1884.\")\n\nFigure 7.1: xkcd: What do we mean by low-dimensional? We live in 3 dimensions,\nor 4 if you count time, a plane has 2 dimensions, a line has one dimension. A\npoint is said to be zero-dimensional. For the amusing novel referenced in the\ncartoon see Abbott ([1884](16-chap.html#ref-Abbott:1884)).\n\nIf we are studying only one variable, i.e., just the third column of the\nturtles matrix5, we say we are looking at one-dimensional data. Such a vector,\nsay all the turtle weights, can be visualized by plots such as those that we\nsaw in [Section 3.6](03-chap.html#sec-graphics-univar), e.g., a histogram. If\nwe compute a one number summary, say mean or median, we have made a zero-\ndimensional summary of our one-dimensional data. This is already an example of\ndimension reduction.\n\n5 The third column of a matrix \\\\(X\\\\) is denoted mathematically by\n\\\\({\\mathbf x}_{\\cdot 3}\\\\) or accessed in R using `X[, 3]`.\n\nIn [Chapter 3](03-chap.html) we studied two-dimensional scatterplots. We saw\nthat if there are too many observations, it can be beneficial to group the\ndata into (hexagonal) bins: these are _two-dimensional_ histograms. When\nconsidering two variables (\\\\(x\\\\) and \\\\(y\\\\)) measured together on a set of\nobservations, the **correlation coefficient** measures how the variables co-\nvary. This is a single number summary of two-dimensional data. Its formula\ninvolves the summaries \\\\(\\bar{x}\\\\) and \\\\(\\bar{y}\\\\):\n\n\\\\[ \\hat{\\rho}= \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}\n{\\sqrt{\\sum_{i=1}^n (x_i-\\bar{x})^2} \\sqrt{\\sum_{j=1}^n (y_j-\\bar{y})^2}}\n\\tag{7.1}\\\\]\n\nIn R, we use the `cor` function to calculate its value. Applied to a matrix\nthis function computes all the two way correlations between continuous\nvariables. In [Chapter 9](09-chap.html) we will see how to analyse\nmultivariate categorical data.\n\n__\n\nQuestion 7.2\n\nCompute the matrix of all correlations between the measurements from the\nturtles data. What do you notice ?\n\n__\n\nSolution\n\n__\n\nWe take out the categorical variable and compute the matrix.\n\n    \n    \n    cor(turtles[, -1])__\n    \n    \n              length     width    height\n    length 1.0000000 0.9783116 0.9646946\n    width  0.9783116 1.0000000 0.9605705\n    height 0.9646946 0.9605705 1.0000000\n\nWe see that this square matrix is symmetric and the values are all close to 1.\nThe diagonal values are always 1.\n\nIt is always beneficial to start a multidimensional analysis by checking these\nsimple one-dimensional and two-dimensional summary statistics using visual\ndisplays such as those we look at in the next two questions.\n\n__\n\nQuestion 7.3\n\n  1. Produce all pairwise scatterplots, as well as the one-dimensional histograms on the diagonal, for the turtles data. Use the package **[GGally](https://cran.r-project.org/web/packages/GGally/)**.  \n\n  2. Guess the underlying or “true dimension” of these data?\n\n__\n\nSolution\n\n__\n\n    \n    \n    library(\"ggplot2\")\n    library(\"dplyr\")\n    library(\"GGally\")\n    ggpairs(turtles[, -1], axisLabels = \"none\")__\n\n[![](07-chap_files/figure-html/fig-turtlespairs-1.png)](07-chap_files/figure-\nhtml/fig-turtlespairs-1.png \"Figure 7.2: All pairs of bivariate scatterplots\nfor the three biometric measurements on painted turtles.\")\n\nFigure 7.2: All pairs of bivariate scatterplots for the three biometric\nmeasurements on painted turtles.\n\nFrom Figure 7.2, it looks like all three of the variables are highly\ncorrelated and mostly reflect the same “underlying” variable, which we might\ninterpret as _the size_ of the turtle.\n\n__\n\nQuestion 7.4\n\nCompute all pairwise correlations of the variables in the `athletes` data and\ndisplay the matrix in a heatmap. What do you notice?\n\n__\n\nSolution\n\n__\n\n    \n    \n    library(\"pheatmap\")\n    pheatmap(cor(athletes), cell.width = 10, cell.height = 10)__\n\n[![](07-chap_files/figure-html/fig-\nheatmapathletes-1.png)](07-chap_files/figure-html/fig-heatmapathletes-1.png\n\"Figure 7.3: Heatmap of correlations between variables in the athletes data.\nHigher values are color coded red-orange. The hierarchical clustering shows a\ngrouping of related disciplines.\")\n\nFigure 7.3: Heatmap of correlations between variables in the `athletes` data.\nHigher values are color coded red-orange. The hierarchical clustering shows a\ngrouping of related disciplines.\n\nFigure 7.3 shows how the 10 variables cluster into groups: running, throwing\nand jumping.\n\n### 7.2.2 Preprocessing the data\n\nIn many cases, different variables are measured in different units, so they\nhave different baselines and different scales6. These are not directly\ncomparable in their original form.\n\n6 Common measures of scale are the range and the standard deviation. For\ninstance, the times for the 110 metres vary between 14.18 and 16.2, with a\nstandard deviation of 0.51, whereas the times to complete the 1500 metres vary\nbetween 256.64 and 303.17, with a standard deviation of 13.66; more than an\norder of magnitude larger. Moreover, the `athletes` data also contain\nmeasurements in different units (seconds, metres), whose choice is arbitrary\n(lengths could also be recorded in centimetres or feet, times in\nmilliseconds).\n\nFor PCA and many other methods, we therefore need to transform the numeric\nvalues to some common scale in order to make comparisons meaningful.\n**Centering** means subtracting the mean, so that the mean of the centered\ndata is at the origin. **Scaling** or **standardizing** then means dividing by\nthe standard deviation, so that the new standard deviation is \\\\(1\\\\). In\nfact, we have already encountered these operations when computing the\ncorrelation coefficient (Equation 7.1): the correlation coefficient is simply\nthe vector product of the centered and scaled variables. To perform these\noperations, there is the R function `scale`, whose default behavior when given\na matrix or a data frame is to make every column have a mean of zero and a\nstandard deviation of \\\\(1\\\\).\n\n__\n\nQuestion 7.5\n\n  1. Compute the means and standard deviations of the `turtle` data, then use the `scale` function to center and standardize the continuous variables. Call this `scaledTurtles`, then verify the new values for mean and standard deviation of `scaledTurtles`.  \n\n  2. Make a scatterplot of the scaled and centered width and height variables of the turtle data and color the points by their sex.\n\n__\n\nSolution\n\n__\n\n    \n    \n    apply(turtles[,-1], 2, sd)__\n    \n    \n       length     width    height \n    20.481602 12.675838  8.392837 \n    \n    \n    apply(turtles[,-1], 2, mean)__\n    \n    \n       length     width    height \n    124.68750  95.43750  46.33333 \n    \n    \n    scaledTurtles = scale(turtles[, -1])\n    apply(scaledTurtles, 2, mean)__\n    \n    \n           length         width        height \n    -1.432050e-18  1.940383e-17 -2.870967e-16 \n    \n    \n    apply(scaledTurtles, 2, sd)__\n    \n    \n    length  width height \n         1      1      1 \n    \n    \n    data.frame(scaledTurtles, sex = turtles[, 1]) %>%\n      ggplot(aes(x = width, y = height, group = sex)) +\n        geom_point(aes(color = sex)) + coord_fixed()__\n\n[![](07-chap_files/figure-html/fig-turtlesDim12-1.png)](07-chap_files/figure-\nhtml/fig-turtlesDim12-1.png \"Figure 7.4: Turtles data projected onto the plane\ndefined by the width and height variables: each point colored according to\nsex.\")\n\nFigure 7.4: Turtles data projected onto the plane defined by the `width` and\n`height` variables: each point colored according to `sex`.\n\nWe have already encountered other data transformation choices in Chapters\n[4](04-chap.html) and [5](05-chap.html), where we used the `log` and `asinh`\nfunctions. The aim of these transformations is (usually) variance\nstabilization, i.e., to make the variances of replicate measurements of _one\nand the same variable_ in different parts of its dynamic range more similar.\nIn contrast, the standardizing transformation described above aims to make the\nscale (as measured by mean and standard deviation) of _different variables_\nthe same.\n\nSometimes it is preferable to leave variables at different scales because they\nare truly of different importance. If their original scale is relevant, then\nwe can (should) leave the data as is. In other cases, the variables have\ndifferent precisions known a priori. We will see in [Chapter 9](09-chap.html)\nthat there are several ways of weighting such variables.\n\nAfter preprocessing the data, we are ready to undertake data _simplification_\nthrough **dimension reduction**.\n\n![](imgs/book_icon.png)\n\nUseful books with relevant chapters are Flury ([1997](16-chap.html#ref-Flury))\nfor an introductory account and Mardia, Kent, and Bibby\n([1979](16-chap.html#ref-Mardia)) for a detailed mathematical approach.\n\n## 7.3 Dimension reduction\n\nWe will explain dimension reduction from several different perspectives. It\nwas invented in 1901 by Karl Pearson ([Pearson 1901](16-chap.html#ref-\nPearson1901)) as a way to reduce a two-variable scatterplot to a single\ncoordinate. It was used by statisticians in the 1930s to summarize a battery\nof psychological tests run on the same subjects ([Hotelling\n1933](16-chap.html#ref-Hotelling:1933ki)); thus providing overall scores that\nsummarize many tested variables at once.\n\n[![Principal and principle are two different words, which have different\nmeanings. So please do not confuse them. With PCA, it is always\nprincipal.](imgs/devil.png)](imgs/devil.png \"Principal and principle are two\ndifferent words, which have different meanings. So please do not confuse them.\nWith PCA, it is always principal.\")\n\n_Principal_ and _principle_ are two different words, which have different\nmeanings. So please do not confuse them. With PCA, it is always _principal_.\n\nThis idea of **principal** scores inspired the name Principal Component\nAnalysis (abbreviated PCA). PCA is called an **unsupervised learning**\ntechnique because, as in clustering, it treats all variables as having the\nsame **status**. We are not trying to predict or explain one particular\nvariable’s value from the others; rather, we are trying to find a mathematical\nmodel for an underlying structure for all the variables. PCA is primarily an\nexploratory technique that produces maps that show the relations between\nvariables and between observations in a useful way.\n\nWe first provide a flavor of what this multivariate analysis does to the data.\nThere is an elegant mathematical formulation of these methods through linear\nalgebra, although here we will try to minimize its use and focus on\nvisualization and data examples.\n\nWe use geometrical **projections** that take points in higher-dimensional\nspaces and projects them down onto lower dimensions. Figure 7.5 shows the\nprojection of the point \\\\(A\\\\) onto the line generated by the vector\n\\\\({\\mathbf v}\\\\).\n\n[![](07-chap_files/figure-html/fig-projectv-1.png)](07-chap_files/figure-\nhtml/fig-projectv-1.png \"Figure 7.5: Point A is projected onto the red line\ngenerated by the vector v. The dashed projection line is perpendicular \\(or\northogonal\\) to the red line. The intersection point of the projection line\nand the red line is called the orthogonal projection of A onto the red line\ngenerated by the vector v.\")\n\nFigure 7.5: Point \\\\(A\\\\) is projected onto the red line generated by the\nvector \\\\(v\\\\). The dashed projection line is perpendicular (or\n**orthogonal**) to the red line. The intersection point of the projection line\nand the red line is called the orthogonal projection of A onto the red line\ngenerated by the vector \\\\(v\\\\).\n\nPCA is a **linear** technique, meaning that we look for linear relations\nbetween variables and that we will use new variables that are linear functions\nof the original ones (\\\\(f(ax+by)=af(x)+b(y)\\\\)). The linearity constraints\nmakes computations particularly easy. We will see non-linear techniques in\n[Chapter 9](09-chap.html).\n\n### 7.3.1 Lower-dimensional projections\n\nHere we show one way of projecting two-dimensional data onto a line using the\n`athletes` data. The code below provides the preprocessing and plotting steps\nthat were used to generate Figure 7.6:\n\n    \n    \n    athletes = data.frame(scale(athletes))\n    ath_gg = ggplot(athletes, aes(x = weight, y = disc)) +\n      geom_point(size = 2, shape = 21)\n    ath_gg + geom_point(aes(y = 0), colour = \"red\") +\n      geom_segment(aes(xend = weight, yend = 0), linetype = \"dashed\")__\n\n[![](07-chap_files/figure-html/fig-SimpleScatter-1.png)](07-chap_files/figure-\nhtml/fig-SimpleScatter-1.png \"Figure 7.6: Scatterplot of two variables showing\nthe projection on the horizontal x axis \\(defined by y=0\\) in red and the\nlines of projection appear as dashed.\")\n\nFigure 7.6: Scatterplot of two variables showing the projection on the\nhorizontal x axis (defined by \\\\(y=0\\\\)) in red and the lines of projection\nappear as dashed.\n\n__\n\nTask\n\n  1. Calculate the variance of the red points in Figure 7.6.  \n\n  2. Make a plot showing projection lines onto the \\\\(y\\\\) axis and projected points.  \n\n  3. Compute the variance of the points projected onto the vertical \\\\(y\\\\) axis.\n\n### 7.3.2 How do we summarize two-dimensional data by a line?\n\nIn general, we lose information about the points when we project from two\ndimensions (a plane) to one (a line). If we do it just by using the original\ncoordinates, as we did on the `weight` variable in Figure 7.6, we lose all the\ninformation about the `disc` variable. Our goal is to keep as much information\nas we can about _both_ variables. There are actually many ways of projecting\nthe point cloud onto a line. One is to use what are known as **regression\nlines**. Let’s look at these lines and how they are constructed in R.\n\n#### Regressing one variable on the other\n\nIf you have seen linear regression, you already know how to compute lines that\nsummarize scatterplots; **linear regression** is a **supervised** method that\ngives preference minimizing the residual sum of squares in one direction: that\nof the response variable.\n\n#### Regression of the `disc` variable on `weight`.\n\nIn Figure 7.7, we use the `lm` (linear model) function to find the regression\nline. Its slope and intercept are given by the values in the `coefficients`\nslot of the resulting object `reg1`.\n\n    \n    \n    reg1 = lm(disc ~ weight, data = athletes)\n    a1 = reg1$coefficients[1] # intercept\n    b1 = reg1$coefficients[2] # slope\n    pline1 = ath_gg + geom_abline(intercept = a1, slope = b1,\n        col = \"blue\", linewidth = 1.5)\n    pline1 + geom_segment(aes(xend = weight, yend = reg1$fitted),\n        colour = \"red\", arrow = arrow(length = unit(0.15, \"cm\")))__\n\n[![](07-chap_files/figure-html/fig-Reg1-1.png)](07-chap_files/figure-html/fig-\nReg1-1.png \"Figure 7.7: The blue line minimizes the sum of squares of the\nvertical residuals \\(in red\\).\")\n\nFigure 7.7: The blue line minimizes the sum of squares of the vertical\nresiduals (in red).\n\n#### Regression of `weight` on `discus`.\n\nFigure 7.8 shows the line produced when reversing the roles of the two\nvariables; `weight` becomes the response variable.\n\n    \n    \n    reg2 = lm(weight ~ disc, data = athletes)\n    a2 = reg2$coefficients[1] # intercept\n    b2 = reg2$coefficients[2] # slope\n    pline2 = ath_gg + geom_abline(intercept = -a2/b2, slope = 1/b2,\n        col = \"darkgreen\", linewidth = 1.5)\n    pline2 + geom_segment(aes(xend=reg2$fitted, yend=disc),\n        colour = \"orange\", arrow = arrow(length = unit(0.15, \"cm\")))__\n\n[![](07-chap_files/figure-html/fig-Reg2-1.png)](07-chap_files/figure-html/fig-\nReg2-1.png \"Figure 7.8: The green line minimizes the sum of squares of the\nhorizontal residuals \\(in orange\\).\")\n\nFigure 7.8: The green line minimizes the sum of squares of the horizontal\nresiduals (in orange).\n\nEach of the regression lines in Figures 7.7 and 7.8 gives us an approximate\nlinear relationship between `disc` and `weight`. However, the relationship\ndiffers depending on which of the variables we choose to be the predictor and\nwhich the response.\n\n__\n\nQuestion 7.6\n\nHow large is the variance of the projected points that lie on the blue\nregression line of Figure 7.7? Compare this to the variance of the data when\nprojected on the original axes, `weight` and `disc`.\n\n__\n\nSolution\n\n__\n\nPythagoras’ theorem tells us that the squared length of the hypotenuse of a\nright-angled triangle is equal to the sum of the squared lengths of the other\ntwo sides, which we apply as follows:\n\n    \n    \n    var(athletes$weight) + var(reg1$fitted)__\n    \n    \n    [1] 1.650204\n\nThe variances of the points along the original axes `weight` and `disc` are 1,\nsince we scaled the variables.\n\n#### A line that minimizes distances in both directions\n\nFigure 7.9 shows the line chosen to minimize the sum of squares of the\northogonal (perpendicular) projections of data points onto it; we call this\nthe **principal component** line. All our three ways of fitting a line\n(Figures 7.7–7.9) together in one plot are shown in Figure 7.10.\n\n    \n    \n    xy = cbind(athletes$disc, athletes$weight)\n    svda = svd(xy)\n    pc = xy %*% svda$v[, 1] %*% t(svda$v[, 1])\n    bp = svda$v[2, 1] / svda$v[1, 1]\n    ap = mean(pc[, 2]) - bp * mean(pc[, 1])\n    ath_gg + geom_segment(xend = pc[, 1], yend = pc[, 2]) +\n      geom_abline(intercept = ap, slope = bp, col = \"purple\", linewidth = 1.5)__\n\n[![](07-chap_files/figure-html/fig-PCAmin-1.png)](07-chap_files/figure-\nhtml/fig-PCAmin-1.png \"Figure 7.9: The purple principal component line\nminimizes the sums of squares of the orthogonal projections.\")\n\nFigure 7.9: The purple **principal component** line minimizes the sums of\nsquares of the orthogonal projections.\n\n[![](07-chap_files/figure-html/fig-PCAR1R2-1-1.png)](07-chap_files/figure-\nhtml/fig-PCAR1R2-1-1.png \"Figure 7.10: The blue line minimizes the sum of\nsquares of the vertical residuals, the green line minimizes the horizontal\nresiduals, the purple line, called the principal component, minimizes the\northogonal projections. Notice the ordering of the slopes of the three\nlines.\")\n\nFigure 7.10: The blue line minimizes the sum of squares of the vertical\nresiduals, the green line minimizes the horizontal residuals, the purple line,\ncalled the **principal component** , minimizes the orthogonal projections.\nNotice the ordering of the slopes of the three lines.\n\n__\n\nQuestion 7.7\n\n  1. What is particular about the slope of the purple line?  \n\n  2. Redo the plots on the original (unscaled) variables. What happens?\n\n__\n\nSolution\n\n__\n\nThe lines computed here depend on the choice of units. Because we have made\nthe standard deviations equal to one for both variables, the PCA line is the\ndiagonal that cuts exactly in the middle of both regression lines. Since the\ndata were centered by subtracting their means, the line passes through the\norigin \\\\((0,0)\\\\).\n\n__\n\nQuestion 7.8\n\nCompute the variance of the points on the purple line.\n\n__\n\nSolution\n\n__\n\nWe have computed the coordinates of the points when we made the plot, these\nare in the `pc` vector:\n\n    \n    \n    apply(pc, 2, var)__\n    \n    \n    [1] 0.9031761 0.9031761\n    \n    \n    sum(apply(pc, 2, var))__\n    \n    \n    [1] 1.806352\n\nWe see that the variance along this axis is larger than the other variances we\ncalculated in Question 7.6.\n\nPythagoras’ theorem tells us two interesting things here:\n\n  * If we are minimizing in both horizontal and vertical directions we are in fact minimizing the orthogonal projections onto the line from each point.\n\n  * The total variability of the points is measured by the sum of squares of the projection of the points onto the center of gravity, which is the origin (0,0) if the data are centered. This is called the _total variance_ or the **inertia** of the point cloud. This inertia can be decomposed into the sum of the squares of the projections onto the line plus the variances along that line. For a fixed variance, minimizing the projection distances also maximizes the variance along that line. Often we define the first principal component as the line with maximum variance.\n\n## 7.4 The new linear combinations\n\n![](imgs/Vegetables_small.jpg)\n\nThe PC line we found in the previous section could be written\n\nImage credit: Sara Holmes\n\n\\\\[ PC = \\frac{1}{2} \\mbox{disc} + \\frac{1}{2} \\mbox{weight}. \\tag{7.2}\\\\]\n\nPrincipal components are _linear combinations_ of the variables that were\noriginally measured, they provide a _new coordinate system_. To understand\nwhat a **linear combination** really is, we can take an analogy. When making a\nhealthy juice mix, you will follow a recipe like\n\n\\\\[ \\begin{align} V &= 2 \\times \\text{Beet} + 1 \\times \\text{Carrot} \\\\\\ &\\+\n\\tfrac{1}{2} \\text{Gala} + \\tfrac{1}{2} \\text{GrannySmith} \\\\\\ &\\+ 0.02 \\times\n\\text{Ginger} + 0.25 \\times \\text{Lemon}. \\end{align} \\\\]\n\nThis recipe is a linear combination of individual juice types (the original\nvariables). The result is a new variable, \\\\(V\\\\), and the coefficients\n\\\\((2,1,\\frac{1}{2},\\frac{1}{2},0.02,0.25)\\\\) are called the **loadings**.\n\n__\n\nQuestion 7.9\n\nHow would you compute the calories in a glass of juice?\n\n### 7.4.1 Optimal lines\n\nA linear combination of variables defines a line in higher dimensions in the\nsame way we constructed lines in the scatterplot plane of two dimensions. As\nwe saw in that case, there are many ways to choose lines onto which we project\nthe data, there is however a `best’ line for our purpose.\n\nThe total variance of all the points in all the variables can de decomposed.\nIn PCA, we use the fact that the total sums of squares of the distances\nbetween the points and any line can be decomposed into the distance to the\nline and the variance along the line.\n\nWe saw that the principal component minimizes the distance to the line, and it\nalso maximizes the variance of the projections along the line.\n\nWhy is maximizing the variance along a line a good idea? Let’s look at another\nexample of a projection from three dimensions into two. In fact, human vision\ndepends on such dimension reduction:\n\n![Figure 7.11: A mystery silhouette.](imgs/CAM3.png)\n\nFigure 7.11: A mystery silhouette.\n\n__\n\nQuestion 7.10\n\nIn Figure 7.11, there is a two-dimensional projection of a three-dimensional\nobject. What is the object?\n\n__\n\nQuestion 7.11\n\nWhich of the two projections, Figure 7.11 or 7.13, do you find more\ninformative, and why?\n\n__\n\nSolution\n\n__\n\nOne can argue that the projection that maximizes the area of the shadow shows\nmore `information’.\n\n## 7.5 The PCA workflow\n\n[![](imgs/orgacp1.png)](imgs/orgacp1.png \"Figure 7.12: Many choices have to be\nmade during PCA processing.\")\n\nFigure 7.12: Many choices have to be made during PCA processing.\n\nPCA is based on the principle of finding the axis showing the largest\ninertia/variability, removing the variability in that direction and then\niterating to find the next best orthogonal axis, and so on. In fact, we do not\nhave to run iterations, all the axes can be found in one linear algebra\noperation called the **S** ingular **V** alue **D** ecomposition (we will\ndelve more deeply into the details below).\n\nIn the diagram in Figure 7.12, we see that first the means and variances are\ncomputed and the choice of whether to work directly with the covariance matrix\nor with the correlation matrix has to be made. The next step is the choice of\n\\\\(k\\\\), the number of components we deem relevant to the data. We say that\n\\\\(k\\\\) is the rank of the approximation. The best choice of \\\\(k\\\\) is a\ndifficult question, and we discuss on how to approach it below. The choice of\n\\\\(k\\\\) requires looking at a plot of the variances explained by the\nsuccessive principal components. Once we have chosen \\\\(k\\\\), we can proceed\nto the projections of the data in the new \\\\(k\\\\)-dimensional subspace.\n\nThe end results of the PCA workflow are useful maps of both the variables and\nthe samples. Understanding how these maps are constructed will maximize the\ninformation we can gather from them.\n\n## 7.6 The inner workings of PCA: rank reduction\n\nThis is a small section for those whose background in linear algebra is but a\nfaint memory. It tries to give some intuition to the singular value\ndecomposition method underlying PCA, without too much notation.\n\n[![](imgs/CAM4.png)](imgs/CAM4.png \"Figure 7.13: Another two-dimensional\nprojection of the same object shown in Figure fig-cam1. Here, the perspective\nis more informative. Generally, choosing the perspective such that the spread\n\\(in other words, the variance\\) of the points is maximal generally provides\nmost information. We want to see as much of the variation as possible, that’s\nwhat PCA does.\")\n\nFigure 7.13: Another two-dimensional projection of the same object shown in\nFigure 7.11. Here, the perspective is more informative. Generally, choosing\nthe perspective such that the spread (in other words, the variance) of the\npoints is maximal generally provides most information. We want to see as much\nof the variation as possible, that’s what PCA does.\n\nThe singular value decomposition of a matrix finds horizontal and vertical\nvectors (called the singular vectors) and normalizing values (called singular\nvalues). As before, we start by giving the forward-generative explanation\nbefore doing the actual reverse engineering that is used in creating the\ndecomposition. To calibrate the meaning of each step, we will start with an\nartificial example before moving to the complexity of real data.\n\n### 7.6.1 Rank-one matrices\n\nA simple generative model demonstrates the meaning of the **rank of a matrix**\nand explains how we find it in practice. Suppose we have two vectors, \\\\(u\\\\)\n(a one-column matrix), and \\\\(v^t=t(v)\\\\) (a one-row matrix–the transpose of a\none-column matrix \\\\(v\\\\)). For instance, \\\\(u =\\left( \\begin{smallmatrix}\n1\\\\\\2\\\\\\3\\\\\\4 \\end{smallmatrix} \\right)\\\\) and \\\\(v =\\left(\n\\begin{smallmatrix} 2\\\\\\4\\\\\\8 \\end{smallmatrix} \\right)\\\\). The transpose of\n\\\\(v\\\\) is written \\\\(v^t = t(v) = (2\\; 4\\; 8)\\\\). We multiply a copy of\n\\\\(u\\\\) by each of the elements of \\\\(v^t\\\\) in turn as follows:\n\nStep 0:  \n\nX | 2 | 4 | 8  \n---|---|---|---  \n1 |  |  |   \n2 |  |  |   \n3 |  |  |   \n4 |  |  |   \n  \nStep 1:  \n\nX | 2 | 4 | 8  \n---|---|---|---  \n1 | 2 |  |   \n2 | 4 |  |   \n3 | 6 |  |   \n4 | 8 |  |   \n  \nStep 2:  \n\nX | 2 | 4 | 8  \n---|---|---|---  \n1 | 2 | 4 |   \n2 | 4 | 8 |   \n3 | 6 | 12 |   \n4 | 8 | 16 |   \n  \nStep 3:  \n\nX | 2 | 4 | 8  \n---|---|---|---  \n1 | 2 | 4 | 8  \n2 | 4 | 8 | 16  \n3 | 6 | 12 | 24  \n4 | 8 | 16 | 32  \n  \nThus, the \\\\((2,3)\\\\) entry of the matrix \\\\(X\\\\), written \\\\(x_{2,3}\\\\), is\nobtained by multiplying \\\\(u_2\\\\) by \\\\(v_3\\\\). We can write this\n\n\\\\[ X=\\begin{pmatrix} 2&4&8\\\\\\ 4&8&16\\\\\\ 6 &12&24\\\\\\ 8&16&32\\\\\\ \\end{pmatrix}\n= u * t(v)= u * v^t \\tag{7.3}\\\\]\n\nThe matrix \\\\(X\\\\) we obtain here is said to be of rank 1, because both\n\\\\(u\\\\) and \\\\(v\\\\) have one column.\n\n__\n\nQuestion 7.12\n\nWhy can we say that writing \\\\(X = u*v^t\\\\) is more economical than spelling\nout the full matrix \\\\(X\\\\)?\n\n__\n\nSolution\n\n__\n\n\\\\(X\\\\) has \\\\(4\\times3=12\\\\) elements, while in terms of \\\\(u\\\\) and \\\\(v\\\\)\nit can be expressed by only \\\\(4+3=7\\\\) numbers. The compression is even more\nimpressive when \\\\(u\\\\) or \\\\(v\\\\) are longer.\n\nOn the other hand, suppose that we want to reverse the process and simplify\nanother matrix \\\\(X\\\\) given below with 3 rows and 4 columns (12 numbers). Can\nwe always express it in a similar way as a product of vectors without loss of\ninformation? In the diagrams shown in Figures 7.14 and 7.15, the colored boxes\nhave areas proportional to the numbers in the cells of the matrix (7.4).\n\n[![](imgs/SVD-mosaicXplot0.png)](imgs/SVD-mosaicXplot0.png \"Figure 7.14: Some\nspecial matrices have numbers in them that make them easy to decompose. Each\ncolored rectangle in this diagram has an area that corresponds to the number\nin it.\")\n\nFigure 7.14: Some special matrices have numbers in them that make them easy to\ndecompose. Each colored rectangle in this diagram has an area that corresponds\nto the number in it.\n\n__\n\nQuestion 7.13\n\nHere is a matrix \\\\(X\\\\) we want to decompose.\n\n\\\\[ \\begin{array}{rrrrr} \\hline {\\large X} & x_{.1} & x_{.2} & x_{.3} & x_{.4}\n\\\\\\ \\hline x_{1.} & 780 & 936 & 1300 & 728\\\\\\ x_{2.} & 75 & 90 & 125 & 70\\\\\\\nx_{3.} & 540 & 648 & 900 & 504\\\\\\ \\hline \\end{array} \\tag{7.4}\\\\]\n\n\\\\(X\\\\) has been redrawn as series of rectangles in Figure 7.14. What numbers\ncould we put in the white \\\\(u\\\\) and \\\\(v\\\\) boxes so that the values of the\nsides of the rectangle give the numbers as their product?\n\nA matrix with the special property of being perfectly “rectangular” like\n\\\\(X\\\\) is said to be of rank 1. We can represent the numbers in \\\\(X\\\\) by\nthe areas of rectangles, where the sides of rectangles are given by the values\nin the side vectors (\\\\(u\\\\) and \\\\(v\\\\)).\n\n![](imgs/SVD-mosaicXplot1.png):\n\")\n\n(a)\n\n![](imgs/SVD-mosaicXplot2.png):\n\")\n\n(b)\n\n![](imgs/SVD-mosaicXplot3.png):\n\")\n\n(c)\n\nFigure 7.15: The numbers in the cells are equal to the product of the\ncorresponding margins in (a), (b) and (c). We could make the cells from\nproducts in several ways. In (c), we force the margins to have norm \\\\(1\\\\).\n\nWe see in Figure 7.15 that the decomposition of \\\\(X\\\\) is not unique: there\nare several candidate choices for the vectors \\\\(u\\\\) and \\\\(v\\\\). We will\nmake the choice unique by requiring that the sum of the squares of each\nvector’s elements add to 1 (we say the vectors \\\\(v\\\\) and \\\\(u\\\\) have norm\n1). Then we have to keep track of one extra number by which to multiply each\nof the products, and which represents the “overall scale” of \\\\(X\\\\). This is\nthe value we have put in the upper left hand corner. It is called the singular\nvalue \\\\(s_1\\\\). In the R code below, we start by supposing we know the values\nin `u`, `v` and `s1`; later we will see a function that finds them for us.\nLet’s check the multiplication and norm properties in R:\n\n    \n    \n    X = matrix(c(780,  75, 540,\n                 936,  90, 648,\n                1300, 125, 900,\n                 728,  70, 504), nrow = 3)\n    u = c(0.8196, 0.0788, 0.5674)\n    v = c(0.4053, 0.4863, 0.6754, 0.3782)\n    s1 = 2348.2\n    sum(u^2)__\n    \n    \n    [1] 1\n    \n    \n    sum(v^2)__\n    \n    \n    [1] 1\n    \n    \n    s1 * u %*% t(v)__\n    \n    \n         [,1] [,2] [,3] [,4]\n    [1,]  780  936 1300  728\n    [2,]   75   90  125   70\n    [3,]  540  648  900  504\n    \n    \n    X - s1 * u %*% t(v)__\n    \n    \n             [,1]   [,2]   [,3]   [,4]\n    [1,] -0.03419 0.0745 0.1355 0.1221\n    [2,]  0.00403 0.0159 0.0252 0.0186\n    [3,] -0.00903 0.0691 0.1182 0.0982\n\n__\n\nQuestion 7.14\n\nTry `svd(X)` in R. Look at the components of the output of the `svd` function\ncarefully. Check the norm of the columns of the matrices that result from this\ncall. Where did the above value of `s1` = 2348.2 come from?\n\n__\n\nSolution\n\n__\n\n    \n    \n    svd(X)$u[, 1]\n    svd(X)$v[, 1]\n    sum(svd(X)$u[, 1]^2)\n    sum(svd(X)$v[, 1]^2)\n    svd(X)$d __\n\nIn fact, in this particular case we were lucky: we see that the second and\nthird singular values are 0 (up to the numeric precision we care about). That\nis why we say that \\\\(X\\\\) is of **rank** 1\\. For a more general matrix\n\\\\(X\\\\), it is rare to be able to write \\\\(X\\\\) exactly as this type of two-\nvector product. The next subsection shows how we can decompose \\\\(X\\\\) when it\nis not of rank 1: we will just need more pieces.\n\n### 7.6.2 How do we find such a decomposition in a unique way?\n\nIn the above decomposition, there were three elements: the horizontal and\nvertical singular vectors, and the diagonal corner, called the singular value.\nThese can be found using the singular value decomposition function (`svd`).\nFor instance:\n\n    \n    \n    Xtwo = matrix(c(12.5, 35.0, 25.0, 25, 9, 14, 26, 18, 16, 21, 49, 32,\n           18, 28, 52, 36, 18, 10.5, 64.5, 36), ncol = 4, byrow = TRUE)\n    USV = svd(Xtwo)__\n\n__\n\nQuestion 7.15\n\nLook at the `USV` object, the result of calling the `svd` function. What are\nits components?\n\n__\n\nSolution\n\n__\n\n    \n    \n    names(USV)__\n    \n    \n    [1] \"d\" \"u\" \"v\"\n    \n    \n    USV$d __\n    \n    \n    [1] 1.350624e+02 2.805191e+01 3.111680e-15 2.290270e-15\n\nSo 135.1 is the first singular value `USV$d[1]`.\n\n__\n\nQuestion 7.16\n\nCheck how each successive pair of singular vectors improves our approximation\nto `Xtwo`. What do you notice about the third and fourth singular values?\n\n__\n\nSolution\n\n__\n\n    \n    \n    Xtwo - USV$d[1] * USV$u[, 1] %*% t(USV$v[, 1])\n    Xtwo - USV$d[1] * USV$u[, 1] %*% t(USV$v[, 1]) -\n           USV$d[2] * USV$u[, 2] %*% t(USV$v[, 2])__\n\nThe third and fourth singular values are so small that they do not improve the\napproximation (within rounding errors), so we can conclude that `Xtwo` is of\nrank 2.\n\nAgain, there are many ways to write a rank two matrix such as `Xtwo` as a sum\nof rank one matrices: in order to ensure uniqueness, we impose yet another7\ncondition on the singular vectors. The output vectors of the singular\ndecomposition do not only have their norms equal to 1, each column vector in\nthe \\\\(U\\\\) matrix is orthogonal to all the previous ones. We write\n\\\\(u_{\\cdot 1} \\perp u_{\\cdot 2}\\\\), this means that the sum of the products\nof the values in the same positions is \\\\(0\\\\): \\\\(\\sum_i u_{i1} u_{i2} =\n0\\\\). Ditto for the \\\\(V\\\\) matrix.\n\n7 Above, we had chosen the norm of the vectors to be 1.\n\n__\n\nTask\n\nCheck the orthonormality by computing the cross product of the \\\\(U\\\\) and\n\\\\(V\\\\) matrices:\n\n    \n    \n    t(USV$u) %*% USV$u\n    t(USV$v) %*% USV$v __\n\nLet’s submit our `scaledTurtles` matrix to a singular value decomposition.\n\n    \n    \n    turtles.svd = svd(scaledTurtles)\n    turtles.svd$d __\n    \n    \n    [1] 11.746475  1.419035  1.003329\n    \n    \n    turtles.svd$v __\n    \n    \n              [,1]       [,2]        [,3]\n    [1,] 0.5787981  0.3250273  0.74789704\n    [2,] 0.5779840  0.4834699 -0.65741263\n    [3,] 0.5752628 -0.8127817 -0.09197088\n    \n    \n    dim(turtles.svd$u)__\n    \n    \n    [1] 48  3\n\n__\n\nQuestion 7.17\n\nWhat can you conclude about the turtles matrix from the `svd` output?\n\n__\n\nSolution\n\n__\n\nThe first column of `turtles.svd$v` shows that the coefficients for the three\nvariables are practically equal. Other noticeable “coincidences” include:\n\n    \n    \n    sum(turtles.svd$v[,1]^2)__\n    \n    \n    [1] 1\n    \n    \n    sum(turtles.svd$d^2) / 47 __\n    \n    \n    [1] 3\n\nWe see that the coefficients are in fact \\\\(\\sqrt{1/3}\\\\) and the sum of\nsquares of the singular values is equal to \\\\((n-1)\\times p\\\\).\n\n### 7.6.3 Singular value decomposition\n\n![](imgs/SumRankOneD.png)\n\n\\\\(X\\\\) is decomposed additively into rank-one pieces. Each of the \\\\(u\\\\)\nvectors is combined into the \\\\(U\\\\) matrix, and each of \\\\(v\\\\) vectors into\n\\\\(V\\\\). The _Singular Value Decomposition_ is\n\n\\\\[ {\\mathbf X} = U S V^t, V^t V={\\mathbb I}, U^t U={\\mathbb I}, \\tag{7.5}\\\\]\n\nwhere \\\\(S\\\\) is the diagonal matrix of singular values, \\\\(V^t\\\\) is the\ntranspose of \\\\(V\\\\), and \\\\({\\mathbb I}\\\\) is the Identity matrix. Expression\n7.5 can be written elementwise as\n\n\\\\[ X_{ij} = u_{i1}s_1v_{1j} + u_{i2}s_2v_{2j} + u_{i3}s_3v_{3j} +... +\nu_{ir}s_rv_{rj}, \\\\]\n\n\\\\(U\\\\) and \\\\(V\\\\) are said to be orthonormal8, because their self-\ncrossproducts are the identity matrix.\n\n8 Nothing to do with the normal distribution, it stands for orthogonal and\nhaving norm 1.\n\n### 7.6.4 Principal components\n\nThe singular vectors from the singular value decomposition (provided by the\n`svd` function in R) contain the coefficients to put in front of the original\nvariables to make the more informative ones we call the principal components.\nWe write this as:\n\n\\\\[ Z_1=v_{11} X_{\\cdot 1} +v_{21} X_{\\cdot 2} + v_{31} X_{\\cdot 3}+ \\cdots +\nv_{p1} X_{\\cdot p}. \\\\]\n\nIf `usv = svd(X)`, then \\\\((v_{11},v_{21},v_{31},...)\\\\) are given by the\nfirst column of `usv$v`; similarly for \\\\(Z_2\\\\) with the second column of\n`usv$v`, and so son. \\\\(p\\\\) is the number of columns of \\\\(X\\\\) and the\nnumber of rows of \\\\(V\\\\). These new variables \\\\(Z_1, Z_2, Z_3, ...\\\\) have\nvariances that decrease in size: \\\\(s_1^2 \\geq s_2^2 \\geq s_3^2 \\geq ...\\\\).\n\n__\n\nQuestion 7.18\n\nCompute the first principal component for the turtles data by multiplying by\nthe first singular value `d[1]` by `u[,1]`. What is another way of computing\nit ?\n\n__\n\nSolution\n\n__\n\nWe show this using the code:\n\n    \n    \n    US = turtles.svd$u[,1, drop = FALSE] %*% turtles.svd$d[1]\n    XV = scaledTurtles %*% turtles.svd$v[, 1, drop = FALSE]\n    max(abs(US-XV))__\n\nWe can also see using matrix algebra that \\\\(XV\\\\) and \\\\(US\\\\) are the same.\nRemember that \\\\(V\\\\) is orthogonal, so \\\\(V^t V={\\mathbb I}\\\\) and \\\\(XV =\nUSV^tV=US\\,{\\mathbb I}\\\\).\n\n_Note:_ The `drop = FALSE` argument in the first line of the below code makes\nsure that the selected matrix column retains _matrix_ / _array_ class\nattributes and thus is eligible for the matrix multiplication operator.\nAlternatively, you could use the regular multiplication operator `*`. In the\nsecond line, the `drop = FALSE` is not strictly necessary, but we have it\nthere for symmetry.\n\nHere are two useful facts, first in words, then with the mathematical\nshorthand.\n\nThe number of principal components \\\\(k\\\\) is always chosen to be fewer than\nthe number of original variables or the number of observations. We are\n“lowering” the dimension of the problem:\n\n\\\\[ k\\leq \\min(n,p). \\\\]\n\nThe principal component transformation is defined so that the first principal\ncomponent has the largest possible variance (that is, accounts for as much of\nthe variability in the data as possible), and each successive component in\nturn has the highest variance possible under the constraint that it be\northogonal to the preceding components:\n\n\\\\[ \\max_{aX \\perp bX}\\mbox{var}(\\mbox{Proj}_{aX} (X)), \\qquad \\mbox{where }\nbX=\\mbox{previous components} \\\\]\n\n## 7.7 Plotting the observations in the principal plane\n\nWe revisit our two-variable athletes data with the `discus` and the `weight`\nvariables. In Section 7.3.2, we computed the first principal component and\nrepresented it as the purple line in Figure 7.10. We showed that \\\\(Z_1\\\\) was\nthe linear combination given by the diagonal. As the coefficients have to have\ntheir sum of squares add to \\\\(1\\\\), we have that\n\\\\[Z_1=-0.707*\\text{athletes\\$disc}- 0.707*\\text{athletes\\$weight}.\\\\]\n\nThis is the same as if the two coordinates were \\\\(c_1=0.7071\\\\) and\n\\\\(c_2=0.7071\\\\).\n\n__\n\nQuestion 7.19\n\nWhat part of the output of the `svd` functions leads us to the first PC\ncoefficients, also known as the PC **loadings** ?\n\nNote that we use `svda` which was the svd applied to the two variables\n`discus` and `weight`.\n\n__\n\nSolution\n\n__\n\n    \n    \n    svda$v[,1]__\n    \n    \n    [1] -0.7071068 -0.7071068\n\nIf we rotate the `(discus, weight)` plane by making the purple line the\nhorizontal \\\\(x\\\\) axis, we obtain what is know as the first **principal\nplane**.\n\n    \n    \n    ppdf = tibble(PC1n = -svda$u[, 1] * svda$d[1],\n                  PC2n =  svda$u[, 2] * svda$d[2])\n    gg = ggplot(ppdf, aes(x = PC1n, y = PC2n)) + \n        geom_point() + \n        geom_hline(yintercept = 0, color = \"purple\", linewidth = 1.5, alpha = 0.5) +\n        xlab(\"PC1 \")+ ylab(\"PC2\") + xlim(-3.5, 2.7) + ylim(-2, 2) + coord_fixed()\n    gg + geom_point(aes(x = PC1n, y = 0), color = \"red\") +\n         geom_segment(aes(xend = PC1n, yend = 0), color = \"red\") \n    gg + geom_point(aes(x = 0, y = PC2n), color = \"blue\") +\n         geom_segment(aes(yend = PC2n, xend = 0), color = \"blue\") +\n         geom_vline(xintercept = 0, color = \"skyblue\", linewidth = 1.5, alpha = 0.5) __\n\n[![](07-chap_files/figure-html/fig-pcablue-1.png)](07-chap_files/figure-\nhtml/fig-pcablue-1.png \"Figure 7.16 \\(a\\): \")\n\n(a)\n\n[![](07-chap_files/figure-html/fig-pcablue-2.png)](07-chap_files/figure-\nhtml/fig-pcablue-2.png \"Figure 7.16 \\(b\\): \")\n\n(b)\n\nFigure 7.16: In the case where we only have two original variables, the PCA\ntransformation is a simple rotation; the new coordinates are always chosen to\nbe the horizontal and vertical axes.\n\n__\n\nQuestion 7.20\n\n  1. What is the mean of the sums of squares of the red segments in Figure 7.16 equal to?  \n\n  2. How does this compare to the variance of the red points?  \n\n  3. Compute the ratio of the standard deviation of the blue segments to the red segments in Figure 7.16. Compare this to the ratio of singular values 1 and 2.\n\n__\n\nSolution\n\n__\n\n  1. The sum of squares of the red segments corresponds to the square of the second singular value:\n\n    \n    \n    sum(ppdf$PC2n^2) __\n    \n    \n    [1] 6.196729\n    \n    \n    svda$d[2]^2 __\n    \n    \n    [1] 6.196729\n\nSince the mean of the red segments is zero, the above quantities are also\nproportional to the variance:\n\n    \n    \n    mean(ppdf$PC2n) __\n    \n    \n    [1] 5.451106e-16\n    \n    \n    var(ppdf$PC2n) * (nrow(ppdf)-1)__\n    \n    \n    [1] 6.196729\n\n  2. The variance of the red points is `var(ppdf$PC1n)`, which is larger than what we calculated in a) by design of the first PC.\n\n    \n    \n    var(ppdf$PC1n) __\n    \n    \n    [1] 1.806352\n    \n    \n    var(ppdf$PC2n) __\n    \n    \n    [1] 0.1936478\n\n  3. We take the ratios of the standard deviations explained by the points on the vertical and horizontal axes by computing:\n\n    \n    \n    sd(ppdf$PC1n) / sd(ppdf$PC2n)__\n    \n    \n    [1] 3.054182\n    \n    \n    svda$d[1] / svda$d[2]__\n    \n    \n    [1] 3.054182\n\n__\n\nTask\n\nUse `prcomp` to compute the PCA of the first two columns of the athletes data,\nlook at the output. Compare to the singular value decomposition.\n\n### 7.7.1 PCA of the turtles data\n\nWe now want to do a complete PCA analysis on the turtles data. Remember, we\nalready looked at the summary statistics for the one- and two-dimensional\ndata. Now we are going to answer the question about the “true” dimensionality\nof these rescaled data.\n\nIn the following code, we use the function `princomp`. Its return value is a\nlist of all the important pieces of information needed to plot and interpret a\nPCA.\n\n[![In fact, PCA is such a fundamental technique that there are many different\nimplementations of it in various R packages. Unfortunately, the input\narguments and the formatting and naming of their output is not standardized,\nand some even use different conventions for the scaling of their output. We\nwill experiment with several different ones to familiarize ourselves with\nthese choices.](imgs/devil.png)](imgs/devil.png \"In fact, PCA is such a\nfundamental technique that there are many different implementations of it in\nvarious R packages. Unfortunately, the input arguments and the formatting and\nnaming of their output is not standardized, and some even use different\nconventions for the scaling of their output. We will experiment with several\ndifferent ones to familiarize ourselves with these choices.\")\n\nIn fact, PCA is such a fundamental technique that there are many different\nimplementations of it in various R packages. Unfortunately, the input\narguments and the formatting and naming of their output is not standardized,\nand some even use different conventions for the scaling of their output. We\nwill experiment with several different ones to familiarize ourselves with\nthese choices.\n\n    \n    \n    cor(scaledTurtles)__\n    \n    \n              length     width    height\n    length 1.0000000 0.9783116 0.9646946\n    width  0.9783116 1.0000000 0.9605705\n    height 0.9646946 0.9605705 1.0000000\n    \n    \n    pcaturtles = princomp(scaledTurtles)\n    pcaturtles __\n    \n    \n    Call:\n    princomp(x = scaledTurtles)\n    \n    Standard deviations:\n       Comp.1    Comp.2    Comp.3 \n    1.6954576 0.2048201 0.1448180 \n    \n     3  variables and  48 observations.\n    \n    \n    library(\"factoextra\")\n    fviz_eig(pcaturtles, geom = \"bar\", bar_width = 0.4) + ggtitle(\"\")__\n\n[![](07-chap_files/figure-html/fig-PCAturtles-1.png)](07-chap_files/figure-\nhtml/fig-PCAturtles-1.png \"Figure 7.17: The screeplot shows the eigenvalues\nfor the standardized turtles data \\(scaledTurtles\\): there is one large value\nand two small ones. The data are \\(almost\\) one-dimensional. We will see why\nthis dimension is called an axis of size, a frequent phenomenon in biometric\ndata \\[@Jolicoeur1960\\].\")\n\nFigure 7.17: The screeplot shows the eigenvalues for the standardized turtles\ndata (`scaledTurtles`): there is one large value and two small ones. The data\nare (almost) one-dimensional. We will see why this dimension is called an axis\nof size, a frequent phenomenon in biometric data ([Jolicoeur and Mosimann\n1960](16-chap.html#ref-Jolicoeur1960)).\n\n__\n\nQuestion 7.21\n\nMany PCA functions have been created by different teams who worked in\ndifferent areas at different times. This can lead to confusion, especially\nbecause they have different naming conventions. Let’s compare three of them;\nrun the following lines of code and look at the resulting objects:\n\n    \n    \n    svd(scaledTurtles)$v[, 1]\n    prcomp(turtles[, -1])$rotation[, 1]\n    princomp(scaledTurtles)$loadings[, 1]\n    library(\"ade4\")\n    dudi.pca(turtles[, -1], nf = 2, scannf = FALSE)$c1[, 1]__\n\nWhat happens when you disable the scaling in the `prcomp` and `princomp`\nfunctions?\n\nIn what follows, we always suppose that the matrix \\\\(X\\\\) represents the\ncentered and scaled matrix.\n\n__\n\nQuestion 7.22\n\nThe coordinates of the observations in the new variables from the `prcomp`\nfunction (call it `res`) are in the `scores` slot of the result. Take a look\nat PC1 for the `turtles` and compare it to `res$scores`. Compare the standard\ndeviation `sd1` to that in the `res` object and to the standard deviation of\nthe scores.\n\n__\n\nSolution\n\n__\n\n    \n    \n    res = princomp(scaledTurtles)\n    PC1 = scaledTurtles %*% res$loadings[,1]\n    sd1 = sqrt(mean(res$scores[, 1]^2))__\n\n__\n\nQuestion 7.23\n\nCheck the orthogonality of the `res$scores` matrix. Why can’t we say that it\nis **orthonormal**?\n\nNow we are going to combine both the PC scores (\\\\(US\\\\)) and the loadings-\ncoefficients (\\\\(V\\\\)). The plots with both the samples and the variables\nrepresented are called **biplots**. This can be done in one line using the\nfollowing\n**[factoextra](https://cran.r-project.org/web/packages/factoextra/)** package\nfunction.\n\n    \n    \n    fviz_pca_biplot(pcaturtles, label = \"var\", habillage = turtles[, 1]) +\n      ggtitle(\"\")__\n\n[![](07-chap_files/figure-html/fig-turtlebiplot-1.png)](07-chap_files/figure-\nhtml/fig-turtlebiplot-1.png \"Figure 7.18: A biplot of the first two dimensions\nshowing both variables and observations. The arrows show the variables. The\nturtles are labeled by sex. The extended horizontal direction is due to the\nsize of the first eigenvalue, which is much larger than the second.\")\n\nFigure 7.18: A biplot of the first two dimensions showing both variables and\nobservations. The arrows show the variables. The turtles are labeled by sex.\nThe extended horizontal direction is due to the size of the first eigenvalue,\nwhich is much larger than the second.\n\n[![Beware the aspect ratio when plotting a PCA. It is rare to have the two\ncomponents be of similar norm, so square shaped plots will be the exception.\nMore common are elongated plots, which show that the horizontal \\(first\\)\nprincipal component is more important than the second. This matters, e.g., for\ninterpreting distances between points in the\nplots.](imgs/devil.png)](imgs/devil.png \"Beware the aspect ratio when plotting\na PCA. It is rare to have the two components be of similar norm, so square\nshaped plots will be the exception. More common are elongated plots, which\nshow that the horizontal \\(first\\) principal component is more important than\nthe second. This matters, e.g., for interpreting distances between points in\nthe plots.\")\n\nBeware the aspect ratio when plotting a PCA. It is rare to have the two\ncomponents be of similar norm, so square shaped plots will be the exception.\nMore common are elongated plots, which show that the horizontal (first)\nprincipal component is more important than the second. This matters, e.g., for\ninterpreting distances between points in the plots.\n\n__\n\nQuestion 7.24\n\nIs it possible to have a PCA plot with the PC1 as the horizontal axis whose\nheight is longer than its width?\n\n__\n\nSolution\n\n__\n\nThe variance of points in the PC1 direction is \\\\(\\lambda_1=s_1^2\\\\) which is\nalways larger than \\\\(\\lambda_2=s_2^2\\\\), so the PCA plot will always be wider\nthan high.\n\n__\n\nQuestion 7.25\n\nLooking at Figure 7.18: a) Did the males or female turtles tend to be larger?  \nb) What do the arrows tell us about the correlations?\n\n__\n\nQuestion 7.26\n\nCompare the variance of each new coordinate to the eigenvalues returned by the\nPCA `dudi.pca` function.\n\n__\n\nSolution\n\n__\n\n    \n    \n    pcadudit = dudi.pca(scaledTurtles, nf = 2, scannf = FALSE)\n    apply(pcadudit$li, 2, function(x) sum(x^2)/48)__\n    \n    \n         Axis1      Axis2 \n    2.93573765 0.04284387 \n    \n    \n    pcadudit$eig __\n    \n    \n    [1] 2.93573765 0.04284387 0.02141848\n\nNow we look at the relationships between the variables, both old and new by\ndrawing what is known as the correlation circle. The aspect ratio is 1 here\nand the variables are represented by arrows as shown in Figure 7.19. The\nlengths of the arrows indicate the quality of the projections onto the first\nprincipal plane:\n\n    \n    \n    fviz_pca_var(pcaturtles, col.circle = \"black\") + ggtitle(\"\") +\n      xlim(c(-1.2, 1.2)) + ylim(c(-1.2, 1.2))__\n\n[![](07-chap_files/figure-html/fig-\nturtlesCirclef-1.png)](07-chap_files/figure-html/fig-turtlesCirclef-1.png\n\"Figure 7.19: Part of the “circle of correlations” showing the original\nvariables. Their correlations with each other and with the new principal\ncomponents are given by the angles between the vectors and between the axes\nand the vectors.\")\n\nFigure 7.19: Part of the “circle of correlations” showing the original\nvariables. Their correlations with each other and with the new principal\ncomponents are given by the angles between the vectors and between the axes\nand the vectors.\n\n__\n\nQuestion 7.27\n\nExplain the relationships between the number of rows of our turtles data\nmatrix and the following numbers:\n\n    \n    \n    svd(scaledTurtles)$d/pcaturtles$sdev\n    sqrt(47)__\n\n__\n\nSolution\n\n__\n\nWhen computing the variance covariance matrix, many implementations use\n\\\\(1/(n-1)\\\\) as the denominator. Here, \\\\(n=48\\\\) so the sum of the variances\nare off by a factor of 48/47.\n\nThese data are a good example of how sometimes almost all the variation in the\ndata can be captured in a lower-dimensional space: here, three-dimensional\ndata can be essentially replaced by a line. Keep in mind:\n\\\\(X^tC=VSU^tUS=VS^2.\\\\) The **principal components** are the columns of the\nmatrix \\\\(C=US\\\\). The \\\\(p\\\\) columns of \\\\(U\\\\) (the matrix given as `USV$u`\nin the output from the `svd` function above) are rescaled to have norms\n\\\\((s_1^2,s_2^2,...,s_p^2)\\\\). Each column has a different variance it is\n_responsible_ for explaining. Notice that these will be decreasing numbers.\n\nIf we only want the first one then it is just \\\\(c_1=s_1 u_1\\\\). Notice that\n\\\\(||c_1||^2=s_1^tu_1 u_1^t s_1= s_1^2 u_1^tu_1=s_1^2=\\lambda_1\\\\)\n\nIf the matrix \\\\(X\\\\) comes from the study of \\\\(n\\\\) different samples or\nspecimens, then the principal components provides new coordinates for these\n\\\\(n\\\\) points as in Figure 7.16. These are sometimes called the _scores_ in\nthe results of PCA functions.\n\n[![](imgs/xkcdEigenVectors.jpg)](imgs/xkcdEigenVectors.jpg \"Figure 7.20:\nAnother great xkcd take: this time eigenvectors.\")\n\nFigure 7.20: Another great xkcd take: this time eigenvectors.\n\nBefore we go into more detailed examples, let’s summarize what SVD and PCA\nprovide:\n\n  * Each principal component has a variance measured by the corresponding eigenvalue, the square of the corresponding singular value.\n\n  * The new variables are made to be orthogonal. Since they are also centered, this means they are uncorrelated. In the case of normal distributed data, this also means they are independent. \n\n  * When the variables are have been rescaled, the sum of the variances of all the variables is the number of variables (\\\\(=p\\\\)). The sum of the variances is computed by adding the diagonal of the crossproduct matrix9.\n\n  * The principal components are ordered by the size of their eigenvalues. We always check the screeplot before deciding how many components to retain. It is also best practice to do as we did in Figure 7.18 and annotate each PC axis with the proportion of variance it explains.\n\n**Eigen Decomposition:** The crossproduct of X with itself verifies\n\\\\[X^tX=VSU^tUSV^t=VS^2V^t=V\\Lambda V^t\\\\] where \\\\(V\\\\) is called the\neigenvector matrix of the symmetric matrix \\\\(X^tX\\\\) and \\\\(\\Lambda\\\\) is the\ndiagonal matrix of eigenvalues of \\\\(X^tX\\\\).\n\n9 This sum of the diagonal elements is called **the trace** of the matrix.\n\n__\n\nTask\n\nLook up eigenvalue in the Wikipedia. Try to find a sentence that defines it\nwithout using a formula. Why would eigenvectors come into use in Cinderella\n(at a stretch)? (See the xkcd cartoon in Figure 7.20.)\n\n![](imgs/book_icon.png)\n\nFor help with the basics of linear algebra, a motivated student pressed for\ntime may consult [Khan’s Academy](https://www.khanacademy.org/math/linear-\nalgebra/alternate-bases/eigen-everything/v/linear-algebra-introduction-to-\neigenvalues-and-eigenvectors). If you have more time and would like in depth\ncoverage, [Gil Strang’s MIT\ncourse](http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-\nspring-2010) is a classic, and [some of the book is available\nonline](http://math.mit.edu/linearalgebra) ([Strang 2009](16-chap.html#ref-\nStrang:09)).\n\n### 7.7.2 A complete analysis: the decathlon athletes\n\nWe started looking at these data earlier in this chapter. Here, we will follow\nstep by step a complete multivariate analysis. First, let us have another look\nat the correlation matrix (rounded to 2 digits after the decimal point), which\ncaptures the bivariate associations. We already plotted it as a colored\nheatmap in Figure 7.3.\n\n    \n    \n    cor(athletes) |> round(2)__\n    \n    \n            m100  long weight  high  m400  m110  disc  pole javel m1500\n    m100    1.00 -0.54  -0.21 -0.15  0.61  0.64 -0.05 -0.39 -0.06  0.26\n    long   -0.54  1.00   0.14  0.27 -0.52 -0.48  0.04  0.35  0.18 -0.40\n    weight -0.21  0.14   1.00  0.12  0.09 -0.30  0.81  0.48  0.60  0.27\n    high   -0.15  0.27   0.12  1.00 -0.09 -0.31  0.15  0.21  0.12 -0.11\n    m400    0.61 -0.52   0.09 -0.09  1.00  0.55  0.14 -0.32  0.12  0.59\n    m110    0.64 -0.48  -0.30 -0.31  0.55  1.00 -0.11 -0.52 -0.06  0.14\n    disc   -0.05  0.04   0.81  0.15  0.14 -0.11  1.00  0.34  0.44  0.40\n    pole   -0.39  0.35   0.48  0.21 -0.32 -0.52  0.34  1.00  0.27 -0.03\n    javel  -0.06  0.18   0.60  0.12  0.12 -0.06  0.44  0.27  1.00  0.10\n    m1500   0.26 -0.40   0.27 -0.11  0.59  0.14  0.40 -0.03  0.10  1.00\n\nThen we look at the screeplot, which will help us choose a rank \\\\(k\\\\) for\nrepresenting the essence of these data.\n\n    \n    \n    pca.ath = dudi.pca(athletes, scannf = FALSE)\n    pca.ath$eig __\n    \n    \n     [1] 3.4182381 2.6063931 0.9432964 0.8780212 0.5566267 0.4912275 0.4305952\n     [8] 0.3067981 0.2669494 0.1018542\n    \n    \n    fviz_eig(pca.ath, geom = \"bar\", bar_width = 0.3) + ggtitle(\"\")__\n\n[![](07-chap_files/figure-html/fig-screeplota-1.png)](07-chap_files/figure-\nhtml/fig-screeplota-1.png \"Figure 7.21: The screeplot of the athletes data\nindicates that most of the variation in the data can be captured in a two-\ndimensional plane \\(spanned by the first two principal components\\).\")\n\nFigure 7.21: The screeplot of the athletes data indicates that most of the\nvariation in the data can be captured in a two-dimensional plane (spanned by\nthe first two principal components).\n\nThe screeplot in Figure 7.21 shows a clear drop in the eigenvalues after the\nsecond one. This indicates that a good approximation will be obtained at rank\n2. Let’s look at an interpretation of the first two axes by projecting the\nloadings of the original variables onto the two new ones, the principal\ncomponents.\n\n    \n    \n    fviz_pca_var(pca.ath, col.var = \"blue\", repel = TRUE) + ggtitle(\"\")__\n\n[![](07-chap_files/figure-html/fig-athletecorr-1.png)](07-chap_files/figure-\nhtml/fig-athletecorr-1.png \"Figure 7.22: Correlation circle of the original\nvariables.\")\n\nFigure 7.22: Correlation circle of the original variables.\n\nThe correlation circle Figure 7.22 displays the projection of the original\nvariables onto the two first new principal axes. The angles between vectors\nare interpreted as correlations. On the right side of the plane, we have the\ntrack and field events (m110, m100, m400, m1500), and on the left, we have the\nthrowing and jumping events. Maybe there is an opposition of skills as\ncharacterized in the correlation matrix. We did see the correlations were\nnegative between variables from these two groups. How can we interpret this?\n\nIt seems that those who throw the best have lower scores in the track\ncompetitions. In fact, if we look at the original measurements, we can see\nwhat is happening. The athletes who run in short times are the stronger ones,\nas are the ones who throw or jump longer distances. We should probably change\nthe scores of the track variables and redo the analysis.\n\n__\n\nQuestion 7.28\n\nWhat transformations of the variables induce the best athletic performances to\nvary in the same direction, i.e., be mostly positively correlated?\n\n__\n\nSolution\n\n__\n\nIf we change the signs on the running performances, almost all the variables\nwill be positively correlated.\n\n    \n    \n    runningvars = grep(\"^m\", colnames(athletes), value = TRUE)\n    runningvars __\n    \n    \n    [1] \"m100\"  \"m400\"  \"m110\"  \"m1500\"\n    \n    \n    athletes[, runningvars] = -athletes[, runningvars]\n    cor(athletes) |> round(2)__\n    \n    \n           m100 long weight high  m400 m110  disc pole javel m1500\n    m100   1.00 0.54   0.21 0.15  0.61 0.64  0.05 0.39  0.06  0.26\n    long   0.54 1.00   0.14 0.27  0.52 0.48  0.04 0.35  0.18  0.40\n    weight 0.21 0.14   1.00 0.12 -0.09 0.30  0.81 0.48  0.60 -0.27\n    high   0.15 0.27   0.12 1.00  0.09 0.31  0.15 0.21  0.12  0.11\n    m400   0.61 0.52  -0.09 0.09  1.00 0.55 -0.14 0.32 -0.12  0.59\n    m110   0.64 0.48   0.30 0.31  0.55 1.00  0.11 0.52  0.06  0.14\n    disc   0.05 0.04   0.81 0.15 -0.14 0.11  1.00 0.34  0.44 -0.40\n    pole   0.39 0.35   0.48 0.21  0.32 0.52  0.34 1.00  0.27  0.03\n    javel  0.06 0.18   0.60 0.12 -0.12 0.06  0.44 0.27  1.00 -0.10\n    m1500  0.26 0.40  -0.27 0.11  0.59 0.14 -0.40 0.03 -0.10  1.00\n    \n    \n    pcan.ath = dudi.pca(athletes, nf = 2, scannf = FALSE)\n    pcan.ath$eig __\n    \n    \n     [1] 3.4182381 2.6063931 0.9432964 0.8780212 0.5566267 0.4912275 0.4305952\n     [8] 0.3067981 0.2669494 0.1018542\n\nNow all the negative correlations are quite small. The screeplot will show no\nchange, as the eigenvalues of the matrix are unaffected by the above sign\nflips. The only ouput that changes are the signs of the coefficients of the\nprincipal component loadings for the variables whose signs we flipped.\n\n    \n    \n    fviz_pca_var(pcan.ath, col.var = \"blue\", repel = TRUE) + ggtitle(\"\")__\n\n[![](07-chap_files/figure-html/fig-athletecorrn-1.png)](07-chap_files/figure-\nhtml/fig-athletecorrn-1.png \"Figure 7.23: Correlation circle after changing\nthe signs of the running variables.\")\n\nFigure 7.23: Correlation circle after changing the signs of the running\nvariables.\n\nFigure 7.23 shows the correlation circle of the transformed variables. We now\nsee we have a broad common overall axis: all the arrows are pointing broadly\nin the same direction.\n\nWe now plot the athletes projected in the principal plane using:\n\n    \n    \n    fviz_pca_ind(pcan.ath, repel = TRUE) + ggtitle(\"\") __\n\n[![](07-chap_files/figure-html/fig-athletepc-1.png)](07-chap_files/figure-\nhtml/fig-athletepc-1.png \"Figure 7.24: First principal plane showing the\nprojections of the athletes. Do you notice something about the organization of\nthe numbers?\")\n\nFigure 7.24: First principal plane showing the projections of the athletes. Do\nyou notice something about the organization of the numbers?\n\n__\n\nQuestion 7.29\n\nIf we look at the athletes themselves as they are shown in Figure 7.24, we\nnotice a slight ordering effect. Do you see a relation between the performance\nof the athletes and their numbering in Figure 7.24 ?\n\n__\n\nSolution\n\n__\n\nIf you play join the dots following the order of the numbers, you will\nprobably realize you are spending more time on one side of the plot than you\nwould be if the numbers were randomly assigned.\n\nIt turns out there is complementary information available in the `olympic`\ndataset. An extra vector variable called `score` reports the final scores at\nthe competition, the men’s decathlon at the 1988 Olympics.\n\n    \n    \n    olympic$score __\n    \n    \n     [1] 8488 8399 8328 8306 8286 8272 8216 8189 8180 8167 8143 8114 8093 8083 8036\n    [16] 8021 7869 7860 7859 7781 7753 7745 7743 7623 7579 7517 7505 7422 7310 7237\n    [31] 7231 7016 6907\n\nSo let us look at the scatterplot comparing the first principal component\ncoordinate of the athletes to this score. This is shown in Figure 7.25. We can\nsee a strong correlation between the two variables. We note that athlete\nnumber 1 (who in fact won the Olympic decathlon gold medal) has the highest\nscore, but not the highest value in PC1. Why do you think that is?\n\n    \n    \n    ggplot(data = tibble(pc1 = pcan.ath$li[, 1], score = olympic$score, label = rownames(athletes)),\n           mapping = aes(y = score, x = pc1)) + \n       geom_text(aes(label = label)) + stat_smooth(method = \"lm\", se = FALSE)__\n\n[![](07-chap_files/figure-html/fig-\nAthleteScorePCA-1.png)](07-chap_files/figure-html/fig-AthleteScorePCA-1.png\n\"Figure 7.25: Scatterplot between olympic$score and the first principal\ncomponent. The points are labeled by their order in the data set. We can see a\nstrong correlation. Why is it not a perfectly linear fit?\")\n\nFigure 7.25: Scatterplot between `olympic$score` and the first principal\ncomponent. The points are labeled by their order in the data set. We can see a\nstrong correlation. Why is it not a perfectly linear fit?\n\n### 7.7.3 How to choose \\\\(k\\\\), the number of dimensions ?\n\n[![](07-chap_files/figure-html/fig-screeploteq-1.png)](07-chap_files/figure-\nhtml/fig-screeploteq-1.png \"Figure 7.26: A screeplot showing ‘dangerously’\nsimilar variances. Choosing to cutoff at a hard threshold of 80% of the\nvariance would give unstable PC plots. With so such cutoff, the axes\ncorresponding to the 3D subspace of 3 similar eigenvalues are unstable and\ncannot be individually interpreted.\")\n\nFigure 7.26: A screeplot showing ‘dangerously’ similar variances. Choosing to\ncutoff at a hard threshold of 80% of the variance would give unstable PC\nplots. With so such cutoff, the axes corresponding to the 3D subspace of 3\nsimilar eigenvalues are unstable and cannot be individually interpreted.\n\nWe have seen in the examples that the first step in PCA is to make the\nscreeplot of the variances of the new variables (equal to the\n**eigenvalues**). We cannot decide how many dimensions are needed before\nseeing this plot. The reason is that there are situations when the principal\ncomponents are ill-defined: when two or three successive PCs have very similar\nvariances, giving a screeplot as in Figure 7.26, the subspace corresponding to\na group of similar eigenvalues exists. In this case this would be 3D space\ngenerated by \\\\(u_2,u_3,u_4\\\\). The vectors are not meaningful individually\nand one cannot interpret their loadings. This is because a very slight change\nin one observations could give a completely different set of three vectors.\nThese would generate the same 3D space, but could have very different\nloadings. We say the PCs are unstable.\n\n## 7.8 PCA as an exploratory tool: using extra information\n\nWe have seen that unlike regression, PCA treats all variables equally (to the\nextent that they were preprocessed to have equivalent standard deviations).\nHowever, it is still possible to map other continuous variables or categorical\nfactors onto the plots in order to help interpret the results. Often we have\ncomplementary information on the samples, for example, diagnostic labels in\nthe diabetes data or the cell types in the T-cell gene expression data.\n\nHere we see how we can use such extra variables to inform our interpretation.\nThe best place to store such so-called _metadata_ is in appropriate slots of\nthe data object (such as in the Bioconductor _SummarizedExperiment_ class);\nthe second-best, in additional columns of the data frame that also contains\nthe numeric data. In practice, such information is often stored in a more or\nless cryptic manner in the row names of the matrix. Below, we need to face the\nlatter scenario, and we use `substr` gymnastics to extract the cell types and\nshow the screeplot in Figure 7.27 and the PCA in Figure 7.28.\n\n    \n    \n    pcaMsig3 = dudi.pca(Msig3transp, center = TRUE, scale = TRUE,\n                        scannf = FALSE, nf = 4)\n    fviz_screeplot(pcaMsig3) + ggtitle(\"\")__\n\n[![](07-chap_files/figure-html/fig-tcellexpr-1.png)](07-chap_files/figure-\nhtml/fig-tcellexpr-1.png \"Figure 7.27: T-cell expression PCA screeplot.\")\n\nFigure 7.27: T-cell expression PCA screeplot.\n\n    \n    \n    ids = rownames(Msig3transp)\n    celltypes = factor(substr(ids, 7, 9))\n    status = factor(substr(ids, 1, 3))\n    table(celltypes)__\n    \n    \n    celltypes\n    EFF MEM NAI \n     10   9  11 \n    \n    \n    cbind(pcaMsig3$li, tibble(Cluster = celltypes, sample = ids)) %>%\n    ggplot(aes(x = Axis1, y = Axis2)) +\n      geom_point(aes(color = Cluster), size = 5) +\n      geom_hline(yintercept = 0, linetype = 2) +\n      geom_vline(xintercept = 0, linetype = 2) +\n      scale_color_discrete(name = \"Cluster\") + coord_fixed()__\n\n[![](07-chap_files/figure-html/fig-tcelltypes-1-1.png)](07-chap_files/figure-\nhtml/fig-tcelltypes-1-1.png \"Figure 7.28: PCA of gene expression for a subset\nof 156 genes involved in specificities of each of the three separate T-cell\ntypes: effector, naïve and memory. Again, we see that the plot is elongated\nalong the the first axis, as that explains much of the variance. Notice that\none of the T-cells seems to be mislabeled.\")\n\nFigure 7.28: PCA of gene expression for a subset of 156 genes involved in\nspecificities of each of the three separate T-cell types: effector, naïve and\nmemory. Again, we see that the plot is elongated along the the first axis, as\nthat explains much of the variance. Notice that one of the T-cells seems to be\nmislabeled.\n\n### 7.8.1 Mass Spectroscopy Data Analysis\n\nThese data requires delicate preprocessing before we obtain our desired matrix\nwith the relevant features as columns and the samples as rows. Starting with\nthe raw mass spectroscopy readings, the steps involve extracting peaks of\nrelevant features, aligning them across multiple samples and estimating peak\nheights. We refer the reader to the vignette of the Bioconductor\n**[xcms](https://bioconductor.org/packages/xcms/)** package for gruesome\ndetails. We load a matrix of data generated in such a way from the file\n`mat1xcms.RData`. The output of the below code is in Figures 7.29 and 7.30.\n\n    \n    \n    load(\"../data/mat1xcms.RData\")\n    dim(mat1)__\n    \n    \n    [1] 399  12\n    \n    \n    pcamat1 = dudi.pca(t(mat1), scannf = FALSE, nf = 3)\n    fviz_eig(pcamat1, geom = \"bar\", bar_width = 0.7) + ggtitle(\"\")__\n\n[![](07-chap_files/figure-html/fig-xset3scree-1.png)](07-chap_files/figure-\nhtml/fig-xset3scree-1.png \"Figure 7.29: Screeplot showing the eigenvalues for\nthe mice data.\")\n\nFigure 7.29: Screeplot showing the eigenvalues for the mice data.\n\n    \n    \n    dfmat1 = cbind(pcamat1$li, tibble(\n        label = rownames(pcamat1$li),\n        number = substr(label, 3, 4),\n        type = factor(substr(label, 1, 2))))\n    pcsplot = ggplot(dfmat1,\n      aes(x=Axis1, y=Axis2, label=label, group=number, colour=type)) +\n     geom_text(size = 4, vjust = -0.5)+ geom_point(size = 3)+ylim(c(-18,19))\n    pcsplot + geom_hline(yintercept = 0, linetype = 2) +\n      geom_vline(xintercept = 0, linetype = 2)__\n\n[![](07-chap_files/figure-html/fig-\nStretchedbiplot-1.png)](07-chap_files/figure-html/fig-Stretchedbiplot-1.png\n\"Figure 7.30: The first principal plane for the mat1 data. It explains 59% of\nthe variance.\")\n\nFigure 7.30: The first principal plane for the `mat1` data. It explains 59% of\nthe variance.\n\n__\n\nQuestion 7.30\n\nLooking at Figure 7.30, do the samples seem to be randomly placed in the\nplane? Do you notice any structure explained by the labels?\n\n__\n\nSolution\n\n__\n\nThe answer becomes (even more) evident if you make this plot. Knockouts are\nalways below their paired wildtype sample. We will revisit this example when\nwe look at supervised multivariate methods in our next chapter.\n\n    \n    \n    pcsplot + geom_line(colour = \"red\")__\n\n### 7.8.2 Biplots and scaling\n\nIn the previous example, the number of variables measured was too large to\nenable useful concurrent plotting of both variables and samples. In this\nexample we plot the PCA biplot of a simple data set where chemical\nmeasurements were made on different wines for which we also have a categorical\n`wine.class` variable. We start the analysis by looking at the two-dimensional\ncorrelations and a heatmap of the variables.\n\n    \n    \n    library(\"pheatmap\")\n    load(\"../data/wine.RData\")\n    load(\"../data/wineClass.RData\")\n    wine[1:2, 1:7]__\n    \n    \n      Alcohol MalicAcid  Ash AlcAsh  Mg Phenols Flav\n    1   14.23      1.71 2.43   15.6 127    2.80 3.06\n    2   13.20      1.78 2.14   11.2 100    2.65 2.76\n    \n    \n    pheatmap(1 - cor(wine), treeheight_row = 0.2)__\n\n[![](07-chap_files/figure-html/fig-WineHeatplot-1.png)](07-chap_files/figure-\nhtml/fig-WineHeatplot-1.png \"Figure 7.31: The difference between 1 and the\ncorrelation can be used as a distance between variables and is used to make a\nheatmap of the associations between the variables.\")\n\nFigure 7.31: The difference between 1 and the correlation can be used as a\ndistance between variables and is used to make a heatmap of the associations\nbetween the variables.\n\nA **biplot** is a simultaneous representation of both the space of\nobservations and the space of variables. In the case of a PCA biplot like\nFigure 7.32 the arrows represent the directions of the old variables as they\nproject onto the plane defined by the first two new axes. Here the\nobservations are just colored dots, the color has been chosen according to\nwhich type of wine is being plotted. We can interpret the variables’\ndirections with regards to the sample points, for instance the blue points are\nfrom the barbera group and show higher Malic Acid content than the other\nwines.\n\n    \n    \n    winePCAd = dudi.pca(wine, scannf=FALSE)\n    table(wine.class)__\n    \n    \n    wine.class\n        barolo grignolino    barbera \n            59         71         48 \n    \n    \n    fviz_pca_biplot(winePCAd, geom = \"point\", habillage = wine.class,\n       col.var = \"violet\", addEllipses = TRUE, ellipse.level = 0.69) +\n       ggtitle(\"\") + coord_fixed()__\n\n[![](07-chap_files/figure-html/fig-WineBiplot2-1.png)](07-chap_files/figure-\nhtml/fig-WineBiplot2-1.png \"Figure 7.32: PCA biplot including ellipses for the\nthree types of wine: barolo, grignolino and barbera. For each ellipsis, the\naxis lengths are given by one standard deviation. Small angles between the\nvectors Phenols, Flav and Proa indicate that they are strongly correlated,\nwhereas Hue and Alcohol are uncorrelated.\")\n\nFigure 7.32: PCA biplot including ellipses for the three types of wine:\nbarolo, grignolino and barbera. For each ellipsis, the axis lengths are given\nby one standard deviation. Small angles between the vectors `Phenols`, `Flav`\nand `Proa` indicate that they are strongly correlated, whereas `Hue` and\n`Alcohol` are uncorrelated.\n\nInterpretation of multivariate plots requires the use of as much of the\navailable information as possible; here we have used the samples and their\ngroups as well as the variables to understand the main differences between the\nwines.\n\n### 7.8.3 An example of weighted PCA\n\nSometimes we want to see variability between different groups or observations,\nbut want to weight them. This can be the case if, e.g., the groups have very\ndifferent sizes. Let’s re-examine the Hiiragi data we already saw in [Chapter\n3](03-chap.html). In the code below, we select the wildtype (WT) samples and\nthe top 100 features with the highest overall variance.\n\n    \n    \n    data(\"x\", package = \"Hiiragi2013\")\n    xwt = x[, x$genotype == \"WT\"]\n    sel = order(rowVars(Biobase::exprs(xwt)), decreasing = TRUE)[1:100]\n    xwt = xwt[sel, ]\n    tab = table(xwt$sampleGroup)\n    tab __\n    \n    \n         E3.25 E3.5 (EPI)  E3.5 (PE) E4.5 (EPI)  E4.5 (PE) \n            36         11         11          4          4 \n    \n    \n    xwt$weight = 1 / as.numeric(tab[xwt$sampleGroup])\n    pcaMouse = dudi.pca(as.data.frame(t(Biobase::exprs(xwt))),\n      row.w = xwt$weight,\n      center = TRUE, scale = TRUE, nf = 2, scannf = FALSE)\n    fviz_eig(pcaMouse) + ggtitle(\"\")__\n\n[![](07-chap_files/figure-html/fig-resPCADscree-1.png)](07-chap_files/figure-\nhtml/fig-resPCADscree-1.png \"Figure 7.33: Screeplot from the weighted PCA of\nthe Hiiragi data. The drop after the second eigenvalue suggests that a two-\ndimensional PCA is appropriate.\")\n\nFigure 7.33: Screeplot from the weighted PCA of the Hiiragi data. The drop\nafter the second eigenvalue suggests that a two-dimensional PCA is\nappropriate.\n\n    \n    \n    fviz_pca_ind(pcaMouse, geom = \"point\", col.ind = xwt$sampleGroup) +\n      ggtitle(\"\") + coord_fixed()__\n\nWe see from `tab` that the groups are represented rather unequally. To account\nfor this, we reweigh each sample by the inverse of its group size. The\nfunction `dudi.pca` in the\n**[ade4](https://cran.r-project.org/web/packages/ade4/)** package has a\n`row.w` argument into which we can enter the weights. The output of the code\nis in Figures 7.33 and 7.34.\n\n[![](07-chap_files/figure-html/fig-resPCADplot-1.png)](07-chap_files/figure-\nhtml/fig-resPCADplot-1.png \"Figure 7.34: Output from weighted PCA on the\nHiiragi data. The samples are colored according to their groups.\")\n\nFigure 7.34: Output from weighted PCA on the Hiiragi data. The samples are\ncolored according to their groups.\n\n## 7.9 Summary of this chapter\n\n**Preprocessing matrices** Multivariate data analyses require “conscious”\npreprocessing. After consulting all the means, variances and one-dimensional\nhistograms we saw how to rescale and recenter the data.\n\n**Projecting onto new variables** We saw how we can make projections into\nlower dimensions (2D planes and 3D are the most frequently used) of high\ndimensional data without losing too much information. PCA searches for new\n“more informative” variables that are linear combinations of the original\n(old) ones.\n\n**Matrix decomposition** PCA is based on finding decompositions of the matrix\n\\\\(X\\\\) called SVD. This decomposition provides a lower rank approximation and\nis equivalent to the eigendecomposition of \\\\(X^tX\\\\). The squares of the\nsingular values are equal to the eigenvalues and to the variances of the new\nvariables. We systematically plotted these values before deciding how many\naxes are necessary to reproduce the signal in the data.\n\nTwo eigenvalues which are quite close can give rise to scores or PC scores\nwhich are highly unstable. It is always necessary to look at the screeplot of\nthe eigenvalues and avoid separating the axes corresponding to the these close\neigenvalues. This may require using interactive three or four-dimensional\nprojections, which are available in several R packages.\n\n**Biplot representations** The space of observations is naturally a\n\\\\(p\\\\)-dimensional space (the \\\\(p\\\\) original variables provide the\ncoordinates). The space of variables is \\\\(n\\\\)-dimensional. Both\ndecompositions we have studied (singular values / eigenvalues and singular\nvectors / eigenvectors) provide new coordinates for both of these spaces,\nsometimes we call one the dual of the other. We can plot the projection of\nboth the observations and the variables onto the same eigenvectors. This\nprovides a biplot that can be useful for interpreting the PCA output.\n\n**Projecting other group variables** Interpretation of PCA can also be\nfacilitated by redundant or contiguous data about the observations.\n\n## 7.10 Further reading\n\nThe best way to deepen your understanding of singular value decomposition is\nto read Chapter 7 of Strang ([2009](16-chap.html#ref-Strang:09)). The whole\nbook sets the foundations for the linear algebra necessary to understanding\nthe meaning of the rank of matrix and the duality between row spaces and\ncolumn spaces ([Holmes 2006](16-chap.html#ref-frenchway)).\n\nComplete textbooks have been written on the subject of PCA and related\nmethods. Mardia, Kent, and Bibby ([1979](16-chap.html#ref-Mardia)) is a\nstandard text that covers all multivariate methods in a classical way, with\nlinear algebra and matrices. By making the parametric assumptions that the\ndata come from multivariate normal distributions, Mardia, Kent, and Bibby\n([1979](16-chap.html#ref-Mardia)) also provide inferential tests for the\nnumber of components and limiting properties for principal components.\nJolliffe ([2002](16-chap.html#ref-Jolliffe)) is a book-long treatment of\neverything to do with PCA with extensive examples.\n\nWe can incorporate supplementary information into weights for the observations\nand the variables. This was introduced in the 1970’s by French data\nscientists, see Holmes ([2006](16-chap.html#ref-frenchway)) for a review and\n[Chapter 9](09-chap.html) for further examples.\n\nImprovements to the interpretation and stability of PCA can be obtained by\nadding a penalty that minimizes the number of nonzero coefficients that appear\nin the linear combinations. Zou, Hastie, and Tibshirani\n([2006](16-chap.html#ref-Zou2006)) and Witten, Tibshirani, and Hastie\n([2009](16-chap.html#ref-Witten2009)) have developed sparse versions of\nprincipal component analysis, and their packages\n**[elasticnet](https://cran.r-project.org/web/packages/elasticnet/)** and\n**[PMA](https://cran.r-project.org/web/packages/PMA/)** provide\nimplementations in R.\n\n## 7.11 Exercises\n\n__\n\nExercise 7.1\n\nRevise the material about svd by reading sections 1, 2, and 3 of the\n[Wikipedia article about\nSVD](http://en.wikipedia.org/wiki/Singular_value_decomposition). It will also\nbe beneficial to read about the related eigenvalue decomposition by reading\nsections 1, 2, and 2.1 of the [Wikipedia article about eigendecomposition of a\nmatrix](http://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix). We know\nthat we can decompose a \\\\(n\\\\) row by \\\\(p\\\\) column rank 1 matrix \\\\(X\\\\)\nas:\n\n\\\\[ **X** = \\begin{pmatrix} x_{11} & x_{12} & ... & x_{1p}\\\\\\ x_{21} & x_{22}\n& ... & x_{2p}\\\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\\\ x_{n1} & x_{n2} & ...\n& x_{np} \\end{pmatrix} = \\begin{pmatrix} u_{11} \\\\\\ u_{21} \\\\\\ \\vdots \\\\\\\nu_{n1} \\\\\\ \\end{pmatrix} \\times \\begin{pmatrix} v_{11} & v_{21} & \\cdots &\nv_{p1} \\end{pmatrix} \\\\]\n\n  1. If \\\\(X\\\\) has no rows and no columns which are all zeros, then is this decomposition unique?  \n\n  2. Generate a rank-one matrix. Start by taking a vector of length 15 with values from 2 to 30 in increments of 2, and a vector of length 4 with values 3, 6, 9, 12, then take their outer product.\n\n    \n    \n    u = seq(2, 30, by = 2)\n    v = seq(3, 12, by = 3)\n    X1 = u %*% t(v)__\n\nWhy do we have to take `t(v)`?\n\n  3. Now we add some noise in the form a matrix we call `Materr` so we have an “approximately rank-one” matrix.\n\n    \n    \n    Materr = matrix(rnorm(60,1),nrow=15,ncol=4)\n    X = X1+Materr __\n\nVisualize \\\\(X\\\\) using `ggplot`.\n\n  4. Redo the same analyses with a rank 2 matrix.\n\n__\n\nSolution\n\n__\n\nNote that `X1` can also be computed as\n\n    \n    \n    outer(u, v)__\n    \n    \n    ggplot(data=data.frame(X), aes(x=X1, y=X2, col=X3, size=X4)) + geom_point()__\n\nHere we see that the data looks linear in all four dimensions. This is what it\nmeans to be of rank-one. Now let’s consider a rank 2 matrix.\n\n    \n    \n    n = 100\n    p = 4\n    Y2 = outer(rnorm(n), rnorm(p)) + outer(rnorm(n), rnorm(p))\n    head(Y2)__\n    \n    \n                [,1]       [,2]         [,3]        [,4]\n    [1,] -0.44143871  2.3213197  0.433215525 -1.35523790\n    [2,]  0.79620920 -1.0748037  1.217052906 -1.13096295\n    [3,]  0.16787281  0.2259296  0.547203332 -0.75836031\n    [4,]  0.87269426 -1.9208649  0.856966180 -0.38621340\n    [5,]  0.03751521 -0.1480678 -0.005217966  0.05864122\n    [6,]  0.50195482 -2.0409896 -0.108241027  0.85336630\n    \n    \n    ggplot(data=data.frame(Y2), aes(x=X1, y=X2, col=X3, size=X4)) + geom_point()__\n\nNow there are obviously at least two dimensions because if we project the data\nonto the first two coordinates (by default called `X1` and `X2` when you\nconvert a matrix into a data frame in R), then the data varies in both\ndimensions. So the next step is to try to decide if there are more than two\ndimensions. The top right points are the closest to you (they’re biggest) and\nas you go down and left in the plot those points are farther away. In the left\nare the bluest points and they seem to get darker linearly as you move right.\nAs you can probably tell, it is very hard to visually discover a low\ndimensional space in higher dimensions, even when “high dimensions” only means\n4! This is one reason why we rely on the singular value decomposition.\n\n    \n    \n    svd(Y2)$d # two non-zero eigenvalues __\n    \n    \n    [1] 2.637465e+01 1.266346e+01 3.144564e-15 1.023131e-15\n    \n    \n    Y = Y2 + matrix(rnorm(n*p, sd=0.01),n,p) # add some noise to Y2\n    svd(Y)$d # four non-zero eigenvalues (but only 2 big ones)__\n    \n    \n    [1] 26.39673712 12.68547439  0.10735103  0.09104741\n\nHere we have two dimensions which are non-zero and two dimensions which are\napproximately 0 (for “Y2”, they are within square root of computer tolerance\nof 0).\n\n__\n\nExercise 7.2\n\n  1. create a first a matrix of highly correlated bivariate data such as that shown in Figure 7.35.  \nHint: Use the function `mvrnorm`.\n\nCheck the rank of the matrix by looking at its singular values.\n\n  2. perform a PCA and show the rotated principal component axes.\n\n__\n\nSolution\n\n__\n\n  1. we generate correlated bivariate normal data using:\n\n    \n    \n    library(\"MASS\")\n    mu1 = 1; mu2 = 2; s1=2.5; s2=0.8; rho=0.9;\n    sigma = matrix(c(s1^2, s1*s2*rho, s1*s2*rho, s2^2),2)\n    sim2d = data.frame(mvrnorm(50, mu = c(mu1,mu2), Sigma = sigma))\n    svd(scale(sim2d))$d __\n    \n    \n    [1] 9.647686 2.218592\n    \n    \n    svd(scale(sim2d))$v[,1]__\n    \n    \n    [1] 0.7071068 0.7071068\n\n  2. We use `prcomp` to perform a PCA and the scores provide the desired rotation.\n\n    \n    \n    respc = princomp(sim2d)\n    dfpc  = data.frame(pc1=respc$scores[,1], \n                       pc2=respc$scores[,2])\n    \n    ggplot(data.frame(sim2d), aes(x=X1,y=X2)) + geom_point()\n    ggplot(dfpc, aes(x=pc1, y=pc2)) + geom_point() + coord_fixed(2)__\n\n[![](07-chap_files/figure-html/fig-binormalpc-1.png)](07-chap_files/figure-\nhtml/fig-binormalpc-1.png \"Figure 7.35 \\(a\\): \\\\text{}\")\n\n(a) \\\\(\\text{}\\\\)\n\n[![](07-chap_files/figure-html/fig-binormalpc-2.png)](07-chap_files/figure-\nhtml/fig-binormalpc-2.png \"Figure 7.35 \\(b\\): \\\\text{}\")\n\n(b) \\\\(\\text{}\\\\)\n\nFigure 7.35: The original data shown in scatterplot (A) and the plot obtained\nusing the principal component rotation (B).\n\n__\n\nExercise 7.3\n\nPart (a) in Figure 7.35 shows a very elongated plotting region, why?  \nWhat happens if you do not use the `coord_fixed()` option and have a square\nplotting zone? Why can this be misleading?\n\n__\n\nExercise 7.4\n\nLet’s revisit the Hiiragi data and compare the weighted and unweighted\napproaches.\n\n  1. make a correlation circle for the unweighted Hiiragi data `xwt`. Which genes have the best projections on the first principal plane (best approximation)?\n\n  2. make a biplot showing the labels of the extreme gene-variables that explain most of the variance in the first plane. Add the the sample-points.\n\nAbbott, Edwin A. 1884. _Flatland: A Romance of Many Dimensions_. OUP Oxford.\n\nFlury, Bernard. 1997. _A First Course in Multivariate Statistics_. Springer.\n\nHolmes, Susan. 2006. “Multivariate Analysis: The French way.” In _Probability\nand Statistics: Essays in Honor of David a. Freedman_ , edited by D. Nolan and\nT. P. Speed. Vol. 56. IMS Lecture Notes–Monograph Series. Beachwood, OH: IMS.\n<http://www.imstat.org/publications/lecnotes.htm>.\n\nHolmes, Susan, Michael He, Tong Xu, and Peter P Lee. 2005. “Memory t Cells\nHave Gene Expression Patterns Intermediate Between Naive and Effector.” _PNAS_\n102 (15): 5519–23.\n\nHotelling, Harold. 1933. “Analysis of a Complex of Statistical Variables into\nPrincipal Components.” _Journal of Educational Psychology_ 24 (6): 417–41.\n\nJolicoeur, Pierre, and James E Mosimann. 1960. “Size and Shape Variation in\nthe Painted Turtle. A Principal Component Analysis.” _Growth_ 24: 339–54.\n\nJolliffe, Ian. 2002. _Principal Component Analysis_. Wiley Online Library.\n\nMardia, Kanti, John T Kent, and John M Bibby. 1979. _Multiariate Analysis_.\nNew York: Academic Press.\n\nPearson, Karl. 1901. “LIII. On Lines and Planes of Closest Fit to Systems of\nPoints in Space.” _The London, Edinburgh, and Dublin Philosophical Magazine\nand Journal of Science_ 2 (11): 559–72.\n\nStrang, Gilbert. 2009. _Introduction to Linear Algebra_. Fourth. Wellesley-\nCambridge Press.\n\nWitten, Daniela M, Robert Tibshirani, and Trevor Hastie. 2009. “A Penalized\nMatrix Decomposition, with Applications to Sparse Principal Components and\nCanonical Correlation Analysis.” _Biostatistics_ , kxp008.\n\nZou, Hui, Trevor Hastie, and Robert Tibshirani. 2006. “Sparse Principal\nComponent Analysis.” _Journal of Computational and Graphical Statistics_ 15\n(2): 265–86.\n\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["msmb.css"],"toc":true,"output-file":"07-chap.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}