{"title":"5.1 Goals for this chapter","markdown":{"headingText":"5.1 Goals for this chapter","containsRefs":false,"markdown":"![](imgs/starlings_copyrightfree.jpg)\n\nFinding categories of cells, illnesses, organisms and then naming them is a\ncore activity in the natural sciences. In [Chapter 4](04-chap.html) we’ve seen\nthat some data can be modeled as mixtures from different groups or populations\nwith a clear parametric generative model. We saw how in those examples we\ncould use the EM algorithm to disentangle the components. We are going to\nextend the idea of unraveling of different groups to cases where the\n**clusters** do not necessarily have nice elliptic1 shapes.\n\n1 Mixture modeling with multivariate normal distributions implies elliptic\ncluster boundaries.\n\nClustering takes data (continuous or quasi-continuous) and adds to them a new\ncategorical _group_ variable that can often simplify decision making; even if\nthis sometimes comes at a cost of ignoring _intermediate_ states. For\ninstance, medical decisions are simplified by replacing possibly complex,\nhigh-dimensional diagnostic measurements by simple groupings: a full report of\nnumbers associated with fasting glucose, glycated hemoglobin and plasma\nglucose two hours after intake is replaced by assigning the patient to a\ndiabetes mellitus “group”.\n\nIn this chapter, we will study how to find meaningful clusters or groups in\nboth low-dimensional and high-dimensional **nonparametric** settings. However,\nthere is a caveat: clustering algorithms are designed to find clusters, so\nthey will find clusters, even where there are none2. So, cluster _validation_\nis an essential component of our process, especially if there is no prior\ndomain knowledge that supports the existence of clusters.\n\n2 This is reminescent of humans: we like to see patterns—even in randomness.\n\n\nIn this chapter we will\n\n  * Study the different types of data that can be beneficially clustered.\n\n  * See measures of (dis)similarity and **distances** that help us define clusters.\n\n  * Uncover hidden or latent clustering by partitioning the data into _tighter_ sets.\n\n  * Use clustering when given biomarkers on each of hundreds of thousands cells. We’ll see that for instance immune cells can be naturally grouped into tight subpopulations.\n\n  * Run nonparametric algorithms such as **\\\\(k\\\\) -means**, **\\\\(k\\\\) -medoids** on real single cell data.\n\n  * Experiment with recursive approaches to clustering that combine observations and groups into a hierarchy of sets; these methods are known as **hierarchical clustering**.\n\n  * Study how to validate clusters through resampling-based bootstrap approaches, which we will demonstrate on a single-cell dataset.\n\n[![](imgs/SnowMapSmallest_web.png)](imgs/SnowMapSmallest_web.png \"Figure 5.1:\nJohn Snow’s map of cholera cases: small barcharts at each house indicate a\nclustering of diagnosed cases.\")\n\nFigure 5.1: John Snow’s map of cholera cases: small barcharts at each house\nindicate a clustering of diagnosed cases.\n\n[![David Freedman has a wonderful detailed account of all the steps that led\nto this discovery \\(Freedman 1991\\).](imgs/book_icon.png)](imgs/book_icon.png\n\"David Freedman has a wonderful detailed account of all the steps that led to\nthis discovery \\[@freedman1991\\].\")\n\nDavid Freedman has a wonderful detailed account of all the steps that led to\nthis discovery ([Freedman 1991](16-chap.html#ref-freedman1991)).\n\n## 5.2 What are the data and why do we cluster them?\n\n### 5.2.1 Clustering can sometimes lead to discoveries.\n\nJohn Snow made a map of cholera cases and identified _clusters_ of cases. He\nthen collected additional information about the situation of the pumps. The\nproximity of dense clusters of cases to the Broadstreet pump pointed to the\nwater as a possible culprit. He collected separate sources of information that\nenabled him to infer the source of the cholera outbreak.\n\nNow, let’s look at another map of London, shown in Figure 5.2. The red dots\ndesignate locations that were bombed during World War II. Many theories were\nput forward during the war by the analytical teams. They attempted to find a\nrational explanation for the bombing patterns (proximity to utility plants,\narsenals, \\\\(...\\\\)). In fact, after the war it was revealed that the bombings\nwere randomly distributed without any attempt at hitting particular targets.\n\n[![](imgs/RedBombsLondon_web.png)](imgs/RedBombsLondon_web.png \"Figure 5.2:\nHere is a map of the location of the bombs that were dropped on London on\nSeptember, 7th, 1940 as depicted by the website of the British National\nArchives http://bombsight.org.\")\n\nFigure 5.2: Here is a map of the location of the bombs that were dropped on\nLondon on September, 7th, 1940 as depicted by the website of the British\nNational Archives <http://bombsight.org>.\n\nClustering is a useful technique for understanding complex multivariate data;\nit is an **unsupervised** 3. Exploratory techniques show groupings that can be\nimportant in interpreting the data.\n\n3 Thus named because all variables have the same status, we are not trying to\npredict or learn the value of one variable (the supervisory response) based on\nthe information from explanatory variables.\n\nFor instance, clustering has enabled researchers to enhance their\nunderstanding of cancer biology. Tumors that appeared to be the same based on\ntheir anatomical location and histopathology fell into multiple clusters based\non their molecular signatures, such as gene expression data ([Hallett et al.\n2012](16-chap.html#ref-Hallett2012)). Eventually, such clusterings might lead\nto the definition of new, more relevant disease types. Relevance is evidenced,\ne.g., by the fact that they are associated with different patient outcomes.\nWhat we aim to do in this chapter is understand how pictures like Figure 5.3\nare constructed and how to interpret them.\n\n[![](imgs/BreastCancerSubType_Biomed.png)](imgs/BreastCancerSubType_Biomed.png\n\"Figure 5.3: The breast cancer samples \\(shown from The Cancer Genome Atlas\n\\(TCGA\\) and the Molecular Taxonomy of Breast Cancer International Consortium\n\\(METABRIC\\)\\) can be split into groups using their miRNA expression\n\\[@Aure2017\\]. The authors show in the lower plots that the survival times in\ndifferent % groups were different. Thus these clusters were biologically and\nclinically relevant. The promise of such analyses is that the groups can be\nused to provide more specific, optimized treatments.\")\n\nFigure 5.3: The breast cancer samples (shown from The Cancer Genome Atlas\n(TCGA) and the Molecular Taxonomy of Breast Cancer International Consortium\n(METABRIC)) can be split into groups using their miRNA expression ([Aure et\nal. 2017](16-chap.html#ref-Aure2017)). The authors show in the lower plots\nthat the survival times in different % groups were different. Thus these\nclusters were biologically and clinically relevant. The promise of such\nanalyses is that the groups can be used to provide more specific, optimized\ntreatments.\n\nIn [Chapter 4](04-chap.html), we have already studied one technique, the EM\nalgorithm, for uncovering groups. The techniques we explore in this chapter\nare more general and can be applied to more complex data. Many of them are\nbased on distances between pairs of observations (this can be all versus all,\nor sometimes only all versus some), and they make no explicit assumptions\nabout the generative mechanism of the data involving particular families of\ndistributions, such as normal, gamma-Poisson, etc. There is a proliferation of\nclustering algorithms in the literature and in the scientific software\nlandscape; this can be intimidating. In fact it is linked to the diversity of\nthe types of data and the objectives pursued in different domains.\n\n__\n\nTask\n\nLook up the [BiocViews\nClustering](http://www.bioconductor.org/packages/release/BiocViews.html) or\nthe [Cluster view on CRAN](https://cran.r-project.org/web/views/Cluster.html)\nand count the number of packages providing clustering tools.\n\n[![](imgs/ClusteringA.png)](imgs/ClusteringA.png \"Figure 5.4: We decompose the\nchoices made in a clustering algorithm according to the steps taken: starting\nfrom an observations-by-features rectangular table X, we choose an\nobservations-to-observations distance measure and compute the distance matrix,\nhere schematized by the triangle. The distances are used to construct the\nclusters. On the left, we schematize agglomerative methods, that build a\nhierarchical clustering tree; on the right, partitioning methods that separate\nthe data into subsets. Both types of methods require a choice to be made: the\nnumber k of clusters. For partitionning approaches such as k-means this choice\nhas to be made at the outset; for hierarchical clustering this can be deferred\nto the end of the analysis.\")\n\nFigure 5.4: We decompose the choices made in a clustering algorithm according\nto the steps taken: starting from an observations-by-features rectangular\ntable \\\\(X\\\\), we choose an observations-to-observations distance measure and\ncompute the distance matrix, here schematized by the triangle. The distances\nare used to construct the clusters. On the left, we schematize agglomerative\nmethods, that build a hierarchical clustering tree; on the right, partitioning\nmethods that separate the data into subsets. Both types of methods require a\nchoice to be made: the number \\\\(k\\\\) of clusters. For partitionning\napproaches such as \\\\(k\\\\)-means this choice has to be made at the outset; for\nhierarchical clustering this can be deferred to the end of the analysis.\n\n## 5.3 How do we measure similarity?\n\n[![Of a feather: how the distances are measured and similarities between\nobservations defined has a strong impact on the clustering\nresult.](imgs/devil.png)](imgs/devil.png \"Of a feather: how the distances are\nmeasured and similarities between observations defined has a strong impact on\nthe clustering result.\")\n\n**Of a feather** : how the distances are measured and similarities between\nobservations defined has a strong impact on the clustering result.\n\nOur first step is to decide what we mean by _similar_. There are multiple ways\nof comparing birds: for instance, a distance using size and weight will give a\ndifferent clustering than one using diet or habitat. Once we have chosen the\nrelevant features, we have to decide how we combine differences between the\nmultiple features into a single number. Here is a selection of choices, some\nof them are illustrated in Figure 5.5.\n\n![Figure 5.5 \\(a\\):](imgs/FourDistances_a.png)\n\n(a)\n\n![Figure 5.5 \\(b\\):](imgs/FourDistances_b.png)\n\n(b)\n\n![Figure 5.5 \\(c\\):](imgs/FourDistances_c.png)\n\n(c)\n\n![Figure 5.5 \\(d\\):](imgs/FourDistances_d.png)\n\n(d)\n\nFigure 5.5: Equal-distance contour plots according to four different\ndistances: points on any one curve are all the same distance from the center\npoint.\n\n**Euclidean** The Euclidean distance between two points \\\\(A=(a_1,...,a_p)\\\\)\nand \\\\(B= (b_1,...,b_p)\\\\) in a \\\\(p\\\\)-dimensional space (for the \\\\(p\\\\)\nfeatures) is the square root of the sum of squares of the differences in all\n\\\\(p\\\\) coordinate directions:\n\n\\\\[ d(A,B)=\\sqrt{(a_1-b_1)^2+(a_2-b_2)^2+... +(a_p-b_p)^2}. \\\\]\n\n**Manhattan** The Manhattan, City Block, Taxicab or \\\\(L_1\\\\) distance takes\nthe sum of the absolute differences in all coordinates.\n\n\\\\[ d(A,B)=|a_1-b_1|+|a_2-b_2|+... +|a_p-b_p|. \\\\]\n\n**Maximum** The maximum of the absolute differences between coordinates is\nalso called the \\\\(L_\\infty\\\\) distance:\n\n\\\\[ d_\\infty(A,B)= \\max_{i}|a_i-b_i|. \\\\]\n\n**Weighted Euclidean distance** is a generalization of the ordinary Euclidean\ndistance, by giving different directions in feature space different weights.\nWe have already encountered one example of a weighted Euclidean distance in\n[Chapter 2](02-chap.html), the \\\\(\\chi^2\\\\) distance. It is used to compare\nrows in contingency tables, and the weight of each feature is the inverse of\nthe expected value. The _Mahalanobis_ distance is another weighted Euclidean\ndistance that takes into account the fact that different features may have a\ndifferent dynamic range, and that some features may be positively or\nnegatively correlated with each other. The weights in this case are derived\nfrom the covariance matrix of the features. See also Question 5.1.\n\n**Minkowski** Allowing the exponent to be \\\\(m\\\\) instead of \\\\(2\\\\), as in\nthe Euclidean distance, gives the Minkowski distance\n\n\\\\[ d(A,B) = \\left( (a_1-b_1)^m+(a_2-b_2)^m+... +(a_p-b_p)^m\n\\right)^\\frac{1}{m}. \\tag{5.1}\\\\]\n\n**Edit, Hamming** This distance is the simplest way to compare character\nsequences. It simply counts the number of differences between two character\nstrings. This could be applied to nucleotide or amino acid sequences –\nalthough in that case, the different character substitutions are usually\nassociated with different contributions to the distance (to account for\nphysical or evolutionary similarity), and deletions and insertions may also be\nallowed.\n\n**Binary** When the two vectors have binary bits as coordinates, we can think\nof the non-zero elements as ‘on’ and the zero elements as ‘off’. The binary\ndistance is the proportion of features having only one bit on amongst those\nfeatures that have at least one bit on.\n\n**Jaccard Distance** Occurrence of traits or features in ecological or\nmutation data can be translated into presence and absence and encoded as 1’s\nand 0’s. In such situations, co-occurence is often more informative than co-\nabsence. For instance, when comparing mutation patterns in HIV, the co-\nexistence in two different strains of a mutation tends to be a more important\nobservation than its co-absence. For this reason, biologists use the **Jaccard\nindex**. Let’s call our two observation vectors \\\\(S\\\\) and \\\\(T\\\\),\n\\\\(f_{11}\\\\) the number of times a feature co-occurs in \\\\(S\\\\) and \\\\(T\\\\),\n\\\\(f_{10}\\\\) (and \\\\(f_{01}\\\\)) the number of times a feature occurs in\n\\\\(S\\\\) but not in \\\\(T\\\\) (and vice versa), and \\\\(f_{00}\\\\) the number of\ntimes a feature is co-absent. The Jaccard index is\n\n\\\\[ J(S,T) = \\frac{f_{11}}{f_{01}+f_{10}+f_{11}}, \\tag{5.2}\\\\]\n\n(i.e., it ignores \\\\(f_{00}\\\\)), and the **Jaccard dissimilarity** is\n\n\\\\[ d_J(S,T) = 1-J(S,T) = \\frac{f_{01}+f_{10}}{f_{01}+f_{10}+f_{11}}.\n\\tag{5.3}\\\\]\n\n**Correlation based distance**\n\n\\\\[ d(A,B)=\\sqrt{2(1-\\text{cor}(A,B))}. \\\\]\n\n[![](05-chap_files/figure-html/fig-Mahalanobis-1.png)](05-chap_files/figure-\nhtml/fig-Mahalanobis-1.png \"Figure 5.6: An example for the use of Mahalanobis\ndistances to measure the distance of a new data point \\(red\\) from two cluster\ncenters.\")\n\nFigure 5.6: An example for the use of Mahalanobis distances to measure the\ndistance of a new data point (red) from two cluster centers.\n\n__\n\nQuestion 5.1\n\nWhich of the two cluster centers in Figure 5.6 is the red point closest to?\n\n__\n\nSolution\n\n__\n\nA naïve answer would use the Euclidean metric and decide that the point is\ncloser to the left cluster. However, as we see that the features have\ndifferent ranges and correlations, and that these even differ between the two\nclusters, it makes sense to use cluster-specific Mahalanobis distances. The\nfigure shows contour lines for both clusters. These were obtained from a\ndensity estimate; the Mahalanobis distance approximates these contours with\nellipses. The distance between the red point and each of the cluster centers\ncorresponds to the number of contour lines crossed. We see that as the group\non the right is more spread out, the red point is in fact closer to it.\n\n[![](imgs/DistanceTriangle.png)](imgs/DistanceTriangle.png \"Figure 5.7: The\nlower triangle of distances can be computed by any of a hundred different\nfunctions in various R packages \\(vegdist in vegan, daisy in cluster,\ngenetic_distance in gstudio, dist.dna in ape, Dist in amap, distance in\necodist, dist.multiPhylo in distory, shortestPath in gdistance, % dudi.dist\nand dist.genet in ade4\\).\")\n\nFigure 5.7: The lower triangle of distances can be computed by any of a\nhundred different functions in various R packages (`vegdist` in\n**[vegan](https://cran.r-project.org/web/packages/vegan/)** , `daisy` in\n**[cluster](https://cran.r-project.org/web/packages/cluster/)** ,\n`genetic_distance` in\n**[gstudio](https://cran.r-project.org/web/packages/gstudio/)** , `dist.dna`\nin **[ape](https://cran.r-project.org/web/packages/ape/)** , `Dist` in\n**[amap](https://cran.r-project.org/web/packages/amap/)** , `distance` in\n**[ecodist](https://cran.r-project.org/web/packages/ecodist/)** ,\n`dist.multiPhylo` in\n**[distory](https://cran.r-project.org/web/packages/distory/)** ,\n`shortestPath` in\n**[gdistance](https://cran.r-project.org/web/packages/gdistance/)** , %\n`dudi.dist` and `dist.genet` in\n**[ade4](https://cran.r-project.org/web/packages/ade4/)**).\n\n### 5.3.1 Computations related to distances in R\n\nThe `dist` function in R is designed to use less space than the full \\\\(n^2\\\\)\npositions a complete \\\\(n \\times n\\\\) distance matrix between \\\\(n\\\\) objects\nwould require. The function computes one of six choices of distance\n(`euclidean`, `maximum`, `manhattan`, `canberra`, `binary`, `minkowski`) and\noutputs a vector of values sufficient to reconstruct the complete distance\nmatrix. The function returns a special object of class `dist` that encodes the\nrelevant vector of size \\\\(n\\times(n-1)/2\\\\). Here is the output for a \\\\(3\\\\)\nby \\\\(3\\\\) matrix:\n\n    \n    \n    mx  = c(0, 0, 0, 1, 1, 1)\n    my  = c(1, 0, 1, 1, 0, 1)\n    mz  = c(1, 1, 1, 0, 1, 1)\n    mat = rbind(mx, my, mz)\n    dist(mat)__\n    \n    \n             mx       my\n    my 1.732051         \n    mz 2.000000 1.732051\n    \n    \n    dist(mat, method = \"binary\")__\n    \n    \n              mx        my\n    my 0.6000000          \n    mz 0.6666667 0.5000000\n\nIn order to access a particular distance (for example the distance between\nobservations 1 and 2), one has to turn the `dist` class object back into a\nmatrix.\n\n    \n    \n    load(\"../data/Morder.RData\")\n    sqrt(sum((Morder[1, ] - Morder[2, ])^2))__\n    \n    \n    [1] 5.593667\n    \n    \n    as.matrix(dist(Morder))[2, 1]__\n    \n    \n    [1] 5.593667\n\nLet’s look at how we would compute the Jaccard distance we defined above\nbetween HIV strains.\n\n    \n    \n    mut = read.csv(\"../data/HIVmutations.csv\")\n    mut[1:3, 10:16]__\n    \n    \n      p32I p33F p34Q p35G p43T p46I p46L\n    1    0    1    0    0    0    0    0\n    2    0    1    0    0    0    1    0\n    3    0    1    0    0    0    0    0\n\n__\n\nQuestion 5.2\n\nCompare the Jaccard distance (available as the function `vegdist` in the R\npackage **[vegan](https://cran.r-project.org/web/packages/vegan/)**) between\nmutations in the HIV data `mut` to the correlation based distance.\n\n__\n\nSolution\n\n__\n\n    \n    \n    library(\"vegan\")\n    mutJ = vegdist(mut, \"jaccard\")\n    mutC = sqrt(2 * (1 - cor(t(mut))))\n    mutJ __\n    \n    \n          1     2     3     4\n    2 0.800                  \n    3 0.750 0.889            \n    4 0.900 0.778 0.846      \n    5 1.000 0.800 0.889 0.900\n    \n    \n    as.dist(mutC)__\n    \n    \n         1    2    3    4\n    2 1.19               \n    3 1.10 1.30          \n    4 1.32 1.13 1.30     \n    5 1.45 1.19 1.30 1.32\n\n[![](imgs/birds_and_dinosaurs.png)](imgs/birds_and_dinosaurs.png \"Figure 5.8:\nAn example of computing the cophenetic distance \\(xkcd\\).\")\n\nFigure 5.8: An example of computing the cophenetic distance (xkcd).\n\nIt can also be interesting to compare complex objects that are not traditional\nvectors or real numbers using dissimilarities or distances. Gower’s distance\nfor data of mixed modalities (both categorical factors and continuous\nvariables) can be computed with the `daisy` function. In fact, distances can\nbe defined between any pairs of objects, not just points in \\\\({\\mathbb\nR}^p\\\\) or character sequences. For instance, the `shortest.paths` function\nfrom the **[igraph](https://cran.r-project.org/web/packages/igraph/)** package\nthat we will see in [Chapter 10](10-chap.html) computes the distance between\nvertices on a graph and the function `cophenetic` computes the distance\nbetween leaves of a tree as illustrated in Figure 5.8. We can compute the\ndistance between trees using `dist.multiPhylo` in the\n**[distory](https://cran.r-project.org/web/packages/distory/)** package.\n\nThe Jaccard index between graphs can be computed by looking at two graphs\nbuilt on the same nodes and counting the number of co-occurring edges. This is\nimplemented in the function `similarity` in the\n**[igraph](https://cran.r-project.org/web/packages/igraph/)** package.\nDistances and dissimilarities are also used to compare images, sounds, maps\nand documents. A distance can usefully encompass domain knowledge and, if\ncarefully chosen, can lead to the solution of many hard problems involving\nheterogeneous data. Asking yourself what is the _relevant_ notion of\n“closeness” or similarity for your data can provide useful ways of\nrepresenting them, as we will explore in [Chapter 9](09-chap.html).\n\n## 5.4 Nonparametric mixture detection\n\n### 5.4.1 \\\\(k\\\\)-methods: \\\\(k\\\\)-means, \\\\(k\\\\)-medoids and PAM\n\n[![The centers of the groups are sometimes called medoids, thus the name PAM\n\\(partitioning around medoids\\).](imgs/devil.png)](imgs/devil.png \"The centers\nof the groups are sometimes called medoids, thus the name PAM \\(partitioning\naround medoids\\).\")\n\nThe centers of the groups are sometimes called medoids, thus the name PAM\n(partitioning around medoids).\n\nPartitioning or iterative relocation methods work well in high-dimensional\nsettings, where we cannot4 easily use probability densities, the EM algorithm\nand parametric mixture modeling in the way we did in [Chapter\n4](04-chap.html). Besides the distance measure, the main choice to be made is\nthe number of clusters \\\\(k\\\\). The PAM (partitioning around medoids, Kaufman\nand Rousseeuw ([2009](16-chap.html#ref-Kaufman2009))) method is as follows:\n\n4 This is due to the so-called curse of dimensionality. We will discuss this\nin more detail in [Chapter 12](12-chap.html).\n\n  1. Starts from a matrix of \\\\(p\\\\) features measured on a set of \\\\(n\\\\) observations.\n\n  2. Randomly pick \\\\(k\\\\) distinct _cluster centers_ out of the \\\\(n\\\\) observations (“seeds”).\n\n  3. Assign each of the remaining observation to the group to whose center it is the closest.\n\n  4. For each group, choose a new center from the observations in the group, such that the sum of the distances of group members to the center is minimal; this is called the _medoid_.\n\n  5. Repeat Steps 3 and 4 until the groups stabilize.\n\nEach time the algorithm is run, different initial seeds will be picked in Step\n2, and in general, this can lead to different final results. A popular\nimplementation is the `pam` function in the package\n**[cluster](https://cran.r-project.org/web/packages/cluster/)**.\n\nA slight variation of the method replaces the medoids by the arithmetic means\n(centers of gravity) of the clusters and is called \\\\(k\\\\)-means. While in\nPAM, the centers are observations, this is not, in general, the case with\n\\\\(k\\\\)-means. The function `kmeans` comes with every installation of R in the\n**[stats](https://cran.r-project.org/web/packages/stats/)** package; an\nexample run is shown in Figure 5.9.\n\n[![](05-chap_files/figure-html/fig-clust-\nkmeansastep1-1.png)](05-chap_files/figure-html/fig-clust-kmeansastep1-1.png\n\"Figure 5.9 \\(a\\): \")\n\n(a)\n\n[![](05-chap_files/figure-html/fig-clust-\nkmeansastep1-2.png)](05-chap_files/figure-html/fig-clust-kmeansastep1-2.png\n\"Figure 5.9 \\(b\\): \")\n\n(b)\n\n[![](05-chap_files/figure-html/fig-clust-\nkmeansastep1-3.png)](05-chap_files/figure-html/fig-clust-kmeansastep1-3.png\n\"Figure 5.9 \\(c\\): \")\n\n(c)\n\nFigure 5.9: An example run of the \\\\(k\\\\)-means algorithm. The initial,\nrandomly chosen centers (black circles) and groups (colors) are shown in (a).\nThe group memberships are assigned based on their distance to centers. At each\niteration (b) and (c), the group centers are redefined, and the points\nreassigned to the cluster centers.\n\nThese so-called \\\\(k\\\\)-methods are the most common off-the-shelf methods for\nclustering; they work particularly well when the clusters are of comparable\nsize and convex (blob-shaped). On the other hand, if the true clusters are\nvery different in size, the larger ones will tend to be broken up; the same is\ntrue for groups that have pronounced non-spherical or non-elliptic shapes.\n\n__\n\nQuestion 5.3\n\nThe \\\\(k\\\\)-means algorithm alternates between computing the average point and\nassigning the points to clusters. How does this alternating, iterative method\ndiffer from an EM-algorithm?\n\n__\n\nSolution\n\n__\n\nIn the EM algorithm, each point participates in the computation of the mean of\nall the groups through a probabilistic weight assigned to it. In the\n\\\\(k\\\\)-means method, the points are either attributed to a cluster or not, so\neach point participates only, and entirely, in the computation of the center\nof one cluster.\n\n### 5.4.2 Tight clusters with resampling\n\nThere are clever schemes that repeat the process many times using different\ninitial centers or resampled datasets. Repeating a clustering procedure\nmultiple times on the same data, but with different starting points creates\n_strong forms_ according to Diday and Brito ([1989](16-chap.html#ref-\nDiday1989)). Repeated subsampling of the dataset and applying a clustering\nmethod will result in groups of observations that are “almost always” grouped\ntogether; these are called _tight clusters_ ([Tseng and Wong\n2005](16-chap.html#ref-Tseng:2005)). The study of strong forms or tight\nclusters facilitates the choice of the number of clusters. A recent package\ndeveloped to combine and compare the output from many different clusterings is\n**[clusterExperiment](https://bioconductor.org/packages/clusterExperiment/)**.\nHere we give an example from its vignette. Single-cell RNA-Seq experiments\nprovide counts of reads, representing gene transcripts, from individual cells.\nThe single cell resolution enables scientists, among other things, to follow\ncell lineage dynamics. Clustering has proved very useful for analysing such\ndata.\n\n__\n\nQuestion 5.4\n\nFollow the vignette of the package\n**[clusterExperiment](https://bioconductor.org/packages/clusterExperiment/)**.\nCall the ensemble clustering function `clusterMany`, using `pam` for the\nindividual clustering efforts. Set the choice of genes to include at either\nthe 60, 100 or 150 most variable genes. Plot the clustering results for\n\\\\(k\\\\) varying between 4 and 9. What do you notice?\n\n__\n\nSolution\n\n__\n\nThe following code produces Figure 5.10.\n\n    \n    \n    library(\"clusterExperiment\")\n    fluidigm = scRNAseq::ReprocessedFluidigmData()\n    se = fluidigm[, fluidigm$Coverage_Type == \"High\"]\n    assays(se) = list(normalized_counts = as.matrix(limma::normalizeQuantiles(assay(se))))\n    ce = clusterMany(se, clusterFunction = \"pam\", ks = c(5, 7, 9), run = TRUE,\n                     isCount = TRUE, reduceMethod = \"var\", nFilterDims = c(60, 100, 150))__\n    \n    \n    9 parameter combinations, 0 use sequential method, 0 use subsampling method\n    Running Clustering on Parameter Combinations...\n    done.\n    \n    \n    clusterLabels(ce) = sub(\"FilterDims\", \"\", clusterLabels(ce))\n    plotClusters(ce, whichClusters = \"workflow\", axisLine = -1)__\n\n[![](05-chap_files/figure-html/fig-quiltclust-1-1.png)](05-chap_files/figure-\nhtml/fig-quiltclust-1-1.png \"Figure 5.10: Comparison of clustering results\n\\(rows\\), for different numbers of included genes and for varying numbers of\nclusters, k. Each column of the heatmap corresponds to a cell, and the colors\nrepresent the cluster assignments.\")\n\nFigure 5.10: Comparison of clustering results (rows), for different numbers of\nincluded genes and for varying numbers of clusters, \\\\(k\\\\). Each column of\nthe heatmap corresponds to a cell, and the colors represent the cluster\nassignments.\n\n## 5.5 Clustering examples: flow cytometry and mass cytometry\n\n[![You can find reviews of bioinformatics methods for flow cytometry in\n\\(O’Neill et al. 2013\\) and a well-kept wikipedia\narticle.](imgs/book_icon.png)](imgs/book_icon.png \"You can find reviews of\nbioinformatics methods for flow cytometry in \\[@oneill2013flow\\] and a well-\nkept wikipedia article.\")\n\nYou can find reviews of bioinformatics methods for flow cytometry in ([O’Neill\net al. 2013](16-chap.html#ref-oneill2013flow)) and [a well-kept wikipedia\narticle](https://en.wikipedia.org/wiki/Flow_cytometry_bioinformatics).\n\nStudying measurements on single cells improves both the focus and resolution\nwith which we can analyze cell types and dynamics. Flow cytometry enables the\nsimultaneous measurement of about 10 different cell markers. Mass cytometry\nexpands the collection of measurements to as many as 80 proteins per cell. A\nparticularly promising application of this technology is the study of immune\ncell dynamics.\n\n### 5.5.1 Flow cytometry and mass cytometry\n\nAt different stages of their development, immune cells express unique\ncombinations of proteins on their surfaces. These protein-markers are called\n**CD** s (**clusters of differentiation**) and are collected by flow cytometry\n(using fluorescence, see Hulett et al. ([1969](16-chap.html#ref-flowsort))) or\nmass cytometry (using single-cell atomic mass spectrometry of heavy element\nreporters, see Bendall et al. ([2012](16-chap.html#ref-BendallCell))). An\nexample of a commonly used CD is CD4, this protein is expressed by helper T\ncells that are referred to as being “CD4+”. Note however that some cells\nexpress CD4 (thus are CD4+), but are not actually helper T cells. We start by\nloading some useful Bioconductor packages for cytometry data,\n**[flowCore](https://bioconductor.org/packages/flowCore/)** and\n**[flowViz](https://bioconductor.org/packages/flowViz/)** , and read in an\nexamplary data object `fcsB` as follows:\n\n    \n    \n    library(\"flowCore\")\n    library(\"flowViz\")\n    fcsB = read.FCS(\"../data/Bendall_2011.fcs\", truncate_max_range = FALSE)\n    slotNames(fcsB)__\n    \n    \n    [1] \"exprs\"       \"parameters\"  \"description\"\n\nFigure 5.11 shows a scatterplot of two of the variables available in the\n`fcsB` data. (We will see how to make such plots below.) We can see clear\nbimodality and clustering in these two dimensions.\n\n__\n\nQuestion 5.5\n\n  1. Look at the structure of the `fcsB` object (hint: the `colnames` function). How many variables were measured?  \n\n  2. Subset the data to look at the first few rows (hint: use `Biobase::exprs(fcsB)`). How many cells were measured?\n\n### 5.5.2 Data preprocessing\n\nFirst we load the table data that reports the mapping between isotopes and\nmarkers (antibodies), and then we replace the isotope names in the column\nnames of `fcsB` with the marker names. This makes the subsequent analysis and\nplotting code more readable:\n\n    \n    \n    markersB = readr::read_csv(\"../data/Bendall_2011_markers.csv\")\n    mt = match(markersB$isotope, colnames(fcsB))\n    stopifnot(!any(is.na(mt)))\n    colnames(fcsB)[mt] = markersB$marker __\n\nNow we are ready to generate Figure 5.11\n\n    \n    \n    flowPlot(fcsB, plotParameters = colnames(fcsB)[2:3], logy = TRUE)__\n\n[![](05-chap_files/figure-html/fig-\nObviousClusters-1.png)](05-chap_files/figure-html/fig-ObviousClusters-1.png\n\"Figure 5.11: Cell measurements that show clear clustering in two\ndimensions.\")\n\nFigure 5.11: Cell measurements that show clear clustering in two dimensions.\n\nPlotting the data in two dimensions as in Figure 5.11 already shows that the\ncells can be grouped into subpopulations. Sometimes just one of the markers\ncan be used to define populations on their own; in that case simple\n**rectangular gating** is used to separate the populations; for instance, CD4+\ncells can be gated by taking the subpopulation with high values for the CD4\nmarker. Cell clustering can be improved by carefully choosing transformations\nof the data. The left part of Figure 5.12 shows a simple one dimensional\nhistogram before transformation; on the right of Figure 5.12 we see the\ndistribution after transformation. It reveals a bimodality and the existence\nof two cell populations.\n\n**Data Transformation: hyperbolic arcsin (asinh)**. It is standard to\ntransform both flow and mass cytometry data using one of several special\nfunctions. We take the example of the inverse hyperbolic sine (asinh):\n\n\\\\[ \\operatorname{asinh}(x) = \\log{(x + \\sqrt{x^2 + 1})}. \\\\]\n\nFrom this we can see that for large values of \\\\(x\\\\),\n\\\\(\\operatorname{asinh}(x)\\\\) behaves like the logarithm and is practically\nequal to \\\\(\\log(x)+\\log(2)\\\\); for small \\\\(x\\\\) the function is close to\nlinear in \\\\(x\\\\).\n\n__\n\nTask\n\nTry running the following code to see the two main regimes of the\ntransformation: small values and large values.\n\n    \n    \n    v1 = seq(0, 1, length.out = 100)\n    plot(log(v1), asinh(v1), type = 'l')__\n    \n    \n     plot(v1, asinh(v1), type = 'l')__\n    \n    \n    v3 = seq(30, 3000, length = 100)\n    plot(log(v3), asinh(v3), type= 'l')__\n\nThis is another example of a variance stabilizing transformation, also\nmentioned in Chapters [4](04-chap.html) and [8](08-chap.html). Figure 5.12 is\nproduced by the following code, which uses the `arcsinhTransform` function in\nthe **[flowCore](https://bioconductor.org/packages/flowCore/)** package.\n\n    \n    \n    asinhtrsf = arcsinhTransform(a = 0.1, b = 1)\n    fcsBT = transform(fcsB, transformList(colnames(fcsB)[-c(1, 2, 41)], asinhtrsf))\n    densityplot(~`CD3all`, fcsB)\n    densityplot(~`CD3all`, fcsBT)__\n\n[![](05-chap_files/figure-html/fig-\nplotTransformations-1.png)](05-chap_files/figure-html/fig-\nplotTransformations-1.png \"Figure 5.12 \\(a\\): \")\n\n(a)\n\n[![](05-chap_files/figure-html/fig-\nplotTransformations-2.png)](05-chap_files/figure-html/fig-\nplotTransformations-2.png \"Figure 5.12 \\(b\\): \")\n\n(b)\n\nFigure 5.12: Panel (a) shows the histogram of the CD3all variable: the cells\nare clustered around 0 with a few large values. In (b), we see that after an\nasinh transformation, the cells cluster and fall into two groups or types.\n\n__\n\nQuestion 5.6\n\nHow many dimensions does the following code use to split the data into 2\ngroups using \\\\(k\\\\)-means ?\n\n    \n    \n    kf = kmeansFilter(\"CD3all\" = c(\"Pop1\",\"Pop2\"), filterId=\"myKmFilter\")\n    fres = flowCore::filter(fcsBT, kf)\n    summary(fres)__\n    \n    \n    Pop1: 33434 of 91392 events (36.58%)\n    Pop2: 57958 of 91392 events (63.42%)\n    \n    \n    fcsBT1 = flowCore::split(fcsBT, fres, population = \"Pop1\")\n    fcsBT2 = flowCore::split(fcsBT, fres, population = \"Pop2\")__\n\nFigure 5.13, generated by the following code, shows a naïve projection of the\ndata into the two dimensions spanned by the CD3 and CD56 markers:\n\n    \n    \n    library(\"flowPeaks\")\n    fp = flowPeaks(Biobase::exprs(fcsBT)[, c(\"CD3all\", \"CD56\")])\n    plot(fp)__\n\n[![](05-chap_files/figure-html/fig-flowCD3CD56-1-1.png)](05-chap_files/figure-\nhtml/fig-flowCD3CD56-1-1.png \"Figure 5.13: After transformation these cells\nwere clustered using kmeans.\")\n\nFigure 5.13: After transformation these cells were clustered using `kmeans`.\n\nWhen plotting points that densely populate an area we should try to avoid\noverplotting. We saw some of the preferred techniques in [Chapter\n3](03-chap.html); here we use contours and shading. This is done as follows:\n\n    \n    \n    flowPlot(fcsBT, plotParameters = c(\"CD3all\", \"CD56\"), logy = FALSE)\n    contour(fcsBT[, c(40, 19)], add = TRUE)__\n\n[![](05-chap_files/figure-html/fig-\ngroupcontourCD3CD56-1.png)](05-chap_files/figure-html/fig-\ngroupcontourCD3CD56-1.png \"Figure 5.14: Like Figure fig-flowCD3CD56-1, using\ncontours.\")\n\nFigure 5.14: Like Figure 5.13, using contours.\n\nThis produces Figure 5.14—a more informative version of Figure 5.13.\n\n__\n\nTask\n\nThe Bioconductor package\n**[ggcyto](https://bioconductor.org/packages/ggcyto/)** enables the plotting\nof each patient’s data in a different facet using `ggplot`. Try comparing the\noutput using this approach to what we did above, along the following lines:\n\n    \n    \n    library(\"ggcyto\")\n    library(\"labeling\")\n    \n    p1 = ggcyto(fcsB, aes(x = CD4)) + geom_histogram(bins = 60)\n    p2 = ggcyto(fcsB, aes(x = CD8)) + geom_histogram(bins = 60)\n    p3 = ggcyto(fcsB, aes(x = CD4, y = CD8)) + geom_density2d(colour = \"black\")\n    \n    fcsBT = transform(fcsB, transformList(colnames(fcsB)[-c(1, 2, 41)], \n                                          arcsinhTransform(a = 0, b = 1)))\n                                          \n    p1t = ggcyto(fcsBT, aes(x = CD4))            + geom_histogram(bins = 90)\n    p2t = ggcyto(fcsBT, aes(x = CD4,y = CD8))    + geom_density2d(colour = \"black\")\n    p3t = ggcyto(fcsBT, aes(x = CD45RA,y = CD20))+ geom_density2d(colour = \"black\")__\n\n### 5.5.3 Density-based clustering\n\nData sets such as flow cytometry, that contain only a few markers and a large\nnumber of cells, are amenable to density-based clustering. This method looks\nfor regions of high density separated by sparser regions. It has the advantage\nof being able to cope with clusters that are not necessarily convex. One\nimplementation of such a method is called dbscan. Let’s look at an example by\nrunning the following code.\n\n    \n    \n    library(\"dbscan\")\n    mc5 = Biobase::exprs(fcsBT)[, c(15,16,19,40,33)]\n    res5 = dbscan::dbscan(mc5, eps = 0.65, minPts = 30)\n    mc5df = data.frame(mc5, cluster = as.factor(res5$cluster))\n    table(mc5df$cluster)__\n    \n    \n        0     1     2     3     4     5     6     7     8 \n    76053  4031  5450  5310   257   160    63    25    43 \n    \n    \n    ggplot(mc5df, aes(x=CD4,    y=CD8,  col=cluster))+geom_density2d()\n    ggplot(mc5df, aes(x=CD3all, y=CD20, col=cluster))+geom_density2d()__\n\n[![](05-chap_files/figure-html/fig-dbscanfcs5-1.png)](05-chap_files/figure-\nhtml/fig-dbscanfcs5-1.png \"Figure 5.15 \\(a\\): \")\n\n(a)\n\n[![](05-chap_files/figure-html/fig-dbscanfcs5-2.png)](05-chap_files/figure-\nhtml/fig-dbscanfcs5-2.png \"Figure 5.15 \\(b\\): \")\n\n(b)\n\nFigure 5.15: These two plots show the results of clustering with `dbscan`\nusing five markers. Here we only show the projections of the data into the\nCD4-CD8 and C3all-CD20 planes.\n\nThe output is shown in Figure 5.15. The overlaps of the clusters in the 2D\nprojections enable us to appreciate the multidimensional nature of the\nclustering.\n\n__\n\nQuestion 5.7\n\nTry increasing the dimension to 6 by adding one CD marker-variables from the\ninput data.  \nThen vary `eps`, and try to find four clusters such that at least two of them\nhave more than 100 points.  \nRepeat this will 7 CD marker-variables, what do you notice?\n\n__\n\nSolution\n\n__\n\nAn example with the following 6 markers\n\n    \n    \n    mc6 = Biobase::exprs(fcsBT)[, c(15, 16, 19, 33, 25, 40)]\n    res = dbscan::dbscan(mc6, eps = 0.65, minPts = 20)\n    mc6df = data.frame(mc6, cluster = as.factor(res$cluster))\n    table(mc6df$cluster)__\n    \n    \n        0     1     2     3     4     5     6 \n    91068    34    61    20    67   121    21 \n\nWe see that with eps= 0.75 it is easier to find large enough clusters than if\nwe take eps=0.65, with eps=0.55 it is impossible. As we increase the\ndimensionality to 7, we have to make eps even larger.\n\n    \n    \n    mc7 = Biobase::exprs(fcsBT)[, c(11, 15, 16, 19, 25, 33, 40)]\n    res = dbscan::dbscan(mc7, eps = 0.95, minPts = 20)\n    mc7df = data.frame(mc7, cluster = as.factor(res$cluster))\n    table(mc7df$cluster)__\n    \n    \n        0     1     2     3     4     5     6     7     8     9    10 \n    90249    21   102   445   158   119    19   224    17    20    18 \n\nThis shows us the so-called **curse of dimensionality** in action, of which\nmore in [Chapter 12](12-chap.html).\n\n#### How does density-based clustering (dbscan) work ?\n\nThe dbscan method clusters points in dense regions according to the _density-\nconnectedness_ criterion. It looks at small neighborhood spheres of radius\n\\\\(\\epsilon\\\\) to see if points are connected.\n\nThe building block of dbscan is the concept of density-reachability: a point\n\\\\(q\\\\) is directly **density-reachable** from a point \\\\(p\\\\) if it is not\nfurther away than a given threshold \\\\(\\epsilon\\\\), and if \\\\(p\\\\) is\nsurrounded by sufficiently many points such that one may consider \\\\(p\\\\) (and\n\\\\(q\\\\)) be part of a dense region. We say that \\\\(q\\\\) is _density-reachable_\nfrom \\\\(p\\\\) if there is a sequence of points \\\\(p_1,...,p_n\\\\) with \\\\(p_1 =\np\\\\) and \\\\(p_n = q\\\\), so that each \\\\(p_{i + 1}\\\\) is directly density-\nreachable from \\\\(p_i\\\\).\n\n[![It is important that the method looks for high density of points in a\nneighborhood. Other methods exist that try to define clusters by a void, or\n“missing points” between clusters. But these are vulnerable to the curse of\ndimensionality; these can create spurious\n“voids”.](imgs/devil.png)](imgs/devil.png \"It is important that the method\nlooks for high density of points in a neighborhood. Other methods exist that\ntry to define clusters by a void, or “missing points” between clusters. But\nthese are vulnerable to the curse of dimensionality; these can create spurious\n“voids”.\")\n\nIt is important that the method looks for high density of points in a\nneighborhood. Other methods exist that try to define clusters by a void, or\n“missing points” between clusters. But these are vulnerable to the curse of\ndimensionality; these can create spurious “voids”.\n\nA _cluster_ is then a subset of points that satisfy the following properties:\n\n  1. All points within the cluster are mutually density-connected.\n\n  2. If a point is density-connected to any point of the cluster, it is part of the cluster as well.\n\n  3. Groups of points must have at least `MinPts` points to count as a cluster.\n\n## 5.6 Hierarchical clustering\n\n[![](imgs/LinnaeusClass-01.png)](imgs/LinnaeusClass-01.png \"Figure 5.16: A\nsnippet of Linnus’ taxonomy that clusters organisms according to feature\nsimilarities.\")\n\nFigure 5.16: A snippet of Linnus’ taxonomy that clusters organisms according\nto feature similarities.\n\nHierarchical clustering is a bottom-up approach, where similar observations\nand subclasses are assembled iteratively. Figure 5.16 shows how Linnæus made\nnested clusters of organisms according to specific characteristics. Such\nhierarchical organization has been useful in many fields and goes back to\nAristotle who postulated a _ladder of nature_.\n\n**Dendrogram ordering**. As you can see in the example of Figure 5.17, the\norder of the labels does not matter within sibling pairs. Horizontal distances\nare usually meaningless, while the vertical distances do encode some\ninformation. These properties are important to remember when making\ninterpretations about neighbors that are not monophyletic (i.e., not in the\nsame subtree or clade), but appear as neighbors in the plot (for instance B\nand D in the right hand tree are non-monophyletic neighbors).\n\n[![](imgs/SameTree-01.png)](imgs/SameTree-01.png \"Figure 5.17: Three\nrepresentations of the same hierarchical clustering tree.\")\n\nFigure 5.17: Three representations of the **same** hierarchical clustering\ntree.\n\n**Top-down hierarchies**. An alternative, top-down, approach takes all the\nobjects and splits them sequentially according to a chosen criterion. Such so-\ncalled **recursive partitioning** methods are often used to make decision\ntrees. They can be useful for prediction (say, survival time, given a medical\ndiagnosis): we are hoping in those instances to split heterogeneous\npopulations into more homogeneous subgroups by partitioning. In this chapter,\nwe concentrate on the bottom-up approaches. We will return to partitioning\nwhen we talk about supervised learning and classification in [Chapter\n12](12-chap.html).\n\n### 5.6.1 How to compute (dis)similarities between aggregated clusters?\n\n[![](imgs/ClusterStepChoiceSingle1b.png)](imgs/ClusterStepChoiceSingle1b.png\n\"Figure 5.18: In the single linkage method, the distance between groups C_1\nand C_2 is defined as the distance between the closest two points from the\ngroups.\")\n\nFigure 5.18: In the single linkage method, the distance between groups\n\\\\(C_1\\\\) and \\\\(C_2\\\\) is defined as the distance between the closest two\npoints from the groups.\n\nA hierarchical clustering algorithm, which works by aggregation, is easy\nenough to get started, by grouping the most similar observations together. But\nwe will need more than just the distances between all pairs of individual\nobjects. Once an aggregation is made, one is required to say how the distance\nbetween the newly formed cluster and all other points (or existing clusters)\nis computed. There are different choices, all based on the object-object\ndistances, and each choice results in a different type of hierarchical\nclustering.\n\nThe **minimal jump** method, also called **single linkage** or nearest\nneighbor method computes the distance between clusters as the smallest\ndistance between any two points in the two clusters (as shown in Figure 5.18):\n\n\\\\[ d_{12} = \\min_{i \\in C_1, j \\in C_2 } d_{ij}. \\\\]\n\nThis method tends to create clusters that look like contiguous strings of\npoints. The cluster tree often looks like a comb.\n\n[![](imgs/ClusterStepChoiceComplete1b.png)](imgs/ClusterStepChoiceComplete1b.png\n\"Figure 5.19: In the complete linkage method, the distance between groups C_1\nand C_2 is defined as the maximum distance between pairs of points from the\ntwo groups.\")\n\nFigure 5.19: In the complete linkage method, the distance between groups\n\\\\(C_1\\\\) and \\\\(C_2\\\\) is defined as the maximum distance between pairs of\npoints from the two groups.\n\nThe **maximum jump** (or **complete linkage**) method defines the distance\nbetween clusters as the largest distance between any two objects in the two\nclusters, as represented in Figure 5.19:\n\n\\\\[ d_{12} = \\max_{i \\in C_1, j \\in C_2 } d_{ij}. \\\\]\n\nThe **average linkage** method is half way between the two above (here,\n\\\\(|C_k|\\\\) denotes the number of elements of cluster \\\\(k\\\\)):\n\n\\\\[ d_{12} = \\frac{1}{|C_1| |C_2|}\\sum_{i \\in C_1, j \\in C_2 } d_{ij} \\\\]\n\n[![](imgs/BetweenWithinb.png)](imgs/BetweenWithinb.png \"Figure 5.20: The Ward\nmethod maximizes the between group sum of squares \\(red edges\\), while\nminimizing the sums of squares within groups \\(black edges\\).\")\n\nFigure 5.20: The Ward method maximizes the between group sum of squares (red\nedges), while minimizing the sums of squares within groups (black edges).\n\n**Ward’s method** takes an analysis of variance approach, where the goal is to\nminimize the variance within clusters. This method is very efficient, however,\nit tends to create clusters of smaller sizes.\n\nAdvantages and disadvantages of various choices of defining distances between aggregates ([Chakerian and Holmes 2012](16-chap.html#ref-distory-paper)). Method | Pros | Cons  \n---|---|---  \nSingle linkage | number of clusters | comblike trees  \nComplete linkage | compact classes | one observation can alter groups  \nAverage linkage | similar size and variance | not robust  \nCentroid | robust to outliers | smaller number of clusters  \nWard | minimising an inertia | classes small if high variability  \n  \n[![](imgs/CalderHand.png)](imgs/CalderHand.png \"Figure 5.21: Hierarchical\nclustering output has similar properties to a mobile: the branches can rotate\nfreely from their suspension points.\")\n\nFigure 5.21: Hierarchical clustering output has similar properties to a\nmobile: the branches can rotate freely from their suspension points.\n\nThese are the choices we have to make building hierarchical clustering trees.\nAn advantage of hierarchical clustering compared to the partitioning methods\nis that it offers a graphical diagnostic of the strength of groupings: the\nlength of the inner edges in the tree.\n\nWhen we have prior knowledge that the clusters are about the same size, using\naverage linkage or Ward’s method of minimizing the within class variance is\nthe best tactic.\n\n__\n\nQuestion 5.8\n\n**Hierarchical clustering for cell populations** The `Morder` data are gene\nexpression measurements for 156 genes on T cells of 3 types (naïve, effector,\nmemory) from 10 patients ([Holmes et al. 2005](16-chap.html#ref-\nholmes2005memory)). Using the\n**[pheatmap](https://cran.r-project.org/web/packages/pheatmap/)** package,\nmake two simple heatmaps, without dendogram or reordering, for Euclidean and\nManhattan distances of these data.\n\n__\n\nQuestion 5.9\n\nNow, look at the differences in orderings in the hierarchical clustering trees\nwith these two distances. What differences are noticeable?\n\n![](imgs/single14heatmap.png):\n\")\n\n(a)\n\n![](imgs/average14heatmap.png):\n\")\n\n(b)\n\n[![](imgs/complete14heatmap.png)](imgs/complete14heatmap.png\n\"Figure 5.22 \\(c\\): \")\n\n(c)\n\nFigure 5.22: Three hierarchical clustering plots made with different\nagglomeration choices. Note the comb-like structure for single linkage in (a).\nThe average (b) and complete linkage (c) trees only differ by the lengths of\ntheir inner branches.\n\n[![](imgs/apeclust14.png)](imgs/apeclust14.png \"Figure 5.23: This tree can be\ndrawn in many different ways. The ordering of the leaves as it is appears here\nis \\(8,11,9,10,7,5,6,1,4,2,3\\).\")\n\nFigure 5.23: This tree can be drawn in many different ways. The ordering of\nthe leaves as it is appears here is \\\\((8,11,9,10,7,5,6,1,4,2,3)\\\\).\n\n__\n\nQuestion 5.10\n\nA hierarchical clustering tree is like the Calder mobile in Figure 5.21 that\ncan swing around many internal pivot points, giving many orderings of the tips\nconsistent with a given tree. Look at the tree in Figure 5.23. How many ways\nare there of ordering the tip labels and still maintain consistence with that\ntree?\n\nIt is common to see heatmaps whose rows and/or columns are ordered based on a\nhierarchical clustering tree. Sometimes this makes some clusters look very\nstrong – stronger than what the tree really implies. There are alternative\nways of ordering the rows and columns in heatmaps, for instance, in the\npackage **[NeatMap](https://cran.r-project.org/web/packages/NeatMap/)** , that\nuses ordination methods5 to find orderings.\n\n5 These will be explained in [Chapter 9](09-chap.html).\n\n## 5.7 Validating and choosing the number of clusters\n\nThe clustering methods we have described are tailored to deliver good\ngroupings of the data under various constraints. However, keep in mind that\nclustering methods will always deliver groups, even if there are none. If, in\nfact, there are no real clusters in the data, a hierarchical clustering tree\nmay show relatively short inner branches; but it is difficult to quantify\nthis. In general it is important to validate your choice of clusters with more\nobjective criteria.\n\nOne criterion to assess the quality of a clustering result is to ask to what\nextent it maximizes the between group differences while keeping the within-\ngroup distances small (maximizing the lengths of red lines and minimizing\nthose of the black lines in Figure 5.20). We formalize this with the within-\ngroups sum of squared distances (WSS):\n\n\\\\[ \\text{WSS}_k=\\sum_{\\ell=1}^k \\sum_{x_i \\in C_\\ell} d^2(x_i,\n\\bar{x}_{\\ell}) \\tag{5.4}\\\\]\n\nHere, \\\\(k\\\\) is the number of clusters, \\\\(C_\\ell\\\\) is the set of objects in\nthe \\\\(\\ell\\\\)-th cluster, and \\\\(\\bar{x}_\\ell\\\\) is the center of mass (the\naverage point) in the \\\\(\\ell\\\\)-th cluster. We state the dependence on\n\\\\(k\\\\) of the WSS in Equation 5.4 as we are interested in comparing this\nquantity across different values of \\\\(k\\\\), for the same cluster algorithm.\nStated as it is, the WSS is however not a sufficient criterion: the smallest\nvalue of WSS would simply be obtained by making each point its own cluster.\nThe WSS is a useful building block, but we need more sophisticated ideas than\njust looking at this number alone.\n\nOne idea is to look at \\\\(\\text{WSS}_k\\\\) as a function of \\\\(k\\\\). This will\nalways be a decreasing function, but if there is a pronounced region where it\ndecreases sharply and then flattens out, we call this an _elbow_ and might\ntake this as a potential sweet spot for the number of clusters.\n\n__\n\nQuestion 5.11\n\n**An alternative expression for \\\\(\\text{WSS}_k\\\\)**. Use R to compute the sum\nof distances between all pairs of points in a cluster and compare it to\n\\\\(\\text{WSS}_k\\\\). Can you see how \\\\(\\text{WSS}_k\\\\) can also be written:\n\n\\\\[ \\text{WSS}_k=\\sum_{\\ell=1}^k \\frac{1}{2 n_\\ell} \\sum_{x_i \\in C_\\ell}\n\\sum_{x_j \\in C_\\ell} d^2(x_i,x_j), \\tag{5.5}\\\\]\n\nwhere \\\\(n_\\ell\\\\) is the size of the \\\\(\\ell\\\\)-th cluster.\n\nQuestion 5.11 shows us that the within-cluster sums of squares\n\\\\(\\text{WSS}_k\\\\) measures both the distances of all points in a cluster to\nits center, and the average distance between all pairs of points in the\ncluster.\n\nWhen looking at the behavior of various indices and statistics that help us\ndecide how many clusters are appropriate for the data, it can be useful to\nlook at cases where we actually know the right answer.\n\nTo start, we simulate data coming from four groups. We use the pipe (`%>%`)\noperator and the `bind_rows` function from\n**[dplyr](https://cran.r-project.org/web/packages/dplyr/)** to concatenate the\nfour _tibble_ s corresponding to each cluster into one big _tibble_.6\n\n6 The pipe operator passes the value to its left into the function to its\nright. This can make the flow of data easier to follow in code: `f(x) %>%\ng(y)` is equivalent to `g(f(x), y)`.\n\n    \n    \n    library(\"dplyr\")\n    simdat = lapply(c(0, 8), function(mx) {\n      lapply(c(0,8), function(my) {\n        tibble(x = rnorm(100, mean = mx, sd = 2),\n               y = rnorm(100, mean = my, sd = 2),\n               class = paste(mx, my, sep = \":\"))\n       }) %>% bind_rows\n    }) %>% bind_rows\n    simdat __\n    \n    \n    # A tibble: 400 × 3\n            x      y class\n        <dbl>  <dbl> <chr>\n     1 -2.42  -4.59  0:0  \n     2  1.89  -1.56  0:0  \n     3  0.558  2.17  0:0  \n     4  2.51  -0.873 0:0  \n     5 -2.52  -0.766 0:0  \n     6  3.62   0.953 0:0  \n     7  0.774  2.43  0:0  \n     8 -1.71  -2.63  0:0  \n     9  2.01   1.28  0:0  \n    10  2.03  -1.25  0:0  \n    # ℹ 390 more rows\n    \n    \n    simdatxy = simdat[, c(\"x\", \"y\")] # without class label __\n    \n    \n    ggplot(simdat, aes(x = x, y = y, col = class)) + geom_point() +\n      coord_fixed()__\n\n[![](05-chap_files/figure-html/fig-simdat-1-1.png)](05-chap_files/figure-\nhtml/fig-simdat-1-1.png \"Figure 5.24: The simdat data colored by the class\nlabels. Here, we know the labels since we generated the data – usually we do\nnot know them.\")\n\nFigure 5.24: The `simdat` data colored by the class labels. Here, we know the\nlabels since we generated the data – usually we do not know them.\n\nWe compute the within-groups sum of squares for the clusters obtained from the\n\\\\(k\\\\)-means method:\n\n    \n    \n    wss = tibble(k = 1:8, value = NA_real_)\n    wss$value[1] = sum(scale(simdatxy, scale = FALSE)^2)\n    for (i in 2:nrow(wss)) {\n      km  = kmeans(simdatxy, centers = wss$k[i])\n      wss$value[i] = sum(km$withinss)\n    }\n    ggplot(wss, aes(x = k, y = value)) + geom_col()__\n\n[![](05-chap_files/figure-html/fig-WSS-1.png)](05-chap_files/figure-html/fig-\nWSS-1.png \"Figure 5.25: The barchart of the WSS statistic as a function of k\nshows that the last substantial jump is just before k=4. This indicates that\nthe best choice for these data is k=4.\")\n\nFigure 5.25: The barchart of the WSS statistic as a function of \\\\(k\\\\) shows\nthat the last substantial jump is just before \\\\(k=4\\\\). This indicates that\nthe best choice for these data is \\\\(k=4\\\\).\n\n__\n\nQuestion 5.12\n\n  1. Run the code above several times and compare the `wss` values for different runs. Why are they different?  \n\n  2. Create a set of data with uniform instead of normal distributions with the same range and dimensions as `simdat`. Compute the WSS values for for thess data. What do you conclude?\n\n__\n\nQuestion 5.13\n\nThe so-called **Calinski-Harabasz** index uses the WSS and BSS (between group\nsums of squares). It is inspired by the \\\\(F\\\\) statistic — the ratio of the\nmean sum of squares explained by a factor to the mean residual sum of squares\n— used in analysis of variance:\n\n\\\\[ \\text{CH}(k)=\\frac{\\text{BSS}_k}{\\text{WSS}_k}\\times\\frac{N-k}{N-1} \\qquad\n\\text{where} \\quad \\text{BSS}_k = \\sum_{\\ell=1}^k\nn_\\ell(\\bar{x}_{\\ell}-\\bar{x})^2, \\\\]\n\nwhere \\\\(\\bar{x}\\\\) is the overall center of mass (average point). Plot the\nCalinski-Harabasz index for the `simdat` data.\n\n__\n\nSolution\n\n__\n\nHere is the code to generate Figure 5.26.\n\n    \n    \n    library(\"fpc\")\n    library(\"cluster\")\n    CH = tibble(\n      k = 2:8,\n      value = sapply(k, function(i) {\n        p = pam(simdatxy, i)\n        calinhara(simdatxy, p$cluster)\n      })\n    )\n    ggplot(CH, aes(x = k, y = value)) + geom_line() + geom_point() +\n      ylab(\"CH index\")__\n\n[![](05-chap_files/figure-html/fig-CHIndex-1-1.png)](05-chap_files/figure-\nhtml/fig-CHIndex-1-1.png \"Figure 5.26: The Calinski-Harabasz index, i.,e., the\nratio of the between and within group variances for different choices of k,\ncomputed on the simdat data.\")\n\nFigure 5.26: The Calinski-Harabasz index, i.,e., the ratio of the between and\nwithin group variances for different choices of \\\\(k\\\\), computed on the\n`simdat` data.\n\n### 5.7.1 Using the gap statistic\n\nTaking the logarithm of the within-sum-of-squares (\\\\(\\log(\\text{WSS}_k)\\\\))\nand comparing it to averages from simulated data with less structure can be a\ngood way of choosing \\\\(k\\\\). This is the basic idea of the **gap statistic**\nintroduced by Tibshirani, Walther, and Hastie ([2001](16-chap.html#ref-\ngap2001)). We compute \\\\(\\log(\\text{WSS}_k)\\\\) for a range of values of\n\\\\(k\\\\), the number of clusters, and compare it to that obtained on reference\ndata of similar dimensions with various possible ‘non-clustered’\ndistributions. We can use uniformly distributed data as we did above or data\nsimulated with the same covariance structure as our original data.\n\n![](imgs/roulette.png)\n\nThis algorithm is a Monte Carlo method that compares the gap statistic for the\nobserved data to an average over simulations of data with similar structure.\n\n**Algorithm for computing the gap statistic([Tibshirani, Walther, and Hastie\n2001](16-chap.html#ref-gap2001)):**\n\n  1. Cluster the data with \\\\(k\\\\) clusters and compute \\\\(\\text{WSS}_k\\\\) for the various choices of \\\\(k\\\\).\n\n  2. Generate \\\\(B\\\\) plausible reference data sets, using Monte Carlo sampling from a homogeneous distribution and redo Step 1 above for these new simulated data. This results in \\\\(B\\\\) new within-sum-of-squares for simulated data \\\\(W_{kb}^*\\\\), for \\\\(b=1,...,B\\\\).\n\n  3. Compute the \\\\(\\text{gap}(k)\\\\)-statistic:\n\n\\\\[ \\text{gap}(k) = \\overline{l}_k - \\log \\text{WSS}_k \\quad\\text{with}\\quad\n\\overline{l}_k =\\frac{1}{B}\\sum_{b=1}^B \\log W^*_{kb} \\\\]\n\nNote that the first term is expected to be bigger than the second one if the\nclustering is good (i.e., the WSS is smaller); thus the gap statistic will be\nmostly positive and we are looking for its highest value.\n\n  4. We can use the standard deviation\n\n\\\\[ \\text{sd}_k^2 =\n\\frac{1}{B-1}\\sum_{b=1}^B\\left(\\log(W^*_{kb})-\\overline{l}_k\\right)^2 \\\\]\n\nto help choose the best \\\\(k\\\\). Several choices are available, for instance,\nto choose the smallest \\\\(k\\\\) such that\n\n\\\\[ \\text{gap}(k) \\geq \\text{gap}(k+1) - s'_{k+1}\\qquad \\text{where }\ns'_{k+1}=\\text{sd}_{k+1}\\sqrt{1+1/B}. \\\\]\n\nThe packages **[cluster](https://cran.r-project.org/web/packages/cluster/)**\nand **[clusterCrit](https://cran.r-project.org/web/packages/clusterCrit/)**\nprovide implementations.\n\n__\n\nQuestion 5.14\n\nMake a function that plots the gap statistic as in Figure 5.27. Show the\noutput for the `simdat` example dataset clustered with the `pam` function.\n\n__\n\nSolution\n\n__\n\n    \n    \n    library(\"cluster\")\n    library(\"ggplot2\")\n    pamfun = function(x, k)\n      list(cluster = pam(x, k, cluster.only = TRUE))\n    \n    gss = clusGap(simdatxy, FUN = pamfun, K.max = 8, B = 50,\n                  verbose = FALSE)\n    plot_gap = function(x) {\n      gstab = data.frame(x$Tab, k = seq_len(nrow(x$Tab)))\n      ggplot(gstab, aes(k, gap)) + geom_line() +\n        geom_errorbar(aes(ymax = gap + SE.sim,\n                          ymin = gap - SE.sim), width=0.1) +\n        geom_point(size = 3, col=  \"red\")\n    }\n    plot_gap(gss)__\n\n[![](05-chap_files/figure-html/fig-GapStat-1-1.png)](05-chap_files/figure-\nhtml/fig-GapStat-1-1.png \"Figure 5.27: The gap statistic, see Question wrn-\nclustering-gapstat.\")\n\nFigure 5.27: The gap statistic, see Question 5.14.\n\nLet’s now use the method on a real example. We load the\n**[Hiiragi](https://bioconductor.org/packages/Hiiragi/)** data that we already\nexplored in [Chapter 3](03-chap.html) and will see how the cells cluster.\n\n    \n    \n    library(\"Hiiragi2013\")__\n    \n    \n    In chunk 'Hiiragi': Warning: replacing previous import 'boot::logit' by 'gtools::logit' whenloading 'Hiiragi2013'\n    \n    \n    In chunk 'Hiiragi': Warning: replacing previous import 'boot::inv.logit' by 'gtools::inv.logit'when loading 'Hiiragi2013'\n    \n    \n    data(\"x\")__\n\nWe start by choosing the 50 most variable genes (features)7.\n\n7 The intention behind this step is to reduce the influence of technical (or\nbatch) effects. Although individually small, when accumulated over all the\n45101 features in `x`, many of which match genes that are weakly or not\nexpressed, without this feature selection step, such effects are prone to\nsuppress the biological signal.\n\n    \n    \n    selFeats = order(rowVars(Biobase::exprs(x)), decreasing = TRUE)[1:50]\n    embmat = t(Biobase::exprs(x)[selFeats, ])\n    embgap = clusGap(embmat, FUN = pamfun, K.max = 24, verbose = FALSE)\n    k1 = maxSE(embgap$Tab[, \"gap\"], embgap$Tab[, \"SE.sim\"])\n    k2 = maxSE(embgap$Tab[, \"gap\"], embgap$Tab[, \"SE.sim\"],\n               method = \"Tibs2001SEmax\")\n    c(k1, k2)__\n    \n    \n    [1] 9 7\n\nThe default choice for the number of clusters, `k1`, is the first value of\n\\\\(k\\\\) for which the gap is not larger than the first local maximum minus a\nstandard error \\\\(s\\\\) (see the manual page of the `clusGap` function). This\ngives a number of clusters \\\\(k = 9\\\\), whereas the choice recommended by\nTibshirani, Walther, and Hastie ([2001](16-chap.html#ref-gap2001)) is the\nsmallest \\\\(k\\\\) such that \\\\(\\text{gap}(k) \\geq \\text{gap}(k+1) -\ns_{k+1}'\\\\), this gives \\\\(k = 7\\\\). Let’s plot the gap statistic (Figure\n5.28).\n\n    \n    \n    plot(embgap, main = \"\")\n    cl = pamfun(embmat, k = k1)$cluster\n    table(pData(x)[names(cl), \"sampleGroup\"], cl)__\n    \n    \n                     cl\n                       1  2  3  4  5  6  7  8  9\n      E3.25           23 11  1  1  0  0  0  0  0\n      E3.25 (FGF4-KO)  0  0  1 16  0  0  0  0  0\n      E3.5 (EPI)       2  1  0  0  0  8  0  0  0\n      E3.5 (FGF4-KO)   0  0  8  0  0  0  0  0  0\n      E3.5 (PE)        0  0  0  0  9  2  0  0  0\n      E4.5 (EPI)       0  0  0  0  0  0  0  4  0\n      E4.5 (FGF4-KO)   0  0  0  0  0  0  0  0 10\n      E4.5 (PE)        0  0  0  0  0  0  4  0  0\n\n[![](05-chap_files/figure-html/fig-gapHiiragi-1-1.png)](05-chap_files/figure-\nhtml/fig-gapHiiragi-1-1.png \"Figure 5.28: The gap statistic for the\nHiiragi2013 data.\")\n\nFigure 5.28: The gap statistic for the\n**[Hiiragi2013](https://bioconductor.org/packages/Hiiragi2013/)** data.\n\nAbove we see the comparison between the clustering that we got from `pamfun`\nwith the sample labels in the annotation of the data.\n\n__\n\nQuestion 5.15\n\nHow do the results change if you use all the features in `x`, rather than\nsubsetting the top 50 most variable genes?\n\n### 5.7.2 Cluster validation using the bootstrap\n\n[![](imgs/BootstrapClusterNew.png)](imgs/BootstrapClusterNew.png\n\"Figure 5.29 \\(a\\): \")\n\n(a)\n\n[![](imgs/BootstrapCluster2New.png)](imgs/BootstrapCluster2New.png\n\"Figure 5.29 \\(b\\): \")\n\n(b)\n\nFigure 5.29: Different samples from the same distribution \\\\(F\\\\) lead to\ndifferent clusterings. In (a), we see the true sampling variability. The\nbootstrap simulates this sampling variability by drawing subsamples using the\nempirical distribution function \\\\(\\hat{F}_n\\\\) as shown in (b).\n\nWe saw the bootstrap principle in [Chapter 4](04-chap.html): ideally, we would\nlike to use many new samples (sets of data) from the underlying data\ngenerating process, for each of them apply our clustering method, and then see\nhow stable the clusterings are, or how much they change, using an index such\nas those we used above to compare clusterings. Of course, we don’t have these\nadditional samples. So we are, in fact, going to create new datasets simply by\ntaking different random subsamples of the data, look at the different\nclusterings we get each time, and compare them. Tibshirani, Walther, and\nHastie ([2001](16-chap.html#ref-gap2001)) recommend using bootstrap resampling\nto infer the number of clusters using the gap statistic.\n\nWe will continue using the\n**[Hiiragi2013](https://bioconductor.org/packages/Hiiragi2013/)** data. Here\nwe follow the investigation of the hypothesis that the inner cell mass (ICM)\nof the mouse blastocyst in embyronic day 3.5 (E3.5) falls “naturally” into two\nclusters corresponding to pluripotent epiblast (EPI) versus primitive endoderm\n(PE), while the data for embryonic day 3.25 (E3.25) do not yet show this\nsymmetry breaking.\n\nWe will not use the true group labels in our clustering and only use them in\nthe final interpretation of the results. We will apply the bootstrap to the\ntwo different data sets (E3.5) and (E3.25) separately. Each step of the\nbootstrap will generate a clustering of a random subset of the data and we\nwill need to compare these through a consensus of an ensemble of clusters.\nThere is a useful framework for this in the\n**[clue](https://cran.r-project.org/web/packages/clue/)** package ([Hornik\n2005](16-chap.html#ref-Hornik2005)). The function `clusterResampling`, taken\nfrom the supplement of Ohnishi et al. ([2014](16-chap.html#ref-Ohnishi2014)),\nimplements this approach:\n\n    \n    \n    clusterResampling = function(x, ngenes = 50, k = 2, B = 250,\n                                 prob = 0.67) {\n      mat = Biobase::exprs(x)\n      ce = cl_ensemble(list = lapply(seq_len(B), function(b) {\n        selSamps = sample(ncol(mat), size = round(prob * ncol(mat)),\n                          replace = FALSE)\n        submat = mat[, selSamps, drop = FALSE]\n        sel = order(rowVars(submat), decreasing = TRUE)[seq_len(ngenes)]\n        submat = submat[sel,, drop = FALSE]\n        pamres = pam(t(submat), k = k)\n        pred = cl_predict(pamres, t(mat[sel, ]), \"memberships\")\n        as.cl_partition(pred)\n      }))\n      cons = cl_consensus(ce)\n      ag = sapply(ce, cl_agreement, y = cons)\n      list(agreements = ag, consensus = cons)\n    }__\n\nThe function `clusterResampling` performs the following steps:\n\n  1. Draw a random subset of the data (the data are either all E3.25 or all E3.5 samples) by selecting 67% of the samples without replacement.\n\n  2. Select the top `ngenes` features by overall variance (in the subset).\n\n  3. Apply \\\\(k\\\\)-means clustering and predict the cluster memberships of the samples that were not in the subset with the `cl_predict` method from the **[clue](https://cran.r-project.org/web/packages/clue/)** package, through their proximity to the cluster centres.\n\n  4. Repeat steps 1-3 `B` times.\n\n  5. Apply consensus clustering (`cl_consensus`).\n\n  6. For each of the `B` clusterings, measure the agreement with the consensus through the function(`cl_agreement`). Here a good agreement is indicated by a value of 1, and less agreement by smaller values. If the agreement is generally high, then the clustering into \\\\(k\\\\) classes can be considered stable and reproducible; inversely, if it is low, then no stable partition of the samples into \\\\(k\\\\) clusters is evident.\n\nAs a measure of between-cluster distance for the consensus clustering, the\n_Euclidean_ dissimilarity of the memberships is used, i.e., the square root of\nthe minimal sum of the squared differences of \\\\(\\mathbf{u}\\\\) and all column\npermutations of \\\\(\\mathbf{v}\\\\), where \\\\(\\mathbf{u}\\\\) and \\\\(\\mathbf{v}\\\\)\nare the cluster membership matrices. As agreement measure for Step\n[clueagree], the quantity \\\\(1 - d/m\\\\) is used, where \\\\(d\\\\) is the\nEuclidean dissimilarity, and \\\\(m\\\\) is an upper bound for the maximal\nEuclidean dissimilarity.\n\n    \n    \n    iswt = (x$genotype == \"WT\")\n    cr1 = clusterResampling(x[, x$Embryonic.day == \"E3.25\" & iswt])\n    cr2 = clusterResampling(x[, x$Embryonic.day == \"E3.5\"  & iswt])__\n\nThe results are shown in Figure 5.30. They confirm the hypothesis that the\nE.35 data fall into two clusters.\n\n    \n    \n    ag1 = tibble(agreements = cr1$agreements, day = \"E3.25\")\n    ag2 = tibble(agreements = cr2$agreements, day = \"E3.5\")\n    p1 <- ggplot(bind_rows(ag1, ag2), aes(x = day, y = agreements)) +\n      geom_boxplot() +\n      ggbeeswarm::geom_beeswarm(cex = 1.5, col = \"#0000ff40\")\n    mem1 = tibble(y = sort(cl_membership(cr1$consensus)[, 1]),\n                  x = seq(along = y), day = \"E3.25\")\n    mem2 = tibble(y = sort(cl_membership(cr2$consensus)[, 1]),\n                  x = seq(along = y), day = \"E3.5\")\n    p2 <- ggplot(bind_rows(mem1, mem2), aes(x = x, y = y, col = day)) +\n      geom_point() + facet_grid(~ day, scales = \"free_x\")\n    gridExtra::grid.arrange(p1, p2, widths = c(2.4,4.0))__\n\n[![](05-chap_files/figure-html/fig-figClue1-1.png)](05-chap_files/figure-\nhtml/fig-figClue1-1.png \"Figure 5.30: Cluster stability analysis with E3.25\nand E3.5 samples. Left: beeswarm plots of the cluster agreements with the\nconsensus, for the B clusterings; 1 indicates perfect agreement, lower values\nindicate lower degrees of agreement. Right: membership probabilities of the\nconsensus clustering. For E3.25, the probabilities are diffuse, indicating\nthat the individual clusterings often disagree, whereas for E3.5, the\ndistribution is bimodal, with only one ambiguous sample.\")\n\nFigure 5.30: Cluster stability analysis with E3.25 and E3.5 samples. Left:\nbeeswarm plots of the cluster agreements with the consensus, for the `B`\nclusterings; \\\\(1\\\\) indicates perfect agreement, lower values indicate lower\ndegrees of agreement. Right: membership probabilities of the consensus\nclustering. For E3.25, the probabilities are diffuse, indicating that the\nindividual clusterings often disagree, whereas for E3.5, the distribution is\nbimodal, with only one ambiguous sample.\n\n#### Computational and memory Issues\n\n[![Computational complexity. An algorithm is said to be O\\(n^k\\), if, as n\ngets larger, the resource consumption \\(CPU time or memory\\) grows\nproportionally to n^k. There may be other \\(sometimes considerable\\) baseline\ncosts, or costs that grow proportionally to lower powers of n, but these\nalways become negligible compared to the leading term as\nn\\\\to\\\\infty.](imgs/devil.png)](imgs/devil.png \"Computational complexity. An\nalgorithm is said to be O\\(n^k\\), if, as n gets larger, the resource\nconsumption \\(CPU time or memory\\) grows proportionally to n^k. There may be\nother \\(sometimes considerable\\) baseline costs, or costs that grow\nproportionally to lower powers of n, but these always become negligible\ncompared to the leading term as n\\\\to\\\\infty.\")\n\n**Computational complexity**. An algorithm is said to be \\\\(O(n^k)\\\\), if, as\n\\\\(n\\\\) gets larger, the resource consumption (CPU time or memory) grows\nproportionally to \\\\(n^k\\\\). There may be other (sometimes considerable)\nbaseline costs, or costs that grow proportionally to lower powers of \\\\(n\\\\),\nbut these always become negligible compared to the leading term as\n\\\\(n\\to\\infty\\\\).\n\nIt is important to remember that the computation of all versus all distances\nof \\\\(n\\\\) objects is an \\\\(O(n^2)\\\\) operation (in time and memory). Classic\nhierarchical clustering approaches (such as `hclust` in the\n**[stats](https://cran.r-project.org/web/packages/stats/)** package) are even\n\\\\(O(n^3)\\\\) in time. For large \\\\(n\\\\), this may become impractical8. We can\navoid the complete computation of the all-vs-all distance matrix. For\ninstance, \\\\(k\\\\)-means has the advantage of only requiring \\\\(O(n)\\\\)\ncomputations, since it only keeps track of the distances between each object\nand the cluster centers, whose number remains the same even if \\\\(n\\\\)\nincreases.\n\n8 E.g., the distance matrix for one million objects, stored as 8-byte floating\npoint numbers, would take up about 4 Terabytes, and an `hclust`-like algorithm\nwould run 30 years even under the optimistic assumption that each of the\niterations only takes a nanosecond.\n\nFast implementations such as\n**[fastclust](https://cran.r-project.org/web/packages/fastclust/)** ([Müllner\n2013](16-chap.html#ref-Mullner:2013)) and\n**[dbscan](https://cran.r-project.org/web/packages/dbscan/)** have been\ncarefully optimized to deal with a large number of observations.\n\n## 5.8 Clustering as a means for denoising\n\nConsider a set of measurements that reflect some underlying true values (say,\nspecies represented by DNA sequences from their genomes), but have been\ndegraded by technical noise. Clustering can be used to remove such noise.\n\n### 5.8.1 Noisy observations with different baseline frequencies\n\nSuppose that we have a bivariate distribution of observations made with the\nsame error variances. However, the sampling is from two groups that have very\ndifferent baseline frequencies. Suppose, further, that the errors are\ncontinuous independent bivariate normally distributed. We have \\\\(10^{3}\\\\) of\n`seq1` and \\\\(10^{5}\\\\) of `seq2`, as generated for instance by the code:\n\n    \n    \n    library(\"mixtools\")\n    seq1 = rmvnorm(n = 1e3, mu = -c(1, 1), sigma = 0.5 * diag(c(1, 1)))\n    seq2 = rmvnorm(n = 1e5, mu =  c(1, 1), sigma = 0.5 * diag(c(1, 1)))\n    twogr = data.frame(\n      rbind(seq1, seq2),\n      seq = factor(c(rep(1, nrow(seq1)),\n                     rep(2, nrow(seq2))))\n    )\n    colnames(twogr)[1:2] = c(\"x\", \"y\")\n    library(\"ggplot2\")\n    ggplot(twogr, aes(x = x, y = y, colour = seq,fill = seq)) +\n      geom_hex(alpha = 0.5, bins = 50) + coord_fixed()__\n\n[![](05-chap_files/figure-html/fig-seqradius-1.png)](05-chap_files/figure-\nhtml/fig-seqradius-1.png \"Figure 5.31: Although both groups have noise\ndistributions with the same variances, the apparent radii of the groups are\nvery different. The 10^{5} instances in seq2 have many more opportunities for\nerrors than what we see in seq1, of which there are only 10^{3}. Thus we see\nthat frequencies are important in clustering the data.\")\n\nFigure 5.31: Although both groups have noise distributions with the same\nvariances, the apparent radii of the groups are very different. The\n\\\\(10^{5}\\\\) instances in `seq2` have many more opportunities for errors than\nwhat we see in `seq1`, of which there are only \\\\(10^{3}\\\\). Thus we see that\nfrequencies are important in clustering the data.\n\nThe observed values would look as in Figure 5.31.\n\n__\n\nQuestion 5.16\n\nTake the data `seq1` and `seq2` and cluster them into two groups according to\ndistance from group center. Do you think the results should depend on the\nfrequencies of each of the two sequence types?\n\n__\n\nSolution\n\n__\n\nSuch an approach, often used in taxonomic clustering, also called OTU\n-operational taxonomic unit clustering ([Caporaso et al.\n2010](16-chap.html#ref-caporaso2010qiime); [P. D. Schloss et al.\n2009](16-chap.html#ref-mothur))) methods is sub-optimal.\n\nThe methods based solely on similarities suffer from the biases inherent in\nthe _representativeness_ heuristic. Let’s make a brief digression into the\nworld of cognitive psychology that helps explain how our natural inclination\nto use only representativeness and a distance-based heuristic in clustering\nand taxonomic assignment can lead to biased results.\n\nIn the 1970s, Tversky and Kahneman ([1975](16-chap.html#ref-\ntversky1975judgment)) pointed out that we generally assign groups by looking\nat the most similar _representatives_. In clustering and group assignments\nthat would mean assigning a new sequence to the group according to the\ndistance to its center. In fact this is equivalent to taking balls with the\nsame radius regardless of the differences in prevalence of the different\ngroups. This psychological error was first discussed in an important Science\npaper that covers many different heuristics and biases([Tversky and Kahneman\n1974](16-chap.html#ref-tversky1974heuristics)).\n\n[![See Kahneman \\(2011\\) for a book-length treatment of our natural heuristics\nand the ways in which they can mislead us when we make probability\ncalculations \\(we recommend especially Chapters 14 and\n15\\).](imgs/book_icon.png)](imgs/book_icon.png \"See @kahneman2011 for a book-\nlength treatment of our natural heuristics and the ways in which they can\nmislead us when we make probability calculations \\(we recommend especially\nChapters 14 and 15\\).\")\n\nSee Kahneman ([2011](16-chap.html#ref-kahneman2011)) for a book-length\ntreatment of our natural heuristics and the ways in which they can mislead us\nwhen we make probability calculations (we recommend especially Chapters 14 and\n15).\n\n__\n\nTask\n\nSimulate `n=2000` binary variables of length `len=200` that indicate the\nquality of `n` sequencing reads of length `len`. For simplicity, let us assume\nthat sequencing errors occur independently and uniformly with probability\n`perr=0.001`. That is, we only care whether a base was called correctly\n(`TRUE`) or not (`FALSE`).\n\n    \n    \n    n    = 2000\n    len  = 200\n    perr = 0.001\n    seqs = matrix(runif(n * len) >= perr, nrow = n, ncol = len)__\n\nNow, compute all pairwise distances between reads.\n\n    \n    \n    dists = as.matrix(dist(seqs, method = \"manhattan\"))__\n\nFor various values of number of reads `k` (from 2 to `n`), the maximum\ndistance within this set of reads is computed by the code below and shown in\nFigure 5.32.\n\n    \n    \n    library(\"tibble\")\n    dfseqs = tibble(\n      k = 10 ^ seq(log10(2), log10(n), length.out = 20),\n      diameter = vapply(k, function(i) {\n        s = sample(n, i)\n        max(dists[s, s])\n        }, numeric(1)))\n    ggplot(dfseqs, aes(x = k, y = diameter)) + geom_point()+geom_smooth()__\n\n[![](05-chap_files/figure-html/fig-diameter-1.png)](05-chap_files/figure-\nhtml/fig-diameter-1.png \"Figure 5.32: The diameter of a set of sequences as a\nfunction of the number of sequences.\")\n\nFigure 5.32: The diameter of a set of sequences as a function of the number of\nsequences.\n\nWe will now improve the 16SrRNA-read clustering using a denoising mechanism\nthat incorporates error probabilities.\n\n### 5.8.2 Denoising 16S rRNA sequences\n\n**What are the data?** In the bacterial 16SrRNA gene there are so-called\n**variable regions** that are taxa-specific. These provide fingerprints that\nenables _taxon_ 9 identification. The raw data are FASTQ-files with quality\nscored sequences of PCR-amplified DNA regions10. We use an iterative\nalternating approach11 to build a probabilistic noise model from the data. We\ncall this a _de novo_ method, because we use clustering, and we use the\ncluster centers as our denoised sequence variants (a.k.a. Amplicon Sequence\nVariants, ASVs, see ([Benjamin J. Callahan, McMurdie, and Holmes\n2017](16-chap.html#ref-Callahan:2017))). After finding all the denoised\nvariants, we create contingency tables of their counts across the different\nsamples. We will show in [Chapter 10](10-chap.html) how these tables can be\nused to infer properties of the underlying bacterial communities using\nnetworks and graphs.\n\n9 Calling different groups of bacteria _taxa_ rather than _species_ highlights\nthe approximate nature of the concept, as the notion of species is more fluid\nin bacteria than, say, in animals.\n\n10 The [FASTQ format is described\nhere](https://en.wikipedia.org/wiki/FASTQ_format).\n\n11 Similar to the EM algorithm we saw in [Chapter 4](04-chap.html).\n\nIn order to improve data quality, one often has to start with the raw data and\nmodel all the sources of variation carefully. One can think of this as an\nexample of _cooking from scratch_ (see the gruesome details in Ben J. Callahan\net al. ([2016](16-chap.html#ref-Callahan2016Bioc)) and Exercise 5.5).\n\n__\n\nQuestion 5.17\n\nSuppose that we have two sequences of length 200 (`seq1` and `seq2`) present\nin our sample at very different abundances. We are told that the technological\nsequencing errors occur as independent Bernoulli(0.0005) random events for\neach nucleotide.  \nWhat is the distribution of the number of errors per sequence?\n\n__\n\nSolution\n\n__\n\nProbability theory tells us that the sum of 200 independent Poisson(0.0005)\nwill be Poisson(0.1).\n\nWe can also verify this by Monte Carlo simulation:\n\n    \n    \n    simseq10K = replicate(1e5, sum(rpois(200, 0.0005)))\n    mean(simseq10K)__\n    \n    \n    [1] 0.10143\n    \n    \n    vcd::distplot(simseq10K, \"poisson\")__\n\n[![](05-chap_files/figure-html/fig-seqradiusex-1.png)](05-chap_files/figure-\nhtml/fig-seqradiusex-1.png \"Figure 5.33: distplot for the simseq10K data.\")\n\nFigure 5.33: `distplot` for the `simseq10K` data.\n\nFigure 5.33 shows us how close the distribution is to being Poisson\ndistributed.\n\n### 5.8.3 Infer sequence variants\n\nThe DADA method (Divisive Amplicon Denoising Algorithm, Rosen et al.\n([2012](16-chap.html#ref-Rosen:2012))) uses a parameterized model of\nsubstitution errors that distinguishes sequencing errors from real biological\nvariation. The model computes the probabilities of base substitutions, such as\nseeing an \\\\({\\tt A}\\\\) instead of a \\\\({\\tt C}\\\\). It assumes that these\nprobabilities are independent of the position along the sequence. Because\nerror rates vary substantially between sequencing runs and PCR protocols, the\nmodel parameters are estimated from the data themselves using an EM-type\napproach. A read is classified as noisy or exact given the current parameters,\nand the noise model parameters are updated accordingly12.\n\n12 In the case of a large data set, the noise model estimation step does not\nhave to be done on the complete set. See\n<https://benjjneb.github.io/dada2/bigdata.html> for tricks and tools when\ndealing with large data sets.\n\n13 F stands for forward strand and R for reverse strand.\n\nThe dereplicated sequences13 are read in and then divisive denoising and\nestimation is run with the `dada` function as in the following code:\n\n    \n    \n    derepFs = readRDS(file=\"../data/derepFs.rds\")\n    derepRs = readRDS(file=\"../data/derepRs.rds\")\n    library(\"dada2\")\n    ddF = dada(derepFs, err = NULL, selfConsist = TRUE)\n    ddR = dada(derepRs, err = NULL, selfConsist = TRUE)__\n\nIn order to verify that the error transition rates have been reasonably well\nestimated, we inspect the fit between the observed error rates (black points)\nand the fitted error rates (black lines) (Figure 5.34).\n\n    \n    \n    plotErrors(ddF)__\n    \n    \n    In chunk 'fig-rerrorprofile1': Warning: Removed 82 rows containing missing values or values outside the scale range(`geom_line()`).\n\n[![](05-chap_files/figure-html/fig-\nrerrorprofile1-1.png)](05-chap_files/figure-html/fig-rerrorprofile1-1.png\n\"Figure 5.34: Forward transition error rates as provided by plotErrors\\(ddF\\).\nThis shows the frequencies of each type of nucleotide transition as a function\nof quality.\")\n\nFigure 5.34: Forward transition error rates as provided by `plotErrors(ddF)`.\nThis shows the frequencies of each type of nucleotide transition as a function\nof quality.\n\nOnce the errors have been estimated, the algorithm is rerun on the data to\nfind the sequence variants:\n\n    \n    \n    dadaFs = dada(derepFs, err=ddF[[1]]$err_out, pool = TRUE)\n    dadaRs = dada(derepRs, err=ddR[[1]]$err_out, pool = TRUE)__\n\n**Note:** The sequence inference function can run in two different modes:\nIndependent inference by sample (`pool = FALSE`), and pooled inference from\nthe sequencing reads combined from all samples. Independent inference has two\nadvantages: as a functions of the number of samples, computation time is\nlinear and memory requirements are constant. Pooled inference is more\ncomputationally taxing, however it can improve the detection of rare variants\nthat occur just once or twice in an individual sample but more often across\nall samples. As this dataset is not particularly large, we performed pooled\ninference.\n\nSequence inference removes nearly all substitution and **indel** 14 errors\nfrom the data. We merge the inferred forward and reverse sequences, while\nremoving paired sequences that do not perfectly overlap as a final control\nagainst residual errors.\n\n14 The term _indel_ stands for insertion-deletion; when comparing two\nsequences that differ by a small stretch of characters, it is a matter of\nviewpoint whether this is an insertion or a deletion, thus the name.\n\n    \n    \n    mergers = mergePairs(dadaFs, derepFs, dadaRs, derepRs)__\n\nWe produce a contingency table of counts of ASVs. This is a higher-resolution\nanalogue of the “OTU15 table”, i.e., a samples by features table whose cells\ncontain the number of times each sequence variant was observed in each sample.\n\n15 operational taxonomic units\n\n    \n    \n    seqtab.all = makeSequenceTable(mergers[!grepl(\"Mock\",names(mergers))])__\n\n__\n\nQuestion 5.18\n\nExplore the components of the objects `dadaRs` and `mergers`.\n\n__\n\nSolution\n\n__\n\n`dadaRs` is a list of length 20. Its elements are objects class _dada_ that\ncontain the denoised reads. We will see in [Chapter 10](10-chap.html) how to\nalign the sequences, assign their taxonomies and combine them with the sample\ninformation for downstream analyses.\n\n    \n    \n    [1] 20\n    \n    \n    [1] 20\n    \n    \n    [1] \"list\"\n    \n    \n     [1] \"F3D0\"   \"F3D1\"   \"F3D141\" \"F3D142\" \"F3D143\" \"F3D144\" \"F3D145\" \"F3D146\"\n     [9] \"F3D147\" \"F3D148\" \"F3D149\" \"F3D150\" \"F3D2\"   \"F3D3\"   \"F3D5\"   \"F3D6\"  \n    [17] \"F3D7\"   \"F3D8\"   \"F3D9\"   \"Mock\"  \n    \n    \n    [1] \"list\"\n    \n    \n    [1] 20\n\nChimera are sequences that are artificially created during PCR amplification\nby the melding of two (in rare cases, more) of the original sequences. To\ncomplete our denoising workflow, we remove them with a call to the function\n`removeBimeraDenovo`, leaving us with a clean contingency table we will use\nlater on.\n\n    \n    \n    seqtab = removeBimeraDenovo(seqtab.all)__\n\n__\n\nQuestion 5.19\n\nWhy do you think the chimera are quite easy to recognize?  \nWhat proportion of the reads were chimeric in the `seqtab.all` data?  \nWhat proportion of unique sequence variants are chimeric?\n\n__\n\nSolution\n\n__\n\nHere we observed some sequence variants as chimeric, but these only represent\n7% of all reads.\n\n## 5.9 Summary of this chapter\n\n**Of a feather: how to compare observations** We saw at the start of the\nchapter how finding the **right distance** is an essential first step in a\nclustering analysis; this is a case where the _garbage in, garbage out_ motto\nis in full force. Always choose a distance that is scientifically meaningful\nand compare output from as many distances as possible; sometimes the same data\nrequire different distances when different scientific objectives are pursued.\n\n**Two ways of clustering** We saw there are two approaches to clustering:\n\n  * iterative partitioning approaches such as \\\\(k\\\\)-means and \\\\(k\\\\)-medoids (PAM) that alternated between estimating the cluster centers and assigning points to them;  \n\n  * hierarchical clustering approaches that first agglomerate points, and subsequently the growing clusters, into nested sequences of sets that can be represented by hierarchical clustering _trees_.\n\n**Biological examples** Clustering is important tool for finding latent\nclasses in single cell measurements, especially in immunology and single cell\ndata analyses. We saw how density-based clustering is useful for lower\ndimensional data where sparsity is not an issue.\n\n**Validating** Clustering algorithms _always_ deliver clusters, so we need to\nassess their quality and the number of clusters to choose carefully. Such\nvalidation steps are performed using visualization tools and repeating the\nclustering on many resamples of the data. We saw how statistics such as\nWSS/BSS or \\\\(\\log(\\text{WSS})\\\\) can be calibrated using simulations on data\nwhere we understand the group structure and can provide useful benchmarks for\nchoosing the number of clusters on new data. Of course, the use of\nbiologically relevant information to inform and confirm the meaning of\nclusters is always the best validation approach.\n\nThere is arguably no ground truth to compare a clustering result against, in\ngeneral. The old adage of “all models are wrong, some are useful” also applies\nhere. A good clustering is one that turns out to be useful.\n\n**Distances and probabilities** Finally: distances are not everything. We\nshowed how important it was to take into account baseline frequencies and\nlocal densities when clustering. This is essential in a cases such as\nclustering to denoise 16S rRNA sequence reads where the true class or taxa\ngroup occur at very different frequencies.\n\n## 5.10 Further reading\n\nFor a complete book on _Finding groups in data_ , see Kaufman and Rousseeuw\n([2009](16-chap.html#ref-Kaufman2009)). The vignette of the\n**[clusterExperiment](https://bioconductor.org/packages/clusterExperiment/)**\npackage contains a complete workflow for generating clusters using many\ndifferent techniques, including preliminary dimension reduction (PCA) that we\nwill cover in [Chapter 7](07-chap.html). There is no consensus on methods for\ndeciding how many clusters are needed to describe data in the absence of\ncontiguous biological information. However, making hierarchical clusters of\nthe _strong forms_ is a method that has the advantage of allowing the user to\ndecide how far down to cut the hierarchical tree and be careful not to cut in\nplaces where these inner branches are short. See the vignette of\n**[clusterExperiment](https://bioconductor.org/packages/clusterExperiment/)**\nfor an application to single cell RNA experimental data.\n\nIn analyzing the Hiiragi data, we used cluster probabilities, a concept\nalready mentioned in [Chapter 4](04-chap.html), where the EM algorithm used\nthem as weights to compute expected value statistics. The notion of\nprobabilistic clustering is well-developed in the Bayesian nonparametric\nmixture framework, which enriches the mixture models we covered in [Chapter\n4](04-chap.html) to more general settings. See Dundar et al.\n([2014](16-chap.html#ref-Dundar2014)) for a real example using this framework\nfor flow cytometry. In the denoising and assignment of high-throughput\nsequencing reads to specific strains of bacteria or viruses, clustering is\nessential. In the presence of noise, clustering into groups of _true_ strains\nof very unequal sizes can be challenging. Using the data to create a noise\nmodel enables both denoising and cluster assignment concurrently. Denoising\nalgorithms such as those by Rosen et al. ([2012](16-chap.html#ref-Rosen:2012))\nor Benjamin J. Callahan et al. ([2016](16-chap.html#ref-dada2)) use an\niterative workflow inspired by the EM method ([McLachlan and Krishnan\n2007](16-chap.html#ref-mclachlan2007algorithm)).\n\n## 5.11 Exercises\n\n__\n\nExercise 5.1\n\nWe can define the average dissimilarity of a point \\\\(x_i\\\\) to a cluster\n\\\\(C_k\\\\) as the average of the distances from \\\\(x_i\\\\) to all points in\n\\\\(C_k\\\\). Let \\\\(A(i)\\\\) be the average dissimilarity of all points in the\ncluster that \\\\(x_i\\\\) belongs to. Let \\\\(B(i)\\\\) be the lowest average\ndissimilarity of \\\\(x_i\\\\) to any other cluster of which \\\\(x_i\\\\) is not a\nmember. The cluster with this lowest average dissimilarity is said to be the\n**neighboring cluster** of \\\\(x_i\\\\), because it is the next best fit cluster\nfor point \\\\(x_i\\\\). The **silhouette index** is\n\n\\\\[ S(i)=\\frac{B(i)-A(i)}{\\max_i(A(i),B(i))}. \\\\]\n\nCompute the silhouette index for the `simdat` data we simulated in Section\n5.7.\n\n    \n    \n    library(\"cluster\")\n    pam4 = pam(simdatxy, 4)\n    sil = silhouette(pam4, 4)\n    plot(sil, col=c(\"red\",\"green\",\"blue\",\"purple\"), main=\"Silhouette\")__\n\nChange the number of clusters \\\\(k\\\\) and assess which \\\\(k\\\\) gives the best\nsilhouette index.\n\nNow, repeat this for groups that have uniform (unclustered) data distributions\nover a whole range of values.\n\n__\n\nExercise 5.2\n\nMake a “character” representation of the distance between the 20 locations in\nthe `dune` data from the\n**[vegan](https://cran.r-project.org/web/packages/vegan/)** package using the\nfunction `symnum`.\n\nMake a heatmap plot of these distances.\n\n__\n\nExercise 5.3\n\nLoad the `spirals` data from the\n**[kernlab](https://cran.r-project.org/web/packages/kernlab/)** package. Plot\nthe results of using \\\\(k\\\\)-means on the data. This should give you something\nsimilar to Figure 5.35.\n\n[![](05-chap_files/figure-html/fig-kmeanspital1-1.png)](05-chap_files/figure-\nhtml/fig-kmeanspital1-1.png \"Figure 5.35 \\(a\\): \")\n\n(a)\n\n[![](05-chap_files/figure-html/fig-kmeanspital1-2.png)](05-chap_files/figure-\nhtml/fig-kmeanspital1-2.png \"Figure 5.35 \\(b\\): \")\n\n(b)\n\nFigure 5.35: An example of non-convex clusters. In (a), we show the result of\n\\\\(k\\\\)-means clustering with \\\\(k=2\\\\). In (b), we have the output from\n`dbscan`. The colors represent the three clusters found by the algorithm for\nthe settings .\n\nYou’ll notice that the clustering in Figure 5.35 seems unsatisfactory. Show\nhow a different method, such as `specc` or `dbscan`, could cluster `spirals`\ndata in a more useful manner.\n\nRepeat the `dbscan` clustering with different parameters. How robust is the\nnumber of groups?\n\n__\n\nExercise 5.4\n\nLooking at graphical representations in simple two-dimensional maps can often\nreveal important clumping patterns. We saw an example for this with the map\nthat enabled Snow to discover the source of the London cholera outbreak. Such\nclusterings can often indicate important information about hidden variables\nacting on the observations. Look at a map for breast cancer incidence in the\nUS at:  \n<http://www.huffingtonpost.com/bill-davenhall/post_1663_b_817254.html>\n([Mandal et al. 2009](16-chap.html#ref-mandal2009)); the areas of high\nincidence seem spatially clustered. Can you guess the reason(s) for this\nclustering and high incidence rates on the West and East coasts and around\nChicago?\n\n__\n\nExercise 5.5\n\n**Amplicon bioinformatics: from raw reads to dereplicated sequences**. As a\nsupplementary exercise, we provide the intermediate steps necessary to a full\ndata preprocessing workflow for denoising 16S rRNA sequences. We start by\nsetting the directories and loading the downloaded data:\n\n    \n    \n    base_dir = \"../data\"\n    miseq_path = file.path(base_dir, \"MiSeq_SOP\")\n    filt_path = file.path(miseq_path, \"filtered\")\n    fnFs = sort(list.files(miseq_path, pattern=\"_R1_001.fastq\"))\n    fnRs = sort(list.files(miseq_path, pattern=\"_R2_001.fastq\"))\n    sampleNames = sapply(strsplit(fnFs, \"_\"), `[`, 1)\n    if (!file_test(\"-d\", filt_path)) dir.create(filt_path)\n    filtFs = file.path(filt_path, paste0(sampleNames, \"_F_filt.fastq.gz\"))\n    filtRs = file.path(filt_path, paste0(sampleNames, \"_R_filt.fastq.gz\"))\n    fnFs = file.path(miseq_path, fnFs)\n    fnRs = file.path(miseq_path, fnRs)\n    print(length(fnFs))__\n    \n    \n    [1] 20\n\nThe data are highly-overlapping Illumina Miseq \\\\(2\\times 250\\\\) amplicon\nsequences from the V4 region of the 16S rRNA gene ([Kozich et al.\n2013](16-chap.html#ref-Kozich2013)). There were originally 360 fecal samples\ncollected longitudinally from 12 mice over the first year of life. These were\ncollected by P. D. Schloss et al. ([2012](16-chap.html#ref-\nschloss2012stabilization)) to investigate the development and stabilization of\nthe murine microbiome. We have selected 20 samples to illustrate how to\npreprocess the data.\n\nWe will need to filter out low-quality reads and trim them to a consistent\nlength. While generally recommended filtering and trimming parameters serve as\na starting point, no two datasets are identical and therefore it is always\nworth inspecting the quality of the data before proceeding. We show the\nsequence quality plots for the two first samples in Figure 5.36. They are\ngenerated by:\n\n    \n    \n    plotQualityProfile(fnFs[1:2]) + ggtitle(\"Forward\")\n    plotQualityProfile(fnRs[1:2]) + ggtitle(\"Reverse\")__\n\n[![](05-chap_files/figure-html/fig-profile-1-1.png)](05-chap_files/figure-\nhtml/fig-profile-1-1.png \"Figure 5.36 \\(a\\): \")\n\n(a)\n\n[![](05-chap_files/figure-html/fig-profile-1-2.png)](05-chap_files/figure-\nhtml/fig-profile-1-2.png \"Figure 5.36 \\(b\\): \")\n\n(b)\n\nFigure 5.36: Quality scores. The lines show positional summary statistics:\ngreen is the mean, orange is the median, and the dashed orange lines are the\n25th and 75th quantiles.\n\nNote that we also see the background distribution of quality scores at each\nposition in Figure 5.36 as a grey-scale heat map. The dark colors correspond\nto higher frequency.\n\n__\n\nExercise 5.6\n\nGenerate similar plots for four randomly selected sets of forward and reverse\nreads. Compare forward and reverse read qualities; what do you notice?\n\n__\n\nSolution\n\n__\n\n    \n    \n    ii = sample(length(fnFs), 4)\n    plotQualityProfile(fnFs[ii]) + ggtitle(\"Forward\")__\n    \n    \n     plotQualityProfile(fnRs[ii]) + ggtitle(\"Reverse\")__\n\n__\n\nExercise 5.7\n\nHere, the forward reads maintain high quality throughout, while the quality of\nthe reverse reads drops significantly at about position 160. Therefore, we\ntruncate the forward reads at position 240, and trimm the first 10 nucleotides\nas these positions are of lower quality. The reverse reads are trimmed at\nposition 160. Combine these trimming parameters with standard filtering\nparameters remember to enforce a maximum of 2 expected errors per-read. (Hint:\nTrim and filter on paired reads jointly, i.e., both reads must pass the filter\nfor the pair to pass. The input arguments should be chosen following the\n**[dada2](https://bioconductor.org/packages/dada2/)** vignette carefully. We\nrecommend filtering out all reads with any ambiguous nucleotides.)\n\n__\n\nSolution\n\n__\n\nMost Illumina sequencing data show a trend of decreasing quality towards the\nend of the reads.\n\n    \n    \n    out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),\n            maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,  trimLeft=10,\n            compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE\n    head(out)__\n    \n    \n                                  reads.in reads.out\n    F3D0_S188_L001_R1_001.fastq       7793      7139\n    F3D1_S189_L001_R1_001.fastq       5869      5314\n    F3D141_S207_L001_R1_001.fastq     5958      5478\n    F3D142_S208_L001_R1_001.fastq     3183      2926\n    F3D143_S209_L001_R1_001.fastq     3178      2955\n    F3D144_S210_L001_R1_001.fastq     4827      4323\n\nThe `maxN` parameter omits all reads with more than `maxN = 0` ambiguous\nnucleotides and `maxEE` at 2 excludes reads with more than 2 expected errors.\n\nThe sequence data was imported into R from demultiplexed _fastq_ files (i.e.\none _fastq_ for each sample) and simultaneously dereplicated to remove\nredundancy. Name the resulting objects by their sample provenance; they will\nhave _derep_ as their class.\n\n    \n    \n    derepFs = derepFastq(filtFs, verbose = FALSE)\n    derepRs = derepFastq(filtRs, verbose = FALSE)\n    names(derepFs) = sampleNames\n    names(derepRs) = sampleNames __\n\n__\n\nExercise 5.8\n\nUse R to create a map like the one shown in Figure 5.2. Hint: go to the\n[website of the British National Archives](http://bombsight.org) and download\nstreet addresses of hits, use an address resolution service to convert these\ninto geographic coordinates, and display these as points on a map of London.\n\n__\n\nSolution\n\n__\n\nSee the Gist\n<https://gist.github.com/wolfganghuber/523622aff6a156d26e77a87ccbe7bd0d> by\nAndrzej Oles.\n\nAure, Miriam Ragle, Valeria Vitelli, Sandra Jernström, Surendra Kumar, Marit\nKrohn, Eldri U Due, Tonje Husby Haukaas, et al. 2017. “Integrative Clustering\nReveals a Novel Split in the Luminal A Subtype of Breast Cancer with Impact on\nOutcome.” _Breast Cancer Research_ 19 (1): 44.\n\nBendall, Sean C, Garry P Nolan, Mario Roederer, and Pratip K Chattopadhyay.\n2012. “A Deep Profiler’s Guide to Cytometry.” _Trends in Immunology_ 33 (7):\n323–32.\n\nCallahan, Ben J, Kris Sankaran, Julia A Fukuyama, Paul J McMurdie, and Susan P\nHolmes. 2016. “Bioconductor Workflow for Microbiome Data Analysis: From Raw\nReads to Community Analyses.” _F1000Research_ 5\\.\n\nCallahan, Benjamin J, Paul J McMurdie, and Susan P Holmes. 2017. “Exact\nSequence Variants Should Replace Operational Taxonomic Units in Marker Gene\nData Analysis.” _ISME Journal_ , 1–5.\n\nCallahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy J\nJohnson, and Susan P Holmes. 2016. “DADA2: High Resolution Sample Inference\nfrom Amplicon Data.” _Nature Methods_ , 1–4.\n\nCaporaso, J. G., J. Kuczynski, J. Stombaugh, K. Bittinger, F. D. Bushman, E.\nK. Costello, N. Fierer, et al. 2010. “QIIME Allows Analysis of High-Throughput\nCommunity Sequencing Data.” _Nature Methods_ 7 (5): 335–36.\n\nChakerian, John, and Susan Holmes. 2012. “Computational Tools for Evaluating\nPhylogenetic and Hierarchical Clustering Trees.” _Journal of Computational and\nGraphical Statistics_ 21 (3): 581–99.\n\nDiday, Edwin, and M Paula Brito. 1989. “Symbolic Cluster Analysis.” In\n_Conceptual and Numerical Analysis of Data_ , 45–84. Springer.\n\nDundar, Murat, Ferit Akova, Halid Z. Yerebakan, and Bartek Rajwa. 2014. “A\nNon-Parametric Bayesian Model for Joint Cell Clustering and Cluster Matching:\nIdentification of Anomalous Sample Phenotypes with Random Effects.” _BMC\nBioinformatics_ 15 (1): 1–15. <https://doi.org/10.1186/1471-2105-15-314>.\n\nFreedman, David A. 1991. “Statistical Models and Shoe Leather.” _Sociological\nMethodology_ 21 (2): 291–313.\n\nHallett, Robin M, Anna Dvorkin-Gheva, Anita Bane, and John A Hassell. 2012. “A\nGene Signature for Predicting Outcome in Patients with Basal-Like Breast\nCancer.” _Scientific Reports_ 2\\.\n\nHolmes, Susan, Michael He, Tong Xu, and Peter P Lee. 2005. “Memory t Cells\nHave Gene Expression Patterns Intermediate Between Naive and Effector.” _PNAS_\n102 (15): 5519–23.\n\nHornik, Kurt. 2005. “A CLUE for CLUster Ensembles.” _Journal of Statistical\nSoftware_ 14 (12).\n\nHulett, Henry R, William A Bonner, Janet Barrett, and Leonard A Herzenberg.\n1969. “Cell Sorting: Automated Separation of Mammalian Cells as a Function of\nIntracellular Fluorescence.” _Science_ 166 (3906): 747–49.\n\nKahneman, Daniel. 2011. _Thinking, Fast and Slow_. Macmillan.\n\nKaufman, Leonard, and Peter J Rousseeuw. 2009. _Finding Groups in Data: An\nIntroduction to Cluster Analysis_. Vol. 344. John Wiley & Sons.\n\nKozich, James J, Sarah L Westcott, Nielson T Baxter, Sarah K Highlander, and\nPatrick D Schloss. 2013. “Development of a Dual-Index Sequencing Strategy and\nCuration Pipeline for Analyzing Amplicon Sequence Data on the MiSeq Illumina\nSequencing Platform.” _Applied and Environmental Microbiology_ 79 (17):\n5112–20.\n\nMandal, Rakesh, Sophie St-Hilaire, John G Kie, and DeWayne Derryberry. 2009.\n“Spatial Trends of Breast and Prostate Cancers in the United States Between\n2000 and 2005.” _International Journal of Health Geographics_ 8 (1): 53.\n\nMcLachlan, Geoffrey, and Thriyambakam Krishnan. 2007. _The EM Algorithm and\nExtensions_. Vol. 382. John Wiley & Sons.\n\nMüllner, Daniel. 2013. “Fastcluster: Fast Hierarchical, Agglomerative\nClustering Routines for r and Python.” _Journal of Statistical Software_ 53\n(9): 1–18.\n\nO’Neill, Kieran, Nima Aghaeepour, Josef Špidlen, and Ryan Brinkman. 2013.\n“Flow Cytometry Bioinformatics.” _PLoS Computational Biology_ 9 (12):\ne1003365.\n\nOhnishi, Y., W. Huber, A. Tsumura, M. Kang, P. Xenopoulos, K. Kurimoto, A. K.\nOles, et al. 2014. “Cell-to-Cell Expression Variability Followed by Signal\nReinforcement Progressively Segregates Early Mouse Lineages.” _Nature Cell\nBiology_ 16 (1): 27–37.\n\nRosen, Michael J, Benjamin J Callahan, Daniel S Fisher, and Susan P Holmes.\n2012. “Denoising PCR-Amplified Metagenome Data.” _BMC Bioinformatics_ 13 (1):\n283.\n\nSchloss, P D, S L Westcott, T Ryabin, J R Hall, M Hartmann, E B Hollister, R A\nLesniewski, et al. 2009. “Introducing mothur: Open-Source, Platform-\nIndependent, Community-Supported Software for Describing and Comparing\nMicrobial Communities.” _Applied and Environmental Microbiology_ 75 (23):\n7537–41.\n\nSchloss, P. D., A. M. Schuber, J. P. Zackular, K. D. Iverson, Young V. B., and\nPetrosino J. F. 2012. “Stabilization of the Murine Gut Microbiome Following\nWeaning.” _Gut Microbes_ 3 (4): 383–93.\n\nTibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. “Estimating the\nNumber of Clusters in a Data Set via the Gap Statistic.” _JRSSB_ 63 (2):\n411–23.\n\nTseng, George C, and Wing H Wong. 2005. “Tight Clustering: A Resampling-Based\nApproach for Identifying Stable and Tight Patterns in Data.” _Biometrics_ 61\n(1): 10–16.\n\nTversky, Amos, and Daniel Kahneman. 1974. “Heuristics and Biases: Judgement\nUnder Uncertainty.” _Science_ 185: 1124–30.\n\n———. 1975. “Judgment Under Uncertainty: Heuristics and Biases.” In _Utility,\nProbability, and Human Decision Making_ , 141–62. Springer.\n\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["msmb.css"],"toc":true,"output-file":"05-chap.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}