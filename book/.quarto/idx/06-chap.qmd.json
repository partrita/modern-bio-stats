{"title":"6.1 Goals for this Chapter","markdown":{"headingText":"6.1 Goals for this Chapter","containsRefs":false,"markdown":"[![](imgs/xkcdmulttest-newspapertitle.png)](imgs/xkcdmulttest-\nnewspapertitle.png)\n\nHypothesis testing is one of the workhorses of science. It is how we can draw\nconclusions or make decisions based on finite samples of data. For instance,\nnew treatments for a disease are usually approved on the basis of clinical\ntrials that aim to decide whether the treatment has better efficacy compared\nto the other available options, and an acceptable trade-off of side effects.\nSuch trials are expensive and can take a long time. Therefore, the number of\npatients we can enroll is limited, and we need to base our inference on a\nlimited sample of observed patient responses. The data are noisy, since a\npatient’s response depends not only on the treatment, but on many other\nfactors outside of our control. The sample size needs to be large enough to\nenable us to make a reliable conclusion. On the other hand, it also must not\nbe too large, so that we do not waste precious resources or time, e.g., by\nmaking drugs more expensive than necessary, or by denying patients that would\nbenefit from the new drug access to it. The machinery of hypothesis testing\nwas developed largely with such applications in mind, although today it is\nused much more widely.\n\nIn biological data analysis (and in many other fields1) we see hypothesis\ntesting applied to screen thousands or millions of possible hypotheses to find\nthe ones that are worth following up. For instance, researchers screen genetic\nvariants for associations with a phenotype, or gene expression levels for\nassociations with disease. Here, “worthwhile” is often interpreted as\n“statistically significant”, although the two concepts are clearly not the\nsame. It is probably fair to say that statistical significance is a necessary\ncondition for making a data-driven decision to find something interesting, but\nit’s clearly not sufficient. In any case, such large-scale association\nscreening is closely related to multiple hypothesis testing.\n\n1 Detecting credit card fraud, email spam detection, \\\\(...\\\\)\n\n\nIn this chapter we will:\n\n  * Familiarize ourselves with the statistical machinery of hypothesis testing, its vocabulary, its purpose, and its strengths and limitations.\n\n  * Understand what multiple testing means.\n\n  * See that multiple testing is not a problem – but rather, an opportunity, as it overcomes many of the limitations of single testing.\n\n  * Understand the false discovery rate.\n\n  * Learn how to make diagnostic plots.\n\n  * Use hypothesis weighting to increase the power of our analyses.\n\n### 6.1.1 Drinking from the firehose\n\n[![](imgs/active-substance-discovery-robot-screening-robot.jpg)](imgs/active-\nsubstance-discovery-robot-screening-robot.jpg \"Figure 6.1: High-throughput\ndata in modern biology are screened for associations with millions of\nhypothesis tests. \\(Source: Bayer\\)\")\n\nFigure 6.1: High-throughput data in modern biology are screened for\nassociations with millions of hypothesis tests. ([Source:\nBayer](https://www.research.bayer.com/en/automated-search-for-active-\ningredients-with-robots.aspx))\n\nIf statistical testing—decision making with uncertainty—seems a hard task when\nmaking a single decision, then brace yourself: in genomics, or more generally\nwith “big data”, we need to accomplish it not once, but thousands or millions\nof times. In [Chapter 2](02-chap.html), we saw the example of epitope\ndetection and the challenges from considering not only one, but several\npositions. Similarly, in whole genome sequencing, we scan every position in\nthe genome for evidence of a difference in the DNA sequencing data at hand and\na reference sequence (or, another set of sequencing data): that’s in the order\nof six billion tests if we are looking at human data! In genetic or chemical\ncompound screening, we test each of the reagents for an effect in the assay,\ncompared to a control: that’s again tens of thousands, if not millions of\ntests. In [Chapter 8](08-chap.html), we will analyse RNA-Seq data for\ndifferential expression by applying a hypothesis test to each of the thousands\nof genes assayed.\n\n### 6.1.2 Testing versus classification\n\nSuppose we measured the abundance level of a marker molecule to decide whether\nsome cells we are studying are in cell state A or B. First, let’s consider\nthat we have no prior assumption, and that we just want to use the data to\nmake a more or less symmetric choice between the two outcomes. This is a\n_classification_ task. We’ll cover classification in [Chapter\n12](12-chap.html). In this chapter, we consider the asymmetric case: based on\nwhat we already know (we could call this our _prior_ knowledge), cell state A\nis predominant, the “default”. We’ll only call B if there is strong enough\nevidence for it. Maybe B is rare, unusual, interesting, and if we find it, we\nare going to commit resources to study it further. Whereas A is uninteresting\nand demands no futher follow-up. In such cases, the machinery of hypothesis\ntesting is for us.\n\nFormally, there are many similarities between hypothesis testing and\nclassification. In both cases, we aim to use data to choose between several\npossible decisions. The distinction can be a fuzzy, and it is even possible to\nthink of hypothesis testing as a special case of classification. However,\nthese two approaches are geared towards different objectives and underlying\nassumptions. When you encounter a statistical decision problem, it can be\nuseful to check which approach is more appropriate.\n\n### 6.1.3 False discovery rate or p-value?\n\n[![](06-chap_files/figure-html/fig-testing-\nFDRvspstatic1-1.png)](06-chap_files/figure-html/fig-testing-\nFDRvspstatic1-1.png \"Figure 6.2: Making a binary \\(yes/no\\) decision. Here, we\ncall the two possible decisions “positive” and “negative” based on some\ncontinuous-valued score x, shown along the x-axis. The curve shaded in blue\nshows the distribution density of x for one of the classes \\(the negatives\\),\nthe curve shaded in red, for the other class \\(the positives\\). The\ndistributions are distinctive \\(the red values are generally lower\\), but have\nsome overlap. The vertical black bar marks some choice of a decision boundary,\nwhich results in four possible outcomes highlighted by the color key.\")\n\nFigure 6.2: Making a binary (yes/no) decision. Here, we call the two possible\ndecisions “positive” and “negative” based on some continuous-valued score\n\\\\(x\\\\), shown along the \\\\(x\\\\)-axis. The curve shaded in blue shows the\ndistribution density of \\\\(x\\\\) for one of the classes (the negatives), the\ncurve shaded in red, for the other class (the positives). The distributions\nare distinctive (the red values are generally lower), but have some overlap.\nThe vertical black bar marks some choice of a decision boundary, which results\nin four possible outcomes highlighted by the color key.\n\n[![](06-chap_files/figure-html/fig-testing-\nFDRvspstatic2-1.png)](06-chap_files/figure-html/fig-testing-\nFDRvspstatic2-1.png \"Figure 6.3: Analogous to Figure fig-testing-\nFDRvspstatic1, but now we have transformed x from its original range to the\nrange \\[0,1\\] using a non-linear, strictly increasing transformation function\np=f\\(x\\), which we chose such that the resulting blue distribution is uniform.\nSuch a function always exists: it is the cumulative distribution function of x\n\\(we have seen it in sec-graphics-ecdf\\). We call the result a p-value. The\ndefinition of the FDR in Equation eq-testing-simplefdr applies equally well in\nFigure fig-testing-FDRvspstatic1 and here.\")\n\nFigure 6.3: Analogous to Figure 6.2, but now we have transformed \\\\(x\\\\) from\nits original range to the range \\\\([0,1]\\\\) using a non-linear, strictly\nincreasing transformation function \\\\(p=f(x)\\\\), which we chose such that the\nresulting blue distribution is uniform. Such a function always exists: it is\nthe cumulative distribution function of \\\\(x\\\\) (we have seen it in [Section\n3.6.7](03-chap.html#sec-graphics-ecdf)). We call the result a **p-value**. The\ndefinition of the FDR in Equation 6.2 applies equally well in Figure 6.2 and\nhere.\n\n[![](06-chap_files/figure-html/fig-testing-\nFDRvspanim-1.gif)](06-chap_files/figure-html/fig-testing-FDRvspanim-1.gif\n\"Figure 6.4: The animation highlights the analogies between using a generic\nscore x \\(as in Figure fig-testing-FDRvspstatic1\\) and a p-value from a formal\nhypothesis test \\(as in Figure fig-testing-FDRvspstatic2\\) for decision\nmaking. We will come back to these concepts in terms of the two-group model in\nsec-testing-localfdr and Figure fig-testing-lfdr.\")\n\nFigure 6.4: The animation highlights the analogies between using a generic\nscore \\\\(x\\\\) (as in Figure 6.2) and a p-value from a formal hypothesis test\n(as in Figure 6.3) for decision making. We will come back to these concepts in\nterms of the two-group model in Section 6.10 and Figure 6.17.\n\nHypothesis testing has traditionally been taught with p-values first,\nintroducing them as the primal, basic concept. Multiple testing and false\ndiscovery rates were then presented as derived, additional ideas. There are\ngood mathematical and practical reasons for doing so, and the rest of this\nchapter follows this tradition. However, in this prefacing section we would\nlike to point out that the false discovery rate is in fact the more intuitive\nconcept, and that it also tends to be more useful in practice. A p-value is\nsomething more abstract, and it is often not quite clear what to do with it,\nand thus keeps confusing those who want to use it for decision making. So here\nis an attempt to be more pedagogical, revert the order, and learn about false\ndiscovery rates first. We can then think of p-values as something second-best,\nwhich we have to resort to if the false discovery rate is not accessible.\n\nConsider Figure 6.2, which represents a binary decision problem. Let’s say we\ncall a _discovery_ whenever the summary statistic \\\\(x\\\\) is particularly\nsmall, i.e., when it falls to the left of the vertical black bar2. Then the\n_false discovery rate_ 3 (FDR) is simply the fraction of false discoveries\namong all discoveries, i.e.:\n\n2 This is “without loss of generality”: we could also flip the \\\\(x\\\\)-axis\nand call something with a high score a discovery.\n\n3 This is a rather informal definition. For more precise definitions, see for\ninstance ([Storey 2003](16-chap.html#ref-Storey:AnnStat:2003); [Efron\n2010](16-chap.html#ref-Efron2010)) and Section 6.10.\n\n\\\\[ \\text{FDR}=\\frac{\\text{area shaded in light blue}}{\\text{sum of the areas\nleft of the vertical bar (light blue + strong red)}}. \\tag{6.1}\\\\]\n\nThe FDR depends not only on the position of the decision threshold (the\nvertical bar), but also on the shape and location of the two distributions,\nand on their relative sizes. In Figures 6.2 and 6.3, the overall blue area is\ntwice as big as the overall red area, reflecting the fact that the blue class\nis (in this example) twice as prevalent (or: a priori, twice as likely) as the\nred class.\n\nNote that this definition does not require the concept or even the calculation\nof a p-value. It works for any arbitrarily defined score \\\\(x\\\\). However, it\nrequires knowledge of three things:\n\n  1. the distribution of \\\\(x\\\\) in the blue class (the blue curve),\n\n  2. the distribution of \\\\(x\\\\) in the red class (the red curve),\n\n  3. the relative sizes of the blue and the red classes.\n\nIf we know these, then we are basically done at this point; or we can move on\nto supervised classification in [Chapter 12](12-chap.html), which deals with\nthe extension of Figure 6.2 to multivariate \\\\(x\\\\).\n\nVery often, however, we do not know all of these, and this is the realm of\nhypothesis testing. In particular, suppose that one of the two classes (say,\nthe blue one) is easier than the other, and we can figure out its\ndistribution, either from first principles or simulations. We use that fact to\ntransform our score \\\\(x\\\\) to a standardized range between 0 and 1 (see\nFigures 6.2—6.4), which we call the _p-value_. We give the class a fancier\nname: _null hypothesis_. This addresses Point 1 in the above list. We do not\ninsist on knowing Point 2 (and we give another fancy name, _alternative\nhypothesis_ , to the red class). As for Point 3, we can use the conservative\nupper limit that the null hypothesis is far more prevalent (or: likely) than\nthe alternative and do our calculations under the condition that the null\nhypothesis is true. This is the traditional approach to hypothesis testing.\n\nThus, instead of basing our decision-making on the intuitive FDR (Equation\n6.2), we base it on the\n\n\\\\[ \\text{p-value}=\\frac{\\text{area shaded in light blue}}{\\text{overall blue\narea}}. \\tag{6.2}\\\\]\n\nIn other words, the p-value is the precise and often relatively easy-to-\ncompute answer to a rather convoluted question (and perhaps the wrong\nquestion). The FDR answers the right question, but requires a lot more input,\nwhich we often do not have.\n\n### 6.1.4 The multiple testing opportunity\n\nHere is the good news about multiple testing: even if we do not know Items 2\nand 3 from the bullet list above explicitly for our tests (and perhaps even if\nwe are unsure about Point 1 ([Efron 2010](16-chap.html#ref-Efron2010))), we\nmay be able to infer this information from the multiplicity—and thus convert\np-values into estimates of the FDR!\n\nThus, multiple testing tends to make our inference better, and our task\nsimpler. Since we have so much data, we do not only have to rely on abstract\nassumptions. We can check empirically whether the requirements of the tests\nare actually met by the data. All this can be incredibly helpful, and we get\nit _because_ of the multiplicity. So we should think about multiple testing\nnot as a “problem” or a “burden”, but as an opportunity!\n\n## 6.2 An example: coin tossing\n\nSo now let’s dive into hypothesis testing, starting with single testing. To\nreally understand the mechanics, we use one of the simplest possible examples:\nsuppose we are flipping a coin to see if it is fair4. We flip the coin 100\ntimes and each time record whether it came up heads or tails. So, we have a\nrecord that could look something like this:\n\n4 We don’t look at coin tossing because it’s inherently important, but because\nit is an easy “model system” (just as we use model systems in biology):\neverything can be calculated easily, and you do not need a lot of domain\nknowledge to understand what coin tossing is. All the important concepts come\nup, and we can apply them, only with more additional details, to other\napplications.\n\nHHTTHTHTT…\n\nwhich we can simulate in R. Let’s assume we are flipping a biased coin, so we\nset `probHead` different from 1/2:\n\n    \n    \n    set.seed(0xdada)\n    numFlips = 100\n    probHead = 0.6\n    coinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n      replace = TRUE, prob = c(probHead, 1 - probHead))\n    head(coinFlips)__\n    \n    \n    [1] \"T\" \"T\" \"H\" \"T\" \"H\" \"H\"\n\nNow, if the coin were fair, we would expect half of the time to get heads.\nLet’s see.\n\n    \n    \n    table(coinFlips)__\n    \n    \n    coinFlips\n     H  T \n    59 41 \n\nSo that is different from 50/50. Suppose we showed the data to a friend\nwithout telling them whether the coin is fair, and their prior assumption,\ni.e., their null hypothesis, is that coins are, by and large, fair. Would the\ndata be strong enough to make them conclude that this coin isn’t fair? They\nknow that random sampling differences are to be expected. To decide, let’s\nlook at the sampling distribution of our test statistic – the total number of\nheads seen in 100 coin tosses – for a fair coin5. As we saw in [Chapter\n1](01-chap.html), the number, \\\\(k\\\\), of heads, in \\\\(n\\\\) independent tosses\nof a coin is\n\n5 We haven’t really defined what we mean be fair – a reasonable definition\nwould be that head and tail are equally likely, and that the outcome of each\ncoin toss does not depend on the previous ones. For more complex applications,\nnailing down the most suitable null hypothesis can take some thought.\n\n\\\\[ P(K=k\\,|\\,n, p) = \\left(\\begin{array}{c}n\\\\\\k\\end{array}\\right)\np^k\\;(1-p)^{n-k}, \\tag{6.3}\\\\]\n\nwhere \\\\(p\\\\) is the probability of heads (0.5 if we assume a fair coin). We\nread the left hand side of the above equation as “the probability that the\nobserved value for \\\\(K\\\\) is \\\\(k\\\\), given the values of \\\\(n\\\\) and\n\\\\(p\\\\)”. Statisticians like to make a difference between all the possible\nvalues of a statistic and the one that was observed6, and we use the upper\ncase \\\\(K\\\\) for the possible values (so \\\\(K\\\\) can be anything between 0 and\n100), and the lower case \\\\(k\\\\) for the observed value.\n\n6 In other words, \\\\(K\\\\) is the abstract random variable in our probabilistic\nmodel, whereas \\\\(k\\\\) is its realization, that is, a specific data point.\n\nWe plot Equation 6.3 in Figure 6.5; for good measure, we also mark the\nobserved value `numHeads` with a vertical blue line.\n\n    \n    \n    library(\"dplyr\")\n    k = 0:numFlips\n    numHeads = sum(coinFlips == \"H\")\n    binomDensity = tibble(k = k,\n         p = dbinom(k, size = numFlips, prob = 0.5))__\n    \n    \n    library(\"ggplot2\")\n    ggplot(binomDensity) +\n      geom_bar(aes(x = k, y = p), stat = \"identity\") +\n      geom_vline(xintercept = numHeads, col = \"blue\")__\n\n[![](06-chap_files/figure-html/fig-testing-\ndbinom-1.png)](06-chap_files/figure-html/fig-testing-dbinom-1.png \"Figure 6.5:\nThe binomial distribution for the parameters n=100 and p=0.5, according to\nEquation eq-testing-dbinom.\")\n\nFigure 6.5: The binomial distribution for the parameters \\\\(n=100\\\\) and\n\\\\(p=0.5\\\\), according to Equation 6.3.\n\nSuppose we didn’t know about Equation 6.3. We can still use Monte Carlo\nsimulation to give us something to compare with:\n\n    \n    \n    numSimulations = 10000\n    outcome = replicate(numSimulations, {\n      coinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n                         replace = TRUE, prob = c(0.5, 0.5))\n      sum(coinFlips == \"H\")\n    })\n    ggplot(tibble(outcome)) + xlim(-0.5, 100.5) +\n      geom_histogram(aes(x = outcome), binwidth = 1, center = 50) +\n      geom_vline(xintercept = numHeads, col = \"blue\")__\n\n[![](06-chap_files/figure-html/fig-rbinom-1.png)](06-chap_files/figure-\nhtml/fig-rbinom-1.png \"Figure 6.6: An approximation of the binomial\ndistribution from 10000 simulations \\(same parameters as Figure fig-testing-\ndbinom\\).\")\n\nFigure 6.6: An approximation of the binomial distribution from \\\\(10000\\\\)\nsimulations (same parameters as Figure 6.5).\n\nAs expected, the most likely number of heads is 50, that is, half the number\nof coin flips. But we see that other numbers near 50 are also quite likely.\nHow do we quantify whether the observed value, 59, is among those values that\nwe are likely to see from a fair coin, or whether its deviation from the\nexpected value is already large enough for us to conclude with enough\nconfidence that the coin is biased? We divide the set of all possible \\\\(k\\\\)\n(0 to 100) in two complementary subsets, the **rejection region** and the\nregion of no rejection. Our choice here7 is to fill up the rejection region\nwith as many \\\\(k\\\\) as possible while keeping their total probability,\nassuming the null hypothesis, below some threshold \\\\(\\alpha\\\\) (say, 0.05).\n\n7 More on this in Section 6.3.1.\n\nIn the code below, we use the function `arrange` from the\n**[dplyr](https://cran.r-project.org/web/packages/dplyr/)** package to sort\nthe p-values from lowest to highest, then pass the result to `mutate`, which\nadds another dataframe column `reject` that is defined by computing the\ncumulative sum (`cumsum`) of the p-values and thresholding it against `alpha`.\nThe logical vector `reject` therefore marks with `TRUE` a set of `k`s whose\ntotal probability is less than `alpha`. These are marked in Figure 6.7, and we\ncan see that our rejection region is not contiguous – it comprises both the\nvery large and the very small values of `k`.\n\n    \n    \n    library(\"dplyr\")\n    alpha = 0.05\n    binomDensity = arrange(binomDensity, p) |>\n            mutate(reject = (cumsum(p) <= alpha))\n    \n    ggplot(binomDensity) +\n      geom_bar(aes(x = k, y = p, col = reject), stat = \"identity\") +\n      scale_colour_manual(\n        values = c(`TRUE` = \"red\", `FALSE` = \"darkgrey\")) +\n      geom_vline(xintercept = numHeads, col = \"blue\") +\n      theme(legend.position = \"none\")__\n\n[![](06-chap_files/figure-html/fig-testing-\nfindrej-1.png)](06-chap_files/figure-html/fig-testing-findrej-1.png\n\"Figure 6.7: As Figure fig-testing-dbinom, with rejection region \\(red\\) that\nhas been chosen such that it contains the maximum number of bins whose total\narea is at most \\\\alpha=0.05.\")\n\nFigure 6.7: As Figure 6.5, with rejection region (red) that has been chosen\nsuch that it contains the maximum number of bins whose total area is at most\n\\\\(\\alpha=0.05\\\\).\n\nThe explicit summation over the probabilities is clumsy, we did it here for\npedagogic value. For one-dimensional distributions, R provides not only\nfunctions for the densities (e.g., `dbinom`) but also for the cumulative\ndistribution functions (`pbinom`), which are more precise and faster than\n`cumsum` over the probabilities. These should be used in practice.\n\n__\n\nTask\n\nDo the computations for the rejection region and produce a plot like Figure\n6.7 without using `dbinom` and `cumsum`, and with using `pbinom` instead.\n\nWe see in Figure 6.7 that the observed value, 59, lies in the grey shaded\narea, so we would _not_ reject the null hypothesis of a fair coin from these\ndata at a significance level of \\\\(\\alpha=0.05\\\\).\n\n__\n\nQuestion 6.1\n\nDoes the fact that we don’t reject the null hypothesis mean that the coin is\nfair?\n\n__\n\nQuestion 6.2\n\nWould we have a better chance of detecting that the coin is not fair if we did\nmore coin tosses? How many?\n\n__\n\nQuestion 6.3\n\nIf we repeated the whole procedure and again tossed the coin 100 times, might\nwe _then_ reject the null hypothesis?\n\n__\n\nQuestion 6.4\n\nThe rejection region in Figure 6.7 is asymmetric – its left part ends with\n\\\\(k=40\\\\), while its right part starts with \\\\(k=61\\\\). Why is that? Which\nother ways of defining the rejection region might be useful?\n\nWe have just gone through the steps of a binomial test. In fact, this is such\na frequent activity in R that it has been wrapped into a single function, and\nwe can compare its output to our results.\n\n    \n    \n    binom.test(x = numHeads, n = numFlips, p = 0.5)__\n    \n    \n        Exact binomial test\n    \n    data:  numHeads and numFlips\n    number of successes = 59, number of trials = 100, p-value = 0.08863\n    alternative hypothesis: true probability of success is not equal to 0.5\n    95 percent confidence interval:\n     0.4871442 0.6873800\n    sample estimates:\n    probability of success \n                      0.59 \n\n## 6.3 The five steps of hypothesis testing\n\nLet’s summarise the general principles of hypothesis testing:\n\n  1. Decide on the effect that you are interested in, design a suitable experiment or study, pick a data summary function and **test statistic**.\n\n  2. Set up a **null hypothesis** , which is a simple, computationally tractable model of reality that lets you compute the **null distribution** , i.e., the possible outcomes of the test statistic and their probabilities under the assumption that the null hypothesis is true.\n\n  3. Decide on the **rejection region** , i.e., a subset of possible outcomes whose total probability is small8.\n\n  4. Do the experiment and collect the data9; compute the test statistic.\n\n  5. Make a decision: reject the null hypothesis10 if the test statistic is in the rejection region.\n\n8 More on this in Section 6.3.1.\n\n9 Or if someone else has already done it, download their data.\n\n10 That is, conclude that it is unlikely to be true.\n\nNote how in this idealized workflow, we make all the important decisions in\nSteps 1–3 before we have even seen the data. As we already alluded to in the\nIntroduction (Figures [1](00-chap.html#fig-Fisher) and [2](00-chap.html#fig-\niterative)), this is often not realistic. We will also come back to this\nquestion in Section 6.6.\n\nThere was also idealization in our null hypothesis that we used in the example\nabove: we postulated that a fair coin should have a probability of exactly 0.5\n(not, say, 0.500001) and that there should be absolutely no dependence between\ntosses. We did not worry about any possible effects of air drag, elasticity of\nthe material on which the coin falls, and so on. This gave us the advantage\nthat the null hypothesis was computationally tractable, namely, with the\nbinomial distribution. Here, these idealizations may not seem very\ncontroversial, but in other situations the trade-off between how tractable and\nhow realistic a null hypothesis is can be more substantial. The problem is\nthat if a null hypothesis is too idealized to start with, rejecting it is not\nall that interesting. The result may be misleading, and certainly we are\nwasting our time.\n\nThe test statistic in our example was the total number of heads. Suppose we\nobserved 50 tails in a row, and then 50 heads in a row. Our test statistic\nignores the order of the outcomes, and we would conclude that this is a\nperfectly fair coin. However, if we used a different test statistic, say, the\nnumber of times we see two tails in a row, we might notice that there is\nsomething funny about this coin.\n\n__\n\nQuestion 6.5\n\nWhat is the null distribution of this different test statistic?\n\n__\n\nQuestion 6.6\n\nWould a test based on that statistic be generally preferable?\n\n__\n\nSolution\n\n__\n\nNo, while it has more power to detect such correlations between coin tosses,\nit has _less_ power to detect bias in the outcome.\n\nWhat we have just done is look at two different classes of **alternative\nhypotheses**. The first class of alternatives was that subsequent coin tosses\nare still independent of each other, but that the probability of heads\ndiffered from 0.5. The second one was that the overall probability of heads\nmay still be 0.5, but that subsequent coin tosses were correlated.\n\n__\n\nQuestion 6.7\n\nRecall the concept of sufficient statistics from [Chapter 1](01-chap.html). Is\nthe total number of heads a sufficient statistic for the binomial\ndistribution? Why might it be a good test statistic for our first class of\nalternatives, but not for the second?\n\nSo let’s remember that we typically have multiple possible choices of test\nstatistic (in principle it could be any numerical summary of the data). Making\nthe right choice is important for getting a test with good power11. What the\nright choice is will depend on what kind of alternatives we expect. This is\nnot always easy to know in advance.\n\n11 See [Section 1.4.1](01-chap.html#sec-generative-SimulatingForPower) and\n6.4.\n\n12 The assumptions don’t need to be _exactly_ true – it is sufficient that the\ntheory’s predictions are an acceptable approximation of the truth.\n\nOnce we have chosen the test statistic we need to compute its null\ndistribution. You can do this either with pencil and paper or by computer\nsimulations. A pencil and paper solution is parametric and leads to a closed\nform mathematical expression (like Equation 6.3), which has the advantage that\nit holds for a range of model parameters of the null hypothesis (such as\n\\\\(n\\\\), \\\\(p\\\\)). It can also be quickly computed for any specific set of\nparameters. But it is not always as easy as in the coin tossing example.\nSometimes a pencil and paper solution is impossibly difficult to compute. At\nother times, it may require simplifying assumptions. An example is a null\ndistribution for the \\\\(t\\\\)-statistic (which we will see later in this\nchapter). We can compute this if we assume that the data are independent and\nnormally distributed: the result is called the \\\\(t\\\\)-distribution. Such\nmodelling assumptions may be more or less realistic. Simulating the null\ndistribution offers a potentially more accurate, more realistic and perhaps\neven more intuitive approach. The drawback of simulating is that it can take a\nrather long time, and we need extra work to get a systematic understanding of\nhow varying parameters influence the result. Generally, it is more elegant to\nuse the parametric theory when it applies12. When you are in doubt, simulate –\nor do both.\n\n### 6.3.1 The rejection region\n\nHow to choose the right rejection region for your test? First, what should its\nsize be? That is your choice of the **significance level** or false positive\nrate \\\\(\\alpha\\\\), which is the total probability of the test statistic\nfalling into this region even if the null hypothesis is true13.\n\n13 Some people at some point in time for a particular set of questions\ncolluded on \\\\(\\alpha=0.05\\\\) as being “small”. But there is nothing special\nabout this number, and in any particular case the best choice for a decision\nthreshold may very much depend on context ([Wasserstein and Lazar\n2016](16-chap.html#ref-Wasserstein2016:ASA); [Altman and Krzywinski\n2017](16-chap.html#ref-Altman:PoS:2017)).\n\nGiven the size, the next question is about its shape. For any given size,\nthere are usually multiple possible shapes. It makes sense to require that the\nprobability of the test statistic falling into the rejection region is as\nlarge possible if the alternative hypothesis is true. In other words, we want\nour test to have high **power** , or true positive rate.\n\nThe criterion that we used in the code for computing the rejection region for\nFigure 6.7 was to make the region contain as many `k` as possible. That is\nbecause in absence of any information about the alternative distribution, one\n`k` is as good as any other, and we maximize their total number.\n\nA consequence of this is that in Figure 6.7 the rejection region is split\nbetween the two tails of the distribution. This is because we anticipate that\nunfair coins could have a bias either towards head or toward tail; we don’t\nknow. If we did know, we would instead concentrate our rejection region all on\nthe appropriate side, e.g., the right tail if we think the bias would be\ntowards head. Such choices are also referred to as _two-sided_ and _one-sided_\ntests. More generally, if we have assumptions about the alternative\ndistribution, this can influence our choice of the shape of the rejection\nregion.\n\n## 6.4 Types of error\n\nHaving set out the mechanics of testing, we can assess how well we are doing.\nTable 6.1 compares reality (whether or not the null hypothesis is in fact\ntrue) with our decision whether or not to reject the null hypothesis after we\nhave seen the data.\n\nTest vs reality | Null hypothesis is true | \\\\(...\\\\) is false  \n---|---|---  \n**Reject null hypothesis** | Type I error (false positive) | True positive  \n**Do not reject** | True negative | Type II error (false negative)  \n  \nTable 6.1: Types of error in a statistical test.\n\nIt is always possible to reduce one of the two error types at the cost of\nincreasing the other one. The real challenge is to find an acceptable trade-\noff between both of them. This is exemplified in Figure 6.2. We can always\ndecrease the **false positive rate** (FPR) by shifting the threshold to the\nright. We can become more “conservative”. But this happens at the price of\nhigher **false negative rate** (FNR). Analogously, we can decrease the FNR by\nshifting the threshold to the left. But then again, this happens at the price\nof higher FPR. A bit on terminology: the FPR is the same as the probability\n\\\\(\\alpha\\\\) that we mentioned above. \\\\(1 - \\alpha\\\\) is also called the\n**specificity** of a test. The FNR is sometimes also called \\\\(\\beta\\\\), and\n\\\\(1 - \\beta\\\\) the **power** , **sensitivity** or **true positive rate** of a\ntest.\n\n__\n\nQuestion 6.8\n\nAt the end of Section 6.3, we learned about one- and two-sided tests. Why does\nthis distinction exist? Why don’t we always just use the two-sided test, which\nis sensitive to a larger class of alternatives?\n\n## 6.5 The t-test\n\nMany experimental measurements are reported as rational numbers, and the\nsimplest comparison we can make is between two groups, say, cells treated with\na substance compared to cells that are not. The basic test for such situations\nis the \\\\(t\\\\)-test. The test statistic is defined as\n\n\\\\[ t = c \\; \\frac{m_1-m_2}{s}, \\tag{6.4}\\\\]\n\nwhere \\\\(m_1\\\\) and \\\\(m_2\\\\) are the mean of the values in the two groups,\n\\\\(s\\\\) is the pooled standard deviation and \\\\(c\\\\) is a constant that\ndepends on the sample sizes, i.e., the numbers of observations \\\\(n_1\\\\) and\n\\\\(n_2\\\\) in the two groups. In formulas14,\n\n14 Everyone should try to remember Equation 6.4, whereas many people get by\nwith looking up Equation 6.5 when they need it.\n\n\\\\[ \\begin{align} m_g &= \\frac{1}{n_g} \\sum_{i=1}^{n_g} x_{g, i}\n\\quad\\quad\\quad g=1,2\\\\\\ s^2 &= \\frac{1}{n_1+n_2-2} \\left( \\sum_{i=1}^{n_1}\n\\left(x_{1,i} - m_1\\right)^2 + \\sum_{j=1}^{n_2} \\left(x_{2,j} - m_2\\right)^2\n\\right)\\\\\\ c &= \\sqrt{\\frac{n_1n_2}{n_1+n_2}} \\end{align} \\tag{6.5}\\\\]\n\nwhere \\\\(x_{g, i}\\\\) is the \\\\(i^{\\text{th}}\\\\) data point in the\n\\\\(g^{\\text{th}}\\\\) group. Let’s try this out with the `PlantGrowth` data from\nR’s **datasets** package.\n\n    \n    \n    library(\"ggbeeswarm\")\n    data(\"PlantGrowth\")\n    ggplot(PlantGrowth, aes(y = weight, x = group, col = group)) +\n      geom_beeswarm() + theme(legend.position = \"none\")\n    tt = with(PlantGrowth, \n           t.test(weight[group ==\"ctrl\"],\n                  weight[group ==\"trt2\"],\n                  var.equal = TRUE))\n    tt __\n    \n    \n        Two Sample t-test\n    \n    data:  weight[group == \"ctrl\"] and weight[group == \"trt2\"]\n    t = -2.134, df = 18, p-value = 0.04685\n    alternative hypothesis: true difference in means is not equal to 0\n    95 percent confidence interval:\n     -0.980338117 -0.007661883\n    sample estimates:\n    mean of x mean of y \n        5.032     5.526 \n\n[![](06-chap_files/figure-html/fig-testing-\nplantgrowth-1.png)](06-chap_files/figure-html/fig-testing-plantgrowth-1.png\n\"Figure 6.8: The PlantGrowth data.\")\n\nFigure 6.8: The `PlantGrowth` data.\n\n__\n\nQuestion 6.9\n\nWhat do you get from the comparison with `trt1`? What for `trt1` versus\n`trt2`?\n\nTo compute the p-value, the `t.test` function uses the asymptotic theory for\nthe \\\\(t\\\\)-statistic Equation 6.4; this theory states that under the null\nhypothesis of equal means in both groups, the statistic follows a known,\nmathematical distribution, the so-called \\\\(t\\\\)-distribution with\n\\\\(n_1+n_2-2\\\\) degrees of freedom. The theory uses additional technical\nassumptions, namely that the data are independent and come from a normal\ndistribution with the same standard deviation. We could be worried about these\nassumptions. Clearly they do not hold: weights are always positive, while the\nnormal distribution extends over the whole real axis. The question is whether\nthis deviation from the theoretical assumption makes a real difference. We can\nuse a permutation test to figure this out (we will discuss the idea behind\npermutation tests in a bit more detail in Section 6.5.1).\n\n__\n\nQuestion 6.10\n\nPlot the theoretical and simulated null distribution for the above\n\\\\(t\\\\)-test, in a similar fashion as in Figure 6.5 and Figure 6.6 for the\ncoin flips.\n\n__\n\nSolution\n\n__\n\n    \n    \n    plgr = dplyr::filter(PlantGrowth, group %in% c(\"ctrl\", \"trt2\"))\n    \n    alpha  = 0.05\n    xrange = 5 * c(-1, 1)\n    deckel = function(x) ifelse(x < xrange[1], xrange[1], ifelse(x > xrange[2], xrange[2], x))\n    \n    sim_null = tibble(\n      t = replicate(10000, t.test(weight ~ sample(group), var.equal = TRUE, data = plgr)$statistic)\n    )\n    sim_thresh = quantile(sim_null$t, c(alpha/2, 1-alpha/2))\n    sim_null = mutate(sim_null, \n      t = deckel(t),        # avoid warnings about out of range data\n      reject = ifelse(t <= sim_thresh[1], \"low\", ifelse(t > sim_thresh[2], \"high\", \"none\"))\n    ) \n    \n    theo_thresh = qt(c(alpha/2, 1-alpha/2), df =  nrow(plgr) - 2)\n    theo_null = tibble(\n      t = seq(-5, 5, by = 0.05),\n      density = dt (x = t, df = nrow(plgr)  - 2),\n      reject = ifelse(t <= theo_thresh[1], \"low\", ifelse(t > theo_thresh[2], \"high\", \"none\"))\n    )\n    \n    p1 = ggplot(sim_null, aes(x = t, col = reject, fill = reject)) +\n           geom_bar(stat = \"bin\", breaks = seq(-5, 5, by = 0.2)) \n    p2 = ggplot(theo_null, aes(x = t, y = density, col = reject, fill = reject)) +\n           geom_area() \n    \n    for (p in list(p1, p2))\n      print(p + \n            geom_vline(xintercept = tt$statistic, col = \"#101010\") +\n            scale_colour_manual(values = c(low = \"blue\", high = \"red\", none = \"darkgrey\")) +\n            scale_fill_manual(values = c(low = \"blue\", high = \"red\", none = \"darkgrey\")) + \n            xlim(xrange) + theme(legend.position = \"none\"))__\n\n[![](06-chap_files/figure-html/fig-testing-tnull-1.png)](06-chap_files/figure-\nhtml/fig-testing-tnull-1.png \"Figure 6.9: \")\n\nFigure 6.9\n\n[![](06-chap_files/figure-html/fig-testing-tnull-2.png)](06-chap_files/figure-\nhtml/fig-testing-tnull-2.png \"Figure 6.10: \")\n\nFigure 6.10\n\n__\n\nQuestion 6.11\n\nWould the solution to the preceding question get simpler if we considered the\nabsolute value of \\\\(t\\\\) instead of \\\\(t\\\\) itself?\n\n__\n\nSolution\n\n__\n\nYes, if we are not interested in the sign of the difference, we can directly\nwork on \\\\(\\text{abs}(t)\\\\), and have only a single rejection region instead\nof a lower and an upper one.\n\nThe \\\\(t\\\\)-test comes in multiple flavors, all of which can be chosen through\nparameters of the `t.test` function. What we did above is called a two-sided\ntwo-sample unpaired test with equal variance. _Two-sided_ refers to the fact\nthat we were open to reject the null hypothesis if the weight of the treated\nplants was either larger or smaller than that of the untreated ones.\n\n_Two-sample_ 15 indicates that we compared the means of two groups to each\nother; another option is to compare the mean of one group against a given,\nfixed number.\n\n15 It can be confusing that the term _sample_ has a different meaning in\nstatistics than in biology. In biology, a sample is a single specimen on which\nan assay is performed; in statistics, it is a set of measurements, e.g., the\n\\\\(n_1\\\\)-tuple \\\\(\\left(x_{1,1},...,x_{1,n_1}\\right)\\\\) in Equation 6.5,\nwhich can comprise several biological samples. In contexts where this double\nmeaning might create confusion, we refer to the data from a single biological\nsample as an _observation_.\n\n_Unpaired_ means that there was no direct 1:1 mapping between the measurements\nin the two groups. If, on the other hand, the data had been measured on the\nsame plants before and after treatment, then a paired test would be more\nappropriate, as it looks at the change of weight within each plant, rather\nthan their absolute weights.\n\n_Equal variance_ refers to the way the statistic Equation 6.4 is calculated.\nThat expression is most appropriate if the variances within each group are\nabout the same. If they are very different, an alternative form (Welch’s\n\\\\(t\\\\)-test) and associated asymptotic theory exist.\n\n**The independence assumption**. Now let’s try something peculiar: duplicate\nthe data.\n\n    \n    \n    with(rbind(PlantGrowth, PlantGrowth),\n           t.test(weight[group == \"ctrl\"],\n                  weight[group == \"trt2\"],\n                  var.equal = TRUE))__\n    \n    \n        Two Sample t-test\n    \n    data:  weight[group == \"ctrl\"] and weight[group == \"trt2\"]\n    t = -3.1007, df = 38, p-value = 0.003629\n    alternative hypothesis: true difference in means is not equal to 0\n    95 percent confidence interval:\n     -0.8165284 -0.1714716\n    sample estimates:\n    mean of x mean of y \n        5.032     5.526 \n\nNote how the estimates of the group means (and thus, of the difference) are\nunchanged, but the p-value is now much smaller! We can conclude two things\nfrom this:\n\n  * The power of the \\\\(t\\\\)-test depends on the sample size. Even if the underlying biological differences are the same, a dataset with more observations tends to give more significant results16.\n\n  * The assumption of independence between the measurements is really important. Blatant duplication of the same data is an extreme form of dependence, but to some extent the same thing happens if you mix up different levels of replication. For instance, suppose you had data from 8 plants, but measured the same thing twice on each plant (technical replicates), then pretending that these are now 16 independent measurements is wrong.\n\n16 You can also see this from the way the numbers \\\\(n_1\\\\) and \\\\(n_2\\\\)\nappear in Equation 6.5.\n\n### 6.5.1 Permutation tests\n\nWhat happened above when we contrasted the outcome of the parametric\n\\\\(t\\\\)-test with that of the permutation test applied to the\n\\\\(t\\\\)-statistic? It’s important to realize that these are two different\ntests, and the similarity of their outcomes is desirable, but coincidental. In\nthe parametric test, the null distribution of the \\\\(t\\\\)-statistic follows\nfrom the assumed null distribution of the data, a multivariate normal\ndistribution with unit covariance in the \\\\((n_1+n_2)\\\\)-dimensional space\n\\\\(\\mathbb{R}^{n_1+n_2}\\\\), and is continuous: the \\\\(t\\\\)-distribution. In\ncontrast, the permutation distribution of our test statistic is discrete, as\nit is obtained from the finite set of \\\\((n_1+n_2)!\\\\) permutations17 of the\nobservation labels, from a single instance of the data (the \\\\(n_1+n_2\\\\)\nobservations). All we assume here is that under the null hypothesis, the\nvariables \\\\(X_{1,1},...,X_{1,n_1},X_{2,1},...,X_{2,n_2}\\\\) are exchangeable.\nLogically, this assumption is implied by that of the parametric test, but is\nweaker. The permutation test employs the \\\\(t\\\\)-statistic, but not the\n\\\\(t\\\\)-distribution (nor the normal distribution). The fact that the two\ntests gave us a very similar result is a consequence of the Central Limit\nTheorem.\n\n17 Or a random subset, in case we want to save computation time.\n\n## 6.6 P-value hacking\n\nLet’s go back to the coin tossing example. We did not reject the null\nhypothesis (that the coin is fair) at a level of 5%—even though we “knew” that\nit is unfair. After all, `probHead` was chosen as 0.6 in Section 6.2. Let’s\nsuppose we now start looking at different test statistics. Perhaps the number\nof consecutive series of 3 or more heads. Or the number of heads in the first\n50 coin flips. And so on. At some point we will find a test that happens to\nresult in a small p-value, even if just by chance (after all, the probability\nfor the p-value to be less than 0.05 under the null hypothesis—fair coin—is\none in twenty). We just did what is called **p-value hacking** 18 ([Head et\nal. 2015](16-chap.html#ref-Head:PLoSBiol:2015:pvaluehacking)). You see what\nthe problem is: in our zeal to prove our point we tortured the data until some\nstatistic did what we wanted. A related tactic is **hypothesis switching** or\n**HARKing** – hypothesizing after the results are known: we have a dataset,\nmaybe we have invested a lot of time and money into assembling it, so we need\nresults. We come up with lots of different null hypotheses and test\nstatistics, test them, and iterate, until we can report something.\n\n18 <http://fivethirtyeight.com/features/science-isnt-broken>\n\nThese tactics violate the rules of hypothesis testing, as described in Section\n6.3, where we laid out one sequential procedure of choosing the hypothesis and\nthe test, and then collecting the data. But, as we saw in [Chapter\n2](02-chap.html), such tactics can be tempting in reality. With biological\ndata, we tend to have so many different choices for “normalising” the data,\ntransforming the data, trying to adjust for batch effects, removing outliers,\n…. The topic is complex and open-ended. Wasserstein and Lazar\n([2016](16-chap.html#ref-Wasserstein2016:ASA)) give a readable short summary\nof the problems with how p-values are used in science, and of some of the\nmisconceptions. They also highlight how p-values can be fruitfully used. The\nessential message is: be completely transparent about your data, what analyses\nwere tried, and how they were done. Provide the analysis code. Only with such\ncontextual information can a p-value be useful.\n\n**Avoid fallacy**. Keep in mind that our statistical test is never attempting\nto prove our null hypothesis is true - we are simply saying whether or not\nthere is evidence for it to be false. If a high p-value _were_ indicative of\nthe truth of the null hypothesis, we could formulate a completely crazy null\nhypothesis, do an utterly irrelevant experiment, collect a small amount of\ninconclusive data, find a p-value that would just be a random number between 0\nand 1 (and so with some high probability above our threshold \\\\(\\alpha\\\\))\nand, whoosh, our hypothesis would be demonstrated!\n\n## 6.7 Multiple testing\n\n__\n\nQuestion 6.12\n\nLook up [xkcd cartoon 882](http://xkcd.com/882). Why didn’t the newspaper\nreport the results for the other colors?\n\nThe quandary illustrated in the cartoon occurs with high-throughput data in\nbiology. And with force! You will be dealing not only with 20 colors of\njellybeans, but, say, with 20,000 genes that were tested for differential\nexpression between two conditions, or with 6 billion positions in the genome\nwhere a DNA mutation might have happened. So how do we deal with this? Let’s\nlook again at our table relating statistical test results with reality (Table\n6.1), this time framing everything in terms of many hypotheses.\n\nTest vs reality | Null hypothesis is true | \\\\(...\\\\) is false | Total  \n---|---|---|---  \n**Rejected** | \\\\(V\\\\) | \\\\(S\\\\) | \\\\(R\\\\)  \n**Not rejected** | \\\\(U\\\\) | \\\\(T\\\\) | \\\\(m-R\\\\)  \n**Total** | \\\\(m_0\\\\) | \\\\(m-m_0\\\\) | \\\\(m\\\\)  \n  \nTable 6.2: Types of error in multiple testing. The letters designate the\nnumber of times each type of error occurs.\n\n  * \\\\(m\\\\): total number of tests (and null hypotheses)\n\n  * \\\\(m_0\\\\): number of true null hypotheses\n\n  * \\\\(m-m_0\\\\): number of false null hypotheses\n\n  * \\\\(V\\\\): number of false positives (a measure of type I error)\n\n  * \\\\(T\\\\): number of false negatives (a measure of type II error)\n\n  * \\\\(S\\\\), \\\\(U\\\\): number of true positives and true negatives\n\n  * \\\\(R\\\\): number of rejections\n\nIn the rest of this chapter, we look at different ways of taking care of the\ntype I and II errors.\n\n## 6.8 The family wise error rate\n\nThe **family wise error rate** (FWER) is the probability that \\\\(V>0\\\\), i.e.,\nthat we make one or more false positive errors. We can compute it as the\ncomplement of making no false positive errors at all19.\n\n19 Assuming independence.\n\n\\\\[ \\begin{align} P(V>0) &= 1 - P(\\text{no rejection of any of $m_0$ nulls})\n\\\\\\ &= 1 - (1 - \\alpha)^{m_0} \\to 1 \\quad\\text{as } m_0\\to\\infty. \\end{align}\n\\tag{6.6}\\\\]\n\nFor any fixed \\\\(\\alpha\\\\), this probability is appreciable as soon as\n\\\\(m_0\\\\) is in the order of \\\\(1/\\alpha\\\\), and it tends towards 1 as\n\\\\(m_0\\\\) becomes larger. This relationship can have serious consequences for\nexperiments like DNA matching, where a large database of potential matches is\nsearched. For example, if there is a one in a million chance that the DNA\nprofiles of two people match by random error, and your DNA is tested against a\ndatabase of 800000 profiles, then the probability of a random hit with the\ndatabase (i.e., without you being in it) is:\n\n    \n    \n    1 - (1 - 1/1e6)^8e5 __\n    \n    \n    [1] 0.5506712\n\nThat’s pretty high. And once the database contains a few million profiles\nmore, a false hit is virtually unavoidable.\n\n__\n\nQuestion 6.13\n\nProve that the probability Equation 6.6 does indeed become very close to 1\nwhen \\\\(m_0\\\\) is large.\n\n### 6.8.1 Bonferroni method\n\nHow are we to choose the per-hypothesis \\\\(\\alpha\\\\) if we want FWER control?\nThe above computations suggest that the product of \\\\(\\alpha\\\\) with \\\\(m_0\\\\)\nmay be a reasonable ballpark estimate. Usually we don’t know \\\\(m_0\\\\), but we\nknow \\\\(m\\\\), which is an upper limit for \\\\(m_0\\\\), since \\\\(m_0\\le m\\\\). The\nBonferroni method is simply that if we want FWER control at level\n\\\\(\\alpha_{\\text{FWER}}\\\\), we should choose the per hypothesis threshold\n\\\\(\\alpha = \\alpha_{\\text{FWER}}/m\\\\). Let’s check this out on an example.\n\n    \n    \n    m = 10000\n    ggplot(tibble(\n      alpha = seq(0, 7e-6, length.out = 100),\n      p     = 1 - (1 - alpha)^m),\n      aes(x = alpha, y = p)) +  geom_line() +\n      xlab(expression(alpha)) +\n      ylab(\"Prob( no false rejection )\") +\n      geom_hline(yintercept = 0.05, col = \"red\")__\n\n[![](06-chap_files/figure-html/fig-testing-\nbonferroni-1.png)](06-chap_files/figure-html/fig-testing-bonferroni-1.png\n\"Figure 6.11: Bonferroni method. The plot shows the graph of eq-testing-\nbonferroni for m=10000 as a function of \\\\alpha.\")\n\nFigure 6.11: Bonferroni method. The plot shows the graph of 6.6 for\n\\\\(m=10000\\\\) as a function of \\\\(\\alpha\\\\).\n\nIn Figure 6.11, the black line intersects the red line (which corresponds to a\nvalue of 0.05) at \\\\(\\alpha=5.13\\times 10^{-6}\\\\), which is just a little bit\nmore than the value of \\\\(0.05/m\\\\) implied by the Bonferroni method.\n\n__\n\nQuestion 6.14\n\nWhy are the two values not exactly the same?\n\nA potential drawback of this method, however, is that if \\\\(m_0\\\\) is large,\nthe rejection threshold is very small. This means that the individual tests\nneed to be very powerful if we want to have any chance of detecting something.\nOften FWER control is too stringent, and would lead to an ineffective use of\nthe time and money that was spent to generate and assemble the data. We will\nnow see that there are more nuanced methods of controlling our type I error.\n\n## 6.9 The false discovery rate\n\nLet’s look at some data. We load up the RNA-Seq dataset `airway`, which\ncontains gene expression measurements (gene-level counts) of four primary\nhuman airway smooth muscle cell lines with and without treatment with\ndexamethasone, a synthetic glucocorticoid. We’ll use the\n**[DESeq2](https://bioconductor.org/packages/DESeq2/)** method that we’ll\ndiscuss in more detail in [Chapter 8](08-chap.html) For now it suffices to say\nthat it performs a test for differential expression for each gene.\nConceptually, the tested null hypothesis is similar to that of the\n\\\\(t\\\\)-test, although the details are slightly more involved since we are\ndealing with count data.\n\n    \n    \n    library(\"DESeq2\")\n    library(\"airway\")\n    data(\"airway\")\n    aw   = DESeqDataSet(se = airway, design = ~ cell + dex)\n    aw   = DESeq(aw)\n    awde = as.data.frame(results(aw)) |> dplyr::filter(!is.na(pvalue))__\n\n__\n\nTask\n\nHave a look at the content of `awde`.\n\n__\n\nTask\n\n(Optional) Consult the **[DESeq2](https://bioconductor.org/packages/DESeq2/)**\nvignette and/or [Chapter 8](08-chap.html) for more information on what the\nabove code chunk does.\n\n### 6.9.1 The p-value histogram\n\nThe **p-value histogram** is an important sanity check for any analysis that\ninvolves multiple tests. It is a mixture composed of two components:\n\n**null:** the p-values resulting from the tests for which the null hypothesis\nis true.\n\n**alt:** the p-values resulting from the tests for which the null hypothesis\nis not true. The relative size of these two components depends on the fraction\nof true nulls and true alternatives (i.e., on \\\\(m_0\\\\) and \\\\(m\\\\)), and it\ncan often be visually estimated from the histogram. If our analysis has high\nstatistical power, then the second component (“alt”) consists of mostly small\np-values, i.e., appears as a peak near 0 in the histogram; if the power is not\nhigh for some of the alternatives, we expect that this peak extends towards\nthe right, i.e., has a “shoulder”. For the “null” component, we expect (by\ndefinition of the p-value for continuous data and test statistics) a uniform\ndistribution in \\\\([0,1]\\\\). Let’s plot the histogram of p-values for the\nairway data.\n\n    \n    \n    ggplot(awde, aes(x = pvalue)) +\n      geom_histogram(binwidth = 0.025, boundary = 0)__\n\n[![](06-chap_files/figure-html/fig-testing-\nawpvhist-1.png)](06-chap_files/figure-html/fig-testing-awpvhist-1.png\n\"Figure 6.12: p-value histogram of for the airway data.\")\n\nFigure 6.12: p-value histogram of for the `airway` data.\n\nIn Figure 6.12 we see the expected mixture. We also see that the null\ncomponent is not exactly flat (uniform): this is because the data are counts.\nWhile these appear quasi-continuous when high, for the tests with low counts\nthe discreteness of the data and the resulting p-values shows up in the spikes\ntowards the right of the histogram.\n\nNow suppose we reject all tests with a p-value less than \\\\(\\alpha\\\\). We can\nvisually determine an estimate of the false discovery proportion with a plot\nsuch as in Figure 6.13, generated by the following code.\n\n    \n    \n    alpha = binw = 0.025\n    pi0 = 2 * mean(awde$pvalue > 0.5)\n    ggplot(awde,\n      aes(x = pvalue)) + geom_histogram(binwidth = binw, boundary = 0) +\n      geom_hline(yintercept = pi0 * binw * nrow(awde), col = \"blue\") +\n      geom_vline(xintercept = alpha, col = \"red\")__\n\n[![](06-chap_files/figure-html/fig-testing-\nawpvvisfdr-1.png)](06-chap_files/figure-html/fig-testing-awpvvisfdr-1.png\n\"Figure 6.13: Visual estimation of the FDR with the p-value histogram.\")\n\nFigure 6.13: Visual estimation of the FDR with the p-value histogram.\n\nWe see that there are 4772 p-values in the first bin \\\\([0,\\alpha]\\\\), among\nwhich we expect around 945 to be nulls (as indicated by the blue line). Thus\nwe can estimate the fraction of false rejections as\n\n    \n    \n    pi0 * alpha / mean(awde$pvalue <= alpha)__\n    \n    \n    [1] 0.1980092\n\nThe **false discovery rate** (FDR) is defined as\n\n\\\\[ \\text{FDR} = \\text{E}\\\\!\\left [\\frac{V}{\\max(R, 1)}\\right ], \\tag{6.7}\\\\]\n\nwhere \\\\(R\\\\) and \\\\(V\\\\) are as in Table 6.2. The expression in the\ndenominator makes sure that the FDR is well-defined even if \\\\(R=0\\\\) (in that\ncase, \\\\(V=0\\\\) by implication). Note that the FDR becomes identical to the\nFWER if all null hypotheses are true, i.e., if \\\\(V=R\\\\). \\\\(\\text{E[ ]}\\\\)\nstands for the **expected value**. That means that the FDR is not a quantity\nassociated with a specific outcome of \\\\(V\\\\) and \\\\(R\\\\) for one particular\nexperiment. Rather, given our choice of tests and associated rejection rules\nfor them, it is the average20 proportion of type I errors out of the\nrejections made, where the average is taken (at least conceptually) over many\nreplicate instances of the experiment.\n\n20 Since the FDR is an expectation value, it does not provide worst case\ncontrol: in any single experiment, the so-called false discovery proportion\n(FDP), that is the realized value \\\\(v/r\\\\) (without the \\\\(\\text{E[ ]}\\\\)),\ncould be much higher or lower.\n\n### 6.9.2 The Benjamini-Hochberg algorithm for controlling the FDR\n\nThere is a more elegant alternative to the “visual FDR” method of the last\nsection. The procedure, introduced by Benjamini and Hochberg\n([1995](16-chap.html#ref-BH:1995)) has these steps:\n\n  * First, order the p-values in increasing order, \\\\(p_{(1)} ... p_{(m)}\\\\)\n\n  * Then for some choice of \\\\(\\varphi\\\\) (our target FDR), find the largest value of \\\\(k\\\\) that satisfies: \\\\(p_{(k)} \\leq \\varphi \\, k / m\\\\)\n\n  * Finally reject the hypotheses \\\\(1, ..., k\\\\)\n\nWe can see how this procedure works when applied to our RNA-Seq p-values\nthrough a simple graphical illustration:\n\n    \n    \n    phi  = 0.10\n    awde = mutate(awde, rank = rank(pvalue))\n    m    = nrow(awde)\n    \n    ggplot(dplyr::filter(awde, rank <= 7000), aes(x = rank, y = pvalue)) +\n      geom_line() + geom_abline(slope = phi / m, col = \"red\")__\n\n[![](06-chap_files/figure-html/fig-testing-BH-1.png)](06-chap_files/figure-\nhtml/fig-testing-BH-1.png \"Figure 6.14: Visualization of the Benjamini-\nHochberg procedure. Shown is a zoom-in to the 7000 lowest p-values.\")\n\nFigure 6.14: Visualization of the Benjamini-Hochberg procedure. Shown is a\nzoom-in to the 7000 lowest p-values.\n\nThe method finds the rightmost point where the black (our p-values) and red\nlines (slope \\\\(\\varphi / m\\\\)) intersect. Then it rejects all tests to the\nleft.\n\n    \n    \n    kmax = with(arrange(awde, rank),\n             last(which(pvalue <= phi * rank / m)))\n    kmax __\n    \n    \n    [1] 4099\n\n__\n\nQuestion 6.15\n\nCompare the value of `kmax` with the number of 4772 from above (Figure 6.13).\nWhy are they different?\n\n__\n\nQuestion 6.16\n\nLook at the code associated with the option `method=\"BH\"` of the `p.adjust`\nfunction that comes with R. How does it compare to what we did above?\n\n__\n\nQuestion 6.17\n\n**Schweder and Spj øtvoll plot**: check out Figures 1–3 in Schweder and\nSpjøtvoll ([1982](16-chap.html#ref-SchwederSpjotvoll1982)). Make a similar\nplot for the data in `awde`. How does it relate to Figures 6.14 and 6.13?\n\n__\n\nSolution\n\n__\n\nThirteen years before Benjamini and Hochberg ([1995](16-chap.html#ref-\nBH:1995)), Schweder and Spjøtvoll ([1982](16-chap.html#ref-\nSchwederSpjotvoll1982)) suggested a diagnostic plot of the observed\n\\\\(p\\\\)-values that permits estimation of the fraction of true null\nhypotheses. For a series of hypothesis tests \\\\(H_1, ..., H_m\\\\) with\n\\\\(p\\\\)-values \\\\(p_i\\\\), they suggested plotting\n\n\\\\[ \\left( 1-p_i, N(p_i) \\right) \\mbox{ for } i \\in 1, ..., m, \\tag{6.8}\\\\]\n\nwhere \\\\(N(p)\\\\) is the number of \\\\(p\\\\)-values greater than \\\\(p\\\\). An\napplication of this diagnostic plot to `awde$pvalue` is shown in Figure 6.15.\nWhen all null hypotheses are true, each of the \\\\(p\\\\)-values is uniformly\ndistributed in \\\\([0,1]\\\\), Consequently, the empirical cumulative\ndistribution of the sample \\\\((p_1, ..., p_m)\\\\) is expected to be close to\nthe line \\\\(F(t)=t\\\\). By symmetry, the same applies to \\\\((1 - p_1, ..., 1 -\np_m)\\\\). When (without loss of generality) the first \\\\(m_0\\\\) null hypotheses\nare true and the other \\\\(m-m_0\\\\) are false, the empirical cumulative\ndistribution of \\\\((1-p_1, ..., 1-p_{m_0})\\\\) is again expected to be close to\nthe line \\\\(F_0(t)=t\\\\). The empirical cumulative distribution of\n\\\\((1-p_{m_0+1}, ..., 1-p_{m})\\\\), on the other hand, is expected to be close\nto a function \\\\(F_1(t)\\\\) which stays below \\\\(F_0\\\\) but shows a steep\nincrease towards 1 as \\\\(t\\\\) approaches \\\\(1\\\\). In practice, we do not know\nwhich of the null hypotheses are true, so we only observe a mixture whose\nempirical cumulative distribution is expected to be close to\n\n\\\\[ F(t) = \\frac{m_0}{m} F_0(t) + \\frac{m-m_0}{m} F_1(t). \\tag{6.9}\\\\]\n\nSuch a situation is shown in Figure 6.15. If \\\\(F_1(t)/F_0(t)\\\\) is small for\nsmall \\\\(t\\\\) (i.e., the tests have reasonable power), then the mixture\nfraction \\\\(\\frac{m_0}{m}\\\\) can be estimated by fitting a line to the left-\nhand portion of the plot, and then noting its height on the right. Such a fit\nis shown by the red line. Here, we focus on those tests for which the count\ndata are not all very small numbers (`baseMean>=1`), since for these the\np-value null distribution is sufficiently close to uniform (i.e., does not\nshow the discreteness mentioned above), but you could try the making the same\nplot on all of the genes.\n\n    \n    \n    awdef = awde |>\n      dplyr::filter(baseMean >=1) |> \n      arrange(pvalue) |>\n      mutate(oneminusp = 1 - pvalue,\n             N = n() - row_number())\n    jj = round(nrow(awdef) * c(1, 0.5))\n    slope = with(awdef, diff(N[jj]) / diff(oneminusp[jj]))\n    ggplot(awdef) +\n      geom_point(aes(x = oneminusp, y = N), size = 0.15) + \n      xlab(expression(1-p[i])) +\n      ylab(expression(N(p[i]))) +\n      geom_abline(intercept = 0, slope = slope, col = \"red3\") +\n      geom_hline(yintercept = slope, linetype = \"dotted\") +\n      geom_vline(xintercept = 1, linetype = \"dotted\") +\n      geom_text(x = 0, y = slope, label = paste(round(slope)), \n                hjust = -0.1, vjust = -0.25) __\n\n[![](06-chap_files/figure-html/fig-testing-\nSchwederSpjotvoll-1.png)](06-chap_files/figure-html/fig-testing-\nSchwederSpjotvoll-1.png \"Figure 6.15: Schweder and Spjtvoll plot, as described\nin the answer to Question wrn-testing-SchwederSpjotvoll.\")\n\nFigure 6.15: Schweder and Spjtvoll plot, as described in the answer to\nQuestion 6.17.\n\nThere are 22853 rows in `awdef`, thus, according to this simple estimate,\nthere are 22853-17302=5551 alternative hypotheses.\n\n## 6.10 The local FDR\n\n[![](imgs/xkcd1132.png)](imgs/xkcd1132.png \"Figure 6.16: From\nhttp://xkcd.com/1132 – While the frequentist only has the currently available\ndata, the Bayesian can draw on her understanding of the world or on previous\nexperience. As a Bayesian, she would know enough about physics to understand\nthat our sun’s mass is too small to become a nova. Even if she does not know\nphysics, she might be an empirical Bayesian and draw her prior from a myriad\nprevious days where the sun did not go nova.\")\n\nFigure 6.16: From <http://xkcd.com/1132> – While the frequentist only has the\ncurrently available data, the Bayesian can draw on her understanding of the\nworld or on previous experience. As a Bayesian, she would know enough about\nphysics to understand that our sun’s mass is too small to become a nova. Even\nif she does not know physics, she might be an **empirical Bayesian** and draw\nher prior from a myriad previous days where the sun did not go nova.\n\nWhile the xkcd cartoon in the chapter’s opening figure ends with a rather\nsinister interpretation of the multiple testing problem as a way to accumulate\nerrors, Figure 6.16 highlights the multiple testing opportunity: when we do\nmany tests, we can use the multiplicity to increase our understanding beyond\nwhat’s possible with a single test.\n\n[![](06-chap_files/figure-html/fig-testing-lfdr-1.png)](06-chap_files/figure-\nhtml/fig-testing-lfdr-1.png \"Figure 6.17: Local false discovery rate and the\ntwo-group model, with some choice of f_{\\\\text{alt}}\\(p\\), and \\\\pi_0=0.6;\ndensities \\(top\\) and distribution functions \\(bottom\\).\")\n\nFigure 6.17: Local false discovery rate and the two-group model, with some\nchoice of \\\\(f_{\\text{alt}}(p)\\\\), and \\\\(\\pi_0=0.6\\\\); densities (top) and\ndistribution functions (bottom).\n\nLet’s get back to the histogram in Figure 6.13. Conceptually, we can think of\nit in terms of the so-called two-groups model ([Efron 2010](16-chap.html#ref-\nEfron2010)):\n\n\\\\[ f(p)= \\pi_0 + (1-\\pi_0) f_{\\text{alt}}(p), \\tag{6.10}\\\\]\n\nHere, \\\\(f(p)\\\\) is the density of the distribution (what the histogram would\nlook like with an infinite amount of data and infinitely small bins),\n\\\\(\\pi_0\\\\) is a number between 0 and 1 that represents the size of the\nuniform component, and \\\\(f_{\\text{alt}}\\\\) is the alternative component. This\nis a mixture model, as we already saw in [Chapter 4](04-chap.html). The\nmixture densities and the marginal density \\\\(f(p)\\\\) are visualized in the\nupper panel of Figure 6.17: the blue areas together correspond to the graph of\n\\\\(f_{\\text{alt}}(p)\\\\), the grey areas to that of \\\\(f_{\\text{null}}(p) =\n\\pi_0\\\\). If we now consider one particular cutoff \\\\(p\\\\) (say, \\\\(p=0.1\\\\)\nas in Figure 6.17), then we can compute the probability that a hypothesis that\nwe reject at this cutoff is a false positive, as follows. We decompose the\nvalue of \\\\(f\\\\) at the cutoff (red line) into the contribution from the nulls\n(light red, \\\\(\\pi_0\\\\)) and from the alternatives (darker red, \\\\((1-\\pi_0)\nf_{\\text{alt}}(p)\\\\)). The **local false discovery rate** is then\n\n\\\\[ \\text{fdr}(p) = \\frac{\\pi_0}{f(p)}. \\tag{6.11}\\\\]\n\nBy definition this quantity is between 0 and 1. Note how the \\\\(\\text{fdr}\\\\)\nin Figure 6.17 is a monotonically increasing function of \\\\(p\\\\), and this\ngoes with our intuition that the fdr should be lowest for the smallest \\\\(p\\\\)\nand then gradually get larger, until it reaches 1 at the very right end. We\ncan make a similar decomposition not only for the red line, but also for the\narea under the curve. This is\n\n\\\\[ F(p) = \\int_0^p f(t)\\,dt, \\tag{6.12}\\\\]\n\nand the ratio of the dark grey area (that is, \\\\(\\pi_0\\\\) times \\\\(p\\\\)) to\nthe overall area \\\\(F(p)\\\\) is the **tail area false discovery rate** (Fdr21),\n\n21 The convention is to use the lower case abbreviation fdr for the local, and\nthe abbreviation Fdr for the tail-area false discovery rate in the context of\nthe two-groups model Equation 6.10. The abbreviation FDR is used for the\noriginal definition Equation 6.7, which is a bit more general, namely, it does\nnot depend on the modelling assumptions of Equation 6.10.\n\n\\\\[ \\text{Fdr}(p) = \\frac{\\pi_0\\,p}{F(p)}. \\tag{6.13}\\\\]\n\nWe’ll use the data version of \\\\(F\\\\) for diagnostics in Figure 6.21.\n\nThe packages **[qvalue](https://bioconductor.org/packages/qvalue/)** and\n**[fdrtool](https://cran.r-project.org/web/packages/fdrtool/)** offer\nfacilities to fit these models to data.\n\n    \n    \n    library(\"fdrtool\")\n    ft = fdrtool(awde$pvalue, statistic = \"pvalue\")__\n\nIn **[fdrtool](https://cran.r-project.org/web/packages/fdrtool/)** , what we\ncalled \\\\(\\pi_0\\\\) above is called `eta0`:\n\n    \n    \n    ft$param[,\"eta0\"]__\n    \n    \n         eta0 \n    0.8822922 \n\n__\n\nQuestion 6.18\n\nWhat do the plots that are produced by the above call to `fdrtool` show?\n\n__\n\nTask\n\nExplore the other elements of the _list_ `ft`.\n\n__\n\nQuestion 6.19\n\nWhat does the _empirical_ in empirical Bayes methods stand for?\n\n### 6.10.1 Local versus total\n\nThe FDR (or the Fdr) is a set property. It is a single number that applies to\na whole set of rejections made in the course of a multiple testing analysis.\nIn contrast, the fdr is a local property. It applies to an individual\nhypothesis. Recall Figure 6.17, where the fdr was computed for each point\nalong the \\\\(x\\\\)-axis of the density plot, whereas the Fdr depends on the\nareas to the left of the red line.\n\n__\n\nQuestion 6.20\n\nCheck out the concepts of _total cost_ and _marginal cost_ in economics. Can\nyou see an analogy with Fdr and fdr?\n\n__\n\nSolution\n\n__\n\nFor a production process that produces a set of \\\\(m\\\\) products, the total\ncost is the sum of the all costs involved. The average cost of a product is a\nhypothetical quantity, computed as the total cost divided by \\\\(m\\\\). The\nmarginal cost is the cost of making one additional product, and is often very\ndifferent from the average cost. For instance, learning to play a single\nBeethoven sonata on the piano may take an uninitiated person a substantial\namount of time, but then playing it once more requires comparatively little\nadditional effort: the marginal costs are much less than the fixed (and thus\nthe total) costs. An example for marginal costs that are higher than the\naverage costs is running: putting on your shoes and going out for a 10km run\nmay be quite tolerable (perhaps even fun) to most people, whereas each\nadditional 10km could add disproportional discomfort.\n\n### 6.10.2 Terminology\n\nHistorically, the terms _multiple testing correction_ and _adjusted p-value_\nhave been used for process and output. In the context of false discovery\nrates, these terms are not helpful, if not confusing. We advocate avoiding\nthem. They imply that we start out with a set of p-values \\\\((p_1,...,p_m)\\\\),\napply some canonical procedure, and obtain a set of “corrected” or “adjusted”\np-values \\\\((p_1^{\\text{adj}},...,p_m^{\\text{adj}})\\\\). However, the output of\nthe Benjamini-Hochberg method is not p-values, and neither are the FDR, Fdr or\nthe fdr. Remember that FDR and Fdr are set properties, and associating them\nwith an individual test makes as much sense as confusing average and marginal\ncosts. Fdr and fdr also depend on a substantial amount of modelling\nassumptions. In the next session, you will also see that the method of\nBenjamini-Hochberg is not the only game in town, and that there are important\nand useful extensions, which further displace any putative direct\ncorrespondence between the set of hypotheses and p-values that are input into\na multiple testing procedure, and its outputs.\n\n## 6.11 Independent hypothesis weighting\n\nThe Benjamini-Hochberg method and the two-groups model, as we have seen them\nso far, implicitly assume _exchangeability_ of the hypotheses: all we use are\nthe p-values. Beyond these, we do not take into account any additional\ninformation. This is not always optimal, and here we’ll study ways of how to\nimprove on this.\n\nLet’s look at an example. Intuitively, the signal-to-noise ratio for genes\nwith larger numbers of reads mapped to them should be better than for genes\nwith few reads, and that should affect the power of our tests. We look at the\nmean of normalized counts across observations. In the\n**[DESeq2](https://bioconductor.org/packages/DESeq2/)** package this quantity\nis called the `baseMean`.\n\n    \n    \n    awde$baseMean[1]__\n    \n    \n    [1] 708.6022\n    \n    \n    cts = counts(aw, normalized = TRUE)[1, ]\n    cts __\n    \n    \n    SRR1039508 SRR1039509 SRR1039512 SRR1039513 SRR1039516 SRR1039517 SRR1039520 \n      663.3142   499.9070   740.1528   608.9063   966.3137   748.3722   836.2487 \n    SRR1039521 \n      605.6024 \n    \n    \n    mean(cts)__\n    \n    \n    [1] 708.6022\n\nNext we produce the histogram of this quantity across genes, and plot it\nagainst the p-values (Figures 6.18 and 6.19).\n\n    \n    \n    ggplot(awde, aes(x = asinh(baseMean))) +\n      geom_histogram(bins = 60)__\n\n[![](06-chap_files/figure-html/fig-testing-basemean-\nhist-1.png)](06-chap_files/figure-html/fig-testing-basemean-hist-1.png\n\"Figure 6.18: Histogram of baseMean. We see that it covers a large dynamic\nrange, from close to 0 to around 330000.\")\n\nFigure 6.18: Histogram of `baseMean`. We see that it covers a large dynamic\nrange, from close to 0 to around 330000.\n\n    \n    \n    ggplot(awde, aes(x = rank(baseMean), y = -log10(pvalue))) +\n      geom_hex(bins = 60) +\n      theme(legend.position = \"none\")__\n\n[![](06-chap_files/figure-html/fig-testing-basemean-\nscp-1.png)](06-chap_files/figure-html/fig-testing-basemean-scp-1.png\n\"Figure 6.19: Scatterplot of the rank of baseMean versus the negative\nlogarithm of the p-value. For small values of baseMean, no small p-values\noccur. Only for genes whose read counts across all observations have a certain\nsize, the test for differential expression has power to come out with a small\np-value.\")\n\nFigure 6.19: Scatterplot of the rank of `baseMean` versus the negative\nlogarithm of the p-value. For small values of `baseMean`, no small p-values\noccur. Only for genes whose read counts across all observations have a certain\nsize, the test for differential expression has power to come out with a small\np-value.\n\n__\n\nQuestion 6.21\n\nWhy did we use the \\\\(\\text{asinh}\\\\) transformation for the histogram? How\ndoes it look like with no transformation, the logarithm, the shifted\nlogarithm, i.e., \\\\(\\log(x+\\text{const.})\\\\)?\n\n__\n\nQuestion 6.22\n\nIn the scatterplot, why did we use \\\\(-\\log_{10}\\\\) for the p-values? Why the\nrank transformation for the `baseMean`?\n\nFor convenience, we discretize `baseMean` into a factor variable `group`,\nwhich corresponds to six equal-sized groups.\n\n    \n    \n    awde = mutate(awde, stratum = cut(baseMean, include.lowest = TRUE,\n      breaks = signif(quantile(baseMean,probs=seq(0,1,length.out=7)),2)))__\n\nIn Figures 6.20 and 6.21 we see the histograms of p-values and the ECDFs\nstratified by `stratum`.\n\n    \n    \n    ggplot(awde, aes(x = pvalue)) + facet_wrap( ~ stratum, nrow = 4) +\n      geom_histogram(binwidth = 0.025, boundary = 0)__\n\n[![](06-chap_files/figure-html/fig-testing-awde-stratified-\nhist-1.png)](06-chap_files/figure-html/fig-testing-awde-stratified-hist-1.png\n\"Figure 6.20: p-value histograms of the airway data, stratified into equally\nsized groups defined by increasing value of baseMean.\")\n\nFigure 6.20: p-value histograms of the airway data, stratified into equally\nsized groups defined by increasing value of `baseMean`.\n\n    \n    \n    ggplot(awde, aes(x = pvalue, col = stratum)) +\n      stat_ecdf(geom = \"step\") + theme(legend.position = \"bottom\")__\n\n[![](06-chap_files/figure-html/fig-testing-awde-stratified-\necdf-1.png)](06-chap_files/figure-html/fig-testing-awde-stratified-ecdf-1.png\n\"Figure 6.21: Same data as in Figure fig-testing-awde-stratified-hist, shown\nwith ECDFs.\")\n\nFigure 6.21: Same data as in Figure 6.20, shown with ECDFs.\n\nIf we were to fit the two-group model to these strata separately, we would get\nquite different estimates for \\\\(\\pi_0\\\\) and \\\\(f_{\\text{alt}}\\\\). For the\nmost lowly expressed genes, the power of the\n**[DESeq2](https://bioconductor.org/packages/DESeq2/)** -test is low, and the\np-values essentially all come from the null component. As we go higher in\naverage expression, the height of the small-p-values peak in the histograms\nincreases, reflecting the increasing power of the test.\n\nCan we use that to improve our handling of the multiple testing? It turns out\nthat this is possible. One approach is **independent hypothesis weighting**\n(IHW) ([Ignatiadis et al. 2016](16-chap.html#ref-Ignatiadis:2016); [Ignatiadis\nand Huber 2021](16-chap.html#ref-Ignatiadis:JRSSB:2021))22.\n\n22 There are a number of other approaches, see e.g., a benchmark study by\nKorthauer et al. ([2019](16-chap.html#ref-Korthauer:GB:2019)) or the citations\nin the paper by Ignatiadis and Huber ([2021](16-chap.html#ref-\nIgnatiadis:JRSSB:2021)).\n\n    \n    \n    library(\"IHW\")\n    ihw_res = ihw(awde$pvalue, awde$baseMean, alpha = 0.1)\n    rejections(ihw_res)__\n    \n    \n    [1] 4892\n\nLet’s compare this to what we get from the ordinary (unweighted) Benjamini-\nHochberg method:\n\n    \n    \n    padj_BH = p.adjust(awde$pvalue, method = \"BH\")\n    sum(padj_BH < 0.1)__\n    \n    \n    [1] 4099\n\nWith hypothesis weighting, we get more rejections. For these data, the\ndifference is notable though not spectacular; this is because their signal-to-\nnoise ratio is already quite high. In other situations, where there is less\npower to begin with (e.g., where there are fewer replicates, the data are more\nnoisy, or the effect of the treatment is less drastic), the difference from\nusing IHW can be more pronounced.\n\nWe can have a look at the weights determined by the `ihw` function (Figure\n6.22).\n\n    \n    \n    plot(ihw_res)__\n\n[![](06-chap_files/figure-html/fig-testing-\nihwplot-1.png)](06-chap_files/figure-html/fig-testing-ihwplot-1.png\n\"Figure 6.22: Hypothesis weights determined by the ihw function. Here the\nfunction’s default settings chose 22 strata, while in our manual exploration\nabove \\(Figures fig-testing-awde-stratified-hist, fig-testing-awde-stratified-\necdf\\) we had used 6; in practice, this is a minor detail.\")\n\nFigure 6.22: Hypothesis weights determined by the `ihw` function. Here the\nfunction’s default settings chose 22 strata, while in our manual exploration\nabove (Figures 6.20, 6.21) we had used 6; in practice, this is a minor detail.\n\nIntuitively, what happens here is that IHW chooses to put more weight on the\nhypothesis strata with higher `baseMean`, and low weight on those with very\nlow counts. The Benjamini-Hochberg method has a certain type-I error budget,\nand rather than spreading it equally among all hypotheses, here we take it\naway from those strata that have little change of small fdr anyway, and\n“invest” it in strata where many hypotheses can be rejected at small fdr.\n\n__\n\nQuestion 6.23\n\nWhy does Figure 6.22 show 5 curves, rather than only one?\n\nSuch possibilities for stratification by an additional summary statistic\nbesides the p-value—in our case, the `baseMean`—exist in many multiple testing\nsituations. Informally, we need such a so-called _covariate_ to be\n\n  * statistically independent from our p-values under the null, but\n\n  * informative of the prior probability \\\\(\\pi_0\\\\) and/or the power of the test (the shape of the alternative density, \\\\(f_{\\text{alt}}\\\\)) in the two-groups model.\n\nThese requirements can be assessed through diagnostic plots as in Figures\n6.18—6.21.\n\n## 6.12 Summary of this chapter\n\nWe have explored the concepts behind _single hypothesis testing_ and then\nmoved on to _multiple testing_. We have seen how some of the limitations of\ninterpreting a single p-value from a single test can be overcome once we are\nable to consider a whole distribution of outcomes from many tests. We have\nalso seen that there are often additional summary statistics of our data,\nbesides the p-values. We called them informative covariates, and we saw how we\ncan use them to weigh the p-values and overall get more (or better)\ndiscoveries.\n\nThe usage of hypothesis testing in the _multiple testing_ scenario is quite\ndifferent from that in the _single test_ case: for the latter, the hypothesis\ntest might literally be the final result, the culmination of a long and\nexpensive data acquisition campaign (ideally, with a prespecified hypothesis\nand data analysis plan). In the multiple testing case, its outcome will often\njust be an intermediate step: a subset of most worthwhile hypotheses selected\nby screening a large initial set. This subset is then followed up by more\ncareful analyses.\n\nWe have seen the concept of the _false discovery rate_ (FDR). It is important\nto keep in mind that this is an average property, for the subset of hypotheses\nthat were selected. Like other averages, it does not say anything about the\nindividual hypotheses. Then there is the concept of the _local false discovery\nrate_ (fdr), which indeed does apply to an individual hypothesis. The local\nfalse discovery rate is however quite unrelated to the p-value, as the two-\ngroup model showed us. Much of the confusion and frustration about p-values\nseems to come from the fact that people would like to use them for purposes\nthat the fdr is made for. It is perhaps a historical aberration that so much\nof applied sciences focuses on p-values and not local false discovery rate. On\nthe other hand, there are also practical reasons, since a p-value is readily\ncomputed, whereas a fdr is difficult to estimate or control from data without\nmaking strong modelling assumptions.\n\nWe saw the importance of diagnostic plots, in particular, to always look at\nthe p-value histograms when encountering a multiple testing analysis.\n\n## 6.13 Further reading\n\n  * A comprehensive text book treatment of multiple testing is given by Efron ([2010](16-chap.html#ref-Efron2010)).\n\n  * Outcome switching in clinical trials: <http://compare-trials.org>\n\n  * For hypothesis weighting, the **[IHW](https://bioconductor.org/packages/IHW/)** vignette, the IHW paper ([Ignatiadis et al. 2016](16-chap.html#ref-Ignatiadis:2016)) and the references therein.\n\n## 6.14 Exercises\n\n__\n\nExercise 6.1\n\nIdentify an application from your scientific field of expertise that relies on\nmultiple testing. Find an exemplary dataset and plot the histogram of\np-values. Are the hypotheses all exchangeable, or is there one or more\ninformative covariates? Plot the stratified histograms.\n\n__\n\nExercise\n\nWhy do mathematical statisticians focus so much on the null hypothesis of a\ntest, compared to the alternative hypothesis?\n\n__\n\nExercise 6.2\n\nHow can we ever prove that the null hypothesis is true? Or that the\nalternative is true?\n\n__\n\nExercise 6.3\n\nMake a less extreme example of correlated test statistics than the data\nduplication at the end of Section 6.5. Simulate data with true null hypotheses\nonly, and let the data morph from having completely independent replicates\n(columns) to highly correlated as a function of some continuous-valued control\nparameter. Check type-I error control (e.g., with the p-value histogram) as a\nfunction of this control parameter.\n\n__\n\nExercise 6.4\n\nFind an example in the published literature that looks as if p-value hacking,\noutcome switching, HARKing played a role.\n\n__\n\nExercise 6.5\n\nThe FDR is an expectation value, i.e., it is used if we want to control the\naverage behavior of a procedure. Are there methods for worst case control?\n\n__\n\nExercise 6.6\n\nWhat is the memory and time complexity of the Benjamini-Hochberg algorithm?\nHow about the IHW method? Can you fit polynomial functions as a function of\nthe number of tests \\\\(m\\\\)? Hint: Simulate data with increasing numbers of\nhypothesis tests, measure time and memory consumption with functions such as\n`pryr::object_size` or `microbenchmark` from the eponymous package, and plot\nthese against \\\\(m\\\\) in a double-logarithmic plot.\n\nAltman, Naomi, and Martin Krzywinski. 2017. “Points of Significance:\nInterpreting p Values.” _Nature Methods_ 14 (3): 213–14.\n<https://doi.org/10.1038/nmeth.4210>.\n\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery\nRate: A Practical and Powerful Approach to Multiple Testing.” _Journal of the\nRoyal Statistical Society B_ 57: 289–300.\n\nEfron, Bradley. 2010. _Large-Scale Inference: Empirical Bayes Methods for\nEstimation, Testing, and Prediction_. Cambridge University Press.\n\nHead, Megan L, Luke Holman, Rob Lanfear, Andrew T Kahn, and Michael D\nJennions. 2015. “The Extent and Consequences of p-Hacking in Science.” _PLoS\nBiology_ 13 (3): e1002106.\n\nIgnatiadis, Nikolaos, and Wolfgang Huber. 2021. “Covariate Powered Cross-\nWeighted Multiple Testing.” _Journal of the Royal Statistical Society: Series\nB_ 83: 720–51. <https://doi.org/10.1111/rssb.12411>.\n\nIgnatiadis, Nikolaos, Bernd Klaus, Judith Zaugg, and Wolfgang Huber. 2016.\n“Data-Driven Hypothesis Weighting Increases Detection Power in Genome-Scale\nMultiple Testing.” _Nature Methods_ 13: 577–80.\n\nKorthauer, K., P. K. Kimes, C. Duvallet, A. Reyes, A. Subramanian, M. Teng, C.\nShukla, E. J. Alm, and S. C. Hicks. 2019. “A practical guide to methods\ncontrolling false discoveries in computational biology.” _Genome Biology_ 20\n(1): 118.\n\nSchweder, T., and E. Spjøtvoll. 1982. “Plots of P-values to Evaluate Many\nTests Simultaneously.” _Biometrika_ 69: 493–502.\n<https://doi.org/10.1093/biomet/69.3.493>.\n\nStorey, John D. 2003. “The Positive False Discovery Rate: A Bayesian\nInterpretation and the q-Value.” _The Annals of Statistics_ 31 (6).\n<https://doi.org/10.1214/aos/1074290335>.\n\nWasserstein, Ronald L, and Nicole A Lazar. 2016. “The ASA’s Statement on\np-Values: Context, Process, and Purpose.” _The American Statistician_.\n\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["msmb.css"],"toc":true,"output-file":"06-chap.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}