{"title":"11.1 Goals for this chapter","markdown":{"headingText":"11.1 Goals for this chapter","containsRefs":false,"markdown":"![](imgs/chap13-colorLabelscellbodies.png)\n\nImages are a rich source of data. In this chapter, we will see how\nquantitative information can be extracted from images, and how we can use\nstatistical methods to summarize and understand the data. The goal of the\nchapter is to show that getting started working with image data is easy – if\nyou are able to handle the basic R environment, you are ready to start working\nwith images. That said, this chapter is not a general introduction to image\nanalysis. The field is extensive; it touches many areas of signal processing,\ninformation theory, mathematics, engineering and computer science, and there\nare excellent books that present a systematic overview.\n\nWe will mainly study series of two-dimensional images, in particular, images\nof cells. We will learn how to identify the cells’ positions and shapes and\nhow to quantitatively measure characteristics of the identified shapes and\npatterns, such as sizes, intensities, color distributions and relative\npositions. Such information can then be used in down-stream analyses: for\ninstance, we can compare cells between different conditions, say under the\neffect of different drugs, or in different stages of differentiation and\ngrowth; or we can measure how the objects in the image relate to each other,\ne.g., whether they like to cluster together or repel each other, or whether\ncertain characteristics tend to be shared between neighboring objects,\nindicative of cell-to-cell communication. In the language of genetics, what\nthis means is that we can use images as complex phenotypes or as multivariate\nquantitative traits.\n\nWe will here not touch upon image analysis in more than two dimensions: we\nwon’t consider 3D segmentation and registration, nor temporal tracking. These\nare sophisticated tasks for which specialized software would likely perform\nbetter than what we could assemble in the scope of this chapter.\n\nThere are similarities between data from high-throughput imaging and other\nhigh-throughput data in genomics. Batch effects tend to play a role, for\ninstance because of changes in staining efficiency, illumination or many other\nfactors. We’ll need to take appropriate precautions in our experimental design\nand analysis choices. In principle, the intensity values in an image can be\ncalibrated in physical units, corresponding, say to radiant energy or\nfluorophore concentration; however this is not always done in practice in\nbiological imaging, and perhaps also not needed. Somewhat easier to achieve\nand clearly valuable is a calibration of the spatial dimensions of the image,\ni.e., the conversion factor between pixel units and metric distances.\n\n\nIn this chapter we will:\n\n  * Learn how to read, write and manipulate images in R.\n\n  * Understand how to apply filters and transformations to images.\n\n  * Combine these skills to do segmentation and feature extraction; we will use cell segmentation as an example.\n\n  * Learn how to use statistical methods to analyse spatial distributions and dependencies.\n\n  * Get to know the most basic distribution for a spatial point process: the homogeneous Poisson process.\n\n  * Recognize whether your data fit that basic assumption or whether they show evidence of clumping or exclusion.\n\n## 11.2 Loading images\n\nA useful toolkit for handling images in R is the Bioconductor package\n**[EBImage](https://bioconductor.org/packages/EBImage/)** ([Pau et al.\n2010](16-chap.html#ref-EBImage)). We start out by reading in a simple picture\nto demonstrate the basic functions.\n\n    \n    \n    library(\"EBImage\")\n    imagefile = system.file(\"images\", \"mosquito.png\", package = \"MSMB\")\n    mosq = readImage(imagefile)__\n\n**[EBImage](https://bioconductor.org/packages/EBImage/)** currently supports\nthree image file formats: `jpeg`, `png` and `tiff`. Above, we loaded a sample\nimage from the **[MSMB](https://bioconductor.org/packages/MSMB/)** package.\nWhen you are working with your own data, you do not need that package, just\nprovide the name(s) of your file(s) to the `readImage` function. As you will\nsee later in this chapter, `readImage` can read multiple images in one go,\nwhich are then all assembled into a single image data object. For this to\nwork, the images need to have the same dimensions and color mode.\n\n__\n\nQuestion 11.1\n\nThe **RBioFormats** package (available on GitHub:\n<https://github.com/aoles/RBioFormats>) provides functionality for reading and\nwriting many more image file formats. How many different file formats are\nsupported?\n\n__\n\nSolution\n\n__\n\nSee the manual page of the `read.image` function in the **RBioFormats**\npackage (note that this is distinct from `EBImage::readImage`) and the online\ndocumentation of the Bio-Formats project on the website of The Open Microscopy\nEnvironment, <http://www.openmicroscopy.org/site/support/bio-\nformats5.5/supported-formats.html>.\n\n## 11.3 Displaying images\n\nLet’s visualize the image that we just read in. The basic function is\n`EBImage::display`.\n\n    \n    \n    EBImage::display(mosq)__\n\nThe above command opens the image in a window of your web browser (as set by\n`getOption(\"browser\")`). Using the mouse or keyboard shortcuts, you can zoom\nin and out of the image, pan and cycle through multiple image frames.\n\nAlternatively, we can also display the image using R’s built-in plotting by\ncalling `display` with the argument `method = \"raster\"`. The image then goes\nto the current device. In this way, we can combine image data with other\nplotting functionality, for instance, to add text labels.\n\n    \n    \n    EBImage::display(mosq)\n    text(x = 85, y = 800, label = \"A mosquito\", adj = 0, col = \"orange\", cex = 1.5)__\n\n[![](11-chap_files/figure-html/fig-mosquito-1.jpeg)](11-chap_files/figure-\nhtml/fig-mosquito-1.jpeg \"Figure 11.1: Mosquito discovered deceased in the\nsuburbs of Decatur, Georgia \\(credit: CDC / Janice Haney Carr\\).\")\n\nFigure 11.1: Mosquito discovered deceased in the suburbs of Decatur, Georgia\n(credit: CDC / Janice Haney Carr).\n\nThe resulting plot is shown in Figure 11.1. As usual, the graphics displayed\nin an R device can be saved using the\n**[base](https://cran.r-project.org/web/packages/base/)** R functions\n`dev.print` or `dev.copy`.\n\nNote that we can also read and view color images, see Figure 11.2.\n\n    \n    \n    imagefile = system.file(\"images\", \"hiv.png\", package = \"MSMB\")\n    hivc = readImage(imagefile)__\n    \n    \n    EBImage::display(hivc, method = \"raster\")__\n\n[![](11-chap_files/figure-html/fig-hiv-1.jpeg)](11-chap_files/figure-html/fig-\nhiv-1.jpeg \"Figure 11.2: Scanning electron micrograph of HIV-1 virions budding\nfrom a cultured lymphocyte \\(credit: CDC / C. Goldsmith, P. Feorino, E.L.\nPalmer, W.R. McManus\\).\")\n\nFigure 11.2: Scanning electron micrograph of HIV-1 virions budding from a\ncultured lymphocyte (credit: CDC / C. Goldsmith, P. Feorino, E.L. Palmer, W.R.\nMcManus).\n\nFurthermore, if an image has multiple frames, they can be displayed all at\nonce in a grid arrangement by specifying the function argument `all = TRUE`\n(Figure 11.3),\n\n    \n    \n    nuc = readImage(system.file(\"images\", \"nuclei.tif\", package = \"EBImage\"))\n    EBImage::display(1 - nuc, all = TRUE)__\n\n[![](11-chap_files/figure-html/fig-image-\noneminus-1.png)](11-chap_files/figure-html/fig-image-oneminus-1.png\n\"Figure 11.3: Tiled display of four images of cell nuclei from the EBImage\npackage.\")\n\nFigure 11.3: Tiled display of four images of cell nuclei from the\n**[EBImage](https://bioconductor.org/packages/EBImage/)** package.\n\nor we can just view a single frame, for instance, the second one.\n\n    \n    \n    EBImage::display(1 - nuc, frame = 2)__\n\n__\n\nQuestion 11.2\n\nWhy did we pass the argument `1 - nuc` to the `display` function in the code\nfor Figure 11.3? How does it look if we display `nuc` directly?\n\n## 11.4 How are images stored in R?\n\nLet’s dig into what’s going on by first identifying the class of the image\nobject.\n\n    \n    \n    class(mosq)__\n    \n    \n    [1] \"Image\"\n    attr(,\"package\")\n    [1] \"EBImage\"\n\nSo we see that this object has the class _Image_. This is not one of the base\nR classes, rather, it is defined by the package\n**[EBImage](https://bioconductor.org/packages/EBImage/)**. We can find out\nmore about this class through the help browser or by typing `class ? Image`.\nThe class is derived from the base R class _array_ , so you can do with\n_Image_ objects everything that you can do with R arrays; in addition, they\nhave some extra features and behaviors1.\n\n1 In R’s parlance, the extra features are called **slots** and the behaviors\nare called methods; methods are a special kind of function.\n\n__\n\nQuestion 11.3\n\nHow can you find out what the slots of an _Image_ object are and which methods\ncan be applied to it?\n\n__\n\nSolution\n\n__\n\nThe class definition is easy, it is accessed with `showClass(\"Image\")`.\nFinding all the methods applicable to the _Image_ class by an analogous call\nto an R function is painful; your best bet is to consult the manual page of\nthe class and see which methods the author chose to mention.\n\nThe dimensions of the image can be extracted using the `dim` method, just like\nfor regular arrays.\n\n    \n    \n    dim(mosq)__\n    \n    \n    [1] 1400  952\n\nThe `hist` method has been redefined2 compared to the ordinary `hist` function\nfor arrays: it uses different and possibly more useful defaults (Figure 11.4).\n\n2 In object oriented parlance, _overloaded_.\n\n    \n    \n    hist(mosq)__\n\n[![](11-chap_files/figure-html/fig-mosqhist-1.png)](11-chap_files/figure-\nhtml/fig-mosqhist-1.png \"Figure 11.4: Histogram of the pixel intensities in\nmosq. Note that the range is between 0 and 1.\")\n\nFigure 11.4: Histogram of the pixel intensities in `mosq`. Note that the range\nis between 0 and 1.\n\nIf we want to directly access the data matrix as an R _array_ , we can use the\naccessor function `imageData`.\n\n    \n    \n    imageData(mosq)[1:3, 1:6]__\n    \n    \n              [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n    [1,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784\n    [2,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784\n    [3,] 0.1960784 0.1960784 0.2000000 0.2039216 0.2000000 0.1960784\n\nA useful summary of an _Image_ object is printed if we simply type the\nobject’s name.\n\n    \n    \n    mosq __\n    \n    \n    Image \n      colorMode    : Grayscale \n      storage.mode : double \n      dim          : 1400 952 \n      frames.total : 1 \n      frames.render: 1 \n    \n    imageData(object)[1:5,1:6]\n              [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n    [1,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784\n    [2,] 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784 0.1960784\n    [3,] 0.1960784 0.1960784 0.2000000 0.2039216 0.2000000 0.1960784\n    [4,] 0.1960784 0.1960784 0.2039216 0.2078431 0.2000000 0.1960784\n    [5,] 0.1960784 0.2000000 0.2117647 0.2156863 0.2000000 0.1921569\n\nNow let us look at the color image.\n\n    \n    \n    hivc __\n    \n    \n    Image \n      colorMode    : Color \n      storage.mode : double \n      dim          : 1400 930 3 \n      frames.total : 3 \n      frames.render: 1 \n    \n    imageData(object)[1:5,1:6,1]\n         [,1] [,2] [,3] [,4] [,5] [,6]\n    [1,]    0    0    0    0    0    0\n    [2,]    0    0    0    0    0    0\n    [3,]    0    0    0    0    0    0\n    [4,]    0    0    0    0    0    0\n    [5,]    0    0    0    0    0    0\n\nThe two images differ by their property `colorMode`, which is `Grayscale` for\n`mosq` and `Color` for `hivc`. What is the point of this property? It turns\nout to be convenient when we are dealing with stacks of images. If `colorMode`\nis `Grayscale`, then the third and all higher dimensions of the array are\nconsidered as separate image frames corresponding, for instance, to different\n\\\\(z\\\\)-positions, time points, replicates, etc. On the other hand, if\n`colorMode` is `Color`, then the third dimension is assumed to hold different\ncolor channels, and only the fourth and higher dimensions – if present – are\nused for multiple image frames. In `hivc`, there are three color channels,\nwhich correspond to the red, green and blue intensities of our photograph.\nHowever, this does not necessarily need to be the case, there can be any\nnumber of color channels.\n\n__\n\nQuestion 11.4\n\nDescribe how R stores the data `nuc`.\n\n__\n\nSolution\n\n__\n\n    \n    \n    nuc __\n    \n    \n    Image \n      colorMode    : Grayscale \n      storage.mode : double \n      dim          : 510 510 4 \n      frames.total : 4 \n      frames.render: 4 \n    \n    imageData(object)[1:5,1:6,1]\n               [,1]       [,2]       [,3]       [,4]       [,5]       [,6]\n    [1,] 0.06274510 0.07450980 0.07058824 0.08235294 0.10588235 0.09803922\n    [2,] 0.06274510 0.05882353 0.07843137 0.09019608 0.09019608 0.10588235\n    [3,] 0.06666667 0.06666667 0.08235294 0.07843137 0.09411765 0.09411765\n    [4,] 0.06666667 0.06666667 0.07058824 0.08627451 0.08627451 0.09803922\n    [5,] 0.05882353 0.06666667 0.07058824 0.08235294 0.09411765 0.10588235\n    \n    \n    dim(imageData(nuc))__\n    \n    \n    [1] 510 510   4\n\nWe see that we have 4 frames in total, which correspond to the 4 separate\nimages (`frames.render`).\n\n## 11.5 Writing images to file\n\nDirectly saving images to disk in the array representation that we saw in the\nprevious section would lead to large file sizes – in most cases, needlessly\nlarge. It is common to use compression algorithms to reduce the storage\nconsumption. There are two main types of image3 compression:\n\n3 In an analogous way, this is also true for movies and music.\n\n  * Lossless compression: it is possible to exactly reconstruct the original image data from the compressed file. Simple priciples of lossless compression are: (i) do not spend more bits on representing a pixel than needed (e.g., the pixels in the `mosq` image have a range of 256 gray scale values, and this could be represented by 8 bits, although `mosq` stores them in a 64-bit numeric format4); and (2) identify patterns (such as those that you saw above in the printed pixel values for `mosq` and `hivc`) and represent them by much shorter to write down rules instead.\n\n  * Lossy compression: additional savings are made compared to lossless compression by dropping details that a human viewer would be unlikely to notice anyway.\n\n4 While this is somewhat wasteful of memory, it is more compatible with the\nway the rest of R works, and is rarely a limiting factor on modern computer\nhardware.\n\n5 <https://en.wikipedia.org/wiki/Portable_Network_Graphics>\n\n6 <https://en.wikipedia.org/wiki/JPEG>\n\nAn example for a storage format with lossless compression is PNG5, an example\nfor lossy compression is the JPEG6 format. While JPEG is good for your holiday\npictures, it is good practice to store scientific images in a lossless format.\n\nWe read the image `hivc` from a file in PNG format, so let’s now write it out\nas a JPEG file. The lossiness is specified by the _quality_ parameter, which\ncan lie between 1 (worst) and 100 (best).\n\n    \n    \n    output_file = file.path(tempdir(), \"hivc.jpeg\")\n    writeImage(hivc, output_file, quality = 85)__\n\nSimilarly, we could have written the image as a `TIFF` file and chosen among\nseveral compression algorithms (see the manual page of the `writeImage` and\n`writeTiff` functions). The package **RBioFormats** lets you write to many\nfurther image file formats.\n\n__\n\nQuestion 11.5\n\nHow big is the `hivc` object in R’s memory? How big is the JPEG file? How much\nRAM would you expect a three color, 16 Megapixel image to occupy?\n\n__\n\nSolution\n\n__\n\n    \n    \n    object.size(hivc) |> format(units = \"Mb\")__\n    \n    \n    [1] \"29.8 Mb\"\n    \n    \n    (object.size(hivc) / prod(dim(hivc))) |> format() |> paste(\"per pixel\")__\n    \n    \n    [1] \"8 bytes per pixel\"\n    \n    \n    file.info( output_file )$size __\n    \n    \n    [1] 294904\n    \n    \n    16 * 3 * 8 __\n    \n    \n    [1] 384\n\n## 11.6 Manipulating images\n\nNow that we know that images are stored as arrays of numbers in R, our method\nof manipulating images becomes clear – simple algebra! For example, we can\ntake our original image, shown again in Figure 11.5a, and flip the bright\nareas to dark and vice versa by multiplying the image with -1 Figure 11.5b).\n\n    \n    \n    mosqinv = normalize(-mosq)__\n\n__\n\nQuestion 11.6\n\nWhat does the function `normalize` do?\n\nWe could also adjust the contrast through multiplication (Figure 11.5c) and\nthe gamma-factor through exponentiation Figure 11.5d).\n\n    \n    \n    mosqcont = mosq * 3\n    mosqexp = mosq ^ (1/3)__\n\n[![](11-chap_files/figure-html/fig-manip1-1.jpeg)](11-chap_files/figure-\nhtml/fig-manip1-1.jpeg \"Figure 11.5 \\(a\\): \")\n\n(a)\n\n[![](11-chap_files/figure-html/fig-manip1-2.jpeg)](11-chap_files/figure-\nhtml/fig-manip1-2.jpeg \"Figure 11.5 \\(b\\): \")\n\n(b)\n\n[![](11-chap_files/figure-html/fig-manip1-3.jpeg)](11-chap_files/figure-\nhtml/fig-manip1-3.jpeg \"Figure 11.5 \\(c\\): \")\n\n(c)\n\n[![](11-chap_files/figure-html/fig-manip1-4.jpeg)](11-chap_files/figure-\nhtml/fig-manip1-4.jpeg \"Figure 11.5 \\(d\\): \")\n\n(d)\n\nFigure 11.5: The original mosquito image (a) and three different image\ntransformations: (b) subtraction, (c) multiplication, (d) power\ntransformation.\n\nFurthermore, we can crop, threshold and transpose images with matrix\noperations (Figure 11.6).\n\n    \n    \n    mosqcrop   = mosq[100:438, 112:550]\n    mosqthresh = mosq > 0.5\n    mosqtransp = transpose(mosq)__\n\n[![](11-chap_files/figure-html/fig-manip5-1.png)](11-chap_files/figure-\nhtml/fig-manip5-1.png \"Figure 11.6 \\(a\\): \")\n\n(a)\n\n[![](11-chap_files/figure-html/fig-manip5-2.png)](11-chap_files/figure-\nhtml/fig-manip5-2.png \"Figure 11.6 \\(b\\): \")\n\n(b)\n\n[![](11-chap_files/figure-html/fig-manip5-3.png)](11-chap_files/figure-\nhtml/fig-manip5-3.png \"Figure 11.6 \\(c\\): \")\n\n(c)\n\nFigure 11.6: Three further image transformations: (a) cropping, (b)\nthresholding, (c) transposition.\n\n__\n\nQuestion 11.7\n\nWhat data type is `mosqthresh`, the result of the thresholding?\n\n__\n\nSolution\n\n__\n\nIt is an _Image_ object whose pixels are binary values represented by an R\narray of type _logical_. You can inspect the object by typing its name into\nthe console.\n\n    \n    \n    mosqthresh __\n    \n    \n    Image \n      colorMode    : Grayscale \n      storage.mode : logical \n      dim          : 1400 952 \n      frames.total : 1 \n      frames.render: 1 \n    \n    imageData(object)[1:5,1:6]\n          [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n    [1,] FALSE FALSE FALSE FALSE FALSE FALSE\n    [2,] FALSE FALSE FALSE FALSE FALSE FALSE\n    [3,] FALSE FALSE FALSE FALSE FALSE FALSE\n    [4,] FALSE FALSE FALSE FALSE FALSE FALSE\n    [5,] FALSE FALSE FALSE FALSE FALSE FALSE\n\n__\n\nQuestion 11.8\n\nInstead of the `transpose` function as above, could we also use R’s\n**[base](https://cran.r-project.org/web/packages/base/)** function `t`?\n\n__\n\nSolution\n\n__\n\nIn this instance, the values of `t(mosq)` and `transpose(mosq)` happen to be\nthe same, but `transpose` is preferable since it also works with color and\nmultiframe images.\n\n## 11.7 Spatial transformations\n\nWe just saw one type of spatial transformation, transposition, but there are\nmany more—here are some examples:\n\n    \n    \n    mosqrot   = EBImage::rotate(mosq, angle = 30)\n    mosqshift = EBImage::translate(mosq, v = c(100, 170))\n    mosqflip  = flip(mosq)\n    mosqflop  = flop(mosq)__\n\n[![](11-chap_files/figure-html/fig-flipflop-1.jpeg)](11-chap_files/figure-\nhtml/fig-flipflop-1.jpeg \"Figure 11.7 \\(a\\): \")\n\n(a)\n\n[![](11-chap_files/figure-html/fig-flipflop-2.jpeg)](11-chap_files/figure-\nhtml/fig-flipflop-2.jpeg \"Figure 11.7 \\(b\\): \")\n\n(b)\n\n[![](11-chap_files/figure-html/fig-flipflop-3.jpeg)](11-chap_files/figure-\nhtml/fig-flipflop-3.jpeg \"Figure 11.7 \\(c\\): \")\n\n(c)\n\n[![](11-chap_files/figure-html/fig-flipflop-4.jpeg)](11-chap_files/figure-\nhtml/fig-flipflop-4.jpeg \"Figure 11.7 \\(d\\): \")\n\n(d)\n\nFigure 11.7: Spatial transformations: (a) rotation, (b) translation, (c)\nreflection about the central horizontal axis (`flip`), (d) reflection about\nthe central vertical axis (`flop`).\n\nIn the code above, the function `rotate`7 rotates the image clockwise with the\ngiven angle, `translate` moves the image by the specified two-dimensional\nvector (pixels that end up outside the image region are cropped, and pixels\nthat enter into the image region are set to zero). The functions `flip` and\n`flop` reflect the image around the central horizontal and vertical axis,\nrespectively. The results of these operations are shown in Figure 11.7.\n\n7 Here we call the function with its namespace qualifier `EBImage::` to avoid\nconfusion with a function of the same name in the namespace of the\n**[spatstat](https://cran.r-project.org/web/packages/spatstat/)** package,\nwhich we will attach later.\n\n## 11.8 Linear filters\n\nLet’s now switch to an application in cell biology. We load images of human\ncancer cells that were studied by Laufer, Fischer and co-workers ([Laufer et\nal. 2013](16-chap.html#ref-Laufer:NatMeth:2013)). They are shown in Figure\n11.8.\n\n    \n    \n    imagefiles = system.file(\"images\", c(\"image-DAPI.tif\", \"image-FITC.tif\", \"image-Cy3.tif\"), package = \"MSMB\")\n    cells = readImage(imagefiles)__\n\n[![](11-chap_files/figure-html/fig-LauferCells-1.png)](11-chap_files/figure-\nhtml/fig-LauferCells-1.png \"Figure 11.8: Human colon cancer cells \\(HCT116\\).\nThe four images show the same cells: the leftmost image corresponds to DAPI\nstaining of the cells’ DNA, the second to immunostaining against alpha-\ntubulin, the third to actin. They are displayed as gray-scale images. The\nrightmost image is obtained by overlaying the three images as color channels\nof an RGB image \\(red: actin, green: alpha-tubulin, blue: DNA\\).\")\n\nFigure 11.8: Human colon cancer cells (HCT116). The four images show the same\ncells: the leftmost image corresponds to DAPI staining of the cells’ DNA, the\nsecond to immunostaining against alpha-tubulin, the third to actin. They are\ndisplayed as gray-scale images. The rightmost image is obtained by overlaying\nthe three images as color channels of an RGB image (red: actin, green: alpha-\ntubulin, blue: DNA).\n\nThe _Image_ object `cells` is a three-dimensional array of size 340\n\\\\(\\times\\\\) 490 \\\\(\\times\\\\) 3, where the last dimension indicates that there\nare three individual grayscale frames. Our goal now is to computationally\nidentify and quantitatively characterize the cells in these images. That by\nitself would be a modest goal, but note that the dataset of Laufer et\nal.contains over 690,000 images, each of which has 2,048 \\\\(\\times\\\\) 2,048\npixels. Here, we are looking at three of these, out of which a small region\nwas cropped. Once we know how to achieve our stated goal, we can apply our\nabilities to such large image collections, and that is no longer a modest aim!\n\n### 11.8.1 Interlude: the intensity scale of images\n\nHowever, before we can start with real work, we need to deal with a slightly\nmundane data conversion issue. This is, of course, not unusual. Let us inspect\nthe dynamic range (the minimum and the maximum value) of the images.\n\n    \n    \n    apply(cells, 3, range)__\n    \n    \n          image-DAPI  image-FITC   image-Cy3\n    [1,] 0.001586938 0.002899214 0.001663233\n    [2,] 0.031204700 0.062485695 0.055710689\n\nWe see that the maximum values are small numbers well below 1. The reason for\nthis is that the `readImage` function recognizes that the `TIFF` images uses\n16 bit integers to represent each pixel, and it returns the data – as is\ncommon for numeric variables in R – in an array of double precision floating\npoint numbers, with the integer values (whose theoretical range is from 0 to\n\\\\(2^{16}-1=65535\\\\)) stored in the mantissa of the floating point\nrepresentation and the exponents chosen so that the theoretical range is\nmapped to the interval \\\\([0,1]\\\\). However, the scanner that was used to\ncreate these images only used the lower 11 or 12 bits, and this explains the\nsmall maximum values in the images. We can rescale these data to approximately\ncover the range \\\\([0,1]\\\\) as follows8.\n\n8 The function `normalize` provides a more flexible interface to the scaling\nof images.\n\n    \n    \n    cells[,,1]   = 32 * cells[,,1]\n    cells[,,2:3] = 16 * cells[,,2:3]\n    apply(cells, 3, range)__\n    \n    \n         image-DAPI image-FITC  image-Cy3\n    [1,] 0.05078202 0.04638743 0.02661173\n    [2,] 0.99855039 0.99977111 0.89137102\n\nWe can keep in mind that these multiplications with a multiple of 2 have no\nimpact on the underlying precision of the stored data.\n\n### 11.8.2 Noise reduction by smoothing\n\nNow we are ready to get going with analyzing the images. As our first goal is\nsegmentation of the images to identify the individual cells, we can start by\nremoving local artifacts or noise from the images through smoothing. An\nintuitive approach is to define a window of a selected size around each pixel\nand average the values within that window. After applying this procedure to\nall pixels, the new, smoothed image is obtained. Mathematically, we can\nexpress this as\n\n\\\\[ f^*(x,y) = \\frac{1}{N} \\sum_{s=-a}^{a}\\sum_{t=-a}^{a} f(x+s, y+t),\n\\tag{11.1}\\\\]\n\nwhere \\\\(f(x,y)\\\\) is the value of the pixel at position \\\\(x\\\\), \\\\(y\\\\), and\n\\\\(a\\\\) determines the window size, which is \\\\(2a+1\\\\) in each direction.\n\\\\(N=(2a+1)^2\\\\) is the number of pixels averaged over, and \\\\(f^*\\\\) is the\nnew, smoothed image.\n\nMore generally, we can replace the moving average by a weighted average, using\na weight function \\\\(w\\\\), which typically has highest weight at the window\nmidpoint (\\\\(s=t=0\\\\)) and then decreases towards the edges.\n\n\\\\[ (w * f)(x,y) = \\sum_{s=-\\infty}^{+\\infty} \\sum_{t=-\\infty}^{+\\infty}\nw(s,t)\\, f(x+s, y+s) \\tag{11.2}\\\\]\n\nFor notational convenience, we let the summations range from \\\\(-\\infty\\\\) to\n\\\\(\\infty\\\\), even if in practice the sums are finite as \\\\(w\\\\) has only a\nfinite number of non-zero values. In fact, we can think of the weight function\n\\\\(w\\\\) as another image, and this operation is also called the _convolution_\nof the images \\\\(f\\\\) and \\\\(w\\\\), indicated by the the symbol \\\\(*\\\\). In\n**[EBImage](https://bioconductor.org/packages/EBImage/)** , the 2-dimensional\nconvolution is implemented by the function `filter2`, and the auxiliary\nfunction `makeBrush` can be used to generate weight functions \\\\(w\\\\).\n\n    \n    \n    w = makeBrush(size = 51, shape = \"gaussian\", sigma = 7)\n    nucSmooth = filter2(getFrame(cells, 1), w)__\n\n[![](11-chap_files/figure-html/fig-nucSmooth-1.png)](11-chap_files/figure-\nhtml/fig-nucSmooth-1.png \"Figure 11.9: nucSmooth, a smoothed version of the\nDNA channel in the image object cells \\(the original version is shown in the\nleftmost panel of Figure fig-LauferCells\\).\")\n\nFigure 11.9: `nucSmooth`, a smoothed version of the DNA channel in the image\nobject `cells` (the original version is shown in the leftmost panel of Figure\n11.8).\n\n__\n\nQuestion 11.9\n\nHow does the weight matrix `w` look like?\n\n__\n\nSolution\n\n__\n\nSee Figure 11.10\n\n    \n    \n    library(\"tibble\")\n    library(\"ggplot2\")\n    tibble(w = w[(nrow(w)+1)/2, ]) |>\n      ggplot(aes(y = w, x = seq(along = w))) + geom_point()__\n\n[![](11-chap_files/figure-html/fig-image-filter2-1.png)](11-chap_files/figure-\nhtml/fig-image-filter2-1.png \"Figure 11.10: The middle row of the weight\nmatrix, w\\[26, \\].\")\n\nFigure 11.10: The middle row of the weight matrix, `w[`26`, ]`.\n\nIn fact, the `filter2` function does not directly perform the summation\nindicated in Equation 11.2. Instead, it uses the Fast Fourier Transformation\nin a way that is mathematically equivalent and computationally more efficient.\n\nThe convolution in Equation 11.2 is a _linear_ operation, in the sense that\n\\\\(w*(c_1f_1+c_2f_2)= c_1w*f_1 + c_2w*f_2\\\\) for any two images \\\\(f_1\\\\),\n\\\\(f_2\\\\) and numbers \\\\(c_1\\\\), \\\\(c_2\\\\). There is beautiful and powerful\ntheory underlying linear filters ([Vetterli, Kovačević, and Goyal\n2014](16-chap.html#ref-FoundationSignalProcessing)).\n\nTo proceed we now use smaller smoothing bandwidths than what we displayed in\nFigure 11.9 for demonstration. Let’s use a `sigma` of 1 pixel for the DNA\nchannel and 3 pixels for actin and tubulin.\n\n    \n    \n    cellsSmooth = Image(dim = dim(cells))\n    sigma = c(1, 3, 3)\n    for(i in seq_along(sigma))\n      cellsSmooth[,,i] = filter2( cells[,,i],\n             filter = makeBrush(size = 51, shape = \"gaussian\",\n                                sigma = sigma[i]) )__\n\nThe smoothed images have reduced pixel noise, yet still the needed resolution.\n\n## 11.9 Adaptive thresholding\n\nThe idea of adaptive thresholding is that, compared to straightforward\nthresholding as we did for Figure 11.6b, the threshold is allowed to be\ndifferent in different regions of the image. In this way, one can anticipate\nspatial dependencies of the underlying background signal caused, for instance,\nby uneven illumination or by stray signal from nearby bright objects. In fact,\nwe have already seen an example for uneven background in the bottom right\nimage of Figure 11.3.\n\nOur colon cancer images (Figure 11.8) do not have such artefacts, but for\ndemonstration, let’s simulate uneven illumination by multiplying the image\nwith a two-dimensional bell function `illuminationGradient`, which has highest\nvalue in the middle and falls off to the sides (Figure 11.11).\n\n    \n    \n    py = seq(-1, +1, length.out = dim(cellsSmooth)[1])\n    px = seq(-1, +1, length.out = dim(cellsSmooth)[2])\n    illuminationGradient = Image(outer(py, px, function(x, y) exp(-(x^2 + y^2))))\n    nucBadlyIlluminated = cellsSmooth[,,1] * illuminationGradient __\n\nWe now define a smoothing window, `disc`, whose size is 21 pixels, and\ntherefore bigger than the nuclei we want to detect, but small compared to the\nlength scales of the illumination artifact. We use it to compute the image\n`localBackground` (shown in Figure 11.11 (c)) and the thresholded image\n`nucBadThresh`.\n\n    \n    \n    disc = makeBrush(21, \"disc\")\n    disc = disc / sum(disc)\n    localBackground = filter2(nucBadlyIlluminated, disc)\n    offset = 0.02\n    nucBadThresh = (nucBadlyIlluminated - localBackground > offset)__\n\n[![](11-chap_files/figure-html/fig-illumination-1.png)](11-chap_files/figure-\nhtml/fig-illumination-1.png \"Figure 11.11 \\(a\\): \")\n\n(a)\n\n[![](11-chap_files/figure-html/fig-illumination-2.png)](11-chap_files/figure-\nhtml/fig-illumination-2.png \"Figure 11.11 \\(b\\): \")\n\n(b)\n\n[![](11-chap_files/figure-html/fig-illumination-3.png)](11-chap_files/figure-\nhtml/fig-illumination-3.png \"Figure 11.11 \\(c\\): \")\n\n(c)\n\n[![](11-chap_files/figure-html/fig-illumination-4.png)](11-chap_files/figure-\nhtml/fig-illumination-4.png \"Figure 11.11 \\(d\\): \")\n\n(d)\n\nFigure 11.11: a: `illuminationGradient`, a function that has its maximum at\nthe center and falls off towards the sides, and which simulates uneven\nillumination sometimes seen in images. (b) `nucBadlyIlluminated`, the image\nthat results from multiplying the DNA channel in `cellsSmooth` with\n`illuminationGradient`. (c) `localBackground`, the result of applying a linear\nfilter with a bandwidth that is larger than the objects to be detected. (d)\n`nucBadThresh`, the result of adaptive thresholding. The nuclei at the\nperiphery of the image are reasonably well identified, despite the drop off in\nsignal strength.\n\nAfter having seen that this may work, let’s do the same again for the actual\n(not artificially degraded) image, as we need this for the next steps.\n\n    \n    \n    nucThresh = (cellsSmooth[,,1] - filter2(cellsSmooth[,,1], disc) > offset)__\n\nBy comparing each pixel’s intensity to a background determined from the values\nin a local neighborhood, we assume that the objects are relatively sparse\ndistributed in the image, so that the signal distribution in the neighborhood\nis dominated by background. For the nuclei in our images, this assumption\nmakes sense, for other situations, you may need to make different assumptions.\nThe adaptive thresholding that we have done here uses a linear filter,\n`filter2`, and therefore amounts to (weighted) local averaging. Other\ndistribution summaries, e.g. the median or a low quantile, tend to be\npreferable, even if they are computationally more expensive. For local median\nfiltering, **[EBimage](https://bioconductor.org/packages/EBimage/)** provides\nthe function `medianFilter`.\n\n## 11.10 Morphological operations on binary images\n\nThe thresholded image `nucThresh` (shown in the left panel of Figure [morphop]\nis not yet satisfactory. The boundaries of the nuclei are slightly rugged, and\nthere is noise at the single-pixel level. An effective and simple way to\nremove these nuisances is given by a set of morphological operations ([Serra\n1983](16-chap.html#ref-MathematicalMorphology)).\n\nProvided a binary image (with values, say, 0 and 1, representing back- and\nforeground pixels), and a binary mask9 (which is sometimes also called the\nstructuring element), these operations work as follows.\n\n9 An example for a mask is a circle with a given radius, or more precisely,\nthe set of pixels within a certain distance from a center pixel.\n\n  * `erode`: For every foreground pixel, put the mask around it, and if any pixel under the mask is from the background, then set all these pixels to background.\n\n  * `dilate`: For every background pixel, put the mask around it, and if any pixel under the mask is from the foreground, then set all these pixels to foreground.\n\n  * `open`: perform `erode` followed by `dilate`.\n\nWe can also think of these operations as filters, however, in contrast to the\nlinear filters of Section 11.8 they operate on binary images only, and there\nis no linearity.\n\nLet us apply morphological opening to our image.\n\n    \n    \n    nucOpened = EBImage::opening(nucThresh, kern = makeBrush(5, shape = \"disc\"))__\n\nThe result of this is subtle, and you will have to zoom into the images in\nFigure 11.12 to spot the differences, but this operation manages to smoothen\nout some pixel-level features in the binary images that for our application\nare undesirable.\n\n## 11.11 Segmentation of a binary image into objects\n\nThe binary image `nucOpened` represents a segmentation of the image into\nforeground and background pixels, but not into individual nuclei. We can take\none step further and extract individual objects defined as connected sets of\npixels. In **[EBImage](https://bioconductor.org/packages/EBImage/)** , there\nis a handy function for this purpose, `bwlabel`.\n\n    \n    \n    nucSeed = bwlabel(nucOpened)\n    table(nucSeed)__\n    \n    \n    nucSeed\n         0      1      2      3      4      5      6      7      8      9     10 \n    155408    511    330    120    468    222    121    125    159    116    520 \n        11     12     13     14     15     16     17     18     19     20     21 \n       115    184    179    116    183    187    303    226    164    309    194 \n        22     23     24     25     26     27     28     29     30     31     32 \n       148    345    287    203    379    371    208    222    320    443    409 \n        33     34     35     36     37     38     39     40     41     42     43 \n       493    256    169    225    376    214    228    341    269    119    315 \n\nThe function returns an image, `nucSeed`, of integer values, where 0\nrepresents the background, and the numbers from 1 to 43 index the different\nidentified objects.\n\n__\n\nQuestion 11.10\n\nWhat are the numbers in the above table?\n\n__\n\nSolution\n\n__\n\nThey correspond to the area (in pixels) of each of the objects. We could use\nthis information to remove objects that are too large or too small compared to\nwhat we expect.\n\nTo visualize such images, the function `colorLabels` is convenient, which\nconverts the (grayscale) integer image into a color image, using distinct,\narbitrarily chosen colors for each object.\n\n    \n    \n    EBImage::display(colorLabels(nucSeed))__\n\nThis is shown in the middle panel of Figure 11.12. The result is already\nencouraging, although we can spot two types of errors:\n\n  * Some neighboring objects were not properly separated.\n\n  * Some objects contain holes.\n\nIndeed, we could change the occurrences of these by playing with the disc size\nand the parameter `offset` in Section 11.9: making the offset higher reduces\nthe probability that two neighboring object touch and are seen as one object\nby `bwlabel`; on the other hand, that leads to even more and even bigger\nholes. Vice versa for making it lower.\n\nSegmentation is a rich and diverse field of research and engineering, with a\nlarge body of literature, software tools ([Schindelin et al.\n2012](16-chap.html#ref-schindelin2012fiji); [Chaumont et al.\n2012](16-chap.html#ref-chaumont2012icy); [Carpenter et al.\n2006](16-chap.html#ref-carpenter2006cellprofiler); [Held et al.\n2010](16-chap.html#ref-held2010cellcognition)) and practical experience in the\nimage analysis and machine learning communities. What is the adequate approach\nto a given task depends hugely on the data and the underlying question, and\nthere is no universally best method. It is typically even difficult to obtain\na “ground truth” or “gold standards” by which to evaluate an analysis –\nrelying on manual annotation of a modest number of selected images is not\nuncommon. Despite the bewildering array of choices, it is easy to get going,\nand we need not be afraid of starting out with a simple solution, which we can\nsuccessively refine. Improvements can usually be gained from methods that\nallow inclusion of more prior knowledge of the expected shapes, sizes and\nrelations between the objects to be identified.\n\nFor statistical analyses of high-throughput images, we may choose to be\nsatisfied with a simple method that does not rely on too many parameters or\nassumptions and results in a perhaps sub-optimal but rapid and good enough\nresult ([Rajaram et al. 2012](16-chap.html#ref-PhenoRipper)). In this spirit,\nlet us proceed with what we have. We generate a lenient foreground mask, which\nsurely covers all nuclear stained regions, even though it also covers some\nregions between nuclei. To do so, we simply apply a second, less stringent\nadaptive thresholding.\n\n    \n    \n    nucMask = cellsSmooth[,,1] - filter2(cellsSmooth[,,1], disc) > 0 __\n\nand apply another morphological operation, `fillHull`, which fills holes that\nare surrounded by foreground pixels.\n\n    \n    \n    nucMask = fillHull(nucMask)__\n\nTo improve `nucSeed`, we can now _propagate_ its segmented objects until they\nfill the mask defined by `nucMask`. Boundaries between nuclei, in those places\nwhere the mask is connected, can be drawn by Voronoi tessellation, which is\nimplemented in the function `propagate`, and will be explained in the next\nsection.\n\n    \n    \n    nuclei = propagate(cellsSmooth[,,1], nucSeed, mask = nucMask)__\n\nThe result is displayed in the rightmost panel of Figure 11.12.\n\n[![](11-chap_files/figure-html/fig-morphop-1.png)](11-chap_files/figure-\nhtml/fig-morphop-1.png \"Figure 11.12 \\(a\\): \")\n\n(a)\n\n[![](11-chap_files/figure-html/fig-morphop-2.png)](11-chap_files/figure-\nhtml/fig-morphop-2.png \"Figure 11.12 \\(b\\): \")\n\n(b)\n\n[![](11-chap_files/figure-html/fig-morphop-3.png)](11-chap_files/figure-\nhtml/fig-morphop-3.png \"Figure 11.12 \\(c\\): \")\n\n(c)\n\n[![](11-chap_files/figure-html/fig-morphop-4.png)](11-chap_files/figure-\nhtml/fig-morphop-4.png \"Figure 11.12 \\(d\\): \")\n\n(d)\n\n[![](11-chap_files/figure-html/fig-morphop-5.png)](11-chap_files/figure-\nhtml/fig-morphop-5.png \"Figure 11.12 \\(e\\): \")\n\n(e)\n\nFigure 11.12: Different steps in the segmentation of the nuclei. From (a-e):\n`nucThresh`, `nucOpened`, `nucSeed`, `nucMask`, `nuclei`.\n\n## 11.12 Voronoi tessellation\n\nVoronoi tessellation is useful if we have a set of seed points (or regions)\nand want to partition the space that lies between these seeds in such a way\nthat each point in the space is assigned to its closest seed. As this is an\nintuitive and powerful idea, we’ll use this section for a short digression on\nit. Let us consider a basic example. We use the image `nuclei` as seeds. To\ncall the function `propagate`, we also need to specify another image: for now\nwe just provide a trivial image of all zeros, and we set the parameter\n`lambda` to a large positive value (we will come back to these choices).\n\n    \n    \n    zeros        = Image(dim = dim(nuclei))\n    voronoiExamp = propagate(seeds = nuclei, x = zeros, lambda = 100)\n    voronoiPaint = paintObjects(voronoiExamp, 1 - nucOpened)__\n\n[![](11-chap_files/figure-html/fig-voronoiPaint-1.png)](11-chap_files/figure-\nhtml/fig-voronoiPaint-1.png \"Figure 11.13: Example of a Voronoi segmentation,\nindicated by the gray lines, using the nuclei \\(indicated by black regions\\)\nas seeds.\")\n\nFigure 11.13: Example of a Voronoi segmentation, indicated by the gray lines,\nusing the nuclei (indicated by black regions) as seeds.\n\n__\n\nQuestion 11.11\n\nHow do you select partition elements from the tessellation?\n\n__\n\nSolution\n\n__\n\nThe result, `voronoiExamp`, of the above call to `propagate` is simply an\nimage of integers whose values indicate the different partitions.\n\n    \n    \n    head(table(voronoiExamp))__\n    \n    \n    voronoiExamp\n       1    2    3    4    5    6 \n    5645 4735  370 5964 3333 1377 \n    \n    \n    ind = which(voronoiExamp == 13, arr.ind = TRUE)\n    head(ind, 3)__\n    \n    \n         row col\n    [1,] 112 100\n    [2,] 113 100\n    [3,] 114 100\n\nThe result is shown in Figure 11.13. This looks interesting, but perhaps not\nyet as useful as the image `nuclei` in Figure [morphop]. We note that the\nbasic definition of Voronoi tessellation, which we have given above, allows\nfor two generalizations:\n\n  * By default, the space that we partition is the full, rectangular image area – but indeed we could restrict ourselves to any arbitrary subspace. This is akin to finding the shortest distance from each point to the next seed not in a simple flat landscape, but in a landscape that is interspersed by lakes and rivers (which you cannot cross), so that all paths need to remain on the land. `propagate` allows for this generalization through its `mask` parameter.\n\n  * By default, we think of the space as flat – but in fact it could have hills and canyons, so that the distance between two points in the landscape not only depends on their \\\\(x\\\\)\\- and \\\\(y\\\\)-positions but also on the ascents and descents, up and down in \\\\(z\\\\)-direction, that lie in between. We can think of \\\\(z\\\\) as an “elevation”. You can specify such a landscape to `propagate` through its `x` argument.\n\nMathematically, we say that instead of the simple default case (a flat\nrectangle, or image, with a Euclidean metric on it), we perform the Voronoi\nsegmentation on a Riemann manifold that has a special shape and a special\nmetric. Let us use the notation \\\\(x\\\\) and \\\\(y\\\\) for the column and row\ncoordinates of the image, and \\\\(z\\\\) for the elevation. For two neighboring\npoints, defined by coordinates \\\\((x, y, z)\\\\) and \\\\((x+\\text{d}x,\ny+\\text{d}y, z+\\text{d}z)\\\\), the distance \\\\(\\text{d}s\\\\) between them is\nthus not obtained by the usual Euclidean metric on the 2D image,\n\n\\\\[ \\text{d}s^2 = \\text{d}x^2 + \\text{d}y^2 \\tag{11.3}\\\\]\n\nbut instead\n\n\\\\[ \\text{d}s^2 = \\frac{2}{\\lambda+1} \\left[ \\lambda \\left( \\text{d}x^2 +\n\\text{d}y^2 \\right) + \\text{d}z^2 \\right], \\tag{11.4}\\\\]\n\nwhere the parameter \\\\(\\lambda\\\\) is a real number \\\\(\\ge0\\\\). To understand\nthis, lets look at some important cases:\n\n\\\\[ \\begin{aligned} \\lambda=1:&\\quad \\text{d}s^2 = \\text{d}x^2 + \\text{d}y^2 +\n\\text{d}z^2\\\\\\ \\lambda=0:&\\quad \\text{d}s^2 = 2\\, \\text{d}z^2\\\\\\\n\\lambda\\to\\infty:&\\quad \\text{d}s^2 = 2 \\left( \\text{d}x^2 + \\text{d}y^2\n\\right)\\\\\\ \\end{aligned} \\tag{11.5}\\\\]\n\nFor \\\\(\\lambda=1\\\\), the metric becomes the isotropic Euclidean metric, i.e.,\na movement in \\\\(z\\\\)-direction is equally “expensive” or “far” as in\n\\\\(x\\\\)\\- or \\\\(y\\\\)-direction. In the extreme case of \\\\(\\lambda=0\\\\), only\nthe \\\\(z\\\\)-movements matter, whereas lateral movements (in \\\\(x\\\\)\\- or\n\\\\(y\\\\)-direction) do not contribute to the distance. In the other extreme\ncase, \\\\(\\lambda\\to\\infty\\\\), only lateral movements matter, and movement in\n\\\\(z\\\\)-direction is “free”. Distances between points further apart are\nobtained by summing \\\\(\\text{d}s\\\\) along the shortest path between them. The\nparameter \\\\(\\lambda\\\\) serves as a convenient control of the relative\nweighting between sideways movement (along the \\\\(x\\\\) and \\\\(y\\\\) axes) and\nvertical movement. Intuitively, if you imagine yourself as a hiker in such a\nlandscape, by choosing \\\\(\\lambda\\\\) you can specify how much you are prepared\nto climb up and down to overcome a mountain, versus walking around it. When we\nused `lambda = 100` in our call to `propagate` at the begin of this section,\nthis value was effectively infinite, so we were in the third boundary case of\nEquation 11.5.\n\nFor the purpose of cell segmentation, these ideas were put forward by Thouis\nJones et al. ([Jones, Carpenter, and Golland 2005](16-chap.html#ref-\njones2005voronoi); [Carpenter et al. 2006](16-chap.html#ref-\ncarpenter2006cellprofiler)), who also wrote the efficient algorithm that is\nused by `propagate`.\n\n__\n\nTask\n\nTry out the effect of using different \\\\(\\lambda\\\\)s.\n\n## 11.13 Segmenting the cell bodies\n\n[![](11-chap_files/figure-html/fig-\nhistcellbody-1-1.png)](11-chap_files/figure-html/fig-histcellbody-1-1.png\n\"Figure 11.14: Histogram of the actin channel in cellsSmooth, after taking the\nlogarithm.\")\n\nFigure 11.14: Histogram of the actin channel in `cellsSmooth`, after taking\nthe logarithm.\n\n[![](11-chap_files/figure-html/fig-\nhistcellbody-2-1.png)](11-chap_files/figure-html/fig-histcellbody-2-1.png\n\"Figure 11.15: Zoom into Figure fig-histcellbody-1.\")\n\nFigure 11.15: Zoom into Figure 11.14.\n\nTo determine a mask of cytoplasmic area in the images, let us explore a\ndifferent way of thresholding, this time using a global threshold which we\nfind by fitting a mixture model to the data. The histograms show the\ndistributions of the pixel intensities in the actin image. We look at the data\non the logarithmic scale, and in Figure 11.15 zoom into the region where most\nof the data lie.\n\n    \n    \n    hist(log(cellsSmooth[,,3]) )\n    hist(log(cellsSmooth[,,3]), xlim = -c(3.6, 3.1), breaks = 300)__\n\nLooking at the these histograms for many images, we can set up the following\nmodel for the purpose of segmentation: the signal in the cytoplasmic channels\nof the _Image_ `cells` is a mixture of two distributions, a log-Normal\nbackground and a foreground with another, unspecified, rather flat, but mostly\nnon-overlapping distribution10. Moreover the majority of pixels are from the\nbackground. We can then find robust estimates for the location and width\nparameters of the log-Normal component from the half range mode (implemented\nin the package\n**[genefilter](https://bioconductor.org/packages/genefilter/)**) and from the\nroot mean square of the values that lie left of the mode.\n\n10 This is an application of the ideas we saw in [Chapter 4](04-chap.html) on\nmixture models.\n\n    \n    \n    library(\"genefilter\")\n    bgPars = function(x) {\n      x    = log(x)\n      loc  = half.range.mode( x )\n      left = (x - loc)[ x < loc ]\n      wid  = sqrt( mean(left^2) )\n      c(loc = loc, wid = wid, thr = loc + 6*wid)\n    }\n    cellBg = apply(cellsSmooth, MARGIN = 3, FUN = bgPars)\n    cellBg __\n    \n    \n               [,1]        [,2]        [,3]\n    loc -2.90176965 -2.94427499 -3.52191681\n    wid  0.00635322  0.01121337  0.01528207\n    thr -2.86365033 -2.87699477 -3.43022437\n\nThe function defines as a threshold the location `loc` plus 6 widths `wid`11.\n\n11 The choice of the number 6 here is ad hoc; we could make the choice of\nthreshold more objective by estimating the weights of the two mixture\ncomponents and assigning each pixel to either fore- or background based on its\nposterior probability according to the mixture model. More advanced\nsegmentation methods use the fact that this is really a classification problem\nand include additional features and more complex classifiers to separate\nforeground and background regions (e.g., ([Berg et al. 2019](16-chap.html#ref-\nilastik:NatMeth:2019))).\n\n    \n    \n    hist(log(cellsSmooth[,,3]), xlim = -c(3.6, 3.1), breaks = 300)\n    abline(v = cellBg[c(\"loc\", \"thr\"), 3], col = c(\"brown\", \"red\"))__\n\n[![](11-chap_files/figure-html/fig-\nhistcellbody-3-1.png)](11-chap_files/figure-html/fig-histcellbody-3-1.png\n\"Figure 11.16: As in Figure fig-histcellbody-2, but with loc and thr shown by\nvertical lines.\")\n\nFigure 11.16: As in Figure 11.15, but with `loc` and `thr` shown by vertical\nlines.\n\nWe can now define `cytoplasmMask` by the union of all those pixels that are\nabove the threshold in the actin or tubulin image, or that we have already\nclassified as nuclear in the image `nuclei`.\n\n    \n    \n    cytoplasmMask = (cellsSmooth[,,2] > exp(cellBg[\"thr\", 2])) |\n           nuclei | (cellsSmooth[,,3] > exp(cellBg[\"thr\", 3]))__\n\nThe result is shown in the left panel of Figure 11.17. To define the cellular\nbodies, we can now simply extend the nucleus segmentation within this mask by\nthe Voronoi tessellation based propagation algorithm of Section 11.12. This\nmethod makes sure that there is exactly one cell body for each nucleus, and\nthe cell bodies are delineated in such a way that a compromise is reached\nbetween compactness of cell shape and following the actin and\n\\\\(\\alpha\\\\)-tubulin intensity signal in the images. In the terminology of the\n`propagate` algorithm, cell shape is kept compact by the \\\\(x\\\\) and \\\\(y\\\\)\ncomponents of the distance metric 11.4, and the actin signal is used for the\n\\\\(z\\\\) component. \\\\(\\lambda\\\\) controls the trade-off.\n\n    \n    \n    cellbodies = propagate(x = cellsSmooth[,,3], seeds = nuclei,\n                           lambda = 1.0e-2, mask = cytoplasmMask)__\n\nAs an alternative representation to the `colorLabel` plots, we can also\ndisplay the segmentations of nuclei and cell bodies on top of the original\nimages using the `paintObjects` function; the Images `nucSegOnNuc`,\n`nucSegOnAll` and `cellSegOnAll` that are computed below are show in the\nmiddle to right panels of Figure 11.17\n\n    \n    \n    cellsColor = EBImage::rgbImage(red   = cells[,,3],\n                                   green = cells[,,2],\n                                   blue  = cells[,,1])\n    nucSegOnNuc  = paintObjects(nuclei, tgt = EBImage::toRGB(cells[,,1]), col = \"#ffff00\")\n    nucSegOnAll  = paintObjects(nuclei,     tgt = cellsColor,    col = \"#ffff00\")\n    cellSegOnAll = paintObjects(cellbodies, tgt = nucSegOnAll,   col = \"#ff0080\")__\n\n[![](11-chap_files/figure-html/fig-cellbodies-1.png)](11-chap_files/figure-\nhtml/fig-cellbodies-1.png \"Figure 11.17 \\(a\\): \")\n\n(a)\n\n[![](11-chap_files/figure-html/fig-cellbodies-2.png)](11-chap_files/figure-\nhtml/fig-cellbodies-2.png \"Figure 11.17 \\(b\\): \")\n\n(b)\n\n[![](11-chap_files/figure-html/fig-cellbodies-3.png)](11-chap_files/figure-\nhtml/fig-cellbodies-3.png \"Figure 11.17 \\(c\\): \")\n\n(c)\n\n[![](11-chap_files/figure-html/fig-cellbodies-4.png)](11-chap_files/figure-\nhtml/fig-cellbodies-4.png \"Figure 11.17 \\(d\\): \")\n\n(d)\n\n[![](11-chap_files/figure-html/fig-cellbodies-5.png)](11-chap_files/figure-\nhtml/fig-cellbodies-5.png \"Figure 11.17 \\(e\\): \")\n\n(e)\n\nFigure 11.17: Steps in the segmentation of the cell bodies. From (a-d):\n`cytoplasmMask`, `cellbodies` (blue: DAPI, red: actin, green: alpha-tubulin),\n`nucSegOnNuc`, `nucSegOnAll`, `cellSegOnAll`.\n\n## 11.14 Feature extraction\n\nNow that we have the segmentations `nuclei` and `cellbodies` together with the\noriginal image data `cells`, we can compute various descriptors, or features,\nfor each cell. We already saw in the beginning of Section 11.11 how to use the\nbase R function `table` to determine the total number and sizes of the\nobjects. Let us now take this further and compute the mean intensity of the\nDAPI signal (`cells[,,1]`) in the segmented nuclei, the mean actin intensity\n(`cells[,,3]`) in the segmented nuclei and the mean actin intensity in the\ncell bodies.\n\n    \n    \n    meanNucInt       = tapply(cells[,,1], nuclei, mean)\n    meanActIntInNuc  = tapply(cells[,,3], nuclei, mean)\n    meanActIntInCell = tapply(cells[,,3], cellbodies, mean)__\n\nWe can visualize the features in pairwise scatterplots (Figure 11.18). We see\nthat they are correlated with each other, although each feature also carries\nindependent information.\n\n    \n    \n    library(\"GGally\")\n    ggpairs(tibble(meanNucInt, meanActIntInNuc, meanActIntInCell))__\n\n[![](11-chap_files/figure-html/fig-pairsint-1.png)](11-chap_files/figure-\nhtml/fig-pairsint-1.png \"Figure 11.18: Pairwise scatterplots of per-cell\nintensity descriptors.\")\n\nFigure 11.18: Pairwise scatterplots of per-cell intensity descriptors.\n\nWith a little more work, we could also compute more sophisticated summary\nstatistics – e.g., the ratio of nuclei area to cell body area; or entropies,\nmutual information and correlation of the different fluorescent signals in\neach cell body, as more or less abstract measures of cellular morphology. Such\nmeasures can be used, for instance, to detect subtle drug induced changes of\ncellular architecture.\n\nWhile it is easy and intuitive to perform these computations using basic R\nidioms like in the `tapply` expressions above, the package\n**[EBImage](https://bioconductor.org/packages/EBImage/)** also provides the\nfunction `computeFeatures` which efficiently computes a large collection of\nfeatures that have been commonly used in the literature (a pioneering\nreference is Boland and Murphy. ([2001](16-chap.html#ref-BolandMurphy))).\nDetails about this function are described in its manual page, and an example\napplication is worked through in the\n**[HD2013SGI](https://bioconductor.org/packages/HD2013SGI/)** vignette. Below,\nwe compute features for intensity, shape and texture for each cell from the\nDAPI channel using the nucleus segmentation (`nuclei`) and from the actin and\ntubulin channels using the cell body segmentation (`cytoplasmRegions`).\n\n    \n    \n    F1 = computeFeatures(nuclei,     cells[,,1], xname = \"nuc\",  refnames = \"nuc\")\n    F2 = computeFeatures(cellbodies, cells[,,2], xname = \"cell\", refnames = \"tub\")\n    F3 = computeFeatures(cellbodies, cells[,,3], xname = \"cell\", refnames = \"act\")\n    dim(F1)__\n    \n    \n    [1] 43 89\n\n`F1` is a matrix with 43 rows (one for each cell) and 89 columns, one for each\nof the computed features.\n\n    \n    \n    F1[1:3, 1:5]__\n    \n    \n      nuc.0.m.cx nuc.0.m.cy nuc.0.m.majoraxis nuc.0.m.eccentricity nuc.0.m.theta\n    1   119.5523   17.46895          44.86819            0.8372059     -1.314789\n    2   143.4511   15.83709          26.15009            0.6627672     -1.213444\n    3   336.5401   11.48175          18.97424            0.8564444      1.470913\n\nThe column names encode the type of feature, as well the color channel(s) and\nsegmentation mask on which it was computed. We can now use multivariate\nanalysis methods – like those we saw in Chapters [5](05-chap.html),\n[7](07-chap.html) and [9](09-chap.html) – for many dfferent tasks, such as\n\n  * detecting cell subpopulations (clustering)\n\n  * classifying cells into pre-defined cell types or phenotypes (classification)\n\n  * seeing whether the absolute or relative frequencies of the subpopulations or cell types differ between images that correspond to different biological conditions\n\nIn addition to these “generic” machine learning tasks, we also know the cell’s\nspatial positions, and in the following we will explore some ways to make use\nof these in our analyses.\n\n__\n\nTask\n\nUse explorative multivariate methods to visualize the matrices `F1`, `F2`,\n`F3`: PCA, heatmap. What’s special about the “outlier” cells?\n\n## 11.15 Spatial statistics: point processes\n\nIn the previous sections, we have seen ways how to use images of cells to\nextract their positions and various shape and morphological features. We’ll\nnow explore spatial distributions of the position. In order to have\ninteresting data to work on, we’ll change datasets and look at breast cancer\nlymph node biopsies.\n\n### 11.15.1 A case study: Interaction between immune cells and cancer cells\n\nThe lymph nodes function as an immunologic filter for the bodily fluid known\nas lymph. Antigens are filtered out of the lymph in the lymph node before\nreturning it to the circulation. Lymph nodes are found throughout the body,\nand are composed mostly of T cells, B cells, dendritic cells and macrophages.\nThe nodes drain fluid from most of our tissues. The lymph ducts of the breast\nusually drain to one lymph node first, before draining through the rest of the\nlymph nodes underneath the arm. That first lymph node is called the _sentinel_\nlymph node. In a similar fashion as the spleen, the macrophages and dendritic\ncells that capture antigens present these foreign materials to T and B cells,\nconsequently initiating an immune response.\n\nT lymphocytes are usually divided into two major subsets that are functionally\nand phenotypically different.\n\n  * CD4+ T-cells, or T helper cells: they are pertinent coordinators of immune regulation. The main function of T helper cells is to augment or potentiate immune responses by the secretion of specialized factors that activate other white blood cells to fight off infection.\n\n  * CD8+ T cells, or T killer/suppressor cells: these cells are important in directly killing certain tumor cells, viral-infected cells and sometimes parasites. The CD8+ T cells are also important for the down-regulation of immune responses.\n\nBoth types of T cells can be found throughout the body. They often depend on\nthe secondary lymphoid organs (the lymph nodes and spleen) as sites where\nactivation occurs.\n\nDendritic Cells or CD1a cells are antigen-presenting cells that process\nantigen and present peptides to T cells.\n\nTyping the cells can be done by staining the cells with protein antibodies\nthat provide specific signatures. For instance, different types of immune\ncells have different proteins expressed, mostly in their cell membranes.\n\n[![](imgs/SixPanelsLymphsmall.jpg)](imgs/SixPanelsLymphsmall.jpg\n\"Figure 11.19: Biopsy of an enlarged lymph node revealed an intact capsule and\nobliterated sinuses \\(upper left panel, stained with hematoxylin and eosin,\noriginal magnification \\\\times 100\\). The infiltrate was composed of an\nadmixture of small lymphocytes, macrophages, and plasma cells \\(upper right\npanel, hematoxylin and eosin, original magnification \\\\times 400\\). The\ninfiltrate was composed of a mixture of CD3 positive T-cells \\(including both\nCD4 and CD8 positive cells\\) and CD20 positive B-cells. Numerous macrophages\nwere also CD4 positive. \\(From: Hurley et al., Diagnostic Pathology \\(2008\\)\n3:13\\)\")\n\nFigure 11.19: Biopsy of an enlarged lymph node revealed an intact capsule and\nobliterated sinuses (upper left panel, stained with hematoxylin and eosin,\noriginal magnification \\\\(\\times\\\\) 100). The infiltrate was composed of an\nadmixture of small lymphocytes, macrophages, and plasma cells (upper right\npanel, hematoxylin and eosin, original magnification \\\\(\\times\\\\) 400). The\ninfiltrate was composed of a mixture of CD3 positive T-cells (including both\nCD4 and CD8 positive cells) and CD20 positive B-cells. Numerous macrophages\nwere also CD4 positive. (From: Hurley et al., Diagnostic Pathology (2008)\n3:13)\n\n[![](imgs/testscan_1_2_RGB99-4525D.jpg)](imgs/testscan_1_2_RGB99-4525D.jpg\n\"Figure 11.20: A stained lymph node; this image is the basis for the spatial\ndata in brcalymphnode.\")\n\nFigure 11.20: A stained lymph node; this image is the basis for the spatial\ndata in `brcalymphnode`.\n\nWe’ll look at data by Setiadi et al. ([2010](16-chap.html#ref-Setiadi2010)).\nAfter segmentating the image shown in Figure 11.20 using the segmentation\nmethod _GemIdent_ ([Holmes, Kapelner, and Lee 2009](16-chap.html#ref-\nHolmes2009)), the authors obtained the coordinates and the type of all the\ncells in the image. We call this type of data a _marked point process_ , and\nit can be seen as a simple table with 3 columns.\n\n    \n    \n    library(\"readr\")\n    library(\"dplyr\")\n    cellclasses = c(\"T_cells\", \"Tumor\", \"DCs\", \"other_cells\")\n    brcalymphnode = lapply(cellclasses, function(k) {\n        read_csv(file.path(\"..\", \"data\", sprintf(\"99_4525D-%s.txt\", k))) |>\n        transmute(x = globalX, y = globalY, class = k)\n    }) |> bind_rows() |> mutate(class = factor(class))\n    \n    brcalymphnode __\n    \n    \n    # A tibble: 209,462 × 3\n           x     y class  \n       <dbl> <dbl> <fct>  \n     1  6355 10382 T_cells\n     2  6356 10850 T_cells\n     3  6357 11070 T_cells\n     4  6357 11082 T_cells\n     5  6358 10600 T_cells\n     6  6361 10301 T_cells\n     7  6369 10309 T_cells\n     8  6374 10395 T_cells\n     9  6377 10448 T_cells\n    10  6379 10279 T_cells\n    # ℹ 209,452 more rows\n    \n    \n    table(brcalymphnode$class)__\n    \n    \n            DCs other_cells     T_cells       Tumor \n            878       77081      103681       27822 \n\nWe see that there are over a 100,000 T cells, around 28,000 tumor cells, and\nonly several hundred dendritic cells. Let’s plot the \\\\(x\\\\)\\- and\n\\\\(y\\\\)-positions of the cells (Figure 11.21).\n\n    \n    \n    ggplot(filter(brcalymphnode, class %in% c(\"T_cells\", \"Tumor\")),\n       aes(x = x, y = y, col = class)) + geom_point(shape = \".\") +\n       facet_grid( . ~ class) + guides(col = \"none\")__\n\n[![](11-chap_files/figure-html/fig-brcalntcells-1.png)](11-chap_files/figure-\nhtml/fig-brcalntcells-1.png \"Figure 11.21: Scatterplot of the x and y\npositions of the T- and tumor cells in brcalymphnode. The locations were\nobtained by a segmentation algorithm from a high resolution version of\nFigure fig-stainedlymphnode. Some rectangular areas in the T-cells plot are\nsuspiciously empty, this could be because the corresponding image tiles within\nthe overall composite image went missing, or were not analyzed.\")\n\nFigure 11.21: Scatterplot of the \\\\(x\\\\) and \\\\(y\\\\) positions of the T- and\ntumor cells in `brcalymphnode`. The locations were obtained by a segmentation\nalgorithm from a high resolution version of Figure 11.20. Some rectangular\nareas in the T-cells plot are suspiciously empty, this could be because the\ncorresponding image tiles within the overall composite image went missing, or\nwere not analyzed.\n\n__\n\nQuestion 11.12\n\nCompare Figures 11.20 and 11.21. Why are the \\\\(y\\\\)-axis inverted relative to\neach other?\n\n__\n\nSolution\n\n__\n\nFigure 11.20 follows the convention for image data, where the origin is in the\ntop left corner of the image, while Figure 11.21 follows the convention for\nCartesian plots, with the origin at the bottom left.\n\nTo use the functionality of the\n**[spatstat](https://cran.r-project.org/web/packages/spatstat/)** package, it\nis convenient to convert our data in `brcalymphnode` into an object of class\n_ppp_ ; we do this by calling the eponymous function.\n\n    \n    \n    library(\"spatstat\")__\n    \n    \n    ln = with(brcalymphnode, ppp(x = x, y = y, marks = class, \n                                 xrange = range(x), yrange = range(y)))\n    ln __\n    \n    \n    Marked planar point pattern: 209462 points\n    Multitype, with levels = DCs, other_cells, T_cells, Tumor \n    window: rectangle = [3839, 17276] x [6713, 23006] units\n\n_ppp_ objects are designed to capture realizations of a **spatial point\nprocess** , that is, a set of isolated points located in a mathematical space;\nin our case, as you can see above, the space is a two-dimensional rectangle\nthat contains the range of the \\\\(x\\\\)\\- and \\\\(y\\\\)-coordinates. In addition,\nthe points can be _marked_ with certain properties. In `ln`, the mark is\nsimply the _factor_ variable `class`. More generally, it could be several\nattributes, times, or quantitative data as well. There are similarities\nbetween a marked point process and an image, although for the former, the\npoints can lie anywhere within the space, whereas in an image, the pixels are\ncovering the space in regular, rectangular way.\n\n### 11.15.2 Convex hull\n\nAbove, we (implicitly) confined the point process to lie in a rectangle. In\nfact, the data generating process is more confined, by the shape of the tissue\nsection. We can approximate this and compute a tighter region from the convex\nhull of the points12.\n\n12 You can use `str(cvxhull)` to look at the internal structure of this S3\nobject.\n\n    \n    \n    cvxhull = convexhull.xy(cbind(ln$x, ln$y))\n    ggplot(as_tibble(cvxhull$bdry[[1]]), aes(x = x, y = y)) +\n      geom_polygon(fill = NA, col = \"black\") + geom_point() + coord_fixed()__\n\n[![](11-chap_files/figure-html/fig-convhull-1.png)](11-chap_files/figure-\nhtml/fig-convhull-1.png \"Figure 11.22: Polygon describing the convex hull of\nthe points in ln.\")\n\nFigure 11.22: Polygon describing the convex hull of the points in `ln`.\n\nWe can see the polygon in Figure 11.22 and now call `ppp` again, this time\nwith the polygon.\n\n    \n    \n    ln = with(brcalymphnode, ppp(x = x, y = y, marks = class, \n                                 poly = cvxhull$bdry[[1]]))\n    ln __\n    \n    \n    Marked planar point pattern: 209462 points\n    Multitype, with levels = DCs, other_cells, T_cells, Tumor \n    window: polygonal boundary\n    enclosing rectangle: [3839, 17276] x [6713, 23006] units\n\n### 11.15.3 Other ways of defining the space for the point process\n\nWe do not have to use the convex hull to define the space on which the point\nprocess is considered. Alternatively, we could have provided an image mask to\n`ppp` that defines the space based on prior knowledge; or we could use density\nestimation on the sampled points to only identify a region in which there is a\nhigh enough point density, ignoring sporadic outliers. These choices are part\nof the analyst’s job when considering spatial point processes.\n\n## 11.16 First order effects: the intensity\n\nOne of the most basic questions of spatial statistics is whether neighboring\npoints are “clustering”, i.e., whether and to what extent they are closer to\neach other than expected “by chance”; or perhaps the opposite, whether they\nseem to repel each other. There are many examples where this kind of question\ncan be asked, for instance\n\n  * crime patterns within a city,\n\n  * disease patterns within a country,\n\n  * soil measurements in a region.\n\nIt is usually not hard to find reasons why such patterns exist: good and bad\nneighborhoods, local variations in lifestyle or environmental exposure, the\ncommon geological history of the soil. Sometimes there may also be mechanisms\nby which the observed events attract or repel each other – the proverbial\n“broken windows” in a neighborhood, or the tendency of many cell types to\nstick close to other cells.\n\nThe cell example highlights that spatial clustering (or anticlustering) can\ndepend on the objects’ attributes (or marks, in the parlance of spatial point\nprocesses). It also highlights that the answer can depend on the length scale\nconsidered. Even if cells attract each other, they have a finite size, and\ncannot occupy the same space. So there will be some minmal distance between\nthem, on the scale of which they essentially repel each other, while at\nfurther distances, they attract.\n\nTo attack these questions more quantitatively, we need to define a\nprobabilistic model of what we expect _by chance_. Let’s count the number of\npoints lying in a subregion, say, a circle of area \\\\(a\\\\) around a point\n\\\\(p=(x,y)\\\\); call this \\\\(N(p, a)\\\\)13 The mean and covariance of \\\\(N\\\\)\nprovide first and second order properties. The first order is the intensity of\nthe process:\n\n13 As usual, we use the uppercase notation \\\\(N(p, a)\\\\) for the random\nvariable, and the lowercase \\\\(n(p, a)\\\\) for its realizations, or samples.\n\n\\\\[ \\lambda(p) = \\lim_{a\\rightarrow 0} \\frac{E[ N(p, a)]}{a}. \\tag{11.6}\\\\]\n\nHere we used infinitesimal calculus to define the local intensity\n\\\\(\\lambda(p)\\\\). As for time series, a stationary process is one where we\nhave homogeneity all over the region, i.e., \\\\(\\lambda(p) = \\text{const.}\\\\);\nthen the intensity in an area \\\\(A\\\\) is proportional to the area:\n\\\\(E[N(\\cdot, A)] = \\lambda A\\\\). Later we’ll also look at higher order\nstatistics, such as the spatial covariance\n\n\\\\[ \\gamma(p_1, p_2) = \\lim_{a \\rightarrow 0} \\frac{E \\left[ \\left(N(p_1, a) -\nE[ N(p_1, a)] \\right) \\left(N(p_2, a) - E[ N(p_2, a)] \\right) \\right]}{a^2}.\n\\tag{11.7}\\\\]\n\nIf the process is stationary, this will only depend on the relative position\nof the two points (the vector between them). If it only depends on the\ndistance, i.e., only on the length but not on the direction of the vector, it\nis called second order isotropic.\n\n[![](imgs/Rain-Drops-small.jpg)](imgs/Rain-Drops-small.jpg \"Figure 11.23: Rain\ndrops falling on the floor are modelled by a Poisson process. The number of\ndrops falling on a particular spot only depends on the rate \\\\lambda \\(and on\nthe size of the spot\\), but not on what happens at other spots.\")\n\nFigure 11.23: Rain drops falling on the floor are modelled by a Poisson\nprocess. The number of drops falling on a particular spot only depends on the\nrate \\\\(\\lambda\\\\) (and on the size of the spot), but not on what happens at\nother spots.\n\n### 11.16.1 Poisson Process\n\nThe simplest spatial process is the Poisson process. We will use it as a null\nmodel against which to compare our data. It is stationary with intensity\n\\\\(\\lambda\\\\), and there are no further dependencies between occurrences of\npoints in non-overlapping regions of the space. Moreover, the number of points\nin a region of area \\\\(A\\\\) follows a Poisson distribution with rate\n\\\\(\\lambda A\\\\).\n\n### 11.16.2 Estimating the intensity\n\nTo estimate the intensity, divide up the area into subregions, small enough to\nsee potential local variations of \\\\(\\lambda(p)\\\\), but big enough to contain\na sufficient sample of points. This is analogous to 2D density estimation, and\ninstead of hard region boundaries, we can use a smooth kernel function\n\\\\(K\\\\).\n\n\\\\[ \\hat{\\lambda}(p) = \\sum_i e(p_i) K(p-p_i). \\tag{11.8}\\\\]\n\nThe kernel function depends on a smoothing parameter, \\\\(\\sigma\\\\), the larger\nit is, the larger the regions over which we compute the local estimate for\neach \\\\(p\\\\). \\\\(e(p)\\\\) is an edge correction factor, and takes into account\nthe estimation bias caused when the support of the kernel (the “smoothing\nwindow”) would fall outside the space on which the point process is defined.\nThe function `density`, which is defined for _ppp_ objects in the\n**[spatstat](https://cran.r-project.org/web/packages/spatstat/)** package,\nimplements Equation 11.8.\n\n    \n    \n    d = density(subset(ln, marks == \"Tumor\"), edge=TRUE, diggle=TRUE)\n    plot(d)__\n\n[![](11-chap_files/figure-html/fig-densityppp1-1.png)](11-chap_files/figure-\nhtml/fig-densityppp1-1.png \"Figure 11.24: Intensity estimate for the cells\nmarked Tumor in ppp. The support of the estimate is the polygon that we\nspecified earlier on \\(Figure fig-convhull\\).\")\n\nFigure 11.24: Intensity estimate for the cells marked `Tumor` in `ppp`. The\nsupport of the estimate is the polygon that we specified earlier on (Figure\n11.22).\n\nThe plot is shown in Figure 11.24.\n\n__\n\nQuestion 11.13\n\nHow does the estimate look without edge correction?\n\n__\n\nSolution\n\n__\n\n    \n    \n    d0 = density(subset(ln, marks == \"Tumor\"), edge = FALSE)\n    plot(d0)__\n\n[![](11-chap_files/figure-html/fig-densityppp0-1.png)](11-chap_files/figure-\nhtml/fig-densityppp0-1.png \"Figure 11.25: As Figure fig-densityppp1, but\nwithout edge correction.\")\n\nFigure 11.25: As Figure 11.24, but without edge correction.\n\nNow estimated intensity is smaller towards the edge of the space, reflecting\nedge bias (Figure 11.25).\n\n`density` gives us as estimate of the _intensity_ of the point process. A\nrelated, but different task is the estimation of the (conditional)\n_probability_ of being a particular cell class. The function `relrisk`\ncomputes a nonparametric estimate of the spatially varying risk of a\nparticular event type. We’re interested in the probability that a cell that is\npresent at particular spatial location will be a tumor cell (Figure 11.26).\n\n    \n    \n    rr = relrisk(ln, sigma = 250)__\n    \n    \n    plot(rr)__\n\n[![](11-chap_files/figure-html/fig-relrisk-1.png)](11-chap_files/figure-\nhtml/fig-relrisk-1.png \"Figure 11.26: Estimates of the spatially varying\nprobability of each of the cell classes, conditional on there being cells.\")\n\nFigure 11.26: Estimates of the spatially varying probability of each of the\ncell classes, conditional on there being cells.\n\n## 11.17 Second order effects: spatial dependence\n\nIf we pick a point at random in our spatial process, what is the distance\n\\\\(W\\\\) to its nearest neighbor? For a homogenous Poisson process, the\ncumulative distribution function of this distance is\n\n\\\\[ G(w) = P(W\\leq w) = 1-e^{-\\lambda \\pi w^2}. \\tag{11.9}\\\\]\n\nPlotting \\\\(G\\\\) gives a way of noticing departure from the homogenous Poisson\nprocess. An estimator of \\\\(G\\\\), which also takes into account edge effects\n([A. J. Baddeley 1998](16-chap.html#ref-Baddeley1998); [Ripley\n1988](16-chap.html#ref-RipleySISS1988)), is provided by the function `Gest` of\nthe **[spatstat](https://cran.r-project.org/web/packages/spatstat/)** package.\n\n    \n    \n    gln = Gest(ln)\n    gln __\n    \n    \n    Function value object (class 'fv')\n    for the function r -> G(r)\n    .....................................................................\n            Math.label      Description                                  \n    r       r               distance argument r                          \n    theo    G[pois](r)      theoretical Poisson G(r)                     \n    han     hat(G)[han](r)  Hanisch estimate of G(r)                     \n    rs      hat(G)[bord](r) border corrected estimate of G(r)            \n    km      hat(G)[km](r)   Kaplan-Meier estimate of G(r)                \n    hazard  hat(h)[km](r)   Kaplan-Meier estimate of hazard function h(r)\n    theohaz h[pois](r)      theoretical Poisson hazard function h(r)     \n    .....................................................................\n    Default plot formula:  .~r\n    where \".\" stands for 'km', 'rs', 'han', 'theo'\n    Recommended range of argument r: [0, 20.998]\n    Available range of argument r: [0, 52.443]\n    \n    \n    library(\"RColorBrewer\")\n    plot(gln, xlim = c(0, 10), lty = 1, col = brewer.pal(4, \"Set1\"))__\n\n[![](11-chap_files/figure-html/fig-Gest-1.png)](11-chap_files/figure-html/fig-\nGest-1.png \"Figure 11.27: Estimates of G, using three different edge effect\ncorrections –which here happen to essentially lie on top of each other– and\nthe theoretical distribution for a homogenous Poisson process.\")\n\nFigure 11.27: Estimates of \\\\(G\\\\), using three different edge effect\ncorrections –which here happen to essentially lie on top of each other– and\nthe theoretical distribution for a homogenous Poisson process.\n\nThe printed summary of the object `gln` gives an overview of the computed\nestimates; further explanations are in the manual page of `Gest`. In Figure\n11.27 we see that the empirical distribution function and that of our null\nmodel, a homogenous Poisson process with a suitably chosen intensity, cross at\naround 4.5 units. Cell to cell distances that are shorter than this value are\nless likely than for the null model, in particular, there are essentially no\ndistances below around 2; this, of course, reflects the fact that our cells\nhave finite size and cannot overlap the same space. There seems to be trend to\navoid very large distances –compared to the Poisson process–, perhaps\nindicative of a tendency of the cells to cluster.\n\n### 11.17.1 Ripley’s \\\\(K\\\\) function\n\nIn homogeneous spatial Poisson process, if we randomly pick any point and\ncount the number of points within a distance of at most \\\\(r\\\\), we expect\nthis number to grow as the area of the circle, \\\\(\\pi r^2\\\\). For a given\ndataset, we can compare this expectation to the observed number of neighbors\nwithin distance \\\\(r\\\\), averaged across all points.\n\nThe \\\\(K\\\\) function (variously called _Ripley’s \\\\(K\\\\)-function_ or the\n_reduced second moment function_) of a stationary point process is defined so\nthat \\\\(\\lambda K(r)\\\\) is the expected number of (additional) points within a\ndistance \\\\(r\\\\) of a given, randomly picked point. Remember that\n\\\\(\\lambda\\\\) is the intensity of the process, i.e., the expected number of\npoints per unit area. The \\\\(K\\\\) function is a second order moment property\nof the process.\n\nThe definition of \\\\(K\\\\) can be generalized to inhomogeneous point processes\nand written as in ([A. Baddeley, Moller, and Waagepetersen\n2000](16-chap.html#ref-Baddeley2000)),\n\n\\\\[ K_{\\scriptsize \\mbox{inhom}}(r)= \\sum_{i,j} 𝟙_{d(p_i, p_j) \\le r} \\times\n\\frac{e(p_i, p_j, r)} { \\lambda(x_i) \\lambda(x_j) }, \\tag{11.10}\\\\]\n\nwhere \\\\(d(p_i, p_j)\\\\) is the distance between points \\\\(p_i\\\\) and\n\\\\(p_j\\\\), and \\\\(e(p_i, p_j, r)\\\\) is an edge correction factor14. For\nestimation and visualisation, it is useful to consider a transformation of\n\\\\(K\\\\) (and analogously, of \\\\(K_{\\scriptsize \\mbox{inhom}}\\\\)), the so-\ncalled \\\\(L\\\\) function.\n\n14 See the manual page of `Kinhom` for more.\n\n\\\\[ L(r)=\\sqrt{\\frac{K(r)}{\\pi}}. \\tag{11.11}\\\\]\n\nFor a homogeneous spatial Poisson process, the theoretical value is \\\\(L(r) =\nr\\\\). By comparing that to the estimate of \\\\(L\\\\) for a dataset, we can learn\nabout inter-point dependence and spatial clustering. The square root in\nEquation 11.11 has the effect of stabilising the variance of the estimator, so\nthat compared to \\\\(K\\\\), \\\\(L\\\\) is more appropriate for data analysis and\nsimulations. The computations in the function `Linhom` of the\n**[spatstat](https://cran.r-project.org/web/packages/spatstat/)** package take\na few minutes for our data (Figure 11.28).\n\n    \n    \n    Lln = Linhom(subset(ln, marks == \"T_cells\"))__\n    \n    \n    Lln __\n    \n    \n    Function value object (class 'fv')\n    \n    \n    for the function r -> L[inhom](r)\n    \n    \n    ................................................................................\n               Math.label                \n    r          r                         \n    theo       L[pois](r)                \n    border     {hat(L)[inhom]^{bord}}(r) \n    bord.modif {hat(L)[inhom]^{bordm}}(r)\n               Description                                      \n    r          distance argument r                              \n    theo       theoretical Poisson L[inhom](r)                  \n    border     border-corrected estimate of L[inhom](r)         \n    bord.modif modified border-corrected estimate of L[inhom](r)\n    ................................................................................\n    Default plot formula:  .~.x\n    where \".\" stands for 'bord.modif', 'border', 'theo'\n    Recommended range of argument r: [0, 694.7]\n    Available range of argument r: [0, 694.7]\n    \n    \n    plot(Lln, lty = 1, col = brewer.pal(3, \"Set1\"))__\n\n[![](11-chap_files/figure-html/fig-images-Lln-1.png)](11-chap_files/figure-\nhtml/fig-images-Lln-1.png \"Figure 11.28: Estimate of L_{\\\\scriptsize\n\\\\mbox{inhom}}, Equations eq-kinhom and eq-Lest, of the T cell pattern.\")\n\nFigure 11.28: Estimate of \\\\(L_{\\scriptsize \\mbox{inhom}}\\\\), Equations 11.10\nand 11.11, of the T cell pattern.\n\nWe could now proceed with looking at the \\\\(L\\\\) function also for other cell\ntypes, and for different tumors as well as for healthy lymph nodes. This is\nwhat Setiadi and colleagues did in their report ([Setiadi et al.\n2010](16-chap.html#ref-Setiadi2010)), where by comparing the spatial grouping\npatterns of T and B cells between healthy and breast cancer lymph nodes they\nsaw that B cells appeared to lose their normal localization in the\nextrafollicular region of the lymph nodes in some tumors.\n\n#### The pair correlation function\n\ndescribes how point density varies as a function of distance from a reference\npoint. It provides a perspective inspired by physics for looking at spatial\nclustering. For a stationary point process, it is defined as\n\n\\\\[ g(r)=\\frac{1}{2\\pi r}\\frac{dK}{dr}(r). \\tag{11.12}\\\\]\n\nFor a stationary Poisson process, the pair correlation function is identically\nequal to 1. Values \\\\(g(r) < 1\\\\) suggest inhibition between points; values\ngreater than 1 suggest clustering.\n\nThe **[spatstat](https://cran.r-project.org/web/packages/spatstat/)** package\nallows computing estimates of \\\\(g\\\\) even for inhomogeneous processes, if we\ncall `pcf` as below, the definition 11.12 is applied to the estimate of\n\\\\(K_{\\scriptsize \\mbox{inhom}}\\\\).\n\n    \n    \n    pcfln = pcf(Kinhom(subset(ln, marks == \"T_cells\")))__\n    \n    \n    plot(pcfln, lty = 1)\n    plot(pcfln, lty = 1, xlim = c(0, 10))__\n\n[![](11-chap_files/figure-html/fig-images-pcf-1.png)](11-chap_files/figure-\nhtml/fig-images-pcf-1.png \"Figure 11.29 \\(a\\): \")\n\n(a)\n\n[![](11-chap_files/figure-html/fig-images-pcf-2.png)](11-chap_files/figure-\nhtml/fig-images-pcf-2.png \"Figure 11.29 \\(b\\): \")\n\n(b)\n\nFigure 11.29: Estimate of the pair correlation function, Equation 11.12, of\nthe T cell pattern.\n\nAs we see in Figure 11.29, the T cells cluster, although at very short\ndistances, there is also evidence for avoidance.\n\n__\n\nQuestion 11.14\n\nThe sampling resolution in the plot of the pair correlation function in the\nbottom panel of Figure 11.29 is low; how can it be increased?\n\n__\n\nSolution\n\n__\n\nThe answer lies in the `r` argument of the `Kinhom` function; see Figure\n11.30.\n\n    \n    \n    pcfln2 = pcf(Kinhom(subset(ln, marks == \"T_cells\"),\n                        r = seq(0, 10, by = 0.2)))\n    plot(pcfln2, lty = 1)__\n\n[![](11-chap_files/figure-html/fig-samplingpcf-1.png)](11-chap_files/figure-\nhtml/fig-samplingpcf-1.png \"Figure 11.30: Answer to Question wrn-images-\nsamplingpcf: as in the bottom panel of Figure fig-images-pcf, but with denser\nsampling.\")\n\nFigure 11.30: Answer to Question 11.14: as in the bottom panel of Figure\n11.29, but with denser sampling.\n\n## 11.18 Summary of this chapter\n\nWe learned to work with image data in R. Images are basically just arrays, and\nwe can use familiar idioms to manipulate them. We can extract quantitative\nfeatures from images, and then many of the analytical questions are not unlike\nthose with other high-throughput data: we summarize the features into\nstatistics such as means and variances, do hypothesis testing for differences\nbetween conditions, perform analysis of variance, apply dimension reduction,\nclustering and classification.\n\nOften we want to compute such quantitative features not on the whole image,\nbut for individual objects shown in the image, and then we need to first\nsegment the image to demarcate the boundaries of the objects of interest. We\nsaw how to do this for images of nuclei and cells.\n\nWhen the interest is on the positions of the objects and how these positions\nrelate to each other, we enter the realm of _spatial statistics_. We have\nexplored some of the functionality of the\n**[spatstat](https://cran.r-project.org/web/packages/spatstat/)** package,\nhave encountered the point process class, and we learned some of the specific\ndiagnostic statistics used for point patterns, like Ripley’s \\\\(K\\\\) function.\n\n## 11.19 Further reading\n\n  * There is a vast amount of literature on image analysis. When navigating it, it is helpful to realize that the field is driven by two forces: specific application domains (we saw the analysis of high-throughput cell-based assays) and available computer hardware. Some algorithms and concepts that were developed in the 1970s are still relevant, others have been superseeded by more systematic and perhaps computationally more intensive methods. Many algorithms imply certain assumptions about the nature of the data and and scientific questions asked, which may be fine for one application, but need a fresh look in another. A classic introduction is _The Image Processing Handbook_ ([Russ and Neal 2015](16-chap.html#ref-RussImageProcessingHandbook)), which now is its seventh edition.\n\n  * For spatial point pattern analysis, Diggle ([2013](16-chap.html#ref-DiggleSPP); [Ripley 1988](16-chap.html#ref-RipleySISS1988); [Cressie 1991](16-chap.html#ref-CressieSSD1991); [Chiu et al. 2013](16-chap.html#ref-MeckeSG2013)).\n\n## 11.20 Exercises\n\n__\n\nExercise 11.1\n\nLoad some images from your personal photo library into R and try out the\nmanipulations from Section 11.6 on them.\n\n__\n\nExercise 11.2\n\nExplore the effect of the parameter `lambda` in the `propagate` function\n(Sections 11.12, 11.13) using a\n**[shiny](https://cran.r-project.org/web/packages/shiny)** app that displays\nthe `cellbodies` image as in Figure 11.17.\n\n__\n\nExercise 11.3\n\nConsider the two-dimensional empirical autocorrelation function,\n\n\\\\[ a(v_x, v_y) = \\frac{1}{|I|} \\sum_{(x,y)\\in I} B(x, y)\\;B(x+v_x, \\, y+v_y),\n\\tag{11.13}\\\\]\n\nwhere \\\\(B\\\\) is an image, i.e., a function over the set of pixels \\\\(I\\\\),\nthe tuple \\\\((x,y)\\\\) runs over all the pixel coordinates, and \\\\(v=(v_x,\nv_y)\\\\) is the offset vector. Using the [Wiener–Khinchin\ntheorem](https://mathworld.wolfram.com/Wiener-KhinchinTheorem.html), we can\ncompute this function efficiently using the Fast Fourier Transformation.\n\n    \n    \n    autocorr2d = function(x) {\n      y = fft(x/sum(x))\n      abs(gsignal::fftshift(fft(y * Conj(y), inverse = TRUE), MARGIN = 1:2)) \n    }__\n\nBelow, we’ll use this little helper function, which shows a matrix as a\nheatmap with **[ggplot2](https://cran.r-project.org/web/packages/ggplot2)**\n(similar to base R’s `image`).\n\n    \n    \n    matrix_as_heatmap = function(m)\n      ggplot(reshape2::melt(m), aes(x = Var1, y = Var2, fill = value)) + \n        geom_tile() + coord_fixed() +\n        scale_fill_continuous(type = \"viridis\") +\n        scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0))__\n\nNow let’s apply `autocorr2d` to each of the three color channels separately.\nThe result is shown in Figure 11.31.\n\n    \n    \n    nm = dimnames(cells)[[3]]\n    ac = lapply(nm, function(i) autocorr2d(cells[,, i])) |> setNames(sub(\"^image-\", \"\", nm))\n    \n    for (w in names(ac)) \n      print(matrix_as_heatmap(ac[[w]]) + ggtitle(w))\n    \n    cy = dim(cells)[1] / 2\n    cx = dim(cells)[2] / 2\n    r  = round(sqrt((col(cells[,,1]) - cx)^2 + (row(cells[,,1]) - cy)^2))\n    \n    matrix_as_heatmap(r) + ggtitle(\"radius r\")__\n\n[![](11-chap_files/figure-html/fig-autocorr2d-1.png)](11-chap_files/figure-\nhtml/fig-autocorr2d-1.png \"Figure 11.31 \\(a\\): \")\n\n(a)\n\n[![](11-chap_files/figure-html/fig-autocorr2d-2.png)](11-chap_files/figure-\nhtml/fig-autocorr2d-2.png \"Figure 11.31 \\(b\\): \")\n\n(b)\n\n[![](11-chap_files/figure-html/fig-autocorr2d-3.png)](11-chap_files/figure-\nhtml/fig-autocorr2d-3.png \"Figure 11.31 \\(c\\): \")\n\n(c)\n\n[![](11-chap_files/figure-html/fig-autocorr2d-4.png)](11-chap_files/figure-\nhtml/fig-autocorr2d-4.png \"Figure 11.31 \\(d\\): \")\n\n(d)\n\nFigure 11.31: Autocorrelation functions of the three color channels of the\n`cells` image, shown as heatmaps. The peaks in the centres correspond to\nsignal correlations over short distances. Also shown is the radial coordinate\n`r`.\n\nSince the images are (or should be) isotropic, i.e., there is no preferred\ndirection, we can average over the angular coordinate. The result is shown in\nFigure 11.32. We can see that the signals in the different color channels have\ndifferent length scales.\n\n    \n    \n    aggregate_by_radius = function(x, r)\n      tibble(x = as.vector(x),\n             r = as.vector(r)) |>\n      group_by(r) |>\n      summarize(value = mean(x))\n    \n    lapply(names(ac), function(w) \n      cbind(channel = w, \n            aggregate_by_radius(ac[[w]], r))\n      ) |> \n      bind_rows() |> \n      dplyr::filter(r <= 50) |>\n      ggplot(aes(x = r, y = value, col = channel)) + geom_line() + \n        scale_color_manual(values = c(`Cy3` = \"red\", `FITC` = \"green\", `DAPI` = \"blue\"))__\n\nExtend the `autocorr2d` function to also compute the cross-correlation between\ndifferent channels.\n\n  * What is the motivation behind the `sum` normalization in the above implementation `autocorr2d`?\n  * Would it make sense to subtract the mean of `x` before the other computations?\n  * What is the relation between this function and the usual empirical variance or correlation, i.e. the functions `var` and `sd` in base R?\n  * How might plots such as Figure 11.32 be used for the construction of quality metrics in a high-throughput screening setting, i.e., when thousands or millions of images need to be analyzed?\n  * How would a 3- or \\\\(n\\\\)-dimensional extension of `autocorr2d` look like? What would it be good for?\n\n[![](11-chap_files/figure-html/fig-autocorr1d-1.png)](11-chap_files/figure-\nhtml/fig-autocorr1d-1.png \"Figure 11.32: Autocorrelation functions of the\nthree color channels of the cells image, aggregated by radius.\")\n\nFigure 11.32: Autocorrelation functions of the three color channels of the\n`cells` image, aggregated by radius.\n\n__\n\nExercise 11.4\n\nHave a look at the workshop “Working with Image Data”\n<https://github.com/wolfganghuber/WorkingWithImageData>, which goes through\nsome of the same content as this chapter, but on different images, and also\nhas additional examples on segmentation and optical flow.\n\n__\n\nExercise 11.5\n\nCompute and display the Voronoi tessellation for the Ukrainian cities from\n[Chapter 9](09-chap.html). Either use their MDS-coordinates in the 2D plane\nwith Euclidean distances, or the latitudes and longitudes using the great\ncircle distance (Haversine formula).\n\n__\n\nExercise 11.6\n\nDownload 3D image data from light sheet microscopy15, load it into an\n**[EBImage](https://bioconductor.org/packages/EBImage/)** _Image_ object and\nexplore the data.\n\n15 For instance, <http://www.digital-embryo.org>\n\nBaddeley, Adrain, Jesper Moller, and Rasmus Waagepetersen. 2000. “Non- and\nSemiparametric Estimation of Interaction in Inhomogeneous Point Patterns.”\n_Statistica Neerlandica_ 54: 329–50.\n\nBaddeley, Adrian J. 1998. “Spatial Sampling and Censoring.” In _Stochastic\nGeometry: Likelihood and Computation_ , edited by O. E. Barndorff-Nielsen, W.\nS. Kendall, and M. N. M. van Lieshout, 37–78. Chapman; Hall.\n\nBerg, Stuart, Dominik Kutra, Thorben Kroeger, Christoph N Straehle, Bernhard X\nKausler, Carsten Haubold, Martin Schiegg, et al. 2019. “Ilastik: Interactive\nMachine Learning for (Bio)image Analysis.” _Nature Methods_ 16 (12): 1226–32.\n\nBoland, Michael V., and Robert F. Murphy. 2001. “A neural network classifier\ncapable of recognizing the patterns of all major subcellular structures in\nfluorescence microscope images of HeLa cells.” _Bioinformatics_ 17 (12):\n1213–23.\n\nCarpenter, Anne E, Thouis R Jones, Michael R Lamprecht, Colin Clarke, In Han\nKang, Ola Friman, David A Guertin, Joo Han Chang, Robert A Lindquist, and\nJason Moffat. 2006. “CellProfiler: Image Analysis Software for Identifying and\nQuantifying Cell Phenotypes.” _Genome Biology_ 7: R100.\n\nChaumont, Fabrice de, Stéphane Dallongeville, Nicolas Chenouard, Nicolas\nHervé, Sorin Pop, Thomas Provoost, Vannary Meas-Yedid, et al. 2012. “Icy: an\nopen bioimage informatics platform for extended reproducible research.”\n_Nature Methods_ 9: 690–96.\n\nChiu, Sung Nok, Dietrich Stoyan, Wilfrid S. Kendall, and Joseph Mecke. 2013.\n_Stochastic Geometry and Its Applications_. Springer.\n\nCressie, Noel A. 1991. _Statistics for Spatial Data_. John Wiley; Sons.\n\nDiggle, Peter J. 2013. _Statistical Analysis of Spatial and Spatio-Temporal\nPoint Patterns_. Chapman; Hall/CRCs.\n\nHeld, M., M. H. A. Schmitz, B. Fischer, T. Walter, B. Neumann, M. H. Olma, M.\nPeter, J. Ellenberg, and D. W. Gerlich. 2010. “CellCognition: Time-Resolved\nPhenotype Annotation in High-Throughput Live Cell Imaging.” _Nature Methods_\n7: 747.\n\nHolmes, Susan, Adam Kapelner, and Peter P Lee. 2009. “An Interactive Java\nStatistical Image Segmentation System: GemIdent.” _Journal of Statistical\nSoftware_ 30 (10).\n\nJones, T., A. Carpenter, and P. Golland. 2005. “Voronoi-Based Segmentation of\nCells on Image Manifolds.” _Computer Vision for Biomedical Image Applications_\n, 535.\n\nLaufer, Christina, Bernd Fischer, Maximilian Billmann, Wolfgang Huber, and\nMichael Boutros. 2013. “Mapping genetic interactions in human cancer cells\nwith RNAi and multiparametric phenotyping.” _Nature Methods_ 10: 427–31.\n\nPau, Grégoire, Florian Fuchs, Oleg Sklyar, Michael Boutros, and Wolfgang\nHuber. 2010. “EBImage R Package for Image Processing with Applications to\nCellular Phenotypes.” _Bioinformatics_ 26 (7): 979–81.\n\nRajaram, S., B. Pavie, L. F. Wu, and S. J. Altschuler. 2012. “PhenoRipper:\nsoftware for rapidly profiling microscopy images.” _Nature Methods_ 9: 635–37.\n\nRipley, B. D. 1988. _Statistical Inference for Spatial Processes._ Cambridge\nUniversity Press.\n\nRuss, John C., and F. Brent Neal. 2015. _The Image Processing Handbook_. 7th\ned. CRC Press;\n\nSchindelin, Johannes, Ignacio Arganda-Carreras, Erwin Frise, Verena Kaynig,\nMark Longair, Tobias Pietzsch, Stephan Preibisch, et al. 2012. “Fiji: an open-\nsource platform for biological-image analysis.” _Nature Methods_ 9: 676–82.\n\nSerra, Jean. 1983. _Image Analysis and Mathematical Morphology_. Academic\nPress.\n\nSetiadi, A Francesca, Nelson C Ray, Holbrook E Kohrt, Adam Kapelner, Valeria\nCarcamo-Cavazos, Edina B Levic, Sina Yadegarynia, et al. 2010. “Quantitative,\nArchitectural Analysis of Immune Cell Subsets in Tumor-Draining Lymph Nodes\nfrom Breast Cancer Patients and Healthy Lymph Nodes.” _PLoS One_ 5 (8):\ne12420.\n\nVetterli, Martin, Jelena Kovačević, and Vivek Goyal. 2014. _Foundations of\nSignal Processing_. Cambridge University Press.\n\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["msmb.css"],"toc":true,"output-file":"11-chap.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}