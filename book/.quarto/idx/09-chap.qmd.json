{"title":"9.1 Goals for this chapter","markdown":{"headingText":"9.1 Goals for this chapter","containsRefs":false,"markdown":"![](imgs/Brighton_West_Pier_20090214_sunset.jpg)\n\nReal situations often involve point clouds, gradients, graphs, attraction\npoints, noise and different spatial milieux, a little like this picture where\nwe have a rigid skeleton, waves, sun and starlings.\n\nIn [Chapter 7](07-chap.html), we saw how to summarize rectangular matrices\nwhose columns were continuous variables. The maps we made used unsupervised\ndimensionality reduction techniques such as principal component analysis aimed\nat isolating the most important _signal_ component in a matrix \\\\(X\\\\) when\nall the columns have meaningful variances.\n\nHere we extend these ideas to more complex heterogeneous data where continuous\nand categorical variables are combined. Indeed, sometimes our observations\ncannot be easily described by sets of individual variables or coordinates –\nbut it is possible to determine distances or (dis)similarities between them,\nor to describe relationships between them using a graph or a tree. Examples\ninclude species in a species tree or biological sequences. Outside of biology,\nexamples include text documents or movie files, where we may have a reasonable\nmethod to determine (dis)similarity between them, but no obvious variables or\ncoordinates.\n\nThis chapter contains more advanced techniques, for which we often omit\ntechnical details. Having come this far, we hope that by giving you some\nhands-on experience with examples, and extensive references, to enable you to\nunderstand and use some of the more `cutting edge’ techniques in nonlinear\nmultivariate analysis.\n\n\nIn this chapter, we will:\n\n  * Extend linear dimension reduction methods to cases when the distances between observations are available, known as **m** ulti**d** imensional **s** caling (MDS) or principal coordinates analysis.\n\n  * Find modifications of MDS that are nonlinear and robust to outliers.\n\n  * Encode combinations of categorical data and continuous data as well as so-called ‘supplementary’ information. We will see that this enables us to deal with _batch effects_.\n\n  * Use chi-square distances and **c** orrespondence **a** nalysis (CA) to see where categorical data (contingency tables) contain notable dependencies.\n\n  * Generalize clustering methods that can uncover latent variables that are not categorical. This will allow us to detect gradients, “pseudotime” and hidden nonlinear effects in our data.\n\n  * Generalize the notion of variance and covariance to the study of tables of data from multiple different data domains.\n\n## 9.2 Multidimensional scaling and ordination\n\nSometimes, data are _not_ represented as points in a feature space. This can\noccur when we are provided with (dis)similarity matrices between objects such\nas drugs, images, trees or other complex objects, which have no obvious\ncoordinates in \\\\({\\mathbb R}^n\\\\).\n\nIn [Chapter 5](05-chap.html) we saw how to produce _clusters_ from distances.\nHere our goal is to visualize the data in maps in low dimensional spaces\n(e.g., planes) reminiscent of the ones we make from the first few principal\naxes in PCA.\n\nWe start with an intuitive example using geography data. In Figure 9.1, a\nheatmap and clustering of the distances between cities and places in Ukraine1\nare shown.\n\n1 The provenance of these data are described in the script ukraine-dists.R in\nthe data folder.\n\n    \n    \n    library(\"pheatmap\")\n    data(\"ukraine_dists\", package = \"MSMB\")\n    as.matrix(ukraine_dists)[1:4, 1:4]__\n    \n    \n                   Kyiv    Odesa Sevastopol Chernihiv\n    Kyiv         0.0000 441.2548   687.7551  128.1287\n    Odesa      441.2548   0.0000   301.7482  558.6483\n    Sevastopol 687.7551 301.7482     0.0000  783.6561\n    Chernihiv  128.1287 558.6483   783.6561    0.0000\n    \n    \n    pheatmap(as.matrix(ukraine_dists), \n      color = colorRampPalette(c(\"#0057b7\", \"#ffd700\"))(50),\n      breaks = seq(0, max(ukraine_dists)^(1/2), length.out = 51)^2,\n      treeheight_row = 10, treeheight_col = 10)__\n\n[![](09-chap_files/figure-html/fig-HeatDists-1.png)](09-chap_files/figure-\nhtml/fig-HeatDists-1.png \"Figure 9.1: A heatmap of the ukraine_dists distance\nmatrix. Distances are measured in kilometres. The function has re-arranged the\norder of the cities, and grouped the closest ones.\")\n\nFigure 9.1: A heatmap of the `ukraine_dists` distance matrix. Distances are\nmeasured in kilometres. The function has re-arranged the order of the cities,\nand grouped the closest ones.\n\nBesides `ukraine_dists`, which contains the pairwise distances, the RData file\nthat we loaded above also contains the dataframe `ukraine_coords` with the\nlongitudes and latitudes; we will use this later as a ground truth. Given the\ndistances, multidimensional scaling (MDS) provides a “map” of their relative\nlocations. It will not be possible to arrange the cities such that their\nEuclidean distances on a 2D plane exactly reproduce the given distance matrix:\nthe cities lie on the curved surface of the Earth rather than in a plane.\nNevertheless, we can expect to find a two dimensional embedding that\nrepresents the data well. With biological data, our 2D embeddings are likely\nto be much less clearcut. We call the function with:\n\n    \n    \n    ukraine_mds = cmdscale(ukraine_dists, eig = TRUE)__\n\nWe make a function that we will reuse several times in this chapter to make a\nscreeplot from the result of a call to the `cmdscale` function:\n\n    \n    \n    library(\"dplyr\")\n    library(\"ggplot2\")\n    plotscree = function(x, m = length(x$eig)) {\n      ggplot(tibble(eig = x$eig[seq_len(m)], k = seq(along = eig)),\n        aes(x = k, y = eig)) + theme_minimal() +\n        scale_x_discrete(\"k\", limits = as.factor(seq_len(m))) + \n        geom_bar(stat = \"identity\", width = 0.5, fill = \"#ffd700\", col = \"#0057b7\")\n    }__\n    \n    \n    plotscree(ukraine_mds, m = 4)__\n\n[![](09-chap_files/figure-html/fig-plotscreeeig-1.png)](09-chap_files/figure-\nhtml/fig-plotscreeeig-1.png \"Figure 9.2: Screeplot of the first four\neigenvalues. There is a pronounced drop after the first two eigenvalues, which\nindicates that the data are well described by a two-dimensional embedding.\")\n\nFigure 9.2: Screeplot of the first four eigenvalues. There is a pronounced\ndrop after the first two eigenvalues, which indicates that the data are well\ndescribed by a two-dimensional embedding.\n\n__\n\nQuestion 9.1\n\nLook at all the eigenvalues output by the `cmdscale` function: what do you\nnotice?\n\n__\n\nSolution\n\n__\n\nIf you execute:\n\n    \n    \n    ukraine_mds$eig |> signif(3)__\n    \n    \n     [1]  3.91e+06  1.08e+06  3.42e+02  4.84e-01  2.13e-01  3.83e-05  5.90e-06\n     [8]  5.82e-07  8.79e-08  4.94e-08  6.52e-10  2.84e-10  1.84e-10  5.22e-11\n    [15]  4.89e-11  4.57e-11 -3.26e-12 -2.55e-11 -5.90e-11 -6.55e-11 -1.40e-10\n    [22] -1.51e-10 -3.46e-10 -3.76e-10 -4.69e-10 -2.24e-09 -1.51e-08 -9.60e-05\n    [29] -2.51e-04 -1.41e-02 -1.19e-01 -3.58e+02 -8.85e+02\n    \n    \n    plotscree(ukraine_mds)__\n\n[![](09-chap_files/figure-html/fig-\nplotscreeeigall-1.png)](09-chap_files/figure-html/fig-plotscreeeigall-1.png\n\"Figure 9.3: Screeplot of all the eigenvalues.\")\n\nFigure 9.3: Screeplot of all the eigenvalues.\n\nyou will note that unlike in PCA, there are some negative eigenvalues. These\nare due to the way `cmdscale` works.\n\nThe main output from the `cmdscale` function are the coordinates of the two-\ndimensional embedding, which we show in Figure 9.4 (we will discuss how the\nalgorithm works in the next section).\n\n    \n    \n    ukraine_mds_df = tibble(\n      PCo1 = ukraine_mds$points[, 1],\n      PCo2 = ukraine_mds$points[, 2],\n      labs = rownames(ukraine_mds$points)\n    )\n    library(\"ggrepel\")\n    g = ggplot(ukraine_mds_df, aes(x = PCo1, y = PCo2, label = labs)) +\n      geom_point() + geom_text_repel(col = \"#0057b7\") + coord_fixed() \n    g __\n\n[![](09-chap_files/figure-html/fig-ukrainemds1-1.png)](09-chap_files/figure-\nhtml/fig-ukrainemds1-1.png \"Figure 9.4: MDS map based on the distances.\")\n\nFigure 9.4: MDS map based on the distances.\n\nNote that while relative positions are correct, the orientation of the map is\nunconventional: Crimea is at the top. This is a common phenomenon with methods\nthat reconstruct planar embeddings from distances. Since the distances between\nthe points are invariant under rotations and reflections (axis flips), any\nsolution is as good as any other solution that relates to it via rotation or\nreflection. Functions like `cmdscale` will pick one of the equally optimal\nsolutions, and the particular choice can depend on minute details of the data\nor the computing platform being used. Here, we can transform our result into a\nmore conventional orientation by reversing the sign of the \\\\(y\\\\)-axis. We\nredraw the map in Figure 9.5 and compare this to the true longitudes and\nlatitudes from the `ukraine_coords` dataframe (Figure 9.6).\n\n    \n    \n    g %+% mutate(ukraine_mds_df, PCo1 = PCo1, PCo2 = -PCo2)__\n\n[![](09-chap_files/figure-html/fig-ukrainemds2-1.png)](09-chap_files/figure-\nhtml/fig-ukrainemds2-1.png \"Figure 9.5: Same as Figure fig-ukrainemds1, but\nwith y-axis flipped.\")\n\nFigure 9.5: Same as Figure 9.4, but with y-axis flipped.\n\n    \n    \n    data(\"ukraine_coords\", package = \"MSMB\")\n    print.data.frame(ukraine_coords[1:4,  c(\"city\", \"lat\", \"lon\")])__\n    \n    \n            city      lat      lon\n    1       Kyiv 50.45003 30.52414\n    2      Odesa 46.48430 30.73229\n    3 Sevastopol 44.60544 33.52208\n    4  Chernihiv 51.49410 31.29433\n    \n    \n    ggplot(ukraine_coords, aes(x = lon, y = lat, label = city)) +\n      geom_point() + geom_text_repel(col = \"#0057b7\")__\n\n[![](09-chap_files/figure-html/fig-ukrainecoord1-1.png)](09-chap_files/figure-\nhtml/fig-ukrainecoord1-1.png \"Figure 9.6: True latitudes and longitudes, taken\nfrom the ukraine_coords dataframe.\")\n\nFigure 9.6: True latitudes and longitudes, taken from the `ukraine_coords`\ndataframe.\n\n__\n\nQuestion 9.2\n\nWe drew the longitudes and latitudes in the right panel of Figure 9.6 without\nattention to aspect ratio. What is the right aspect ratio for this plot?\n\n__\n\nSolution\n\n__\n\nThere is no simple relationship between the distances that correspond to 1\ndegree change in longitude and to 1 degree change in latitude, so the choice\nis difficult to make. Even under the simplifying assumption that our Earth is\nspherical and has a radius of 6371 km, it’s complicated: one degree in\nlatitude always corresponds to a distance of 111 km\n(\\\\(6371\\times2\\pi/360\\\\)), as does one degree of longitude on the equator.\nHowever, when you move away from the equator, a degree of longitude\ncorresponds to shorter and shorter distances (and to no distance at all at the\npoles). Pragmatically, for displays such as in Figure 9.6, we could choose a\nvalue for the aspect ratio that’s somewhere in the middle between the Northern\nand Southern most points, say, the cosine for 48 degrees.\n\n__\n\nQuestion 9.3\n\nAdd international borders and geographic features such as rivers to Figure\n9.6.\n\n__\n\nSolution\n\n__\n\nA start point is provided by the code below, which adds the international\nborders as a polygon (Figure 9.7).\n\n    \n    \n    library(\"maps\")\n    ua_borders = dplyr::filter(map_data(\"world\"), region == \"Ukraine\")\n    ggplot(ukraine_coords, aes(x = lon, y = lat)) + \n      geom_polygon(data = ua_borders, aes(x = long, y = lat, group = subregion), fill = \"#ffd700\", color = \"#0057b7\") +\n      geom_point() + \n      geom_text_repel(aes(label = city)) +\n      coord_fixed(1/cos(48/180*pi))__\n\nThere is a lot of [additional infrastructure available in R for geospatial\ndata](https://cran.r-project.org/web/views/Spatial.html), including vector and\nraster data types.\n\n[![](09-chap_files/figure-html/fig-\nukrainewithborders-1.png)](09-chap_files/figure-html/fig-\nukrainewithborders-1.png \"Figure 9.7: International borders added to\nFigure fig-ukrainecoord1.\")\n\nFigure 9.7: International borders added to Figure 9.6.\n\n**Note:** MDS creates similar output as PCA, however there is only one\n‘dimension’ to the data (the sample points). There is no ‘dual’ dimension,\nthere are no biplots and no loading vectors. This is a drawback when coming to\ninterpreting the maps. Interpretation can be facilitated by examining\ncarefully the extreme points and their differences.\n\n### 9.2.1 How does the method work?\n\nLet’s take a look at what would happen if we really started with points whose\ncoordinates were known2. We put these coordinates into the two columns of a\nmatrix with as many rows as there are points. Now we compute the distances\nbetween points based on these coordinates. To go from the coordinates \\\\(X\\\\)\nto distances, we write \\\\[d^2_{i,j} = (x_i^1 - x_j^1)^2 + \\dots + (x_i^p -\nx_j^p)^2.\\\\] We will call the matrix of squared distances `DdotD` in R and\n\\\\(D\\bullet D\\\\) in the text . We want to find points such that the square of\ntheir distances is as close as possible to the \\\\(D\\bullet D\\\\) observed.3\n\nThis is different from \\\\(DD\\\\) or \\\\(D^2\\\\), the matrix-multiplication of\n\\\\(D\\\\) with itself.\n\n3 Here we commit a slight ‘abuse’ by using the longitudes and latitudes of our\ncities as Cartesian coordinates and ignoring the fact that they are\ncurvilinear coordinates on a sphere-like surface.\n\n2 Here we commit a slight ‘abuse’ by using the longitude and longitude of our\ncities as Cartesian coordinates and ignoring the curvature of the earth’s\nsurface. Check out the internet for information on the Haversine formula.\n\n    \n    \n    X = with(ukraine_coords, cbind(lon, lat * cos(48)))\n    DdotD = as.matrix(dist(X)^2)__\n\nThe relative distances do not depend on the point of origin of the data. We\ncenter the data by using the centering matrix \\\\(H\\\\) defined as\n\\\\(H=I-\\frac{1}{n}{\\mathbf{11}}^t\\\\). Let’s check the _centering_ property of\n\\\\(H\\\\) using:\n\n    \n    \n    n = nrow(X)\n    H = diag(rep(1,n))-(1/n) * matrix(1, nrow = n, ncol = n)\n    Xc = sweep(X,2,apply(X,2,mean))\n    Xc[1:2, ]__\n    \n    \n                lon          \n    [1,] -1.1722946 -1.184705\n    [2,] -0.9641429  1.353935\n    \n    \n    HX = H %*% X\n    HX[1:2, ]__\n    \n    \n                lon          \n    [1,] -1.1722946 -1.184705\n    [2,] -0.9641429  1.353935\n    \n    \n    apply(HX, 2, mean)__\n    \n    \n              lon               \n    -1.618057e-15  1.747077e-16 \n\n__\n\nQuestion 9.4\n\nCall `B0` the matrix obtained by applying the centering matrix both to the\nright and to the left of `DdotD` Consider the points centered at the origin\ngiven by the \\\\(HX\\\\) matrix and compute its cross product, we’ll call this\n`B2`. What do you have to do to `B0` to make it equal to `B2`?\n\n__\n\nSolution\n\n__\n\n    \n    \n    B0 = H  %*% DdotD %*% H\n    B2 = HX %*% t(HX)\n    B2[1:3, 1:3] / B0[1:3, 1:3]__\n    \n    \n         [,1] [,2] [,3]\n    [1,] -0.5 -0.5 -0.5\n    [2,] -0.5 -0.5 -0.5\n    [3,] -0.5 -0.5 -0.5\n    \n    \n    max(abs(-0.5 * B0 - B2))__\n    \n    \n    [1] 9.237056e-14\n\nTherefore, given the squared distances between rows (\\\\(D\\bullet D\\\\)) and the\ncross product of the centered matrix \\\\(B=(HX)(HX)^t\\\\), we have shown:\n\n\\\\[ -\\frac{1}{2} H(D\\bullet D) H=B \\tag{9.1}\\\\]\n\nThis is always true, and we use it to reverse-engineer an \\\\(X\\\\) which\nsatisfies Equation 9.1 when we are given \\\\(D\\bullet D\\\\) to start with.\n\n#### From \\\\(D\\bullet D\\\\) to \\\\(X\\\\) using singular vectors.\n\nWe can go backwards from a matrix \\\\(D\\bullet D\\\\) to \\\\(X\\\\) by taking the\neigen-decomposition of \\\\(B\\\\) as defined in Equation 9.1. This also enables\nus to choose how many coordinates, or columns, we want for the \\\\(X\\\\) matrix.\nThis is very similar to how PCA provides the best rank \\\\(r\\\\) approximation.  \n**Note** : As in PCA, we can write this using the singular value decomposition\nof \\\\(HX\\\\) (or the eigen decomposition of \\\\(HX(HX)^t\\\\)):\n\n\\\\[ HX^{(r)} = US^{(r)}V^t \\mbox{ with } S^{(r)} \\mbox{ the diagonal matrix of\nthe first } r \\mbox{ singular values}, \\\\]\n\nThis provides the best approximate representation in an Euclidean space of\ndimension \\\\(r\\\\). The algorithm gives us the coordinates of points that have\napproximately the same distances as those provided by the \\\\(D\\\\) matrix.\n\n\\\\[S^{(r)} = \\begin{pmatrix} s_1 & 0 & 0 & 0 & ...\\\\\\ 0 & s_2 & 0 & 0 & ...\\\\\\\n0 & 0 & ... & ... & ...\\\\\\ 0 & 0 & ... & s_r & ...\\\\\\ ...& ...& ...& 0 & 0 \\\\\\\n\\end{pmatrix}\\\\]The method is often called Principal Coordinates Analysis, or\nPCoA which stresses the connection to PCA.\n\n#### Classical MDS Algorithm.\n\nIn summary, given an \\\\(n \\times n\\\\) matrix of squared interpoint distances\n\\\\(D\\bullet D\\\\), we can find points and their coordinates \\\\(\\tilde{X}\\\\) by\nthe following operations:\n\n  1. Double center the interpoint distance squared and multiply it by \\\\(-\\frac{1}{2}\\\\):  \n\\\\(B = -\\frac{1}{2}H D\\bullet D H\\\\).\n\n  2. Diagonalize \\\\(B\\\\): \\\\(\\quad B = U \\Lambda U^t\\\\).\n\n  3. Extract \\\\(\\tilde{X}\\\\): \\\\(\\quad \\tilde{X} = U \\Lambda^{1/2}\\\\).\n\n#### Finding the right underlying dimensionality.\n\nAs an example, let’s take objects for which we have similarities (surrogrates\nfor distances) but for which there is no natural underlying Euclidean space.\n\nIn a psychology experiment from the 1950s, Ekman ([1954](16-chap.html#ref-\nEkman:1954)) asked 31 subjects to rank the similarities of 14 different\ncolors. His goal was to understand the underlying dimensionality of color\nperception. The similarity or confusion matrix was scaled to have values\nbetween 0 and 1. The colors that were often confused had similarities close to\n1. We transform the data into a dissimilarity by subtracting the values from\n1:\n\n    \n    \n    ekm = read.table(\"../data/ekman.txt\", header=TRUE)\n    rownames(ekm) = colnames(ekm)\n    disekm = 1 - ekm - diag(1, ncol(ekm))\n    disekm[1:5, 1:5]__\n    \n    \n         w434 w445 w465 w472 w490\n    w434 0.00 0.14 0.58 0.58 0.82\n    w445 0.14 0.00 0.50 0.56 0.78\n    w465 0.58 0.50 0.00 0.19 0.53\n    w472 0.58 0.56 0.19 0.00 0.46\n    w490 0.82 0.78 0.53 0.46 0.00\n    \n    \n    disekm = as.dist(disekm)__\n\nWe compute the MDS coordinates and eigenvalues. We combine the eigenvalues in\nthe screeplot shown in Figure 9.8:\n\n    \n    \n    mdsekm = cmdscale(disekm, eig = TRUE)\n    plotscree(mdsekm)__\n\n[![](09-chap_files/figure-html/fig-ekmanMDSeig-1.png)](09-chap_files/figure-\nhtml/fig-ekmanMDSeig-1.png \"Figure 9.8: The screeplot shows us that the\nphenomenon is largely two dimensional.\")\n\nFigure 9.8: The screeplot shows us that the phenomenon is largely two\ndimensional.\n\nWe plot the different colors using the first two principal coordinates as\nfollows:\n\n    \n    \n    dfekm = mdsekm$points[, 1:2] |>\n      `colnames<-`(paste0(\"MDS\", 1:2)) |>\n      as_tibble() |>\n      mutate(\n        name = rownames(ekm),\n        rgb = photobiology::w_length2rgb(as.numeric(sub(\"w\", \"\", name))))\n    ggplot(dfekm, aes(x = MDS1, y = MDS2)) +\n      geom_point(col = dfekm$rgb, size = 4) +\n      geom_text_repel(aes(label = name)) + coord_fixed()__\n\n[![](09-chap_files/figure-html/fig-ekmanMDS-1-1.png)](09-chap_files/figure-\nhtml/fig-ekmanMDS-1-1.png \"Figure 9.9: The layout of the scatterpoints in the\nfirst two dimensions has a horseshoe shape. The labels and colors show that\nthe arch corresponds to the wavelengths.\")\n\nFigure 9.9: The layout of the scatterpoints in the first two dimensions has a\nhorseshoe shape. The labels and colors show that the arch corresponds to the\nwavelengths.\n\nFigure 9.9 shows the Ekman data in the new coordinates. There is a striking\npattern that calls for explanation. This horseshoe or arch structure in the\npoints is often an indicator of a sequential latent ordering or gradient in\nthe data ([Diaconis, Goel, and Holmes 2008](16-chap.html#ref-Goel:2007aa)). We\nwill revisit this in Section 9.5.\n\n### 9.2.2 Robust versions of MDS\n\nMultidimensional scaling aims to minimize the difference between the squared\ndistances as given by \\\\(D\\bullet D\\\\) and the squared distances between the\npoints with their new coordinates. Unfortunately, this objective tends to be\nsensitive to outliers: one single data point with large distances to everyone\nelse can dominate, and thus skew, the whole analysis. Often, we like to use\nsomething that is more robust, and one way to achieve this is to disregard the\nactual values of the distances and only ask that the relative rankings of the\noriginal and the new distances are as similar as possible. Such a rank based\napproach is robust: its sensitivity to outliers is reduced.\n\n**Robustness:** A method is robust if it is not too influenced by a few\noutliers. For example, the median of a set of \\\\(n\\\\) numbers does not change\nby a lot even if we change 20 the numbers by arbitrarily large amounts; to\ndrastically shift the median, we need to change more than half of the numbers.\nIn contrast, we can change the mean by a large amount by just manipulating one\nof the numbers. We say that the _breakdown point_ of the median is 1/2, while\nthat of the mean is only \\\\(1/n\\\\). Both mean and median are estimators of the\n_location_ of a distribution (i.e., what is a \"typical\" value of the numbers),\nbut the median is more robust. The median is based on the ranks; more\ngenerally, methods based on ranks are often more robust than those based on\nthe actual values. Many nonparametric tests are based on reductions of data to\ntheir ranks.\n\nWe will use the Ekman data to show how useful robust methods are when we are\nnot quite sure about the ‘scale’ of our measurements. Robust ordination,\ncalled non metric multidimensional scaling (NMDS for short) only attempts to\nembed the points in a new space such that the **order** of the reconstructed\ndistances in the new map is the same as the ordering of the original distance\nmatrix.\n\nNon metric MDS looks for a transformation \\\\(f\\\\) of the given dissimilarities\nin the matrix \\\\(d\\\\) and a set of coordinates in a low dimensional space\n(_the map_) such that the distance in this new map is \\\\(\\tilde{d}\\\\) and\n\\\\(f(d)\\thickapprox \\tilde{d}\\\\). The quality of the approximation can be\nmeasured by the standardized residual sum of squares (stress) function:\n\n\\\\[ \\text{stress}^2=\\frac{\\sum(f(d)-\\tilde{d})^2}{\\sum d^2}. \\\\]\n\nNMDS is not sequential in the sense that we have to specify the underlying\ndimensionality at the outset and the optimization is run to maximize the\nreconstruction of the distances according to that number. There is no notion\nof percentage of variation explained by individual axes as provided in PCA.\nHowever, we can make a simili-screeplot by running the program for all the\nsuccessive values of \\\\(k\\\\) (\\\\(k=1, 2, 3, ...\\\\)) and looking at how well\nthe stress drops. Here is an example of looking at these successive\napproximations and their goodness of fit. As in the case of diagnostics for\nclustering, we will take the number of axes **after** the stress has a steep\ndrop.\n\nBecause each calculation of a NMDS result requires a new optimization that is\nboth random and dependent on the \\\\(k\\\\) value, we use a similar procedure to\nwhat we did for clustering in [Chapter 4](04-chap.html). We execute the\n`metaMDS` function, say, 100 times for each of the four possible values of\n\\\\(k\\\\) and record the stress values.\n\n    \n    \n    library(\"vegan\")\n    nmds.stress = function(x, sim = 100, kmax = 4) {\n      sapply(seq_len(kmax), function(k)\n        replicate(sim, metaMDS(x, k = k, autotransform = FALSE)$stress))\n    }\n    stress = nmds.stress(disekm, sim = 100)\n    dim(stress)__\n\nLet’s look at the boxplots of the results. This can be a useful diagnostic\nplot for choosing \\\\(k\\\\) (Figure 9.10).\n\n    \n    \n    dfstr = reshape2::melt(stress, varnames = c(\"replicate\",\"dimensions\"))\n    ggplot(dfstr, aes(y = value, x = dimensions, group = dimensions)) +\n      geom_boxplot()__\n\n[![](09-chap_files/figure-html/fig-\nNMDSscreeplot-1-1.png)](09-chap_files/figure-html/fig-NMDSscreeplot-1-1.png\n\"Figure 9.10: Several replicates at each dimension were run to evaluate the\nstability of the stress. We see that the stress drops dramatically with two or\nmore dimensions, thus indicating that a two dimensional solution is\nappropriate here.\")\n\nFigure 9.10: Several replicates at each dimension were run to evaluate the\nstability of the stress. We see that the stress drops dramatically with two or\nmore dimensions, thus indicating that a two dimensional solution is\nappropriate here.\n\nWe can also compare the distances and their approximations using what is known\nas a Shepard plot for \\\\(k=2\\\\) for instance, computed with:\n\n    \n    \n    nmdsk2 = metaMDS(disekm, k = 2, autotransform = FALSE)\n    stressplot(nmdsk2, pch = 20)__\n\n[![](09-chap_files/figure-html/fig-\nShepardsplot-1-1.png)](09-chap_files/figure-html/fig-Shepardsplot-1-1.png\n\"Figure 9.11: The Shepard’s plot compares the original distances or\ndissimilarities \\(along the horizonal axis\\) to the reconstructed distances,\nin this case for k=2 \\(vertical axis\\).\")\n\nFigure 9.11: The Shepard’s plot compares the original distances or\ndissimilarities (along the horizonal axis) to the reconstructed distances, in\nthis case for \\\\(k=2\\\\) (vertical axis).\n\nBoth the Shepard’s plot in Figure 9.11 and the screeplot in Figure 9.10 point\nto a two-dimensional solution for Ekman’s color confusion study. Let us\ncompare the output of the two different MDS programs, the classical metric\nleast squares approximation and the nonmetric rank approximation method. The\nright panel of Figure 9.12 shows the result from the nonmetric rank\napproximation, the left panel is the same as Figure 9.9. The projections are\nalmost identical in both cases. For these data, it makes little difference\nwhether we use a Euclidean or nonmetric multidimensional scaling method.\n\n    \n    \n    nmdsk2$points[, 1:2] |> \n      `colnames<-`(paste0(\"NmMDS\", 1:2)) |>\n      as_tibble() |> \n      bind_cols(dplyr::select(dfekm, rgb, name)) |>\n      ggplot(aes(x = NmMDS1, y = NmMDS2)) +\n        geom_point(col = dfekm$rgb, size = 4) +\n        geom_text_repel(aes(label = name))__\n\n[![](09-chap_files/figure-html/fig-ekmannonMDS-1-1.png)](09-chap_files/figure-\nhtml/fig-ekmannonMDS-1-1.png \"Figure 9.12 \\(a\\): \")\n\n(a)\n\n[![](09-chap_files/figure-html/fig-ekmannonMDS-1-2.png)](09-chap_files/figure-\nhtml/fig-ekmannonMDS-1-2.png \"Figure 9.12 \\(b\\): \")\n\n(b)\n\nFigure 9.12: Comparison of the output from (a) the classical multidimensional\nscaling (same as Figure 9.9) and (b) the nonmetric version.\n\n## 9.3 Contiguous or supplementary information\n\n[![Metadata: Many programs and workflows for biological sequence analysis or\nassays separate the environmental and contextual information, which they call\nmetadata, from the assay data or sequence reads. We discourage such practice\nas the exact connections between the samples and covariates are important. A\nlost connection between the assays and covariates makes later analyses\nimpossible. Covariates such as clinical history, time, batch or location are\nimportant and should be considered components of the\ndata.](imgs/devil.png)](imgs/devil.png \"Metadata: Many programs and workflows\nfor biological sequence analysis or assays separate the environmental and\ncontextual information, which they call metadata, from the assay data or\nsequence reads. We discourage such practice as the exact connections between\nthe samples and covariates are important. A lost connection between the assays\nand covariates makes later analyses impossible. Covariates such as clinical\nhistory, time, batch or location are important and should be considered\ncomponents of the data.\")\n\n**Metadata:** Many programs and workflows for biological sequence analysis or\nassays separate the environmental and contextual information, which they call\n**metadata** , from the assay data or sequence reads. We discourage such\npractice as the exact connections between the samples and covariates are\nimportant. A lost connection between the assays and covariates makes later\nanalyses impossible. Covariates such as clinical history, time, batch or\nlocation are important and should be considered components of the data.\n\nIn [Chapter 3](03-chap.html) we introduced the R _data.frame_ class that\nenables us to combine heterogeneous data types: categorical factors, text and\ncontinuous values. Each row of a dataframe corresponds to an object, or a\nrecord, and the columns are the different variables, or features.\n\nExtra information about sample batches, dates of measurement, different\nprotocols are often named _metadata_ ; this can be misnomer if it is implied\nthat metadata are somehow less important. Such information is _real data_ that\nneed to be integrated into the analyses. We typically store it in a\n_data.frame_ or a similar R class and tightly link it to the primary assay\ndata.\n\n### 9.3.1 Known batches in data\n\nHere we show an example of an analysis that was done by Holmes et al.\n([2011](16-chap.html#ref-holmes2011psb)) on bacterial abundance data from\n_Phylochip_ ([Brodie et al. 2006](16-chap.html#ref-Brodie:2006)) microarrays.\nThe experiment was designed to detect differences between a group of healthy\nrats and a group who had Irritable Bowel Disease ([Nelson et al.\n2010](16-chap.html#ref-Nelson:2011)). This example shows a case where the\nnuisance batch effects become apparent in the analysis of experimental data.\nIt is an illustration of the fact that best practices in data analyses are\nsequential and that it is better to analyse data as they are collected to\nadjust for severe problems in the experimental design _as they occur_ ,\ninstead of having to deal with them _post mortem_ 4.\n\n4 Fisher’s terminology, see [Chapter 13](13-chap.html).\n\nWhen data collection started on this project, days 1 and 2 were delivered and\nwe made the plot that appears in Figure 9.14. This showed a definite _day_\neffect. When investigating the source of this effect, we found that both the\nprotocol and the array were different in days 1 and 2. This leads to\nuncertainty in the source of variation, we call this **confounding** of\neffects.\n\n[![Bioconductor container: These data are an example of an awkward way of\ncombining batch information with the actual data. The day information has been\ncombined with the array data and encoded as a number and could be confused\nwith a continuous variable. We will see in the next section a better practice\nfor storing and manipulating heterogeneous data using a Bioconductor container\ncalled SummarizedExperiment.](imgs/devil.png)](imgs/devil.png \"Bioconductor\ncontainer: These data are an example of an awkward way of combining batch\ninformation with the actual data. The day information has been combined with\nthe array data and encoded as a number and could be confused with a continuous\nvariable. We will see in the next section a better practice for storing and\nmanipulating heterogeneous data using a Bioconductor container called\nSummarizedExperiment.\")\n\n**Bioconductor container:** These data are an example of an awkward way of\ncombining batch information with the actual data. The `day` information has\nbeen combined with the array data and encoded as a number and could be\nconfused with a continuous variable. We will see in the next section a better\npractice for storing and manipulating heterogeneous data using a Bioconductor\ncontainer called _SummarizedExperiment_.\n\nWe load the data and the packages we use for this section:\n\n    \n    \n    IBDchip = readRDS(\"../data/vsn28Exprd.rds\")\n    library(\"ade4\")\n    library(\"factoextra\")\n    library(\"sva\")__\n\n__\n\nQuestion 9.5\n\nWhat class is the `IBDchip` ? Look at the last row of the matrix, what do you\nnotice?\n\n__\n\nSolution\n\n__\n\n    \n    \n    class(IBDchip)__\n    \n    \n    [1] \"matrix\" \"array\" \n    \n    \n    dim(IBDchip)__\n    \n    \n    [1] 8635   28\n    \n    \n    tail(IBDchip[,1:3])__\n    \n    \n                                     20CF     20DF     20MF\n    bm-026.1.sig_st              7.299308 7.275802 7.383103\n    bm-125.1.sig_st              8.538857 8.998562 9.296096\n    bru.tab.d.HIII.Con32.sig_st  6.802736 6.777566 6.859950\n    bru.tab.d.HIII.Con323.sig_st 6.463604 6.501139 6.611851\n    bru.tab.d.HIII.Con5.sig_st   5.739235 5.666060 5.831079\n    day                          2.000000 2.000000 2.000000\n    \n    \n    table(IBDchip[nrow(IBDchip), ])__\n    \n    \n     1  2  3 \n     8 16  4 \n\nThe data are normalized abundance measurements of 8634 taxa measured on 28\nsamples. We use a rank-threshold transformation, giving the top 3000 most\nabundant taxa scores from 3000 to 1, and letting the remaining (low abundant)\nones all have a score of 1. We also separate out the proper assay data from\nthe (awkwardly placed) day variable, which should be considered a factor5:\n\n5 Below, we show how to arrange these data into a Bioconductor\n`SummarizedExperiment`, which is a much more sane way of storing such data.\n\n    \n    \n    assayIBD = IBDchip[-nrow(IBDchip), ]\n    day      = factor(IBDchip[nrow(IBDchip), ])__\n\nInstead of using the continuous, somehow normalized data, we use a robust\nanalysis replacing the values by their ranks. The lower values are considered\nties encoded as a threshold chosen to reflect the number of expected taxa\nthought to be present:\n\n    \n    \n    rankthreshPCA = function(x, threshold = 3000) {\n      ranksM = apply(x, 2, rank)\n      ranksM[ranksM < threshold] = threshold\n      ranksM = threshold - ranksM\n      dudi.pca(t(ranksM), scannf = FALSE, nf = 2)\n    }\n    pcaDay12 = rankthreshPCA(assayIBD[, day != 3])\n    fviz_eig(pcaDay12, bar_width = 0.6) + ggtitle(\"\")__\n\n[![](09-chap_files/figure-html/fig-screepc12-1.png)](09-chap_files/figure-\nhtml/fig-screepc12-1.png \"Figure 9.13: The screeplot shows us that the samples\ncan be usefully represented in a two dimensional embedding.\")\n\nFigure 9.13: The screeplot shows us that the samples can be usefully\nrepresented in a two dimensional embedding.\n\n    \n    \n    day12 = day[ day!=3 ]\n    rtPCA1 = fviz(pcaDay12, element = \"ind\", axes = c(1, 2), geom = c(\"point\", \"text\"),\n      habillage = day12, repel = TRUE, palette = \"Dark2\",\n      addEllipses = TRUE, ellipse.type = \"convex\") + ggtitle(\"\") +\n      coord_fixed()\n    rtPCA1 __\n\n[![](09-chap_files/figure-html/fig-rankthreshPCA-1.png)](09-chap_files/figure-\nhtml/fig-rankthreshPCA-1.png \"Figure 9.14: We have used colors to identify the\ndifferent days and have kept the sample labels as well. We have also added\nconvex hulls for each day. The group mean is identified as the point with the\nlarger symbol \\(circle, triangle or square\\).\")\n\nFigure 9.14: We have used colors to identify the different days and have kept\nthe sample labels as well. We have also added convex hulls for each day. The\ngroup mean is identified as the point with the larger symbol (circle, triangle\nor square).\n\n__\n\nQuestion 9.6\n\nWhy do we use a threshold for the ranks?\n\n__\n\nSolution\n\n__\n\nLow abundances, at noise level occur for species that are not really present,\nof which there are more than half. A large jump in rank for these observations\ncould easily occur without any meaningful reason. Thus we create a large\nnumber of ties for low abundance.\n\nFigure 9.14 shows that the sample arrange themselves naturally into two\ndifferent groups according to the day of the samples. After discovering this\neffect, we delved into the differences that could explain these distinct\nclusters. There were two different protocols used (protocol 1 on day 1,\nprotocol 2 on day 2) _and_ unfortunately two different provenances for the\narrays used on those two days (array 1 on day 1, array 2 on day 2).\n\nA third set of data of four samples had to be collected to deconvolve the\nconfounding effect. Array 2 was used with protocol 2 on Day 3, Figure 9.15\nshows the new PCA plot with all the samples created by the following:\n\n    \n    \n    pcaDay123 = rankthreshPCA(assayIBD)\n    fviz(pcaDay123, element = \"ind\", axes = c(1, 2), geom = c(\"point\", \"text\"),\n      habillage = day, repel = TRUE, palette = \"Dark2\",\n      addEllipses = TRUE, ellipse.type = \"convex\") + \n      ggtitle(\"\") + coord_fixed()__\n\n[![](09-chap_files/figure-html/fig-\nThreesetspca123-1.png)](09-chap_files/figure-html/fig-Threesetspca123-1.png\n\"Figure 9.15 \\(a\\): \")\n\n(a)\n\n[![](09-chap_files/figure-html/fig-\nThreesetspca123-2.png)](09-chap_files/figure-html/fig-Threesetspca123-2.png\n\"Figure 9.15 \\(b\\): \")\n\n(b)\n\nFigure 9.15: When comparing the three day analysis to that of the first two\ndays, we notice the inversion of signs in the coordinates on the second axis:\nthis has no biological relevance. The important finding is that group 3\noverlaps heavily with group 1 indicating that it was the protocol change on\nDay 2 which created the variability.\n\n__\n\nQuestion 9.7\n\nIn which situation would it be preferable to make confidence ellipses around\nthe group means using the following code?\n\n    \n    \n    fviz_pca_ind(pcaDay123, habillage = day, labelsize = 3,\n      palette = \"Dark2\", addEllipses = TRUE, ellipse.level = 0.69)__\n\n[![](09-chap_files/figure-html/fig-screepc123-1.png)](09-chap_files/figure-\nhtml/fig-screepc123-1.png \"Figure 9.16: The eigenvalue screeplot the case of 3\ngroups is extremely similar to that with two groups shown in Figure fig-\nscreepc12.\")\n\nFigure 9.16: The eigenvalue screeplot the case of 3 groups is extremely\nsimilar to that with two groups shown in Figure 9.13.\n\nThrough this visualization we were able to uncover a flaw in the original\nexperimental design. The first two batches shown in green and brown were both\nbalanced with regards to IBS and healthy rats. They do show very different\nlevels of variability and overall multivariate coordinates. In fact, there are\ntwo **confounded** effects. Both the arrays and protocols were different on\nthose two days. We had to run a third batch of experiments on day 3,\nrepresented in purple, this used protocol from day 1 and the arrays from day\n2. The third group faithfully overlaps with batch 1, telling us that the\nchange in protocol was responsible for the variability.\n\n### 9.3.2 Removing batch effects\n\nThrough the combination of the continuous measurements from `assayIBD` and the\n**supplementary** batch number as a factor, the PCA map has provided an\ninvaluable investigation tool. This is a good example of the use of\n**supplementary points** 6. The mean-barycenter points are created by using\nthe group-means of points in each of the three groups and serve as extra\nmarkers on the plot.\n\n6 This is called a supplementary point because the new observation-point is\nnot used in the matrix decomposition.\n\nWe can decide to re-align the three groups by subtracting the group means so\nthat all the batches are centered on the origin. A slightly more effective way\nis to use the `ComBat` function available in the\n**[sva](https://bioconductor.org/packages/sva/)** package. This function uses\na similar, but slightly more sophisticated method (Empirical Bayes mixture\napproach ([Leek et al. 2010](16-chap.html#ref-Leek:2010:batch))). We can see\nits effect on the data by redoing our robust PCA (see the result in Figure\n9.17):\n\n    \n    \n    model0 = model.matrix(~1, day)\n    combatIBD = ComBat(dat = assayIBD, batch = day, mod = model0)\n    pcaDayBatRM = rankthreshPCA(combatIBD)\n    fviz(pcaDayBatRM, element = \"ind\", geom = c(\"point\", \"text\"),\n      habillage = day, repel=TRUE, palette = \"Dark2\", addEllipses = TRUE,\n      ellipse.type = \"convex\", axes =c(1,2)) + coord_fixed() + ggtitle(\"\")__\n\n[![](09-chap_files/figure-html/fig-CombatIBD-1.png)](09-chap_files/figure-\nhtml/fig-CombatIBD-1.png \"Figure 9.17: The modified data with the batch\neffects removed now show three batch-groups heavily overlapping and centered\nalmost at the origin.\")\n\nFigure 9.17: The modified data with the batch effects removed now show three\nbatch-groups heavily overlapping and centered almost at the origin.\n\n### 9.3.3 Hybrid data and Bioconductor containers\n\nA more rational way of combining the batch and treatment information into\ncompartments of a composite object is to use the _SummarizedExperiment_ class.\nIt includes special _slots_ for the assay(s) where rows represent features of\ninterest (e.g., genes, transcripts, exons, etc.) and columns represent\nsamples. Supplementary information about the features can be stored in a\n_DataFrame_ object, accessible using the function `rowData`. Each row of the\n_DataFrame_ provides information on the feature in the corresponding row of\nthe _SummarizedExperiment_ object.\n\n[![A confusing notational similarity occurs here, in the SummarizedExperiment\nframework a DataFrame is not the same as a\ndata.frame.](imgs/devil.png)](imgs/devil.png \"A confusing notational\nsimilarity occurs here, in the SummarizedExperiment framework a DataFrame is\nnot the same as a data.frame.\")\n\nA confusing notational similarity occurs here, in the SummarizedExperiment\nframework a `DataFrame` is not the same as a _data.frame._\n\nHere we insert the two covariates day and treatment in the `colData` object\nand combine it with assay data in a new _SummarizedExperiment_ object.\n\n    \n    \n    library(\"SummarizedExperiment\")\n    treatment  = factor(ifelse(grepl(\"Cntr|^C\", colnames(IBDchip)), \"CTL\", \"IBS\"))\n    sampledata = DataFrame(day = day, treatment = treatment)\n    chipse = SummarizedExperiment(assays  = list(abundance = assayIBD),\n                                  colData = sampledata)__\n\nThis is the best way to keep all the relevant data together, it will also\nenable you to quickly filter the data while keeping all the information\naligned properly.\n\n[![You can explore composite objects using the Environment pane in RStudio.\nYou will see that in chipse, some of the slots are\nempty.](imgs/devil.png)](imgs/devil.png \"You can explore composite objects\nusing the Environment pane in RStudio. You will see that in chipse, some of\nthe slots are empty.\")\n\nYou can explore composite objects using the Environment pane in RStudio. You\nwill see that in `chipse`, some of the slots are empty.\n\n__\n\nQuestion 9.8\n\nMake a new _SummarizedExperiment_ object by choosing the subset of the samples\nthat were created on day 2.\n\n__\n\nSolution\n\n__\n\n    \n    \n    chipse[, day == 2]__\n    \n    \n    class: SummarizedExperiment \n    dim: 8634 16 \n    metadata(0):\n    assays(1): abundance\n    rownames(8634): 01010101000000.2104_gPM_GC 01010101000000.2141_gPM_GC\n      ... bru.tab.d.HIII.Con323.sig_st bru.tab.d.HIII.Con5.sig_st\n    rowData names(0):\n    colnames(16): 20CF 20DF ... IBSM IBSP\n    colData names(2): day treatment\n\nColumns of the _DataFrame_ represent different attributes of the features of\ninterest, e.g., gene or transcript IDs, etc. Here is an example of hybrid data\ncontainer from single cell experiments (see Bioconductor workflow in\nPerraudeau et al. ([2017](16-chap.html#ref-Perraudeau:2017)) for more\ndetails).\n\n    \n    \n    corese = readRDS(\"../data/normse.rds\")\n    norm = assays(corese)$normalizedValues __\n\nAfter the pre-processing and normalization steps prescribed in the workflow,\nwe retain the 1000 most variable genes measured on 747 cells.\n\n__\n\nQuestion 9.9\n\nHow many different batches do the cells belong to ?\n\n__\n\nSolution\n\n__\n\n    \n    \n    length(unique(colData(corese)$Batch))__\n    \n    \n    [1] 18\n\nWe can look at a PCA of the normalized values and check graphically that the\nbatch effect has been removed:\n\n    \n    \n    respca = dudi.pca(t(norm), nf = 3, scannf = FALSE)\n    plotscree(respca, 15)\n    PCS = respca$li[, 1:3]__\n\n[![](09-chap_files/figure-html/fig-screeplotnorm-1.png)](09-chap_files/figure-\nhtml/fig-screeplotnorm-1.png \"Figure 9.18: Screeplot of the PCA of the\nnormalized data.\")\n\nFigure 9.18: Screeplot of the PCA of the normalized data.\n\n[![We have set up colors for the clusters as in the workflow, \\(the code is\nnot shown here\\).](imgs/devil.png)](imgs/devil.png \"We have set up colors for\nthe clusters as in the workflow, \\(the code is not shown here\\).\")\n\nWe have set up colors for the clusters as in the workflow, (the code is not\nshown here).\n\nSince the screeplot in Figure 9.18 shows us that we must not dissociate axes 2\nand 3, we will make a three dimensional plot with the\n**[rgl](https://cran.r-project.org/web/packages/rgl/)** package. We use the\nfollowing interactive code:\n\n    \n    \n    library(\"rgl\")\n    batch = colData(corese)$Batch\n    plot3d(PCS,aspect=sqrt(c(84,24,20)),col=col_clus[batch])\n    plot3d(PCS,aspect=sqrt(c(84,24,20)),\n    col = col_clus[as.character(publishedClusters)])__\n\n[![](imgs/plotnormpcabatch1.png)](imgs/plotnormpcabatch1.png\n\"Figure 9.19 \\(a\\): \")\n\n(a)\n\n![](imgs/plotnormpclust1.png):\n\")\n\n(b)\n\nFigure 9.19: Two-dimensional screenshots of three-dimensional\n**[rgl](https://cran.r-project.org/web/packages/rgl/)** plots. The points are\ncolored according to batch numbers in (a), and according to the original\nclustering in (b). We can see that the batch effect has been effectively\nremoved and that the cells show the original clustering.\n\n**Note:** Of course, the book medium is limiting here, as we are showing two\nstatic projections that do not do justice to the depth available when looking\nat the interactive dynamic plots as they appear using the `plot3d` function.\nWe encourage the reader to experiment extensively with these and other\ninteractive packages and they provide a much more intuitive experience of the\ndata.\n\n## 9.4 Correspondence analysis for contingency tables\n\n### 9.4.1 Cross-tabulation and contingency tables\n\nCategorical data abound in biological settings: sequence status (CpG/non-CpG),\nphenotypes, taxa are often coded as factors as we saw in Chapter 2. Cross-\ntabulation of two such variables gives us a **contingency table** ; the result\nof counting the co-occurrence of two phenotypes (sex and colorblindness was\nsuch an example). We saw that the first step is to look at the independence of\nthe two categorical variables; the standard statistical measure of\nindependence uses the **chisquare distance**. This quantity will replace the\nvariance we used for continuous measurements.\n\nThe columns and rows of the table have the same `status’ and we are not in\nsupervised/regression type setting. We won’t see a sample/variable divide; as\na consequence the rows and columns will have the same status and we will\n‘center’ both the rows and the columns. This symmetry will also translate in\nour use of **biplots** where both dimensions appear on the same plot.\n\nTable 9.1: Sample by mutation matrix.\n\nPatient | Mut1 | Mut2 | Mut3 | ...  \n---|---|---|---|---  \nAHX112 | 0 | 0 | 0 |   \nAHX717 | 1 | 0 | 1 |   \nAHX543 | 1 | 0 | 0 |   \n  \n#### Transforming the data to tabular form.\n\nIf the data are collected as long lists with each subject (or sample)\nassociated to its levels of the categorical variables, we may want to\ntransform them into a contingency table. Here is an example. In Table 9.1 HIV\nmutations are tabulated as indicator (0/1) binary variables. These data are\nthen transformed into a **mutation co-occurrence matrix** shown in Table 9.2.\n\nTable 9.2: Cross-tabulation of the HIV mutations showing two-way co-\noccurrences.\n\nPatient | Mut1 | Mut2 | Mut3 | ...  \n---|---|---|---|---  \nMut1 | 853 | 29 | 10 |   \nMut2 | 29 | 853 | 52 |   \nMut3 | 10 | 52 | 853 |   \n  \n__\n\nQuestion 9.10\n\nWhat information is lost in this cross-tabulation ?  \nWhen will this matter?\n\nHere are some co-occurrence data from the HIV database ([Rhee et al.\n2003](16-chap.html#ref-HIVdb)). Some of these mutations have a tendency to co-\noccur.\n\n__\n\nQuestion 9.11\n\nTest the hypothesis of independence of the mutations.\n\nBefore explaining the details of how correspondence analysis works, let’s look\nat the output of one of many correspondence analysis functions. We use\n`dudi.coa` from the **[ade4](https://cran.r-project.org/web/packages/ade4/)**\npackage to plot the mutations in a lower dimensional projection; the procedure\nfollows what we did for PCA.\n\n    \n    \n    cooc = read.delim2(\"../data/coccurHIV.txt\", header = TRUE, sep = \",\")\n    cooc[1:4, 1:11]__\n    \n    \n        X4S X6D X6K X11R X20R X21I X35I X35L X35M X35T X39A\n    4S    0  28   8    0   99    0   22    5   15    3   45\n    6D   26   0   0   34  131    0  108    4   30   13   84\n    6K    7   0   0    6   45    0    5   13   38   35   12\n    11R   0  35   7    0  127   12   60   17   15    6   42\n    \n    \n    HIVca = dudi.coa(cooc, nf = 4, scannf = FALSE)\n    fviz_eig(HIVca, geom = \"bar\", bar_width = 0.6) + ggtitle(\"\")__\n\n[![](09-chap_files/figure-html/fig-HIVnnrti-1.png)](09-chap_files/figure-\nhtml/fig-HIVnnrti-1.png \"Figure 9.20: The dependencies between HIV mutations\nis clearly a three dimensional phenomenon, the three first eigenvalues show a\nclear signal in the data.\")\n\nFigure 9.20: The dependencies between HIV mutations is clearly a three\ndimensional phenomenon, the three first eigenvalues show a clear signal in the\ndata.\n\n[![](imgs/scatter3d-HIV.png)](imgs/scatter3d-HIV.png \"Figure 9.21: A\nscreenshot of the output from an interactive 3d plotting function\n\\(plot3d\\).\")\n\nFigure 9.21: A screenshot of the output from an interactive 3d plotting\nfunction (`plot3d`).\n\nAfter looking at a screeplot, we see that dimensionality of the underlying\nvariation is definitely three dimensional, we plot these three dimensions.\nIdeally this would be done with an interactive three-dimensional plotting\nfunction such as that provided through the package\n**[rgl](https://cran.r-project.org/web/packages/rgl/)** as shown in Figure\n9.21.\n\n__\n\nQuestion 9.12\n\nUsing the **[car](https://cran.r-project.org/web/packages/car/)** and\n**[rgl](https://cran.r-project.org/web/packages/rgl/)** packages make 3d\nscatterplot similar to Figure 9.21.  \nCompare to the plot obtained using aspect=FALSE with the plot3d function from\n**[rgl](https://cran.r-project.org/web/packages/rgl/)**.  \nWhat structure do you notice by rotating the cloud of points?\n\n__\n\nSolution\n\n__\n\n    \n    \n    library(\"rgl\")\n    CA1=HIVca$li[,1];CA2=HIVca$li[,2];CA3=HIVca$li[,3]\n    plot3d(CA1,CA2,CA3,aspect=FALSE,col=\"purple\")__\n    \n    \n    fviz_ca_row(HIVca,axes = c(1, 2),geom=\"text\", col.row=\"purple\",\n      labelsize=3)+ggtitle(\"\") + xlim(-0.55, 1.7) + ylim(-0.53,1.1) +\n      theme_bw() +  coord_fixed()\n    fviz_ca_row(HIVca,axes = c(1, 3), geom=\"text\",col.row=\"purple\",\n        labelsize=3)+ggtitle(\"\")+ xlim(-0.55, 1.7)+ylim(-0.5,0.6) +\n        theme_bw() + coord_fixed()__\n\n[![](09-chap_files/figure-html/fig-HIVca-1.png)](09-chap_files/figure-\nhtml/fig-HIVca-1.png \"Figure 9.22 \\(a\\): \")\n\n(a)\n\n[![](09-chap_files/figure-html/fig-HIVca-2.png)](09-chap_files/figure-\nhtml/fig-HIVca-2.png \"Figure 9.22 \\(b\\): \")\n\n(b)\n\nFigure 9.22: Two planar maps of the mutations defined with the horizontal axis\ncorresponding to the first eigenvector of the CA and the vertical axis being\nthe second axis in (a), and the third in (b); notice the difference in\nheights.\n\n__\n\nQuestion 9.13\n\nShow the code for plotting the plane defined by axes 1 and 3 of the\ncorrespondence analysis respecting the scaling of the vertical axis as shown\nin the bottom figure of Figure 9.22.\n\n__\n\nSolution\n\n__\n\n    \n    \n    fviz_ca_row(HIVca, axes=c(1, 3), geom=\"text\", col.row=\"purple\", labelsize=3) +\n      ggtitle(\"\") + theme_minimal() + coord_fixed()__\n\nThis first example showed how to map all the different levels of one\ncategorical variable (the mutations) in a similar way to how PCA projects\ncontinuous variables. We will now explore how this can be extended to two or\nmore categorical variables.\n\n### 9.4.2 Hair color, eye color and phenotype co-occurrence\n\nWe will consider a small table, so we can follow the analysis in detail. The\ndata are a contingency table of hair-color and eye-color phenotypic co-\noccurrence from students as shown in Table 9.3. In [Chapter 2](02-chap.html),\nwe used a \\\\(\\chi^2\\\\) test to detect possible dependencies:\n\n    \n    \n    HairColor = HairEyeColor[,,2]\n    chisq.test(HairColor)__\n    \n    \n        Pearson's Chi-squared test\n    \n    data:  HairColor\n    X-squared = 106.66, df = 9, p-value < 2.2e-16\n\nTable 9.3: Cross tabulation of students hair and eye color.\n\n| Brown | Blue | Hazel | Green  \n---|---|---|---|---  \nBlack | 36 | 9 | 5 | 2  \nBrown | 66 | 34 | 29 | 14  \nRed | 16 | 7 | 7 | 7  \nBlond | 4 | 64 | 5 | 8  \n  \nHowever, stating _non independence_ between hair and eye color is not enough.\nWe need a more detailed explanation of where the dependencies occur: which\nhair color occurs more often with green eyes ? Are some of the variable levels\nindependent? In fact we can study the departure from independence using a\nspecial weighted version of SVD. This method can be understood as a simple\nextension of PCA and MDS to contingency tables.\n\n#### Independence: computationally and visually.\n\nWe start by computing the row and column sums; we use these to build the table\nthat would be expected if the two phenotypes were independent. We call this\nexpected table `HCexp`.\n\n    \n    \n    rowsums = as.matrix(apply(HairColor, 1, sum))\n    rowsums __\n    \n    \n          [,1]\n    Black   52\n    Brown  143\n    Red     37\n    Blond   81\n    \n    \n    colsums = as.matrix(apply(HairColor, 2, sum))\n    t(colsums)__\n    \n    \n         Brown Blue Hazel Green\n    [1,]   122  114    46    31\n    \n    \n    HCexp = rowsums %*%t (colsums) / sum(colsums)__\n\nNow we compute the \\\\(\\chi^2\\\\) (chi-squared) statistic, which is the sum of\nthe scaled residuals for each of the cells of the table:\n\n    \n    \n    sum((HairColor  - HCexp)^2/HCexp)__\n    \n    \n    [1] 106.6637\n\nWe can study these residuals from the expected table, first numerically then\nin Figure 9.23.\n\n    \n    \n    round(t(HairColor-HCexp))__\n    \n    \n           Hair\n    Eye     Black Brown Red Blond\n      Brown    16    10   2   -28\n      Blue    -10   -18  -6    34\n      Hazel    -3     8   2    -7\n      Green    -3     0   3     0\n    \n    \n    library(\"vcd\")\n    mosaicplot(HairColor, shade=TRUE, las=1, type=\"pearson\", cex.axis=0.7, main=\"\")__\n\n[![](09-chap_files/figure-html/fig-MosaicHair-1.png)](09-chap_files/figure-\nhtml/fig-MosaicHair-1.png \"Figure 9.23: Visualization of the departure from\nindependence. Now, the boxes are proportional in size to the actual observed\ncounts and we no longer have a ‘rectangular’ property. The departure from\nindependence is measured in Chisquared distance for each of the boxes and\ncolored according to whether the residuals are large and positive. Dark blue\nindicates a positive association, for instance between blue eyes and blonde\nhair, red indicates a negative association such as in the case of blond hair\nand brown eyes.\")\n\nFigure 9.23: Visualization of the departure from independence. Now, the boxes\nare proportional in size to the actual observed counts and we no longer have a\n‘rectangular’ property. The departure from independence is measured in\nChisquared distance for each of the boxes and colored according to whether the\nresiduals are large and positive. Dark blue indicates a positive association,\nfor instance between blue eyes and blonde hair, red indicates a negative\nassociation such as in the case of blond hair and brown eyes.\n\n#### Mathematical Formulation.\n\nHere are the computations we just did in R in a more mathematical form. For a\ngeneral contingency table \\\\({\\mathbf N}\\\\) with \\\\(I\\\\) rows and \\\\(J\\\\)\ncolumns and a total sample size of \\\\(n=\\sum_{i=1}^I \\sum_{j=1}^J n_{ij}=\nn_{\\cdot \\cdot}\\\\). If the two categorical variables were independent, each\ncell frequency would be approximately equal to\n\n\\\\[ n_{ij} = \\frac{n_{i \\cdot}}{n} \\frac{n_{\\cdot j}}{n} \\times n \\\\]\n\ncan also be written:\n\n\\\\[ {\\mathbf N} = {\\mathbf c r'} \\times n, \\qquad \\mbox{ where } c=\n\\frac{1}{n} {{\\mathbf N}} {\\mathbb 1}_m \\;\\mbox{ and }\\; r'=\\frac{1}{n}\n{\\mathbf N}' {\\mathbb 1}_p \\\\]\n\nThe departure from independence is measured by the \\\\(\\chi^2\\\\) statistic\n\n\\\\[ {\\cal X}^2=\\sum_{i,j}\n\\frac{\\left(n_{ij}-\\frac{n_{i\\cdot}}{n}\\frac{n_{\\cdot j}}{n}n\\right)^2}\n{\\frac{n_{i\\cdot}n_{\\cdot j}}{n^2}n} \\\\]\n\nOnce we have ascertained that the two variables are not independent, we use a\nweighted multidimensional scaling using \\\\(\\chi^2\\\\) distances to visualize\nthe associations.\n\n**Correspondece Analysis functions** `CCA` in\n**[vegan](https://cran.r-project.org/web/packages/vegan/)** , `CA` in\n**[FactoMineR](https://cran.r-project.org/web/packages/FactoMineR/)** ,\n`ordinate` in **[phyloseq](https://bioconductor.org/packages/phyloseq/)** ,\n`dudi.coa` in **[ade4](https://cran.r-project.org/web/packages/ade4/)**.\n\nThe method is called **Correspondence Analysis (CA)** or **Dual Scaling** and\nthere are multiple R packages that implement it.\n\nHere we make a simple biplot of the Hair and Eye colors.\n\n    \n    \n    HC = as.data.frame.matrix(HairColor)\n    coaHC = dudi.coa(HC,scannf=FALSE,nf=2)\n    round(coaHC$eig[1:3]/sum(coaHC$eig)*100)__\n    \n    \n    [1] 89 10  2\n    \n    \n    fviz_ca_biplot(coaHC, repel=TRUE, col.col=\"brown\", col.row=\"purple\") +\n      ggtitle(\"\") + ylim(c(-0.5,0.5))__\n\n[![](09-chap_files/figure-html/fig-HCscatter-1.png)](09-chap_files/figure-\nhtml/fig-HCscatter-1.png \"Figure 9.24: The CA plot gives a representation of a\nlarge proportion of the chisquare distance between the data and the values\nexpected under independence. The first axis shows a contrast between black\nhaired and blonde haired students, mirrored by the brown eye, blue eye\nopposition. In CA the two categories play symmetric roles and we can interpret\nthe proximity of Blue eyes and Blond hair has meaning that there is strong co-\noccurence of these categories.\")\n\nFigure 9.24: The CA plot gives a representation of a large proportion of the\nchisquare distance between the data and the values expected under\nindependence. The first axis shows a contrast between black haired and blonde\nhaired students, mirrored by the brown eye, blue eye opposition. In CA the two\ncategories play symmetric roles and we can interpret the proximity of Blue\neyes and Blond hair has meaning that there is strong co-occurence of these\ncategories.\n\n__\n\nQuestion 9.14\n\nWhat percentage of the Chisquare statistic is explained by the first two axes\nof the Correspondence Analysis?\n\n__\n\nQuestion 9.15\n\nCompare the results with those obtained by using `CCA` in the\n**[vegan](https://cran.r-project.org/web/packages/vegan/)** package with the\nappropriate value for the `scaling` parameter.\n\n__\n\nSolution\n\n__\n\n    \n    \n    library(\"vegan\")\n    res.ca = vegan::cca(HairColor)\n    plot(res.ca, scaling=3)__\n\n#### Interpreting the biplots\n\nCA has a special barycentric property: the biplot scaling is chosen so that\nthe row points are placed at the center of gravity of the column levels with\ntheir respective weights. For instance, the Blue eyes column point is at the\ncenter gravity of the (Black, Brown, Red, Blond) with weights proportional to\n(9, 34, 7, 64). The Blond row point is very heavily weighted, this is why\nFigure 9.24 shows Blond and Blue quite close together.\n\n## 9.5 Finding time…and other important gradients.\n\nAll the methods we have studied in the last sections are commonly known as\n**ordination** methods. In the same way **clustering** allowed us to detect\nand interpret a hidden factor/categorical variable, ordination enables us to\ndetect and interpret a hidden ordering, gradient or latent variable in the\ndata.\n\n![](imgs/ProustProxy.png)\n\nEcologists have a long history of interpreting the arches formed by\nobservations points in correspondence analysis and principal components as\necological gradients ([Prentice 1977](16-chap.html#ref-Prentice:1977)). Let’s\nillustrate this first with a very simple data set on which we perform a\ncorrespondence analysis.\n\nThe first examples of seriation or chronology detection was that of\narchaelogical artifacts by Kendall ([1969](16-chap.html#ref-Kendall:1969)),\nwho used presence/absence of features on pottery to date them. These so-called\nseriation methods are still relevant today as we follow developmental\ntrajectories in single cell data for instance.\n\n    \n    \n    load(\"../data/lakes.RData\")\n    lakelike[1:3,1:8]__\n    \n    \n         plant1 plant2 plant3 plant4 plant5 plant6 plant7 plant8\n    loc1      6      4      0      3      0      0      0      0\n    loc2      4      5      5      3      4      2      0      0\n    loc3      3      4      7      4      5      2      1      1\n    \n    \n    reslake=dudi.coa(lakelike,scannf=FALSE,nf=2)\n    round(reslake$eig[1:8]/sum(reslake$eig),2)__\n    \n    \n    [1] 0.56 0.25 0.09 0.03 0.03 0.02 0.01 0.00\n\nWe plot both the row-location points (Figure 9.25 (a)) and the biplot of both\nlocation and plant species in the lower part of Figure 9.25 (b); this plot was\nmade with:\n\n    \n    \n    fviz_ca_row(reslake,repel=TRUE)+ggtitle(\"\")+ylim(c(-0.55,1.7))\n    fviz_ca_biplot(reslake,repel=TRUE)+ggtitle(\"\")+ylim(c(-0.55,1.7))__\n\n[![](09-chap_files/figure-html/fig-LakeCAr-1.png)](09-chap_files/figure-\nhtml/fig-LakeCAr-1.png \"Figure 9.25 \\(a\\): \")\n\n(a)\n\n[![](09-chap_files/figure-html/fig-LakeCAr-2.png)](09-chap_files/figure-\nhtml/fig-LakeCAr-2.png \"Figure 9.25 \\(b\\): \")\n\n(b)\n\nFigure 9.25: The locations near the lake are ordered along an arch as shown in\n(a). In the biplot (b), we can see which plants are most frequent at which\nlocations by looking at the red triangles closest to the blue points.\n\n__\n\nQuestion 9.16\n\nLooking back at the raw matrix `lakes` as it appears, do you see a pattern in\nits entries?  \nWhat would happen if the plants had been ordered by actual taxa names for\ninstance?\n\n### 9.5.1 Dynamics of cell development\n\nWe will now analyse a more interesting data set that was published by Moignard\net al. ([2015](16-chap.html#ref-Moignard:2015)). This paper describes the\ndynamics of blood cell development. The data are single cell gene expression\nmeasurements of 3,934 cells with blood and endothelial potential from five\npopulations from between embryonic days E7.0 and E8.25.\n\n[![](imgs/CellTree.png)](imgs/CellTree.png \"Figure 9.26: The four cell\npopulations studied here are representative of three sequential states\n\\(PS,NP,HF\\) and two possible final branches \\(4SG and 4SFG^{-}\\).\")\n\nFigure 9.26: The four cell populations studied here are representative of\nthree sequential states (PS,NP,HF) and two possible final branches (4SG and\n4SFG\\\\(^{-}\\\\)).\n\nRemember from [Chapter 4](04-chap.html) that several different distances are\navailable for comparing our cells. Here, we start by computing both an\n\\\\(L_2\\\\) distance and the \\\\(\\ell_1\\\\) distance between the 3,934 cells.\n\n    \n    \n    Moignard = readRDS(\"../data/Moignard.rds\")\n    cellt = rowData(Moignard)$celltypes\n    colsn = c(\"red\", \"purple\", \"orange\", \"green\", \"blue\")\n    blom = assay(Moignard)\n    dist2n.euclid = dist(blom)\n    dist1n.l1     = dist(blom, \"manhattan\")__\n\nThe classical multidimensional scaling on these two distances matrices can be\ncarried out using:\n\n    \n    \n    ce1Mds = cmdscale(dist1n.l1,     k = 20, eig = TRUE)\n    ce2Mds = cmdscale(dist2n.euclid, k = 20, eig = TRUE)\n    perc1  = round(100*sum(ce1Mds$eig[1:2])/sum(ce1Mds$eig))\n    perc2  = round(100*sum(ce2Mds$eig[1:2])/sum(ce2Mds$eig))__\n\nWe look at the underlying dimension and see in Figure 9.27 that two dimensions\ncan provide a substantial fraction of the variance.\n\n    \n    \n    plotscree(ce1Mds, m = 4)\n    plotscree(ce2Mds, m = 4)__\n\n[![](09-chap_files/figure-html/fig-CMDSplotscree-1.png)](09-chap_files/figure-\nhtml/fig-CMDSplotscree-1.png \"Figure 9.27 \\(a\\): \")\n\n(a)\n\n[![](09-chap_files/figure-html/fig-CMDSplotscree-2.png)](09-chap_files/figure-\nhtml/fig-CMDSplotscree-2.png \"Figure 9.27 \\(b\\): \")\n\n(b)\n\nFigure 9.27: Screeplots from MDS on \\\\(\\ell_1\\\\) (a) and \\\\(L_2\\\\) (b)\ndistances. We see that the eigenvalues are extremely similar and both point to\na \\\\(2\\\\) dimensional phenomenon.\n\nThe first 2 coordinates account for 78 % of the variability when the\n\\\\(\\ell_1\\\\) distance is used between cells, and 57% when the \\\\(L^2\\\\)\ndistance is used. We see in Figure 9.28 (a) the first plane for the MDS on the\n\\\\(\\ell_1\\\\) distances between cells:\n\n    \n    \n    c1mds = ce1Mds$points[, 1:2] |>\n            `colnames<-`(paste0(\"L1_PCo\", 1:2)) |>\n            as_tibble()\n    ggplot(c1mds, aes(x = L1_PCo1, y = L1_PCo2, color = cellt)) +\n      geom_point(aes(color = cellt), alpha = 0.6) +\n      scale_colour_manual(values = colsn) + guides(color = \"none\")\n    c2mds = ce2Mds$points[, 1:2] |>\n            `colnames<-`(paste0(\"L2_PCo\", 1:2)) |>\n            as_tibble()\n    ggplot(c2mds, aes(x = L2_PCo1, y = L2_PCo2, color = cellt)) +\n      geom_point(aes(color = cellt), alpha = 0.6) +\n       scale_colour_manual(values = colsn) + guides(color = \"none\")__\n\n[![](09-chap_files/figure-html/fig-CMDSplotL2-1.png)](09-chap_files/figure-\nhtml/fig-CMDSplotL2-1.png \"Figure 9.28 \\(a\\): \")\n\n(a)\n\n[![](09-chap_files/figure-html/fig-CMDSplotL2-2.png)](09-chap_files/figure-\nhtml/fig-CMDSplotL2-2.png \"Figure 9.28 \\(b\\): \")\n\n(b)\n\nFigure 9.28: Moignard cell data colored according to the cell types (blue: PS,\ngreen: NP, yellow: HF, red: 4SG, purple: 4SFG\\\\(^-\\\\)) in the two dimensional\nMDS plots created. In (a) using \\\\(\\ell_1\\\\) distances and in (b) using the L2\ndistances.\n\nFigure 9.28 (b) is created in the same way and shows the two-dimensional\nprojection created by using MDS on the L2 distances.\n\nFigure 9.28 shows that both distances (L1 and L2) give the same first plane\nfor the MDS with very similar representations of the underlying gradient\nfollowed by the cells.\n\nWe can see from Figure 9.28 that the cells are not distributed uniformly in\nthe lower dimensions we have been looking at, we see a definite organization\nof the points. All the cells of type 4SG represented in red form an elongated\ncluster who are much less mixed with the other cell types.\n\n### 9.5.2 Local, nonlinear methods\n\nMultidimensional scaling and non metric multidimensional scaling aims to\nrepresent **all** distances as precisely as possible and the large distances\nbetween far away points skew the representations. It can be beneficial when\nlooking for gradients or low dimensional manifolds to restrict ourselves to\napproximations of points that are close together. This calls for methods that\ntry to represent local (small) distances well and do not try to approximate\ndistances between faraway points with too much accuracy.\n\nThere has been substantial progress in such methods in recent years. The use\nof **kernels** computed using the calculated interpoint distances allows us to\ndecrease the importance of points that are far apart. A radial basis kernel is\nof the form\n\n\\\\[ 1-\\exp\\left(-\\frac{d(x,y)^2}{\\sigma^2}\\right), \\quad\\mbox{where } \\sigma^2\n\\mbox{ is fixed.} \\\\]\n\nIt has the effect of heavily discounting large distances. This can be very\nuseful as the precision of interpoint distances is often better at smaller\nranges; several examples of such methods are covered in Exercise 9.6 at the\nend of this chapter.\n\n__\n\nQuestion 9.17\n\nWhy do we take the difference between the 1 and the exponential?  \nWhat happens when the distance between \\\\(x\\\\) and \\\\(y\\\\) is very big?\n\n#### t-SNE.\n\nThis widely used method adds flexibility to the kernel defined above and\nallows the \\\\(\\sigma^2\\\\) parameter to vary locally (there is a normalization\nstep so that it averages to one). The t-SNE method starts out from the\npositions of the points in the high dimensional space and derives a\nprobability distribution on the set of pairs of points, such that the\nprobabilities are proportional to the points’ proximities or similarities. It\nthen uses this distribution to construct a representation of the dataset in\nlow dimensions. The method is not robust and has the property of separating\nclusters of points artificially; however, this property can also help clarify\na complex situation. One can think of it as a method akin to graph (or\nnetwork) layout algorithms. They stretch the data to clarify relations between\nthe very close (in the network: connected) points, but the distances between\nmore distal (in the network: unconnected) points cannot be interpreted as\nbeing on the same scales in different regions of the plot. In particular,\nthese distances will depend on the local point densities. Here is an example\nof the output of t-SNE on the cell data:\n\n    \n    \n    library(\"Rtsne\")\n    restsne = Rtsne(blom, dims = 2, perplexity = 30, verbose = FALSE,\n                    max_iter = 900)\n    dftsne = restsne$Y[, 1:2] |>\n             `colnames<-`(paste0(\"axis\", 1:2)) |>\n             as_tibble()\n    ggplot(dftsne,aes(x = axis1, y = axis2, color = cellt)) +\n      geom_point(aes(color = cellt), alpha = 0.6) +\n       scale_color_manual(values = colsn) + guides(color = \"none\")\n    restsne3 = Rtsne(blom, dims = 3, perplexity = 30, verbose = FALSE,\n                     max_iter = 900)\n    dftsne3 = restsne3$Y[, 1:3] |>\n              `colnames<-`(paste0(\"axis\", 1:3)) |> \n              as_tibble()\n    ggplot(dftsne3,aes(x = axis3, y = axis2, group = cellt)) +\n          geom_point(aes(color = cellt), alpha = 0.6) +\n          scale_colour_manual(values = colsn) + guides(color = \"none\")__\n\n[![](09-chap_files/figure-html/fig-tsnecells-1.jpeg)](09-chap_files/figure-\nhtml/fig-tsnecells-1.jpeg \"Figure 9.29 \\(a\\): \")\n\n(a)\n\n[![](09-chap_files/figure-html/fig-tsnecells-2.jpeg)](09-chap_files/figure-\nhtml/fig-tsnecells-2.jpeg \"Figure 9.29 \\(b\\): \")\n\n(b)\n\nFigure 9.29: The four cell populations studied here are representative of\nthree sequential states (PS,NP,HF) and two possible final branches (4SG and\n4SFG\\\\(^{-}\\\\)). The plot on the left was obtained by choosing 2 dimensions\nfor t-sne at a perplexity of 30. The lower plot has obtained by choosing 3\ndimensions, we can see that this third t-SNE axis represented here as the\nhorizontal axis.\n\nIn this case in order to see the subtle differences between MDS and t-SNE, it\nis really necessary to use 3d plotting.\n\n__\n\nTask\n\nUse the **[rgl](https://cran.r-project.org/web/packages/rgl/)** package to\nlook at the three t-SNE dimensions and add the correct cell type colors to the\ndisplay.\n\nTwo of these 3d snapshots are shown in Figure 9.30, we see a much stronger\ngrouping of the purple points than in the MDS plots.\n\n**Note:** A site worth visiting in order to appreciate more about the\nsensitivity of the t-SNE method to the complexity and \\\\(\\sigma\\\\) parameters\ncan be found at <http://distill.pub/2016/misread-tsne>.\n\n[![](imgs/tsnemoignard3scrop.png)](imgs/tsnemoignard3scrop.png\n\"Figure 9.30 \\(a\\): \")\n\n(a)\n\n[![](imgs/tsnemoignard3crop.png)](imgs/tsnemoignard3crop.png\n\"Figure 9.30 \\(b\\): \")\n\n(b)\n\nFigure 9.30: Moignard cell data colored according to the cell types (blue: PS,\ngreen: NP, yellow: HF, red: 4SG, purple: 4SFG\\\\(^-\\\\)) in the three-\ndimensional t-SNE layouts. We can see that the purple cells (4SFG\\\\(^-\\\\))\nsegregate at the outer shell on the top of the point cloud.\n\n__\n\nQuestion 9.18\n\nVisualize a two-dimensional t-SNE embedding of the Ukraine distances from\nSection 9.2.\n\n__\n\nSolution\n\n__\n\n    \n    \n    ukraine_tsne = Rtsne(ukraine_dists, is_distance = TRUE, perplexity = 8)\n    ukraine_tsne_df = tibble(\n      PCo1 = ukraine_tsne$Y[, 1],\n      PCo2 = ukraine_tsne$Y[, 2],\n      labs = attr(ukraine_dists, \"Labels\")\n    )\n    ggplot(ukraine_tsne_df, aes(x = PCo1, y = PCo2, label = labs)) +\n      geom_point() + geom_text_repel(col = \"#0057b7\") + coord_fixed() __\n\n[![](09-chap_files/figure-html/fig-ukrainetsne-1.png)](09-chap_files/figure-\nhtml/fig-ukrainetsne-1.png \"Figure 9.31: t-SNE map based of Ukraine.\")\n\nFigure 9.31: t-SNE map based of Ukraine.\n\nThere are several other nonlinear methods for estimating nonlinear\ntrajectories followed by points in the relevant state spaces. Here are a few\nexamples.\n\n**[RDRToolbox](https://cran.r-project.org/web/packages/RDRToolbox/)** Local\nlinear embedding (**LLE**) and **isomap**\n\n**[diffusionMap](https://cran.r-project.org/web/packages/diffusionMap/)** This\npackage models connections between points as a Markovian kernel.\n\n**[kernlab](https://cran.r-project.org/web/packages/kernlab/)** Kernel methods\n\n**[LPCM-package](https://cran.r-project.org/web/packages/LPCM-package/)**\nLocal principal curves\n\n## 9.6 Multitable techniques\n\nCurrent studies often attempt to quantify variation in the microbial, genomic,\nand metabolic measurements across different experimental conditions. As a\nresult, it is common to perform multiple assays on the same biological samples\nand ask what features – microbes, genes, or metabolites, for example – are\nassociated with different sample conditions. There are many ways to approach\nthese questions. Which to apply depends on the study’s focus.\n\n### 9.6.1 Co-variation, inertia, co-inertia and the RV coefficient\n\nAs in physics, we define inertia as a sum of distances with ‘weighted’ points.\nThis enables us to compute the inertia of counts in a contingency table as the\nweighted sum of the squares of distances between observed and expected\nfrequencies (as in the chisquare statistic).\n\nAnother generalization of variance-inertia is the useful Phylogenetic\ndiversity index. (computing the sum of distances between a subset of taxa\nthrough the tree). Other useful generalizations include using variability of\npoints on a graph taken from standard spatial statistics.\n\nIf we want to study two standardized variables measured at the same 10\nlocations together, we use their **covariance**. If \\\\(x\\\\) represents the\nstandardized PH, and and \\\\(y\\\\) the standardized humidity, we measure their\ncovariation using the mean\n\n\\\\[ \\text{cov}(x,y) = \\text{mean}(x1*y1 + x2*y2 + x3*y3 + \\cdots + x10*y10).\n\\tag{9.2}\\\\]\n\nIf \\\\(x\\\\) and \\\\(y\\\\) co-vary in the same direction, this will be big. We saw\nhow useful the correlation coefficient we defined in [Chapter 8](08-chap.html)\nwas to our multivariate analyses. Multitable generalizations will be just as\nuseful.\n\n### 9.6.2 Mantel coefficient and a test of distance correlation\n\n[![There are some precautions to be taken when using the Mantel coefficient,\nsee a critical review in Guillot and Rousset\n\\(2013\\).](imgs/devil.png)](imgs/devil.png \"There are some precautions to be\ntaken when using the Mantel coefficient, see a critical review in\n@Guillot:2013.\")\n\nThere are some precautions to be taken when using the Mantel coefficient, see\na critical review in Guillot and Rousset ([2013](16-chap.html#ref-\nGuillot:2013)).\n\nThe Mantel coefficient, one of the earliest version of association measures,\nis probably also the most popular now, especially in ecology ([Josse and\nHolmes 2016](16-chap.html#ref-Josse:2016)). Given two dissimilarity matrices\n\\\\(D^X\\\\) and \\\\(D^Y\\\\) associated with \\\\(X\\\\) and \\\\(Y\\\\), make these\nmatrices into vectors the way the R `dist` function does, and compute their\nlinear correlation. A prototypical application is, for instance, to compute\n\\\\(D^X\\\\) from the soil chemistry at 17 different locations and to use\n\\\\(D^Y\\\\) to represent dissimilarities in plant occurences as measured by the\nJaccard index between the same 17 locations. The Mantel coefficient is defined\nmathematically as:\n\n\\\\[ r_m(X, Y) = \\frac{ \\sum_{i\\neq j} \\left(d_{ij}^X − \\bar{d}^X \\right)\n\\left(d_{ij}^Y − \\bar{d}^Y \\right) } { \\sqrt{ \\left( \\sum_{i\\neq j}\n\\left(d_{ij}^X − \\bar{d}^X \\right)^2 \\right) \\left( \\sum_{i\\neq j}\n\\left(d_{ij}^Y − \\bar{d}^Y \\right)^2 \\right) } }, \\\\]\n\nwith \\\\(\\bar{d}^X\\\\) (resp. \\\\(\\bar{d}^Y\\\\)) the mean of the upper diagonal\nelements of the dissimilarity matrix associated to \\\\(d^X\\\\) (resp. to\n\\\\(d^Y\\\\)). The main difference between the Mantel coefficient and the others\nsuch as the RV or the dCov is the absence of double centering. Due to the\ndependences within distances matrix, the Mantel correlation’s null\ndistribution and its statistical significance cannot be assessed as simply for\nregular correlation ccoefficients. Instead, it is usually assessed via\npermutation testing. See Josse and Holmes ([2016](16-chap.html#ref-\nJosse:2016)) for a review with historical background and modern incarnations.\nThe coefficient and associated tests are implemented in several R packages\nsuch as **[ade4](https://cran.r-project.org/web/packages/ade4/)** ([Chessel,\nDufour, and Thioulouse 2004](16-chap.html#ref-ade4)),\n**[vegan](https://cran.r-project.org/web/packages/vegan/)** and\n**[ecodist](https://cran.r-project.org/web/packages/ecodist/)** ([Goslee,\nUrban, et al. 2007](16-chap.html#ref-Goslee:2007)).\n\n### 9.6.3 The RV coefficient\n\nThe global measure of similarity of two data tables as opposed to two vectors\ncan be done by a generalization of covariance provided by an inner product\nbetween tables that gives the RV coefficient, a number between 0 and 1, like a\ncorrelation coefficient, but for tables.\n\n\\\\[ RV(A,B)=\\frac{Tr(A'BAB')}{\\sqrt{Tr(A'A)}\\sqrt{Tr(B'B)}} \\\\]\n\nThere are several other measures of matrix correlation available in the\npackage\n**[MatrixCorrelation](https://cran.r-project.org/web/packages/MatrixCorrelation/)**.\n\nIf we do ascertain a link between two matrices, we then need to find a way to\nunderstand that link. One such method is explained in the next section.\n\n### 9.6.4 Canonical correlation analysis (CCA)\n\nCCA is a method similar to PCA as it was developed by Hotelling in the 1930s\nto search for associations between two sets of continuous variables \\\\(X\\\\)\nand \\\\(Y\\\\). Its goal is to find a linear projection of the first set of\nvariables that maximally correlates with a linear projection of the second set\nof variables.\n\nFinding correlated functions (covariates) of the two views of the same\nphenomenon by discarding the representation-specific details (noise) is\nexpected to reveal the underlying hidden yet influential factors responsible\nfor the correlation.\n\nLet us consider two matrices:\n\n  * the \\\\(n\\times p\\\\) matrix \\\\(X\\\\), and\n  * the \\\\(n\\times p\\\\) matrix \\\\(Y\\\\).\n\nThe \\\\(p\\\\) columns of \\\\(X\\\\) and the \\\\(q\\\\) columns of \\\\(Y\\\\) correspond\nto variables, and the rows correspond to the same \\\\(n\\\\) experimental units.\nWe denote the \\\\(j\\\\)-th column of the matrix \\\\(X\\\\) by \\\\(X_j\\\\), likewise\nthe \\\\(k\\\\)-th column of \\\\(Y\\\\) by \\\\(Y_k\\\\). Without loss of generality it\nwill be assumed that the columns of \\\\(X\\\\) and \\\\(Y\\\\) are standardized (mean\n0 and variance 1).\n\nClassical CCA assumes that \\\\(p \\leq n\\\\) and \\\\(q \\leq n\\\\), and that the\nmatrices \\\\(X\\\\) and \\\\(Y\\\\) are of full column rank \\\\(p\\\\) and \\\\(q\\\\)\nrespectively. In the following, CCA is presented as a problem solved through\nan iterative algorithm. The first stage of CCA consists of finding two vectors\n\\\\(a =(a_1,...,a_p)^t\\\\) and \\\\(b =(b_1,...,b_q)^t\\\\) that maximize the\ncorrelation between the linear combinations \\\\(U\\\\) and \\\\(V\\\\) defined as\n\n\\\\[ \\begin{aligned} U=Xa&=&a_1 X_1 +a_2 X_2 +\\cdots a_p X_p\\\\\\ V=Yb&=&b_1 Y_1\n+b_2 Y_2 +\\cdots a_q Y_q \\end{aligned} \\\\]\n\nand assuming that the vectors \\\\(a\\\\) and \\\\(b\\\\) are normalized so that\n\\\\(\\text{var}(U) = \\text{var}(V) = 1\\\\). In other words, the problem consists\nof finding \\\\(a\\\\) and \\\\(b\\\\) such that\n\n\\\\[ \\rho_1 = \\text{cor}(U, V) = \\max_{a,b} \\text{cor} (Xa, Yb)\\quad\n\\text{subject to}\\quad \\text{var}(Xa)=\\text{var}(Yb) = 1. \\tag{9.3}\\\\]\n\nThe resulting variables \\\\(U\\\\) and \\\\(V\\\\) are called the first canonical\nvariates and \\\\(\\rho_1\\\\) is referred to as the first canonical correlation.\n\n**Note:** Higher order canonical variates and canonical correlations can be\nfound as a stepwise problem. For \\\\(s = 1,...,p\\\\), we can successively find\npositive correlations \\\\(\\rho_1 \\geq \\rho_2 \\geq ... \\geq \\rho_p\\\\) with\ncorresponding vectors \\\\((a^1, b^1), ..., (a^p, b^p)\\\\), by maximizing\n\n\\\\[ \\rho_s = \\text{cor}(U^s,V^s) = \\max_{a^s,b^s} \\text{cor} (Xa^s,Yb^s)\\quad\n\\text{subject to}\\quad \\text{var}(Xa^s) = \\text{var}(Yb^s)=1 \\tag{9.4}\\\\]\n\nunder the additional restrictions\n\n\\\\[ \\text{cor}(U^s,U^t) = \\text{cor}(V^s, V^t)=0 \\quad\\text{for}\\quad 1 \\leq t\n< s \\leq p. \\tag{9.5}\\\\]\n\nWe can think of CCA as a generalization of PCA where the variance we maximize\nis the ‘covariance’ between the two matrices (see Holmes\n([2006](16-chap.html#ref-frenchway)) for more details).\n\n### 9.6.5 Sparse canonical correlation analysis (sCCA)\n\nWhen the number of variables in each table is very large finding two very\ncorrelated vectors can be too easy and unstable: we have too many degrees of\nfreedom.\n\n[![We will see many examples of regularization and danger of overfitting in\nsec-supervised.](imgs/devil.png)](imgs/devil.png \"We will see many examples of\nregularization and danger of overfitting in sec-supervised.\")\n\nWe will see many examples of regularization and danger of overfitting in\n[Chapter 12](12-chap.html).\n\nThen it is beneficial to add a penalty maintains the number of non-zero\ncoefficients to a minimum. This approach is called sparse canonical\ncorrelation analysis (sparse CCA or sCCA), a method well-suited to both\nexploratory comparisons between samples and the identification of features\nwith interesting **co** variation. We will use an implementation from the\n**[PMA](https://cran.r-project.org/web/packages/PMA/)** package.\n\nHere we study a dataset collected by Kashyap et al. ([2013](16-chap.html#ref-\nKashyap:2013)) with two tables. One is a contingency table of bacterial\nabundances and another an abundance table of metabolites. There are 12\nsamples, so \\\\(n = 12\\\\). The metabolite table has measurements on \\\\(p =\n637\\\\) feature and the bacterial abundances had a total of $ q = 20,609$ OTUs,\nwhich we will filter down to around 200. We start by loading the data.\n\n    \n    \n    library(\"genefilter\")\n    load(\"../data/microbe.rda\")\n    metab = read.csv(\"../data/metabolites.csv\", row.names = 1) |> as.matrix()__\n\nWe first filter down to bacteria and metabolites of interest, removing (“by\nhand”) those that are zero across many samples and giving an upper threshold\nof 50 to the large values. We transform the data to weaken the heavy tails.\n\n    \n    \n    library(\"phyloseq\")\n    metab   = metab[rowSums(metab == 0) <= 3, ]\n    microbe = prune_taxa(taxa_sums(microbe) > 4, microbe)\n    microbe = filter_taxa(microbe, filterfun(kOverA(3, 2)), TRUE)\n    metab   = log(1 + metab, base = 10)\n    X       = log(1 + as.matrix(otu_table(microbe)), base = 10)__\n\nA second step in our preliminary analysis is to look if there is any\nassociation between the two matrices using the `RV.test` from the\n**[ade4](https://cran.r-project.org/web/packages/ade4/)** package:\n\n    \n    \n    colnames(metab) = colnames(X)\n    pca1 = dudi.pca(t(metab), scal = TRUE, scann = FALSE)\n    pca2 = dudi.pca(t(X), scal = TRUE, scann = FALSE)\n    rv1 = RV.rtest(pca1$tab, pca2$tab, 999)\n    rv1 __\n    \n    \n    Monte-Carlo test\n    Call: RV.rtest(df1 = pca1$tab, df2 = pca2$tab, nrepet = 999)\n    \n    Observation: 0.8400429 \n    \n    Based on 999 replicates\n    Simulated p-value: 0.002 \n    Alternative hypothesis: greater \n    \n        Std.Obs Expectation    Variance \n    6.231661953 0.314166070 0.007121318 \n\nWe can now apply sparse CCA. This method compares sets of features across\nhigh-dimensional data tables, where there may be more measured features than\nsamples. In the process, it chooses a subset of available features that\ncapture the most covariance – these are the features that reflect signals\npresent across multiple tables. We then apply PCA to this selected subset of\nfeatures. In this sense, we use sparse CCA as a screening procedure, rather\nthan as an ordination method.\n\nThe implementation is below. The parameters `penaltyx` and `penaltyz` are\nsparsity penalties. Smaller values of `penaltyx` will result in fewer selected\nmicrobes, similarly `penaltyz` modulates the number of selected metabolites.\nWe tune them manually to facilitate subsequent interpretation – we generally\nprefer more sparsity than the default parameters would provide.\n\n    \n    \n    library(\"PMA\")\n    ccaRes = CCA(t(X), t(metab), penaltyx = 0.15, penaltyz = 0.15, \n                 typex = \"standard\", typez = \"standard\")__\n    \n    \n    123456789\n    \n    \n    ccaRes __\n    \n    \n    Call: CCA(x = t(X), z = t(metab), typex = \"standard\", typez = \"standard\", \n        penaltyx = 0.15, penaltyz = 0.15)\n    \n    \n    Num non-zeros u's:  5 \n    Num non-zeros v's:  16 \n    Type of x:  standard \n    Type of z:  standard \n    Penalty for x: L1 bound is  0.15 \n    Penalty for z: L1 bound is  0.15 \n    Cor(Xu,Zv):  0.9904707\n\nWith these parameters, 5 bacteria and 16 metabolites were selected based on\ntheir ability to explain covariation between tables. Further, these features\nresult in a correlation of 0.99 between the two tables. We interpret this to\nmean that the microbial and metabolomic data reflect similar underlying\nsignals, and that these signals can be approximated well by the selected\nfeatures. Be wary of the correlation value, however, since the scores are far\nfrom the usual bivariate normal cloud. Further, note that it is possible that\nother subsets of features could explain the data just as well – sparse CCA has\nminimized redundancy across features, but makes no guarantee that these are\nthe “true” features in any sense.\n\nNonetheless, we can still use these 21 features to compress information from\nthe two tables without much loss. To relate the recovered metabolites and OTUs\nto characteristics of the samples on which they were measured, we use them as\ninput to an ordinary PCA. We have omitted the code we used to generate Figure\n9.32, we refer the reader to the online material accompanying the book or the\nworkflow published in Callahan et al. ([2016](16-chap.html#ref-\nCallahan2016Bioc)).\n\nFigure 9.32 displays the PCA _triplot_ , where we show different types of\nsamples and the multidomain features (Metabolites and OTUs). This allows\ncomparison across the measured samples – triangles for knockout and circles\nfor wild type –and characterizes the influence the different features –\ndiamonds with text labels. For example, we see that the main variation in the\ndata is across PD and ST samples, which correspond to the different diets.\nFurther, large values of 15 of the features are associated with ST status,\nwhile small values for 5 of them indicate PD status.\n\n[![](09-chap_files/figure-html/fig-\nmultitableinterpretpca-1.png)](09-chap_files/figure-html/fig-\nmultitableinterpretpca-1.png \"Figure 9.32: A PCA triplot produced from the CCA\nselected features from muliple data types \\(metabolites and OTUs\\).\")\n\nFigure 9.32: A PCA triplot produced from the CCA selected features from\nmuliple data types (metabolites and OTUs).\n\nThe advantage of the sparse CCA screening is now clear – we can display most\nof the variation across samples using a relatively simple plot, and can avoid\nplotting the hundreds of additional points that would be needed to display all\nof the features.\n\n### 9.6.6 Canonical (or constrained) correspondence analysis (CCpnA)\n\n[![Notational overload for CCA: Originally invented by Braak \\(1985\\) and\ncalled Canonical Correspondence analysis, we will call this method Constrained\nCorrespondence Analysis and abbreviate it CCpnA to avoid confusion with\nCanonical Correlation Analysis \\(CCA\\). However several R packages, such as\nade4 and vegan use the name cca for their correspondence analyses\nfunction.](imgs/devil.png)](imgs/devil.png \"Notational overload for CCA:\nOriginally invented by @terBraak:1985 and called Canonical Correspondence\nanalysis, we will call this method Constrained Correspondence Analysis and\nabbreviate it CCpnA to avoid confusion with Canonical Correlation Analysis\n\\(CCA\\). However several R packages, such as ade4 and vegan use the name cca\nfor their correspondence analyses function.\")\n\n**Notational overload for CCA** : Originally invented by Braak\n([1985](16-chap.html#ref-terBraak:1985)) and called Canonical Correspondence\nanalysis, we will call this method Constrained Correspondence Analysis and\nabbreviate it CCpnA to avoid confusion with Canonical Correlation Analysis\n(CCA). However several R packages, such as\n**[ade4](https://cran.r-project.org/web/packages/ade4/)** and\n**[vegan](https://cran.r-project.org/web/packages/vegan/)** use the name `cca`\nfor their correspondence analyses function.\n\nThe term constrained correspondence analysis translates the fact that this\nmethod is similar to a constrained regression. The method attempts to force\nthe latent variables to be correlated with the environmental variables\nprovided as `explanatory’.\n\nCCpnA creates biplots where the positions of samples are determined by\nsimilarity in both species signatures and environmental characteristics. In\ncontrast, principal components analysis or correspondence analysis only look\nat species signatures. More formally, it ensures that the resulting CCpnA\ndirections lie in the span of the environmental variables. For thorough\nexplanations see Braak ([1985](16-chap.html#ref-terBraak:1985); [Greenacre\n2007](16-chap.html#ref-greenacre2007)).\n\nThis method can be run using the function `ordinate` in\n**[phyloseq](https://bioconductor.org/packages/phyloseq/)**. In order to use\nthe covariates from the sample data, we provide an extra argument, specifying\nwhich of the features to consider.\n\nHere, we take the data we denoised using\n**[dada2](https://bioconductor.org/packages/dada2/)** in [Chapter\n4](04-chap.html). We will see more details about creating the _phyloseq_\nobject in [Chapter 10](10-chap.html). For the time being, we use the\n`otu_table` component containing a contingency table of counts for different\ntaxa. We would like to compute the constrained correspondence analyses that\nexplain the taxa abundances by the age and family relationship (both variables\nare contained in the `sample_data` slot of the `ps1` object).\n\nWe would like to make two dimensional plots showing only using the four most\nabundant taxa (making the biplot easier to read):\n\n    \n    \n    ps1=readRDS(\"../data/ps1.rds\")\n    ps1p=filter_taxa(ps1, function(x) sum(x) > 0, TRUE)\n    psCCpnA = ordinate(ps1p, \"CCA\",\n                     formula = ps1p ~ ageBin + family_relationship)__\n\nTo access the positions for the biplot, we can use the `scores` function in\nthe **[vegan](https://cran.r-project.org/web/packages/vegan/)**. Further, to\nfacilitate figure annotation, we also join the site scores with the\nenvironmental data in the `sample_data` slot. Of the 23 total taxonomic\norders, we only explicitly annotate the four most abundant – this makes the\nbiplot easier to read.\n\n    \n    \n    evalProp = 100 * psCCpnA$CCA$eig[1:2] / sum(psCCpnA$CA$eig)\n    ggplot() +\n     geom_point(data = sites,aes(x =CCA2, y =CCA1),shape =2,alpha=0.5) +\n     geom_point(data = species,aes(x =CCA2,y =CCA1,col = Order),size=1)+\n     geom_text_repel(data = dplyr::filter(species, CCA2 < (-2)),\n                       aes(x = CCA2, y = CCA1, label = otu_id),\n                       size = 2, segment.size = 0.1) +\n     facet_grid(. ~ ageBin) +\n     guides(col = guide_legend(override.aes = list(size = 2))) +\n     labs(x = sprintf(\"Axis2 [%s%% variance]\", round(evalProp[2])),\n          y = sprintf(\"Axis1 [%s%% variance]\", round(evalProp[1]))) +\n     scale_color_brewer(palette = \"Set1\") + theme(legend.position=\"bottom\")__\n\n[![](09-chap_files/figure-html/fig-ccpnaplotage-1.png)](09-chap_files/figure-\nhtml/fig-ccpnaplotage-1.png \"Figure 9.33: The mouse and taxa scores generated\nby CCpnA. The sites \\(mice samples\\) are triangles; species are circles,\nrespectively. The separate panels indicate different age groups.\")\n\nFigure 9.33: The mouse and taxa scores generated by CCpnA. The sites (mice\nsamples) are triangles; species are circles, respectively. The separate panels\nindicate different age groups.\n\n__\n\nQuestion 9.19\n\nLook up the extra code for creating the `tax` and `species` objects in the\nonline resources accompanying the book. Then make the analogue of Figure 9.33\nbut using litter as the faceting variable.\n\n__\n\nSolution\n\n__\n\n[![](09-chap_files/figure-html/fig-\nccpnaplotlitter-1.png)](09-chap_files/figure-html/fig-ccpnaplotlitter-1.png\n\"Figure 9.34: The analogue to Figure fig-ccpnaplotage, faceting by litter\nmembership rather than age bin.\")\n\nFigure 9.34: The analogue to Figure 9.33, faceting by litter membership rather\nthan age bin.\n\nFigures 9.33 and 9.34 show the plots of these annotated scores, splitting\nsites by their age bin and litter membership, respectively. Note that to keep\nthe appropriate aspect ratio in the presence of faceting, we have taken the\nvertical axis as our first canonical component. We have labeled individual\nbacteria that are outliers along the second CCpnA direction.\n\nEvidently, the first CCpnA direction distinguishes between mice in the two\nmain age bins. Circles on the left and right of the biplot represent bacteria\nthat are characteristic of younger and older mice, respectively. The second\nCCpnA direction splits off the few mice in the oldest age group; it also\npartially distinguishes between the two litters. These samples low in the\nsecond CCpnA direction have more of the outlier bacteria than the others.\n\nThis CCpnA analysis supports the conclusion that the main difference between\nthe microbiome communities of the different mice lies along the age axis.\nHowever, in situations where the influence of environmental variables is not\nso strong, CCA can have more power in detecting such associations. In general,\nit can be applied whenever it is desirable to incorporate supplemental data,\nbut in a way that (1) is less aggressive than supervised methods, and (2) can\nuse several environmental variables at once.\n\n## 9.7 Summary of this chapter\n\n**Heterogeneous data** A mixture of many continuous and a few categorical\nvariables can be handled by adding the categorical variables as supplementary\ninformation to the PCA. This is done by projecting the mean of all points in a\ngroup onto the map.\n\n**Using distances** Relations between data objects can often be summarized as\ninterpoint distances (whether distances between trees, images, graphs, or\nother complex objects).\n\n**Ordination** A useful representation of these distances is available through\na method similar to PCA called multidimensional scaling (MDS), otherwise known\nas PCoA (principal coordinate analysis). It can be helpful to think of the\noutcome of these analyses as uncovering latent variable. In the case of\nclustering the latent variables are categorical, in ordination they are latent\nvariables like time or environmental gradients like distance to the water.\nThis is why these methods are often called ordination.\n\n**Robust versions** can be used when interpoint distances are wildly\ndifferent. NMDS (nonmetric multidimensional scaling) aims to produce\ncoordinates such that the order of the interpoint distances is respected as\nclosely as possible.\n\n**Correspondence analysis** : a method for computing low dimensional\nprojections that explain dependencies in categorical data. It decomposes\nchisquare distance in much the same way that PCA decomposes variance.\nCorrespondence analysis is usually the best way to follow up on a significant\nchisquare test. Once we have ascertained there are significant dependencies\nbetween different levels of categories, we can map them and interpret\nproximities on this map using plots and biplots.\n\n**Permutation test for distances** Given two sets of distances between the\nsame points, we can measure whether they are related using the Mantel\npermutation test.\n\n**Generalizations of variance and covariance** When dealing with more than one\nmatrix of measurements on the same data, we can generalize the notion of\ncovariance and correlations to vectorial measurements of co-inertia.\n\n**Canonical correlation** is a method for finding a few linear combinations of\nvariables from each table that are as correlated as possible. When using this\nmethod on matrices with large numbers of variables, we use a regularized\nversion with an L1 penalty that reduces the number of non-zero coefficients.\n\n## 9.8 Further reading\n\nInterpretation of PCoA maps and nonlinear embeddings can also be enhanced the\nway we did for PCA using generalizations of the supplementary point method,\nsee Trosset and Priebe ([2008](16-chap.html#ref-Trosset2008)) or Bengio et al.\n([2004](16-chap.html#ref-Bengio2004)). We saw in [Chapter 7](07-chap.html) how\nwe can project one categorical variable onto a PCA. The correspondence\nanalysis framework actually allows us to mix several categorical variables in\nwith any number of continuous variables. This is done through an extension\ncalled multiple correspondence analysis (MCA) whereby we can do the same\nanalysis on a large number of binary categorical variables and obtain useful\nmaps. The trick here will be to turn the continuous variables into categorical\nvariables first. For extensive examples using R see for instance the book by\nPagès ([2016](16-chap.html#ref-Pages:2016)).\n\nA simple extension to PCA that allows for **nonlinear** principal curve\nestimates instead of principal directions defined by eigenvectors was proposed\nin Hastie and Stuetzle ([1989](16-chap.html#ref-Hastie1989)) and is available\nin the package\n**[princurve](https://cran.r-project.org/web/packages/princurve/)**.\n\nFinding curved subspaces containing a high density data for dimensions higher\nthan \\\\(1\\\\) is now called manifold embedding and can be done through\nLaplacian eigenmaps ([Belkin and Niyogi 2003](16-chap.html#ref-Belkin2003)),\nlocal linear embedding as in Roweis and Saul ([2000](16-chap.html#ref-\nRoweis2000)) or using the isomap method ([Tenenbaum, De Silva, and Langford\n2000](16-chap.html#ref-Tenenbaum2000)). For textbooks covering nonlinear\nunsupervised learning methods see Hastie, Tibshirani, and Friedman ([2008,\nchap. 14](16-chap.html#ref-HastieTibshiraniFriedman)) or Izenman\n([2008](16-chap.html#ref-Izenman2008)).\n\nA review of many multitable correlation coefficients, and analysis of\napplications can be found in Josse and Holmes ([2016](16-chap.html#ref-\nJosse:2016)).\n\n## 9.9 Exercises\n\n__\n\nExercise 9.1\n\nWe are going to take another look at the Phylochip data, replacing the\noriginal expression values by presence/absence. We threshold the data to\nretain only those that have a value of at least 8.633 in at least 8 samples7.\n\n    \n    \n    ibd.pres = ifelse(assayIBD[, 1:28] > 8.633, 1, 0)__\n\nPerform a correspondence analysis on these binary data and compare the plot\nyou obtain to what we saw in Figure 9.15.\n\n7 These values were chosen to give about retain about 3,000 taxa, similar to\nour previous choice of threshold.\n\n__\n\nSolution\n\n__\n\nSee Figure 9.35.\n\n    \n    \n    IBDca = dudi.coa(ibd.pres, scannf = FALSE, nf = 4)\n    fviz_eig(IBDca, geom = \"bar\", bar_width = 0.7) +\n        ylab(\"Percentage of chisquare\") + ggtitle(\"\")\n    fviz(IBDca, element = \"col\", axes = c(1, 2), geom = \"point\",\n         habillage = day, palette = \"Dark2\", addEllipses = TRUE, color = day,\n         ellipse.type = \"convex\", alpha = 1, col.row.sup =  \"blue\",\n         select = list(name = NULL, cos2 = NULL, contrib = NULL),\n         repel = TRUE)__\n\n[![](09-chap_files/figure-html/fig-Threesetscoa-1.png)](09-chap_files/figure-\nhtml/fig-Threesetscoa-1.png \"Figure 9.35 \\(a\\): \\\\text{}\")\n\n(a) \\\\(\\text{}\\\\)\n\n[![](09-chap_files/figure-html/fig-Threesetscoa-2.png)](09-chap_files/figure-\nhtml/fig-Threesetscoa-2.png \"Figure 9.35 \\(b\\): \\\\text{}\")\n\n(b) \\\\(\\text{}\\\\)\n\nFigure 9.35: Correspondence analysis on binary data.\n\n__\n\nExercise 9.2\n\nCorrespondence Analysis on color association tables:  \nHere is an example of data collected by looking at the number of Google hits\nresulting from queries of pairs of words. The numbers in Table 9.4 are to be\nmultiplied by 1000. For instance, the combination of the words “quiet” and\n“blue” returned 2,150,000 hits.\n\nTable 9.4: Contingency table of co-occurring terms from search engine results.\n\n| black | blue | green | grey | orange | purple | white  \n---|---|---|---|---|---|---|---  \nquiet | 2770 | 2150 | 2140 | 875 | 1220 | 821 | 2510  \nangry | 2970 | 1530 | 1740 | 752 | 1040 | 710 | 1730  \nclever | 1650 | 1270 | 1320 | 495 | 693 | 416 | 1420  \ndepressed | 1480 | 957 | 983 | 147 | 330 | 102 | 1270  \nhappy | 19300 | 8310 | 8730 | 1920 | 4220 | 2610 | 9150  \nlively | 1840 | 1250 | 1350 | 659 | 621 | 488 | 1480  \nperplexed | 110 | 71 | 80 | 19 | 23 | 15 | 109  \nvirtuous | 179 | 80 | 102 | 20 | 25 | 17 | 165  \n  \nPerform a correspondence analysis of these data. What do you notice when you\nlook at the two-dimensional biplot?\n\n__\n\nSolution\n\n__\n\nSee Figure 9.36. The code is not rendered here, but is shown in the document’s\nsource file.\n\n[![](09-chap_files/figure-html/fig-ColorBiplot-1-1.png)](09-chap_files/figure-\nhtml/fig-ColorBiplot-1-1.png \"Figure 9.36: Correspondence Analysis allows for\na symmetrical graphical representation of two categorical variables, in this\ncase colors and emotions for a contingency table of co-occurrences such as\nTable tbl-colors.\")\n\nFigure 9.36: Correspondence Analysis allows for a symmetrical graphical\nrepresentation of two categorical variables, in this case colors and emotions\nfor a contingency table of co-occurrences such as Table 9.4.\n\n![](imgs/PlatoTableImage.png)\n\n__\n\nExercise 9.3\n\nThe dates Plato wrote his various books are not known. We take the sentence\nendings and use those pattern frequencies as the data.\n\n    \n    \n    platof = read.table(\"../data/platof.txt\", header = TRUE)\n    platof[1:4, ]__\n    \n    \n          Rep Laws Crit Phil Pol Soph Tim\n    uuuuu  42   91    5   24  13   26  18\n    -uuuu  60  144    3   27  19   33  30\n    u-uuu  64   72    3   20  24   31  46\n    uu-uu  72   98    2   25  20   24  14\n    \n    \n    resPlato = dudi.coa(platof, scannf = FALSE, nf = 2)\n    fviz_ca_biplot(resPlato, axes=c(2, 1)) + ggtitle(\"\")\n    fviz_eig(resPlato, geom = \"bar\", width = 0.6) + ggtitle(\"\")__\n\nFigure 9.37: Biplot of Plato’s sentence endings.\n\n  1. From the biplot in Figure 9.37 can you guess at the chronological order of Plato’s works?  \nHint: the first (earliest) is known to be _Republica_. The last (latest) is\nknown to be _Laws_.\n\n  2. Which sentence ending did Plato use more frequently early in his life?\n\n  3. What percentage of the inertia (\\\\(\\chi^2\\\\)-distance) is explained by the map in Figure 9.37?\n\n[![](09-chap_files/figure-html/fig-platoca-1.png)](09-chap_files/figure-\nhtml/fig-platoca-1.png \"Figure 9.37 \\(a\\): \")\n\n(a)\n\n[![](09-chap_files/figure-html/fig-platoca-2.png)](09-chap_files/figure-\nhtml/fig-platoca-2.png \"Figure 9.37 \\(b\\): \")\n\n(b)\n\n__\n\nSolution\n\n__\n\nTo compute the percentage of inertia explained by the first two axes we take\nthe cumulative sum of the eigenvalues at the value 2:\n\n    \n    \n    names(resPlato)__\n    \n    \n     [1] \"tab\"  \"cw\"   \"lw\"   \"eig\"  \"rank\" \"nf\"   \"c1\"   \"li\"   \"co\"   \"l1\"  \n    [11] \"call\" \"N\"   \n    \n    \n    sum(resPlato$eig)__\n    \n    \n    [1] 0.132618\n    \n    \n    percentageInertia=round(100*cumsum(resPlato$eig)/sum(resPlato$eig))\n    percentageInertia __\n    \n    \n    [1]  69  85  92  96  98 100\n    \n    \n    percentageInertia[2]__\n    \n    \n    [1] 85\n\n__\n\nExercise 9.4\n\nWe are going to look at two datasets, one is a perturbed version of the other\nand they both present gradients as often seen in ecological data. Read in the\ntwo species count matrices `lakelike` and `lakelikeh`, which are stored as the\nobject `lakes.RData`. Compare the output of correspondence analysis and\nprincipal component analysis on each of the two data sets; restrict yourself\ntwo dimensions. In the plots and the eigenvalues, what do you notice?\n\n__\n\nSolution\n\n__\n\n    \n    \n    load(\"../data/lakes.RData\")\n    lakelike[ 1:3, 1:8]__\n    \n    \n         plant1 plant2 plant3 plant4 plant5 plant6 plant7 plant8\n    loc1      6      4      0      3      0      0      0      0\n    loc2      4      5      5      3      4      2      0      0\n    loc3      3      4      7      4      5      2      1      1\n    \n    \n    lakelikeh[1:3, 1:8]__\n    \n    \n         plant1 plant2 plant3 plant4 plant5 plant6 plant7 plant8\n    loc1      6      4      0      3      0      0      0      0\n    loc2      4      5      5      3      4      2      0      0\n    loc3      3      4      7      4      5      2      1      1\n    \n    \n    e_coa  = dudi.coa(lakelike,  scannf = FALSE, nf = 2)\n    e_pca  = dudi.pca(lakelike,  scannf = FALSE, nf = 2)\n    eh_coa = dudi.coa(lakelikeh, scannf = FALSE, nf = 2)\n    eh_pca = dudi.pca(lakelikeh, scannf = FALSE, nf = 2)__\n\nComparison (output not shown):\n\n    \n    \n    scatter(e_pca)__\n    \n    \n     scatter(e_coa)__\n    \n    \n     s.label(e_pca$li)__\n    \n    \n     s.label(e_coa$li)__\n    \n    \n     s.label(eh_pca$co)__\n    \n    \n     s.label(eh_pca$li)__\n    \n    \n     s.label(eh_coa$li)__\n    \n    \n     s.label(eh_coa$co)__\n\n__\n\nExercise 9.5\n\nWe analyzed the normalized Moignard data in Section 9.5.1. Now redo the\nanalysis with the _raw_ data (in file _nbt.3154-S3-raw.csv_) and compare the\noutput with that obtained using the normalized values.\n\n__\n\nSolution\n\n__\n\n    \n    \n    moignard_raw = as.matrix(read.csv(\"../data/nbt.3154-S3-raw.csv\", row.names = 1))\n    dist2r.euclid = dist(moignard_raw)\n    dist1r.l1     = dist(moignard_raw, \"manhattan\")\n    cells1.cmds = cmdscale(dist1r.l1,     k = 20, eig = TRUE)\n    cells2.cmds = cmdscale(dist2r.euclid, k = 20, eig = TRUE)\n    sum(cells1.cmds$eig[1:2]) / sum(cells1.cmds$eig)__\n    \n    \n    [1] 0.776075\n    \n    \n    sum(cells2.cmds$eig[1:2]) / sum(cells2.cmds$eig)__\n    \n    \n    [1] 0.6297133\n\n__\n\nExercise 9.6\n\nWe are going to explore the use of kernel methods.\n\n  1. Compute kernelized distances using the **[kernlab](https://cran.r-project.org/web/packages/kernlab/)** for the Moignard data using various values for the sigma tuning parameter in the definition of the kernels. Then perform MDS on these kernelized distances. What difference is there in variability explained by the first four components of kernel multidimensional scaling?\n\n  2. Make interactive three dimensional representations of the components: is there a projection where you see a branch for the purple points?\n\n__\n\nSolution\n\n__\n\n  1. kernelized distances\n\n    \n    \n    library(\"kernlab\")\n    laplacedot1 = laplacedot(sigma = 1/3934)\n    rbfdot1     = rbfdot(sigma = (1/3934)^2 )\n    Klaplace_cellsn   = kernelMatrix(laplacedot1, blom)\n    KGauss_cellsn     = kernelMatrix(rbfdot1, blom)\n    Klaplace_rawcells = kernelMatrix(laplacedot1, moignard_raw)\n    KGauss_rawcells   = kernelMatrix(rbfdot1, moignard_raw)__\n\nUse kernelized distances to protect against outliers and allows discovery of\nnon-linear components.\n\n    \n    \n    dist1kr = 1 - Klaplace_rawcells\n    dist2kr = 1 - KGauss_rawcells\n    dist1kn = 1 - Klaplace_cellsn\n    dist2kn = 1 - KGauss_cellsn\n    \n    cells1.kcmds = cmdscale(dist1kr, k = 20, eig = TRUE) \n    cells2.kcmds = cmdscale(dist2kr, k = 20, eig = TRUE) \n    \n    percentage = function(x, n = 4) round(100 * sum(x[seq_len(n)]) / sum(x[x>0]))\n    kperc1 = percentage(cells1.kcmds$eig)\n    kperc2 = percentage(cells2.kcmds$eig)\n    \n    cellsn1.kcmds = cmdscale(dist1kn, k = 20, eig = TRUE) \n    cellsn2.kcmds = cmdscale(dist2kn, k = 20, eig = TRUE)__\n\n  2. using a 3d scatterplot interactively:\n\n    \n    \n    colc = rowData(Moignard)$cellcol\n    library(\"scatterplot3d\")\n    scatterplot3d(cellsn2.kcmds$points[, 1:3], color=colc, pch = 20,\n       xlab = \"Axis k1\", ylab = \"Axis k2\", zlab = \"Axis k3\", angle=15)\n    scatterplot3d(cellsn2.kcmds$points[, 1:3], color=colc, pch = 20,\n       xlab = \"Axis k1\", ylab = \"Axis k2\", zlab = \"Axis k3\", angle = -70)__\n\n[![](09-chap_files/figure-html/fig-\nKernelMDSplots-1.png)](09-chap_files/figure-html/fig-KernelMDSplots-1.png\n\"Figure 9.38 \\(a\\): \\\\text{}\")\n\n(a) \\\\(\\text{}\\\\)\n\n[![](09-chap_files/figure-html/fig-\nKernelMDSplots-2.png)](09-chap_files/figure-html/fig-KernelMDSplots-2.png\n\"Figure 9.38 \\(b\\): \\\\text{}\")\n\n(b) \\\\(\\text{}\\\\)\n\nFigure 9.38: Kernel multidimensional scaling.\n\n__\n\nExercise 9.7\n\n**Higher resolution study of cell data.**  \nTake the original expression data `blom` we generated in Section 9.5.1. Map\nthe intensity of expression of each of the top 10 most variable genes onto the\n3d plot made with the diffusion mapping. Which dimension, or which one of the\nprincipal coordinates (1,2,3,4) can be seen as the one that clusters the\n**4SG** (red) points the most?\n\n__\n\nSolution\n\n__\n\n    \n    \n    library(\"rgl\")\n    plot3d(cellsn2.kcmds$points[, 1:3], col = colc, size = 3,\n           xlab = \"Axis1\", ylab = \"Axis2\", zlab = \"Axis3\")\n    plot3d(cellsn2.kcmds$points[, c(1,2,4)], col = colc, size = 3,\n           xlab = \"Axis1\", ylab = \"Axis2\", zlab = \"Axis4\")\n    # Using an L1 distance instead.\n    plot3d(cellsn1.kcmds$points[, 1:3], col = colc, size = 3,\n           xlab = \"Axis1\", ylab = \"Axis2\", zlab = \"Axis3\")\n    plot3d(cellsn1.kcmds$points[, c(1,2,4)], col = colc, size = 3,\n           xlab = \"Axis1\", ylab = \"Axis2\", zlab = \"Axis4\")__\n\nAn implementation in the package\n**[LPCM](https://cran.r-project.org/web/packages/LPCM/)** provides the\nfunction `lpc`, which estimates principal curves. Here we constrain ourselves\nto three dimensions chosen from the output of the diffusion map and create\nsmoothed curves.\n\n    \n    \n    library(\"LPCM\")\n    library(\"diffusionMap\")\n    dmap1 = diffuse(dist1n.l1, neigen = 10)__\n    \n    \n    Performing eigendecomposition\n    Computing Diffusion Coordinates\n    Elapsed time: 5.014 seconds\n    \n    \n    combs = combn(4, 3)\n    lpcplots = apply(combs, 2, function(j) lpc(dmap1$X[, j], scale = FALSE))__\n\nTo get a feel for what the smoothed data are showing us, we take a look at the\ninteractive graphics using the function `plot3d` from them\n**[rgl](https://cran.r-project.org/web/packages/rgl/)** package.\n\n    \n    \n    library(\"rgl\")\n    for (i in seq_along(lpcplots))\n      plot(lpcplots[[i]], type = \"l\", lwd = 3,\n      xlab = paste(\"Axis\", combs[1, i]),\n      ylab = paste(\"Axis\", combs[2, i]),\n      zlab = paste(\"Axis\", combs[3, i]))__\n\nOne way of plotting both the smoothed line and the data points is to add the\nline using the `plot3d` function.\n\n    \n    \n    outlpce134 = lpc(dmap1$X[,c(1,3,4)], scale=FALSE, h=0.5)\n    plot3d(dmap1$X[,c(1,3,4)], col=colc, pch=20, \n           xlab=\"Axis1\", ylab=\"Axis3\", zlab=\"Axis4\")\n    plot3d(outlpce134$LPC, type=\"l\", lwd=7, add=TRUE)\n    \n    outlpce134 = lpc(dmap1$X[,c(1,3,4)], scale=FALSE, h=0.7)\n    plot3d(outlpce134$LPC, type=\"l\", lwd=7,\n           xlab=\"Axis1\", ylab=\"Axis3\", zlab=\"Axis4\")\n    plot3d(dmap1$X[,c(1,3,4)], col=colc, \n           xlab=\"\", ylab=\"\", zlab=\"\", add=TRUE)__\n\n[![](imgs/TripleArm.png)](imgs/TripleArm.png \"Figure 9.39: Diffusion map\nprojection for Axes 1, 3 and 4. The lower figure shows the smoothed path\nfollowed by the cells in their development.\")\n\nFigure 9.39: Diffusion map projection for Axes 1, 3 and 4. The lower figure\nshows the smoothed path followed by the cells in their development.\n\n[![](imgs/SmoothLineP134h7.png)](imgs/SmoothLineP134h7.png \"Figure 9.40:\nDiffusion map projection for Axes 1, 3 and 4. The lower figure shows the\nsmoothed path followed by the cells in their development.\")\n\nFigure 9.40: Diffusion map projection for Axes 1, 3 and 4. The lower figure\nshows the smoothed path followed by the cells in their development.\n\n__\n\nExercise 9.8\n\nHere we explore more refined distances and diffusion maps that can show cell\ndevelopment trajectories as in Figure 9.41.\n\nThe diffusion map method restricts the estimation of distances to local\npoints, thus further pursuing the idea that often only local distances should\nbe represented precisely and as points become further apart they are not being\nmeasured with the same ‘reference’. This method also uses the distances as\ninput but then creates local probabilistic transitions as indicators of\nsimilarity, these are combined into an affinity matrix for which the\neigenvalues and eigenvectors are also computed much like in standard MDS.\n\nCompare the output of the `diffuse` function from the\n**[diffusionMap](https://cran.r-project.org/web/packages/diffusionMap/)**\npackage on both the l1 and l2 distances computed between the cells available\nin the `dist2n.euclid` and `dist1n.l1` objects from Section 9.5.1.\n\n[![](imgs/dmap134.png)](imgs/dmap134.png \"Figure 9.41: Ouput from a three-\ndimensional diffusion map projection.\")\n\nFigure 9.41: Ouput from a three-dimensional diffusion map projection.\n\n__\n\nSolution\n\n__\n\n    \n    \n    library(\"diffusionMap\")\n    dmap2 = diffuse(dist2n.euclid, neigen = 11)__\n    \n    \n    Performing eigendecomposition\n    Computing Diffusion Coordinates\n    Elapsed time: 7.243 seconds\n    \n    \n    dmap1 = diffuse(dist1n.l1, neigen = 11)__\n    \n    \n    Performing eigendecomposition\n    Computing Diffusion Coordinates\n    Elapsed time: 4.849 seconds\n    \n    \n    plot(dmap2)__\n\nNotice that the vanilla plot for a _dmap_ object does not allow the use of\ncolors. As this essential to our understanding of cell development, we add the\ncolors by hand. Of course, here we use static 3d plots but these should\nsupplemented by the plot3d examples we give in the code.\n\nWe use a tailored wrapper function `scp3d`, so that we can easily insert\nrelevant parameters:\n\n    \n    \n    library(\"scatterplot3d\")\n    scp3d = function(axestop = 1:3, dmapRes = dmap1, color = colc,\n               anglea = 20, pch = 20)\n    scatterplot3d(dmapRes$X[, axestop], color = colc,\n        xlab = paste(\"Axis\",axestop[1]), ylab = paste(\"Axis\", axestop[2]),\n        zlab = paste(\"Axis\",axestop[3]), pch = pch, angle = anglea)__\n    \n    \n    scp3d()\n    scp3d(anglea=310)\n    scp3d(anglea=210)\n    scp3d(anglea=150)__\n\nThe best way of visualizing the data is to make a rotatable interactive plot\nusing the **[rgl](https://cran.r-project.org/web/packages/rgl/)** package.\n\n    \n    \n    # interactive plot\n    library(\"rgl\")\n    plot3d(dmap1$X[,1:3], col=colc, size=3)\n    plot3d(dmap1$X[,2:4], col=colc, size=3)__\n\nBelkin, Mikhail, and Partha Niyogi. 2003. “Laplacian Eigenmaps for\nDimensionality Reduction and Data Representation.” _Neural Computation_ 15\n(6): 1373–96.\n\nBengio, Yoshua, Jean-François Paiement, Pascal Vincent, Olivier Delalleau,\nNicolas Le Roux, and Marie Ouimet. 2004. “Out-of-Sample Extensions for LLE,\nIsomap, MDS, Eigenmaps, and Spectral Clustering.” _Advances in Neural\nInformation Processing Systems_ 16: 177–84.\n\nBraak, Cajo ter. 1985. “Correspondence Analysis of Incidence and Abundance\nData: Properties in Terms of a Unimodal Respose.” _Biometrics_ 41 (January).\n\nBrodie, Eoin L, Todd Z DeSantis, Dominique C Joyner, Seung M Baek, Joern T\nLarsen, Gary L Andersen, Terry C Hazen, et al. 2006. “Application of a High-\nDensity Oligonucleotide Microarray Approach to Study Bacterial Population\nDynamics During Uranium Reduction and Reoxidation.” _Applied and Environmental\nMicrobiology_ 72 (9): 6288–98.\n\nCallahan, Ben J, Kris Sankaran, Julia A Fukuyama, Paul J McMurdie, and Susan P\nHolmes. 2016. “Bioconductor Workflow for Microbiome Data Analysis: From Raw\nReads to Community Analyses.” _F1000Research_ 5\\.\n\nChessel, Daniel, Anne Dufour, and Jean Thioulouse. 2004. “The ade4 Package -\ni: One-Table Methods.” _R News_ 4 (1): 5–10.\n<http://CRAN.R-project.org/doc/Rnews/>.\n\nDiaconis, Persi, Sharad Goel, and Susan Holmes. 2008. “Horseshoes in\nMultidimensional Scaling and Kernel Methods.” _Annals of Applied Statistics_\n2: 777. <https://doi.org/DOI:10.1214/08-AOAS165>.\n\nEkman, Gosta. 1954. “Dimensions of Color Vision.” _The Journal of Psychology_\n38 (2): 467–74.\n\nGoslee, Sarah C, Dean L Urban, et al. 2007. “The Ecodist Package for\nDissimilarity-Based Analysis of Ecological Data.” _Journal of Statistical\nSoftware_ 22 (7): 1–19.\n\nGreenacre, Michael J. 2007. _Correspondence Analysis in Practice_. Chapman &\nHall.\n\nGuillot, Gilles, and François Rousset. 2013. “Dismantling the Mantel Tests.”\n_Methods in Ecology and Evolution_ 4 (4): 336–44.\n\nHastie, Trevor, and Werner Stuetzle. 1989. “Principal Curves.” _Journal of the\nAmerican Statistical Association_ 84 (406): 502–16.\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2008. _The Elements of\nStatistical Learning_. 2^{\\text{nd}} ed. Springer.\n\nHolmes, Susan. 2006. “Multivariate Analysis: The French way.” In _Probability\nand Statistics: Essays in Honor of David a. Freedman_ , edited by D. Nolan and\nT. P. Speed. Vol. 56. IMS Lecture Notes–Monograph Series. Beachwood, OH: IMS.\n<http://www.imstat.org/publications/lecnotes.htm>.\n\nHolmes, Susan, Alexander V Alekseyenko, Alden Timme, Tyrrell Nelson, Pankaj\nJay Pasricha, and Alfred Spormann. 2011. “Visualization and Statistical\nComparisons of Microbial Communities Using r Packages on Phylochip Data.” In\n_Pacific Symposium on Biocomputing_ , 142–53. World Scientific.\n\nIzenman, Alan Julian. 2008. “Nonlinear Dimensionality Reduction and Manifold\nLearning.” In _Modern Multivariate Statistical Techniques: Regression,\nClassification, and Manifold Learning_ , 597–632. New York, NY: Springer New\nYork.\n\nJosse, Julie, and Susan Holmes. 2016. “Measuring Multivariate Association and\nBeyond.” _Statistics Surveys_ 10: 132–67.\n\nKashyap, Purna C, Angela Marcobal, Luke K Ursell, Samuel A Smits, Erica D\nSonnenburg, Elizabeth K Costello, Steven K Higginbottom, et al. 2013.\n“Genetically Dictated Change in Host Mucus Carbohydrate Landscape Exerts a\nDiet-Dependent Effect on the Gut Microbiota.” _PNAS_ 110 (42): 17059–64.\n\nKendall, David. 1969. “Incidence Matrices, Interval Graphs and Seriation in\nArcheology.” _Pacific Journal of Mathematics_ 28 (3): 565–70.\n\nLeek, Jeffrey T, Robert B Scharpf, Héctor Corrada Bravo, David Simcha,\nBenjamin Langmead, W Evan Johnson, Donald Geman, Keith Baggerly, and Rafael A\nIrizarry. 2010. “Tackling the Widespread and Critical Impact of Batch Effects\nin High-Throughput Data.” _Nature Reviews Genetics_ 11 (10): 733–39.\n\nMoignard, Victoria, Steven Woodhouse, Laleh Haghverdi, Andrew J Lilly, Yosuke\nTanaka, Adam C Wilkinson, Florian Buettner, et al. 2015. “Decoding the\nRegulatory Network of Early Blood Development from Single-Cell Gene Expression\nMeasurements.” _Nature Biotechnology_.\n\nNelson, Tyrell A, Susan Holmes, Alexander Alekseyenko, Masha Shenoy, Todd\nDeSantis, Cindy Wu, Gary Andersen, et al. 2010. “PhyloChip Microarray Analysis\nReveals Altered Gastrointestinal Microbial Communities in a Rat Model of\nColonic Hypersensitivity.” _Neurogastroenterology & Motility_.\n\nPagès, Jérôme. 2016. _Multiple Factor Analysis by Example Using R_. CRC Press.\n\nPerraudeau, Fanny, Davide Risso, Kelly Street, Elizabeth Purdom, and Sandrine\nDudoit. 2017. “Bioconductor Workflow for Single-Cell RNA Sequencing:\nNormalization, Dimensionality Reduction, Clustering, and Lineage Inference.”\n_F1000Research_ 6\\.\n\nPrentice, IC. 1977. “Non-Metric Ordination Methods in Ecology.” _The Journal\nof Ecology_ , 85–94.\n\nRhee, Soo-Yon, Matthew J Gonzales, Rami Kantor, Bradley J Betts, Jaideep\nRavela, and Robert W Shafer. 2003. “Human Immunodeficiency Virus Reverse\nTranscriptase and Protease Sequence Database.” _Nucleic Acids Research_ 31\n(1): 298–303.\n\nRoweis, Sam T, and Lawrence K Saul. 2000. “Nonlinear Dimensionality Reduction\nby Locally Linear Embedding.” _Science_ 290 (5500): 2323–26.\n\nTenenbaum, Joshua B, Vin De Silva, and John C Langford. 2000. “A Global\nGeometric Framework for Nonlinear Dimensionality Reduction.” _Science_ 290\n(5500): 2319–23.\n\nTrosset, Michael W, and Carey E Priebe. 2008. “The Out-of-Sample Problem for\nClassical Multidimensional Scaling.” _Computational Statistics & Data\nAnalysis_ 52 (10): 4635–42.\n\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["msmb.css"],"toc":true,"output-file":"09-chap.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}