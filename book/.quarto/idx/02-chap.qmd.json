{"title":"2.1 Goals for this chapter","markdown":{"headingText":"2.1 Goals for this chapter","containsRefs":false,"markdown":"![](imgs/StatDiagram.png)\n\nIn the previous chapter, the knowledge of both the generative model and the\nvalues of the parameters provided us with probabilities we could use for\ndecision making – for instance, whether we had really found an epitope. In\nmany real situations, neither the generative model nor the parameters are\nknown, and we will need to estimate them using the data we have collected.\nStatistical modeling works from the data _upwards_ to a model that _might_\nplausibly explain the data1. This upward-reasoning step is called statistical\n**inference**. This chapter will show us some of the distributions and\nestimation mechanisms that serve as building blocks for inference. Although\nthe examples in this chapter are all parametric (i.e., the statistical models\nonly have a small number of unknown parameters), the principles we discuss\nwill generalize.\n\n1 Even if we have found a model that perfectly explains all our current data,\nit could always be that reality is more complex. A new set of data lets us\nconclude that another model is needed, and may include the current model as a\nspecial case or approximation.\n\n[![In a statistical setting, we start with the data X and use them to estimate\nthe parameters. These estimates are denoted by Greek letters with what we call\nhats on them, as in \\\\widehat{\\\\theta}.](imgs/devil.png)](imgs/devil.png \"In a\nstatistical setting, we start with the data X and use them to estimate the\nparameters. These estimates are denoted by Greek letters with what we call\nhats on them, as in \\\\widehat{\\\\theta}.\")\n\nIn a statistical setting, we start with the data \\\\(X\\\\) and use them to\n_estimate_ the parameters. These estimates are denoted by Greek letters with\nwhat we call hats on them, as in \\\\(\\widehat{\\theta}\\\\).\n\n\nIn this chapter we will:\n\n  * See that there is a difference between two subjects that are often confused: “Probability” and “Statistics”.\n\n  * Fit data to probability distributions using histograms and other visualization tricks.\n\n  * Have a first encounter with an estimating procedure known as **maximum likelihood** through a simulation experiment.\n\n  * Make inferences from data for which we have prior information. For this we will use the Bayesian paradigm which will involve new distributions with specially tailored properties. We will use simulations and see how Bayesian estimation differs from simple application of maximum likelihood.\n\n  * Use statistical models and estimation to evaluate dependencies in binomial and multinomial distributions.\n\n  * Analyse some historically interesting genomic data assembled into tables.\n\n  * Make Markov chain models for **dependent** data.\n\n  * Do a few concrete applications counting motifs in whole genomes and manipulate special Bioconductor classes dedicated to genomic data.\n\n![](imgs/Parameters.png)\n\n**Examples of parameters** : the single parameter \\\\(\\lambda\\\\) defines a\nPoisson distribution. The letter \\\\(\\mu\\\\) is often used for the mean of the\nnormal. More generally, we use the Greek letter \\\\(\\theta\\\\) to designate a\ngeneric tuple of parameters necessary to specify a probability model. For\ninstance, in the case of the binomial distribution, \\\\(\\theta=(n,p)\\\\)\ncomprises two numbers, a positive integer and a real number between 0 and 1.\n\n#### Parameters are the key.\n\nWe saw in [Chapter 1](01-chap.html) that the knowledge of all the parameter\nvalues in the epitope example enabled us to use our probability model and test\na null hypothesis based on the data we had at hand. We will see different\napproaches to statistical modeling through some real examples and computer\nsimulations, but let’s start by making a distinction between two situations\ndepending on how much information is available.\n\n## 2.2 The difference between statistical and probabilistic models\n\nA probabilistic analysis is possible when we know a good generative model for\nthe randomness in the data, _and_ we know its parameters’ actual values.\n\n[![](imgs/ProbaDiagram.png)](imgs/ProbaDiagram.png \"Figure 2.1: The\nprobabilistic model we obtained in sec-generative. The data are represented as\nx in green. If we know the true value of \\\\theta, then we can compute the\nprobability of observing x for all possible instances of x, in particular, for\nan x that we observed.\")\n\nFigure 2.1: The probabilistic model we obtained in [Chapter 1](01-chap.html).\nThe data are represented as \\\\(x\\\\) in green. If we know the true value of\n\\\\(\\theta\\\\), then we can compute the probability of observing \\\\(x\\\\) for all\npossible instances of \\\\(x\\\\), in particular, for an \\\\(x\\\\) that we observed.\n\nIn the epitope example, knowing that false positives occur as Bernoulli(0.01)\nper position, the number of patient samples assayed and the length of the\nprotein, meant that there were _no unknown parameters_.\n\nIn such a case, we can use mathematical **deduction** to compute the\nprobability of an event as schematized in Figure 2.1. In the epitope examples,\nwe used the Poisson probability as our **null model** with the given parameter\n\\\\(\\lambda=0.5\\\\). We were able to conclude through mathematical deduction\nthat the chances of seeing a maximum value of 7 or larger was around\n\\\\(10^{-4}\\\\) and thus that in fact the observed data were highly unlikely\nunder that model (or “null hypothesis”).\n\nNow suppose that we know the number of patients and the length of the proteins\n(these are given by the experimental design) but not the distribution itself\nand the false positive rate. Once we observe data, we need to go _up_ from the\ndata to estimate both a probability model \\\\(F\\\\) (Poisson, normal, binomial)\nand eventually the missing parameter(s) for that model. This is the type of\nstatistical **inference** we will explain in this chapter.\n\n## 2.3 A simple example of statistical modeling\n\n#### Start with the data\n\nThere are two parts to the modeling procedure. First we need a reasonable\nprobability _distribution_ to model the data generation process. As we saw in\n[Chapter 1](01-chap.html), discrete count data may be modeled by simple\nprobability distributions such as binomial, multinomial or Poisson\ndistributions. The normal distribution, or bell shaped curve, is often a good\nmodel for continuous measurements. Distributions can also be more complicated\nmixtures of these elementary ones (more on this in [Chapter 4](04-chap.html)).\n\nLet’s revisit the epitope example from the previous chapter, starting without\nthe tricky outlier.\n\n    \n    \n    load(\"../data/e100.RData\")\n    e99 = e100[-which.max(e100)]__\n\n#### Goodness-of-fit : visual evaluation\n\nOur first step is to find a fit from candidate distributions; this requires\nconsulting graphical and quantitative goodness-of-fit plots. For discrete\ndata, we can plot a barplot of frequencies (for continuous data, we would look\nat the histogram) as in Figure 2.2.\n\n    \n    \n    barplot(table(e99), space = 0.8, col = \"chartreuse4\")__\n\n[![](02-chap_files/figure-html/fig-twopoisson-1.png)](02-chap_files/figure-\nhtml/fig-twopoisson-1.png \"Figure 2.2: The observed distribution of the\nepitope data without the outlier.\")\n\nFigure 2.2: The observed distribution of the epitope data without the outlier.\n\nHowever, it is hard to decide which theoretical distribution fits the data\nbest without using a comparison. One visual **goodness-of-fit** diagram is\nknown as the **rootogram** ([Cleveland 1988](16-chap.html#ref-Tukey:1988)); it\nhangs the bars with the observed counts from the theoretical red points. If\nthe counts correspond exactly to their theoretical values, the bottom of the\nboxes will align exactly with the horizontal axis.\n\n    \n    \n    library(\"vcd\")\n    gf1 = goodfit( e99, \"poisson\")\n    rootogram(gf1, xlab = \"\", rect_gp = gpar(fill = \"chartreuse4\"))__\n\n[![](02-chap_files/figure-html/fig-stat-rooto-1.png)](02-chap_files/figure-\nhtml/fig-stat-rooto-1.png \"Figure 2.3: Rootogram showing the square root of\nthe theoretical values as red dots and the square root of the observed\nfrequencies as drop down rectangles. \\(We’ll see a bit below how the goodfit\nfunction decided which \\\\lambda to use.\\)\")\n\nFigure 2.3: Rootogram showing the square root of the theoretical values as red\ndots and the square root of the observed frequencies as drop down rectangles.\n(We’ll see a bit below how the `goodfit` function decided which \\\\(\\lambda\\\\)\nto use.)\n\n__\n\nQuestion 2.1\n\nTo calibrate what such a plot looks like with a known Poisson variable, use\n`rpois` with \\\\(\\lambda\\\\) = 0.05 to generate 100 Poisson distributed numbers\nand draw their rootogram.\n\n__\n\nSolution\n\n__\n\n    \n    \n    simp = rpois(100, lambda = 0.05)\n    gf2 = goodfit(simp, \"poisson\")\n    rootogram(gf2, xlab = \"\")__\n\nWe see that the rootogram for `e99` seems to fit the Poisson model reasonably\nwell. But remember, to make this happen we removed the outlier. The Poisson is\ncompletely determined by one parameter, often called the Poisson mean\n\\\\(\\lambda\\\\). In most cases where we can guess the data follows a Poisson\ndistribution, we will need to estimate the Poisson parameter from the data.\n\n[![The parameter is called the Poisson mean because it is the mean of the\ntheoretical distribution and, as it turns out, is estimated by the sample\nmean. This overloading of the word is confusing to\neveryone.](imgs/devil.png)](imgs/devil.png \"The parameter is called the\nPoisson mean because it is the mean of the theoretical distribution and, as it\nturns out, is estimated by the sample mean. This overloading of the word is\nconfusing to everyone.\")\n\nThe parameter is called the Poisson mean because it is the mean of the\ntheoretical distribution _and_ , as it turns out, is estimated by the sample\nmean. This overloading of the word is confusing to everyone.\n\nThe most common way of estimating \\\\(\\lambda\\\\) is to choose the value\n\\\\(\\hat{\\lambda}\\\\) that makes the observed data the most likely. This is\ncalled the **maximum likelihood estimator** ([Rice 2006, chap.\n8](16-chap.html#ref-Rice:2007), Section 5), often abbreviated **MLE**. We will\nillustrate this rather paradoxical idea in the next section.\n\nAlthough we above took out the extreme observation before taking a guess at\nthe probability distribution, we are going to return to the data with it for\nthe rest of our analysis. In practice we would not know whether there is an\noutlier, and which data point(s) it is / they are. The effect of leaving it in\nis to make our estimate of the mean higher. In turns this would make it more\nlikely that we’d observe a value of 7 under the null model, resulting in a\nlarger p-value. So, if the resulting p-value is small even with the outlier\nincluded, we are assured that our analysis is up to something real. We call\nsuch a tactic being **conservative** : we err on the side of caution, of not\ndetecting something.\n\n#### Estimating the parameter of the Poisson distribution\n\nWhat value for the Poisson mean makes the data the most probable? In a first\nstep, we tally the outcomes.\n\n    \n    \n    table(e100)__\n    \n    \n    e100\n     0  1  2  7 \n    58 34  7  1 \n\nThen we are going to try out different values for the Poisson mean and see\nwhich one gives the best fit to our data. If the mean \\\\(\\lambda\\\\) of the\nPoisson distribution were 3, the counts would look something like this:\n\n    \n    \n    table(rpois(100, 3))__\n    \n    \n     0  1  2  3  4  5  6  7 \n     4 12 23 24 14 16  4  3 \n\nwhich has many more 2’s and 3’s than we see in our data. So we see that\n\\\\(\\lambda=3\\\\) is unlikely to have produced our data, as the counts do not\nmatch up so well.\n\n__\n\nQuestion 2.2\n\nRepeat this simulation with different values of \\\\(\\lambda\\\\). Can you find\none that gives counts close to the observed ones just by trial and error?\n\nSo we could try out many possible values and proceed by brute force. However,\nwe’ll do something more elegant and use a little mathematics to see which\nvalue maximizes the probability of observing our data. Let’s calculate the\nprobability of seeing the data if the value of the Poisson parameter is\n\\\\(m\\\\). Since we suppose the data derive from independent draws, this\nprobability is simply the product of individual probabilities:\n\n\\\\[\\begin{equation*} \\begin{aligned} P(58 \\times 0, 34 \\times 1, 7 \\times 2,\n\\text{one }7 \\;|\\; \\text{data are Poisson}(m)) = P(0)^{58}\\times\nP(1)^{34}\\times P(2)^{7}\\times P(7)^{1}.\\end{aligned} \\end{equation*}\\\\]\n\nFor \\\\(m=3\\\\) we can compute this2.\n\n2 Note how we here use R’s vectorization: the call to `dpois` returns four\nvalues, corresponding to the four different numbers. We then take these to the\npowers of 58, 34, 7 and 1, respectively, using the `^` operator, resulting\nagain in four values. Finally, we collapse them into one number, the product,\nwith the `prod` function.\n\n    \n    \n    prod(dpois(c(0, 1, 2, 7), lambda = 3) ^ (c(58, 34, 7, 1)))__\n    \n    \n    [1] 1.392143e-110\n\n__\n\nQuestion 2.3\n\nCompute the probability as above for \\\\(m=0,1,2\\\\). Does \\\\(m\\\\) have to be\ninteger? Try computing the probability for \\\\(m=0.4\\\\) for example.\n\n__\n\nSolution\n\n__\n\n    \n    \n    prod(dpois(c(0, 1, 2, 7), lambda = 0.4) ^ (c(58, 34, 7, 1)))__\n    \n    \n    [1] 8.5483e-46\n\nThis probability is the **likelihood function** of \\\\(\\lambda\\\\), given the\ndata, and we write it\n\nHere \\\\(L\\\\) stands for likelihood and \\\\(f(k)=e^{-\\lambda}\n\\,\\lambda^k\\,/\\,k!\\\\), the Poisson probability we saw earlier.\n\n\\\\[ L\\left(\\lambda,\\,x=(k_1,k_2,k_3,...)\\right)=\\prod_{i=1}^{100}f(k_i) \\\\]\n\nInstead of working with multiplications of a hundred small numbers, it is\nconvenient3 to take the logarithm. Since the logarithm is strictly increasing,\nif there is a point where the logarithm achieves its maximum within an\ninterval it will also be the maximum for the probability.\n\n3 That’s usually true both for pencil and paper and for computer calculations.\n\n4 Here we again use R’s vector syntax that allows us to write the computation\nwithout an explicit loop over the data points. Compared to the code above,\nhere we call `dpois` on each of the 100 data points, rather than tabulating\n`data` with the `table` function before calling `dpois` only on the distinct\nvalues. This is a simple example for alternative solutions whose results are\nequivalent, but may differ in how easy it is to read the code or how long it\ntakes to execute.\n\nLet’s start with a computational illustration. We compute the likelihood for\nmany different values of the Poisson parameter. To do this we need to write a\nsmall function that computes the probability of the data for different\nvalues4.\n\n    \n    \n    loglikelihood  =  function(lambda, data = e100) {\n      sum(log(dpois(data, lambda)))\n    }__\n\nNow we can compute the likelihood for a whole series of `lambda` values from\n0.05 to 0.95 (Figure 2.4).\n\n    \n    \n    lambdas = seq(0.05, 0.95, length = 100)\n    loglik = vapply(lambdas, loglikelihood, numeric(1))\n    plot(lambdas, loglik, type = \"l\", col = \"red\", ylab = \"\", lwd = 2,\n         xlab = expression(lambda))\n    m0 = mean(e100)\n    abline(v = m0, col = \"blue\", lwd = 2)\n    abline(h = loglikelihood(m0), col = \"purple\", lwd = 2)\n    m0 __\n    \n    \n    [1] 0.55\n\n[![](02-chap_files/figure-html/fig-poislikel-1-1.png)](02-chap_files/figure-\nhtml/fig-poislikel-1-1.png \"Figure 2.4: The red curve is the log-likelihood\nfunction. The vertical line shows the value of m \\(the mean\\) and the\nhorizontal line the log-likelihood of m. It looks like m maximizes the\nlikelihood.\")\n\nFigure 2.4: The red curve is the log-likelihood function. The vertical line\nshows the value of `m` (the mean) and the horizontal line the log-likelihood\nof `m`. It looks like `m` maximizes the likelihood.\n\n__\n\nQuestion 2.4\n\nWhat does the `vapply` function do in the above code? Hint: check its manual\npage.\n\n__\n\nSolution\n\n__\n\n`vapply` takes its first argument, the vector `lambdas` in this case, and\niteratively applies the function `loglikelihood` (its second argument) to each\nof the vector elements. As a result, it returns a vector of the results. The\nfunction also needs a third argument, `numeric(1)` in this case, that\nspecifies what type of value each individual call to `loglikelihood` is\nsupposed to return: a single number. (In general, it could happen that the\nfunction sometimes returns something else, say, a character string, or two\nnumbers; in that case it would not be possible to assemble the overall results\ninto a coherent vector, and `vapply` would complain.)\n\nIn fact there is a shortcut: the function `goodfit`.\n\n    \n    \n    gf  =  goodfit(e100, \"poisson\")\n    names(gf)__\n    \n    \n    [1] \"observed\" \"count\"    \"fitted\"   \"type\"     \"method\"   \"df\"       \"par\"     \n    \n    \n    gf$par __\n    \n    \n    $lambda\n    [1] 0.55\n\nThe output of `goodfit` is a composite object called a list. One of its\ncomponents is called `par` and contains the value(s) of the fitted\nparameter(s) for the distribution studied. In this case it’s only one number,\nthe estimate of \\\\(\\lambda\\\\).\n\n__\n\nQuestion 2.5\n\nWhat are the other components of the output from the `goodfit` function?\n\n__\n\nTask\n\nCompare the value of `m` to the value that we used previously for\n\\\\(\\lambda\\\\), 0.5. Redo the modeling that we did in [Chapter 1](01-chap.html)\nwith `m` instead of 0.5.\n\n### 2.3.1 Classical statistics for classical data\n\nHere is a formal proof of our computational finding that the sample mean\nmaximizes the (log-)likelihood.\n\n\\\\[ \\begin{align} \\log L(\\lambda, x) &= \\sum_{i=1}^{100} - \\lambda +\nk_i\\log\\lambda - \\log(k_i!) \\\\\\ &= -100\\lambda +\n\\log\\lambda\\left(\\sum_{i=1}^{100}k_i\\right) + \\text{const.} \\end{align}\n\\tag{2.1}\\\\]\n\nWe use the catch-all “const.” for terms that do not depend on \\\\(\\lambda\\\\)\n(although they do depend on \\\\(x\\\\), i.e., on the \\\\(k_i\\\\)). To find the\n\\\\(\\lambda\\\\) that maximizes this, we compute the derivative in \\\\(\\lambda\\\\)\nand set it to zero.\n\n\\\\[ \\begin{align} \\frac{d}{d\\lambda}\\log L &= -100 + \\frac{1}{\\lambda}\n\\sum_{i=1}^{100}k_i \\stackrel{?}{=}0 \\\\\\ \\lambda &= \\frac{1}{100}\n\\sum_{i=1}^{100}k_i = \\bar{k} \\end{align} \\tag{2.2}\\\\]\n\nYou have just seen the first steps of a _statistical approach_ , starting\n`from the ground up’ (from the data) to infer the model parameter(s): this is\nstatistical _estimation_ of a parameter from data. Another important component\nwill be choosing which family of distributions we use to model our data; that\npart is done by evaluating the _goodness of fit_. We will encounter this\nlater.\n\nIn the classical _statistical testing_ framework, we consider one single\nmodel, that we call the _null model_ , for the data. The null model formulates\nan “uninteresting” baseline, such as that all observations come from the same\nrandom distribution regardless of which group or treatment they are from. We\nthen test whether there is something more interesting going on by computing\nthe probability that the data are compatible with that model. Often, this is\nthe best we can do, since we do not know in sufficient detail what the\n“interesting”, non-null or alternative model should be. In other situations,\nwe have two competing models that we can compare, as we will see later.\n\n__\n\nQuestion 2.6\n\nWhat is the value of modeling with a known distribution? For instance, why is\nit interesting to know a variable has a Poisson distribution ?\n\n__\n\nSolution\n\n__\n\nModels are concise but expressive representations of the data generating\nprocess. For the Poisson for instance, knowing one number allows us to know\neverything about the distribution, including, as we saw earlier, the\nprobabilities of extreme or rare events.\n\nAnother useful direction is **regression**. We may be interested in knowing\nhow our count-based response variable (e.g., the result of counting sequencing\nreads) depends on a continuous covariate, say, temperature or nutrient\nconcentration. You may already have encountered linear regression, where our\nmodel is that the response variable \\\\(y\\\\) depends on the covariate \\\\(x\\\\)\nvia the equation \\\\(y = ax+b + e\\\\), with parameters \\\\(a\\\\) and \\\\(b\\\\) (that\nwe need to estimate), and with residuals \\\\(e\\\\) whose probability model is a\nnormal distribution (whose variance we usually also need to estimate). For\ncount data the same type of regression model is possible, although the\nprobability distribution for the residuals then needs to be non-normal. In\nthat case we use the **generalized linear models** framework. We will see\nexamples when studying RNA-Seq in [Chapter 8](08-chap.html) and another type\nof next generation sequencing data, 16S rRNA data, in [Chapter\n9](09-chap.html).\n\nKnowing that our probability model involves a Poisson, binomial, multinomial\ndistribution or another parametric family will enable us to have quick answers\nto questions about the parameters of the model and compute quantities such as\np-values and confidence intervals.\n\n## 2.4 Binomial distributions and maximum likelihood\n\nIn a binomial distribution there are two parameters: the number of trials\n\\\\(n\\\\), which is typically known, and the probability \\\\(p\\\\) of seeing a 1\nin a trial. This probability is often unknown.\n\n### 2.4.1 An example\n\nSuppose we take a sample of \\\\(n=120\\\\) males and test them for red-green\ncolorblindness. We can code the data as 0 if the subject is not colorblind and\n1 if he is. We summarize the data by the table:\n\n    \n    \n    table(cb)__\n    \n    \n    cb\n      0   1 \n    110  10 \n\n__\n\nQuestion 2.7\n\nWhich value of \\\\(p\\\\) is the most likely given these data?\n\n__\n\nSolution\n\n__\n\n\\\\(\\hat{p}=\\frac{1}{12}\\\\).\n\n    \n    \n    mean(cb)__\n    \n    \n    [1] 0.08333333\n\n[![However, be careful: sometimes, maximum likelihood estimates are harder to\nguess and to compute, as well as being much less intuitive \\(see Exercise imp-\nmodels-mlmax\\).](imgs/devil.png)](imgs/devil.png \"However, be careful:\nsometimes, maximum likelihood estimates are harder to guess and to compute, as\nwell as being much less intuitive \\(see Exercise imp-models-mlmax\\).\")\n\nHowever, be careful: sometimes, maximum likelihood estimates are harder to\nguess and to compute, as well as being much less intuitive (see Exercise 2.2).\n\nIn this special case, your intuition may give you the estimate\n\\\\(\\hat{p}=\\frac{1}{12}\\\\), which turns out to be the maximum likelihood\nestimate. We put a hat over the letter to remind us that this is not\n(necessarily) the underlying true value, but an estimate we make from the\ndata.\n\nAs before in the case of the Poisson, if we compute the likelihood for many\npossible \\\\(p\\\\), we can plot it and see where its maximum falls (Figure 2.5).\n\n    \n    \n    probs  =  seq(0, 0.3, by = 0.005)\n    likelihood = dbinom(sum(cb), prob = probs, size = length(cb))\n    plot(probs, likelihood, pch = 16, xlab = \"probability of success\",\n           ylab = \"likelihood\", cex=0.6)\n    probs[which.max(likelihood)]__\n    \n    \n    [1] 0.085\n\n[![](02-chap_files/figure-html/fig-likely1-1-1.png)](02-chap_files/figure-\nhtml/fig-likely1-1-1.png \"Figure 2.5: Plot of the likelihood as a function of\nthe probabilities. The likelihood is a function on \\[0, 1\\]. Here we have\nzoomed into the range of \\[0, 0.3\\], as the likelihood is practically zero for\nlarger values of p.\")\n\nFigure 2.5: Plot of the likelihood as a function of the probabilities. The\nlikelihood is a function on \\\\([0, 1]\\\\). Here we have zoomed into the range\nof \\\\([0, 0.3]\\\\), as the likelihood is practically zero for larger values of\n\\\\(p\\\\).\n\nNote: 0.085 is not exactly the value we expected \\\\((\\frac{1}{12})\\\\), and\nthat is because the set of values that we tried (in `probs`) did not include\nthe exact value of \\\\(\\frac{1}{12}\\simeq 0.0833\\\\), so we obtained the next\nbest one. We could use numeric optimisation methods to overcome that.\n\n### 2.4.2 Likelihood for the binomial distribution\n\n[![One can come up with different criteria than maximum likelihood, which lead\nto other estimators. They all carry hats. We’ll see other examples in sec-\nmixtures.](imgs/devil.png)](imgs/devil.png \"One can come up with different\ncriteria than maximum likelihood, which lead to other estimators. They all\ncarry hats. We’ll see other examples in sec-mixtures.\")\n\nOne can come up with different criteria than maximum likelihood, which lead to\nother estimators. They all carry hats. We’ll see other examples in [Chapter\n4](04-chap.html).\n\nThe likelihood and the probability are the same mathematical function, only\ninterpreted in different ways – in one case, the function tells us how\nprobable it is to see a particular set of values of the data, given the\nparameters; in the other case, we consider the data as given, and ask for the\nparameter value(s) that likely generated these data. Suppose \\\\(n=300\\\\), and\nwe observe \\\\(y=40\\\\) successes. Then, for the binomial distribution:\n\n\\\\[ f(p\\,|\\,n,y) = f(y\\,|\\,n,p)={n \\choose y} \\, p^y \\, (1-p)^{(n-y)}.\n\\tag{2.3}\\\\]\n\nAgain, it is more convenient to work with the logarithm of the likelihood,\n\n\\\\[ \\log f(p |y) = \\log {n \\choose y} + y\\log(p) + (n-y)\\log(1-p).\n\\tag{2.4}\\\\]\n\nHere’s a function we can use to calculate it5,\n\n5 In practice, we would try to avoid explicitly computing `choose(n, y)`,\nsince it can be a very large number that tests the limits of our computer’s\nfloating point arithmetic (for `n=300` and `y=40`, it is around 9.8e+49). One\ncould approximate the term using Stirling’s formula, or indeed ignore it, as\nit is only an additive offset independent of \\\\(p\\\\) that does not impact the\nmaximization.\n\n    \n    \n    loglikelihood = function(p, n = 300, y = 40) {\n      log(choose(n, y)) + y * log(p) + (n - y) * log(1 - p)\n    }__\n\nwhich we plot for the range of \\\\(p\\\\) from 0 to 1 (Figure 2.6).\n\n    \n    \n    p_seq = seq(0, 1, by = 0.001)\n    plot(p_seq, loglikelihood(p_seq), xlab = \"p\", ylab = \"log f(p|y)\", type = \"l\")__\n\n[![](02-chap_files/figure-html/fig-\nloglikelihood-1-1.png)](02-chap_files/figure-html/fig-loglikelihood-1-1.png\n\"Figure 2.6: Plot of the log likelihood function for n=300 and y=40.\")\n\nFigure 2.6: Plot of the log likelihood function for \\\\(n=300\\\\) and\n\\\\(y=40\\\\).\n\nThe maximum lies at 40/300 = 0.1333… , consistent with intuition, but we see\nthat other values of \\\\(p\\\\) are almost equally likely, as the function is\nquite flat around the maximum. We will see in a later section how Bayesian\nmethods enable us to work with a range of values for \\\\(p\\\\) instead of just\npicking a single maximum.\n\n## 2.5 More boxes:multinomial data\n\n### 2.5.1 DNA count modeling: base pairs\n\nThere are four basic molecules of DNA: A - adenine, C - cytosine, G - guanine,\nT - thymine. The nucleotides are classified into 2 groups: purines (A and G)\nand pyrimidines (C and T). The binomial would work as a model for the\npurine/pyrimidine groupings but not if we want to use A, C, G, T; for that we\nneed the multinomial model from [Section 1.4](01-chap.html#sec-generative-\nmultinomial). Let’s look at noticeable patterns that occur in these\nfrequencies.\n\n### 2.5.2 Nucleotide bias\n\nThis section combines estimation and testing by simulation in a real example.\nData from one strand of DNA for the genes of _Staphylococcus aureus_ bacterium\nare available in a _fasta_ file `staphsequence.ffn.txt`, which we can read\nwith a function from the Bioconductor package\n**[Biostrings](https://bioconductor.org/packages/Biostrings/)**.\n\n    \n    \n    library(\"Biostrings\")\n    staph = readDNAStringSet(\"../data/staphsequence.ffn.txt\", \"fasta\")__\n\nLet’s look at the first gene:\n\n    \n    \n    staph[1]__\n    \n    \n    DNAStringSet object of length 1:\n        width seq                                               names               \n    [1]  1362 ATGTCGGAAAAAGAAATTTGGGA...AAAAAGAAATAAGAAATGTATAA lcl|NC_002952.2_c...\n    \n    \n    letterFrequency(staph[[1]], letters = \"ACGT\", OR = 0)__\n    \n    \n      A   C   G   T \n    522 219 229 392 \n\n__\n\nQuestion 2.8\n\nWhy did we use double square brackets in the second line?\n\n__\n\nSolution\n\n__\n\nThe double square brackets `[[i]]` extract the sequence of the `i`-th gene as\na _DNAString_ , as opposed to the pair of single brackets `[i]`, which return\na _DNAStringSet_ with just a single _DNAString_ in it. If you look at the\nlength of `staph[1]`, it is 1, whereas `staph[[1]]` has length 1362.\n\n__\n\nQuestion 2.9\n\nFollowing a similar procedure as in [Exercise 1.8](01-chap.html#imp-\ngenerative-genomefrequency), test whether the nucleotides are equally\ndistributed across the four nucleotides for this first gene.\n\nDue to their different physical properties, evolutionary selection can act on\nthe nucleotide frequencies. So we can ask whether, say, the first ten genes\nfrom these data come from the same multinomial. We do not have a prior\nreference, we just want to decide whether the nucleotides occur in the same\nproportions in the first 10 genes. If not, this would provide us with evidence\nfor varying selective pressure on these ten genes.\n\n    \n    \n    letterFrq = vapply(staph, letterFrequency, FUN.VALUE = numeric(4),\n             letters = \"ACGT\", OR = 0)\n    colnames(letterFrq) = paste0(\"gene\", seq(along = staph))\n    tab10 = letterFrq[, 1:10]\n    computeProportions = function(x) { x/sum(x) }\n    prop10 = apply(tab10, 2, computeProportions)\n    round(prop10, digits = 2)__\n    \n    \n      gene1 gene2 gene3 gene4 gene5 gene6 gene7 gene8 gene9 gene10\n    A  0.38  0.36  0.35  0.37  0.35  0.33  0.33  0.34  0.38   0.27\n    C  0.16  0.16  0.13  0.15  0.15  0.15  0.16  0.16  0.14   0.16\n    G  0.17  0.17  0.23  0.19  0.22  0.22  0.20  0.21  0.20   0.20\n    T  0.29  0.31  0.30  0.29  0.27  0.30  0.30  0.29  0.28   0.36\n    \n    \n    p0 = rowMeans(prop10)\n    p0 __\n    \n    \n            A         C         G         T \n    0.3470531 0.1518313 0.2011442 0.2999714 \n\nSo let’s suppose `p0` is the vector of multinomial probabilities for all the\nten genes and use a Monte Carlo simulation to test whether the departures\nbetween the observed letter frequencies and expected values under this\nsupposition are within a plausible range.\n\nWe compute the expected counts by taking the `outer` product of the vector of\nprobabilities p0 with the sums of nucleotide counts from each of the 10\ncolumns, `cs`.\n\n    \n    \n    cs = colSums(tab10)\n    cs __\n    \n    \n     gene1  gene2  gene3  gene4  gene5  gene6  gene7  gene8  gene9 gene10 \n      1362   1134    246   1113   1932   2661    831   1515   1287    696 \n    \n    \n    expectedtab10 = outer(p0, cs, FUN = \"*\")\n    round(expectedtab10)__\n    \n    \n      gene1 gene2 gene3 gene4 gene5 gene6 gene7 gene8 gene9 gene10\n    A   473   394    85   386   671   924   288   526   447    242\n    C   207   172    37   169   293   404   126   230   195    106\n    G   274   228    49   224   389   535   167   305   259    140\n    T   409   340    74   334   580   798   249   454   386    209\n\nWe can now create a random table with the correct column sums using the\n`rmultinom` function. This table is generated according to the null hypothesis\nthat the true proportions are given by `p0`.\n\n    \n    \n    randomtab10 = sapply(cs, function(s) { rmultinom(1, s, p0) } )\n    all(colSums(randomtab10) == cs)__\n    \n    \n    [1] TRUE\n\nNow we repeat this B = 1000 times. For each table we compute our test\nstatistic from [Section 1.4.1](01-chap.html#sec-generative-SimulatingForPower)\nin [Chapter 1](01-chap.html) (the function `stat`) and store the results in\nthe vector `simulstat`. Together, these values constitute our null\ndistribution, as they were generated under the null hypothesis that `p0` is\nthe vector of multinomial proportions for each of the 10 genes.\n\n    \n    \n    stat = function(obsvd, exptd) {\n       sum((obsvd - exptd)^2 / exptd)\n    }\n    B = 1000\n    simulstat = replicate(B, {\n      randomtab10 = sapply(cs, function(s) { rmultinom(1, s, p0) })\n      stat(randomtab10, expectedtab10)\n    })\n    S1 = stat(tab10, expectedtab10)\n    sum(simulstat >= S1)__\n    \n    \n    [1] 0\n    \n    \n    hist(simulstat, col = \"lavender\", breaks = seq(0, 75, length.out=50))\n    abline(v = S1, col = \"red\")\n    abline(v = quantile(simulstat, probs = c(0.95, 0.99)),\n           col = c(\"darkgreen\", \"blue\"), lty = 2)__\n\n[![](02-chap_files/figure-html/fig-quant12-1-1.png)](02-chap_files/figure-\nhtml/fig-quant12-1-1.png \"Figure 2.7: Histogram of simulstat. The value of S1\nis marked by the vertical red line, those of the 0.95 and 0.99 quantiles \\(see\nnext section\\) by the dotted lines.\")\n\nFigure 2.7: Histogram of `simulstat`. The value of `S1` is marked by the\nvertical red line, those of the 0.95 and 0.99 quantiles (see next section) by\nthe dotted lines.\n\nThe histogram is shown in Figure 2.7. We see that the probability of seeing a\nvalue as large as `S1`=70.1 is very small under the _null model_. It happened\n0 times in our 1000 simulations that a value as big as `S1` occurred. Thus the\nten genes do not seem to come from the same multinomial model.\n\n## 2.6 The \\\\(\\chi^2\\\\) distribution\n\nIn fact, we could have used statistical theory to come to the same conclusion\nwithout running these simulations. The theoretical distribution of the\n`simulstat` statistic is called the \\\\(\\chi^2\\\\) (chi-squared) distribution6\nwith parameter 30 (\\\\(=10\\times(4-1)\\\\)). We can use this for computing the\nprobability of having a value as large as `S1` \\\\(=\\\\) 70.1. As we just saw\nabove, small probabilities are difficult to compute by Monte Carlo: the\ngranularity of the computation is \\\\(1/B\\\\), so we cannot estimate any\nprobabilities smaller than that, and in fact the uncertainty of the estimate\nis larger. So if any theory is applicable, that tends to be useful. We can\ncheck how well theory and simulation match up in our case using another visual\ngoodness-of-fit tool: the **quantile-quantile** (**QQ**) plot. When comparing\ntwo distributions, whether from two different samples or from one sample\nversus a theoretical model, just looking at histograms is not informative\nenough. We use a method based on the quantiles of each of the distributions.\n\n6 Strictly speaking, the distribution of `simulstat` is approximately\ndescribed by a \\\\(\\chi^2\\\\) distribution; the approximation is particularly\ngood if the counts in the table are large.\n\n### 2.6.1 Intermezzo: quantiles and the quantile-quantile plot\n\nIn the previous chapter, we ordered the 100 sample values\n\\\\(x_{(1)},x_{(2)},...,x_{(100)}\\\\). Say we want the 22nd percentile. We can\ntake any value between the 22nd and the 23rd value, i.e., any value that\nfulfills \\\\(x_{(22)} \\leq c_{0.22} < x_{(23)}\\\\) is acceptable as a 0.22\n**quantile** (\\\\(c_{0.22}\\\\)). In other words, \\\\(c_{0.22}\\\\) is defined by\n\n\\\\[ \\frac{\\\\# x_i's \\leq c_{0.22}}{n} = 0.22. \\\\]\n\nIn [Section 3.6.7](03-chap.html#sec-graphics-ecdf), we’ll introduce the\n**empirical cumulative distribution** function (**ECDF**) \\\\(\\hat{F}\\\\), and\nwe’ll see that our definition of \\\\(c_{0.22}\\\\) can also be written as\n\\\\(\\hat{F}_n(c_{0.22}) = 0.22\\\\). In Figure 2.7, our histogram of the\ndistribution of `simulstat`, the quantiles \\\\(c_{0.95}\\\\) and \\\\(c_{0.99}\\\\)\nare also shown.\n\n__\n\nQuestion 2.10\n\n  1. Compare the `simulstat` values and 1000 randomly generated \\\\(\\chi^2_{30}\\\\) random numbers by displaying them in histograms with 50 bins each.\n\n  2. Compute the quantiles of the `simulstat` values and compare them to those of the \\\\(\\chi_{30}^2\\\\) distribution. Hint:\n\n    \n    \n    qs = ppoints(100)\n    quantile(simulstat, qs)\n    quantile(qchisq(qs, df = 30), qs)__\n\n[![A name collision occurs here. Statisticians call the summary statistic we\njust computed as simulstat \\(sum of squares of weighted differences\\), the\nchi-squared or \\\\chi^2 statistic. The theoretical distribution \\\\chi^2_\\\\nu is\na distribution in its own right, with a parameter \\\\nu called the degrees of\nfreedom. When reading about the chi-squared or \\\\chi^2, you will need to pay\nattention to the context to see which meaning is\nappropriate.](imgs/devil.png)](imgs/devil.png \"A name collision occurs here.\nStatisticians call the summary statistic we just computed as simulstat \\(sum\nof squares of weighted differences\\), the chi-squared or \\\\chi^2 statistic.\nThe theoretical distribution \\\\chi^2_\\\\nu is a distribution in its own right,\nwith a parameter \\\\nu called the degrees of freedom. When reading about the\nchi-squared or \\\\chi^2, you will need to pay attention to the context to see\nwhich meaning is appropriate.\")\n\nA name collision occurs here. Statisticians call the summary statistic we just\ncomputed as `simulstat` (sum of squares of weighted differences), the **chi-\nsquared** or \\\\(\\chi^2\\\\) _statistic_. The theoretical _distribution_\n\\\\(\\chi^2_\\nu\\\\) is a distribution in its own right, with a parameter\n\\\\(\\nu\\\\) called the degrees of freedom. When reading about the chi-squared or\n\\\\(\\chi^2\\\\), you will need to pay attention to the context to see which\nmeaning is appropriate.\n\n__\n\nQuestion 2.11\n\nDo you know another name for the 0.5 quantile?\n\n__\n\nSolution\n\n__\n\nThe median.\n\n__\n\nQuestion 2.12\n\nIn the above definition, we were a little vague on how the quantile is defined\nin general, i.e., not just for 0.22. How is the quantile computed for any\nnumber between 0 and 1, including ones that are not multiples of \\\\(1/n\\\\)?\n\n__\n\nSolution\n\n__\n\nCheck the manual page of the `quantile` function and its argument named\n`type`.\n\nNow that we have an idea what quantiles are, we can do the quantile-quantile\nplot. We plot the quantiles of the `simulstat` values, which we simulated\nunder the null hypothesis, against the theoretical null distribution\n\\\\(\\chi^2_{30}\\\\) (Figure 2.8):\n\n    \n    \n    qqplot(qchisq(ppoints(B), df = 30), simulstat, main = \"\",\n      xlab = expression(chi[nu==30]^2), asp = 1, cex = 0.5, pch = 16)\n    abline(a = 0, b = 1, col = \"red\")__\n\n[![](02-chap_files/figure-html/fig-qqplot3-1-1.png)](02-chap_files/figure-\nhtml/fig-qqplot3-1-1.png \"Figure 2.8: Our simulated statistic’s distribution\ncompared to \\\\chi_{30}^2 using a quantile-quantile \\(QQ\\) plot, which shows\nthe theoretical quantiles for the \\\\chi^2_{30} distribution on the horizontal\naxis and the sampled ones on the vertical axis.\")\n\nFigure 2.8: Our simulated statistic’s distribution compared to\n\\\\(\\chi_{30}^2\\\\) using a quantile-quantile (QQ) plot, which shows the\ntheoretical **quantiles** for the \\\\(\\chi^2_{30}\\\\) distribution on the\nhorizontal axis and the sampled ones on the vertical axis.\n\nHaving convinced ourselves that `simulstat` is well described by a\n\\\\(\\chi^2_{30}\\\\) distribution, we can use that to compute our p-value, i.e.,\nthe probability that under the null hypothesis (counts are distributed as\nmultinomial with probabilities \\\\(p_{\\text{A}} = 0.35\\\\), \\\\(p_{\\text{C}} =\n0.15\\\\), \\\\(p_{\\text{G}} = 0.2\\\\), \\\\(p_{\\text{T}} = 0.3\\\\)) we observe a\nvalue as high as `S1`=70.1:\n\n    \n    \n    1 - pchisq(S1, df = 30)__\n    \n    \n    [1] 4.74342e-05\n\nWith such a small p-value, the null hypothesis seems improbable. Note how this\ncomputation did not require the 1000 simulations and was faster.\n\n## 2.7 Chargaff’s Rule\n\nThe most important pattern in the nucleotide frequencies was discovered by\nChargaff ([Elson and Chargaff 1952](16-chap.html#ref-Chargaff)).\n\n![](imgs/ChargaffColdSpring_web.jpg)\n\nLong before DNA sequencing was available, using the weight of the molecules,\nhe asked whether the nucleotides occurred at equal frequencies. He called this\nthe tetranucleotide hypothesis. We would translate that into asking whether\n\\\\(p_{\\text{A}} = p_{\\text{C}} = p_{\\text{G}} = p_{\\text{T}}\\\\).\n\nUnfortunately, Chargaff only published the _percentages_ of the mass present\nin different organisms for each of the nucleotides, not the measurements\nthemselves.\n\n    \n    \n    load(\"../data/ChargaffTable.RData\")\n    ChargaffTable __\n    \n    \n                      A    T    C    G\n    Human-Thymus   30.9 29.4 19.9 19.8\n    Mycobac.Tuber  15.1 14.6 34.9 35.4\n    Chicken-Eryth. 28.8 29.2 20.5 21.5\n    Sheep-liver    29.3 29.3 20.5 20.7\n    Sea Urchin     32.8 32.1 17.7 17.3\n    Wheat          27.3 27.1 22.7 22.8\n    Yeast          31.3 32.9 18.7 17.1\n    E.coli         24.7 23.6 26.0 25.7\n\n[![](02-chap_files/figure-html/fig-ChargaffBars-1.png)](02-chap_files/figure-\nhtml/fig-ChargaffBars-1.png \"Figure 2.9: Barplots for the different rows in\nChargaffTable. Can you spot the pattern?\")\n\nFigure 2.9: Barplots for the different rows in `ChargaffTable`. Can you spot\nthe pattern?\n\n__\n\nQuestion 2.13\n\n  * Do these data seem to come from equally likely multinomial categories?\n\n  * Can you suggest an alternative pattern? puted from simulations u\n\n  * Can you do a quantitative analysis of the pattern, perhaps inspired by the simulations above?\n\n__\n\nSolution\n\n__\n\nChargaff _saw_ the answer to this question and postulated a pattern called\n_base pairing_ , which ensured a perfect match of the amount of adenine (A) in\nthe DNA of an organism to the amount of thymine (T). Similarly, whatever the\namount of guanine (G), the amount of cytosine (C) would be the same. This is\nnow called Chargaff’s rule. On the other hand, the amount of C/G in an\norganism could be quite different from that of A/T, with no obvious pattern\nacross organisms. Based on Chargaff’s rule, we might define a statistic\n\n\\\\[ (p_{\\text{C}} - p_{\\text{G}})^2 + (p_{\\text{A}} - p_{\\text{T}})^2, \\\\]\n\nsummed over all rows of the table. We are going to look at a comparison\nbetween the data and what would occur if the nucleotides were ‘exchangeable’,\nin the sense that the probabilities observed in each row were in no particular\norder, so that there were no special relationship between the proportions of\nAs and Ts, or between those of Cs and Gs.\n\n    \n    \n    statChf = function(x){\n      sum((x[, \"C\"] - x[, \"G\"])^2 + (x[, \"A\"] - x[, \"T\"])^2)\n    }\n    chfstat = statChf(ChargaffTable)\n    permstat = replicate(100000, {\n         permuted = t(apply(ChargaffTable, 1, sample))\n         colnames(permuted) = colnames(ChargaffTable)\n         statChf(permuted)\n    })\n    pChf = mean(permstat <= chfstat)\n    pChf __\n    \n    \n    [1] 0.00014\n    \n    \n    hist(permstat, breaks = 100, main = \"\", col = \"lavender\")\n    abline(v = chfstat, lwd = 2, col = \"red\")__\n\n[![](02-chap_files/figure-html/fig-permstatChf-1-1.png)](02-chap_files/figure-\nhtml/fig-permstatChf-1-1.png \"Figure 2.10: Histogram of our statistic statChf\ncomputed from simulations using per-row permutations of the columns. The value\nit yields for the observed data is shown by the red line.\")\n\nFigure 2.10: Histogram of our statistic `statChf` computed from simulations\nusing per-row permutations of the columns. The value it yields for the\nobserved data is shown by the red line.\n\nThe histogram in Figure 2.10 shows that it is quite rare to have a value as\nsmall as the observed 11.1, where the red line is drawn. The probability of\nobserving a value as small or smaller is `pChf`=1.4^{-4}. Thus the data\nstrongly support Chargaff’s insight.\n\n__\n\nQuestion 2.14\n\nWhen computing `pChf`, we only looked at the values in the null distribution\nsmaller than the observed value. Why did we do this in a one-sided way here?\n\n### 2.7.1 Two categorical variables\n\nUp to now, we have visited cases where the data are taken from a sample that\ncan be classified into different boxes: the binomial for Yes/No binary boxes\nand the multinomial distribution for categorical variables such as A, C, G, T\nor different genotypes such aa, aA, AA. However it might be that we measure\ntwo (or more) categorical variables on a set of subjects, for instance eye\ncolor and hair color. We can then cross-tabulate the counts for every\ncombination of eye and hair color. We obtain a table of counts called a\n**contingency table**. This concept is very useful for many biological data\ntypes.\n\n    \n    \n    HairEyeColor[,, \"Female\"]__\n    \n    \n           Eye\n    Hair    Brown Blue Hazel Green\n      Black    36    9     5     2\n      Brown    66   34    29    14\n      Red      16    7     7     7\n      Blond     4   64     5     8\n\n__\n\nQuestion 2.15\n\nExplore the `HairEyeColor` object in R. What data type, shape and dimensions\ndoes it have?\n\n__\n\nSolution\n\n__\n\nIt is a numeric array with three dimensions:\n\n    \n    \n    str(HairEyeColor)__\n    \n    \n     'table' num [1:4, 1:4, 1:2] 32 53 10 3 11 50 10 30 10 25 ...\n     - attr(*, \"dimnames\")=List of 3\n      ..$ Hair: chr [1:4] \"Black\" \"Brown\" \"Red\" \"Blond\"\n      ..$ Eye : chr [1:4] \"Brown\" \"Blue\" \"Hazel\" \"Green\"\n      ..$ Sex : chr [1:2] \"Male\" \"Female\"\n    \n    \n    ## ?HairEyeColor __\n\n#### Color blindness and sex\n\nDeuteranopia is a form of red-green color blindness due to the fact that\nmedium wavelength sensitive cones (green) are missing. A deuteranope can only\ndistinguish 2 to 3 different hues, whereas somebody with normal vision sees 7\ndifferent hues. A survey for this type of color blindness in human subjects\nproduced a two-way table crossing color blindness and sex.\n\n    \n    \n    load(\"../data/Deuteranopia.RData\")\n    Deuteranopia __\n    \n    \n              Men Women\n    Deute      19     2\n    NonDeute 1981  1998\n\nHow do we test whether there is a relationship between sex and the occurrence\nof color blindness? We postulate the null model with two independent\nbinomials: one for sex and one for color blindness. Under this model we can\nestimate all the cells’ multinomial probabilities, and we can compare the\nobserved counts to the expected ones. This is done through the `chisq.test`\nfunction in R.\n\n    \n    \n    chisq.test(Deuteranopia)__\n    \n    \n        Pearson's Chi-squared test with Yates' continuity correction\n    \n    data:  Deuteranopia\n    X-squared = 12.255, df = 1, p-value = 0.0004641\n\nThe small p value tells us that we should expect to see such a table with only\na very small probability under the null model – i.e., if the fractions of\ndeuteranopic color blind among women and men were the same.\n\nWe’ll see another test for this type of data called Fisher’s exact test (also\nknown as the hypergeometric test) in [Section 10.3.2](10-chap.html#sec-graphs-\nGSEA). This test is widely used for testing the over-representations of\ncertain types of genes in a list of significantly expressed ones.\n\n### 2.7.2 A special multinomial: Hardy-Weinberg equilibrium\n\nHere we highlight the use of a multinomial with three possible levels created\nby combining two alleles M and N. Suppose that the overall frequency of allele\nM in the population is \\\\(p\\\\), so that of N is \\\\(q = 1-p\\\\). The Hardy-\nWeinberg model looks at the relationship between \\\\(p\\\\) and \\\\(q\\\\) if there\nis independence of the frequency of both alleles in a genotype, the so-called\n**Hardy-Weinberg equilibrium** (HWE). This would be the case if there is\nrandom mating in a large population with equal distribution of the alleles\namong sexes. The probabilities of the three genotypes are then as follows:\n\n\\\\[ p_{\\text{MM}}=p^2,\\quad p_{\\text{NN}}=q^2,\\quad p_{\\text{MN}}=2pq\n\\tag{2.5}\\\\]\n\nWe only observe the frequencies\n\\\\((n_{\\text{MM}},\\,n_{\\text{MN}},\\,n_{\\text{NN}})\\\\) for the genotypes MM,\nMN, NN and the total number \\\\(S=n_{\\text{MM}}+\nn_{\\text{MN}}+n_{\\text{NN}}\\\\). We can write the likelihood, i.e., the\nprobability of the observed data when the probabilities of the categories are\ngiven by Equation 2.5, using the multinomial formula\n\n\\\\[ P(n_{\\text{MM}},\\,n_{\\text{MN}},\\,n_{\\text{NN}}\\;|\\;p) = {S \\choose\nn_{\\text{MM}},n_{\\text{MN}},n_{\\text{NN}}} (p^2)^{n_{\\text{MM}}} \\,\\times\\,\n(2pq)^{n_{\\text{MN}}} \\,\\times\\, (q^2)^{n_{\\text{NN}}}, \\\\]\n\nand the log-likelihood under HWE\n\n\\\\[ L(p)=n_{\\text{MM}}\\log(p^2)+n_{\\text{MN}}\n\\log(2pq)+n_{\\text{NN}}\\log(q^2). \\\\]\n\nThe value of \\\\(p\\\\) that maximizes the log-likelihood is\n\n\\\\[ p = \\frac{n_{\\text{MM}} + n_{\\text{MN}}/2}{S}. \\\\]\n\nSee ([Rice 2006, chap. 8](16-chap.html#ref-Rice:2007), Section 5) for the\nproof. Given the data \\\\((n_{\\text{MM}},\\,n_{\\text{MN}},\\,n_{\\text{NN}})\\\\),\nthe log-likelihood \\\\(L\\\\) is a function of only one parameter, \\\\(p\\\\).\nFigure 2.11 shows this log-likelihood function for different values of \\\\(p\\\\)\nfor the 216th row of the Mourant data7, computed in the following code.\n\n7 This is genotype frequency data of blood group alleles from Mourant, Kopec,\nand Domaniewska-Sobczak ([1976](16-chap.html#ref-Mourant1976)) available\nthrough the R package\n**[HardyWeinberg](https://cran.r-project.org/web/packages/HardyWeinberg/)**.\n\n    \n    \n    library(\"HardyWeinberg\")\n    data(\"Mourant\")\n    Mourant[214:216,]__\n    \n    \n        Population    Country Total  MM  MN  NN\n    214    Oceania Micronesia   962 228 436 298\n    215    Oceania Micronesia   678  36 229 413\n    216    Oceania     Tahiti   580 188 296  96\n    \n    \n    nMM = Mourant$MM[216]\n    nMN = Mourant$MN[216]\n    nNN = Mourant$NN[216]\n    loglik = function(p, q = 1 - p) {\n      2 * nMM * log(p) + nMN * log(2*p*q) + 2 * nNN * log(q)\n    }\n    xv = seq(0.01, 0.99, by = 0.01)\n    yv = loglik(xv)\n    plot(x = xv, y = yv, type = \"l\", lwd = 2,\n         xlab = \"p\", ylab = \"log-likelihood\")\n    imax = which.max(yv)\n    abline(v = xv[imax], h = yv[imax], lwd = 1.5, col = \"blue\")\n    abline(h = yv[imax], lwd = 1.5, col = \"purple\")__\n\n[![](02-chap_files/figure-html/fig-\nHardyWeinberg-1-1.png)](02-chap_files/figure-html/fig-HardyWeinberg-1-1.png\n\"Figure 2.11: Plot of the log-likelihood for the Tahiti data.\")\n\nFigure 2.11: Plot of the log-likelihood for the Tahiti data.\n\nThe maximum likelihood estimate for the probabilities in the multinomial is\nalso obtained by using the observed frequencies as in the binomial case,\nhowever the estimates have to account for the relationships between the three\nprobabilities. We can compute \\\\(\\hat{p}_{\\text{MM}}\\\\),\n\\\\(\\hat{p}_{\\text{MN}}\\\\) and \\\\(\\hat{p}_{\\text{NN}}\\\\) using the `af`\nfunction from the\n**[HardyWeinberg](https://cran.r-project.org/web/packages/HardyWeinberg/)**\npackage.\n\n    \n    \n    phat  =  af(c(nMM, nMN, nNN))\n    phat __\n    \n    \n            A \n    0.5793103 \n    \n    \n    pMM   =  phat^2\n    qhat  =  1 - phat __\n\nThe expected values under Hardy-Weinberg equilibrium are then\n\n    \n    \n    pHW = c(MM = phat^2, MN = 2*phat*qhat, NN = qhat^2)\n    sum(c(nMM, nMN, nNN)) * pHW __\n    \n    \n        MM.A     MN.A     NN.A \n    194.6483 282.7034 102.6483 \n\nwhich we can compare to the observed values above. We can see that they are\nquite close to the observed values. We could further test whether the observed\nvalues allow us to reject the Hardy-Weinberg model, either by doing a\nsimulation or a \\\\(\\chi^2\\\\) test as above. A visual evaluation of the\ngoodness-of-fit of Hardy-Weinberg was designed by de Finetti ([Finetti\n1926](16-chap.html#ref-definetti26); [Cannings and Edwards\n1968](16-chap.html#ref-Cannings1968)). It places every sample at a point whose\ncoordinates are given by the proportions of each of the different alleles.\n\n#### Visual comparison to the Hardy-Weinberg equilibrium\n\nWe use the `HWTernaryPlot` function to display the data and compare it to\nHardy-Weinberg equilibrium graphically.\n\n    \n    \n    pops = c(1, 69, 128, 148, 192)\n    genotypeFrequencies = as.matrix(Mourant[, c(\"MM\", \"MN\", \"NN\")])\n    HWTernaryPlot(genotypeFrequencies[pops, ],\n            markerlab = Mourant$Country[pops],\n            alpha = 0.0001, curvecols = c(\"red\", rep(\"purple\", 4)),\n            mcex = 0.75, vertex.cex = 1)__\n\n[![](02-chap_files/figure-html/fig-HWtern-1.png)](02-chap_files/figure-\nhtml/fig-HWtern-1.png \"Figure 2.12: This de Finetti plot shows the points as\nbarycenters of the three genotypes using the frequencies as weights on each of\nthe corners of the triangle. The Hardy-Weinberg model is the red curve, the\nacceptance region is between the two purple lines. We see that the US is the\nfurthest from being in HW equilibrium.\")\n\nFigure 2.12: This **de Finetti plot** shows the points as barycenters of the\nthree genotypes using the frequencies as weights on each of the corners of the\ntriangle. The Hardy-Weinberg model is the red curve, the acceptance region is\nbetween the two purple lines. We see that the US is the furthest from being in\nHW equilibrium.\n\n__\n\nQuestion 2.16\n\nMake the ternary plot as in the code above, then add the other data points to\nit, what do you notice? You could back up your discussion using the `HWChisq`\nfunction.\n\n__\n\nSolution\n\n__\n\n    \n    \n    HWTernaryPlot(genotypeFrequencies[-pops, ], \n                  newframe = FALSE, alpha = 0.0001, cex = 0.5)__\n\n__\n\nQuestion 2.17\n\nDivide all total frequencies by 50, keeping the same proportions for each of\nthe genotypes, and recreate the ternary plot.\n\n  * What happens to the points ?\n\n  * What happens to the confidence regions and why?\n\n__\n\nSolution\n\n__\n\n    \n    \n    newgf = round(genotypeFrequencies / 50)\n    HWTernaryPlot(newgf[pops, ],\n                  markerlab = Mourant$Country[pops],\n                  curvecols = c(\"red\", rep(\"purple\", 4)),\n                  alpha = 0.0001, mcex = 0.75, vertex.cex = 1)__\n\n### 2.7.3 Concatenating several multinomials: sequence motifs and logos\n\nThe [Kozak Motif](http://www.sciencegateway.org/resources/kozak.htm) is a\nsequence that occurs close to the start codon **ATG** of a coding region. The\nstart codon itself always has a fixed spelling but in positions 5 to the left\nof it, there is a nucleotide pattern in which the letters are quite far from\nbeing equally likely.\n\nWe summarize this by giving the **position weight matrix** (PWM) or\n**position-specific scoring matrix** (PSSM), which provides the multinomial\nprobabilities at every position. This is encoded graphically by the **sequence\nlogo** (Figure 2.13).\n\n    \n    \n    library(\"seqLogo\")\n    load(\"../data/kozak.RData\")\n    kozak __\n    \n    \n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n    A 0.33 0.25  0.4 0.15 0.20    1    0    0 0.05\n    C 0.12 0.25  0.1 0.40 0.40    0    0    0 0.05\n    G 0.33 0.25  0.4 0.20 0.25    0    0    1 0.90\n    T 0.22 0.25  0.1 0.25 0.15    0    1    0 0.00\n    \n    \n    pwm = makePWM(kozak)\n    seqLogo(pwm, ic.scale = FALSE)__\n\n[![](02-chap_files/figure-html/fig-seqlogo-1-1.png)](02-chap_files/figure-\nhtml/fig-seqlogo-1-1.png \"Figure 2.13: Here is a diagram called a sequence\nlogo for the position dependent multinomial used to model the Kozak motif. It\ncodifies the amount of variation in each of the positions on a log scale. The\nlarge letters represent positions where there is no uncertainty about which\nnucleotide occurs.\")\n\nFigure 2.13: Here is a diagram called a sequence logo for the position\ndependent multinomial used to model the Kozak motif. It codifies the amount of\nvariation in each of the positions on a log scale. The large letters represent\npositions where there is no uncertainty about which nucleotide occurs.\n\nOver the last sections, we’ve seen how the different “boxes” in the\nmultinomial distributions we have encountered very rarely have equal\nprobabilities. In other words, the parameters \\\\(p_1, p_2, ...\\\\) are often\ndifferent, depending on what is being modeled. Examples of multinomials with\nunequal frequencies include the twenty different amino acids, blood types and\nhair color.\n\nIf there are multiple categorical variables, we have seen that they are rarely\nindependent (sex and colorblindness, hair and eye color, …). We will see later\nin [Chapter 9](09-chap.html) that we can explore the patterns in these\ndependencies by using multivariate decompositions of the contingency tables.\nHere, we’ll look at an important special case of dependencies between\ncategorical variables: those that occur along a sequence (or “chain”) of\ncategorical variables, e.g., over time or along a biopolymer.\n\n## 2.8 Modeling sequential dependencies: Markov chains\n\nIf we want to predict tomorrow’s weather, a reasonably good guess is that it\nwill most likely be the same as today’s weather, in addition we may state the\nprobabilities for various kinds of possible changes^[The same reasoning can\nalso be applied in reverse: we could “predict” yesterday’s weather from\ntoday’s.. This method for weather forecasting is an example for the Markov\nassumption: the prediction for tomorrow only depends on the state of things\ntoday, but not on yesterday or three weeks ago (all information we could\npotentially use is already contained in today’s weather). The weather example\nalso highlights that such an assumption need not necessarily be exactly true,\nbut it should be a good enough assumption. It is fairly straightforward to\nextend this assumption to dependencies on the previous \\\\(k\\\\) days, where\n\\\\(k\\\\) is a finite and hopefully not too large number. The essence of the\nMarkov assumption is that the process has a finite “memory”, so that\npredictions only need to look back for a finite amount of time.\n\nInstead of temporal sequences, we can also apply this to biological sequences.\nIn DNA, we may see specific succession of patterns so that pairs of\nnucleotides, called digrams, say, [CG, CA, CC] and [CT] are not equally\nfrequent. For instance, in parts of the genome we see more frequent instances\nof [CA] than we would expect under independence:\n\n\\\\[ P(\\mathtt{CA}) \\neq P(\\mathtt{C}) \\, P(\\mathtt{A}). \\\\]\n\nWe model this dependency in the sequence as a **Markov chain** :\n\n\\\\[ P(\\mathtt{CA}) = P(\\mathtt{NCA}) = P(\\mathtt{NNCA}) = P(...\\mathtt{CA}) =\nP(\\mathtt{C}) \\, P(\\mathtt{A|C}), \\\\]\n\nwhere N stands for any nucleotide, and \\\\(P(\\mathtt{A|C})\\\\) stands for “the\nprobability of \\\\(\\mathtt{A}\\\\), given that the preceding base is a\n\\\\(\\mathtt{C}\\\\)”. Figure 2.14 shows a schematic representation of such\ntransitions on a graph.\n\n[![](02-chap_files/figure-html/fig-\nstatsfourstateMC-1.png)](02-chap_files/figure-html/fig-statsfourstateMC-1.png\n\"Figure 2.14: Visualisation of a 4-state Markov chain. The probability of each\npossible digram \\(e.,g., CA\\) is given by the weight of the edge between the\ncorresponding nodes. So for instance, the probability of CA is given by the\nedge C$ o$ A. We’ll see in sec-images how to use R packages to draw these type\nof network graphs.\")\n\nFigure 2.14: Visualisation of a 4-state Markov chain. The probability of each\npossible digram (e.,g., CA) is given by the weight of the edge between the\ncorresponding nodes. So for instance, the probability of CA is given by the\nedge C$ o$ A. We’ll see in [Chapter 11](11-chap.html) how to use R packages to\ndraw these type of network graphs.\n\n## 2.9 Bayesian Thinking\n\n[![](imgs/turtlesalltheway.png)](imgs/turtlesalltheway.png \"Figure 2.15:\nTurtles all the way down. Bayesian modeling of the uncertainty of the\nparameter of a distribution is done by using a random variable whose\ndistribution may depend on parameters whose uncertainty can be modeled as a\nrandom variable; these are called hierarchical models.\")\n\nFigure 2.15: Turtles all the way down. Bayesian modeling of the uncertainty of\nthe parameter of a distribution is done by using a random variable whose\ndistribution may depend on parameters whose uncertainty can be modeled as a\nrandom variable; these are called hierarchical models.\n\nUp to now we have followed a classical approach, where the parameters of our\nmodels and the distributions they use, i.e., the probabilities of the possible\ndifferent outcomes, represent long term frequencies. The parameters are—at\nleast conceptually—definite, knowable and fixed. We may not know them, so we\nestimate them from the data at hand. However, such an approach does not take\ninto account any information that we might already have, and that might inform\nus on the parameters or make certain parameter values or their combinations\nmore likely than others—even _before_ we see any of the current set of data.\nFor that we need a different approach, in which we use probabilistic models\n(i.e., distributions) to express our prior knowledge8 about the parameters,\nand use the current data to _update_ such knowledge, for instance by shifting\nthose distributions or making them more narrow. Such an approach is provided\nby the Bayesian paradigm (Figure 2.15).\n\n8 Some like to say “our belief(s)”.\n\nThe Bayesian paradigm is a practical approach where _prior_ and _posterior_\ndistributions to model our knowledge _before_ and _after_ collecting some data\nand making an observation. It can be iterated ad infinitum: the posterior\nafter one round of data generation can be used as the prior for the next\nround. Thus, it is also particularly useful for integrating or combining\ninformation from different sources.\n\nThe same idea can also be applied to hypothesis testing, where we want to use\ndata to decide whether we believe that a certain statement—which we might call\nthe hypothesis \\\\(H\\\\)—is true. Here, our “parameter” is the probability that\n\\\\(H\\\\) is true, and we can formalize our prior knowledge in the form of a\n**prior** probability, written \\\\(P(H)\\\\)9. After we see the data, we have the\n**posterior** probability. We write it as \\\\(P(H\\,|\\,D)\\\\), the probability of\n\\\\(H\\\\) given that we saw \\\\(D\\\\). This may be higher or lower than\n\\\\(P(H)\\\\), depending on what the data \\\\(D\\\\) were.\n\n9 For a so-called frequentist, such a probability does not exist. Their\nviewpoint is that, although the truth is unknown, in reality the hypothesis is\neither true or false; there is no meaning in calling it, say, “70% true”.\n\n### 2.9.1 Example: haplotype frequencies\n\nTo keep the mathematical formalism to a minimum, we start with an example from\nforensics, using combined signatures (haplotypes) from the Y chromosome.\n\nA _haplotype_ is a collection of alleles (DNA sequence variants) that are\nspatially adjacent on a chromosome, are usually inherited together\n(recombination tends not to disconnect them), and thus are genetically linked.\nIn this case we are looking at linked variants on the Y chromosome.\n\nFirst we’ll look at the motivation behind haplotype frequency analyses, then\nwe’ll revisit the idea of likelihood. After this, we’ll explain how we can\nthink of unknown parameters as being random numbers themselves, modeling their\nuncertainty with a prior distribution. Then we will see how to incorporate new\ndata observed into the probability distributions and compute posterior\nconfidence statements about the parameters.\n\n[![](imgs/STRDefinition.png)](imgs/STRDefinition.png \"Figure 2.16: A short\ntandem repeat \\(STR\\) in DNA occurs when a pattern of two or more nucleotides\nis repeated, and the repeated sequences are directly adjacent to each other.\nAn STR is also known as a microsatellite. The pattern can range in length from\n2 to 13 nucleotides, and the number of repeats is highly variable across\nindividuals. STR numbers can be used as genetic signatures.\")\n\nFigure 2.16: A short tandem repeat (STR) in DNA occurs when a pattern of two\nor more nucleotides is repeated, and the repeated sequences are directly\nadjacent to each other. An STR is also known as a microsatellite. The pattern\ncan range in length from 2 to 13 nucleotides, and the number of repeats is\nhighly variable across individuals. STR numbers can be used as genetic\nsignatures.\n\n[![](imgs/YSTRPositions.jpg)](imgs/YSTRPositions.jpg \"Figure 2.17: Location of\nshort tandem repeats \\(STR\\) on the human Y chromosome. Source:\nhttps://strbase.nist.gov/ystrpos1.htm\")\n\nFigure 2.17: Location of short tandem repeats (STR) on the human Y chromosome.\nSource: <https://strbase.nist.gov/ystrpos1.htm>\n\n[![](imgs/USY-STR.png)](imgs/USY-STR.png \"Figure 2.18: Y STR haplotype lookup\nfrom a database used by the FBI.\")\n\nFigure 2.18: Y STR haplotype lookup from a database used by the FBI.\n\nWe’re interested in the frequencies of particular Y-haplotypes that consist of\na set of different short tandem repeats (STR). The combination of STR numbers\nat the specific locations used for DNA forensics are labeled by the number of\nrepeats at the specific positions. Here is a short excerpt of such an STR\nhaplotype table:\n\n    \n    \n    haplo6 = read.table(\"../data/haplotype6.txt\", header = TRUE)\n    haplo6 __\n    \n    \n      Individual DYS19 DXYS156Y DYS389m DYS389n DYS389p\n    1         H1    14       12       4      12       3\n    2         H3    15       13       4      13       3\n    3         H4    15       11       5      11       3\n    4         H5    17       13       4      11       3\n    5         H7    13       12       5      12       3\n    6         H8    16       11       5      12       3\n\nThe table says that the haplotype H1 has 14 repeats at position `DYS19`, 12\nrepeats at position `DXYS156Y`, etc. Suppose we want to find the underlying\nproportion \\\\(p\\\\) of a particular haplotype in a population of interest, by\nhaplotyping \\\\(n=300\\\\) men; and suppose we found H1 in \\\\(y=40\\\\) of them. We\nare going to use the binomial distribution \\\\(B(n,p)\\\\) to model this, with\n\\\\(p\\\\) unknown.\n\nThe haplotypes created through the use of these Y-STR profiles are shared\nbetween men in the same patriarchal lineages. Thus, it is possible that two\ndifferent men share the same profile.\n\n### 2.9.2 Simulation study of the Bayesian paradigm for the binomial\ndistribution\n\nInstead of assuming that our parameter \\\\(p\\\\) has one single value (e.g., the\nmaximum likelihood estimate 40/300), the Bayesian approach allows us to see it\nas a draw from a statistical distribution. The distribution expresses our\nbelief about the possible values of the parameter \\\\(p\\\\). In principle, we\ncan use any distribution that we like whose possible values are permissible\nfor \\\\(p\\\\). As here we are looking at a parameter that expresses a proportion\nor a probability, and which takes its values between 0 and 1, it is convenient\nto use the _Beta distribution_. Its density formula is written\n\n\\\\[ f_{\\alpha,\\beta}(x) =\n\\frac{x^{\\alpha-1}\\,(1-x)^{\\beta-1}}{\\text{B}(\\alpha,\\beta)}\\quad\\text{where}\\quad\n\\text{B}(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}.\n\\tag{2.6}\\\\]\n\nWe can see in Figure 2.19 how this function depends on two parameters\n\\\\(\\alpha\\\\) and \\\\(\\beta\\\\), which makes it a very flexible family of\ndistributions (it can “fit” a lot different situations). And it has a nice\nmathematical property: if we start with a prior belief on \\\\(p\\\\) that is\nBeta-shaped, observe a dataset of \\\\(n\\\\) binomial trials, then update our\nbelief, the posterior distribution on \\\\(p\\\\) will also have a Beta\ndistribution, albeit with updated parameters. This is a mathematical fact. We\nwill not prove it here, however we demonstrate it by simulation.\n\n[![](02-chap_files/figure-html/fig-histobeta2-1.png)](02-chap_files/figure-\nhtml/fig-histobeta2-1.png \"Figure 2.19: Beta distributions with\n\\\\alpha=10,20,50 and \\\\beta=30,60,150. We can use these as a prior for\nprobability of success in a binomial experiment. These three distributions\nhave the same mean \\(\\\\frac{\\\\alpha}{\\\\alpha +\\\\beta}\\), but different\nconcentrations around the mean.\")\n\nFigure 2.19: Beta distributions with \\\\(\\alpha=10,20,50\\\\) and\n\\\\(\\beta=30,60,150\\\\). We can use these as a **prior** for probability of\nsuccess in a binomial experiment. These three distributions have the same mean\n(\\\\(\\frac{\\alpha}{\\alpha +\\beta}\\\\)), but different concentrations around the\nmean.\n\n### 2.9.3 The distribution of \\\\(Y\\\\)\n\nFor a given choice of \\\\(p\\\\), we know what the distribution of \\\\(Y\\\\) is, by\nvirtue of Equation 2.3. But what is the distribution of \\\\(Y\\\\) if \\\\(p\\\\)\nitself also varies according to some distribution? We call this the **marginal\ndistribution** of \\\\(Y\\\\). Let’s simulate that. First we generate a random\nsample `rp` of 100000 \\\\(p\\\\)s. For each of them, we then generate a random\nsample of \\\\(Y\\\\), shown in Figure 2.20. In the code below, for the sake of\ndemonstration we use the parameters 50 and 350 for the prior. Such a prior is\nalready quite informative (“peaked”) and may, e.g., reflect beliefs we have\nbased on previous studies. In Question 2.20 you have the opportunity to try\nout a “softer” (less informative) prior. We again use `vapply` to apply a\nfunction, the unnamed (anonymous) function of `x`, across all elements of `rp`\nto obtain as a result another vector `y` of the same length.\n\n    \n    \n    rp = rbeta(100000, 50, 350)\n    y = vapply(rp, \n               function(x) rbinom(1, prob = x, size = 300), \n               integer(1))\n    hist(y, breaks = 50, col = \"orange\", main = \"\", xlab = \"\")__\n\n[![](02-chap_files/figure-html/fig-\nhistmarginal-1-1.png)](02-chap_files/figure-html/fig-histmarginal-1-1.png\n\"Figure 2.20: Marginal Distribution of Y.\")\n\nFigure 2.20: Marginal Distribution of \\\\(Y\\\\).\n\n__\n\nQuestion 2.18\n\nVerify that we could have gotten the same result as in the above code chunk by\nusing R’s vectorisation capabilities and writing `rbinom(length(rp), rp, size\n= 300)`.\n\n__\n\nSolution\n\n__\n\n    \n    \n    set.seed(0xbebe)\n    y1 = vapply(rp, \n                function(x) rbinom(1, prob = x, size = 300), \n                integer(1))\n    set.seed(0xbebe)\n    y2 = rbinom(length(rp), rp, size = 300)\n    stopifnot(identical(y1, y2))__\n\n### 2.9.4 Histogram of all the \\\\(p\\\\)s such that \\\\(Y=40\\\\): the posterior\ndistribution\n\nSo now let’s compute the posterior distribution of \\\\(p\\\\) by conditioning on\nthose outcomes where \\\\(Y\\\\) was 40. We compare it to the theoretical\nposterior, `densPostTheory`, of which more below. The results are shown in\nFigure 2.21.\n\n    \n    \n    pPostEmp = rp[ y == 40 ]\n    hist(pPostEmp, breaks = 40, col = \"chartreuse4\", main = \"\",\n      probability = TRUE, xlab = \"posterior p\")\n    \n    p_seq = seq(0, 1, by = 0.001)\n    densPostTheory = dbeta(p_seq, 50 + 40, 350 + 260)\n    lines(p_seq, densPostTheory, type = \"l\", lwd = 3)__\n\n[![](02-chap_files/figure-html/fig-\ndensityposterior-1-1.png)](02-chap_files/figure-html/fig-\ndensityposterior-1-1.png \"Figure 2.21: Only choosing the values of the\ndistribution with Y=40 gives the posterior distribution of p. The histogram\n\\(green\\) shows the simulated values for the posterior distribution, the line\nthe density of a Beta distribution with the theoretical parameters.\")\n\nFigure 2.21: Only choosing the values of the distribution with \\\\(Y=40\\\\)\ngives the posterior distribution of \\\\(p\\\\). The histogram (green) shows the\nsimulated values for the posterior distribution, the line the density of a\nBeta distribution with the theoretical parameters.\n\nWe can also check the means of both distributions computed above and see that\nthey are close to 4 significant digits.\n\n    \n    \n    mean(pPostEmp)__\n    \n    \n    [1] 0.128726\n    \n    \n    dp = p_seq[2] - p_seq[1]\n    sum(p_seq * densPostTheory * dp)__\n    \n    \n    [1] 0.1285714\n\nTo approximate the mean of the theoretical density `densPostTheory`, we have\nabove literally computed the integral\n\n\\\\[ \\int_0^1 p \\, f(p) \\, dp \\\\]\n\nusing numerical integration, i.e., the `sum` over the integral. This is not\nalways convenient (or feasible), in particular if our model involves not just\na single, scalar parameter \\\\(p\\\\), but has many parameters, so that we are\ndealing with a high-dimensional parameter vector and a high-dimensional\nintegral. If the integral cannot be computed analytically, we can use **Monte\nCarlo integration**. You already saw a very simple instance of Monte Carlo\nintegration in the code above, where we sampled the posterior with `pPostEmp`\nand performed integration to compute the posterior mean by calling R’s `mean`\nfunction. In this case, an alternative Monte Carlo algorithm is to generate\nposterior samples using the `rbeta` function directly with the right\nparameters.\n\n    \n    \n    pPostMC = rbeta(n = 100000, 90, 610)\n    mean(pPostMC)__\n    \n    \n    [1] 0.1285718\n\nWe can check the concordance between the Monte Carlo samples `pPostMC` and\n`pPostEmp`, generated in slightly different ways, using a **quantile-quantile\nplot** (**QQ-plot** , Figure 2.22).\n\n    \n    \n    qqplot(pPostMC, pPostEmp, type = \"l\", asp = 1)\n    abline(a = 0, b = 1, col = \"blue\")__\n\n[![](02-chap_files/figure-html/fig-qqplotbeta-1-1.png)](02-chap_files/figure-\nhtml/fig-qqplotbeta-1-1.png \"Figure 2.22: Quantile-quantile \\(QQ\\) plot of our\nMonte Carlo sample pPostMC from the theoretical distribution and our\nsimulation sample pPostEmp. We could also similarly compare either of these\ntwo distributions to the theoretical distribution function pbeta\\(., 90,\n610\\). If the curve lies on the line y=x, this indicates a good agreement.\nThere are some random differences at the tails.\")\n\nFigure 2.22: Quantile-quantile (QQ) plot of our Monte Carlo sample `pPostMC`\nfrom the theoretical distribution and our simulation sample `pPostEmp`. We\ncould also similarly compare either of these two distributions to the\ntheoretical distribution function `pbeta(., 90, 610)`. If the curve lies on\nthe line \\\\(y=x\\\\), this indicates a good agreement. There are some random\ndifferences at the tails.\n\n__\n\nQuestion 2.19\n\nWhat is the difference between the simulation that results in `pPostEmp` and\nthe Monte Carlo simulation that leads to `pPostMC`?\n\n### 2.9.5 The posterior distribution is also a Beta\n\nNow we have seen that the posterior distribution is also a Beta. In our case\nits parameters \\\\(\\alpha=90\\\\) and \\\\(\\beta=610\\\\) were obtained by summing\nthe prior parameters \\\\(\\alpha=50\\\\), \\\\(\\beta=350\\\\) with the observed\nsuccesses \\\\(y=40\\\\) and the observed failures \\\\(n-y=260\\\\), thus obtaining\nthe posterior\n\n\\\\[ \\text{Beta}(90,\\, 610)=\\text{Beta}(\\alpha+y,\\text{Beta}+(n-y)). \\\\]\n\nWe can use it to give the best10 estimate we can for \\\\(p\\\\) with its\nuncertainty given by the posterior distribution.\n\n10 We could take the value that maximizes the posterior distribution as our\nbest estimate, this is called the **MAP** estimate, and in this case it would\nbe \\\\(\\frac{\\alpha-1}{\\alpha+\\beta-2}=\\frac{89}{698}\\doteq 0.1275\\\\).\n\n### 2.9.6 Suppose we had a second series of data\n\nAfter seeing our previous data, we now have a new prior, \\\\(\\text{Beta}(90,\n610)\\\\). Suppose we collect a new set of data with \\\\(n=150\\\\) observations\nand \\\\(y=25\\\\) successes, thus 125 failures. Now what would we take to be our\nbest guess at \\\\(p\\\\)?\n\nUsing the same reasoning as before, the new posterior will be\n\\\\(\\text{Beta}(90+25=115,\\, 610+125=735)\\\\). The mean of this distribution is\n\\\\(\\frac{115}{115+735}=\\frac{115}{850}\\simeq 0.135\\\\), thus one estimate of\n\\\\(p\\\\) would be 0.135. The **maximum a posteriori** (MAP) estimate would be\nthe mode of \\\\(\\text{Beta}(115, 735)\\\\), ie \\\\(\\frac{114}{848}\\simeq 0.134\\\\).\nLet’s check this numerically.\n\n    \n    \n    densPost2 = dbeta(p_seq, 115, 735)\n    mcPost2   = rbeta(1e6, 115, 735)\n    sum(p_seq * densPost2 * dp)   # mean, by numeric integration __\n    \n    \n    [1] 0.1352941\n    \n    \n    mean(mcPost2)                 # mean by MC __\n    \n    \n    [1] 0.1352655\n    \n    \n    p_seq[which.max(densPost2)]   # MAP estimate __\n    \n    \n    [1] 0.134\n\n__\n\nQuestion 2.20\n\nRedo all the computations replacing our original prior with a softer prior\n(less peaked), meaning that we use less prior information. For instance, try\nBeta(1,1), which is the uniform distribution. How much does this change the\nfinal result?\n\nAs a general rule, the prior rarely changes the posterior distribution\nsubstantially except if it is very peaked. This would be the case if, at the\noutset, we were already rather sure of what to expect. Another case when the\nprior has an influence is if there is very little data.\n\nThe best situation to be in is to have enough data to swamp the prior so that\nits choice doesn’t have much impact on the final result.\n\n### 2.9.7 Confidence Statements for the proportion parameter\n\nNow it is time to conclude about what the proportion \\\\(p\\\\) actually is,\ngiven the data. One summary is a posterior credibility interval, which is a\nBayesian analog of the confidence interval. We can take the 2.5 and 97.5-th\npercentiles of the posterior distribution: \\\\(P(q_{2.5\\%} \\leq p \\leq\nq_{97.5\\%})=0.95\\\\).\n\n    \n    \n    quantile(mcPost2, c(0.025, 0.975))__\n    \n    \n         2.5%     97.5% \n    0.1131080 0.1590221 \n\n[![](imgs/DESeq2-Prediction-Interval.png)](imgs/DESeq2-Prediction-Interval.png\n\"Figure 2.23: An example from @LoveDESeq2 shows plots of the likelihoods\n\\(solid lines, scaled to integrate to 1\\) and the posteriors \\(dashed lines\\)\nfor the green and purple genes and of the prior \\(solid black line\\): due to\nthe higher dispersion of the purple gene, its likelihood is wider and less\npeaked \\(indicating less information\\), and the prior has more influence on\nits posterior than for the green gene. The stronger curvature of the green\nposterior at its maximum translates to a smaller reported standard error for\nthe MAP logarithmic fold change \\(LFC\\) estimate \\(horizontal error bar\\).\")\n\nFigure 2.23: An example from Love, Huber, and Anders ([2014](16-chap.html#ref-\nLoveDESeq2)) shows plots of the likelihoods (solid lines, scaled to integrate\nto 1) and the posteriors (dashed lines) for the green and purple genes and of\nthe prior (solid black line): due to the higher dispersion of the purple gene,\nits likelihood is wider and less peaked (indicating less information), and the\nprior has more influence on its posterior than for the green gene. The\nstronger curvature of the green posterior at its maximum translates to a\nsmaller reported standard error for the MAP logarithmic fold change (LFC)\nestimate (horizontal error bar).\n\n## 2.10 Example: occurrence of a nucleotide pattern in a genome\n\nThe examples we have seen up to now have concentrated on distributions of\ndiscrete counts and categorical data. Let’s look at an example of\ndistributions of distances, which are quasi-continuous. This case study of the\ndistributions of the distances between instances of a specific motif in genome\nsequences will also allow us to explore specific genomic sequence\nmanipulations in Bioconductor.\n\nThe **[Biostrings](https://bioconductor.org/packages/Biostrings/)** package\nprovides tools for working with sequence data. The essential data structures,\nor _classes_ as they are known in R, are _DNAString_ and _DNAStringSet_. These\nenable us to work with one or multiple DNA sequences efficiently .\n\nThe **[Biostrings](https://bioconductor.org/packages/Biostrings/)** package\nalso contains additional classes for representing amino acid sequences, and\nmore general, biology-inspired sequences.\n\n    \n    \n    library(\"Biostrings\")__\n\n__\n\nQuestion 2.21\n\nExplore some of the useful data and functions provided in the Biostrings\npackage by exploring the tutorial vignette.\n\n__\n\nSolution\n\n__\n\nThe first line prints genetic code information, the second one returns IUPAC\nnucleotide ambiguity codes. The third line lists all the vignettes available\nin the **[Biostrings](https://bioconductor.org/packages/Biostrings/)**\npackage, the fourth display one particular vignette.\n\n    \n    \n    GENETIC_CODE\n    IUPAC_CODE_MAP\n    vignette(package = \"Biostrings\")\n    vignette(\"BiostringsQuickOverview\", package = \"Biostrings\")__\n\nThis last command will open a list in your browser window from which you can\naccess the documentation11. The\n**[BSgenome](https://bioconductor.org/packages/BSgenome/)** package provides\naccess to many genomes, and you can access the names of the data packages that\ncontain the whole genome sequences by typing\n\n11 Vignettes are manuals for the packages complete with examples and case\nstudies.\n\n    \n    \n    library(\"BSgenome\")\n    ag = available.genomes()\n    length(ag)__\n    \n    \n    [1] 113\n    \n    \n    ag[1:2]__\n    \n    \n    [1] \"BSgenome.Alyrata.JGI.v1\"              \n    [2] \"BSgenome.Amellifera.BeeBase.assembly4\"\n\nWe are going to explore the occurrence of the `AGGAGGT` motif12 in the genome\nof E.coli. We use the genome sequence of one particular strain, **Escherichia\ncoli** str. K12 substr.DH10B13, whose NCBI accession number is NC_010473.\n\n12 This is the [Shine-Dalgarno](https://en.wikipedia.org/wiki/Shine-\nDalgarno_sequence) motif which helps initiate protein synthesis in bacteria.\n\n13 It is known as the laboratory workhorse, often used in experiments.\n\n    \n    \n    library(\"BSgenome.Ecoli.NCBI.20080805\")\n    Ecoli\n    shineDalgarno = \"AGGAGGT\"\n    ecoli = Ecoli$NC_010473 __\n\nWe can count the pattern’s occurrence in windows of width 50000 using the\n`countPattern` function.\n\n    \n    \n    window = 50000\n    starts = seq(1, length(ecoli) - window, by = window)\n    ends   = starts + window - 1\n    numMatches = vapply(seq_along(starts), function(i) {\n      countPattern(shineDalgarno, ecoli[starts[i]:ends[i]],\n                   max.mismatch = 0)\n      }, numeric(1))\n    table(numMatches)__\n    \n    \n    numMatches\n     0  1  2  3  4 \n    48 32  8  3  2 \n\n__\n\nQuestion 2.22\n\nWhat distribution might this table fit ?\n\n__\n\nSolution\n\n__\n\nThe Poisson is a good candidate, as a quantitative and graphical evaluation\n(see Figure 2.24) for these data shows.\n\n    \n    \n    library(\"vcd\")\n    gf = goodfit(numMatches, \"poisson\")\n    summary(gf)__\n    \n    \n         Goodness-of-fit test for poisson distribution\n    \n                          X^2 df  P(> X^2)\n    Likelihood Ratio 4.134932  3 0.2472577\n    \n    \n    distplot(numMatches, type = \"poisson\")__\n\n[![](02-chap_files/figure-html/fig-poissonness-1.png)](02-chap_files/figure-\nhtml/fig-poissonness-1.png \"Figure 2.24: Evaluation of a Poisson model for\nmotif counts along the sequence Ecoli$NC_010473.\")\n\nFigure 2.24: Evaluation of a Poisson model for motif counts along the sequence\n`Ecoli$NC_010473`.\n\nWe can inspect the matches using the `matchPattern` function.\n\n    \n    \n    sdMatches = matchPattern(shineDalgarno, ecoli, max.mismatch = 0)__\n\nYou can type `sdMatches` in the R command line to obtain a summary of this\nobject. It contains the locations of all 65 pattern matches, represented as a\nset of so-called _views_ on the original sequence. Now what are the distances\nbetween them?\n\n    \n    \n    betweenmotifs = gaps(sdMatches)__\n\nSo these are in fact the 66 complementary regions. Now let’s find a model for\nthe distribution of the gap sizes between motifs. If the motifs occur at\nrandom locations, we expect the gap lengths to follow an exponential\ndistribution14. The code below (whose output is shown in Figure 2.25) assesses\nthis assumption. If the exponential distribution is a good fit, the points\nshould lie roughly on a straight line. The exponential distribution has one\nparameter, the rate, and the line with slope corresponding to an estimate from\nthe data is also shown.\n\n14 How could we guess that the exponential is the right fit here? Whenever we\nhave independent, random Bernoulli occurrences along a sequence, the gap\nlengths are exponential. You may be familiar with radioactive decay, where the\nwaiting times between emissions are also exponentially distributed. It is a\ngood idea if you are not familiar with this distribution to look up more\ndetails in the\n[Wikipedia](http://en.wikipedia.org/wiki/Exponential_distribution).\n\n    \n    \n    library(\"Renext\")\n    expplot(width(betweenmotifs), rate = 1/mean(width(betweenmotifs)),\n            labels = \"fit\")__\n\n[![](02-chap_files/figure-html/fig-expplotdata-1-1.png)](02-chap_files/figure-\nhtml/fig-expplotdata-1-1.png \"Figure 2.25: Evaluation of fit to the\nexponential distribution \\(black line\\) of the gaps between the motifs.\")\n\nFigure 2.25: Evaluation of fit to the exponential distribution (black line) of\nthe gaps between the motifs.\n\n__\n\nQuestion 2.23\n\nThere appears to be a slight deviation from the fitted line in Figure 2.25 at\nthe right tail of the distribution, i.e., for the largest values. What could\nbe the reason?\n\n### 2.10.1 Modeling in the case of dependencies\n\nAs we saw in Section 2.8, nucleotide sequences are often dependent: the\nprobability of seing a certain nucleotide at a given position tends to depend\non the surrounding sequence. Here we are going to put into practice dependency\nmodeling using a **Markov chain**. We are going to look at regions of\nchromosome 8 of the human genome and try to discover differences between\nregions called CpG15 islands and the rest.\n\n15 CpG stands for 5’-C-phosphate-G-3’; this means that a C is connected to a G\nthrough a phosphate along the strand (this is unrelated to C-G base-pairing of\nSection 2.7). The cytosines in the CpG dinucleotide can be methylated,\nchanging the levels of gene expression. This type of gene regulation is part\nof **epigenetics**. Some more information is on Wikipedia: [CpG\nsite](https://en.wikipedia.org/wiki/CpG_site) and\n[epigenetics](https://en.wikipedia.org/wiki/Epigenetics).\n\nWe use data (generated by Irizarry, Wu, and Feinberg ([2009](16-chap.html#ref-\nIrizarry2009))) that tell us where the start and end points of the islands are\nin the genome and look at the frequencies of nucleotides and of the digrams\n‘CG’, ‘CT’, ‘CA’, ‘CC’. So we can ask whether there are dependencies between\nthe nucleotide occurrences and if so, how to model them.\n\n    \n    \n    library(\"BSgenome.Hsapiens.UCSC.hg19\")\n    chr8  =  Hsapiens$chr8\n    CpGtab = read.table(\"../data/model-based-cpg-islands-hg19.txt\",\n                        header = TRUE)\n    nrow(CpGtab)__\n    \n    \n    [1] 65699\n    \n    \n    head(CpGtab)__\n    \n    \n        chr  start    end length CpGcount GCcontent pctGC obsExp\n    1 chr10  93098  93818    721       32       403 0.559  0.572\n    2 chr10  94002  94165    164       12        97 0.591  0.841\n    3 chr10  94527  95302    776       65       538 0.693  0.702\n    4 chr10 119652 120193    542       53       369 0.681  0.866\n    5 chr10 122133 122621    489       51       339 0.693  0.880\n    6 chr10 180265 180720    456       32       256 0.561  0.893\n    \n    \n    irCpG = with(dplyr::filter(CpGtab, chr == \"chr8\"),\n             IRanges(start = start, end = end))__\n\n[![We use the :: operator to call the filter function specifically from the\ndplyr package—and not from any other packages that may happen to be loaded and\ndefining functions of the same name. This precaution is particularly advisable\nin the case of the filter function, since this name is used by quite a few\nother packages. You can think of the normal \\(without ::\\) way of calling R\nfunctions like calling people by their first \\(given\\) names; whereas the\nfully qualified version with :: corresponds to calling someone by their full\nname. At least within the reach of the CRAN and Bioconductor repositories,\nsuch fully qualified names are guaranteed to be\nunique.](imgs/devil.png)](imgs/devil.png \"We use the :: operator to call the\nfilter function specifically from the dplyr package—and not from any other\npackages that may happen to be loaded and defining functions of the same name.\nThis precaution is particularly advisable in the case of the filter function,\nsince this name is used by quite a few other packages. You can think of the\nnormal \\(without ::\\) way of calling R functions like calling people by their\nfirst \\(given\\) names; whereas the fully qualified version with :: corresponds\nto calling someone by their full name. At least within the reach of the CRAN\nand Bioconductor repositories, such fully qualified names are guaranteed to be\nunique.\")\n\nWe use the `::` operator to call the `filter` function specifically from the\n`dplyr` package—and not from any other packages that may happen to be loaded\nand defining functions of the same name. This precaution is particularly\nadvisable in the case of the `filter` function, since this name is used by\nquite a few other packages. You can think of the normal (without `::`) way of\ncalling R functions like calling people by their first (given) names; whereas\nthe fully qualified version with `::` corresponds to calling someone by their\nfull name. At least within the reach of the CRAN and Bioconductor\nrepositories, such fully qualified names are guaranteed to be unique.\n\nIn the line above, we subset (`filter`) the data frame `CpGtab` to only\nchromosome 8, and then we create an _IRanges_ object whose start and end\npositions are defined by the equally named columns of the data frame. In the\n`IRanges` function call (which constructs the object from its arguments), the\nfirst `start` is the argument name of the function, the second `start` refers\nto the column in the data frame obtained as an output from `filter`; and\nsimilarly for `end`. _IRanges_ is a general container for mathematical\nintervals. We create the biological context16 with the next line.\n\n16 The “I” in _IRanges_ stands for “interval”; the “G” in _GRanges_ for\n“genomic”.\n\n    \n    \n    grCpG = GRanges(ranges = irCpG, seqnames = \"chr8\", strand = \"+\")\n    genome(grCpG) = \"hg19\"__\n\nNow let’s visualize; see the output in Figure 2.26.\n\n    \n    \n    library(\"Gviz\")\n    ideo = IdeogramTrack(genome = \"hg19\", chromosome = \"chr8\")\n    plotTracks(\n      list(GenomeAxisTrack(),\n        AnnotationTrack(grCpG, name = \"CpG\"), ideo),\n        from = 2200000, to = 5800000,\n        shape = \"box\", fill = \"#006400\", stacking = \"dense\")__\n\n[![](02-chap_files/figure-html/fig-freqandbayes-\nideo-1.png)](02-chap_files/figure-html/fig-freqandbayes-ideo-1.png\n\"Figure 2.26: Gviz plot of CpG locations in a selected region of chromosome\n8.\")\n\nFigure 2.26: **[Gviz](https://bioconductor.org/packages/Gviz/)** plot of CpG\nlocations in a selected region of chromosome 8.\n\nWe now define so-called views on the chromosome sequence that correspond to\nthe CpG islands, `irCpG`, and to the regions in between (`gaps(irCpG)`). The\nresulting objects `CGIview` and `NonCGIview` only contain the coordinates, not\nthe sequences themselves (these stay in the big object `Hsapiens$chr8`), so\nthey are fairly lightweight in terms of storage.\n\n    \n    \n    CGIview    = Views(unmasked(Hsapiens$chr8), irCpG)\n    NonCGIview = Views(unmasked(Hsapiens$chr8), gaps(irCpG))__\n\nWe compute transition counts in CpG islands and non-islands using the data.\n\n    \n    \n    seqCGI      = as(CGIview, \"DNAStringSet\")\n    seqNonCGI   = as(NonCGIview, \"DNAStringSet\")\n    dinucCpG    = sapply(seqCGI, dinucleotideFrequency)\n    dinucNonCpG = sapply(seqNonCGI, dinucleotideFrequency)\n    dinucNonCpG[, 1]__\n    \n    \n     AA  AC  AG  AT  CA  CC  CG  CT  GA  GC  GG  GT  TA  TC  TG  TT \n    389 351 400 436 498 560 112 603 359 336 403 336 330 527 519 485 \n    \n    \n    NonICounts = rowSums(dinucNonCpG)\n    IslCounts  = rowSums(dinucCpG)__\n\nFor a four state Markov chain as we have, we define the transition matrix as a\nmatrix where the rows are the `from` state and the columns are the `to` state.\n\n    \n    \n    TI  = matrix( IslCounts, ncol = 4, byrow = TRUE)\n    TnI = matrix(NonICounts, ncol = 4, byrow = TRUE)\n    dimnames(TI) = dimnames(TnI) =\n      list(c(\"A\", \"C\", \"G\", \"T\"), c(\"A\", \"C\", \"G\", \"T\"))__\n\nWe use the counts of numbers of transitions of each type to compute\nfrequencies and put them into two matrices.\n\n[![The transition probabilities are probabilities so the rows need to sum to\n1.](imgs/devil.png)](imgs/devil.png \"The transition probabilities are\nprobabilities so the rows need to sum to 1.\")\n\nThe transition probabilities are probabilities so the rows need to sum to 1.\n\n    \n    \n    MI = TI /rowSums(TI)\n    MI __\n    \n    \n               A         C         G         T\n    A 0.20457773 0.2652333 0.3897678 0.1404212\n    C 0.20128250 0.3442381 0.2371595 0.2173200\n    G 0.18657245 0.3145299 0.3450223 0.1538754\n    T 0.09802105 0.3352314 0.3598984 0.2068492\n    \n    \n    MN = TnI / rowSums(TnI)\n    MN __\n    \n    \n              A         C          G         T\n    A 0.3351380 0.1680007 0.23080886 0.2660524\n    C 0.3641054 0.2464366 0.04177094 0.3476871\n    G 0.2976696 0.2029017 0.24655406 0.2528746\n    T 0.2265813 0.1972407 0.24117528 0.3350027\n\n__\n\nQuestion 2.24\n\nAre the transitions different in the different rows? This would mean that, for\ninstance, \\\\(P(\\mathtt{A}\\,|\\,\\mathtt{C}) \\neq\nP(\\mathtt{A}\\,|\\,\\mathtt{T})\\\\).\n\n__\n\nSolution\n\n__\n\nThe transitions are different. For instance, the transitions from C to A and T\nto A for in the islands (MI) transition matrix seem very different (0.201\nversus 0.098).\n\n__\n\nQuestion 2.25\n\nAre the relative frequencies of the different nucleotides different in CpG\nislands compared to elsewhere?\n\n__\n\nSolution\n\n__\n\n    \n    \n    freqIsl = alphabetFrequency(seqCGI, baseOnly = TRUE, collapse = TRUE)[1:4]\n    freqIsl / sum(freqIsl)__\n    \n    \n            A         C         G         T \n    0.1781693 0.3201109 0.3206298 0.1810901 \n    \n    \n    freqNon = alphabetFrequency(seqNonCGI, baseOnly = TRUE, collapse = TRUE)[1:4]\n    freqNon / sum(freqNon)__\n    \n    \n            A         C         G         T \n    0.3008292 0.1993832 0.1993737 0.3004139 \n\nThis shows an inverse pattern: in the CpG islands, C and G have frequencies\naround 0.32, whereas in the non-CpG islands, we have A and T that have\nfrequencies around 0.30.\n\n__\n\nQuestion 2.26\n\nHow can we use these differences to decide whether a given sequence comes from\na CpG island?\n\n__\n\nSolution\n\n__\n\nUse a \\\\(\\chi^2\\\\) statistic to compare the frequencies between the observed\nand `freqIsl` and `freqNon` frequencies. For shorter sequences, this may not\nbe sensitive enough, and a more sensitive approach is given below.\n\nGiven a sequence for which we do not know whether it is in a CpG island or\nnot, we can ask what is the probability it belongs to a CpG island compared to\nsomewhere else. We compute a score based on what is called the odds ratio.\nLet’s do an example: suppose our sequence \\\\(x\\\\) is `ACGTTATACTACG`, and we\nwant to decide whether it comes from a CpG island or not.\n\nIf we model the sequence as a first order Markov chain we can write, supposing\nthat the sequence comes from a CpG island:\n\n\\\\[ \\begin{align} P_{\\text{i}}(x = \\mathtt{ACGTTATACTACG}) = \\;\n&P_{\\text{i}}(\\mathtt{A}) \\, P_{\\text{i}}(\\mathtt{AC})\\,\nP_{\\text{i}}(\\mathtt{CG})\\, P_{\\text{i}}(\\mathtt{GT})\\,\nP_{\\text{i}}(\\mathtt{TT}) \\times \\\\\\ &P_{\\text{i}}(\\mathtt{TA})\\,\nP_{\\text{i}}(\\mathtt{AT})\\, P_{\\text{i}}(\\mathtt{TA})\\,\nP_{\\text{i}}(\\mathtt{AC})\\, P_{\\text{i}}(\\mathtt{CG}). \\end{align} \\\\]\n\nWe are going to compare this probability to the probability for non-islands.\nAs we saw above, these probabilities tend to be quite different. We will take\ntheir ratio and see if it is larger or smaller than 1. These probabilities are\ngoing to be products of many small terms and become very small. We can work\naround this by taking logarithms.\n\n\\\\[ \\begin{align} \\log&\\frac{P(x\\,|\\, \\text{island})}{P(x\\,|\\,\\text{non-\nisland})}=\\\\\\ \\log&\\left( \\frac{P_{\\text{i}}(\\mathtt{A})\\,\nP_{\\text{i}}(\\mathtt{A}\\rightarrow \\mathtt{C})\\,\nP_{\\text{i}}(\\mathtt{C}\\rightarrow \\mathtt{G})\\,\nP_{\\text{i}}(\\mathtt{G}\\rightarrow \\mathtt{T})\\,\nP_{\\text{i}}(\\mathtt{T}\\rightarrow \\mathtt{T})\\,\nP_{\\text{i}}(\\mathtt{T}\\rightarrow \\mathtt{A})} {P_{\\text{n}}(\\mathtt{A})\\,\nP_{\\text{n}}(\\mathtt{A}\\rightarrow \\mathtt{C})\\,\nP_{\\text{n}}(\\mathtt{C}\\rightarrow \\mathtt{G})\\,\nP_{\\text{n}}(\\mathtt{G}\\rightarrow \\mathtt{T})\\, P_{\\text{n}}(\n\\mathtt{T}\\rightarrow \\mathtt{T})\\, P_{\\text{n}}( \\mathtt{T}\\rightarrow\n\\mathtt{A})} \\right. \\times\\\\\\ &\\left.\n\\frac{P_{\\text{i}}(\\mathtt{A}\\rightarrow \\mathtt{T})\\,\nP_{\\text{i}}(\\mathtt{T}\\rightarrow \\mathtt{A})\\,\nP_{\\text{i}}(\\mathtt{A}\\rightarrow \\mathtt{C})\\,\nP_{\\text{i}}(\\mathtt{C}\\rightarrow \\mathtt{G})}\n{P_{\\text{n}}(\\mathtt{A}\\rightarrow \\mathtt{T})\\,\nP_{\\text{n}}(\\mathtt{T}\\rightarrow \\mathtt{A})\\,\nP_{\\text{n}}(\\mathtt{A}\\rightarrow \\mathtt{C})\\,\nP_{\\text{n}}(\\mathtt{C}\\rightarrow \\mathtt{G})} \\right) \\end{align}\n\\tag{2.7}\\\\]\n\nThis is the **log-likelihood ratio** score. To speed up the calculation, we\ncompute the log-ratios\n\\\\(\\log(P_{\\text{i}}(\\mathtt{A})/P_{\\text{n}}(\\mathtt{A})),...,\n\\log(P_{\\text{i}}(\\mathtt{T}\\rightarrow\n\\mathtt{A})/P_{\\text{n}}(\\mathtt{T}\\rightarrow \\mathtt{A}))\\\\) once and for\nall and then sum up the relevant ones to obtain our score.\n\n[![Worked out examples and many useful details can be found in Durbin et al.\n\\(1998\\).](imgs/book_icon.png)](imgs/book_icon.png \"Worked out examples and\nmany useful details can be found in @DEKM.\")\n\nWorked out examples and many useful details can be found in Durbin et al.\n([1998](16-chap.html#ref-DEKM)).\n\n    \n    \n    alpha = log((freqIsl/sum(freqIsl)) / (freqNon/sum(freqNon)))\n    beta  = log(MI / MN)__\n    \n    \n    x = \"ACGTTATACTACG\"\n    scorefun = function(x) {\n      s = unlist(strsplit(x, \"\"))\n      score = alpha[s[1]]\n      if (length(s) >= 2)\n        for (j in 2:length(s))\n          score = score + beta[s[j-1], s[j]]\n      score\n    }\n    scorefun(x)__\n    \n    \n             A \n    -0.2824623 \n\nIn the code below, we pick sequences of length `len = 100` out of the 2855\nsequences in the `seqCGI` object, and then out of the 2854 sequences in the\n`seqNonCGI` object (each of them is a _DNAStringSet_). In the first three\nlines of the `generateRandomScores` function, we drop sequences that contain\nany letters other than A, C, T, G; such as “.” (a character used for undefined\nnucleotides). Among the remaining sequences, we sample with probabilities\nproportional to their length minus `len` and then pick subsequences of length\n`len` out of them. The start points of the subsequences are sampled uniformly,\nwith the constraint that the subsequences have to fit in.\n\n    \n    \n    generateRandomScores = function(s, len = 100, B = 1000) {\n      alphFreq = alphabetFrequency(s)\n      isGoodSeq = rowSums(alphFreq[, 5:ncol(alphFreq)]) == 0\n      s = s[isGoodSeq]\n      slen = sapply(s, length)\n      prob = pmax(slen - len, 0)\n      prob = prob / sum(prob)\n      idx  = sample(length(s), B, replace = TRUE, prob = prob)\n      ssmp = s[idx]\n      start = sapply(ssmp, function(x) sample(length(x) - len, 1))\n      scores = sapply(seq_len(B), function(i)\n        scorefun(as.character(ssmp[[i]][start[i]+(1:len)]))\n      )\n      scores / len\n    }\n    scoresCGI    = generateRandomScores(seqCGI)\n    scoresNonCGI = generateRandomScores(seqNonCGI)__\n    \n    \n    rgs = range(c(scoresCGI, scoresNonCGI))\n    br = seq(rgs[1], rgs[2], length.out = 50)\n    h1 = hist(scoresCGI,    breaks = br, plot = FALSE)\n    h2 = hist(scoresNonCGI, breaks = br, plot = FALSE)\n    plot(h1, col = rgb(0, 0, 1, 1/4), xlim = c(-0.5, 0.5), ylim=c(0,120))\n    plot(h2, col = rgb(1, 0, 0, 1/4), add = TRUE)__\n\n[![](02-chap_files/figure-html/fig-\nScoreMixture-1-1.png)](02-chap_files/figure-html/fig-ScoreMixture-1-1.png\n\"Figure 2.27: Island and non-island scores as generated by the function\ngenerateRandomScores. This is the first instance of a mixture we encounter. We\nwill revisit them in sec-mixtures.\")\n\nFigure 2.27: Island and non-island scores as generated by the function\n`generateRandomScores`. This is the first instance of a **mixture** we\nencounter. We will revisit them in [Chapter 4](04-chap.html).\n\nWe can consider these our _training data_ : from data for which we know the\ntypes, we can see whether our score is useful for discriminating – see Figure\n2.27.\n\n## 2.11 Summary of this chapter\n\nIn this chapter we experienced the basic yoga of statistics: how to go from\nthe data back to the possible generating distributions and how to estimate the\nparameters that define these distributions.\n\n**Statistical models** We showed some specific statistical models for\nexperiments with categorical outcomes (binomial and multinomial).\n\n**Goodness of fit** We used different visualizations and showed how to run\nsimulation experiments to test whether our data could be fit by a fair four\nbox multinomial model. We encountered the chi-square statistic and saw how to\ncompare simulation and theory using a qq-plot.\n\n**Estimation** We explained maximum likelihood and Bayesian estimation\nprocedures. These approaches were illustrated on examples involving nucleotide\npattern discovery and haplotype estimations.\n\n**Prior and posterior distributions** When assessing data of a type that has\nbeen been previously studied, such as haplotypes, it can be beneficial to\ncompute the posterior distribution of the data. This enables us to incorporate\nuncertainty in the decision making, by way of a simple computation. The choice\nof the prior has little effect on the result as long as there is sufficient\ndata.\n\n**CpG islands and Markov chains** We saw how dependencies along DNA sequences\ncan be modeled by Markov chain transitions. We used this to build scores based\non likelihood ratios that enable us to see whether long DNA sequences come\nfrom CpG islands or not. When we made the histogram of scores, we saw in\nFigure 2.27 a noticeable feature: it seemed to be made of two pieces. This\n**bimodality** was our first encounter with mixtures, they are the subject of\n[Chapter 4](04-chap.html).\n\nThis is the first instance of building a model on some training data:\nsequences which we knew were in CpG islands, that we could use later to\nclassify new data. We will develop a much more complete way of doing this in\n[Chapter 12](12-chap.html).\n\n## 2.12 Further reading\n\nOne of the best introductory statistics books available is Freedman, Pisani,\nand Purves ([1997](16-chap.html#ref-Freedman:1997)). It uses box models to\nexplain the important concepts. If you have never taken a statistics class, or\nyou feel you need a refresher, we highly recommend it. Many introductory\nstatistics classes do not cover statistics for discrete data in any depth. The\nsubject is an important part of what we need for biological applications. A\nbook-long introduction to these types of analyses can be found in ([Agresti\n2007](16-chap.html#ref-Agresti:2007)).\n\nHere we gave examples of simple unstructured multinomials. However, sometimes\nthe categories (or boxes) of a multinomial have specific structure. For\ninstance, the 64 possible codons code for 20 amino acids and the stop codons\n(61+3). So we can see the amino acids themselves as a multinomial with 20\ndegrees of freedom. Within each amino acid there are multinomials with\ndiffering numbers of categories (Proline has four: `CCA, CCG, CCC, CCT`, see\nExercise 2.3). Some multivariate methods have been specifically designed to\ndecompose the variability between codon usage within the differently abundant\namino-acids ([Grantham et al. 1981](16-chap.html#ref-Grantham1981); [Perrière\nand Thioulouse 2002](16-chap.html#ref-Perriere2002)), and this enables\ndiscovery of latent gene transfer and translational selection. We will cover\nthe specific methods used in those papers when we delve into the multivariate\nexploration of categorical data in [Chapter 9](09-chap.html).\n\nThere are many examples of successful uses of the Bayesian paradigm to\nquantify uncertainties. In recent years the computation of the posterior\ndistribution has been revolutionized by special types of Monte Carlo that use\neither a Markov chain or random walk or Hamiltonian dynamics. These methods\nprovide approximations that converge to the correct posterior distribution\nafter quite a few iterations. For examples and much more see ([Robert and\nCasella 2009](16-chap.html#ref-Casella2009); [Marin and Robert\n2007](16-chap.html#ref-Marin2007); [McElreath 2015](16-chap.html#ref-\nMcElreath2015)).\n\n## 2.13 Exercises\n\n__\n\nExercise 2.1\n\nGenerate 1,000 random 0/1 variables that model mutations occurring along a\n1,000 long gene sequence. These occur independently at a rate of \\\\(10^{-4}\\\\)\neach. Then sum the 1,000 positions to count how many mutations in sequences of\nlength 1,000.\n\nFind the correct distribution for these mutation sums using a goodness of fit\ntest and make a plot to visualize the quality of the fit.\n\n__\n\nExercise 2.2\n\nMake a function that generates \\\\(n\\\\) random uniform numbers between \\\\(0\\\\)\nand \\\\(7\\\\) and returns their maximum. Execute the function for \\\\(n=25\\\\).\nRepeat this procedure \\\\(B=100\\\\) times. Plot the distribution of these\nmaxima.  \nWhat is the maximum likelihood estimate of the maximum of a sample of size 25\n(call it \\\\(\\hat{\\theta}\\\\))?  \nCan you find a theoretical justification and the true maximum \\\\(\\theta\\\\)?\n\n__\n\nExercise 2.3\n\nA sequence of three nucleotides (a **codon**) taken in a coding region of a\ngene can be transcribed into one of 20 possible amino acids. There are\n\\\\(4^3=64\\\\) possible codon sequences, but only 20 amino acids. We say the\n**genetic code** is redundant: there are several ways to _spell_ each amino\nacid.\n\nThe multiplicity (the number of codons that code for the same amino acid)\nvaries from 2 to 6. The different codon-spellings of each amino acid do not\noccur with equal probabilities. Let’s look at the data for the standard\nlaboratory strain of tuberculosis (H37Rv):\n\n    \n    \n    mtb = read.table(\"../data/M_tuberculosis.txt\", header = TRUE)\n    head(mtb, n = 4)__\n    \n    \n      AmAcid Codon Number PerThous\n    1    Gly   GGG  25874    19.25\n    2    Gly   GGA  13306     9.90\n    3    Gly   GGT  25320    18.84\n    4    Gly   GGC  68310    50.82\n\nThe codons for the amino acid proline are of the form \\\\(CC*\\\\), and they\noccur with the following frequencies in Mycobacterium turberculosis:\n\n    \n    \n    pro  =  mtb[ mtb$AmAcid == \"Pro\", \"Number\"]\n    pro/sum(pro)__\n    \n    \n    [1] 0.54302025 0.10532985 0.05859765 0.29305225\n\n  1. Explore the data `mtb` using `table` to tabulate the `AmAcid` and `Codon` variables.\n\n  2. How was the `PerThous` variable created?\n\n  3. Write an R function that you can apply to the table to find which of the amino acids shows the strongest **codon bias** , i.e., the strongest departure from uniform distribution among its possible spellings.\n\n\\\\(*\\\\) stands for any of the 4 letters, using the computer notation for a\nregular expression.\n\n__\n\nExercise 2.4\n\nDisplay GC content in a running window along the sequence of _Staphylococcus\nAureus_. Read in a _fasta_ file sequence from a file.\n\n    \n    \n    staph = readDNAStringSet(\"../data/staphsequence.ffn.txt\", \"fasta\")__\n\n  1. Look at the complete `staph` object and then display the first three sequences in the set.\n\n  2. Find the GC content along the sequence in sliding windows of width 100.\n\n  3. Display the result of b).\n\n  4. How could we visualize the overall trends of these proportions along the sequence?\n\n__\n\nSolution\n\n__\n\n  1. The data is displayed using:\n\n    \n    \n    staph[1:3, ]__\n    \n    \n    DNAStringSet object of length 3:\n        width seq                                               names               \n    [1]  1362 ATGTCGGAAAAAGAAATTTGGGA...AAAAAGAAATAAGAAATGTATAA lcl|NC_002952.2_c...\n    [2]  1134 ATGATGGAATTCACTATTAAAAG...TTTTACCAATCAGAACTTACTAA lcl|NC_002952.2_c...\n    [3]   246 GTGATTATTTTGGTTCAAGAAGT...TCATTCATCAAGGTGAACAATGA lcl|NC_002952.2_c...\n    \n    \n    staph __\n    \n    \n    DNAStringSet object of length 2650:\n           width seq                                            names               \n       [1]  1362 ATGTCGGAAAAAGAAATTTGGG...AAAGAAATAAGAAATGTATAA lcl|NC_002952.2_c...\n       [2]  1134 ATGATGGAATTCACTATTAAAA...TTACCAATCAGAACTTACTAA lcl|NC_002952.2_c...\n       [3]   246 GTGATTATTTTGGTTCAAGAAG...ATTCATCAAGGTGAACAATGA lcl|NC_002952.2_c...\n       [4]  1113 ATGAAGTTAAATACACTCCAAT...CAAGGTGAAATTATAAAGTAA lcl|NC_002952.2_c...\n       [5]  1932 GTGACTGCATTGTCAGATGTAA...TATGCAAACTTAGACTTCTAA lcl|NC_002952.2_c...\n       ...   ... ...\n    [2646]   720 ATGACTGTAGAATGGTTAGCAG...ACTCCTTTACTTGAAAAATAA lcl|NC_002952.2_c...\n    [2647]  1878 GTGGTTCAAGAATATGATGTAA...CTCCAAAGGGTGAGTGACTAA lcl|NC_002952.2_c...\n    [2648]  1380 ATGGATTTAGATACAATTACGA...CAATTCTGCTTAGGTAAATAG lcl|NC_002952.2_c...\n    [2649]   348 TTGGAAAAAGCTTACCGAATTA...TTTAATAAAAAGATTAAGTAA lcl|NC_002952.2_c...\n    [2650]   138 ATGGTAAAACGTACTTATCAAC...CGTAAAGTTTTATCTGCATAA lcl|NC_002952.2_c...\n\n  2. We can compute the frequencies using the function `letterFrequency`.\n\n    \n    \n    letterFrequency(staph[[1]], letters = \"ACGT\", OR = 0)__\n    \n    \n      A   C   G   T \n    522 219 229 392 \n    \n    \n    GCstaph = data.frame(\n      ID = names(staph),\n      GC = rowSums(alphabetFrequency(staph)[, 2:3] / width(staph)) * 100\n    )__\n\n  3. Plotting can be done as follows, here exemplarily for sequence 364 (Figure 2.28):\n\n    \n    \n    window = 100\n    gc = rowSums( letterFrequencyInSlidingView(staph[[364]], window,\n          c(\"G\",\"C\")))/window\n    plot(x = seq(along = gc), y = gc, type = \"l\")__\n\n[![](02-chap_files/figure-html/fig-SlidingGC-1-1.png)](02-chap_files/figure-\nhtml/fig-SlidingGC-1-1.png \"Figure 2.28: GC content along sequence 364 of the\nStaphylococcus Aureus genome.\")\n\nFigure 2.28: GC content along sequence 364 of the _Staphylococcus Aureus_\ngenome.\n\n  4. We can look at the overall trends by smoothing the data using the function `lowess` along a window.\n\n    \n    \n    plot(x = seq(along = gc), y = gc, type = \"l\")\n    lines(lowess(x = seq(along = gc), y = gc, f = 0.2), col = 2)__\n\n[![](02-chap_files/figure-html/fig-\nSmoothSlidingGC-1-1.png)](02-chap_files/figure-html/fig-\nSmoothSlidingGC-1-1.png \"Figure 2.29: Similar to Figure fig-SlidingGC-1, with\nsmoothing.\")\n\nFigure 2.29: Similar to Figure 2.28, with smoothing.\n\nWe will see later an appropriate way of deciding whether the window has an\nabnormally high GC content by using the idea that as we move along the\nsequences, we are always in one of several possible **states**. However, we\ndon’t directly observe the state, just the sequence. Such models are called\n**hidden (state) Markov models** , or HMM for short (see\n[Wikipedia](http://en.wikipedia.org/wiki/Hidden_Markov_model)). The _Markov_\nin the name of these models is for how they model dependencies between\nneighboring positions, the _hidden_ part indicates that the state is not\ndirectly observed, that is, hidden.\n\n__\n\nExercise 2.5\n\nRedo a figure similar to Figure 2.19, but include two other distributions: the\nuniform (which is Beta(1,1)) and Beta(\\\\(\\frac{1}{2},\\frac{1}{2}\\\\)). What do\nyou notice?\n\n__\n\nSolution\n\n__\n\n    \n    \n    dfbetas = data.frame(\n      p = rep(p_seq, 5),\n      dbeta = c(dbeta(p_seq, 0.5, 0.5), \n                dbeta(p_seq,   1,   1), \n                dbeta(p_seq,  10,  30),\n                dbeta(p_seq,  20,  60), \n                dbeta(p_seq,  50, 150)),\n      pars = rep(c(\"Beta(0.5,0.5)\", \"U(0,1)=Beta(1,1)\", \n                   \"Beta(10,30)\", \"Beta(20,60)\", \n                   \"Beta(50,150)\"), each = length(p_seq)))\n    ggplot(dfbetas) +\n      geom_line(aes(x = p, y = dbeta, colour = pars)) +\n      theme(legend.title = element_blank()) +\n      geom_vline(aes(xintercept = 0.25), colour = \"#990000\", linetype = \"dashed\")__\n\n[![](02-chap_files/figure-html/fig-histobeta4-1-1.png)](02-chap_files/figure-\nhtml/fig-histobeta4-1-1.png \"Figure 2.30: Beta densities for different\nparameter choices.\")\n\nFigure 2.30: Beta densities for different parameter choices.\n\nWhereas the Beta distributions with parameters larger than one are unimodal,\nthe Beta(0.5,0.5) distribution is bimodal and the Beta(1,1) is flat and has no\nmode.\n\n__\n\nExercise 2.6\n\nChoose your own prior for the parameters of the Beta distribution. You can do\nthis by sketching it here:\n<https://jhubiostatistics.shinyapps.io/drawyourprior>. Once you have set up a\nprior, re-analyse the data from Section 2.9.1, where we saw \\\\(Y = 40\\\\)\nsuccesses out of \\\\(n=300\\\\) trials. Compare your posterior distribution to\nthe one we obtained in that section using a QQ-plot.\n\nAgresti, Alan. 2007. _An Introduction to Categorical Data Analysis_. John\nWiley.\n\nCannings, Chris, and Anthony WF Edwards. 1968. “Natural Selection and the de\nFinetti Diagram.” _Annals of Human Genetics_ 31 (4): 421–28.\n\nCleveland, William S. 1988. _The Collected Works of John w. Tukey: Graphics\n1965-1985_. Vol. 5. CRC Press.\n\nDurbin, Richard, Sean Eddy, Anders Krogh, and Graeme Mitchison. 1998.\n_Biological Sequence Analysis_. Cambridge University Press.\n\nElson, D, and E Chargaff. 1952. “On the Desoxyribonucleic Acid Content of Sea\nUrchin Gametes.” _Experientia_ 8 (4): 143–45.\n\nFinetti, Bruno de. 1926. “Considerazioni Matematiche Sull’ereditarieta\nMendeliana.” _Metron_ 6: 3–41.\n\nFreedman, David, Robert Pisani, and Roger Purves. 1997. _Statistics_. New\nYork, NY: WW Norton.\n\nGrantham, Richard, Christian Gautier, Manolo Gouy, M Jacobzone, and R Mercier.\n1981. “Codon Catalog Usage Is a Genome Strategy Modulated for Gene\nExpressivity.” _Nucleic Acids Research_ 9 (1): 213–13.\n\nIrizarry, Rafael A, Hao Wu, and Andrew P Feinberg. 2009. “A Species-\nGeneralized Probabilistic Model-Based Definition of CpG Islands.” _Mammalian\nGenome_ 20 (9-10): 674–80.\n\nLove, Michael I, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation\nof Fold Change and Dispersion for RNA-seq Data with DESeq2.” _Gnome Biology_\n15 (12): 1–21.\n\nMarin, Jean-Michel, and Christian Robert. 2007. _Bayesian Core: A Practical\nApproach to Computational Bayesian Statistics_. Springer Science & Business\nMedia.\n\nMcElreath, Richard. 2015. _Statistical Rethinking: A Bayesian Course with\nExamples in R and Stan_. Chapman; Hall/CRC.\n\nMourant, AE, Ada Kopec, and K Domaniewska-Sobczak. 1976. “The Distribution of\nthe Human Blood Groups 2nd Edition.” Oxford University Press London.\n\nPerrière, Guy, and Jean Thioulouse. 2002. “Use and Misuse of Correspondence\nAnalysis in Codon Usage Studies.” _Nucleic Acids Research_ 30 (20): 4548–55.\n\nRice, John. 2006. _Mathematical Statistics and Data Analysis_. Cengage\nLearning.\n\nRobert, Christian, and George Casella. 2009. _Introducing Monte Carlo Methods\nwith R_. Springer Science & Business Media.\n\nPage built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)\n\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["msmb.css"],"toc":true,"output-file":"02-chap.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}