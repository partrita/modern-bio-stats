![](imgs/t_distribution.png)

생물학적 데이터 분석의 주요 과제 중 하나는 이질성(heterogeneity)을 다루는 것입니다. 우리가 관심을 갖는 양들은 종종 단순하고 단봉형(unimodal)인 "교과서적인 분포"를 보여주지 않습니다. 예를 들어, [2장](02-chap.html)의 마지막 부분에서 [그림 2.27](02-chap.html#fig-ScoreMixture-1)의 서열 점수 히스토그램이 CpG 섬과 비섬(non-islands)에 해당하는 두 개의 별개 모드(modes)를 가지고 있음을 보았습니다. 우리는 이 데이터를 몇 가지(이 경우 두 개) 성분의 단순한 혼합물로 볼 수 있습니다. 이를 **유한 혼합물(finite mixtures)**이라고 부릅니다. 다른 혼합물은 우리가 가진 관측치 수만큼이나 많은 성분을 포함할 수도 있습니다. 이를 **무한 혼합물(infinite mixtures)**이라고 부릅니다1.

1 모델링 선택의 많은 부분이 그러하듯이, 혼합물의 적절한 복잡성은 보는 사람의 관점에 달려 있으며, 종종 데이터의 양과 우리가 달성하고자 하는 해상도 및 매끄러움(smoothness)에 따라 달라집니다.

[1장](01-chap.html)에서 우리는 푸아송 분포를 이용한 단순한 생성 모델이 에피토프(epitope) 검출에서 어떻게 유용한 추론으로 이어지는지 보았습니다. 불행히도, 그러한 단순한 모델로 실제 데이터에 만족스러운 적합(fit)을 얻는 것은 종종 불가능합니다. 하지만 정규 분포나 푸아송 분포와 같은 단순한 모델은 우리가 이 장에서 다룰 혼합 프레임워크를 사용하여 더 현실적인 모델을 구축하기 위한 구성 요소 역할을 할 수 있습니다. 혼합물은 유세포 분석(flow cytometry) 데이터, 생체 측정값, RNA-Seq, ChIP-Seq, 마이크로바이옴 및 현대 생명공학 기술을 사용하여 수집된 다른 많은 유형의 데이터에서 자연스럽게 발생합니다. 이 장에서는 단순한 예제들을 통해 혼합물을 사용하여 더 현실적인 분포 모델을 구축하는 방법을 배울 것입니다.


이 장에서 우리는 다음을 수행할 것입니다:

  * 두 개의 정규 모집단으로 구성된 분포로부터 우리만의 혼합 모델 데이터를 생성합니다.

  * 기댓값-최대화(Expectation-Maximization, EM) 알고리즘을 통해 데이터 세트의 기저에 깔린 혼합물을 어떻게 "역설계(reverse engineer)"할 수 있는지 살펴봅니다.

  * ChIP-Seq 데이터와 같이 0이 아주 많은 데이터를 위해 영-과잉(zero-inflation)이라고 불리는 특수한 유형의 혼합 모델을 사용합니다.

  * 경험적 누적 분포(empirical cumulative distribution)를 발견합니다: 관측된 데이터로부터 구축할 수 있는 특수한 혼합물입니다. 이를 통해 붓스트랩(bootstrap)을 사용하여 우리 추정치의 가변성을 어떻게 시뮬레이션할 수 있는지 살펴볼 것입니다.

  * 많은 성분을 가진 무한 혼합 모델의 사례로서 라플라스 분포(Laplace distribution)를 구축합니다. 이를 사용하여 프로모터(promoter) 길이와 마이크로어레이 강도를 모델링할 것입니다.

  * RNA-Seq 데이터에 유용한 계층적 모델인 감마-포아송(gamma-Poisson) 분포를 처음으로 접하게 됩니다. 이것이 서로 다른 푸아송 분포 소스들을 혼합함으로써 자연스럽게 발생한다는 것을 보게 될 것입니다.

  * 혼합 모델을 통해 데이터 변환을 선택하는 방법을 살펴봅니다.


## 4.2 유한 혼합물

### 4.2.1 단순한 예제와 컴퓨터 실험

여기에 두 개의 동일한 크기의 성분으로 구성된 혼합 모델의 첫 번째 예제가 있습니다. 생성 과정은 두 단계로 이루어집니다:

**공정한 동전을 던집니다.**

앞면이 나오면: 평균 1, 분산 0.25인 정규 분포에서 난수를 생성합니다.

뒷면이 나오면: 평균 3, 분산 0.25인 정규 분포에서 난수를 생성합니다. 그림 4.1에 표시된 히스토그램은 다음 코드를 사용하여 이 두 단계를 10,000번 반복하여 생성되었습니다.

    
    
    coinflips = (runif(10000) > 0.5)
    table(coinflips)__
    
    
    coinflips
    FALSE  TRUE 
     5003  4997 
    
    
oneFlip = function(fl, mean1 = 1, mean2 = 3, sd1 = 0.5, sd2 = 0.5) {
  if (fl) {
   rnorm(1, mean1, sd1)
  } else {
   rnorm(1, mean2, sd2)
  }
}
fairmix = vapply(coinflips, oneFlip, numeric(1))
library("ggplot2")
library("dplyr")
ggplot(tibble(value = fairmix), aes(x = value)) +
     geom_histogram(fill = "purple", binwidth = 0.1)__

[![](04-chap_files/figure-html/fig-twocoins-1.png)](04-chap_files/figure-
html/fig-twocoins-1.png "그림 4.1: 두 정규 분포의 공정한 혼합물로부터 얻은 10,000개의 난수 히스토그램. 히스토그램의 왼쪽 부분은 (A)에서 생성된 숫자들이 지배적이고, 오른쪽은 (B)에서 생성된 숫자들이 지배적입니다.")

그림 4.1: 두 정규 분포의 공정한 혼합물로부터 얻은 10,000개의 난수 히스토그램. 히스토그램의 왼쪽 부분은 (A)에서 생성된 숫자들이 지배적이고, 오른쪽은 (B)에서 생성된 숫자들이 지배적입니다.

__

질문 4.1

R의 벡터화된 구문을 사용하여 `vapply` 루프를 제거하고 `fairmix` 벡터를 더 효율적으로 생성하려면 어떻게 해야 할까요?

__

해결책

__

    
    
    means = c(1, 3)
    sds   = c(0.5, 0.5)
    values = rnorm(length(coinflips),
                   mean = ifelse(coinflips, means[1], means[2]),
                   sd   = ifelse(coinflips, sds[1],   sds[2]))__

__

질문 4.2

개선된 코드를 사용하여 백만 번의 동전 던지기를 수행하고 200개의 빈(bin)을 가진 히스토그램을 만들어 보세요. 무엇을 알 수 있나요?

__

해결책

__

    
    
    fair = tibble(
      coinflips = (runif(1e6) > 0.5),
      values = rnorm(length(coinflips),
                     mean = ifelse(coinflips, means[1], means[2]),
                     sd   = ifelse(coinflips, sds[1],   sds[2])))
    ggplot(fair, aes(x = values)) +
         geom_histogram(fill = "purple", bins = 200)__

[![](04-chap_files/figure-html/fig-
limitinghistogram-1.png)](04-chap_files/figure-html/fig-
limitinghistogram-1.png "그림 4.2: 그림 4.1과 유사하지만 백만 개의 관측치를 사용한 경우.")

그림 4.2: 그림 4.1과 유사하지만 백만 개의 관측치를 사용한 경우.

그림 4.2는 빈의 수와 빈당 관측치 수를 늘림에 따라 히스토그램이 매끄러운 곡선에 가까워지는 것을 보여줍니다. 이 매끄러운 한계 곡선을 확률 변수 `fair$values`의 **밀도(density)** 함수라고 부릅니다.

정규 분포 \(N(\mu,\sigma)\) 확률 변수의 밀도 함수는 명시적으로 쓸 수 있습니다. 우리는 보통 이를 다음과 같이 부릅니다.

\[ \phi(x)=\frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}. \]

__

질문 4.3

  1. `coinflips`가 `TRUE`인 `fair$values` 값들에 대한 히스토그램을 그리세요. 힌트: `aes` 호출 시 `y = after_stat(density)`를 사용하고(이는 수직축이 빈도를 나타냄을 의미함), binwidth를 0.01로 설정하세요.
  2. \(\phi(z)\)에 해당하는 선을 겹쳐서 그리세요.

__

해결책

__

    
    
    ggplot(dplyr::filter(fair, coinflips), aes(x = values)) +
      geom_histogram(aes(y = after_stat(density)), fill = "purple", binwidth = 0.01) +
      stat_function(fun = dnorm, color = "red",
                    args = list(mean = means[1], sd = sds[1]))__

[![](04-chap_files/figure-html/fig-
overlaydensity-1.png)](04-chap_files/figure-html/fig-
overlaydensity-1.png "그림 4.3: 정규 분포 N(\mu=1,\sigma^2=0.5^2)으로부터 얻은 50만 개의 난수 히스토그램. 곡선은 dnorm 함수를 사용하여 계산된 이론적 밀도 \(\phi(x)\)입니다.")

그림 4.3: 정규 분포 \(N(\mu=1,\sigma^2=0.5^2)\)로부터 얻은 50만 개의 난수 히스토그램. 곡선은 `dnorm` 함수를 사용하여 계산된 이론적 밀도 \(\phi(x)\)입니다.

사실 우리는 `fair$values` 전체의 밀도(히스토그램이 나타내는 한계 곡선)에 대한 수학적 공식을 두 밀도의 합으로 쓸 수 있습니다.

\[ f(x)=\frac{1}{2}\phi_1(x)+\frac{1}{2}\phi_2(x), \tag{4.1}\]

여기서 \(\phi_1\)은 정규 분포 \(N(\mu_1=1,\sigma^2=0.25)\)의 밀도이고, \(\phi_2\)는 정규 분포 \(N(\mu_2=3,\sigma^2=0.25)\)의 밀도입니다. 그림 4.4는 다음 코드를 통해 생성되었습니다.

    
    
    fairtheory = tibble(
      x = seq(-1, 5, length.out = 1000),
      f = 0.5 * dnorm(x, mean = means[1], sd = sds[1]) +
          0.5 * dnorm(x, mean = means[2], sd = sds[2]))
    ggplot(fairtheory, aes(x = x, y = f)) +
      geom_line(color = "red", linewidth = 1.5) + ylab("mixture density")__

[![](04-chap_files/figure-html/fig-twodensity-1.png)](04-chap_files/figure-
html/fig-twodensity-1.png "그림 4.4: 혼합물의 이론적 밀도.")

그림 4.4: 혼합물의 이론적 밀도.

이 경우, 두 성분 분포의 중첩이 거의 없기 때문에 혼합 모델이 매우 뚜렷하게 보입니다. 그림 4.4는 두 개의 뚜렷한 정점을 보여줍니다: 우리는 이를 **이봉(bimodal)** 분포라고 부릅니다. 실제로는 많은 경우 혼합 성분 사이의 분리가 그렇게 명확하지 않지만, 그럼에도 불구하고 이는 중요합니다.

[![](04-chap_files/figure-html/fig-histmystery-1.png)](04-chap_files/figure-
html/fig-histmystery-1.png "그림 4.5: 인식하기 더 어려운 두 정규 분포의 혼합물.")

그림 4.5: 인식하기 더 어려운 두 정규 분포의 혼합물.

__

질문 4.4

그림 4.5는 분산이 동일한 두 정규 분포의 공정한 혼합물 히스토그램입니다. 성분 분포의 두 _평균_ 매개변수를 추측할 수 있나요? 힌트: 시행착오를 통해 다양한 혼합물을 시뮬레이션하여 일치하는 히스토그램을 만들 수 있는지 확인할 수 있습니다. 이 장의 R 코드를 살펴보면 데이터가 정확히 어떻게 생성되었는지 알 수 있습니다.

__

해결책

__

다음 코드는 동전의 _앞면_에서 생성된 점은 빨간색으로, _뒷면_에서 생성된 점은 파란색으로 표시합니다. 그림 4.6에 표시된 출력 결과는 두 기저 분포를 보여줍니다.

    
    
    head(mystery, 3)__
    
    
    # A tibble: 3 × 2
      coinflips values
      <lgl>      <dbl>
    1 FALSE       2.40
    2 FALSE       1.66
    3 TRUE        1.22
    
    
br = with(mystery, seq(min(values), max(values), length.out = 30))
ggplot(mystery, aes(x = values)) +
  geom_histogram(data = dplyr::filter(mystery, coinflips),
     fill = "red", alpha = 0.2, breaks = br) +
  geom_histogram(data = dplyr::filter(mystery, !coinflips),
     fill = "darkblue", alpha = 0.2, breaks = br) __

[![](04-chap_files/figure-html/fig-
betterhistogram-1-1.png)](04-chap_files/figure-html/fig-
betterhistogram-1-1.png "그림 4.6: 그림 4.5의 혼합물이지만, 두 성분을 빨간색과 파란색으로 표시한 경우.")

그림 4.6: 그림 4.5의 혼합물이지만, 두 성분을 빨간색과 파란색으로 표시한 경우.

그림 4.6에서 두 성분 분포의 막대는 서로 겹쳐서 표시됩니다. 성분을 표시하는 다른 방법은 아래 코드로 생성된 그림 4.7입니다.

    
    
    ggplot(mystery, aes(x = values, fill = coinflips)) +
      geom_histogram(data = dplyr::filter(mystery, coinflips),
         fill = "red", alpha = 0.2, breaks = br) +
      geom_histogram(data = dplyr::filter(mystery, !coinflips),
         fill = "darkblue", alpha = 0.2, breaks = br) +
      geom_histogram(fill = "purple", breaks = br, alpha = 0.2)__

[![](04-chap_files/figure-html/fig-
comparecomponents-1-1.png)](04-chap_files/figure-html/fig-
comparecomponents-1-1.png "그림 4.7: 그림 4.6과 유사하지만, 두 혼합 성분에 대해 누적 막대(stacked bars)를 사용한 경우.")

그림 4.7: 그림 4.6과 유사하지만, 두 혼합 성분에 대해 누적 막대(stacked bars)를 사용한 경우.

__

질문 4.5

왜 그림 4.7의 막대 높이는 그림 4.5와 같지만, 그림 4.6의 막대 높이는 그렇지 않을까요?

__

해결책

__

그림 4.7과 4.5에서 각 카운트는 빈(bin) 안의 수직 공간의 서로 다른 조각을 차지합니다. 그림 4.6에서는 겹치는(더 어두운) 영역에서 동일한 빈에 떨어지는 일부 카운트들이 서로 겹쳐서 그려지게 됩니다.

그림 4.6과 4.7에서는 데이터의 `coinflips` 열을 사용하여 성분들을 분리해낼 수 있었습니다. 실제 데이터에서는 이 정보가 누락되어 있습니다.

[![유한 혼합물 주제에 대한 단행본 수준의 다룸 (McLachlan and Peel 2004).](imgs/book_icon.png)](imgs/book_icon.png "유한 혼합물 주제에 대한 단행본 수준의 다룸 \[@mclachlan2004\].")

유한 혼합물 주제에 대한 단행본 수준의 연구 ([McLachlan and Peel 2004](16-chap.html#ref-mclachlan2004)).

### 4.2.2 숨겨진 클래스 레이블 발견하기

우리는 숨겨진 그룹화 값을 추론하기 위해 **기댓값-최대화(Expectation-Maximization, EM)** 알고리즘이라 불리는 방법을 사용합니다. EM 알고리즘은 솔루션의 한 부분을 안다고 가정하고 다른 부분을 계산하는 것과, 다른 부분을 안다고 가정하고 첫 번째 부분을 계산하는 것을 번갈아 가며 수행하는 인기 있는 반복 절차입니다. 더 구체적으로는 다음과 같이 교대로 수행합니다:

  * 각 관측치가 서로 다른 혼합 성분에 속할 확률을 안다고 가정하고, 이로부터 성분들의 매개변수를 추정합니다.

  * 혼합 성분들의 매개변수를 안다고 가정하고, 각 관측치가 각 성분에 속할 확률을 추정합니다.

간단한 예제를 들어보겠습니다. 우리는 두 그룹에서 온다고 생각되는 일련의 객체들에 대해 변수 \(x\)를 측정하지만, 그룹 레이블은 알지 못합니다. 우리는 측정되지 않은(잠재된) 그룹 레이블인 \(U\)를 데이터에 **추가(augmenting)** 2하는 것으로 시작합니다.

2 측정되지 않은 또 다른 변수를 추가하는 것으로, 이를 숨겨진 또는 **잠재 변수(latent variable)**라고 부릅니다.

우리는 \(U\)의 값과 그룹들의 기저 분포에 대한 알려지지 않은 매개변수 \(\theta\)를 찾는 데 관심이 있습니다. 우리는 [2장](02-chap.html)에서 소개된 최대 우도(maximum likelihood) 접근법을 사용하여 데이터 \(x\)를 가장 발생 가능하게 만드는 매개변수를 추정할 것입니다. 우리는 확률 밀도를 다음과 같이 쓸 수 있습니다.

\[ p(x,u\,|\,\theta) = p(x\,|\,u,\theta)\,p(u\,|\,\theta). \tag{4.2}\]

#### 정규 혼합물

예를 들어, 우리는 식 4.1의 두 정규 분포를 이용한 이전의 혼합 모델을 다음과 같이 동일하지 않은 혼합 비율을 허용하도록 일반화할 수 있습니다.

\[ f(x)=\lambda\phi_1(x)+(1-\lambda)\phi_2(x), \tag{4.3}\]

여기서 \(\lambda\in[0,1]\)입니다. 앞서와 유사하게, \(\phi_k\)는 각각 \(k=1, 2\)에 대한 정규 분포 \(N(\mu_k,\sigma_k^2)\)의 밀도입니다. 그러면 매개변수 벡터 \(\theta\)는 두 개의 평균, 두 개의 표준 편차, 그리고 혼합 매개변수 \(\lambda\)로 구성된 5-튜플(tuple)입니다. 다시 말해, \(\theta=(\mu_1,\mu_2,\sigma_1,\sigma_2,\lambda)\)입니다. 여기에 그러한 모델에 따라 생성된 데이터의 예가 있습니다. 레이블은 \(u\)로 표시됩니다.

    
    
    mus = c(-0.5, 1.5)
    lambda = 0.5
    u = sample(2, size = 100, replace = TRUE, prob = c(lambda, 1-lambda))
    x = rnorm(length(u), mean = mus[u])
    dux = tibble(u, x)
    head(dux)__
    
    
    # A tibble: 6 × 2
          u     x
      <int> <dbl>
    1     2 0.303
    2     2 2.65 
    3     1 0.484
    4     2 3.04 
    5     2 1.10 
    6     2 1.96 

만약 레이블 \(u\)를 알고 있다면, 우리는 각 그룹에 대해 별도의 MLE를 사용하여 매개변수를 추정할 수 있습니다. 전체 우도(likelihood)는 다음과 같습니다.

\[ p(x, u \,|\, \theta) = \left( \prod_{\{i:\,u_i=1\}} \phi_1(x_i) \right) \times \left( \prod_{\{i:\,u_i=2\}} \phi_2(x_i) \right). \tag{4.4}\]

이 식의 최대화는 세 개의 독립적인 조각으로 나눌 수 있습니다: 식 4.4의 우변에 있는 첫 번째 괄호 안의 식을 최대화하여 \(\mu_1\)과 \(\sigma_1\)을 찾고, 두 번째 조각을 최대화하여 \(\mu_2\)와 \(\sigma_2\)를 찾으며, 레이블의 경험적 빈도로부터 \(\lambda\)를 찾습니다.

    
    
group_by(dux, u) |> summarize(mu = mean(x), sigma = sd(x))__
    
    
    # A tibble: 2 × 3
          u     mu sigma
      <int>  <dbl> <dbl>
    1     1 -0.558  1.05
    2     2  1.41   1.04
    
    
table(dux$u) / nrow(dux)__
    
    
       1    2 
    0.55 0.45 

__

질문 4.6

혼합 비율 \(\lambda=\frac{1}{2}\)은 알고 있지만 \(u_i\)는 모른다고 가정해 봅시다. 이 경우 밀도는 \(\frac{1}{2}\phi_1(x)+\frac{1}{2}\phi_2(x)\)입니다. (로그) 우도를 직접 써보세요. 여기서 MLE를 명시적으로 구하는 것을 방해하는 요인은 무엇인가요?

__

해결책

__

정규 분포의 유한 혼합물에 대한 우도 계산에 대해서는, 예를 들어 Shalizi ([2017](16-chap.html#ref-CosmaShalizi2017))의 _혼합 모델(Mixture Models)_ 장을 참조하세요. "혼합 모델을 추정하려 할 때, 우리는 사후 레이블 확률(posterior label probabilities)에 의해 주어지는 가중치를 사용하여 가중 최대 우도(weighted maximum likelihood)를 수행하게 됩니다. 거듭하지만, 이 확률들은 우리가 추정하려는 매개변수들에 의존하므로, 일종의 악순환이 생기는 것 같습니다."

많은 경우 우리는 \(u\) 레이블도, 혼합 비율도 알지 못합니다. 우리가 할 수 있는 일은 레이블에 대한 초기 추측에서 시작하여 이를 알고 있다고 가정하고 위와 같이 매개변수를 추정한 다음, 우리 추정치가 실질적으로 변하지 않을 때까지(즉, 수렴할 때까지) 그리고 우도가 최적값에 도달할 때까지 매 단계마다 레이블과 매개변수에 대한 현재의 최선의 추측을 업데이트하는 반복적인 사이클을 거치는 것입니다.

사실 우리는 각 관측치에 대해 "딱딱한(hard)" 레이블 \(u\)(그룹 1 아니면 2 중 하나에 속함)를 부여하는 대신, 이를 합이 1이 되는 멤버십 가중치(membership weights)로 대체하는 더 정교한 작업을 수행할 수 있습니다. 혼합 모델 4.3은 다음을 제안합니다.

\[ w(x, 1) = \frac{\lambda\phi_1(x)}{\lambda\phi_1(x)+(1-\lambda)\phi_2(x)} \tag{4.5}\]

이 식은 값 \(x\)를 가진 관측치가 첫 번째 혼합 성분에 의해 생성되었을 확률로 해석될 수 있으며, 두 번째 성분에 대해서는 유사하게 \(w(x, 2) = 1 - w(x, 1)\)이 됩니다. 즉, \(\lambda\)가 아직 보지 못한 관측치가 혼합 성분 1로부터 나올 사전 확률(prior probability)이라면, \(w(x,1)\)은 그 값 \(x\)를 관찰한 후의 대응하는 사후 확률(posterior probability)입니다. 이는 다음과 같은 반복 알고리즘을 제안합니다:

**E 단계** : \(\theta\)(즉, 평균, 표준 편차 및 \(\lambda\))가 알려져 있다고 가정하고, 멤버십 가중치 4.5를 평가합니다.

**M 단계** : 각 관측치 \(x_i\)의 멤버십 가중치가 주어졌을 때, \(\theta\)의 새롭고 개선된 최대 우도 추정치를 결정합니다.

\(\theta\)와 우도가 수렴할 때까지 반복합니다. 이 시점에서 코드로 작성된 데모인 연습 문제 4.1을 확인해 보세요. 사실 이 알고리즘은 여기서 다루는 특정 응용 예제보다 훨씬 더 일반적입니다. ([Bishop 2006](16-chap.html#ref-Bishop:PRML))은 매우 읽기 쉬운 설명을 제시하고 있으며, 주요 내용은 다음과:

우리의 목표는 관측 변수 \(x\), 관찰되지 않은 변수 \(u\) 및 일부 매개변수 \(\theta\)를 포함하는 확률 모델의 주변 우도(marginal likelihood)를 최대화하는 것입니다. 우리의 단순한 예제에서 \(u\)는 두 가지 가능한 값을 가진 범주형 변수이고, \(x\)는 실수입니다. 일반적으로 \(x\)와 \(u\) 모두 모든 유형의 개별 변수들의 튜플(즉, 다변량)일 수 있습니다. 주변 우도는 \(u\)의 모든 가능한 값들에 대해 기댓값(즉, 가중 평균)을 취함으로써 계산됩니다:

\[ L_\text{marg}(\theta; x) = \int_U p(x, u\,|\,\theta) \, \text{d}U. \tag{4.6}\]

우리의 구체적인 예제에서, 적분은 가능한 모든 멤버십 구성에 대해 (확률적으로) 평균을 내는 것에 해당하며, 따라서 멤버십 가중치를 고려한 가중 합이 됩니다.

\[ L_\text{marg}(\theta; x) = \prod_{i=1}^n \sum_{u=1}^2 p(x_i, u\,|\,\theta) \, w(x_i, u\,|\,\theta). \tag{4.7}\]

이 수치를 직접 최대화하는 것은 다루기 힘든(intractable) 일입니다.

데이터와 현재의 매개변수 추정치 \(\theta_t\)가 주어졌을 때 우리가 파악할 수 있는 것은 잠재 변수의 조건부 분포인 \(p(u\,|\,x, \theta_t)\)입니다. 우리는 이를 사용하여 일반적인 매개변수 값 \(\theta\)에 대해 평가된 완전 데이터 로그 우도(complete data log likelihood) \(\log p(x, u\,|\,\theta)\)의 기댓값을 찾을 수 있습니다. 이 기댓값은 종종 다음과 같이 표시됩니다.

\[ Q(\theta, \theta_t) = \int_U \log p(x, u\,|\,\theta) \, p(u\,|\, x, \theta_t) \, \text{d}U. \tag{4.8}\]

M 단계에서는 이 함수를 최대화하여 수정된 매개변수 추정치 \(\theta_{t+1}\)을 결정합니다.

\[ \theta_{t+1} = \arg \max_\theta \, Q(\theta, \theta_t). \tag{4.9}\]

E 단계는 \(Q\)의 핵심 요소인 \(p(u\,|\, x, \theta_t)\)를 계산하는 것으로 구성됩니다.

이 두 단계(E와 M)는 개선 사항이 작아질 때까지 반복됩니다; 이는 우리가 우도의 평평한 부분에 도달했으며 최댓값에 이르렀다는 수치적 지표입니다. 반복 궤적은 시작 지점에 따라 달라지겠지만, 도달하는 지점은 달라지지 않기를 바랍니다. 이는 산 정상에 오르는 것과 비슷합니다. 산 정상에 오르는 길은 시작 지점에 따라 다를 수 있고 경로도 다를 수 있지만, 산봉우리가 하나만 있고 여러 개가 아닌 이상 항상 정상으로 이어집니다. 따라서 예방 조치로서, 이러한 절차를 서로 다른 시작 지점에서 여러 번 반복하여 항상 동일한 답을 얻는지 확인하는 것이 좋습니다.

__

질문 4.7

여러 R 패키지에서 **[mclust](https://cran.r-project.org/web/packages/mclust/)**, **[EMcluster](https://cran.r-project.org/web/packages/EMcluster/)**, **[EMMIXskew](https://cran.r-project.org/web/packages/EMMIXskew/)**를 포함한 EM 구현을 제공합니다. 하나를 선택하여 서로 다른 시작 값으로 EM 함수를 여러 번 실행해 보세요. 그런 다음 **[mixtools](https://cran.r-project.org/web/packages/mixtools/)** 패키지의 `normalmixEM` 함수를 사용하여 출력을 비교해 보세요.

__

해결책

__

여기서는 **[mixtools](https://cran.r-project.org/web/packages/mixtools/)**의 출력을 보여줍니다.

    
    
    library("mixtools")
    y = c(rnorm(100, mean = -0.2, sd = 0.5),
          rnorm( 50, mean =  0.5, sd =   1))
    gm = normalmixEM(y, k = 2, 
                        lambda = c(0.5, 0.5),
                        mu = c(-0.01, 0.01), 
                        sigma = c(3, 3))__
    
    
    number of iterations= 134 
    
    
    with(gm, c(lambda, mu, sigma, loglik))__
    
    
    [1]    0.9263    0.0737   -0.1859    1.9216    0.6134    0.4746 -170.5662

EM 알고리즘은 매우 유익합니다:

  1. 우리는 관측치가 한 그룹에 속하는지 다른 그룹에 속하는지 결정하지 않고, 멤버십 확률을 가중치로 사용하여 여러 그룹에 참여할 수 있게 함으로써 더 미묘한 추정치를 얻는 "부드러운(soft)" 평균화의 첫 번째 사례를 보았습니다 ([Slonim et al. 2005](16-chap.html#ref-Slonim:2005)).

  2. 미지수가 너무 많은 어려운 문제를 더 간단한 문제들을 번갈아 가며 해결함으로써 어떻게 다룰 수 있는지 보여줍니다.

  3. 숨겨진 변수가 있는 데이터 생성 모델을 고려하면서도 그 매개변수를 추정할 수 있었습니다. 숨겨진 변수의 값을 명시적으로 확정하지 않고도 그렇게 할 수 있었습니다: 식 4.8의 기댓값 단계에서 멤버십 확률로 구체화된 이들에 대해 (가중) 평균을 취했습니다. 이 기본 아이디어는 매우 강력하여 기계 학습의 많은 고급 알고리즘의 출발점이 됩니다 ([Bishop 2006](16-chap.html#ref-Bishop:PRML)).

  4. 이는 모델 평균화(model averaging, [Hoeting et al. 1999](16-chap.html#ref-hoeting1999))의 더 일반적인 경우로 확장될 수 있습니다. 우리 데이터에 어떤 모델이 적합한지 확신할 수 없는 경우 여러 모델을 동시에 고려하는 것이 때때로 유익할 수 있습니다. 우리는 이들을 가중 모델로 결합할 수 있습니다. 가중치는 모델의 우도에 의해 제공됩니다.

### 4.2.3 영-과잉 데이터(zero inflated data)를 위한 모델

생태학적 및 분자적 데이터는 종종 카운트(counts)의 형태로 나타납니다. 예를 들어, 여러 장소 각각에 있는 여러 종 각각의 개체 수일 수 있습니다. 그러한 데이터는 종종 두 가지 시나리오의 혼합으로 볼 수 있습니다: 종이 존재하지 않으면 카운트는 반드시 0이지만, _만약_ 종이 존재한다면 우리가 관찰하는 개체 수는 무작위 샘플링 분포에 따라 달라지며, 이 분포에도 0이 포함될 수 있습니다. 우리는 이를 다음과 같이 혼합 모델로 모델링합니다:

\[ f_\text{zi}(x) = \lambda \, \delta(x) + (1-\lambda) \, f_\text{count}(x), \tag{4.10}\]

여기서 \(\delta\)는 모든 질량이 0에 집중되어 있는 확률 분포를 나타내는 디랙 델타(Dirac’s delta) 함수입니다. 첫 번째 혼합 성분에서 발생하는 0은 "구조적(structural)"이라고 불립니다: 우리 예제에서 이는 특정 종이 특정 서식지에 살지 않기 때문에 발생합니다. 두 번째 성분인 \(f_\text{count}\) 역시 단순히 무작위 샘플링으로 인해 0이나 다른 작은 숫자들을 포함할 수 있습니다. R 패키지 **[pscl](https://cran.r-project.org/web/packages/pscl/)** ([Zeileis, Kleiber, and Jackman 2008](16-chap.html#ref-Zeileis:2008))과 **[zicounts](https://cran.r-project.org/web/packages/zicounts/)**는 **영-과잉(zero inflated)** 카운트를 다루기 위한 많은 예제와 함수를 제공합니다.

#### 예시: ChIP-Seq 데이터

ChIP-Seq 데이터의 예제를 살펴봅시다. 이 데이터는 염색질 면역 침전(chromatin immunoprecipitation, ChIP)으로부터 얻은 DNA 조각의 서열입니다. 이 기술은 전사 인자, 뉴클레오솜, 히스톤 수정, 염색질 리모델링 효소, 샤페론, 중합효소 및 기타 단백질의 게놈 DNA 상의 위치를 매핑할 수 있게 해줍니다. 이는 DNA 요소 백과사전(ENCODE) 프로젝트에서 사용된 주요 기술이었습니다. 여기서는 **[mosaicsExample](https://bioconductor.org/packages/mosaicsExample/)** 패키지([Kuan et al. 2011](16-chap.html#ref-Kuan2011statistical))의 예제를 사용합니다. 이 예제는 GM12878 세포주에 적용된 STAT1 단백질 및 H3K4me3 히스톤 수정에 대한 항체의 ChIP-Seq으로부터 22번 염색체에서 측정된 데이터를 보여줍니다. 여기서는 `binTFBS` 객체를 생성하는 코드는 보여주지 않습니다; 이는 이 장의 소스 코드 파일에 나와 있으며 **[mosaics](https://bioconductor.org/packages/mosaics/)** 패키지의 비네트를 따릅니다.

    
    
    binTFBS __
    
    
    Summary: bin-level data (class: BinData)
    ----------------------------------------
    - # of chromosomes in the data: 1
    - total effective tag counts: 462479
      (sum of ChIP tag counts of all bins)
    - control sample is incorporated
    - mappability score is NOT incorporated
    - GC content score is NOT incorporated
    - uni-reads are assumed
    ----------------------------------------

이 객체로부터 빈(bin)당 카운트의 히스토그램을 생성할 수 있습니다.

    
    
    bincts = print(binTFBS)
    ggplot(bincts, aes(x = tagCount)) +
      geom_histogram(binwidth = 1, fill = "forestgreen")__

[![](04-chap_files/figure-html/fig-chipseqzeros-1.png)](04-chap_files/figure-
html/fig-chipseqzeros-1.png "그림 4.8: ChIP-Seq 데이터 세트에서 22번 염색체를 따라 200nt 윈도우에서 발견된 결합 부위의 수.")

그림 4.8: ChIP-Seq 데이터 세트에서 22번 염색체를 따라 200nt 윈도우에서 발견된 결합 부위의 수.

그림 4.8에서 우리는 0이 아주 많다는 것을 알 수 있는데, 비록 이 플롯만으로는 다른 작은 숫자들(\(1,2,...\))의 빈도를 고려할 때 0의 개수가 정말로 특별한지는 즉각적으로 명확하지 않습니다.

__

질문 4.8

  1. \(y\)축에 상용로그(base 10) 스케일을 사용하여 카운트 히스토그램을 다시 그리세요.  

  2. 카운트가 0인 빈(bin)의 비율인 \(\pi_0\)을 추정하세요.

__

해결책

__

    
    
    ggplot(bincts, aes(x = tagCount)) + scale_y_log10() +
       geom_histogram(binwidth = 1, fill = "forestgreen")__

[![](04-chap_files/figure-html/fig-
ChipseqHistlogY-1.png)](04-chap_files/figure-html/fig-
ChipseqHistlogY-1.png "그림 4.9: 그림 4.8과 같지만, y축에 상용로그(base 10) 스케일을 사용한 모습. 0의 분율이 1, 2, ...의 분율과 비교했을 때 높아 보입니다.")

그림 4.9: 그림 4.8과 같지만, \(y\)축에 상용로그(base 10) 스케일을 사용한 모습. 0의 분율이 1, 2, ...의 분율과 비교했을 때 높아 보입니다.

### 4.2.4 두 개 이상의 성분

지금까지 우리는 두 개의 성분을 가진 혼합물을 살펴보았습니다. 우리는 이를 더 확장할 수 있습니다. 예를 들어, 데옥시리보뉴클레오타이드 일인산(deoxyribonucleotide monophosphates)의 혼합물로부터 얻은 N=7,000개의 뉴클레오타이드 무게를 잴 때 (각 유형은 서로 다른 무게를 가지며, 동일한 표준 편차 sd=3으로 측정됨), 다음 코드로 생성된 그림 4.10과 같은 히스토그램을 관찰할 수 있습니다.

    
    
masses = c(A =  331, C =  307, G =  347, T =  322)
probs  = c(A = 0.12, C = 0.38, G = 0.36, T = 0.14)
N  = 7000
sd = 3
nuclt   = sample(length(probs), N, replace = TRUE, prob = probs)
quadwts = rnorm(length(nuclt),
                mean = masses[nuclt],
                sd   = sd)
ggplot(tibble(quadwts = quadwts), aes(x = quadwts)) +
  geom_histogram(bins = 100, fill = "purple")__

[![](04-chap_files/figure-html/fig-
nucleotideweights-1-1.png)](04-chap_files/figure-html/fig-
nucleotideweights-1-1.png "그림 4.10: 7,000개 뉴클레오타이드 질량 측정 시뮬레이션.")

그림 4.10: 7,000개 뉴클레오타이드 질량 측정 시뮬레이션.

__

질문 4.9

이 시뮬레이션 실험을 \(N=1000\)개의 뉴클레오타이드 측정으로 반복해 보세요. 히스토그램에서 무엇을 발견했나요?

__

질문 4.10

\(N=7000\)이지만 표준 편차가 10이면 어떻게 될까요?

__

질문 4.11

그림 4.10에서 시뮬레이션된 분포에 대한 이론적 밀도 곡선을 그리세요.

이 경우, 우리는 충분한 수의 정밀한 측정값을 가지고 있기 때문에 네 개의 뉴클레오타이드를 구별하고 그림 4.10에 표시된 분포를 분해할 수 있습니다. 데이터가 적거나 측정 노이즈가 더 많다면, 네 개의 모드와 분포 성분이 덜 명확해질 것입니다.

## 4.3 경험적 분포와 비모수적 붓스트랩

이 섹션에서는 혼합 모델의 극단적인 사례를 고려할 것입니다. 여기서 우리는 \(n\)개의 데이터 포인트 표본을 \(n\)개의 점 질량(point masses)의 혼합물로 모델링할 것입니다. 우리는 여기서 거의 모든 데이터 세트를 사용할 수 있습니다; 구체적으로, 우리는 다윈(Darwin)의 _Zea Mays_ 데이터3를 사용합니다. 이 데이터에서 그는 15쌍의 _Zea Mays_ 식물의 높이를 비교했습니다 (15개의 자가 수분 식물 대 15개의 타가 수분 식물). 이 데이터는 **[HistData](https://cran.r-project.org/web/packages/HistData/)** 패키지에서 사용할 수 있으며, 우리는 15개의 높이 차이 분포를 플롯합니다:

3 이 데이터는 다윈에 의해 수집되었으며, 그는 사촌인 프랜시스 골턴(Francis Galton)에게 분석을 요청했습니다. R.A. 피셔는 동일한 데이터를 사용하여 쌍체 t-검정(paired t-test)으로 재분석했습니다 ([Bulmer 2003](16-chap.html#ref-bulmer2003)). 우리는 이 예제로 [13장](13-chap.html)에서 다시 돌아올 것입니다.

    
    
    library("HistData")
    ZeaMays$diff __
    
    
     [1]  6.125 -8.375  1.000  2.000  0.750  2.875  3.500  5.125  1.750  3.625
    [11]  7.000  3.000  9.375  7.500 -6.000
    
    
    ggplot(ZeaMays, aes(x = diff, ymax = 1/15, ymin = 0)) +
      geom_linerange(linewidth = 1, col = "forestgreen") + ylim(0, 0.1)__

[![](04-chap_files/figure-html/fig-ecdfZea-1.png)](04-chap_files/figure-
html/fig-ecdfZea-1.png "그림 4.11: 관측된 표본은 각 값에서의 점 질량들의 혼합물로 보여질 수 있습니다 (실제 점 질량은 너비가 전혀 없는 막대일 것입니다).")

그림 4.11: 관측된 표본은 각 값에서의 점 질량들의 혼합물로 보여질 수 있습니다 (실제 점 질량은 너비가 전혀 없는 막대일 것입니다).

[3.6.7절](03-chap.html#sec-graphics-ecdf)에서 우리는 크기 \(n\)인 표본에 대한 경험적 누적 분포 함수(ECDF)가 다음과 같음을 보았습니다.

\[ \hat{F}_n(x)= \frac{1}{n}\sum_{i=1}^n \mathbb 1_{x \leq x_i}, \tag{4.11}\]

그리고 [그림 3.24](03-chap.html#fig-graphics-onedecdf)에서 ECDF 플롯을 보았습니다. 우리는 또한 우리 표본의 _밀도_를 다음과 같이 쓸 수 있습니다.

\[ \hat{f}_n(x) =\frac{1}{n}\sum_{i=1}^n \delta_{x_i}(x) \tag{4.12}\]

일반적으로 확률 분포의 밀도는 (존재한다면) 분포 함수의 도함수입니다. 우리는 여기서 이 원리를 적용했습니다: 식 4.11에 의해 정의된 분포의 밀도는 식 4.12입니다. 함수 \(\delta_a\)를 계단 함수 \({\mathbb 1}_{x \leq a}\)의 "도함수"로 간주할 수 있기 때문에 이렇게 할 수 있었습니다: 이 함수는 계단이 있는 한 지점 \(a\)를 제외하고는 거의 모든 곳에서 완전히 평평하며, 그 지점에서의 값은 "무한"입니다. 식 4.12는 우리의 데이터 표본이 그림 4.11에서 보듯이 관측된 값 \(x_1, x_2, ..., x_n\)에서의 \(n\)개의 **점 질량(point masses)**의 혼합물로 간주될 수 있음을 강조합니다.

이것이 엄밀히 성립하려면 (표준 미적분학을 넘어서는) 약간의 고급 수학이 필요하지만, 여기서는 다루지 않겠습니다.

[![](imgs/BootstrapPrincipleNew.png)](imgs/BootstrapPrincipleNew.png "그림 4.12: 통계량 \tau의 값은 기저 분포 F로부터 생성된 데이터(회색 행렬들)로부터 추정됩니다. F로부터의 서로 다른 표본들은 서로 다른 데이터로 이어지고, 따라서 추정치 \hat{\tau}의 값도 달라집니다: 이를 표집 가변성(sampling variability)이라고 합니다. 모든 \hat{\tau}들의 분포가 표집 분포(sampling distribution)입니다.")

그림 4.12: 통계량 \(\tau\)의 값은 기저 분포 \(F\)로부터 생성된 데이터(회색 행렬들)로부터 추정됩니다. \(F\)로부터의 서로 다른 표본들은 서로 다른 데이터로 이어지고, 따라서 추정치 \(\hat{\tau}\)의 값도 달라집니다: 이를 **표집 가변성(sampling variability)**이라고 합니다. 모든 \(\hat{\tau}\)들의 분포가 **표집 분포(sampling distribution)**입니다.

평균, 최솟값 또는 중앙값과 같은 우리 표본의 통계량은 이제 ECDF의 함수로 쓰일 수 있습니다. 예를 들어, \(\bar{x} = \int \delta_{x_i}(x)\,\text{d}x\)입니다. 또 다른 예로, \(n\)이 홀수라면 중앙값은 정렬된 리스트의 정중앙에 있는 값인 \(x_{(\frac{n+1}{2})}\)입니다.

통계량 \(\hat{\tau}\)의 실제 **표집 분포**는 이를 계산하기 위해 많은 서로 다른 데이터 표본들이 필요하기 때문에 알기 어려운 경우가 많습니다; 이는 그림 4.12에 나와 있습니다.

**붓스트랩(bootstrap)** 원리는 원래 표본으로부터 구축된 경험적 분포로부터 뽑은 새로운 표본들을 생성함으로써 \(\hat{\tau}\)의 실제 표집 분포를 근사합니다 (그림 4.13). 우리는 데이터를 (\(\delta\)들의 혼합 분포로 간주하여) **재사용** 하여 표본을 추출하고, 그로부터 계산된 통계량 \(\hat{\tau}^*\)의 표집 분포를 살펴봄으로써 새로운 "데이터 세트"들을 만듭니다. 이를 **비모수적 붓스트랩(nonparametric bootstrap)** 재표본 추출 접근법이라고 하며, 완전한 참고 문헌은 Bradley Efron과 Tibshirani ([1994](16-chap.html#ref-Efron:1994))를 참조하십시오. 이는 아무리 복잡하더라도 기본적으로 모든 통계량에 적용할 수 있는 매우 다재다능하고 강력한 방법입니다. 우리는 [5장](05-chap.html)에서 이 방법의 응용 예시, 특히 군집화(clustering)에 대해 살펴볼 것입니다.

[![](imgs/BootstrapPrinciple2New.png)](imgs/BootstrapPrinciple2New.png "그림 4.13: 붓스트랩은 (그림 4.12에서처럼) 기저의 실제 분포 F로부터가 아니라 경험적 분포 함수 \(\hat{F}_n\)으로부터 표본을 추출함으로써 표집 가변성을 시뮬레이션합니다.")

그림 4.13: 붓스트랩은 (그림 4.12에서처럼) 기저의 실제 분포 \(F\)로부터가 아니라 경험적 분포 함수 \(\hat{F}_n\)으로부터 표본을 추출함으로써 표집 가변성을 시뮬레이션합니다.

이러한 아이디어들을 사용하여, 그림 4.11에서 보았던 Zea Mays 차이의 중앙값에 대한 표집 분포를 추정해 봅시다. 우리는 이전 섹션들과 유사한 시뮬레이션을 사용합니다: 15개의 값들(이들 각각은 15-성분 혼합물의 한 성분임)로부터 크기가 15인 표본을 \(B=1000\)번 추출합니다. 그런 다음 이 15개 값들로 이루어진 1000개 표본 각각의 중앙값을 계산하고 그 분포를 살펴봅니다: 이것이 중앙값의 붓스트랩 표집 분포입니다.

    
    
    B = 1000
    meds = replicate(B, {
      i = sample(15, 15, replace = TRUE)
      median(ZeaMays$diff[i])
    })
    ggplot(tibble(medians = meds), aes(x = medians)) +
      geom_histogram(bins = 30, fill = "purple")__

[![](04-chap_files/figure-html/fig-bootmedian-1.png)](04-chap_files/figure-
html/fig-bootmedian-1.png "그림 4.14: Zea Mays 차이의 중앙값에 대한 붓스트랩 표집 분포.")

그림 4.14: Zea Mays 차이의 중앙값에 대한 붓스트랩 표집 분포.

__

질문 4.12

이 시뮬레이션들에 기초하여 중앙값에 대한 99% 신뢰 구간을 추정해 보세요. 이 구간과 0 사이의 중첩을 보고 무엇을 결론지을 수 있나요?

__

질문 4.13

**[bootstrap](https://cran.r-project.org/web/packages/bootstrap/)** 패키지의 `bootstrap` 함수를 사용하여 `median`과 `mean` 모두에 대해 동일한 분석을 다시 수행해 보세요. 평균과 중앙값의 표집 분포 사이에서 어떤 차이점을 발견했나요?

__

해결책

__

    
    
    library("bootstrap")
    bootstrap(ZeaMays$diff, B, mean)
    bootstrap(ZeaMays$diff, B, median)__

#### 왜 비모수적(nonparametric)인가요?

이론 통계학에서 비모수적 방법이란 무한히 많은 자유도나 알려지지 않은 매개변수 수를 가진 방법을 말합니다.

![](imgs/devil.png)

실제로는 무한대까지 기다리지 않습니다; 매개변수 수가 가용한 데이터의 양만큼 많거나 그보다 많아지면 우리는 그 방법을 비모수적이라고 부릅니다. 붓스트랩은 \(n\)개의 성분을 가진 혼합물을 사용하므로, 크기 \(n\)인 표본에 대해 비모수적 방법의 요건을 갖춥니다.

이름에도 불구하고, 비모수적 방법은 매개변수를 사용하지 않는 방법이 아닙니다: 모든 통계적 방법은 알려지지 않은 양을 추정합니다.

__

질문 4.14

표본이 \(n=3\)개의 서로 다른 값들로 구성되어 있다면, 몇 가지의 서로 다른 붓스트랩 재표본이 가능할까요? \(n=15\)인 경우에 대해서도 답해 보세요.

__

해결책

__

모든 붓스트랩 재표본의 집합은 합이 \(n\)인 \(n\)개 정수 벡터들의 집합과 동일합니다. 관측치 \(x_1, x_2, ..., x_n\)이 붓스트랩 표본에 나타나는 횟수를 \(\mathbf{k} = (k_1, k_2, ..., k_n)\)이라고 합시다. 각 \(k_i\)를 (다항 분포에서와 같이) 상자로 생각할 수 있으며, \(n\)개의 공을 떨어뜨릴 \(n\)개의 상자가 있습니다. 구성(configurations)의 수를 세는 방법은 \(n\)개의 공을 상자들에 나누는 방법의 수를 세는 것입니다. 즉, 공을 나타내는 `o`를 \(n\)번 쓰고 그 사이에 구분선 `|`를 \(n-1\)번 쓰는 것입니다. 따라서 우리는 `o`(공)나 `|`(구분선) 중 하나를 선택해야 하는 \(2n-1\)개의 자리를 채워야 합니다. \(n=3\)인 경우, 가능한 배치는 `oo||o`일 수 있으며, 이는 \(\mathbf{k} = (2,0,1)\)에 해당합니다. 일반적으로 이 숫자는 \({2n-1} \choose {n-1}\)이며, 따라서 \(n=3\)과 \(15\)에 대한 답은 다음과 같습니다:

    
    
c(N3 = choose(5, 3), N15 = choose(29, 15))__
    
    
          N3      N15 
          10 77558760 

__

질문 4.15

**[bootstrap](https://cran.r-project.org/web/packages/bootstrap/)** 패키지에 구현된 붓스트랩을 사용할 때 발생할 수 있는 두 가지 유형의 오류는 무엇인가요? 이들 중 하나를 개선하기 위해 어떤 매개변수를 수정할 수 있나요?

__

해결책

__

무작위 재표본 추출을 통한 데이터 하위 집합의 몬테카를로 시뮬레이션은 전수 조사 붓스트랩(exhaustive bootstrap)을 근사합니다 ([Diaconis and Holmes 1994](16-chap.html#ref-Diaconis1994)). `bootstrap` 함수의 `nboot` 인수 크기를 늘리면 몬테카를로 오류를 줄일 수 있지만, 전수 조사 붓스트랩 또한 여전히 정확하지는 않습니다: 우리는 여전히 실제 분포 대신 데이터의 근사적인 분포 함수를 사용하고 있기 때문입니다. 표본 크기가 작거나 원래 표본에 편향이 있는 경우, `nboot`를 아무리 크게 선택하더라도 근사는 여전히 상당히 나쁠 수 있습니다.

## 4.4 무한 혼합물

때때로 혼합물은 우리가 각 관측치에 레이블을 할당하려 하지 않더라도 유용할 수 있으며, 달리 말하면 관측치 수만큼 많은 '레이블'을 허용하는 경우입니다. 만약 혼합 성분의 수가 관측치 수만큼 많다면(혹은 더 많다면), 우리는 **무한 혼합물(infinite mixture)** 을 가지고 있다고 말합니다. 몇 가지 예제를 살펴봅시다.

### 4.4.1 정규 분포의 무한 혼합물

[![](imgs/LaplacePortrait_web.png)](imgs/LaplacePortrait_web.png "그림 4.15: 라플라스(Laplace)는 이미 확률 밀도 f_X(y) = \frac{1}{2\phi} \exp\left(-\frac{|y-\theta|}{\phi}\right), \phi > 0가 그 위치 매개변수 \theta로서 중앙값을 가지고, 그 척도 매개변수 \phi를 추정하는 데 중앙값 절대 편차(MAD)가 사용될 수 있음을 알고 있었습니다.")

그림 4.15: 라플라스는 이미 확률 밀도
\[f_X(y)=\frac{1}{2\phi}\exp\left(-\frac{|y-\theta|}{\phi}\right),\qquad\phi>0\]
가 그 위치 매개변수 \(\theta\)로서 중앙값을 가지고, 척도 매개변수 \(\phi\)를 추정하는 데 중앙값 절대 편차(MAD)가 사용될 수 있음을 알고 있었습니다.

다음과 같은 2단계 데이터 생성 체계를 고려해 봅시다:

**레벨 1** 지수 분포로부터 `W`들의 표본을 생성합니다.

    
    
w = rexp(10000, rate = 1)__

**레벨 2** 이 `w`들은 `rnorm`을 사용하여 생성된 평균 \(\mu\)인 정규 변수들의 분산 역할을 합니다.

    
    
    mu  = 0.3
    lps = rnorm(length(w), mean = mu, sd = sqrt(w))
    ggplot(data.frame(lps), aes(x = lps)) +
      geom_histogram(fill = "purple", binwidth = 0.1)__

[![](04-chap_files/figure-html/fig-
Laplacedistribution-1.png)](04-chap_files/figure-html/fig-
Laplacedistribution-1.png "그림 4.16: 라플라스 분포로부터 샘플링된 데이터.")

그림 4.16: 라플라스 분포로부터 샘플링된 데이터.

이것은 상당히 유용한 분포인 것으로 밝혀졌습니다. 이는 잘 이해된 속성들을 가지고 있으며, 중앙값이 그 위치 매개변수 \(\theta\)의 좋은 추정량이고 중앙값 절대 편차가 척도 매개변수 \(\phi\)를 추정하는 데 사용될 수 있음을 증명한 라플라스의 이름을 따서 명명되었습니다. 그림 4.15의 캡션에 있는 공식으로부터 우리는 \(L_1\) 거리(차이의 절댓값)가 라플라스 밀도에서 차지하는 위치가, 정규 밀도에서의 \(L_2\) 거리(차이의 제곱)의 위치와 유사함을 알 수 있습니다.

반대로, 베이지안 회귀4에서 계수들에 라플라스 분포를 사전 분포로 갖는 것은 **라쏘(lasso)** ([Tibshirani 1996](16-chap.html#ref-Tibshirani1996))라고 불리는 \(L_1\) 페널티와 동일하며, 반면 사전 분포로 정규 분포를 갖는 것은 릿지(ridge) 회귀라고 불리는 \(L_2\) 페널티로 이어집니다.

4 이에 익숙하지 않다면 걱정하지 마세요. 그 경우 이 문장은 그냥 건너뛰셔도 됩니다.

__

질문 4.16

대칭 라플라스 분포를 따르는 확률 변수를 정규 및 지수 확률 변수들의 함수로 써보세요.

__

해결책

__

우리는 분산이 지수 변수 \(W\)로 생성되는 계층적 모델을 다음과 같이 쓸 수 있습니다:

\[ X = \sqrt{W} \cdot Z, \qquad W \sim Exp(1), \qquad Z \sim N(0,1). \tag{4.13}\]

#### 비대칭 라플라스 (Asymmetric Laplace)

라플라스 분포에서 정규 성분들의 분산은 \(W\)에 의존하는 반면, 평균은 영향을 받지 않습니다. 한 가지 유용한 확장은 성분들의 위치나 중심을 제어하는 또 다른 매개변수 \(\theta\)를 추가하는 것입니다. 우리는 \(W\)가 지수 변수인 계층적 모델로부터 데이터 `alps`를 생성합니다; 그림 4.17에 표시된 출력 결과는 정규 분포 \(N(\theta+w\mu,\sigma w)\) 난수들의 히스토그램이며, 여기서 \(w\)들 자체는 코드에 표시된 대로 평균이 1인 지수 분포로부터 무작위로 생성되었습니다:

    
    
    mu = 0.3; sigma = 0.4; theta = -1
    w  = rexp(10000, 1)
    alps = rnorm(length(w), theta + mu * w, sigma * sqrt(w))
    ggplot(tibble(alps), aes(x = alps)) +
      geom_histogram(fill = "purple", binwidth = 0.1)__

[![](04-chap_files/figure-html/fig-
ALaplacedistribution-1.png)](04-chap_files/figure-html/fig-
ALaplacedistribution-1.png "그림 4.17: 비대칭 라플라스 분포로부터 생성된 데이터의 히스토그램 – 평균과 분산이 서로 종속적인 많은 정규 분포들의 척도 혼합물(scale mixture)입니다. 우리는 X \sim AL(\theta, \mu, \sigma)라고 씁니다.")

그림 4.17: 비대칭 라플라스 분포로부터 생성된 데이터의 히스토그램 – 평균과 분산이 서로 종속적인 많은 정규 분포들의 척도 혼합물(scale mixture)입니다. 우리는 \(X \sim AL(\theta, \mu, \sigma)\)라고 씁니다.

데이터의 모든 인스턴스가 저마다의 평균과 분산을 가지는 이러한 계층적 혼합 분포는 많은 생물학적 환경에서 유용한 모델입니다. 예제들이 그림 4.18에 나와 있습니다.

[![](imgs/LaplaceMixturePromoterLengths.png)](imgs/LaplaceMixturePromoterLengths.png "그림 4.18 (a): Kristiansson 등(2009)이 연구한 Saccharomyces cerevisiae의 2000bp보다 짧은 프로모터의 길이.")

(a) Kristiansson 등 ([2009](16-chap.html#ref-Kristiansson2009))이 연구한 _Saccharomyces cerevisiae_ 의 2000bp보다 짧은 프로모터의 길이.

[![](imgs/tcellhist.png)](imgs/tcellhist.png "그림 4.18 (b): 20,000개 유전자에 대한 마이크로어레이 유전자 발현 측정값의 로그 비(log-ratios) [@Purdom2005].")

(b) 20,000개 유전자에 대한 마이크로어레이 유전자 발현 측정값의 로그 비(log-ratios) ([Purdom and Holmes 2005](16-chap.html#ref-Purdom2005)).

그림 4.18: 실제 데이터의 히스토그램. 두 분포 모두 비대칭 라플라스 분포로 모델링될 수 있습니다.

__

질문 4.17

마이크로어레이로부터 얻은 유전자 발현 값들의 로그 비를 살펴보면, 그림 4.18의 오른쪽에 표시된 것과 같은 분포를 얻게 됩니다. 데이터가 이러한 형태의 히스토그램을 가지는 이유를 어떻게 설명할 수 있을까요?

라플라스 분포는 생성 과정에 대한 고려가 어떻게 분산과 평균이 연결되어 있는지를 나타내주는 한 예입니다. 비대칭 라플라스 분포 \(AL(\theta, \mu, \sigma)\)의 기댓값과 분산은 다음과 같습니다.

\[ E(X)=\theta+\mu\quad\quad\text{그리고}\quad\quad\text{var}(X)=\sigma^2+\mu^2. \tag{4.14}\]

\(\mu = 0\)인 경우(대칭 라플라스 분포의 사례)를 제외하고는 분산이 평균에 의존한다는 점에 주목하세요. 이 분포를 유용하게 만드는 것은 바로 이 특징입니다. 평균-분산 의존성을 갖는 것은 마이크로어레이 형광 강도든, 질량 분석기의 피크 높이든, 고처리량 시퀀싱의 리드 카운트든 물리적 측정값들에서 매우 흔하게 나타나며, 이는 다음 섹션에서 살펴볼 것입니다.

### 4.4.2 푸아송 변수들의 무한 혼합물

[![](imgs/three-worlds_web.jpg)](imgs/three-worlds_web.jpg "그림 4.19: 호수에서 물고기를 어떻게 셀까요? M.C. 에셔.")

그림 4.19: 호수에서 물고기를 어떻게 셀까요? M.C. 에셔.

실제 세계의 카운트 데이터를 모델링하기 위해 이와 유사한 2단계 계층 모델이 종종 필요합니다. 하위 레벨에서는 단순한 푸아송 및 이항 분포들이 구성 요소 역할을 하지만, 그들의 매개변수들은 어떤 기저의(잠재된) 과정에 의존할 수 있습니다. 예를 들어 생태학에서 우리는 한 지역의 모든 호수에 있는 물고기 종의 변동에 관심이 있을 수 있습니다. 우리는 각 호수의 물고기 종을 샘플링하여 그들의 실제 풍부도를 추정하며, 이는 푸아송 분포로 모델링될 수 있습니다. 하지만 실제 풍부도는 호수마다 다를 것입니다. 그리고 만약 우리가 예를 들어 기후나 고도의 변화가 어떤 역할을 하는지 보고 싶다면, 우리는 그러한 체계적인 효과들을 무작위적인 호수 간 변동으로부터 분리해내야 합니다. 서로 다른 푸아송 비율 매개변수 \(\lambda\)들은 비율들의 분포로부터 오는 것으로 모델링될 수 있습니다. 그러한 계층 모델은 또한 계층 구조에 보충적인 단계들을 추가할 수 있게 해주는데, 예를 들어 우리는 다양한 유형의 물고기에 관심이 있을 수 있고 고도 및 다른 환경적 요인들을 별도로 모델링할 수도 있습니다.

푸아송 변수들의 혼합물로 잘 모델링되는 샘플링 체계의 추가적인 예시로는 RNA-Seq과 같은 고처리량 시퀀싱 응용 분야가 있으며, 이는 [8장](08-chap.html)에서 자세히 다룰 것입니다. 또한 미생물 생태학에서 사용되는 16S rRNA-Seq 데이터도 여기에 해당합니다.

### 4.4.3 감마 분포: 두 개의 매개변수 (모양과 척도)

이제 우리는 이전에 보지 못했던 새로운 분포를 알아가려 합니다. 감마 분포는 (단일 매개변수를 가진) 지수 분포의 확장이며, 두 개의 매개변수를 가지고 있어 더 유연합니다. 이는 종종 계층 모델의 상위 레벨을 위한 구성 요소로 유용합니다. 감마 분포는 양수 값을 가지며 연속적입니다. 지수 분포의 밀도는 0에서 최댓값을 갖고 값이 무한대로 갈수록 단순히 0을 향해 감소하는 반면, 감마 분포의 밀도는 어떤 유한한 값에서 최댓값을 가집니다. 시뮬레이션 예제를 통해 이를 탐구해 봅시다. 그림 4.20의 히스토그램들은 다음 코드 라인들에 의해 생성되었습니다:

    
    
ggplot(tibble(x = rgamma(10000, shape = 2, rate = 1/3)),
       aes(x = x)) + geom_histogram(bins = 100, fill= "purple")
ggplot(tibble(x = rgamma(10000, shape = 10, rate = 3/2)),
       aes(x = x)) + geom_histogram(bins = 100, fill= "purple")__

[![](04-chap_files/figure-html/fig-gammahist1-1.png)](04-chap_files/figure-
html/fig-gammahist1-1.png "그림 4.20 (a): gamma(2, 1/3)")

(a) gamma\((2, 1/3)\)

[![](04-chap_files/figure-html/fig-gammahist1-2.png)](04-chap_files/figure-
html/fig-gammahist1-2.png "그림 4.20 (b): gamma(10, 3/2)")

(b) gamma\((10, 3/2)\)

그림 4.20: 감마 분포의 무작위 표본 히스토그램. 감마 분포는 유연한 2-매개변수 분포입니다: [위키백과 참조](http://en.wikipedia.org/wiki/Gamma_distribution).

#### 감마-포아송 혼합물: 계층적 모델

  1. 감마 분포로부터 매개변수 세트 \(\lambda_1, \lambda_2, ...\)를 생성합니다.

  2. 이들을 사용하여 각 \(\lambda_i\)에 대해 하나씩 포아송(\(\lambda_i\)) 확률 변수 세트를 생성합니다.

    
    
    lambda = rgamma(10000, shape = 10, rate = 3/2)
    gp = rpois(length(lambda), lambda = lambda)
    ggplot(tibble(x = gp), aes(x = x)) +
      geom_histogram(bins = 100, fill= "purple")__

[![](04-chap_files/figure-html/fig-
generatepoissongamma-1.png)](04-chap_files/figure-html/fig-
generatepoissongamma-1.png "그림 4.21: 감마-포아송 계층 모델을 통해 생성된 gp의 히스토그램.")

그림 4.21: 감마-포아송 계층 모델을 통해 생성된 `gp`의 히스토그램.

결과값들은 감마-포아송 혼합물로부터 왔다고 말합니다. 그림 4.21은 `gp`의 히스토그램을 보여줍니다.

__

질문 4.18

  1. 그러한 감마-포아송 혼합물로부터 생성된 값들은 연속형인가요, 아니면 이산형인가요?  

  2. 이 분포의 또 다른 이름은 무엇인가요? 힌트: **[vcd](https://cran.r-project.org/web/packages/vcd/)** 패키지의 `goodfit` 함수에 의해 제공되는 서로 다른 분포들을 시도해 보세요.

__

해결책

__

    
    
    library("vcd")
    ofit = goodfit(gp, "nbinomial")
    plot(ofit, xlab = "")
    ofit$par __
    
    
    $size
    [1] 9.911829
    
    $prob
    [1] 0.5963857

[![](04-chap_files/figure-html/fig-goofy-1.png)](04-chap_files/figure-
html/fig-goofy-1.png "그림 4.22: 적합도(Goodness of fit) 플롯. 루토그램은 감마-포아송 분포(일명 음이항)의 이론적 확률을 빨간색 점으로, 관측 빈도의 제곱근을 직사각형 막대의 높이로 보여줍니다. 막대들은 모두 가로축 가까이에서 끝나며, 이는 음이항 분포에 잘 맞음을 나타냅니다.")

그림 4.22: 적합도(Goodness of fit) 플롯. **루토그램** 은 감마-포아송 분포(일명 음이항 분포)의 이론적 확률을 빨간색 점으로, 관측 빈도의 제곱근을 직사각형 막대의 높이로 보여줍니다. 막대들은 모두 가로축 가까이에서 끝나며, 이는 음이항 분포에 잘 맞음을 나타냅니다.

R과 다른 여러 곳에서 감마-포아송 분포는 **음이항 분포(negative binomial distribution)** 라는 가명으로 통용되기도 합니다. 이 두 명칭은 동의어입니다; 두 번째 명칭은 식 4.15가 이항 분포의 확률과 어떤 형식적인 유사성을 가진다는 사실을 암시합니다. 첫 번째 명칭인 감마-포아송 분포는 그 생성 메커니즘을 더 잘 나타내며, 이것이 우리가 책의 나머지 부분에서 사용할 명칭입니다. 이는 이산 분포이며, 즉 (양의 실수축 전체를 커버하는 감마 분포와 대조적으로) 자연수 값만을 취한다는 뜻입니다. 그 확률 분포는 다음과 같습니다.

\[ \text{P}(K=k)=\left(\begin{array}{c}k+a-1\\k\end{array}\right) \, p^a \, (1-p)^k, \tag{4.15}\]

이는 두 매개변수 \(a\in\mathbb{R}^+\)와 \(p\in[0,1]\)에 의존합니다. 동등하게, 두 매개변수는 평균 \(\mu=pa/(1-p)\)와 **분산(dispersion)** 이라 불리는 매개변수 \(\alpha=1/a\)로 표현될 수 있습니다. 분포의 분산(variance)은 이 매개변수들에 의존하며, \(\mu+\alpha\mu^2\)입니다.

[![](04-chap_files/figure-html/fig-mixtures-
dgammapois-1.png)](04-chap_files/figure-html/fig-mixtures-
dgammapois-1.png "그림 4.23: 감마-포아송 분포를 생성하는 계층 모델의 시각화. 상단 패널은 평균 50(수직 검은색 선)과 분산 30인 감마 분포의 밀도를 보여줍니다. 하나의 특정 실험 반복에서 값 60이 실현되었다고 가정해 봅시다. 이것이 우리의 잠재 변수입니다. 관찰 가능한 결과는 해당 비율 매개변수를 가진 푸아송 분포에 따라 분포하며, 중간 패널에 표시되어 있습니다. 하나의 특정 실험에서 결과는 예를 들어 55일 수 있으며, 점선으로 표시된 초록색 선으로 나타나 있습니다. 전체적으로, 만약 우리가 이러한 두 개의 연속적인 무작위 과정을 여러 번 반복한다면, 결과는 하단 패널에 표시된 것과 같이 분포될 것입니다 – 이것이 감마-포아송 분포입니다.")

그림 4.23: 감마-포아송 분포를 생성하는 계층 모델의 시각화. 상단 패널은 평균 50(수직 검은색 선)과 분산 30인 감마 분포의 밀도를 보여줍니다. 하나의 특정 실험 반복에서 값 60이 실현되었다고 가정해 봅시다. 이것이 우리의 잠재 변수입니다. 관찰 가능한 결과는 해당 비율 매개변수를 가진 푸아송 분포에 따라 분포하며, 중간 패널에 표시되어 있습니다. 하나의 특정 실험에서 결과는 예를 들어 55일 수 있으며, 점선으로 표시된 초록색 선으로 나타나 있습니다. 전체적으로, 만약 우리가 이러한 두 개의 연속적인 무작위 과정을 여러 번 반복한다면, 결과는 하단 패널에 표시된 것과 같이 분포될 것입니다 – 이것이 감마-포아송 분포입니다.

__

질문 4.19

설명을 위한 시뮬레이션보다 분석적인 유도에 더 관심이 있다면, 감마-포아송 확률 분포의 수학적 유도를 직접 작성해 보세요.

__

해결책

__

최종 분포는 2단계 과정의 결과임을 상기하세요:

  1. 밀도로부터 \(\text{gamma}(a,b)\) 분포를 따르는 수 \(x\)를 생성합니다.

\[ f_\Gamma(x, a, b)=\frac{b^a}{\Gamma(a)}\,x^{a-1}\,e^{-b x}, \tag{4.16}\]

여기서 \(\Gamma\)는 소위 \(\Gamma\)-함수이며, \(\Gamma(a)=\int_0^\infty x^{a-1}\,e^{-x}\,\text{d}x\)입니다 (이러한 부수적인 관계가 있음에도 불구하고 감마 분포와 혼동해서는 안 됩니다).

  2. 비율 \(x\)를 가진 푸아송 분포로부터 수 \(k\)를 생성합니다. 확률 분포는 다음과 같습니다.

\[ f_\text{Pois}(k, \lambda=x)=\frac{x^{k}e^{-x}}{k!} \]

만약 \(x\)가 오직 유한한 값들의 집합만을 취한다면, 우리는 각각의 가능한 경우에 대해 \(f_\Gamma\)에 따른 그들의 확률로 가중치를 주어 합산함으로써 문제를 해결할 수 있을 것입니다. 하지만 \(x\)는 연속적이므로, 우리는 이산적인 합 대신 적분으로 써야 합니다. 우리는 \(K\)의 분포를 주변 분포(marginal)라고 부릅니다. 그 확률 질량 함수는 다음과 같습니다.

\[ \begin{aligned} P(K=k)&=&\int_{x=0}^{\infty} \, f_\text{Pois}(k, \lambda=x)\; f_\Gamma(x, a, b) \;dx\\ &=& \int_{x=0}^{\infty} \frac{x^{k}e^{-x}}{k!}\,\frac{b^a}{\Gamma(a)} x^{a-1}e^{-bx}\; dx \end{aligned} \]

항들을 정리하고 \(x\)와 무관한 항들을 적분 밖으로 꺼냅니다.

\[ P(K=k)=\frac{b^a}{\Gamma(a)\,k!} \int_{x=0}^{\infty} x^{k+a-1}e^{-(b+1)x} dx \]

우리는 감마 밀도의 합이 1이라는 것을 알기 때문에: \(\int_0^\infty x^{k+a-1}e^{-(b+1)x} dx = \frac{\Gamma(k+a)}{(b+1)^{k+a}}\) 

\[ \begin{aligned} P(K=k) &= \frac{\Gamma(k+a)}{\Gamma(a)\,\Gamma(k+1)}\frac{b^a}{(b+1)^{a}(b+1)^k} ={k+a-1\choose k}\left(\frac{b}{b+1}\right)^a\left(1-\frac{b}{b+1}\right)^k \end{aligned} \]

여기서 마지막 줄에는 \(\Gamma(v+1)=v!\)임을 사용했습니다. 이는 크기 매개변수 \(a\)와 확률 \(p=\frac{b}{b+1}\)을 가진 감마-포아송 분포인 식 4.15와 동일합니다.

### 4.4.4 분산 안정화 변환 (Variance stabilizing transformations)

우리가 실험 데이터를 분석할 때 제어해야 할 핵심 이슈 중 하나는 기저의 동일한 실제 값에 대한 반복 측정들 사이에, 즉 반복 실험들 사이에 가변성이 얼마나 되느냐 하는 것입니다. 이것이 우리가 어떤 실제적인 차이(예: 서로 다른 조건들 사이의 차이)를 얼마나 잘 볼 수 있는지를 결정할 것입니다. 이 장에서 공부한 계층 모델 유형을 통해 발생하는 데이터는 종종 매우 불균질한 분산을 가지는 것으로 나타나며, 이는 도전 과제가 될 수 있습니다. 우리는 그러한 경우에 **분산 안정화 변환** ([Anscombe 1948](16-chap.html#ref-Anscombe1948))이 어떻게 도움이 될 수 있는지 볼 것입니다. 비율이 10부터 100까지인 일련의 푸아송 변수들로부터 시작해 봅시다:

데이터프레임(더 정확하게는 _tibble_) `simdat`를 어떻게 구성하는지 주목하세요: `lapply` 루프의 출력은 `lam`의 각 값에 대해 하나씩인 _tibble_ 들의 리스트입니다. 파이프 연산자 `|>`를 사용하여 이를 (**[dplyr](https://cran.r-project.org/web/packages/dplyr/)** 패키지의) `bind_rows` 함수로 보냅니다. 그 결과는 모든 리스트 원소들이 서로 깔끔하게 쌓인 데이터프레임이 됩니다.

    
    
simdat = lapply(seq(10, 100, by = 10), function(lam)
        tibble(n = rpois(200, lambda = lam),
               `sqrt(n)` = sqrt(n),
           lambda = lam)) |>
      bind_rows() |
      tidyr::pivot_longer(cols = !lambda)
ggplot(simdat, aes(x = factor(lambda), y = value)) + xlab(expression(lambda)) +
  geom_violin() + facet_grid(rows = vars(name), scales = "free")__

[![](04-chap_files/figure-html/fig-
seriesofpoisson-1.png)](04-chap_files/figure-html/fig-
seriesofpoisson-1.png "그림 4.24: 여덟 가지 서로 다른 평균 lambda 선택에 대한 푸아송 분포 측정 데이터. 상단 패널에서 y축은 데이터에 비례하며, 하단 패널은 제곱근 스케일입니다. 첫 번째 사례에서는 분포 너비가 어떻게 변하는지 주목하세요. 반면 두 번째 사례에서는 그 정도가 덜합니다.")

그림 4.24: 여덟 가지 서로 다른 평균 `lambda` 선택에 대한 푸아송 분포 측정 데이터. 상단 패널에서 \(y\)축은 데이터에 비례하며, 하단 패널은 제곱근 스케일입니다. 첫 번째 사례에서는 분포 너비가 어떻게 변하는지 주목하세요. 반면 두 번째 사례에서는 그 정도가 덜합니다.

그림 4.24의 상단 패널에서 우리가 보는 데이터는 **이분산성(heteroscedasticity)** 이라 불리는 사례입니다: 우리 데이터 공간의 서로 다른 영역에서 데이터의 표준 편차(또는 동등하게 분산)가 다릅니다. 특히 가로축을 따라 평균과 함께 증가합니다. 푸아송 분포의 경우, 우리는 표준 편차가 평균의 제곱근이라는 것을 실제로 알고 있습니다; 다른 유형의 데이터에 대해서는 다른 의존성이 있을 수 있습니다. 이는 우리가 이후 분석 기법(예: 회귀 분석 또는 통계적 검정)을 적용하고 싶을 때 문제가 될 수 있는데, 그러한 기법들은 분산이 동일하다는 가정에 기초하고 있기 때문입니다. 그림 4.24에서 각 람다 값에 대한 반복 실험 횟수는 꽤 큽니다. 실제로는 항상 그렇지는 않습니다. 게다가 데이터가 우리 시뮬레이션에서처럼 알려진 평균에 의해 명시적으로 계층화되어 있지도 않으므로, 비록 이분산성이 존재하더라도 이를 보기가 더 어려울 수 있습니다. 하지만 그림 4.24의 하단 패널에서 보듯이, 우리가 단순히 제곱근 변환을 적용한다면 변환된 변수들은 대략 동일한 분산을 가질 것입니다. 이는 각 관측치에 대한 기저의 평균을 모르더라도 작동하며, 제곱근 변환은 이 정보를 필요로 하지 않습니다. 우리는 다음과 같은 코드로 이를 좀 더 정량적으로 확인할 수 있는데, 이는 서로 다른 `lambda` 선택에 대해 샘플링된 값 `n`과 `sqrt(n)`의 표준 편차를 보여줍니다.

제곱근 변환된 값들의 표준 편차는 일관되게 0.5 근처이므로, 우리는 단위 분산을 달성하기 위해 `2*sqrt(n)` 변환을 사용할 것입니다.

    
    
    summarise(group_by(simdat, name, lambda), sd(value)) |> tidyr::pivot_wider(values_from = `sd(value)`)__
    
    
    # A tibble: 10 × 3
       lambda     n `sqrt(n)`
        <dbl> <dbl>     <dbl>
     1     10  2.95     0.478
     2     20  4.19     0.470
     3     30  5.62     0.521
     4     40  5.99     0.473
     5     50  7.69     0.546
     6     60  7.59     0.492
     7     70  8.69     0.520
     8     80  8.99     0.505
     9     90  9.44     0.498
    10    100  9.84     0.495

감마-포아송 분포를 사용한 또 다른 예제가 그림 4.25에 나와 있습니다. 우리는 감마-포아송 변수 `u`5를 생성하고 평균 주변의 95% 신뢰 구간을 플롯합니다.

5 너무 조밀한 시퀀스를 만들지 않으면서도 평균값 `mu`에 대해 더 넓은 범위의 값을 포착하기 위해, 우리는 기하 급수 \(\mu_{i+1} = 2\mu_i\)를 사용합니다.

    
    
    muvalues = 2^seq(0, 10, by = 1)
    simgp = lapply(muvalues, function(mu) {
      u = rnbinom(n = 1e4, mu = mu, size = 4)
      tibble(mean = mean(u), sd = sd(u),
             lower = quantile(u, 0.025),
             upper = quantile(u, 0.975),
             mu = mu)
      } ) |> bind_rows()
    head(as.data.frame(simgp), 2)__
    
    
        mean       sd lower upper mu
    1 0.9965 1.106440     0     4  1
    2 2.0233 1.748503     0     6  2
    
    
    ggplot(simgp, aes(x = mu, y = mean, ymin = lower, ymax = upper)) +
      geom_point() + geom_errorbar()__

[![](04-chap_files/figure-html/fig-seriesofnb-1.png)](04-chap_files/figure-
html/fig-seriesofnb-1.png "그림 4.25: 1부터 1024까지의 \mu 범위에 대한 감마-포아송 분포 측정 데이터.")

그림 4.25: 1부터 1024까지의 \(\mu\) 범위에 대한 감마-포아송 분포 측정 데이터.

__

질문 4.20

푸아송 분포 데이터에 대한 제곱근 함수와 유사하게, 이 데이터에 대해 분산을 안정화하는 변환을 어떻게 찾을 수 있을까요?

__

해결책

__

만약 우리가 `mu[1]`에 대응하는 (그리고 `simgp$mean[1]` 주변에 중심을 둔) 값들을 그들의 표준 편차 `simgp$sd[1]`로 나누고, `mu[2]`에 대응하는 값들을 그들의 표준 편차 `simgp$sd[2]`로 나누는 식이라면, 그 결과로 나오는 값들은 설계에 의해 1의 표준 편차(따라서 분산)를 가질 것입니다. 그리고 11개의 개별적인 변환을 정의하는 대신, 우리는 적절한 지점에서 적절한 기울기를 가진 하나의 조각별 선형(piecewise linear) **및** 연속 함수를 정의함으로써 우리의 목표를 달성할 수 있습니다.

    
    
simgp = mutate(simgp,
      slopes = 1 / sd,
      trsf   = cumsum(slopes * mean))
ggplot(simgp, aes(x = mean, y = trsf)) +
  geom_point() + geom_line() + xlab("")__

[![](04-chap_files/figure-html/fig-pcwlin-1-1.png)](04-chap_files/figure-
html/fig-pcwlin-1-1.png "그림 4.26: 그림 4.25의 데이터 분산을 안정화하는 조각별 선형 함수.")

그림 4.26: 그림 4.25의 데이터 분산을 안정화하는 조각별 선형 함수.

우리는 그림 4.26에서 이 함수가 특히 하단 끝부분에서 제곱근 함수와 어느 정도 닮았음을 볼 수 있습니다. 상단 끝부분에서는 로그 함수와 더 비슷해 보입니다. 수학적으로 더 깊이 있는 분들은 이러한 수치적 계산의 우아한 확장이 소위 **델타 방법(delta method)** 이라 불리는 약간의 미적분학을 통해 다음과 같이 수행될 수 있음을 알 수 있을 것입니다.

우리의 변환 함수를 \(g\)라고 부르고, 그것이 미분 가능하다고 가정합시다 (이는 매우 합리적인 가정입니다: 우리가 여기서 고려할 만한 거의 모든 함수는 미분 가능하니까요). 또한 우리 확률 변수를 \(X_i\)라고 부르고, 평균을 \(\mu_i\), 분산을 \(v_i\)라고 합시다. 그리고 \(v_i\)와 \(\mu_i\)가 함수 관계 \(v_i = v(\mu_i)\)에 의해 연결되어 있다고 가정합니다. 그러면 그 평균 \(\mu_i\) 근처의 \(X_i\) 값들에 대해,

\[ g(X_i) = g(\mu_i) + g'(\mu_i) (X_i-\mu_i) + ... \tag{4.17}\]

여기서 점들은 우리가 무시할 수 있는 고차 항들을 나타냅니다. 변환된 값들의 분산은 대략 다음과 같습니다.

\[ \begin{align} \text{Var}(g(X_i)) &\simeq g'(\mu_i)^2 \\ \text{Var}(X_i) &= g'(\mu_i)^2 \, v(\mu_i), \end{align} \tag{4.18}\]

여기서 우리는 \(c\)가 상수일 때마다 성립하는 규칙들인 \(\text{Var}(X-c)=\text{Var}(X)\)와 \(\text{Var}(cX)=c^2\,\text{Var}(X)\)를 사용했습니다. 이것이 상수가 되어야 한다는 요구 조건은 다음과 같은 미분 방정식으로 이어집니다.

\[ g'(x) = \frac{1}{\sqrt{v(x)}}. \tag{4.19}\]

주어진 평균-분산 관계 \(v(\mu)\)에 대해, 우리는 함수 \(g\)에 대해 이를 풀 수 있습니다. 몇 가지 간단한 사례들을 확인해 봅시다:

  * 만약 \(v(\mu)=\mu\) (푸아송)라면, 우리는 제곱근 변환인 \(g(x)=\sqrt{x}\)를 얻게 됩니다.

  * 만약 \(v(\mu)=\alpha\,\mu^2\)라면, 미분 방정식 4.19를 푸는 것은 \(g(x)=\log(x)\)를 줍니다. 이것이 왜 많은 데이터 분석 응용 분야에서 로그 변환이 그렇게 인기 있는지 설명해 줍니다: 데이터가 일정한 변동 계수(coefficient of variation)를 가질 때, 즉 표준 편차가 평균에 비례할 때마다 로그 변환은 분산 안정화 변환으로서 작용합니다.

__

질문 4.21

\(v(\mu) = \mu + \alpha\,\mu^2\)와 연관된 분산 안정화 변환은 무엇인가요?

__

해결책

__

이 함수 \(v(\cdot)\)을 사용하여 미분 방정식 4.19를 풀려면, 다음 적분을 계산해야 합니다.

\[\label{eq:} \int \frac{dx}{\sqrt{x + \alpha x^2}}. \tag{4.20}\]

닫힌 형태의 표현식은 ([Bronštein and Semendjajew 1979](16-chap.html#ref-BronsteinSemendjajew))와 같은 참조 표에서 찾아볼 수 있습니다. 이 저자들은 다음과 같은 일반적인 해를 제공합니다.

\[ \int \frac{dx}{\sqrt{ax^2+bx+c}} = \frac{1}{\sqrt{a}} \ln\left(2\sqrt{a(ax^2+bx+c)}+2ax+b\right) + \text{const.}, \tag{4.21}\]

여기에 우리 특수 사례인 \(a=\alpha, b=1, c=0\)을 대입하면 분산 안정화 변환을 얻게 됩니다.

\[ \begin{align} g_\alpha(x) &= \frac{1}{2\sqrt{\alpha}} \ln\left(2\sqrt{\alpha x (\alpha x+1)} + 2\alpha x + 1\right) \\ &= \frac{1}{2\sqrt{\alpha}} {\displaystyle \operatorname {arcosh}} (2\alpha x+1).\ \end{align} \tag{4.22}\]

식 4.22의 두 번째 줄에는 항등식 \({\displaystyle \operatorname {arcosh}}(z) = \ln\left(z+\sqrt{z^2-1}\right)\)을 사용했습니다. \(\alpha\to0\)인 극한에서는 선형 근사 \(\ln(1+\varepsilon)=\varepsilon+O(\varepsilon^2)\)를 사용하여 \(g_0(x)=\sqrt{x}\)임을 알 수 있습니다. 만약 \(g_\alpha\)가 분산 안정화 변환이라면 임의의 수 쌍 \(u\)와 \(v\)에 대해 \(ug_\alpha+v\) 또한 마찬가지이며, 우리는 나중에 분명해질 이유로 추가적인 인자 \(\frac{1}{2}\)을 넣기 위해 이 자유를 사용했음에 유의하세요. 여러분은 식 4.22의 함수 \(g_\alpha\)가 그 도함수를 계산함으로써 조건 4.19를 충족한다는 것을 확인할 수 있는데, 이는 초등적인 계산입니다. 이를 플롯해 봅시다:

    
    
f = function(x, a) 
  ifelse (a==0, 
    sqrt(x), 
    log(2*sqrt(a) * sqrt(x*(a*x+1)) + 2*a*x+1) / (2*sqrt(a)))
x  = seq(0, 24, by = 0.1)
df = lapply(c(0, 0.05*2^(0:5)), function(a) 
  tibble(x = x, a = a, y = f(x, a))) %>% bind_rows()
ggplot(df, aes(x = x, y = y, col = factor(a))) + 
  geom_line() + labs(col = expression(alpha))__

[![](04-chap_files/figure-html/fig-
plotvstgammapoisson-1-1.png)](04-chap_files/figure-
html/fig-
plotvstgammapoisson-1-1.png "그림 4.27: 서로 다른 \alpha 선택에 대한 식 4.22 함수의 그래프.")

그림 4.27: 서로 다른 \(\alpha\) 선택에 대한 식 4.22 함수의 그래프.

그리고 식 4.22의 두 항의 동등성을 경험적으로 확인합니다:

    
    
f2 = function(x, a) ifelse (a==0, sqrt(x), acosh(2*a*x + 1) / (2*sqrt(a)))  
with(df, max(abs(f2(x,a) - y)))__
    
    
    [1] 8.881784e-16

그림 4.27에서 보듯이, 작은 \(x\) 값들에 대해 \(g_\alpha(x)\approx \sqrt{x}\) (\(\alpha\)에 관계없이)인 반면, 큰 값 (\(x\to\infty\))과 \(\alpha>0\)에 대해서는 로그 함수처럼 행동합니다:

\[ \begin{align} &\frac{1}{2\sqrt{\alpha}}\ln\left(2\sqrt{\alpha(\alpha x^2+x)}+2\alpha x+1\right)\\ \approx&\frac{1}{2\sqrt{\alpha}} \ln\left(2\sqrt{\alpha^2x^2}+2\alpha x\right)\\ = &\frac{1}{2\sqrt{\alpha}}\ln\left(4\alpha x\right)\\ = &\frac{1}{2\sqrt{\alpha}}\ln x+\text{const.} \end{align} \]

우리는 예를 들어 다음과 같이 이를 경험적으로 확인할 수 있습니다.

    
    
      a = c(0.2, 0.5, 1)
      f(1e6, a) __
    
    
    [1] 15.196731 10.259171  7.600903
    
    
      1/(2*sqrt(a)) * (log(1e6) + log(4*a))__
    
    
    [1] 15.196728 10.259170  7.600902

## 4.5 이 장의 요약

우리는 생물학적 데이터를 모델링하기 위해 혼합물을 사용하는 동기 부여 예제와 방법들을 제시했습니다. 우리는 EM 알고리즘이 부분적이고 더 단순한 문제들 사이를 반복함으로써 추정하기 어려운 확률 모델을 데이터에 적합시키는 흥미로운 예시임을 보았습니다.

#### 유한 혼합 모델 (Finite mixture models)

우리는 서로 다른 평균과 분산을 가진 두 개 이상의 정규 분포 혼합물을 모델링하는 방법을 보았습니다. 우리는 EM 알고리즘을 사용하여 잠재 변수를 모르더라도 그러한 혼합물로부터 얻은 주어진 데이터 표본을 분해하는 방법을 보았습니다. EM 접근법은 우리가 분포의 모수적 형태와 성분의 수를 알고 있을 것을 요구합니다. [5장](05-chap.html)에서 우리는 그러한 정보에 의존하지 않고도 데이터에서 그룹을 찾는 방법을 볼 것인데, 이를 군집화(clustering)라고 부릅니다. 우리는 군집화와 혼합 모델링 사이에 강력한 개념적 관계가 있음을 염두에 둘 수 있습니다.

#### 흔한 무한 혼합 모델 (Common infinite mixture models)

무한 혼합 모델은 이항, 정규, 푸아송과 같은 더 기본적인 분포들로부터 새로운 분포들(감마-포아송이나 라플라스와 같은)을 구축하는 데 좋습니다. 흔한 예시들은 다음과 같습니다.

  * 정규 혼합물 (종종 평균과 분산에 대한 계층 모델을 가짐);

  * 베타-이항 혼합물 – 이항 분포에서의 확률 \(p\)가 \(\text{beta}(a, b)\) 분포에 따라 생성되는 경우;

  * 리드 카운트를 위한 감마-포아송 ( [8장](08-chap.html) 참조);

  * PCR을 위한 감마-지수.

#### 응용

실험적 가변성의 레이어가 여러 개 있을 때마다 혼합 모델은 유용합니다. 예를 들어, 가장 낮은 레이어에서 우리의 측정 정밀도는 기본적인 물리적 감지 한계에 의해 제한될 수 있으며, 이는 카운팅 기반 어세이의 경우 푸아송 분포로, 연속형 측정의 경우 정규 분포로 모델링될 수 있습니다. 그 위에는 기계 간 변동, 시약 변동, 작업자 변동 등의 레이어가 하나(또는 그 이상) 있을 수 있습니다.

혼합 모델은 데이터에 이질적인 양의 가변성(분산)이 있는 경우가 많다는 사실을 반영합니다. 그러한 경우, 후속 시각화나 분석 전에 적절한 데이터 변환, 즉 분산 안정화 변환이 필요합니다. 우리는 [8장](08-chap.html)에서 RNA-Seq에 대한 예제를 심도 있게 공부할 것이며, 이는 미생물 생태학에서의 차세대 리드 정규화에서도 유용함이 입증되었습니다 ([McMurdie and Holmes 2014](16-chap.html#ref-mcmurdie2014)).

혼합 모델링의 또 다른 중요한 응용은 다중 검정에서의 2-성분 모델이며 – 우리는 이에 대해 [6장](06-chap.html)에서 다시 다룰 것입니다.

#### ECDF와 붓스트랩

우리는 관측된 표본을 혼합물로 사용함으로써 추정치의 표집 분포에 대해 알려주는 많은 시뮬레이션된 표본들을 생성할 수 있음을 보았습니다. 이 방법을 붓스트랩이라고 부르며, 닫힌 형태의 표현식이 가용하지 않을 때에도 추정치를 평가할 수 있는 방법을 제공하므로 우리는 여러 번 이 방법으로 돌아올 것입니다 (우리는 이를 비모수적이라고 부릅니다).

## 4.6 더 읽을거리

유한 혼합 모델에 대한 완전한 단행본 수준의 연구는 McLachlan과 Peel ([2004](16-chap.html#ref-mclachlan2004))에 의해 이루어졌습니다; EM 알고리즘에 대해서는 McLachlan과 Krishnan ([2007](16-chap.html#ref-mclachlan2007algorithm))의 책 또한 참조하십시오. Majorize-Minimization (MM) 프레임워크 내에서 모든 EM 유형 알고리즘을 제시하는 최근의 책은 Lange ([2016](16-chap.html#ref-lange2016mm))에 의한 것입니다.

사실 많은 자연 현상이 혼합물로 보일 수 있는 수학적 이유가 있습니다: 이는 관찰된 사건들이 교환 가능할(발생 순서가 중요하지 않을) 때 발생합니다. 이 이면의 이론은 상당히 수학적이며, 시작하기 좋은 방법은 위키백과 항목과 Diaconis 및 Freedman ([1980](16-chap.html#ref-diaconis1980finite))의 논문을 살펴보는 것입니다.

특히, 우리는 고처리량 데이터를 위해 혼합물을 사용합니다. 여러분은 [8장](08-chap.html)과 [11장](11-chap.html)에서 그 예시들을 보게 될 것입니다.

붓스트랩은 많은 상황에서 사용될 수 있으며 알아두면 매우 유용한 도구입니다. 친절한 설명은 ([B. Efron and Tibshirani 1993](16-chap.html#ref-efront))에서 제공됩니다.

역사적으로 흥미로운 논문은 Anscombe ([1948](16-chap.html#ref-Anscombe1948))에 의한 분산 안정화에 관한 독창적인 기사로, 그는 푸아송 및 감마-포아송 확률 변수에 대해 분산 안정화 변환을 만드는 방법들을 제안했습니다. 분산 안정화는 Rice ([2006, chap. 6](16-chap.html#ref-Rice:2007))와 Kéry 및 Royle ([2015, 35](16-chap.html#ref-Kery2015))와 같은 이론 통계학의 많은 표준 교재들에서 델타 방법을 사용하여 설명됩니다.

Kéry와 Royle ([2015](16-chap.html#ref-Kery2015))는 니치(niche) 및 공간 생태학에서의 풍부도 추정을 위한 계층 모델을 구축하기 위해 R을 사용하는 훌륭한 탐색을 제공합니다.

## 4.7 연습 문제

__

연습 문제 4.1

**EM 알고리즘 단계별 수행.** 예제 데이터 세트로 `Myst.rds` 파일에 있는 값들을 사용합니다. 언제나 그렇듯이, 먼저 데이터를 시각화하는 것이 좋습니다. 히스토그램은 그림 4.28에 나와 있습니다. 우리는 이 데이터를 알려지지 않은 평균과 표준 편차, 그리고 알려지지 않은 혼합 비율을 가진 두 정규 분포의 혼합물로 모델링할 것입니다. 두 성분을 A와 B라고 부르겠습니다.

    
    
mx = readRDS("../data/Myst.rds")$yvar
str(mx)__
    
    
     num [1:1800] 0.3038 0.0596 -0.0204 0.1849 0.2842 ...
    
    
ggplot(tibble(mx), aes(x = mx)) + geom_histogram(binwidth = 0.025)__

우리는 먼저 `mx`에 있는 각 값들에 대해 각 성분에 대한 멤버십 가중치를 무작위로 할당하는 것으로 시작합니다.

    
    
wA = runif(length(mx))
wB = 1 - wA __

또한 관리용 변수들을 설정해야 합니다: `iter`는 EM 알고리즘의 반복 횟수를 셉니다; `loglik`은 현재의 로그 우도를 저장합니다; `delta`는 이전 반복에서 현재 반복까지의 로그 우도 변화를 저장합니다. 우리는 또한 알고리즘의 매개변수인 `tolerance`, `miniter`, `maxiter`를 정의합니다.

    
    
    iter      = 0
    loglik    = -Inf
    delta     = +Inf
    tolerance = 1e-12
    miniter   = 50
    maxiter   = 1000 __

아래 코드를 공부하고 다음 질문들에 답하세요:

  1. 어느 줄이 E 단계에 해당하고, 어느 줄이 M 단계에 해당하나요?

  2. `tolerance`, `miniter`, `maxiter`의 역할은 무엇인가요?

  3. 여기서 우리가 수행하는 작업의 결과를 **[mixtools](https://cran.r-project.org/web/packages/mixtools/)** 패키지의 `normalmixEM` 함수 출력과 비교해 보세요.

    
    
    while((delta > tolerance) && (iter <= maxiter) || (iter < miniter)) {
      lambda = mean(wA)
      muA = weighted.mean(mx, wA)
      muB = weighted.mean(mx, wB)
      sdA = sqrt(weighted.mean((mx - muA)^2, wA))
      sdB = sqrt(weighted.mean((mx - muB)^2, wB))
    
      pA   =    lambda    * dnorm(mx, mean = muA, sd = sdA)
      pB   = (1 - lambda) * dnorm(mx, mean = muB, sd = sdB)
      ptot = pA + pB
      wA   = pA / ptot
      wB   = pB / ptot
    
      loglikOld = loglik
      loglik = sum(log(pA + pB))
      delta = abs(loglikOld - loglik)
      iter = iter + 1
    }
    iter __
    
    
    [1] 447
    
    
c(lambda, muA, muB, sdA, sdB)__
    
    
    [1]  0.4756 -0.1694  0.1473  0.0983  0.1498

[![](04-chap_files/figure-html/fig-
EMillustrate-1-1.png)](04-chap_files/figure-html/fig-
EMillustrate-1-1.png "그림 4.28: EM 알고리즘 예제 데이터인 mx의 히스토그램.")

그림 4.28: EM 알고리즘 예제 데이터인 `mx`의 히스토그램.

__

해결책

__

`while` 루프의 처음 다섯 줄은 **최대화(Maximization) 단계** 를 구현합니다. `wA`와 `wB`의 현재 값이 주어졌을 때, 우리는 최대 우도 추정량들을 사용하여 혼합 모델의 매개변수들을 추정합니다: 혼합 비율 `lambda`는 `wA`의 평균에 의해, 두 정규 분포 성분의 매개변수들(`muA`, `sdA`) 및 (`muB`, `sdB`)은 표본 평균과 표본 표준 편차에 의해 추정됩니다. 멤버십 가중치를 고려하기 위해, 우리는 가중 평균(`weighted.mean` 함수)과 가중 표준 편차를 사용합니다.

그다음은 **기댓값(Expectation) 단계** 가 옵니다. 데이터 벡터 `mx`에 있는 각 요소들에 대해, 우리는 정규 밀도 함수 `dnorm`을 사용하여 생성 분포 모델 A와 B에 대한 확률 밀도 `pA`와 `pB`를 계산하며, 이들은 각각 혼합 비율 `lambda`와 `(1-lambda)`에 의해 가중치가 부여됩니다. 이로부터 우리는 식 4.5에 따라 업데이트된 멤버십 가중치 `wA`와 `wB`를 계산합니다.

멤버십 가중치와 매개변수들이 주어졌을 때, 로그 우도 `loglik`은 쉽게 계산되며, `while` 루프는 이 단계들을 반복합니다.

루프의 종료 기준은 우도의 변화인 `delta`에 기초합니다. 만약 이것이 `tolerance`보다 작아지면 루프는 끝날 수 있습니다. 이는 알고리즘이 수렴했는지 확인하는 간단한 방식입니다. `iter`에 대한 추가 조건들은 적어도 `miniter`번의 반복이 실행되도록 보장하며, 루프가 항상 `maxiter`번의 반복 후에 멈추도록 보장합니다. 후자는 알고리즘이 어떠한 경우에도 유한한 시간 내에 종료되도록 하기 위함입니다. (이러한 반복 알고리즘의 "전문적인" 구현체들은 대개 멈출 최적의 시점을 결정하기 위해 좀 더 세심하게 작동합니다.)

마지막으로, 우리 추정치를 **[mixtools](https://cran.r-project.org/web/packages/mixtools/)** 패키지의 `normalmixEM` 함수로부터 얻은 추정치와 비교해 봅시다.

    
    
gm = mixtools::normalmixEM(mx, k = 2)__
    
    
    number of iterations= 215 
    
    
    with(gm, c(lambda[1], mu, sigma))__
    
    
    [1]  0.4757 -0.1694  0.1473  0.0983  0.1498

__

연습 문제 4.2

우리는 왜 종종 우도 그 자체보다 우도의 로그를 고려할까요? 예를 들어 위에서의 EM 코드에서, 왜 우리는 로그 스케일에서의 확률을 가지고 작업했을까요?

__

해결책

__

(데이터 포인트들이 독립적으로 샘플링될 때마다) 우도는 종종 곱(product)의 형태를 취합니다. 예를 들어 식 4.4의 경우가 그러합니다. 우도 최적화를 위해 미분을 계산할 때, 곱의 미분법(product rule) 적용이 필요할 것입니다. 로그 스케일에서는 곱이 합으로 변하며, 합의 미분은 단지 개별 항들의 미분의 합일 뿐입니다.

추가적인 이유는 컴퓨터가 산술 연산을 구현하는 방식으로부터 옵니다. 컴퓨터는 대개 유한한 비트 수를 가진 숫자의 부동 소수점 표현을 사용합니다. 예를 들어 IEEE 754-2008 표준은 **배정밀도(double-precision)** 수에 대해 64비트를 사용합니다: 부호에 1비트, 가수(mantissa 또는 significand)에 52비트, 지수에 11비트입니다. 이러한 숫자들 사이의 곱셈은 지수의 덧셈을 수반하지만, 지수의 범위는 오직 \(0\)부터 \(2^{11}-1=2047\)까지입니다. 단지 수백 개의 데이터 포인트가 관여된 우도조차 산술 오버플로(overflow)나 정밀도 관련 문제들을 일으킬 수 있습니다. 곱이 합이 되는 로그 스케일에서는 연산 부하가 가수와 지수 사이에 더 잘 분배되는 경향이 있으며, 수백만 개의 데이터 포인트를 가진 로그 우도조차 합리적인 정밀도로 다뤄질 수 있습니다.

로그 확률 벡터 정규화를 위한 [Log-Sum-Exp 트릭](https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/)에 관한 Gregory Gundersen의 포스팅 또한 참조하세요.

__

연습 문제 4.3

4.4.3절의 추정치들인 `ofit$par`에 의해 주어진 매개변수들을 가진 감마-포아송 분포의 이론적 값들과 추정에 사용된 데이터를 QQ-플롯을 사용하여 비교하세요.

__

연습 문제 4.4

**회귀 분석을 위한 혼합 모델링 예제.** **[flexmix](https://cran.r-project.org/web/packages/flexmix/)** 패키지 ([Grün, Scharl, and Leisch 2012](16-chap.html#ref-Grun2012))는 우리가 데이터에 대해 군집화와 회귀 적합을 동시에 수행할 수 있게 해줍니다. **[flexmix](https://cran.r-project.org/web/packages/flexmix/)** 의 표준 M 단계인 `FLXMRglm`은 R의 일반화 선형 모델링 시설(`glm` 함수)에 대한 인터페이스입니다. 패키지와 예제 데이터 세트를 로드하세요.

    
    
    library("flexmix")
    data("NPreg")__

  1. 먼저 데이터를 플롯하고 점들이 어떻게 생성되었는지 추측해 보세요.

  2. 다음 명령어들을 사용하여 2-성분 혼합 모델을 적합시키세요.

    
    
m1 = flexmix(yn ~ x + I(x^2), data = NPreg, k = 2)__

  3. 혼합 성분들의 추정된 매개변수들을 살펴보고 실제 클래스 대 클러스터 멤버십을 교차 분류하는 정오표(truth table)를 만드세요. 객체 `m1`의 요약(summary)은 우리에게 무엇을 보여주나요?

  4. 데이터를 다시 플롯하되, 이번에는 각 점에 추정된 클래스에 따른 색상을 입히세요.

__

해결책

__

    
    
ggplot(NPreg, aes(x = x, y = yn)) + geom_point()__

[![](04-chap_files/figure-html/fig-npreg-1.png)](04-chap_files/figure-
html/fig-npreg-1.png "그림 4.29: 점들은 두 가지 서로 다른 생성 과정으로부터 온 것처럼 보입니다. 하나는 선형이고 다른 하나는 이차(quadratic)입니다.")

그림 4.29: 점들은 두 가지 서로 다른 생성 과정으로부터 온 것처럼 보입니다. 하나는 선형이고 다른 하나는 이차(quadratic)입니다.

성분들은 다음과 같습니다:

    
    
    modeltools::parameters(m1, component = 1)__
    
    
                          Comp.1
    coef.(Intercept) -0.20998685
    coef.x            4.81807854
    coef.I(x^2)       0.03613061
    sigma             3.47665584
    
    
    modeltools::parameters(m1, component = 2)__
    
    
                         Comp.2
    coef.(Intercept) 14.7167886
    coef.x            9.8468507
    coef.I(x^2)      -0.9683734
    sigma             3.4795657

두 성분의 매개변수 추정치들 모두 실제 값들에 가깝습니다. 실제 클래스와 클러스터 멤버십의 교차 집계표는 다음과 같이 얻을 수 있습니다.

    
    
table(NPreg$class, modeltools::clusters(m1))__
    
    
       
         1  2
      1 95  5
      2  5 95

우리 예제 데이터의 경우, 두 성분의 비율은 약 0.7이며, 이는 직선과 포물선이 교차하는 지점에서 클래스들이 중첩됨을 나타냅니다.

    
    
    summary(m1)__

요약 정보는 추정된 사전 확률 \(\hat\pi_k\), 두 클러스터에 할당된 관측치 수, \(p_{nk}>\delta\) (기본값 \(\delta=10^{-4}\))인 관측치 수, 그리고 후자 두 수치 사이의 비율을 보여줍니다. 잘 분리된 성분들의 경우, 0이 아닌 사후 확률 \(p_{nk}\)를 가진 관측치들의 큰 비율이 해당 클러스터에 할당되어야 하며, 이는 1에 가까운 비율을 주게 됩니다.

    
    
    NPreg = mutate(NPreg, gr = factor(class))
    ggplot(NPreg, aes(x = x, y = yn, group = gr)) +
       geom_point(aes(colour = gr, shape = gr)) +
       scale_colour_hue(l = 40, c = 180)__

[![](04-chap_files/figure-html/fig-npregC-1.png)](04-chap_files/figure-
html/fig-npregC-1.png "그림 4.30: 각 점에 추정된 클래스에 따라 색상을 입혀 flexmix를 사용한 회귀 분석 예제. 교차하는 지점에서 우리는 '식별 가능성(identifiability)' 문제를 가짐을 알 수 있습니다: 직선에 속하는 점들과 포물선에 속하는 점들을 구별할 수 없습니다.")

그림 4.30: 각 점에 추정된 클래스에 따라 색상을 입혀 `flexmix`를 사용한 회귀 분석 예제. 교차하는 지점에서 우리는 '식별 가능성(identifiability)' 문제를 가짐을 알 수 있습니다: 직선에 속하는 점들과 포물선에 속하는 점들을 구별할 수 없습니다.

__

연습 문제 4.5

**다른 계층적 노이즈 모델들:**  
분자 생물학 기술적 변동 모델링을 위해 다른 무한 혼합물들의 사용을 탐구한 두 논문을 찾으세요.

__

해결책

__

Chen, Xie, Story ([2011](16-chap.html#ref-Chen2011))의 논문은 비드 어레이(bead arrays)에서의 배경 노이즈 모델링을 위해 지수-푸아송 모델을 탐구합니다. Wills 등 ([2013](16-chap.html#ref-Wills2013))은 여러 푸아송 혼합 모델들을 비교합니다.

Anscombe, Francis J. 1948. “The Transformation of Poisson, Binomial and
Negative-Binomial Data.” _Biometrika_ , 246–54.

Bishop, Christopher M. 2006. _Pattern Recognition and Machine Learning_. Springer.

Bronštein, Il’ja N., and Konstantin A Semendjajew. 1979. _Taschenbuch Der
Mathematik_. B.G. Teubner Verlagsgesellschaft, Leipzig; Verlag Nauka, Moscow.

Bulmer, Michael George. 2003. _Francis Galton: Pioneer of Heredity and
Biometry_. JHU Press.

Chen, Min, Yang Xie, and Michael Story. 2011. “An Exponential-Gamma
Convolution Model for Background Correction of Illumina BeadArray Data.”
_Communications in Statistics-Theory and Methods_ 40 (17): 3055–69.

Diaconis, Persi, and David Freedman. 1980. “Finite Exchangeable Sequences.”
_The Annals of Probability_ , 745–64.

Diaconis, Persi, and Susan Holmes. 1994. “Gray Codes for Randomization
Procedures.” _Statistics and Computing_ 4 (4): 287–302.

Efron, Bradley, and Robert J Tibshirani. 1994. _An Introduction to the
Bootstrap_. CRC press.

Efron, B., and R. Tibshirani. 1993. _An Introduction to the Bootstrap_.
Chapman & Hall/CRC.

Grün, Bettina, Theresa Scharl, and Friedrich Leisch. 2012. “Modelling Time
Course Gene Expression Data with Finite Mixtures of Linear Additive Models.”
_Bioinformatics_ 28 (2): 222–28.
<https://doi.org/10.1093/bioinformatics/btr653>.

Hoeting, Jennifer A, David Madigan, Adrian E Raftery, and Chris T Volinsky.
1999. “Bayesian Model Averaging: A Tutorial.” _Statistical Science_ ,
382–401.

Kéry, Marc, and J Andrew Royle. 2015. _Applied Hierarchical Modeling in
Ecology: Analysis of Distribution, Abundance and Species Richness in r and
BUGS: Volume 1: Prelude and Static Models_. Academic Press.

Kristiansson, Erik, Michael Thorsen, Markus J Tamás, and Olle Nerman. 2009.
“Evolutionary Forces Act on Promoter Length: Identification of Enriched Cis-
Regulatory Elements.” _Molecular Biology and Evolution_ 26 (6): 1299–1307.

Kuan, Pei Fen, Dongjun Chung, Guangjin Pan, James A Thomson, Ron Stewart, and
Sündüz Keleş. 2011. “A Statistical Framework for the Analysis of ChIP-Seq
Data.” _Journal of the American Statistical Association_ 106 (495): 891–903.

Lange, Kenneth. 2016. _MM Optimization Algorithms_. SIAM.

McLachlan, Geoffrey, and Thriyambakam Krishnan. 2007. _The EM Algorithm and
Extensions_. Vol. 382. John Wiley & Sons.

McLachlan, Geoffrey, and David Peel. 2004. _Finite Mixture Models_. John Wiley
& Sons.

McMurdie, Paul J, and Susan Holmes. 2014. “Waste Not, Want Not: Why Rarefying
Microbiome Data Is Inadmissible.” _PLoS Computational Biology_ 10 (4):
e1003531.

Purdom, Elizabeth, and Susan P Holmes. 2005. “Error Distribution for Gene
Expression Data.” _Statistical Applications in Genetics and Molecular Biology_
4 (1).

Rice, John. 2006. _Mathematical Statistics and Data Analysis_. Cengage
Learning.

Shalizi, Cosma. 2017. _Advanced Data Analysis from an Elementary Point of
View_. Cambridge University Press.
<https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/ADAfaEPoV.pdf>.

Slonim, Noam, Gurinder Singh Atwal, Gašper Tkačik, and William Bialek. 2005.
“Information-Based Clustering.” _PNAS_ 102 (51): 18297–302.

Tibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.”
_Journal of the Royal Statistical Society. Series B (Methodological)_ ,
267–88.

Wills, Quin F, Kenneth J Livak, Alex J Tipping, Tariq Enver, Andrew J Goldson,
Darren W Sexton, and Chris Holmes. 2013. “Single-Cell Gene Expression Analysis
Reveals Genetic Associations Masked in Whole-Tissue Experiments.” _Nature
Biotechnology_ 31 (8): 748–52.

Zeileis, Achim, Christian Kleiber, and Simon Jackman. 2008. “Regression Models
for Count Data in R.” _Journal of Statistical Software_ 27 (8).
<http://www.jstatsoft.org/v27/i08/>.

페이지는 R 버전 4.5.1 (2025-06-13)을 사용하여 2025-09-01 01:33에 빌드되었습니다.