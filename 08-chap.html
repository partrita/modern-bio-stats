<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>10&nbsp; 8.1 Goals of this chapter – Modern Statistics for Modern Biology</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./09-chap.html" rel="next">
<link href="./07-chap.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-22854ec117201859c8a7ba6f538122c9.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="msmb.css">
</head>

<body class="nav-sidebar docked quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./08-chap.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">8.1 Goals of this chapter</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Modern Statistics for Modern Biology</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Home</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./00-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The challenge: heterogeneity</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">1.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">2.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">3.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">4.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">5.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">6.1 Goals for this Chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">7.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-chap.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">8.1 Goals of this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">9.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">10.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">11.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">12.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">13.1 Goals for this chapter</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">14-chap.html</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">15-chap.html</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-chap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">16-chap.html</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#some-core-concepts" id="toc-some-core-concepts" class="nav-link active" data-scroll-target="#some-core-concepts"><span class="header-section-number">10.1</span> 8.2 Some core concepts</a></li>
  <li><a href="#count-data" id="toc-count-data" class="nav-link" data-scroll-target="#count-data"><span class="header-section-number">10.2</span> 8.3 Count data</a>
  <ul class="collapse">
  <li><a href="#the-challenges-of-count-data" id="toc-the-challenges-of-count-data" class="nav-link" data-scroll-target="#the-challenges-of-count-data"><span class="header-section-number">10.2.1</span> 8.3.1 The challenges of count data</a></li>
  <li><a href="#rna-seq-what-about-gene-structures-splicing-isoforms" id="toc-rna-seq-what-about-gene-structures-splicing-isoforms" class="nav-link" data-scroll-target="#rna-seq-what-about-gene-structures-splicing-isoforms"><span class="header-section-number">10.2.2</span> 8.3.2 RNA-Seq: what about gene structures, splicing, isoforms?</a></li>
  </ul></li>
  <li><a href="#modeling-count-data" id="toc-modeling-count-data" class="nav-link" data-scroll-target="#modeling-count-data"><span class="header-section-number">10.3</span> 8.4 Modeling count data</a>
  <ul class="collapse">
  <li><a href="#dispersion" id="toc-dispersion" class="nav-link" data-scroll-target="#dispersion"><span class="header-section-number">10.3.1</span> 8.4.1 Dispersion</a></li>
  <li><a href="#normalization" id="toc-normalization" class="nav-link" data-scroll-target="#normalization"><span class="header-section-number">10.3.2</span> 8.4.2 Normalization</a></li>
  </ul></li>
  <li><a href="#a-basic-analysis" id="toc-a-basic-analysis" class="nav-link" data-scroll-target="#a-basic-analysis"><span class="header-section-number">10.4</span> 8.5 A basic analysis</a>
  <ul class="collapse">
  <li><a href="#example-dataset-the-pasilla-data" id="toc-example-dataset-the-pasilla-data" class="nav-link" data-scroll-target="#example-dataset-the-pasilla-data"><span class="header-section-number">10.4.1</span> 8.5.1 Example dataset: the pasilla data</a></li>
  <li><a href="#the-deseq2-method" id="toc-the-deseq2-method" class="nav-link" data-scroll-target="#the-deseq2-method"><span class="header-section-number">10.4.2</span> 8.5.2 The <strong>DESeq2</strong> method</a></li>
  <li><a href="#exploring-the-results" id="toc-exploring-the-results" class="nav-link" data-scroll-target="#exploring-the-results"><span class="header-section-number">10.4.3</span> 8.5.3 Exploring the results</a></li>
  <li><a href="#exporting-the-results" id="toc-exporting-the-results" class="nav-link" data-scroll-target="#exporting-the-results"><span class="header-section-number">10.4.4</span> 8.5.4 Exporting the results</a></li>
  </ul></li>
  <li><a href="#critique-of-default-choices-and-possible-modifications" id="toc-critique-of-default-choices-and-possible-modifications" class="nav-link" data-scroll-target="#critique-of-default-choices-and-possible-modifications"><span class="header-section-number">10.5</span> 8.6 Critique of default choices and possible modifications</a>
  <ul class="collapse">
  <li><a href="#the-few-changes-assumption" id="toc-the-few-changes-assumption" class="nav-link" data-scroll-target="#the-few-changes-assumption"><span class="header-section-number">10.5.1</span> 8.6.1 The few changes assumption</a></li>
  <li><a href="#point-like-null-hypothesis" id="toc-point-like-null-hypothesis" class="nav-link" data-scroll-target="#point-like-null-hypothesis"><span class="header-section-number">10.5.2</span> 8.6.2 Point-like null hypothesis</a></li>
  </ul></li>
  <li><a href="#multi-factor-designs-and-linear-models" id="toc-multi-factor-designs-and-linear-models" class="nav-link" data-scroll-target="#multi-factor-designs-and-linear-models"><span class="header-section-number">10.6</span> 8.7 Multi-factor designs and linear models</a>
  <ul class="collapse">
  <li><a href="#what-is-a-multifactorial-design" id="toc-what-is-a-multifactorial-design" class="nav-link" data-scroll-target="#what-is-a-multifactorial-design"><span class="header-section-number">10.6.1</span> 8.7.1 What is a multifactorial design?</a></li>
  <li><a href="#what-about-noise-and-replicates" id="toc-what-about-noise-and-replicates" class="nav-link" data-scroll-target="#what-about-noise-and-replicates"><span class="header-section-number">10.6.2</span> 8.7.2 What about noise and replicates?</a></li>
  <li><a href="#analysis-of-variance" id="toc-analysis-of-variance" class="nav-link" data-scroll-target="#analysis-of-variance"><span class="header-section-number">10.6.3</span> 8.7.3 Analysis of variance</a></li>
  <li><a href="#robustness" id="toc-robustness" class="nav-link" data-scroll-target="#robustness"><span class="header-section-number">10.6.4</span> 8.7.4 Robustness</a></li>
  </ul></li>
  <li><a href="#generalized-linear-models" id="toc-generalized-linear-models" class="nav-link" data-scroll-target="#generalized-linear-models"><span class="header-section-number">10.7</span> 8.8 Generalized linear models</a>
  <ul class="collapse">
  <li><a href="#modeling-the-data-on-a-transformed-scale" id="toc-modeling-the-data-on-a-transformed-scale" class="nav-link" data-scroll-target="#modeling-the-data-on-a-transformed-scale"><span class="header-section-number">10.7.1</span> 8.8.1 Modeling the data on a transformed scale</a></li>
  <li><a href="#other-error-distributions" id="toc-other-error-distributions" class="nav-link" data-scroll-target="#other-error-distributions"><span class="header-section-number">10.7.2</span> 8.8.2 Other error distributions</a></li>
  <li><a href="#a-generalized-linear-model-for-count-data" id="toc-a-generalized-linear-model-for-count-data" class="nav-link" data-scroll-target="#a-generalized-linear-model-for-count-data"><span class="header-section-number">10.7.3</span> 8.8.3 A generalized linear model for count data</a></li>
  </ul></li>
  <li><a href="#two-factor-analysis-of-the-pasilla-data" id="toc-two-factor-analysis-of-the-pasilla-data" class="nav-link" data-scroll-target="#two-factor-analysis-of-the-pasilla-data"><span class="header-section-number">10.8</span> 8.9 Two-factor analysis of the pasilla data</a></li>
  <li><a href="#further-statistical-concepts" id="toc-further-statistical-concepts" class="nav-link" data-scroll-target="#further-statistical-concepts"><span class="header-section-number">10.9</span> 8.10 Further statistical concepts</a>
  <ul class="collapse">
  <li><a href="#sharing-of-dispersion-information-across-genes" id="toc-sharing-of-dispersion-information-across-genes" class="nav-link" data-scroll-target="#sharing-of-dispersion-information-across-genes"><span class="header-section-number">10.9.1</span> 8.10.1 Sharing of dispersion information across genes</a></li>
  <li><a href="#count-data-transformations" id="toc-count-data-transformations" class="nav-link" data-scroll-target="#count-data-transformations"><span class="header-section-number">10.9.2</span> 8.10.2 Count data transformations</a></li>
  <li><a href="#dealing-with-outliers" id="toc-dealing-with-outliers" class="nav-link" data-scroll-target="#dealing-with-outliers"><span class="header-section-number">10.9.3</span> 8.10.3 Dealing with outliers</a></li>
  <li><a href="#tests-of-_2-fold-change-above-or-below-a-threshold" id="toc-tests-of-_2-fold-change-above-or-below-a-threshold" class="nav-link" data-scroll-target="#tests-of-_2-fold-change-above-or-below-a-threshold"><span class="header-section-number">10.9.4</span> 8.10.4 Tests of \(_2\) fold change above or below a threshold</a></li>
  </ul></li>
  <li><a href="#summary-of-this-chapter" id="toc-summary-of-this-chapter" class="nav-link" data-scroll-target="#summary-of-this-chapter"><span class="header-section-number">10.10</span> 8.11 Summary of this chapter</a></li>
  <li><a href="#further-reading" id="toc-further-reading" class="nav-link" data-scroll-target="#further-reading"><span class="header-section-number">10.11</span> 8.12 Further reading</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">10.12</span> 8.13 Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">8.1 Goals of this chapter</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><a href="imgs/xkcd-1725-linear_regression_2x.png"><img src="imgs/xkcd-1725-linear_regression_2x.png" class="img-fluid"></a></p>
<p>Many measurement devices in biotechnology are based on massively parallel sampling and counting of molecules. One example is high-throughput DNA sequencing. Its applications fall broadly into two main classes of data output: in the first case, the output of interest are the sequences themselves, perhaps also their polymorphisms or differences to other sequences seen before. In the second case, the sequences themselves are more or less well-understood (say, we have a well-assembled and annotated genome), and our interest is on how abundant different sequence regions are in our sample.</p>
<p>For instance, in <strong>RNA-Seq</strong> (<a href="16-chap.html#ref- OzsolakMilos">Ozsolak and Milos 2011</a>), we sequence the RNA molecules found in a population of cells or in a tissue.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="Strictly speaking, we don’t sequence the RNA but the complementary DNA (cDNA) obtained from reverse transcription. The pool of all RNA might be reduced to a subset of interest (e.,g., messenger RNA) by biochemical means, such as poly-A selection or ribosomal RNA depletion. Sensitive variants of RNA-Seq exist that enable assaying single cells, and large numbers of them."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>Strictly speaking, we don’t sequence the RNA but the complementary DNA (cDNA) obtained from reverse transcription. The pool of all RNA might be reduced to a subset of interest (e.,g., messenger RNA) by biochemical means, such as poly-A selection or ribosomal RNA depletion. Sensitive variants of RNA-Seq exist that enable assaying single cells, and large numbers of them.</figcaption>
</figure>
</div>
<p>Strictly speaking, we don’t sequence the RNA but the complementary DNA (cDNA) obtained from reverse transcription. The pool of all RNA might be reduced to a subset of interest (e.,g., messenger RNA) by biochemical means, such as poly-A selection or ribosomal RNA depletion. Sensitive variants of RNA-Seq exist that enable assaying single cells, and large numbers of them.</p>
<p>In <strong>ChIP-Seq</strong> , we sequence DNA regions that are bound to particular DNA- binding proteins (selected by immuno-precipitation); in <strong>RIP-Seq</strong> , RNA molecules or regions of them bound to a particular RNA-binding protein; in <strong>DNA-Seq</strong> , we sequence genomic DNA and are interested in the prevalence of genetic variants in heterogeneous populations of cells, for instance the clonal composition of a tumor. In high-throughput chromatin conformation capture (<strong>HiC</strong>) we aim to map the 3D spatial arrangement of DNA; in <strong>genetic screens</strong> (using, say, RNAi or CRISPR-Cas9 libraries for perturbation and high-throughput sequencing for readout), we’re interested in the proliferation or survival of cells upon gene knockdown, knockout or modification. In microbiome analysis, we study the abundance of different microbial species in complex microbial habitats.</p>
<p>Ideally we might want to sequence and count <em>all</em> molecules of interest in the sample. Generally this is not possible: the biochemical protocols are not 100% efficient, and some molecules or intermediates get lost along the way. Moreover it’s often also not even necessary. Instead, we sequence and count a <em>statistical sample</em>. The sample size will depend on the complexity of the sequence pool assayed; it can go from tens of thousands to billions. This <em>sampling</em> nature of the data is important when it comes to analyzing them. We hope that the sampling is sufficiently representative for us to identify interesting trends and patterns.</p>
<p>In this chapter, we will become familiar with count data in high-throughput sequencing applications such as RNA-Seq. We will understand and model the sampling processes that underlie the data in order to interpret them. Our main aim is to detect and quantify systematic changes between samples from different conditions, say <em>untreated</em> versus <em>treated</em> , where the task is to distinguish such systematic changes from sampling variations and experimental variability within the same conditions. In order to do this, we will also equip ourselves with a set of needed statistical concepts and tools:</p>
<ul>
<li><p>multifactorial designs, linear models and analysis of variance</p></li>
<li><p>generalized linear models</p></li>
<li><p>robustness and outlier detection</p></li>
<li><p>shrinkage estimation</p></li>
</ul>
<p>In fact, these concepts have a much wider range of applications: they can also be applied to other types of data where want to detect differences in noisy data as a function of some experimental covariate. In particular, the framework of generalized linear models is quite abstract and generic, but this has the advantage that it can be adapted to many different data types, so that we don’t need to reinvent the wheel, but rather can immediately enjoy a wide range of associated tools and diagnostics.</p>
<p>As a bonus, we will also look at data transformations that make the data amenable to unsupervised methods such as those that we saw in Chapters <a href="05-chap.html">5</a> and <a href="07-chap.html">7</a>, and which make it easier to visualize the data.</p>
<section id="some-core-concepts" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="some-core-concepts"><span class="header-section-number">10.1</span> 8.2 Some core concepts</h2>
<p>Before we start, let’s settle some key terminology.</p>
<ul>
<li><p>A <em>sequencing library</em> is the collection of DNA molecules used as input for the sequencing machine.</p></li>
<li><p><em>Fragments</em> are the molecules being sequenced. Since the currently most widely used technology1 can only deal with molecules of length around 300–1000 nucleotides, these are obtained by fragmenting the (generally longer) DNA or cDNA molecules of interest.</p></li>
<li><p>A <strong>read</strong> is the sequence obtained from a fragment. With the current technology, the read covers not the whole fragment, but only one or both ends of it, and the read length on either side is up to around 150 nucleotides.</p></li>
</ul>
<p>1 We refer to <a href="https://www.illumina.com/techniques/sequencing.html" class="uri">https://www.illumina.com/techniques/sequencing.html</a></p>
<p>2 For any particular application, it’s best to check the recent literature on the most appropriate approaches and choices.</p>
<p>3 E.g., in the case of RNA-Seq, the genome together with an annotation of its transcripts.</p>
<p>Between sequencing and counting, there is an important <em>aggregation</em> or clustering step involved, which aggregates sequences that belong together: for instance, all reads belonging to the same gene (in RNA-Seq), or to the same binding region (ChIP-Seq). There are several approaches to this and choices to be made, depending on the aim of the experiment2. The methods include explicit alignment or hash-based mapping to a reference sequence3, and reference- independent sequence-similarity based clustering of the reads – especially if there is no obvious reference, such as in metagenomics or metatranscriptomics. We need to choose whether to consider different alleles or isoforms separately, or to merge them into an equivalence class. For simplicity, we’ll use the term <em>gene</em> in this chapter for these operational aggregates, even though they can be various things depending on the particular application.</p>
</section>
<section id="count-data" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="count-data"><span class="header-section-number">10.2</span> 8.3 Count data</h2>
<p>Let us load an example dataset. It resides in the experiment data package <strong><a href="https://bioconductor.org/packages/pasilla/">pasilla</a></strong>.</p>
<pre><code>fn = system.file("extdata", "pasilla_gene_counts.tsv",
                  package = "pasilla", mustWork = TRUE)
counts = as.matrix(read.csv(fn, sep = "\t", row.names = "gene_id"))__</code></pre>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="In the code shown here, we use the function system.file to locate a file that is shipped together with the pasilla package. When you work with your own data, you will need to prepare the matrix counts yourself."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>In the code shown here, we use the function system.file to locate a file that is shipped together with the pasilla package. When you work with your own data, you will need to prepare the matrix counts yourself.</figcaption>
</figure>
</div>
<p>In the code shown here, we use the function <code>system.file</code> to locate a file that is shipped together with the <strong><a href="https://bioconductor.org/packages/pasilla/">pasilla</a></strong> package. When you work with your own data, you will need to prepare the matrix <code>counts</code> yourself.</p>
<p>The data are stored as a rectangular table in a tab-delimited file, which we’ve read into the matrix <code>counts</code>.</p>
<pre><code>dim(counts)__


[1] 14599     7


counts[ 2000+(0:3), ]__


            untreated1 untreated2 untreated3 untreated4 treated1 treated2
FBgn0020369       3387       4295       1315       1853     4884     2133
FBgn0020370       3186       4305       1824       2094     3525     1973
FBgn0020371          1          0          1          1        1        0
FBgn0020372         38         84         29         28       63       28
            treated3
FBgn0020369     2165
FBgn0020370     2120
FBgn0020371        0
FBgn0020372       27</code></pre>
<p>The matrix tallies the number of reads seen for each gene in each sample. We call it the <strong>count table</strong>. It has 14599 rows, corresponding to the genes, and 7 columns, corresponding to the samples. When loading data from a file, a good plausibility check is to print out some of the data, and maybe not only at the very beginning, but also at some random point in the middle, as we have done above.</p>
<p>The table is a matrix of integer values: the value in the \(i\)th row and the \(j\)th column of the matrix indicates how many reads have been mapped to gene \(i\) in sample \(j\). The statistical sampling models that we discuss in this chapter rely on the fact that the values are the direct, “raw” counts of sequencing reads – not some derived quantity, such as normalized counts, counts of covered base pairs, or the like; this would only lead to nonsensical results.</p>
<section id="the-challenges-of-count-data" class="level3" data-number="10.2.1">
<h3 data-number="10.2.1" class="anchored" data-anchor-id="the-challenges-of-count-data"><span class="header-section-number">10.2.1</span> 8.3.1 The challenges of count data</h3>
<p>What are the challenges that we need to overcome with such count data?</p>
<ul>
<li><p>The data have a large dynamic range, starting from zero up to millions. The variance, and more generally, the distribution shape of the data in different parts of the dynamic range are very different. We need to take this phenomenon, called <strong>heteroskedasticity</strong> , into account.</p></li>
<li><p>The data are non-negative integers, and their distribution is not symmetric – thus normal or log-normal distribution models may be a poor fit.</p></li>
<li><p>We need to understand the systematic sampling biases and adjust for them. Confusingly, this is often called <strong>normalization</strong>. Examples are the total sequencing depth of an experiment (even if the true abundance of a gene in two libraries is the same, we expect different numbers of reads for it depending on the total number of reads sequenced), or differing sampling probabilities (even if the true abundance of two genes within a biological sample is the same, we expect different numbers of reads for them if their biophysical properties differ, such as length, GC content, secondary structure, binding partners).</p></li>
<li><p>We need to understand the stochastic properties of the sampling, as well as other sources of stochastic experimental variation. For studies with large numbers of biological samples, this is usually straightforward, and we can even fall back on resampling- or permutation-based methods. For designed experiments, however, sample sizes tend to be limited.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="There are important conceptual and practical differences between experiments and studies – see also sec-design."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>There are important conceptual and practical differences between experiments and studies – see also sec- design.</figcaption>
</figure>
</div>
<p>There are important conceptual and practical differences between experiments and studies – see also <a href="13-chap.html">Chapter 13</a>.</p>
<p>For instance, there are four replicates from the <em>untreated</em> and three from the <em>treated</em> condition in the pasilla data. This means that resampling- or permutation-based methods will not have enough power. To proceed, we need to make distributional assumptions. Essentially, what such assumptions do is that they let us compute the probabilities of rare events in the tails of the distribution – i.e., extraordinarily high or low counts – from a small number of distribution parameters.</p>
<ul>
<li>But even that is often not enough, in particular the estimation of dispersion parameters4 is difficult with small sample sizes. In that case, we need to make further assumptions, such as that genes with similar locations also have similar dispersions. This is called sharing of information across genes, and we’ll come back to it in Section 8.10.1.</li>
</ul>
<p>4 Distributions can be parameterized in various ways; often the parameters correspond to some measure of location and some measure of dispersion; a familiar measure of location is the mean, and a familiar measure of dispersion is the variance (or standard deviation), but for some distributions other measures are also in use.</p>
</section>
<section id="rna-seq-what-about-gene-structures-splicing-isoforms" class="level3" data-number="10.2.2">
<h3 data-number="10.2.2" class="anchored" data-anchor-id="rna-seq-what-about-gene-structures-splicing-isoforms"><span class="header-section-number">10.2.2</span> 8.3.2 RNA-Seq: what about gene structures, splicing, isoforms?</h3>
<p>Eukaryotic genes are complex: most of them consist of multiple exons, and mRNAs result from concatenation of exons through a process called splicing. Alternative splicing and multiple possible choices of start and stop sites enable the generation of multiple, alternative isoforms from the same gene locus. It is possible to use high-throughput sequencing to detect the isoform structures of transcripts. From the fragments that are characteristic for specific isoforms, it is also possible to detect isoform specific abundances. With current RNA-Seq data, which only give us relatively short fragments of the full-length isoforms, it tends to be difficult to assemble and deconvolute full-length isoform structures and abundances (<a href="16-chap.html#ref-SteijgerBertone:2013">Steijger et al. 2013</a>). Because of that, procedures with the more modest aim of making only local statements (e.g., inclusion or exclusion of individual exons) have been formulated (<a href="16-chap.html#ref-Reyes:GnomeResearch:2012">Anders, Reyes, and Huber 2012</a>), and these can be more robust. We can expect that future technologies will sequence full-length transcripts.</p>
</section>
</section>
<section id="modeling-count-data" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="modeling-count-data"><span class="header-section-number">10.3</span> 8.4 Modeling count data</h2>
<section id="dispersion" class="level3" data-number="10.3.1">
<h3 data-number="10.3.1" class="anchored" data-anchor-id="dispersion"><span class="header-section-number">10.3.1</span> 8.4.1 Dispersion</h3>
<p>Consider a sequencing library that contains \(n_1\) fragments corresponding to gene 1, \(n_2\) fragments for gene 2, and so on, with a total library size of \(n = n_1+n_2+\). We submit the library to sequencing and determine the identity of \(r\) randomly sampled fragments. A welcome simplification comes from looking at the orders of magnitude of these numbers:</p>
<ul>
<li><p>the number of genes is in the tens of thousands;</p></li>
<li><p>the value of \(n\) depends on the amount of cells that were used to prepare, but for bulk RNA-Seq it will be in the billions or trillions;</p></li>
<li><p>the number of reads \(r\) is usually in the tens of millions, and thus much smaller than \(n\).</p></li>
</ul>
<p>From this we can conclude that the probability that a given read maps to the \(i^{}\) gene is \(p_i=n_i/n\), and that this is pretty much independent of the outcomes for all the other reads. So we can model the number of reads for gene \(i\) by a Poisson distribution, where the <em>rate</em> of the Poisson process is the product of \(p_i\), the initial proportion of fragments for the \(i^{}\) gene, times \(r\), that is: \(_i=rp_i\).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="In principle, we should consider sampling without replacement and the multinomial distribution here: the probability of sampling a read for the i^{\text{th}} gene depends on how many times the same gene, and other genes, have already been sampled. However, these dependencies are so negligibly small that we’ll ignore them. This is because n is so much larger than r, the number of genes is large, and each individual n_i is small compared to n."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>In principle, we should consider sampling without replacement and the multinomial distribution here: the probability of sampling a read for the i^{\text{th}} gene depends on how many times the same gene, and other genes, have already been sampled. However, these dependencies are so negligibly small that we’ll ignore them. This is because n is so much larger than r, the number of genes is large, and each individual n_i is small compared to n.</figcaption>
</figure>
</div>
<p>In principle, we should consider <strong>sampling without replacement</strong> and the multinomial distribution here: the probability of sampling a read for the \(i^{}\) gene depends on how many times the same gene, and other genes, have already been sampled. However, these dependencies are so negligibly small that we’ll ignore them. This is because \(n\) is so much larger than \(r\), the number of genes is large, and each individual \(n_i\) is small compared to \(n\).</p>
<p>In practice, we are usually not interested in modeling the read counts within a single library, but in comparing the counts between libraries. That is, we want to know whether any differences that we see between different biological conditions – say, the same cell line with and without drug treatment – are larger than expected “by chance”, i.e., larger than what we may expect even between biological replicates. Empirically, it turns out that replicate experiments vary more than what the Poisson distribution predicts. Intuitively, what happens is that \(p_i\) and therefore also \(_i\) vary even between biological replicates; perhaps the temperature at which the cells grew was slightly different, or the amount of drug added varied by a few percent, or the incubation time was slightly longer. To account for that, we need to add another layer of modeling on top. We already saw hierarchical models and mixtures in <a href="04-chap.html">Chapter 4</a>. It turns out that the <strong>gamma-Poisson</strong> (a.k.a. negative binomial) distribution suits our modeling needs. Instead of a single \(\) – which represents both mean and variance –, this distribution has two parameters. In principle, these can be different for each gene, and we will come back to the question of how to estimate them from the data.</p>
</section>
<section id="normalization" class="level3" data-number="10.3.2">
<h3 data-number="10.3.2" class="anchored" data-anchor-id="normalization"><span class="header-section-number">10.3.2</span> 8.4.2 Normalization</h3>
<p>Often, there are systematic biases that have affected the data generation and are worth taking into account. Unfortunately, the term <strong>normalization</strong> is commonly used for that aspect of the analysis, even though it is misleading: it has nothing to do with the normal distribution, norms in a vector space, or normal vectors. Rather, what we aim for is identifying the nature and estimating the magnitude of systematic biases, and take them into account in our model-based analysis of the data.</p>
<p>The most important systematic bias stems from variations in the total number of reads in each sample. If we have more reads for one library than in another, then we might assume that, everything else being equal, the counts are proportional to each other with some proportionality factor \(s\). Naively, we could propose that a decent estimate of \(s\) for each sample is simply given by the sum of the counts of all genes. However, it turns out that we can do better. To understand this, a toy example helps.</p>
<p><a href="08-chap_files/figure-html/fig-countdata- normalization-1.png" title="Figure 8.1: Size factor estimation. The points correspond to hypothetical genes whose counts in two samples are indicated by their x- and y-coordinates. The lines indicate two different ways of size factor estimation explained in the text."><img src="08-chap_files/figure-html/fig-countdata- normalization-1.png" class="img-fluid"></a></p>
<p>Figure 8.1: Size factor estimation. The points correspond to hypothetical genes whose counts in two samples are indicated by their \(x\)- and \(y\)-coordinates. The lines indicate two different ways of size factor estimation explained in the text.</p>
<p>Consider a dataset with 5 genes and two samples as displayed in Figure 8.1. If we estimate \(s\) for each of the two samples by its sum of counts, then the slope of the blue line represents their ratio. According to this, gene C is down-regulated in sample 2 compared to sample 1, while the other genes are all somewhat up-regulated. If we now instead estimate \(s\) such that their ratios correspond to the red line, then we will still conclude that gene C is down-regulated, while the other genes are unchanged. The second version is more parsimonious and is often preferred by scientists. The slope of the red line can be obtained by robust regression. This is what the <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> method does.</p>
<p>__</p>
<p>Question 8.1</p>
<p>For the example dataset <code>count</code> of Section 8.3, how does the output of <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> ’s <code>estimateSizeFactorsForMatrix</code> compare to what you get by simply taking the column sums?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>See Figure 8.2, produced by the code below. In this case, there is not much difference, the results are nearly proportional.</p>
<pre><code>library("tibble")
library("ggplot2")
library("DESeq2")
ggplot(tibble(
  `size factor` = estimateSizeFactorsForMatrix(counts),
  `sum` = colSums(counts)), aes(x = `size factor`, y = `sum`)) +
  geom_point()__</code></pre>
<p><a href="08-chap_files/figure-html/fig-countdata-sfvssum-1.png &quot;Figure 8.2: Size factors versus sums for the pasilla data.&quot;"><img src="08-chap_files/figure-html/fig-countdata- sfvssum-1.png" class="img-fluid"></a></p>
<p>Figure 8.2: Size factors versus sums for the pasilla data.</p>
<p>__</p>
<p>Task</p>
<p>Locate the R sources for this book and have a look at the code that produces Figure 8.1.</p>
<p>__</p>
<p>Question 8.2</p>
<p>Plot the mean-variance relationship for the biological replicates in the pasilla dataset.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>See Figure 8.3, produced by the following code.</p>
<pre><code>library("matrixStats")
sf = estimateSizeFactorsForMatrix(counts)
ncounts  = counts / matrix(sf,
   byrow = TRUE, ncol = ncol(counts), nrow = nrow(counts))
uncounts = ncounts[, grep("^untreated", colnames(ncounts)),
                     drop = FALSE]
ggplot(tibble(
        mean = rowMeans(uncounts),
        var  = rowVars( uncounts)),
     aes(x = log(mean), y = log(var))) +
  geom_hex() + coord_fixed() + theme(legend.position = "none") +
  geom_abline(slope = 1:2, color = c("forestgreen", "red"))__</code></pre>
<p><a href="08-chap_files/figure-html/fig-countdata-varmean-1.png &quot;Figure 8.3: Variance versus mean for the (size factor adjusted) counts data. The axes are logarithmic. Also shown are lines through the origin with slopes 1 (green) and 2 (red).&quot;"><img src="08-chap_files/figure-html/fig-countdata- varmean-1.png" class="img-fluid"></a></p>
<p>Figure 8.3: Variance versus mean for the (size factor adjusted) <code>counts</code> data. The axes are logarithmic. Also shown are lines through the origin with slopes 1 (green) and 2 (red).</p>
<p>The green line (slope 1) is what we expect if the variance (\(v\)) equals the mean (\(m\)), as is the case for a Poisson-distributed random variable: \(v=m\). We see that this approximately fits the data in the lower range. The red line (slope 2) corresponds to the quadratic mean-variance relationship \(v=m^2\); lines parallel to it (not shown) would represent \(v = cm^2\) for various values of \(c\). We can see that in the upper range of the data, the quadratic relationship approximately fits the data, for some value of \(c&lt;1\).</p>
</section>
</section>
<section id="a-basic-analysis" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="a-basic-analysis"><span class="header-section-number">10.4</span> 8.5 A basic analysis</h2>
<section id="example-dataset-the-pasilla-data" class="level3" data-number="10.4.1">
<h3 data-number="10.4.1" class="anchored" data-anchor-id="example-dataset-the-pasilla-data"><span class="header-section-number">10.4.1</span> 8.5.1 Example dataset: the pasilla data</h3>
<p>Let’s return to the <strong><a href="https://bioconductor.org/packages/pasilla/">pasilla</a></strong> data from Section 8.3. These data are from an experiment on <em>Drosophila melanogaster</em> cell cultures that investigated the effect of RNAi knock-down of the splicing factor <em>pasilla</em> (<a href="16-chap.html#ref- Brooks2010">Brooks et al.&nbsp;2011</a>) on the cells’ transcriptome. There were two experimental conditions, termed <em>untreated</em> and <em>treated</em> in the header of the count table that we loaded. They correspond to negative control and to siRNA against <em>pasilla</em>. The experimental metadata of the 7 samples in this dataset are provided in a spreadsheet-like table, which we load.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="In the code shown here, we load the file pasilla_sample_annotation.csv that comes with the pasilla package. We locate it with the function system.file. When you work with your own data, you will need to prepare an analogous file, or directly a dataframe like pasillaSampleAnno."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>In the code shown here, we load the file pasilla_sample_annotation.csv that comes with the pasilla package. We locate it with the function system.file. When you work with your own data, you will need to prepare an analogous file, or directly a dataframe like pasillaSampleAnno.</figcaption>
</figure>
</div>
<p>In the code shown here, we load the file <code>pasilla_sample_annotation.csv</code> that comes with the <strong><a href="https://bioconductor.org/packages/pasilla/">pasilla</a></strong> package. We locate it with the function <code>system.file</code>. When you work with your own data, you will need to prepare an analogous file, or directly a dataframe like <code>pasillaSampleAnno</code>.</p>
<pre><code>annotationFile = system.file("extdata",
  "pasilla_sample_annotation.csv",
  package = "pasilla", mustWork = TRUE)
pasillaSampleAnno = readr::read_csv(annotationFile)
pasillaSampleAnno __


# A tibble: 7 × 6
  file    condition type  `number of lanes` total number of read…¹ `exon counts`
  &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                          &lt;dbl&gt;
1 treate… treated   sing…                 5 35158667                    15679615
2 treate… treated   pair…                 2 12242535 (x2)               15620018
3 treate… treated   pair…                 2 12443664 (x2)               12733865
4 untrea… untreated sing…                 2 17812866                    14924838
5 untrea… untreated sing…                 6 34284521                    20764558
6 untrea… untreated pair…                 2 10542625 (x2)               10283129
7 untrea… untreated pair…                 2 12214974 (x2)               11653031
# ℹ abbreviated name: ¹​`total number of reads`</code></pre>
<p>As we see here, the overall dataset was produced in two batches, the first one consisting of three sequencing libraries that were subjected to single read sequencing, the second batch consisting of four libraries for which paired end sequencing was used. As so often, we need to do some data wrangling: we replace the hyphens in the <code>type</code> column by underscores, as arithmetic operators in factor levels are discouraged by <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> , and convert the <code>type</code> and <code>condition</code> columns into factors, explicitly specifying our prefered order of the levels (the default is alphabetical).</p>
<pre><code>library("dplyr")
pasillaSampleAnno = mutate(pasillaSampleAnno,
condition = factor(condition, levels = c("untreated", "treated")),
type = factor(sub("-.*", "", type), levels = c("single", "paired")))__</code></pre>
<p>We note that the design is approximately balanced between the factor of interest, <code>condition</code>, and the “nuisance factor” <code>type</code>:</p>
<pre><code>with(pasillaSampleAnno,
       table(condition, type))__


           type
condition   single paired
  untreated      2      2
  treated        1      2</code></pre>
<p><strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> uses a specialized data container, called <em>DESeqDataSet</em> to store the datasets it works with. Such use of specialized containers – or, in R terminology, <em>classes</em> – is a common principle of the Bioconductor project, as it helps users to keep together related data. While this way of doing things requires users to invest a little more time upfront to understand the classes, compared to just using basic R data types like <em>matrix</em> and dataframe, it helps avoiding bugs due to loss of synchronization between related parts of the data. It also enables the abstraction and encapsulation of common operations that could be quite wordy if always expressed in basic terms5. <em>DESeqDataSet</em> is an extension of the class <em>SummarizedExperiment</em> in Bioconductor. The <em>SummarizedExperiment</em> class is also used by many other packages, so learning to work with it will enable you to use quite a range of tools.</p>
<p>5 Another advantage is that classes can contain <em>validity</em> methods, which make sure that the data always fulfill certain expectations, for instance, that the counts are positive integers, or that the columns of the counts matrix align with the rows of the sample annotation dataframe.</p>
<p>6 Note how in the code below, we have to put in extra work to match the column names of the <code>counts</code> object with the <code>file</code> column of the <code>pasillaSampleAnno</code> dataframe, in particular, we need to remove the <code>"fb"</code> that happens to be used in the <code>file</code> column for some reason. Such data wrangling is very common. One of the reasons for storing the data in a <em>DESeqDataSet</em> object is that we then no longer have to worry about such things.</p>
<p>We use the constructor function <code>DESeqDataSetFromMatrix</code> to create a <em>DESeqDataSet</em> from the count data matrix <code>counts</code> and the sample annotation dataframe <code>pasillaSampleAnno</code>6.</p>
<pre><code>mt = match(colnames(counts), sub("fb$", "", pasillaSampleAnno$file))
stopifnot(!any(is.na(mt)))

pasilla = DESeqDataSetFromMatrix(
  countData = counts,
  colData   = pasillaSampleAnno[mt, ],
  design    = ~ condition)
class(pasilla)__


[1] "DESeqDataSet"
attr(,"package")
[1] "DESeq2"


is(pasilla, "SummarizedExperiment")__


[1] TRUE</code></pre>
<p>The <em>SummarizedExperiment</em> class – and therefore <em>DESeqDataSet</em> – also contains facilities for storing annotation of the rows of the count matrix. For now, we are content with the gene identifiers from the row names of the <code>counts</code> table.</p>
<p>__</p>
<p>Question 8.3</p>
<p>How can we access the row metadata of a <em>SummarizedExperiment</em> object, i.e., how can we read it out, how can we change it?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Check the manual page of the <em>SummarizedExperiment</em> class and of the methods <code>rowData</code> and <code>rowData&lt;-</code>.</p>
</section>
<section id="the-deseq2-method" class="level3" data-number="10.4.2">
<h3 data-number="10.4.2" class="anchored" data-anchor-id="the-deseq2-method"><span class="header-section-number">10.4.2</span> 8.5.2 The <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> method</h3>
<p>After these preparations, we are now ready to jump straight into differential expression analysis. Our aim is to identify genes that are differentially abundant between the treated and the untreated cells. To this end, we will apply a test that is conceptually similar to the \(t\)-test, which we encountered in <a href="06-chap.html#sec-testing-ttest">Section 6.5</a>, although mathematically somewhat more involved. We will postpone these details for now, and will come back to them in Section 8.7. A choice of standard analysis steps are wrapped into a single function, <code>DESeq</code>.</p>
<pre><code>pasilla = DESeq(pasilla)__</code></pre>
<p>The <code>DESeq</code> function is simply a wrapper that calls, in order, the functions <code>estimateSizeFactors</code> (for normalization, as discussed in Section 8.4.2), <code>estimateDispersions</code> (dispersion estimation) and <code>nbinomWaldTest</code> (hypothesis tests for differential abundance). The test is between the two levels extttuntreated and exttttreated of the factor <code>condition</code>, since this is what we specified when we constructed the <code>pasilla</code> object through the argument <code>design=\simcondition</code>. You can always call each of these three functions individually if you want to modify their behavior or interject custom steps. Let us look at the results.</p>
<pre><code>res = results(pasilla)
res[order(res$padj), ] |&gt; head()__


log2 fold change (MLE): condition treated vs untreated 
Wald test p-value: condition treated vs untreated 
DataFrame with 6 rows and 6 columns
             baseMean log2FoldChange     lfcSE      stat       pvalue
            &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;    &lt;numeric&gt;
FBgn0039155   730.596       -4.61901 0.1687068  -27.3789 4.88599e-165
FBgn0025111  1501.411        2.89986 0.1269205   22.8479 1.53430e-115
FBgn0029167  3706.117       -2.19700 0.0969888  -22.6521 1.33042e-113
FBgn0003360  4343.035       -3.17967 0.1435264  -22.1539 9.56283e-109
FBgn0035085   638.233       -2.56041 0.1372952  -18.6490  1.28772e-77
FBgn0039827   261.916       -4.16252 0.2325888  -17.8965  1.25663e-71
                    padj
               &lt;numeric&gt;
FBgn0039155 4.06661e-161
FBgn0025111 6.38497e-112
FBgn0029167 3.69104e-110
FBgn0003360 1.98979e-105
FBgn0035085  2.14354e-74
FBgn0039827  1.74316e-68</code></pre>
</section>
<section id="exploring-the-results" class="level3" data-number="10.4.3">
<h3 data-number="10.4.3" class="anchored" data-anchor-id="exploring-the-results"><span class="header-section-number">10.4.3</span> 8.5.3 Exploring the results</h3>
<p>The first step after a differential expression analysis is the visualization of the following three or four basic plots:</p>
<ul>
<li><p>the histogram of p-values (Figure 8.4),</p></li>
<li><p>the MA plot (Figure 8.5) and</p></li>
<li><p>an ordination plot (Figure 8.6).</p></li>
<li><p>In addition, a heatmap (Figure 8.7) can be instructive.</p></li>
</ul>
<p>These are essential data quality assessment measures – and the general advice on quality assessment and control given in <a href="13-chap.html#sec- design-quality">Section 13.6</a> also applies here.</p>
<p>The p-value histogram is straightforward (Figure 8.4).</p>
<pre><code>ggplot(as(res, "data.frame"), aes(x = pvalue)) +
  geom_histogram(binwidth = 0.01, fill = "Royalblue", boundary = 0)__</code></pre>
<p><a href="08-chap_files/figure-html/fig-countdata-hist1-1.png" title="Figure 8.4: Histogram of p-values of a differential expression analysis."><img src="08-chap_files/figure-html/fig-countdata- hist1-1.png" class="img-fluid"></a></p>
<p>Figure 8.4: Histogram of p-values of a differential expression analysis.</p>
<p>The distribution displays two main components: a uniform background with values between 0 and 1, and a peak of small p-values at the left. The uniform background corresponds to the non-differentially expressed genes. Usually this is the majority of genes. The left hand peak corresponds to differentially expressed genes7. As we already saw in <a href="06-chap.html">Chapter 6</a>, the ratio of the level of the background to the height of the peak gives us a rough indication of the false discovery rate (FDR) that would be associated with calling the genes in the leftmost bin differentially expressed. In our case, the leftmost bin contains all p-values between 0 and 0.01, which correspond to 993 genes. The background level is at around 100, so the FDR associated with calling all genes in the leftmost bin would be around 10%.</p>
<p>7 For the data shown here, the histogram also contains a few isolated peaks in the middle or towards the right; these stem from genes with small counts and reflect the discreteness of the data.</p>
<p>Sometimes it turns out that the background distribution is not uniform, but shows a tilted shape with an increase towards the right. This tends to be an indication of batch effects; you can explore this further in Exercise 8.1.</p>
<p>To produce the MA plot, we can use the function <code>plotMA</code> in the <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> package (Figure 8.5).</p>
<pre><code>plotMA(pasilla, ylim = c( -2, 2))__</code></pre>
<p><a href="08-chap_files/figure- html/fig-countdata-MA-1.png" title="Figure 8.5: MA plot: fold change versus mean of size-factor normalized counts. Logarithmic scaling is used for both axes. By default, points are colored red if the adjusted p-value is less than 0.1. Points which fall out of the y-axis range are plotted as triangles."><img src="08-chap_files/figure-html/fig-countdata-MA-1.png" class="img-fluid"></a></p>
<p>Figure 8.5: <a href="https://en.wikipedia.org/wiki/MA_plot">MA plot</a>: fold change versus mean of size-factor normalized counts. Logarithmic scaling is used for both axes. By default, points are colored red if the adjusted p-value is less than 0.1. Points which fall out of the \(y\)-axis range are plotted as triangles.</p>
<p>To produce PCA plots similar to those we saw in <a href="07-chap.html">Chapter 7</a>, we can use the <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> function <code>plotPCA</code> (Figure 8.6).</p>
<pre><code>pas_rlog = rlogTransformation(pasilla)
plotPCA(pas_rlog, intgroup=c("condition", "type")) + coord_fixed()__</code></pre>
<p><a href="08-chap_files/figure- html/fig-countdata-PCA-1.png" title="Figure 8.6: PCA plot. The 7 samples are shown in the 2D plane spanned by their first two principal components."><img src="08-chap_files/figure-html/fig-countdata-PCA-1.png" class="img-fluid"></a></p>
<p>Figure 8.6: PCA plot. The 7 samples are shown in the 2D plane spanned by their first two principal components.</p>
<p>As we saw in the previous chapter, this type of plot is useful for visualizing the overall effect of experimental covariates and/or to detect batch effects. Here, the first principal axis, PC1, is mostly aligned with the experimental covariate of interest (untreated / treated), while the second axis is roughly aligned with the sequencing protocol (single / paired).</p>
<p>We used a data transformation, the <strong>regularized logarithm</strong> or <strong>rlog</strong> , which we will investigate more closely in Section 8.10.2.</p>
<p>__</p>
<p>Question 8.4</p>
<p>Do the axes of PCA plot always have to align with known experimental covariates?</p>
<p>Heatmaps can be a powerful way of quickly getting an overview over a matrix- like dataset, count tables included. Below you see how to make a heatmap from the rlog-transformed data. For a matrix as large as <code>counts(pasilla)</code>, it is not practical to plot all of it, so we plot the submatrix of the 30 genes with the highest average expression.</p>
<pre><code>library("pheatmap")
select = order(rowMeans(assay(pas_rlog)), decreasing = TRUE)[1:30]
pheatmap( assay(pas_rlog)[select, ],
     scale = "row",
     annotation_col = as.data.frame(
        colData(pas_rlog)[, c("condition", "type")] ))__</code></pre>
<p><a href="08-chap_files/figure- html/fig-figHeatmap-1-1.png" title="Figure 8.7: Heatmap of regularized log transformed data of the top 30 genes."><img src="08-chap_files/figure-html/fig-figHeatmap-1-1.png" class="img-fluid"></a></p>
<p>Figure 8.7: Heatmap of regularized log transformed data of the top 30 genes.</p>
<p>In Figure 8.7, <code>pheatmap</code> arranged the rows and columns of the matrix by the dendrogram from an unsupervised clustering, and the clustering of the columns (samples) is dominated by the <code>type</code> factor. This highlights that our differential expression analysis above was probably too naive, and that we should adjust for this strong “nuisance” factor when we are interested in testing for differentially expressed genes between conditions. We will do this in Section 8.9.</p>
<p>__</p>
<p>Task</p>
<p>Produce a plot similar to Figure 8.7, but selecting the 30 most highly variable genes instead. What is different? How do the genes with very high mean and those with very high variance relate? How does their data look?</p>
</section>
<section id="exporting-the-results" class="level3" data-number="10.4.4">
<h3 data-number="10.4.4" class="anchored" data-anchor-id="exporting-the-results"><span class="header-section-number">10.4.4</span> 8.5.4 Exporting the results</h3>
<p>An HTML report of the results with plots and sortable/filterable columns can be exported using the <strong><a href="https://bioconductor.org/packages/ReportingTools/">ReportingTools</a></strong> package on a <em>DESeqDataSet</em> that has been processed by the <code>DESeq</code> function. For a code example, see the <em>RNA-Seq differential expression</em> vignette of the <strong><a href="https://bioconductor.org/packages/ReportingTools/">ReportingTools</a></strong> package or the manual page for the <code>publish</code> method for the <em>DESeqDataSet</em> class.</p>
<p>A CSV file of the results can be exported using <code>write.csv</code> (or its counterpart from the <strong><a href="https://cran.r-project.org/web/packages/readr/">readr</a></strong> package).</p>
<pre><code>write.csv(as.data.frame(res), file = "treated_vs_untreated.csv")__</code></pre>
</section>
</section>
<section id="critique-of-default-choices-and-possible-modifications" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="critique-of-default-choices-and-possible-modifications"><span class="header-section-number">10.5</span> 8.6 Critique of default choices and possible modifications</h2>
<section id="the-few-changes-assumption" class="level3" data-number="10.5.1">
<h3 data-number="10.5.1" class="anchored" data-anchor-id="the-few-changes-assumption"><span class="header-section-number">10.5.1</span> 8.6.1 The few changes assumption</h3>
<p>Underlying the default normalization and the dispersion estimation in <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> (and many other differential expression methods) is that most genes are not differentially expressed.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="For the normalization, although not for the dispersion estimation, one can slightly relax this assumption: it is still valid if many genes are changing, but in a way that is balanced between up- and downward directions."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>For the normalization, although not for the dispersion estimation, one can slightly relax this assumption: it is still valid if many genes are changing, but in a way that is balanced between up- and downward directions.</figcaption>
</figure>
</div>
<p>For the normalization, although not for the dispersion estimation, one can slightly relax this assumption: it is still valid if many genes are changing, but in a way that is balanced between up- and downward directions.</p>
<p>This assumption is often reasonable (well-designed experiments usually ask specific questions, so that not everything changes all at once), but what should we do if it does not hold? Instead of applying these operations on the data from all genes, we will then need to identify a subset of (“negative control”) genes for which we believe the assumption is tenable, either because of prior biological knowledge, or because we explicitly controlled their abundance as external “spiked in” features.</p>
<p>__</p>
<p>Task</p>
<p>Run the <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> workflow with size factors and dispersion parameters estimated only from a predefined subset of genes.</p>
</section>
<section id="point-like-null-hypothesis" class="level3" data-number="10.5.2">
<h3 data-number="10.5.2" class="anchored" data-anchor-id="point-like-null-hypothesis"><span class="header-section-number">10.5.2</span> 8.6.2 Point-like null hypothesis</h3>
<p>As a default, the <code>DESeq</code> function tests against the null hypothesis that each gene has the same abundance across conditions; this is a simple and pragmatic choice. Indeed, if the sample size is limited, what is statistically significant also tends to be strong enough to be biologically interesting. But as sample size increases, statistical significance in these tests may be present without much biological relevance. For instance, many genes may be slightly perturbed by downstream, indirect effects. We can modify the test to use a more permissive, interval-based null hypothesis; we will further explore this in Section 8.10.4.</p>
</section>
</section>
<section id="multi-factor-designs-and-linear-models" class="level2" data-number="10.6">
<h2 data-number="10.6" class="anchored" data-anchor-id="multi-factor-designs-and-linear-models"><span class="header-section-number">10.6</span> 8.7 Multi-factor designs and linear models</h2>
<section id="what-is-a-multifactorial-design" class="level3" data-number="10.6.1">
<h3 data-number="10.6.1" class="anchored" data-anchor-id="what-is-a-multifactorial-design"><span class="header-section-number">10.6.1</span> 8.7.1 What is a multifactorial design?</h3>
<p>Let’s assume that in addition to the siRNA knockdown of the pasilla gene, we also want to test the effect of a certain drug. We could then envisage an experiment in which the experimenter treats the cells either with negative control, with the siRNA against pasilla, with the drug, or with both. To analyse this experiment, we can use the notation</p>
<p>\[ y = _0 + x_1 _1 + x_2 <em>2 + x_1x_2</em>{12}. \]</p>
<p>This equation can be parsed as follows. The left hand side, \(y\), is the experimental measurement of interest. In our case, this is the suitably transformed expression level (we’ll discuss this in Section 8.8.3) of a gene. Since in an RNA-Seq experiment there are lots of genes, we’ll have as many copies of Equation 8.1, one for each. The coefficient \(_0\) is the base level of the measurement in the negative control; often it is called the <strong>intercept</strong>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="Sometimes Equation eq-countdata- basiclm is written with an additional term x_0 that is multiplied with \beta_0, where it is understood that x_0=1 always. It turns out that this makes subsequent notation and bookkeeping easier since then the intercept can be handled consistently together with the other \betas, instead of being a separate case."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>Sometimes Equation&nbsp;eq-countdata-basiclm is written with an additional term x_0 that is multiplied with \beta_0, where it is understood that x_0=1 always. It turns out that this makes subsequent notation and bookkeeping easier since then the intercept can be handled consistently together with the other \betas, instead of being a separate case.</figcaption>
</figure>
</div>
<p>Sometimes Equation 8.1 is written with an additional term \(x_0\) that is multiplied with \(_0\), where it is understood that \(x_0=1\) always. It turns out that this makes subsequent notation and bookkeeping easier since then the intercept can be handled consistently together with the other \(\)s, instead of being a separate case.</p>
<p>The design factors \(x_1\) and \(x_2\) are binary indicator variables: \(x_1\) takes the value 1 if the siRNA was transfected and 0 if not, and similarly, \(x_2\) indicates whether the drug was administered. In the experiment where only the siRNA is used, \(x_1=1\) and \(x_2=0\), and the third and fourth terms of Equation 8.1 vanish. Then, the equation simplifies to \(y=_0+_1\). This means that \(_1\) represents the difference between treatment and control. If our measurements are on a logarithmic scale, then</p>
<p>\[ <span class="math display">\[\begin{align} \beta_1 = y-\beta_0
&amp;=\log_2(\text{expression}_{\text{treated}})
-\log_2(\text{expression}_{\text{untreated}})\\\ &amp;=\log_2\frac
{\text{expression}_{\text{treated}}} {\text{expression}_{\text{untreated}}}
\end{align}\]</span> \]</p>
<p>is the logarithmic fold change due to treatment with the siRNA. In exactly the same way, \(_2\) is the logarithmic fold change due to treatment with the drug. What happens if we treat the cells with both siRNA and drug? In that case, \(x_1=x_2=1\), and Equation 8.1 can be rewritten as</p>
<p>\[ _{12} = y - (_0 + _1 + _2). \]</p>
<p>This means that \(_{12}\) is the difference between the observed outcome, \(y\), and the outcome expected from the individual treatments, obtained by adding to the baseline the effect of siRNA alone, \(_1\), and of drug alone, \(_2\).</p>
<p>We call \(_{12}\) the <em>interaction</em> effect of siRNA and drug. It has nothing to do with a physical interaction, the terminology indicates that the effects of these two different experimental factors do not simply add up, but combine in a more complicated fashion.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="Note that the addition is on the logarithmic scale, which corresponds to multiplication on the original scale."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>Note that the addition is on the logarithmic scale, which corresponds to multiplication on the original scale.</figcaption>
</figure>
</div>
<p>Note that the addition is on the logarithmic scale, which corresponds to multiplication on the original scale.</p>
<p>For instance, if the target of the drug and of the siRNA were equivalent, leading to the same effect on the cells, then we biologically expect that \(_1=<em>2\). We also expect that their combination has no further effect, so that \(</em>{12}=-_1\). If, on the other hand, the targets of the drug and of the siRNA are in parallel pathways that can buffer each other, we’ll expect that \(_1\) and \(<em>2\) are both relatively small, but that the combined effect is synergistic, and \(</em>{12}\) is large.</p>
<p>Not always do we care about interactions. Many experiments are designed with multiple factors where we care most about each of their individual effects. In that case, the combinatorial treatment might not be present in the experimental design, and the model to use for the analysis is a version of Equation 8.1 with the rightmost term removed.</p>
<p>We can succinctly encode the design of the experiment in the <em>design matrix</em>. For instance, for the combinatorial experiment described above, the design matrix is</p>
<p>\[ ]</p>
<p>The columns of the design matrix correspond to the experimental factors, and its rows represent the different experimental conditions, four in our case. If, instead, the combinatorial treatment is not performed, then the design matrix is reduced to only the first three rows of 8.4.</p>
</section>
<section id="what-about-noise-and-replicates" class="level3" data-number="10.6.2">
<h3 data-number="10.6.2" class="anchored" data-anchor-id="what-about-noise-and-replicates"><span class="header-section-number">10.6.2</span> 8.7.2 What about noise and replicates?</h3>
<p>Equation 8.1 provides a conceptual decomposition of the observed data into the effects caused by the different experimental variables. If our data (the \(y\)s) were absolutely precise, we could set up a linear system of equations, one equation for each of the four possible experimental conditions represented by the \(x\)s, and solve for the \(\)s.</p>
<p>Of course, we usually wish to analyze real data that are affected by noise. We then need replicates to estimate the levels of noise and assess the uncertainty of our estimated \(\)s. Only then we can empirically assess whether any of the observed changes between conditions are significantly larger than those occuring just due to experimental or natural variation. We need to slightly extend the equation,</p>
<p>\[ y_{j} = x_{j0} ; <em>0 + x</em>{j1} ; <em>1 + x</em>{j2} ; <em>2 + x</em>{j1},x_{j2};_{12} + _j. \]</p>
<p>We have added the index \(j\) and a new term \(<em>j\). The index \(j\) now explicitly counts over our individual replicate experiments; for instance, if for each of the four conditions we perform three replicates, then \(j\) counts from 1 to 12. The design matrix has now 12 rows, and \(x</em>{jk}\) is the value of the matrix in its \(j\)th row and \(k\)th column.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="Remember that since \beta_0 is the intercept, x_{j0}=1 for all j."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>Remember that since \beta_0 is the intercept, x_{j0}=1 for all j.</figcaption>
</figure>
</div>
<p>Remember that since \(<em>0\) is the intercept, \(x</em>{j0}=1\) for all \(j\).</p>
<p>The additional terms \(_j\), which we call the <strong>residuals</strong> , are there to absorb differences between replicates. However, one additional modeling component is needed: the system of twelve equations 8.5 would be underdetermined without further information, since it has now more variables (twelve epsilons and four betas) than it has equations (twelve, one for each \(j\)). To fix this, we require that the \(_j\) be small. One popular way – we’ll encounter others – to overcome this is to minimize the sum of squared residuals,</p>
<p>\[ _j _j^2 . \]</p>
<p>It turns out that with this requirement satisfied, the \(\)s represent the <em>average</em> effects of each of the experimental factors, while the residuals \(_j\) reflect the experimental fluctuations around the mean between the replicates. This approach, which is called the <strong>least sum of squares fitting</strong> , is mathematically convenient, since it can achieved by straightforward matrix algebra. It is what the R function <code>lm</code> does.</p>
<p>__</p>
<p>Question 8.5</p>
<p>An alternative way to write Equation 8.5 is</p>
<p>\[ y_{j} = <em>k x</em>{jk} ; _k + _j. \]</p>
<p>How can this be mapped to Equation 8.5, i.e., what’s with the interaction term \(x_{j1},x_{j2};_{12}\)?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>This is really just a trivial matter of notation: the sum extends over \(k=0,…,3\), where the terms for \(k=0,1,2\) are exactly as we know them already. We write \(<em>{3}\) instead of \(</em>{12}\), and \(x_{j3}\) is defined to be \(x_{j1}x_{j2}\). The generic notation 8.7 is practical to use in computer software that implements linear models and in mathematical proofs. It also highlights that the “scientific content” of a linear model is condensed in its design matrix.</p>
<p>__</p>
<p>Task</p>
<p>Show that if we have fit Equation 8.5 to data such that objective 8.6 holds, the fit residuals \(_j\) have an average of 0.</p>
</section>
<section id="analysis-of-variance" class="level3" data-number="10.6.3">
<h3 data-number="10.6.3" class="anchored" data-anchor-id="analysis-of-variance"><span class="header-section-number">10.6.3</span> 8.7.3 Analysis of variance</h3>
<p>A model like 8.5 is called a <strong>linear model</strong> , and often it is implied that criterion 8.6 is used to fit it to data. This approach is elegant and powerful, but for novices it can take some time to appreciate all its facets. What is the advantage over just simply taking, for each distinct experimental condition, the average over replicates and comparing these values across conditions? In simple cases, the latter approach can be intuitive and effective. However, it comes to its limits when the replicate numbers are not all the same in the different groups, or when one or more of the \(x\)-variables is continuous-valued. In these cases, one will invariably end up with something like fitting 8.5 to the data. A useful way to think about 8.5 is contained in the term <strong>analysis of variance</strong> , abbreviated ANOVA. In fact, what Equation 8.5 does is decompose the variability of \(y\) that we observed in the course of our experiments into elementary components: its baseline value \(_0\), its variability caused by the effect of the first variable, \(_1\), its variability caused by the effect of the second variable, \(<em>2\), its variability caused by the effect of the interaction, \(</em>{12}\), and variability that is unaccounted for. The last of these we commonly call <em>noise</em> , the other ones, <em>systematic variability</em>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="The distinction between noise and systematic variability is in the eye of the beholder, and depends on our model, not on reality."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>The distinction between noise and systematic variability is in the eye of the beholder, and depends on our model, not on reality.</figcaption>
</figure>
</div>
<p>The distinction between noise and systematic variability is in the eye of the beholder, and depends on our model, not on reality.</p>
</section>
<section id="robustness" class="level3" data-number="10.6.4">
<h3 data-number="10.6.4" class="anchored" data-anchor-id="robustness"><span class="header-section-number">10.6.4</span> 8.7.4 Robustness</h3>
<p>The sum 8.6 is sensitive to outliers in the data. A single measurement \(y_{j}\) with an outlying value can draw the \(\) estimates far away from the values implied by the other replicates. This is the well-known fact that methods based on least sum of squares have a low <strong>breakdown point</strong> : if even only a single data point is outlying, the whole statistical result can be strongly affected. For instance, the average of a set of \(n\) numbers has a breakdown point of \(\), meaning that it can be arbitrarily changed by changing only a single one of the numbers. On the other hand, the median has a much higher breakdown point. Changing a single number often has no effect at all, and when it does, the effect is limited to the range of the data points in the middle of the ranking (i.e., those adjacent to rank \(\)). To change the median by an arbitrarily high amount, you need to change half the observations. We call the median <strong>robust</strong> , and its breakdown point is \(\). Remember that the median of a set of numbers \(y_1, y_2, …\) minimizes the sum \(_j|y_j-_0|\).</p>
<p>To achieve a higher degree of robustness against outliers, other choices than the sum of squares 8.6 can be used as the objective of minimization. Among these are:</p>
<p>\[ <span class="math display">\[\begin{align} R &amp;= \sum_j |\varepsilon_j| &amp; \text{Least absolute
deviations} \\\ R &amp;= \sum_j \rho_s(\varepsilon_j) &amp; \text{M-estimation} \\\ R
&amp;= Q_{\theta}\left( \\{\varepsilon_1^2, \varepsilon_2^2,... \\} \right) &amp;
\text{LTS, LQS} \\\ R &amp;= \sum_j w_j \varepsilon_j^2 &amp; \text{general weighted
regression} \end{align}\]</span> \]</p>
<p>Here, \(R\) is the quantity to be minimized. The first choice in Equation 8.8 is called <strong>least absolute deviations</strong> regression. It can be viewed as a generalization of the median. Although conceptually simple, and attractive on first sight, it is harder to minimize than the sum of squares, and it can be less stable and less efficient especially if the data are limited, or do not fit the model8. The second choice in Equation 8.8, also called <strong>M-estimation</strong> , uses a penalization function \(_s\) (least-squares regression is the special case with \(_s()=^2\)) that looks like a quadratic function for a limited range of \(\), but has a smaller slope, flattens out, or even drops back to zero, for absolute values \(||\) that are larger than the scale parameter \(s\). The intention behind this is to downweight the effect of outliers, i.e.&nbsp;of data points that have large residuals (<a href="16-chap.html#ref- Huber:AMS:1964">Huber 1964</a>). A choice of \(s\) needs to be made and determines what is called an outlier. One can even drop the requirement that \(_s\) is quadratic around 0 (as long as its second derivative is positive), and a variety of choices for the function \(_s\) have been proposed in the literature. The aim is to give the estimator desirable statistical properties (say, bias and efficiency) when and where the data fit the model, but to limit or nullify the influence of those data points that do not, and to keep computations tractable.</p>
<p>8 The <a href="https://en.wikipedia.org/wiki/Least_absolute_deviations">Wikipedia article</a> gives an overview.</p>
<p>__</p>
<p>Question 8.6</p>
<p>Plot the graph of the function \(_s()\) proposed by Huber (<a href="16-chap.html#ref-Huber:AMS:1964">1964</a>) for M-estimators.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Huber’s paper defines, on Page 75:</p>
<p>\[ _s() = \{ ]</p>
<p>The graph produced by the below code is shown in Figure 8.8.</p>
<pre><code>rho = function(x, s)
  ifelse(abs(x) &lt; s, x^2 / 2,  s * abs(x) - s^2 / 2)

df = tibble(
  x        = seq(-7, 7, length.out = 100),
  parabola = x ^ 2 / 2,
  Huber    = rho(x, s = 2))

ggplot(reshape2::melt(df, id.vars = "x"),
  aes(x = x, y = value, col = variable)) + geom_line()__</code></pre>
<p><a href="08-chap_files/figure-html/fig-countdata-mestimator-1.png &quot;Figure 8.8: Graph of \rho_s(\varepsilon), for a choice of s=2.&quot;"><img src="08-chap_files/figure-html/fig-countdata- mestimator-1.png" class="img-fluid"></a></p>
<p>Figure 8.8: Graph of \(_s()\), for a choice of \(s=2\).</p>
<p>Choice three in 8.8 generalises the least sum of squares method in yet another way. In <strong>least quantile of squares</strong> (LQS) regression, the the sum over the squared residuals is replaced with a quantile, for instance, \(Q_{50}\), the median, or \(Q_{90}\), the 90%-quantile (<a href="16-chap.html#ref-Rousseeuw:1987">Peter J. Rousseeuw 1987</a>). In a variation thereof, <strong>least trimmed sum of squares</strong> (LTS) regression, a sum of squared residuals is used, but the sum extends not over all residuals, but only over the fraction \(0\) of smallest residuals. The motivation in either case is that outlying data points lead to large residuals, and as long as they are rare, they do not affect the quantile or the trimmed sum.</p>
<p>However, there is a price: while the least sum of squares optimization 8.6 can be done through straightforward linear algebra, more complicated iterative optimization algorithms are needed for M-estimation, LQS and LTS regression.</p>
<p>The final approach in 8.8 represents an even more complex way of weighting down outliers. It assumes that we have some way of deciding what weight \(w_j\) we want to give to each observation, presumably down-weighting outliers. For instance, in Section 8.10.3, we will encounter the approach used by the <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> package, in which the leverage of each data point on the estimated \(\)s is assessed using a measure called Cook’s distance. For those data whose Cook’s distance is deemed too large, the weight \(w_j\) is set to zero, whereas the other data points get \(w_j=1\). In effect, this means that the outlying data points are discarded and that ordinary regression is performed on the others. The extra computational effort of carrying the weights along is negligible, and the optimization is still straightforward linear algebra.</p>
<p>All of these approaches to outlier robustness introduce a degree of subjectiveness and rely on sufficient replication. The subjectiveness is reflected by the parameter choices that need to be made: \(s\) in 8.8 (2), \(\) in 8.8 (3), the weights in 8.8 (4). One scientist’s outlier may be the Nobel prize of another. On the other hand, outlier removal is no remedy for sloppy experiments and no justification for wishful thinking.</p>
<p>__</p>
<p>Task</p>
<p>Search the documentation of R and CRAN packages for implementations of the above robust regression methods. A good place to start is the <a href="https://cran.r-project.org/web/views/Robust.html">CRAN task view on robust statistical methods</a>.</p>
</section>
</section>
<section id="generalized-linear-models" class="level2" data-number="10.7">
<h2 data-number="10.7" class="anchored" data-anchor-id="generalized-linear-models"><span class="header-section-number">10.7</span> 8.8 Generalized linear models</h2>
<p>We need to explore two more theoretical concepts before we can proceed to our next application example. Equations of the form 8.5 model the expected value of the outcome variable, \(y\), as a linear function of the design matrix, and they are fit to data according to the least sum of squares criterion 8.6; or a robust variant thereof. We now want to generalize these assumptions.</p>
<section id="modeling-the-data-on-a-transformed-scale" class="level3" data-number="10.7.1">
<h3 data-number="10.7.1" class="anchored" data-anchor-id="modeling-the-data-on-a-transformed-scale"><span class="header-section-number">10.7.1</span> 8.8.1 Modeling the data on a transformed scale</h3>
<p>We already saw that it can be fruitful to consider the data not on the scale that we obtained them, but after some transformation, for instance, the logarithm. This idea can be generalized, since depending on the context, other transformations are useful. For instance, the linear model 8.5 would not directly be useful for modeling outcomes that are bounded within an interval, say, \([0,1]\) as an indicator of disease risk. In a linear model, the values of \(y\) cover, in principle, the whole real axis. However, if we transform the expression on the right hand with a sigmoid function, for instance, \(f(y) = 1/(1+e^{-y})\), then the range of this function9, is bounded between 0 and 1 and can be used to model such an outcome.</p>
<p>9 It is called the logistic function (<a href="16-chap.html#ref- Verhulst:1845">Verhulst 1845</a>), and the associated regression model is called <strong>logistic regression</strong>.</p>
</section>
<section id="other-error-distributions" class="level3" data-number="10.7.2">
<h3 data-number="10.7.2" class="anchored" data-anchor-id="other-error-distributions"><span class="header-section-number">10.7.2</span> 8.8.2 Other error distributions</h3>
<p>The other generalization regards the minimization criterion 8.6. In fact, this criterion can be derived from a specific probabilistic model and the <strong>maximum likelihood</strong> principle (we already encountered this in <a href="02-chap.html">Chapter 2</a>). To see this, consider the probabilistic model</p>
<p>\[ p(_j) = (-), \]</p>
<p>that is, we believe that the residuals follow a normal distribution with mean 0 and standard deviation \(\). Then it is plausible to demand from a good model (i.e., from a good set of \(\)s) that these probabilities are large. Formally,</p>
<p>\[ _j p(_j) . \]</p>
<p>__</p>
<p>Question 8.7</p>
<p>Show that the maximizing the likelihood 8.10 is equivalent to minimizing the sum of squared residuals 8.6.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Insert 8.9 into 8.10 and take the logarithm.</p>
<p>Let’s revise some core concepts: the left hand side of Equation 8.10, i.e., the product of the probabilities of the residuals, is a function of both the model parameters \(_1, _2, …\) and the data \(y_1, y_2, …\); call it \(f(,y)\). If we think of the model parameters \(\) as given and fixed, then the collapsed function \(f(y)\) simply indicates the probability of the data. We could use it, for instance, to simulate data. If, on the other hand, we consider the data as given, then \(f()\) is a function of the model parameters, and it is called the <em>likelihood</em>. The second view is the one we take when we optimise 8.6 (and thus 8.10), and hence the \(\)s obtained this way are what is called <em>maximum-likelihood estimates</em>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="imgs/devil.png" title="It is good to remember that, while we can use the normal distribution as a convenient argument to motivate least sum of squares regression through the maximum likelihood principle, the data do not have to be distributed according to the normal for least sum of squares regression to provide a useful result. In fact, least sum of squares fitting often provides useful estimates for the \betas even when the data are non-normal, although that depends on the specific circumstances."><img src="imgs/devil.png" class="img-fluid figure-img"></a></p>
<figcaption>It is good to remember that, while we can use the normal distribution as a convenient argument to motivate least sum of squares regression through the maximum likelihood principle, the data do not have to be distributed according to the normal for least sum of squares regression to provide a useful result. In fact, least sum of squares fitting often provides useful estimates for the \betas even when the data are non-normal, although that depends on the specific circumstances.</figcaption>
</figure>
</div>
<p>It is good to remember that, while we can use the normal distribution as a convenient argument to motivate least sum of squares regression through the maximum likelihood principle, the data do not have to be distributed according to the normal for least sum of squares regression to provide a useful result. In fact, least sum of squares fitting often provides useful estimates for the \(\)s even when the data are non-normal, although that depends on the specific circumstances.</p>
<p>The generalization that we can now make is to use a different probabilistic model. We can use the densities of other distributions than the normal instead of Equation 8.9. For instance, to be able to deal with count data, we will use the gamma-Poisson distribution.</p>
</section>
<section id="a-generalized-linear-model-for-count-data" class="level3" data-number="10.7.3">
<h3 data-number="10.7.3" class="anchored" data-anchor-id="a-generalized-linear-model-for-count-data"><span class="header-section-number">10.7.3</span> 8.8.3 A generalized linear model for count data</h3>
<p>The differential expression analysis in <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> uses a generalized linear model of the form:</p>
<p>\[ <span class="math display">\[\begin{align} K_{ij} &amp; \sim \text{GP}(\mu_{ij}, \alpha_i) \\\ \mu_{ij} &amp;=
s_j\, q_{ij} \\\ \log_2(q_{ij}) &amp;= \sum_k x_{jk} \beta_{ik}. \end{align}\]</span> \]</p>
<p>Let us unpack this step by step. The counts \(K_{ij}\) for gene \(i\), sample \(j\) are modeled using a gamma-Poisson (GP) distribution with two parameters, the mean \(<em>{ij}\) and the dispersion \(<em>i\). By default, the dispersion is different for each gene \(i\), but the same across all samples, therefore it has no index \(j\). The second line in Equation 8.11 states that the mean is composed of a sample-specific size factor \(s_j\)10 and \(q</em>{ij}\), which is proportional to the true expected concentration of fragments for gene \(i\) in sample \(j\). The value of \(q</em>{ij}\) is given by the linear model in the third line via the <em>link function</em> , \(<em>2\). The design matrix \((x</em>{jk})\) is the same for all genes (and therefore does not depend on \(i\)). Its rows \(j\) correspond to the samples, its columns \(k\) to the experimental factors. In the simplest case, for a pairwise comparison, the design matrix has only two columns, one of them everywhere filled with 1 (corresponding to \(<em>0\) of Section 8.7.1) and the other one containing 0 or 1 depending on whether the sample belongs to one or the other group. The coefficients \(</em>{ik}\) give the \(_2\) fold changes for gene \(i\) for each column of the design matrix \(X\).</p>
<p>10 The model can be generalized to use sample- <strong>and</strong> gene-dependent normalization factors \(s_{ij}\). This is explained in the documentation of the <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> package.</p>
</section>
</section>
<section id="two-factor-analysis-of-the-pasilla-data" class="level2" data-number="10.8">
<h2 data-number="10.8" class="anchored" data-anchor-id="two-factor-analysis-of-the-pasilla-data"><span class="header-section-number">10.8</span> 8.9 Two-factor analysis of the pasilla data</h2>
<p>Besides the treatment with siRNA, which we have already considered in Section 8.5, the <strong><a href="https://bioconductor.org/packages/pasilla/">pasilla</a></strong> data have another covariate, <code>type</code>, which indicates the type of sequencing that was performed.</p>
<p>We saw in the exploratory data analysis (EDA) plots in Section 8.5.3 that the latter had a considerable systematic effect on the data. Our basic analysis of Section 8.5 did not take this account, but we will do so now. This should help us get a more correct picture of which differences in the data are attributable to the treatment, and which are confounded – or masked – by the sequencing type.</p>
<pre><code>pasillaTwoFactor = pasilla
design(pasillaTwoFactor) = formula(~ type + condition)
pasillaTwoFactor = DESeq(pasillaTwoFactor)__</code></pre>
<p>Of the two variables <code>type</code> and <code>condition</code>, the one of primary interest is <code>condition</code>, and in <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> , the convention is to put it at the end of the formula. This convention has no effect on the model fitting, but it helps simplify some of the subsequent results reporting. Again, we access the results using the <code>results</code> function, which returns a dataframe with the statistics of each gene.</p>
<pre><code>res2 = results(pasillaTwoFactor)
head(res2, n = 3)__


log2 fold change (MLE): condition treated vs untreated 
Wald test p-value: condition treated vs untreated 
DataFrame with 3 rows and 6 columns
             baseMean log2FoldChange     lfcSE       stat    pvalue      padj
            &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt;  &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;
FBgn0000003  0.171569      0.6745518  3.871091  0.1742537  0.861666        NA
FBgn0000008 95.144079     -0.0406731  0.222215 -0.1830351  0.854770  0.951975
FBgn0000014  1.056572     -0.0849880  2.111821 -0.0402439  0.967899        NA</code></pre>
<p>It is also possible to retrieve the \(_2\) fold changes, p-values and adjusted p-values associated with the <code>type</code> variable. The function <code>results</code> takes an argument <code>contrast</code> that lets users specify the name of the variable, the level that corresponds to the numerator of the fold change and the level that corresponds to the denominator of the fold change.</p>
<pre><code>resType = results(pasillaTwoFactor,
  contrast = c("type", "single", "paired"))
head(resType, n = 3)__


log2 fold change (MLE): type single vs paired 
Wald test p-value: type single vs paired 
DataFrame with 3 rows and 6 columns
             baseMean log2FoldChange     lfcSE      stat    pvalue      padj
            &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;
FBgn0000003  0.171569      -1.611546  3.871083 -0.416304  0.677188        NA
FBgn0000008 95.144079      -0.262255  0.220686 -1.188362  0.234691  0.543822
FBgn0000014  1.056572       3.290586  2.087243  1.576522  0.114905        NA</code></pre>
<p>So what did we gain from this analysis that took into account <code>type</code> as a nuisance factor (sometimes also called, more politely, a <em>blocking factor</em>), compared to the simple comparison between two groups of Section 8.5? Let us plot the p-values from both analyses against each other.</p>
<pre><code>trsf = function(x) ifelse(is.na(x), 0, (-log10(x)) ^ (1/6))
ggplot(tibble(pOne = res$pvalue,
              pTwo = res2$pvalue),
    aes(x = trsf(pOne), y = trsf(pTwo))) +
    geom_hex(bins = 75) + coord_fixed() +
    xlab("Single factor analysis (condition)") +
    ylab("Two factor analysis (type + condition)") +
    geom_abline(col = "orange")__</code></pre>
<p><a href="08-chap_files/figure-html/fig-countdata-scpres1res2-1.png &quot;Figure 8.9: Comparison of p-values from the models with a single factor (condition) and with two factors (type + condition). The axes correspond to (-\log_{10}p)^{\frac{1}{6}}, an arbitrarily chosen monotonically decreasing transformation that compresses the dynamic range of the p-values for the purpose of visualization. We can see a trend for the joint distribution to lie above the bisector, indicating that the small p-values in the two-factor analysis are generally smaller than those in the one-factor analysis.&quot;"><img src="08-chap_files/figure-html/fig-countdata- scpres1res2-1.png" class="img-fluid"></a></p>
<p>Figure 8.9: Comparison of p-values from the models with a single factor (condition) and with two factors (type + condition). The axes correspond to \((-_{10}p)^{}\), an arbitrarily chosen monotonically decreasing transformation that compresses the dynamic range of the p-values for the purpose of visualization. We can see a trend for the joint distribution to lie above the bisector, indicating that the small p-values in the two-factor analysis are generally smaller than those in the one-factor analysis.</p>
<p>As we can see in Figure 8.9, the p-values in the two-factor analysis are similar to those from the one-factor analysis, but are generally smaller. The more sophisticated analysis has led to an, albeit modest, increase in power. We can also see this by counting the number of genes that pass a certain significance threshold in each case:</p>
<pre><code>compareRes = table(
   `simple analysis` = res$padj &lt; 0.1,
   `two factor` = res2$padj &lt; 0.1 )
addmargins( compareRes )__


               two factor
simple analysis FALSE TRUE  Sum
          FALSE  6973  289 7262
          TRUE     25 1036 1061
          Sum    6998 1325 8323</code></pre>
<p>The two-factor analysis found 1325 genes differentially expressed at an FDR threshold of 10%, while the one-factor analysis found 1061. The two-factor analysis has increased detection power. In general, the gain can be even much larger, or also smaller, depending on the data. The proper choice of the model requires informed adaptation to the experimental design and data quality.</p>
<p>__</p>
<p>Question 8.8</p>
<p><em>Why</em> do we detect fewer significant genes when we do not take into account the <code>type</code> variable? More generally, what does this mean about the benefit of taking into account (or not) blocking factors?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Without modeling the blocking factor, the variability in the data that is due to it has to be absorbed by the \(\)s. This means that they are generally larger than in the model with the blocking factor. The higher level of noise leads to higher uncertainty in the \(\)-estimates. On the other hand, the model with the blocking factor has more parameters that need to be estimated. In statistical parlance, the fit has fewer “degrees of freedom”. Both of these effects are counteracting, and which of them prevails, and which of the modeling choices yields more or fewer significant results depends on the data.</p>
<p>__</p>
<p>Question 8.9</p>
<p>What is confounding? Can <em>not</em> taking into account a blocking factor also lead to the detection of <em>more</em> genes?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Yes. Imagine the variables <code>condition</code> and <code>type</code> were not as nicely balanced as they are, but partially or fully confounded. In that case, differences in the data due to <code>type</code> could be attributed to <code>condition</code> if a model is fit that does not make it possible to absorb them in the <code>type</code>-effect. Scientifically, such an experiment (and analysis) can be quite an embarrassment.</p>
<p>__</p>
<p>Question 8.10</p>
<p>Consider a paired experimenal design, say, 10 different cell lines each with and without drug treatment. How should this be analyzed?</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>If we just did a simple two-group comparison (treated versus untreated) many of the treatment effects would probably go under in the strong cell line to cell line variation. However, we can set up a <em>paired</em> analysis simply by adding cell line identity as a blocking factor. (Cell line is then really an R <em>factor</em> with 10 different levels, rather than just a 0 vs 1 indicator variable as with the variables that we looked at so far; R’s linear modeling facilities, and also <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> , have no problem dealing with that.)</p>
<p>__</p>
<p>Question 8.11</p>
<p>What can you do if you suspect there are “hidden” factors that affect your data, but they are not documented? (Sometimes, such undocumented covariates are also called <strong>batch effects</strong>.)</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>There are methods that try to identify blocking factors in an unsupervised fashion, see e.g., Leek and Storey (<a href="16-chap.html#ref-LeekStorey:2007">2007</a>; <a href="16-chap.html#ref-Stegle:2010">Stegle et al.&nbsp;2010</a>).</p>
</section>
<section id="further-statistical-concepts" class="level2" data-number="10.9">
<h2 data-number="10.9" class="anchored" data-anchor-id="further-statistical-concepts"><span class="header-section-number">10.9</span> 8.10 Further statistical concepts</h2>
<section id="sharing-of-dispersion-information-across-genes" class="level3" data-number="10.9.1">
<h3 data-number="10.9.1" class="anchored" data-anchor-id="sharing-of-dispersion-information-across-genes"><span class="header-section-number">10.9.1</span> 8.10.1 Sharing of dispersion information across genes</h3>
<p>We already saw an explanation of Bayesian (or empirical Bayes) analysis in <a href="06-chap.html#fig-testing-sunexplode">Figure 6.16</a>. The idea is to use additional information to improve our estimates (information that we either known a priori, or have from analysis of other, but similar data). This idea is particularly useful if the data per se are relatively noisy. <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> uses an empirical Bayes approach for the estimation of the dispersion parameters (the \(\)s in the third line of Equation 8.11) and, optionally, the logarithmic fold changes (the \(\)s). The priors are, in both cases, taken from the distributions of the maximum-likelihood estimates (MLEs) across all genes. It turns out that both of these distributions are uni-modal; in the case of the \(\)s, with a peak at around 0, in the case of the \(\), at a particular value, the typical dispersion. The empirical Bayes machinery then shrinks each per-gene MLE towards that peak, by an amount that depends on the sharpness of the empirical prior distribution and the precision of the ML estimate (the better the latter, the less shrinkage will be done). The mathematics are explained in (<a href="16-chap.html#ref-LoveDESeq2">Michael I. Love, Huber, and Anders 2014</a>), and Figure 8.10 visualizes the approach for the \(\)s.</p>
<p>__</p>
<p>Task</p>
<p>Advanced: check the R code that produces Figure 8.10.</p>
<p>[<img src="08-chap_files/figure-html/fig-countdata- posterior-1.png" class="img-fluid">](08-chap_files/figure-html/fig-countdata-posterior-1.png “Figure&nbsp;8.10&nbsp;(a):”)</p>
<ol type="a">
<li></li>
</ol>
<p>[<img src="08-chap_files/figure-html/fig-countdata- posterior-2.png" class="img-fluid">](08-chap_files/figure-html/fig-countdata-posterior-2.png “Figure&nbsp;8.10&nbsp;(b):”)</p>
<ol start="2" type="a">
<li></li>
</ol>
<p>Figure 8.10: Shrinkage estimation of logarithmic fold change estimates by use of an empirical prior in <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong>. Two genes with similar mean count and MLE logarithmic fold change are highlighted in green and blue. The normalized counts for these genes (a) reveal low dispersion for the gene in blue and high dispersion for the gene in green. In (b), the density plots are shown of the normalized likelihoods (solid lines) and of the posteriors (dashed lines) for the green and blue gene. In addition, the solid black line shows the prior estimated from the MLEs of all genes. Due to the higher dispersion of the green gene, its likelihood is wider and less sharp (indicating less information), and the prior has more influence on its posterior than in the case of the blue gene.</p>
</section>
<section id="count-data-transformations" class="level3" data-number="10.9.2">
<h3 data-number="10.9.2" class="anchored" data-anchor-id="count-data-transformations"><span class="header-section-number">10.9.2</span> 8.10.2 Count data transformations</h3>
<p>For testing for differential expression we operate on raw counts and use discrete distributions. For other downstream analyses – e.g., for visualization or clustering – it might however be useful to work with transformed versions of the count data.</p>
<p>Maybe the most obvious choice of transformation is the logarithm. However, since count values for a gene can become zero, some advocate the use of <strong>pseudocounts</strong> , i.e., transformations of the form</p>
<p>\[ y = _2(n + 1)y = _2(n + n_0), \]</p>
<p>where \(n\) represents the count values and \(n_0\) is a somehow chosen positive constant.</p>
<p>Let’s look at two alternative approaches that offer more theoretical justification, and a rational way of choosing the parameter equivalent to \(n_0\) above. One method incorporates priors on the sample differences, and the other uses the concept of variance-stabilizing transformations.</p>
<section id="variance-stabilizing-transformation" class="level4" data-number="10.9.2.1">
<h4 data-number="10.9.2.1" class="anchored" data-anchor-id="variance-stabilizing-transformation"><span class="header-section-number">10.9.2.1</span> Variance-stabilizing transformation</h4>
<p>We already explored <strong>variance-stabilizing transformations</strong> in <a href="04-chap.html#sec-mixtures-vst">Section 4.4.4</a>. There we computed a piece-wise linear transformation for a discrete set of random variables (<a href="04-chap.html#fig-pcwlin-1">Figure 4.26</a>) and also saw how to use calculus to derive a smooth variance-stabilizing transformation for a gamma-Poisson mixture. These computations are implemented in the <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> package (<a href="16-chap.html#ref-Anders:2010:GB">Anders and Huber 2010</a>):</p>
<pre><code>vsp = varianceStabilizingTransformation(pasilla)__</code></pre>
<p>Let us explore the effect of this on the data, using the first sample as an example, and comparing it to the \(_2\) transformation; the plot is shown in Figure 8.11 and is made with the following:</p>
<pre><code>j = 1
ggplot(
  tibble(
    counts = rep(assay(pasilla)[, j], 2),
    transformed = c(
      assay(vsp)[, j],
      log2(assay(pasilla)[, j])
      ),
    transformation = rep(c("VST", "log2"), each = nrow(pasilla))
  ),
  aes(x = counts, y = transformed, col = transformation)) +
  geom_line() + xlim(c(0, 600)) + ylim(c(0, 9))__</code></pre>
<p><a href="08-chap_files/figure-html/fig-countdata-plotvst-1.png &quot;Figure 8.11: Graph of variance-stabilizing transformation for the data of one of the samples, and for comparison also of the \log_2 transformation. The variance-stabilizing transformation has finite values and finite slope even for counts close to zero, whereas the slope of \log_2 becomes very steep for small counts and is undefined for counts of zero. For large counts, the two transformation are essentially the same.&quot;"><img src="08-chap_files/figure-html/fig-countdata- plotvst-1.png" class="img-fluid"></a></p>
<p>Figure 8.11: Graph of variance-stabilizing transformation for the data of one of the samples, and for comparison also of the \(_2\) transformation. The variance-stabilizing transformation has finite values and finite slope even for counts close to zero, whereas the slope of \(_2\) becomes very steep for small counts and is undefined for counts of zero. For large counts, the two transformation are essentially the same.</p>
</section>
<section id="regularized-logarithm-rlog-transformation" class="level4" data-number="10.9.2.2">
<h4 data-number="10.9.2.2" class="anchored" data-anchor-id="regularized-logarithm-rlog-transformation"><span class="header-section-number">10.9.2.2</span> Regularized logarithm (rlog) transformation</h4>
<p>There is a second way to come up with a data transformation. It is conceptually distinct from variance stabilization. Instead, it builds upon the shrinkage estimation that we already explored in Section 8.10.1. It works by transforming the original count data to a \(<em>2\)-like scale by fitting a “trivial” model with a separate term for each sample and a prior distribution on the coefficients which is estimated from the data. The fitting employs the same regularization as what we discussed in Section 8.10.1. The transformed data \(q</em>{ij}\) are defined by the third line of Equation 8.11, where the design matrix \((x_{jk})\) is of size \(K (K+1)\) – here \(K\) is the number of samples– and has the form</p>
<p>\[ X=(]</p>
<p>Without priors, this design matrix would lead to a non-unique solution, however the addition of a prior on non-intercept \(\)s allows for a unique solution to be found.</p>
<p>In <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> , this functionality is implemented in the function <code>rlogTransformation</code>. It turns out in practice that the rlog transformation is also approximately variance- stabilizing, but in contrast to the variance-stabilizing transformation of Section 8.10.2 it deals better with data in which the size factors of the different samples are very distinct.</p>
<p>__</p>
<p>Question 8.12</p>
<p>Plot mean against standard deviation between replicates for the shifted logarithm 8.12, the regularized log transformation and the variance- stabilizing transformation.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>See Figure 8.12.</p>
<pre><code>library("vsn")
rlp = rlogTransformation(pasilla)

msd = function(x)
  meanSdPlot(x, plot = FALSE)$gg + ylim(c(0, 1)) +
     theme(legend.position = "none")

gridExtra::grid.arrange(
  msd(log2(counts(pasilla, normalized = TRUE) + 1)) +
    ylab("sd(log2)"),
  msd(assay(vsp)) + ylab("sd(vst)"),
  msd(assay(rlp)) + ylab("sd(rlog)"),
  ncol = 3
)__</code></pre>
<p><a href="08-chap_files/figure-html/fig-countdata-meansd-1.png &quot;Figure 8.12: Per-gene standard deviation (sd, taken across samples) against the rank of the mean, for the shifted logarithm \log_2(n+1), the variance- stabilizing transformation (vst) and the rlog. Note that for the leftmost \approx 2,500 genes, the counts are all zero, and hence their standard deviation is zero. The mean-sd dependence becomes more interesting for genes with non-zero counts. Note also the high value of the standard deviation for genes that are weakly detected (but not with all zero counts) when the shifted logarithm is used, and compare to the relatively flat shape of the mean-sd relationship for the variance-stabilizing transformation.&quot;"><img src="08-chap_files/figure-html/fig-countdata- meansd-1.png" class="img-fluid"></a></p>
<p>Figure 8.12: Per-gene standard deviation (sd, taken across samples) against the rank of the mean, for the shifted logarithm \(_2(n+1)\), the variance-stabilizing transformation (vst) and the rlog. Note that for the leftmost \(\) 2,500 genes, the counts are all zero, and hence their standard deviation is zero. The mean-sd dependence becomes more interesting for genes with non-zero counts. Note also the high value of the standard deviation for genes that are weakly detected (but not with all zero counts) when the shifted logarithm is used, and compare to the relatively flat shape of the mean-sd relationship for the variance-stabilizing transformation.</p>
</section>
</section>
<section id="dealing-with-outliers" class="level3" data-number="10.9.3">
<h3 data-number="10.9.3" class="anchored" data-anchor-id="dealing-with-outliers"><span class="header-section-number">10.9.3</span> 8.10.3 Dealing with outliers</h3>
<p>The data sometimes contain isolated instances of very large counts that are apparently unrelated to the experimental or study design, and which may be considered outliers. There are many reasons why outliers can arise, including rare technical or experimental artifacts, read mapping problems in the case of genetically differing samples, and genuine, but rare biological events. In many cases, users appear primarily interested in genes that show a consistent behaviour, and this is the reason why by default, genes that are affected by such outliers are set aside by <code>DESeq</code>. The function calculates, for every gene and for every sample, a diagnostic test for outliers called <strong>Cook’s distance</strong>(<a href="16-chap.html#ref-Cook1977Detection">Cook 1977</a>). Cook’s distance is a measure of how much a single sample is influencing the fitted coefficients for a gene, and a large value of Cook’s distance is intended to indicate an outlier count. <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> automatically flags genes with Cook’s distance above a cutoff and sets their p-values and adjusted p-values to <code>NA</code>.</p>
<p>The default cutoff depends on the sample size and number of parameters to be estimated; <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> uses the \(99%\) quantile of the \(F(p,m-p)\) distribution (with \(p\) the number of parameters including the intercept and \(m\) number of samples).</p>
<p>__</p>
<p>Question 8.13</p>
<p>Check the documentation to see how the default cutoff can be changed, and how the outlier removal functionality can be disabled altogether. How can the computed Cook’s distances be accessed?</p>
<p>With many degrees of freedom – i.e., many more samples than number of parameters to be estimated – it might be undesirable to remove entire genes from the analysis just because their data include a single count outlier. An alternate strategy is to replace the outlier counts with the trimmed mean over all samples, adjusted by the size factor for that sample. This approach is conservative: it will not lead to false positives, as it replaces the outlier value with the value predicted by the null hypothesis.</p>
</section>
<section id="tests-of-_2-fold-change-above-or-below-a-threshold" class="level3" data-number="10.9.4">
<h3 data-number="10.9.4" class="anchored" data-anchor-id="tests-of-_2-fold-change-above-or-below-a-threshold"><span class="header-section-number">10.9.4</span> 8.10.4 Tests of \(_2\) fold change above or below a threshold</h3>
<p>Let’s come back to the point we raised in Section 8.6: how to build into the tests our requirement that we want to detect effects that have a strong enough size, as opposed to ones that are statistically significant, but very small. Two arguments to the <code>results</code> function allow for threshold-based Wald tests: <code>lfcThreshold</code>, which takes a numeric of a non-negative threshold value, and <code>altHypothesis</code>, which specifies the kind of test. It can take one of the following four values, where \(\) is the \(_2\) fold change specified by the <code>name</code> argument, and \(\) represents <code>lfcThreshold</code>:</p>
<ul>
<li><p><code>greater</code>: \(&gt; \)</p></li>
<li><p><code>less</code>: \(&lt; (-)\)</p></li>
<li><p><code>greaterAbs</code>: \(|| &gt; \) (two-tailed test)</p></li>
<li><p><code>lessAbs</code>: \(|| &lt; \) (p-values are the maximum of the upper and lower tests)</p></li>
</ul>
<p>These are demonstrated in the following code and visually by MA-plots in Figure 8.13. (Note that the <code>plotMA</code> method, which is defined in the <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> package, uses base graphics.)</p>
<pre><code>par(mfrow = c(4, 1), mar = c(2, 2, 1, 1))
myMA = function(h, v, theta = 0.5) {
  plotMA(pasilla, lfcThreshold = theta, altHypothesis = h,
         ylim = c(-2.5, 2.5))
  abline(h = v * theta, col = "dodgerblue", lwd = 2)
}
myMA("greaterAbs", c(-1, 1))
myMA("lessAbs",    c(-1, 1))
myMA("greater",          1)
myMA("less",         -1   )__</code></pre>
<p>[<img src="08-chap_files/figure-html/fig-countdata- lfcThresh-1.png" class="img-fluid">](08-chap_files/figure-html/fig-countdata-lfcThresh-1.png “Figure&nbsp;8.13: MA-plots of tests of \log_2 fold change with respect to a threshold value. From top to bottom, the tests are for altHypothesis =”greaterAbs”, “lessAbs”, “greater”, and “less”.”)</p>
<p>Figure 8.13: MA-plots of tests of \(_2\) fold change with respect to a threshold value. From top to bottom, the tests are for <code>altHypothesis = "greaterAbs"</code>, <code>"lessAbs"</code>, <code>"greater"</code>, and <code>"less"</code>.</p>
<p>To produce the results tables instead of MA plots, the same arguments as to <code>plotMA</code> (except <code>ylim</code>) would be provided to the <code>results</code> function.</p>
</section>
</section>
<section id="summary-of-this-chapter" class="level2" data-number="10.10">
<h2 data-number="10.10" class="anchored" data-anchor-id="summary-of-this-chapter"><span class="header-section-number">10.10</span> 8.11 Summary of this chapter</h2>
<p>We have seen how to analyze count tables from high-throughput sequencing (and analagous data types) for differential abundance. We built upon the powerful and elegant framework of linear models. In this framework, we can analyze a basic two-groups comparison as well as more complex multifactorial designs, or experiments with covariates that have more than two levels or are continuous. In ordinary linear models, the sampling distribution of the data around the expected value is assumed to be independent and normal, with zero mean and the same variances. For count data, the distributions are discrete and tend to be skewed (asymmetric) with highly different variances across the dynamic range. We therefore employed a generalization of ordinary linear models, called generalized linear models (GLMs), and in particular considered gamma-Poisson distributed data with dispersion parameters that we needed to estimate from the data.</p>
<p>Since the sampling depth is typically different for different sequencing runs (replicates), we need to estimate the effect of this variable parameter and take it into account in our model. We did this through the size factors \(s_i\). Often this part of the analysis is called <em>normalization</em> (the term is not particularly descriptive, but unfortunately it is now well-settled in the literature).</p>
<p>For designed experiments, the number of replicates is (and should be) usually too small to estimate the dispersion parameter (and perhaps even the model coefficients) from the data for each gene alone. Therefore we use shrinkage or empirical Bayes techniques, which promise large gains in precision for relatively small costs of bias.</p>
<p>While GLMs let us model the data on their original scale, sometimes it is useful to transform the data to a scale where the data are more homoskedastic and fill out the range more uniformly – for instance, for plotting the data, or for subjecting them to general purpose clustering, dimension reduction or learning methods. To this end, we saw the variance stabilizing transformation.</p>
<p>A major, and quite valid critique of differential expression testing such as exercised here is that the null hypothesis – the effect size is exactly zero – is almost never true, and therefore our approach does not provide consistent estimates of what the differentially expressed gene are. In practice, this may be overcome by considering effect size as well as statistical significance. Moreover, we saw how to use “banded” null hypotheses.</p>
</section>
<section id="further-reading" class="level2" data-number="10.11">
<h2 data-number="10.11" class="anchored" data-anchor-id="further-reading"><span class="header-section-number">10.11</span> 8.12 Further reading</h2>
<ul>
<li><p>The <strong><a href="https://bioconductor.org/packages/DESeq2/">DESeq2</a></strong> method is explained in the paper by Michael I. Love, Huber, and Anders (<a href="16-chap.html#ref-LoveDESeq2">2014</a>), and practical aspects of the software in the package vignette. See also the <strong><a href="https://bioconductor.org/packages/edgeR/">edgeR</a></strong> package and paper (<a href="16-chap.html#ref-edgeR:Robinson:2009">Robinson, McCarthy, and Smyth 2009</a>) for a related approach.</p></li>
<li><p>A classic textbook on robust regression and outlier detection is the book by Peter J. Rousseeuw and Leroy (<a href="16-chap.html#ref-Rousseeuw:RobustBook:1987">1987</a>). For more recent developments the <a href="https://cran.r-project.org/web/views/Robust.html">CRAN task view on Robust Statistical Methods</a> is a good starting point.</p></li>
<li><p>The Bioconductor RNA-Seq workflow at <a href="https://www.bioconductor.org/help/workflows/rnaseqGene" class="uri">https://www.bioconductor.org/help/workflows/rnaseqGene</a> (<a href="16-chap.html#ref-BiocRNASeqWorkflow">Michael I. Love et al.&nbsp;2015</a>) covers a number of issues related specifically to RNA-Seq that we have sidestepped here.</p></li>
<li><p>An extension of the generalized linear model that we saw to detecting alternative exon usage from RNA-Seq data is presented in the <strong><a href="https://bioconductor.org/packages/DEXSeq/">DEXSeq</a></strong> paper (<a href="16-chap.html#ref-Reyes:GnomeResearch:2012">Anders, Reyes, and Huber 2012</a>), and applications of these ideas to biological discovery were described by Reyes et al.&nbsp;(<a href="16-chap.html#ref-Reyes:PNAS:2013">2013</a>) and Reyes and Huber (<a href="16-chap.html#ref-Reyes:NAR:2017">2017</a>).</p></li>
<li><p>For some sequencing-based assays, such as RIP-Seq, CLIP-Seq, the biological analysis goal boils down to testing whether the ratio of <em>input</em> and <em>immunoprecipitate</em> (IP) has changed between conditions. Mike Love’s post on the Bioconductor forum provides a clear and quick how-to: <a href="https://support.bioconductor.org/p/61509" class="uri">https://support.bioconductor.org/p/61509</a>.</p></li>
</ul>
</section>
<section id="exercises" class="level2" data-number="10.12">
<h2 data-number="10.12" class="anchored" data-anchor-id="exercises"><span class="header-section-number">10.12</span> 8.13 Exercises</h2>
<p>__</p>
<p>Exercise 8.1</p>
<p><strong>Depletion of small p-values.</strong> Consider the following simple generative model for a histogram of p-values that shows a depletion of small p-values. In Figure 8.14, p-values are shown from a differential expression analysis (in this case, simple \(t\)-tests) in the absence of an association with the tested two-level categorical variable <code>groups</code>. While the histogram is approximately uniform for <code>x1</code>, small p-values are depleted for <code>x2</code>. This is because the batch (encoded by the eponymous variable), which is orthogonal to <code>groups</code> and balanced, introduces additional variability that inflates the denominator of the test statistic.</p>
<pre><code>library("magrittr")
ng = 10000
ns = 12
x1 = x2 = matrix(rnorm(ns * ng), ncol = ns, nrow= ng)
group = factor(letters[1 + seq_len(ns) %% 2])  %T&gt;% print __


 [1] b a b a b a b a b a b a
Levels: a b


batch = factor(ifelse(seq_len(ns) &lt;= ns/2, "B1", "B2")) %T&gt;% print __


 [1] B1 B1 B1 B1 B1 B1 B2 B2 B2 B2 B2 B2
Levels: B1 B2


table(group, batch)__


     batch
group B1 B2
    a  3  3
    b  3  3


x2[, batch=="B2"] = x2[, batch=="B2"] + 2 * rnorm(ng)
pvals = rbind(
  cbind(type = "x1", genefilter::rowttests(x1, fac = group)),
  cbind(type = "x2", genefilter::rowttests(x2, fac = group)))
ggplot(pvals, aes(x = p.value)) + 
  geom_histogram(binwidth = 0.02, boundary = 0) +
  facet_grid(type ~ .)__</code></pre>
<p>Replace the \(t\)-test by a linear model, first, one with only <code>group</code> as a factor, second, one with <code>group + batch</code> (in R’s formula language). Show that the histogram of p-values for the coefficient of <code>group</code> is uniform in both cases, <code>x1</code> and <code>x2</code>.</p>
<p><a href="08-chap_files/figure-html/fig-countdata-exbatch-1.png &quot;Figure 8.14: p-values for the tests performed on x1 and x2 (see code).&quot;"><img src="08-chap_files/figure-html/fig-countdata- exbatch-1.png" class="img-fluid"></a></p>
<p>Figure 8.14: p-values for the tests performed on <code>x1</code> and <code>x2</code> (see code).</p>
<p>__</p>
<p>Exercise 8.2</p>
<p><strong>edgeR.</strong> Do the analyses of Section 8.5 with the <strong><a href="https://bioconductor.org/packages/edgeR/">edgeR</a></strong> package and compare the results: make a scatterplot of the \(_{10}\) p-values, pick some genes where there are large differences, and visualize the raw data to see what is going on. Based on this can you explain the differences?</p>
<p>__</p>
<p>Exercise 8.3</p>
<p><strong>Robustness.</strong> Write a <strong><a href="https://cran.r-project.org/web/packages/shiny/">shiny</a></strong> app that performs linear regression on an example \((x, y)\) dataset (for instance, from the <code>mtcars</code> data) and displays the data as well as the fitted line. Add a widget that lets you move one of the points in \(x\)- and/or \(y\)- direction in a wide range (extending a few times outside the original data range). Add a radio buttons widget that lets you choose between <code>lm</code>, <code>rlm</code> and <code>lqs</code> with its different choices of <code>method</code> (the latter two are in the <strong><a href="https://cran.r-project.org/web/packages/MASS/">MASS</a></strong> package). Bonus: add functions from the <strong><a href="https://cran.r-project.org/web/packages/robustbase/">robustbase</a></strong> package.</p>
<p>__</p>
<p>Solution</p>
<p>__</p>
<p>Code for the file <code>ui.R</code> in the app:</p>
<pre><code>library("shiny")
shinyUI(fluidPage(
  titlePanel("Breakdown"),
  sidebarLayout(
    sidebarPanel(     # select oulier shift
      sliderInput("shift", "Outlier:", min = 0, max = 100, value = 0),
      radioButtons("method", "Method:",
                   c("Non-robust least squares" = "lm",
                     "M-estimation" = "rlm"))
    ),
    mainPanel(       # show fit
      plotOutput("regPlot")
    )
  )
))__</code></pre>
<p>Code for the file <code>server.R</code> in the app:</p>
<pre><code>library("shiny")
library("ggplot2")
library("MASS")
shinyServer(function(input, output) {
  output$regPlot = renderPlot({
    whpt = 15
    mtcars_new = mtcars
    mtcars_new$mpg[whpt] = mtcars_new$mpg[whpt] + input$shift
    reg = switch(input$method,
      lm = lm(mpg ~ disp, data = mtcars_new),
      rlm = rlm(mpg ~ disp, data = mtcars_new),
      stop("Unimplemented method:", input$method)
    )
    ggplot(mtcars_new, aes(x = disp, y = mpg)) + geom_point() +
      geom_abline(intercept = reg$coefficients["(Intercept)"],
                  slope = reg$coefficients["disp"], col = "blue")
  })
})__</code></pre>
<p>Of course you can add many more features.</p>
<p>Anders, Simon, and Wolfgang Huber. 2010. “Differential Expression Analysis for Sequence Count Data.” <em>Genome Biology</em> 11: R106. <a href="http://genomebiology.com/2010/11/10/R106" class="uri">http://genomebiology.com/2010/11/10/R106</a>.</p>
<p>Anders, Simon, Alejandro Reyes, and Wolfgang Huber. 2012. “Detecting differential usage of exons from RNA-Seq data.” <em>Genome Research</em> 22 (10): 2008–17.</p>
<p>Brooks, Angela N, Li Yang, Michael O Duff, Kasper D Hansen, Jung W Park, Sandrine Dudoit, Steven E Brenner, and Brenton R Graveley. 2011. “Conservation of an RNA Regulatory Map Between Drosophila and Mammals.” <em>Genome Research</em> , 193–202. <a href="https://doi.org/10.1101/gr.108662.110" class="uri">https://doi.org/10.1101/gr.108662.110</a>.</p>
<p>Cook, R. Dennis. 1977. “Detection of Influential Observation in Linear Regression.” <em>Technometrics</em>.</p>
<p>Huber, Peter J. 1964. “Robust Estimation of a Location Parameter.” <em>The Annals of Mathematical Statistics</em> 35: 73–101.</p>
<p>Leek, Jeffrey T., and John D. Storey. 2007. “Capturing heterogeneity in gene expression studies by surrogate variable analysis.” <em>PLoS Genetics</em> 3 (9): 1724–35.</p>
<p>Love, Michael I., Simon Anders, Vladislav Kim, and Wolfgang Huber. 2015. “RNA- Seq Workflow: Gene-Level Exploratory Analysis and Differential Expression.” <em>F1000Research</em> 4 (1070). <a href="https://doi.org/10.12688/f1000research.7035.1" class="uri">https://doi.org/10.12688/f1000research.7035.1</a>.</p>
<p>Love, Michael I, Wolfgang Huber, and Simon Anders. 2014. “Moderated Estimation of Fold Change and Dispersion for RNA-seq Data with DESeq2.” <em>Gnome Biology</em> 15 (12): 1–21.</p>
<p>Ozsolak, Fatih, and Patrice M Milos. 2011. “RNA sequencing: advances, challenges and opportunities.” <em>Nature Reviews Genetics</em> 12: 87–98.</p>
<p>Reyes, Alejandro, Simon Anders, Robert J. Weatheritt, Toby J. Gibson, Lars M. Steinmetz, and Wolfgang Huber. 2013. “Drift and Conservation of Differential Exon Usage Across Tissues in Primate Species.” <em>Proceedings of the National Academy of Sciences</em> 110 (38): 15377–82. <a href="https://doi.org/10.1073/pnas.1307202110" class="uri">https://doi.org/10.1073/pnas.1307202110</a>.</p>
<p>Reyes, Alejandro, and Wolfgang Huber. 2017. “Alternative Start and Termination Sites of Transcription Drive Most Transcript Isoform Differences Across Human Tissues.” <em>Nucleic Acids Research</em> 46 (2): 582–92. <a href="https://doi.org/10.1093/nar/gkx1165" class="uri">https://doi.org/10.1093/nar/gkx1165</a>.</p>
<p>Robinson, M. D., D. J. McCarthy, and G. K. Smyth. 2009. “edgeR: A Bioconductor Package for Differential Expression Analysis of Digital Gene Expression Data.” <em>Bioinformatics</em> 26 (1): 139–40. <a href="https://doi.org/10.1093/bioinformatics/btp616" class="uri">https://doi.org/10.1093/bioinformatics/btp616</a>.</p>
<p>Rousseeuw, Peter J. 1987. “Silhouettes: A Graphical Aid to the Interpretation and Validation of Cluster Analysis.” <em>Journal of Computational and Applied Mathematics</em> 20: 53–65.</p>
<p>Rousseeuw, Peter J., and Annick M. Leroy. 1987. <em>Robust Regression and Outlier Detection</em>. Wiley. <a href="https://doi.org/10.1002/0471725382" class="uri">https://doi.org/10.1002/0471725382</a>.</p>
<p>Stegle, O., L. Parts, R. Durbin, and J. Winn. 2010. “A Bayesian framework to account for complex non-genetic factors in gene expression levels greatly increases power in eQTL studies.” <em>PLoS Computational Biology</em> 6 (5): e1000770.</p>
<p>Steijger, T., J. F. Abril, P. G. Engstrom, F. Kokocinski, T. J. Hubbard, R. Guigo, J. Harrow, et al.&nbsp;2013. “Assessment of transcript reconstruction methods for RNA-seq.” <em>Nature Methods</em> 10 (12): 1177–84.</p>
<p>Verhulst, Pierre-François. 1845. “Recherches mathématiques Sur La Loi d’accroissement de La Population.” <em>Nouveaux Mémoires de l’Académie Royale Des Sciences Et Belles-Lettres de Bruxelles</em> 18: 1–42.</p>
<p>Page built at 01:33 on 2025-09-01 using R version 4.5.1 (2025-06-13)</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./07-chap.html" class="pagination-link" aria-label="7.1 Goals for this chapter">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">7.1 Goals for this chapter</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./09-chap.html" class="pagination-link" aria-label="9.1 Goals for this chapter">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">9.1 Goals for this chapter</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>